{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6951b35a-bc9b-41e1-ae21-441ea6c12594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb7d2e40-6d7b-427d-818c-c016e8f0e86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>value</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W12-2037</td>\n",
       "      <td>performance</td>\n",
       "      <td>Experimental results using Wikipedia as the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P07-1058</td>\n",
       "      <td>performance</td>\n",
       "      <td>The current performance level only stresses th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C14-1013</td>\n",
       "      <td>building on past work</td>\n",
       "      <td>We extract the terms from Twitter, and from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C14-1013</td>\n",
       "      <td>novelty</td>\n",
       "      <td>We evaluate our system with the local search t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C14-1013</td>\n",
       "      <td>performance</td>\n",
       "      <td>For precision at rank three, the improvement f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>W19-1410</td>\n",
       "      <td>reproducibility</td>\n",
       "      <td>We use the same POS tag sets for all language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>W16-4804</td>\n",
       "      <td>building on past work</td>\n",
       "      <td>State-of-the-art approaches to related languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>W16-4804</td>\n",
       "      <td>novelty</td>\n",
       "      <td>This paper describes the GW/LT3 contribution t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>W16-4804</td>\n",
       "      <td>performance</td>\n",
       "      <td>GW/LT3 ranked first in the out-of-domain evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5759</th>\n",
       "      <td>W12-2037</td>\n",
       "      <td>novelty</td>\n",
       "      <td>We propose to address the task of helping read...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2108 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acl_id                  value  \\\n",
       "0     W12-2037            performance   \n",
       "6     P07-1058            performance   \n",
       "8     C14-1013  building on past work   \n",
       "11    C14-1013                novelty   \n",
       "12    C14-1013            performance   \n",
       "...        ...                    ...   \n",
       "5749  W19-1410        reproducibility   \n",
       "5750  W16-4804  building on past work   \n",
       "5753  W16-4804                novelty   \n",
       "5754  W16-4804            performance   \n",
       "5759  W12-2037                novelty   \n",
       "\n",
       "                                               response  \n",
       "0     Experimental results using Wikipedia as the co...  \n",
       "6     The current performance level only stresses th...  \n",
       "8     We extract the terms from Twitter, and from th...  \n",
       "11    We evaluate our system with the local search t...  \n",
       "12    For precision at rank three, the improvement f...  \n",
       "...                                                 ...  \n",
       "5749  We use the same POS tag sets for all language ...  \n",
       "5750  State-of-the-art approaches to related languag...  \n",
       "5753  This paper describes the GW/LT3 contribution t...  \n",
       "5754  GW/LT3 ranked first in the out-of-domain evalu...  \n",
       "5759  We propose to address the task of helping read...  \n",
       "\n",
       "[2108 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_acl_gemini.csv')\n",
    "df = df[df.response.notna()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9272e535-0d97-4f2c-a0d9-b77732a3a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d00e6522-f3a3-407b-b304-d5667485e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance\n",
      "Number of documents: 766\n",
      "Best number of topics: 2\n",
      "Best topic coherence: -1.3238947170084712\n",
      "building on past work\n",
      "Number of documents: 576\n",
      "Best number of topics: 3\n",
      "Best topic coherence: -1.3323251786671475\n",
      "novelty\n",
      "Number of documents: 573\n",
      "Best number of topics: 2\n",
      "Best topic coherence: -1.4978714306686434\n",
      "ease of implementation\n",
      "Number of documents: 50\n",
      "Best number of topics: 30\n",
      "Best topic coherence: -6.843512833280342\n",
      "reproducibility\n",
      "Number of documents: 139\n",
      "Best number of topics: 2\n",
      "Best topic coherence: -1.6876909811274983\n",
      "fairness\n",
      "Number of documents: 4\n",
      "Best number of topics: 4\n",
      "Best topic coherence: -5.518855957761511\n"
     ]
    }
   ],
   "source": [
    "for v in df.value.unique():\n",
    "    print(v)\n",
    "    data = df[df.value == v].response\n",
    "    print('Number of documents:', len(data))\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=2)\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "    corpus = gensim.matutils.Sparse2Corpus(data_vectorized, documents_columns=False)\n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus, id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "    data_processed = data.apply(preprocess_text)\n",
    "    num_topics_list = [2, 3, 4, 5, 6, 8, 10, 15, 20, 25, 30]\n",
    "    best_model = None\n",
    "    best_num_topics = 0\n",
    "    best_coherence = -float('inf')\n",
    "    for n in num_topics_list:\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=n, passes=20, random_state=0)\n",
    "        coherence = CoherenceModel(model=lda_model, texts=data_processed, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "        if coherence > best_coherence:\n",
    "            best_model = lda_model\n",
    "            best_num_topics = n\n",
    "            best_coherence = coherence\n",
    "    print('Best number of topics:', best_num_topics)\n",
    "    print('Best topic coherence:', best_coherence)\n",
    "    topics = data_processed.map(lambda doc: max(best_model[dictionary.doc2bow(doc)], key=lambda x: x[1])[0])\n",
    "    probs = data_processed.map(lambda doc: max(best_model[dictionary.doc2bow(doc)], key=lambda x: x[1])[1])\n",
    "    acl_id = df[df.value == v].acl_id\n",
    "    topic_words_dict = {i: [w for w, p in t] for i, t in best_model.show_topics(num_words=10, formatted=False)}\n",
    "    topic_words = topics.map(topic_words_dict)\n",
    "    labeled_topics = pd.DataFrame({'acl_id': acl_id, 'topic': topics, 'topic_words': topic_words, 'probability': probs})\n",
    "    labeled_topics.to_csv(f'topic_data/{v}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
