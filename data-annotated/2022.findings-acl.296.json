{
    "framing_question": "If this paper introduces a task or benchmark, which communities does it state are the subject of the task or benchmark, if any?",
    "response": "The paper introduces a task and benchmark for answer verification in QA-based summarization evaluation metrics. The target community for this task is researchers and practitioners working on QA-based summarization evaluation metrics.",
    "article": "Question answering-based summarization evaluation metrics must automatically determine whether the QA model's prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC. We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others. However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method -or using none at all -has comparable performance to using the best verification method, a result that we attribute to properties of the datasets. 1 Introduction A recent trend in summarization metrics is evaluating the quality of a summary via question answering (QA; Eyal et al., 2019; Scialom et al., 2019 Scialom et al., , 2021;; Durmus et al., 2020; Wang et al., 2020; Deutsch et al., 2021a) . These metrics compare the semantic content of two texts (e.g., the reference and candidate summaries) by generating questions from one and answering those questions against the other. The amount of common semantic content is proportional to the number of questions which are answered correctly. A critical step of QA-based evaluation metrics is to verify whether the QA model's prediction is correct, a task known as answer verification (see Fig. 1 ). This helps to both suppress noisy output from the QA model as well as identify inconsistent information across the texts. He was rescued by his parents before what arrived on the scene? Question Figure 1 : In the answer verification task, the metrics score how likely two phrases from different contexts have the same meaning. Here, the metrics at the bottom score the similarity between \"emergency responders,\" which was used to generate the question from the source text, and \"paramedics,\" the predicted answer from a QA model in the target text. Answer verification is typically done by comparing the prediction to the expected answer by the exact match or token F 1 string comparison methods (Rajpurkar et al., 2016) . However, more sophisticated text comparison methods have been proposed in recent years, and it is unknown whether they provide a benefit in this particular scenario. In this work, we benchmark various answer verification strategies for QA-based summarization evaluation metrics. Our goal is to understand whether methods that are more advanced than lexical overlap are better able to classify phrases as having the same or different meaning as well as whether any such improvements result in the overall QA-based metric being better at replicating human judgments of summary quality. We analyze four answer verification methods, exact match, token F 1 , BERTScore (Zhang et al., 2020) , and LERC, (Chen et al., 2020) in combination with two QA-based metrics, QAEval (Deutsch et al., 2021a) and FEQA (Durmus et al., 2020) . Based on a set of human annotations across two datasets, we find that LERC performs the best at the actual task of answer verification in general, although in some settings it is statistically indistinguishable from token F 1 ( \u00a74.1). However, our results also show that any such improvement in verification performance does not always translate to a better QA-based evaluation metric ( \u00a74.2). We believe these results can be explained by properties of the QA metrics and the datasets. When the QA model performance is high or the verification task is in some sense easy to do, it may not be necessary to have a sophisticated verification method or even use one at all. Despite this, our recommendation is to use both token F 1 and LERC for answer verification since F 1 may suffice in some situations and we suspect LERC does provide additional benefits, although they are difficult to measure. Related Work & Background The majority of summarization evaluation metrics can be viewed as estimating how similar in meaning two pieces of text are. For instance, ROUGE (Lin, 2004) does this by calculating the number of overlapping n-grams between the two texts. Instead of directly comparing the entire texts, QA-based metrics identify specific phrases within the texts which should be compared, as follows. First, a set of questions is automatically generated from one text. Then, those questions are automatically answered against a second text to obtain a set of predicted answers. The final score is proportional to the number of correct predictions, but determining whether those predictions are correct (the task of answer verification) is done by comparing the text of the prediction to the expected answer. Therefore, instead of directly comparing the entire contents of the two texts, QA-based metrics instead reduce the scope of the problem to only comparing specific pairs of phrases. Current QA-based metrics perform the answer verification step by lexical comparison, either exact match or token F 1 . Such metrics include QA-Eval (Deutsch et al., 2021a) , FEQA (Durmus et al., 2020) , and more (Eyal et al., 2019; Wang et al., 2020; Scialom et al., 2019 Scialom et al., , 2021)) . However, any such function which calculates the similarity of arbitrary text can be used instead. This includes embedding-based methods such as BERTScore (Zhang et al., 2020) or metrics which have been trained specifically to do this task, such as LERC (Chen et al., 2020) . Evaluating how these methods perform as answer verification methods for QAbased metrics compared to the lexical baselines is the scope of this work. Other, related work has also benchmarked various answer verification methods (Chen et al., 2019) , but do so as a method for evaluating QA performance rather than as part of a downstream task, as we do in this work. Some concurrent work also tries to improve answer verification by expanding the set of possible expected answers via mining additional aliases from knowledge bases (Si et al., 2021) . Definitions & Methods We define the answer verification task as the following: Given a question, answer, the source text from which the QA pair was generated, a prediction, and the target text the prediction comes from, score how similar the meanings of the answer and prediction are (see Fig. 1 for an example). 2 Answer verification is used by QA-based metrics to suppress noisy outputs from the QA model as well as identify when the QA prediction is correct with respect to the target text but incorrect with respect to the expected answer (e.g., unfaithful information). We analyze four different answer verification methods. Exact Match The exact match (EM) method compares the two phrases to see if they are identical (after light normalization). EM assigns a score of 1 if the phrases are identical and 0 otherwise. Token F 1 The token F 1 comparison calculates an F 1 score based on the number of unigrams the two phrases have in common. This is equivalent to the F 1 variant of ROUGE-1. BERTScore BERTScore (Zhang et al., 2020) compares two pieces of text by aligning the texts' tokens according to which pairs have the highest BERT embedding cosine similarity. We adapt BERTScore to answer verification by encoding the answer and prediction using their respective contexts, then calculating the BERTScore only between the two phrase encodings. Since the output of BERTScore is often in a narrow range of values, we rescale the scores by defining 0 and 1 as the 2.5th and 97.5th percentiles of the BERTScores calculated over the whole dataset. These changes were made to make the score more interpretable as well as prevent outliers from influencing the score rescaling. In practice, we compute the BERTScore using embeddings obtained from RoBERTa-Large (Liu et al., 2019) . LERC Chen et al. (2020) proposed LERC, a learned metric for scoring how similar the expected and predicted answers to a question are conditioned on the question and the target text the prediction comes from. The metric takes as input the target context, question, expected answer, and predicted answer and concatenates them into a single sequence separated by speical tokens. Because it was designed for scoring reading comprehension predictions, it does not use the source text. It then encodes the entire sequence with BERT and trains a regression layer on top of the encodings to predict a similarity score. The learned metric was fine-tuned on 40k human annotations of how similar the two answers are on a scale from 1 to 5. We rescale the output from LERC to be in the range [0, 1]. Experiments The answer verification methods are evaluated independently ( \u00a74.1) as well as in combination with two QA-based metrics ( \u00a74.2), QAEval (Deutsch et al., 2021a) and FEQA (Durmus et al., 2020) . QAEval measures the content quality of a summary (does the summary contain \"summary-worthy\" information) by using a reference summary as the source text and candidate summary as the target text. In contrast, FEQA estimates the faithfulness of the summary (does the summary contain information consistent with the input) by using the candidate summary as the source text and the input document as the target text. The experiments are run on two datasets, TAC'08 (Dang and Owczarzak, 2008) and Summ-Eval (Fabbri et al., 2021) . These datasets have summaries generated by 58 and 16 models for 48 and 100 inputs, respectively, which are annotated with expert judgments. Both QAEval and FEQA are evaluated on SummEval because it contains annotations for both summary quality and faithfulness, whereas only QAEval is evaluated on TAC'08 since it does not have faithfulness judgments. Answer Verification Performance First, we examine how well each answer verification method accurately scores manually labeled answer pairs from the summarization datasets. For each QA metric and dataset combination, we ran the metric on the summaries, then randomly sampled 200 QA predictions (making 600 total). Each prediction and expected answer were manually annotated by the authors for whether or not the two phrases share the same meaning. See Appendix A for additional details on the annotation procedure. Ideally, the answer verification methods should both successfully classify phrases based on their meaning as well as provide a score close to 1 for phrases with the same meaning and close to 0 with different meanings. These properties are quantified by the binary classification accuracy (assigning labels based on a threshold which maximizes this score) as well as the mean squared error (MSE) of the predicted scores, show in Table 1 . We find that LERC is the only method with the best (or tied for the best) performance across all three metric-dataset combinations. Despite LERC's significant improvement on the SummEval data with QAEval predictions, it is statistically indistinguishable from F 1 on the same dataset with FEQA predictions. We believe this can be explained by which texts are being compared for each metric. FEQA compares the generated summary to the input document. Recent summarization models are known to copy heavily from the input with little high-level abstraction or rephrasing, so comparing phrases with token F 1 is likely to be quite successful. In contrast, QAEval compares the reference and generated summaries. The reference summaries are written by humans, and thus more likely to contain information from the input document which is expressed differently. In such a scenario, the learned metric, LERC, shows strong improvements over F 1 . In general, we find that when BERTScore and LERC do improve over F 1 , they do so by identifying paraphrases that have no tokens in common, which sometimes requires world knowledge. Examples of this are included in Appendix C. Overall Metric Evaluation Next, we investigate whether the differences in classification performance of the verification methods translate to downstream improvements in the overall quality of the QA-based metrics. To do so, we evaluate different variants of the metrics that use each answer verification method. For both QAEval and FEQA, the final score for the summary is the output of the answer verification method averaged over all of the QA pairs. 3 QAEval For QAEval, we report the standard system-and summary-level correlations of the metrics' scores to human judgments in Table 2 (due to space constraints, we refer the reader to Deutsch et al. (2021b) for definitions of the correlations). We also compare against the standard BERTScore and ROUGE metrics as well as a QAEval variant which uses no answer verification by always marking the phrases as correct if the QA model predicts the question is answerable, denoted QAEval-IsAns. In general, all of the answer verification methods work comparably well, although BERTScore and LERC do statistically improve over the lexical methods in some settings, but not by large margins. We believe the performance of QAEval-IsAns offers an explanation as follows. Answer verification is not necessary if the QA model is perfect and the summaries are faithful (i.e., the QA prediction is always correct). For SummEval, Deutsch et al. (2021a) demonstrated that QAEval's QA performance was reasonable, and the summaries are very faithful with an average consistency score of 4.7 / 5 according to Fabbri et al. (2021) . Therefore, it may be difficult to demonstrate an improvement with any answer verification method even if it is high quality since the need for answer verification is low. Indeed, we see QAEval-IsAns statistically ties the best methods. On TAC'08, we expect it should be easier to show answer verification helps since Deutsch et al. (2021a) showed the QA performance is poor, suggesting answer verification could help to suppress noisy predictions. Indeed, we do see QAEval-IsAns is statistically out-performed by the verification methods. We suspect the improvements are larger at the system-level than the summarylevel because the system quality is estimated over a larger number of QA pairs than an individual summary's quality is. A larger number of questions reduces any noise introduced by the verification methods, resulting in a more accurate estimate of summary quality and a better metric. FEQA We report the direct correlations between the human judgments and the FEQA variants, ROUGE, BERTScore, and FactCC (Kryscinski et al., 2020) in Table 3 . FactCC is a learned model to predict the factual consistency between two texts that was trained on synthetically generated data. Among the FEQA variants, F 1 is the best or indistinguishable from LERC. This result is expected given how similarly they perform at answer verification on this QA metric and dataset split. This is again likely due to the fact that the summarization models copy heavily from the input documents, so the expected answers and QA model predictions are likely to be quite lexically similar. Overall, the FEQA correlations are still lower than those by FactCC by a large margin. It is also worth nothing that FEQA's correlations are lower than ROUGE-2's, a result which contradicts the findings of Durmus et al. (2020) . However, our experiments were conducted on a different dataset than theirs, and the two datasets' faithfulness scores were annotated in different ways. Thus, we suspect the different conclusions are due different experimental setups; the results cannot necessarily be fairly compared. Conclusion In this work, we benchmarked four different answer verification methods for QA-based summarization evaluation metrics. Although we were able to identify that some methods perform better than others at verification, any such improvement does not necessarily translate a better overall metric quality. We hypothesize that several factors, including the quality of the QA model and properties of the datasets, likely explain this result. Even though token F 1 may be sufficient in some scenarios, we also recommend that practitioners also use LERC since it is likely to provide additional benefits, even if they are not easily measured. A Annotation Details In total, 600 pairs of expected and predicted answers were annotated by one of the authors for whether or not they shared the same meaning. The 600 pairs were sampled as follows: QAEval was used to generate and predict questions on TAC'08 and SummEval and likewise for FEQA on Summ-Eval. Then, 200 questions were sampled uniformly at random from each metric and dataset combination. The criteria for determining whether the two answers conveyed the same meaning was whether they could both be appropriately be used as synonyms given the input context and question. In general, the annotation procedure was relatively straightforward with the majority of the answer pairs being clear synonyms of each other. Example pairs are shown in Table 5 . Some decisions did require world knowledge (e.g., \"Luis Enrique's side\" and \"Barcelona\"), whereas others were clear synonyms (\"EU\" and \"European Union\") or required resolving pronouns. Decisions in cases which were not clear were based on the author's judgment of whether the two phrases seemed equally acceptable to use to answer the question, erring on the side of deciding the phrases are not semantically equivalent. These cases were relatively uncommon. B Additional Results Fig. 2 contains the distributions of score values for token F 1 , BERTScore, and LERC on the Summ-Eval dataset grouped by phrases that have and do no have the same meaning. LERC most confidently separates the positive and negative examples. F 1 performs similarly, except it fails in a large number of cases when the two phrases have no tokens in common. BERTScore tends to mix the scores of the positive and negative classes, although they are separated on average. score of 0). Successfully classing these phrases requires paraphrasing (e.g., \"the child\" and \"toddler\") and, in some cases, world knowledge (e.g., Usain Bolt had won six gold medals when the article was written). In Table 4 , we report the system-and summarylevel correlations on TAC'08 and SummEval with Pearson's r and Spearman's \u03c1 correlation coefficients in addition to the Kendall's \u03c4 which was presented in the main body of the paper. The other coefficients lead to a similar conclusion to that which we made with Kendall's \u03c4 : All answer verification methods perform comparably well, and when BERTScore or LERC does improve over a lexical baseline, it is not by a large margin. Further, using no verification method (QAEval-IsAns) largely performs equally well as QAEval variants which do use a verification step on the SummEval dataset, but not on TAC'08. C Example BERTScore/LERC Improvements Table 5 contains example expected answer and QA model prediction pairs for which BERTScore and LERC improve over exact match and token F 1 . We see that the improvements come from better identifying when the phrases are paraphrases of each other, which sometimes involves world knowledge. Acknowledgments The authors would like to thank the anonymous reviews for their helpful suggestions, which we used to improve the final version of our work. This research is supported by a Focused Award from Google and Contracts FA8750-19-2-0201 and FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.",
    "funding": {
        "military": 1,
        "corporate": 1,
        "research agency": 0,
        "foundation": 0,
        "none": 0
    }
}