{
    "article": "Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases, we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News, Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools. Introduction This paper presents the results of our test suite for WMT20 News Translation Task. 1  The conclusion of Vojt\u011bchov\u00e1 et al. (2019) , a last year's similar effort, states that expert knowledge is vital for correct and comprehensible translation of professional domains, such as Audits or Lease agreements. Furthermore, even MT systems which make fewer mistakes and score above others in both automatic and manual evaluations are prone to making fatal errors related to markable conflicts, which render the whole document translation unusable. In this study, we aim to organize and describe a more detailed study with a higher number of annotators. We show three evaluation approaches: (1) automatic evaluation, (2) fluency and adequacy per document line and (3) detailed markable phenomena evaluation. We compare the results of this evaluation across the three domains and try to explain why all of these evaluations do not produce the same ordering of MT systems by performance. This paper is organized accordingly: Section 1.1 defines the term \"Markable\", Section 1.2 describes the examined documents and Section 2 introduces the two phases of our annotation experiment and shows the annotator user interface in Section 2.3. In Section 3, we discuss the results from both phases and also automatic evaluation. The main results of this examination are shown in Section 3.5 and specific markable examples are discussed in Section 4. We conclude in Section 5. Markable Definition A markable in this context is an occurrence of any technical or non-technical term or expression that satisfies at least one of the following conditions: 1. The term was translated into two or more different ways within one document. 2. The term was translated into two or more different ways across several translations. 3. Two or more terms were translated to a specific expression in one document but have different meanings. To be a markable, the term or expression does not have to be a named entity, but it must be vital to the understanding of the document. In the same order we show examples which satisfy the definition conditions. 1. bytem -It was translated within one document into an apartment and a residence. 2. rodn\u00e9 \u010d\u00edslo -It was translated in one translation to social security number and in another translation to identification number. 3. n\u00e1jemce, podn\u00e1jemce -They have different meanings and in one document they were both translated to tenant. Markables were proposed first by the annotators in the first phase of annotation in Section 2.1 and then filtered manually by us. Test Suite Composition We selected 4 documents, 2 of which were translated in both directions totalling 6 documents. We chose 2 from the professional domain (Audit and Lease) and 2 from the News domain. The overview of their size is shown in Table 1 . The number of markable occurrences is highly dependent on the document domain with the Agreement domain (Lease document) containing the most occurrences. All of the MT systems are participants of the News Translation Task, and we test their performance even outside of this domain. Most of them were bi-directional, and we join the results from both directions when reporting their performance. The only exceptions are eTranslation (only en\u2192cs) and PROMT NMT (only cs\u2192en). Data and Tools Availability All of the document translations and measured data are available in the project repository. Furthermore, the used online markable annotation tool written in TypeScript and Python is documented and also open-source. 2 2 github.com/ELITR/wmt20-elitr-testsuite Annotation Setup For both phases of this experiment, we used 10 native Czech annotators with English proficiency. None of them were professional audit or legal translators. Because each annotator annotated only one or two documents, the aggregated results across domains, labelled as Total, are of less significance than the results in individual domains. Manual Document Evaluation In this phase of the experiment, we wanted to measure the overall document translation quality and also to collect additional markables for use in the following experiment part. We showed the annotators the source document (in Czech) with a line highlighted and then underneath all its translation variants (in English). The current line was also highlighted. Next to every translation was a set of questions related to the just highlighted lines: \u2022 Adequacy: range from 0 (worst) to 1 (best) measuring how much the translated message is content-wise correct regardless of grammatical and fluency errors. \u2022 Fluency: range from 0 (worst) to 1 (best) measuring the fluency of the translation, regardless of the relation of the message to the source and the correct meaning. \u2022 Markables: A text area for reporting markables for the second phase. \u2022 Conflicting markables: checkbox for when there is a markable in conflict (e.g. the terminology change) with a previous occurrence in the document. This corresponds to the first condition in the markable definition in Section 1.1. The default value was No (no conflict) because the distribution was highly imbalanced. Bojar et al. (2016) summarize several methods for machine translation human evaluation: Fluency-Adequacy, Sentence Ranking, Sentence Comprehension, Direct Assessment, Constituent Rating and Constituent Judgement. For our purposes, we chose a method similar to Fluency-Adequacy as one of the standard sentence-centric methods. The difference to the method described is that we showed all the competing MT systems at once, together with the whole document context. Ultimately, we would like the users to rate Fluency-Adequacy of the whole documents, but we suspected that asking annotators to read the whole document and then rating it on two scales would yield unuseful biased results. Manual Markable Evaluation In the following phase, we focused on markables specifically. For every markable in the source, we asked the annotators to examine 11 phenomena. If the given phenomenon is present in the examined markable occurrence, a checkbox next to it should have been checked (Occurrence). Further on a scale 0-1 (not at all-most) the annotator should mark how negatively it affects the quality of the translation (Severity). We list the 11 phenomena we asked the annotators to work with: \u2022 Non-translated: The markable or part of it was not translated. \u2022 Over-translated: The markable was translated, but should not have been. The choice to focus on markables was motivated by the aim to find a way to measure documentlevel performance using human annotators. A good markable translation is not a sufficient condition for document-level performance, but a necessary one. This approach is similar to Constituent Ranking/Judgement described by Bojar et al. (2016) with the difference that we chose to show all the markable occurrences in succession and in all translations in the same screen. We showed the whole translated documents context so that the annotators could refer to previous translations of the markable and the overall context. Interface Figure 1 shows the online interface for the second phase of this experiment. The first text area window contains the source document (e.g. in English). Below it are several translations (e.g. in Czech). Next to each translation is a set of questions. In the source, the current markable occurrence, to which the questions relate, is always displayed in dark blue. The current sentence is highlighted in the translations with light blue. The target words which probably correspond to the current markable (via automatic word alignment) are highlighted in dark blue as well. This alignment is present only for quick navigation as it is not very accurate. In translations, the remaining occurrences of a given markable are highlighted in green to simplify checking for inconsistency. The FOCUS button is used to scroll to the current line in all text areas in case the user scrolled the view to examine the rest of the document. In the first phase, the annotators could return to their previous answers and adjust them, but before continuing to the next line, they had to fill in the current fluency and adequacy. In the second phase, the annotators could freely return to their previous answers and adjust them. The most straightforward approach for them was to annotate a single markable occurrence across all MT systems and the switch to the next one as opposed to annotating all markable occurrences in the first translation, then all markable occurrences in the second translation, and similarly the rest. As soon as we aggregate the statistics over multiple documents (or even translation directions), the effects of which particular annotator annotated which document can start playing a role, but we hope they cancel out on average. Results Automatic Evaluation We measured the system quality using BLEU (Papineni et al., 2002) against a single reference. The results sorted by the score across all documents are shown in Table 2 . BLEU scores across different test sets are, of course, not comparable directly. Only a very big difference, such as that of eTranslation for News and Audit (39.43% and 23.23%) suggests some statistically sound phenomena. We measured the standard deviation across MT systems within individual domains: News (6.19), Audit (2.34) and News-Lease (2.74). The Audit domain was generally the least successful for most of the submitted systems (see Table 3 ) and the Lease domain was more stable in terms of variance. The MT system BLEU variance over annotated lines hints that the better the system, the higher variance it has. This may be because most of the best MT systems are focused on News and fail on other domains, while the lower performant MT systems are low performant systematically across all domains. Overall Manual Evaluation From the first phase (Section 2.1) we collected 13\u00d7328 = 4264 line annotations. From the second phase (Section 2.2) we collected 13\u00d7499 = 6487 markable annotations. The average duration for one annotation of one translated line in the first phase was 25s, while one annotation of one systemmarkable occurrence in the second phase took only 8s. Fluency and Adequacy correlate per line together strongly (0.80), and their product correlates negatively (-0.33) with the number of wrong markables. Because of this strong correlation and also the need to describe the result of the first phase by one number, we focus on Fluency\u00d7Adequacy. MT System Performance The performance per MT system and domain can be seen in Table 4 . The reference translation received a comparably low rating in especially the Audit domain and fared best in the News domain. We see this as a confirmation of the last year's observation and a consequence of using non-expert annotators, who may have not annotated more complex cases thoroughly and were more content with rather general terms and language than what is correct for the specialized auditing domain. No system has shown to be risky (high average but also with high variance). The last column in Table 4 shows, that the better the system, the more consistent it is (lower variation across documents). This did not occur with BLEU. The ordering of systems by annotator assessment is slightly different than by automatic evaluation (Section 3.1). The automatic evaluation correlates with annotator rating (Fluency\u00d7Adequacy) with the coefficient of 0.93 (excluding Reference). Notable is the distinction in the performance of eTranslation in the Audit domain. Its BLEU in this domain (23.23%, Table 2 ) was below average, however it performed best of all submitted MT systems in terms of Fluency\u00d7Adequacy (98.62%, Table 4 ), above Reference. Closer inspection revealed that the translations were very fluent and adequate but usually used vastly different phrasing than in the Reference, leading to very low BLEU scores. Source: In the vast majority of cases, the obligations arising from contracts for financing were properly implemented by the beneficiaries. Reference: Ve v\u011bt\u0161in\u011b p\u0159\u00edpad\u016f byly z\u00e1vazky vypl\u00fdvaj\u00edc\u00ed z podm\u00ednek podpory p\u0159\u00edjemci \u0159\u00e1dn\u011b pln\u011bny.  The example in Figure 2 shows activization (opposite of passivization) in the translation by eTranslation (the beneficiaries fulfilled their obligations) instead of (obligations were fulfilled by the beneficiaries). This resulted in much lower ngram precision and BLEU score in general, even though the sentence is fluent and more adequate than both the Reference and translation by CUNI-DocTransformer. Markable Phenomena and Systems Table 5 shows an overview of types of markable phenomena with the average number of occurrences and Severity across systems. For all systems, Terminology and Conflicting markables had the most significant impact on the translation quality. These two categories clearly differ in Severity with markable conflicts being much more severe than terminological mistakes. Inconsistency, Typography and Disappearance phenomena also heavily impacted the translation quality, although with varying distribution of Occurrences and Severity. Reference differs from MT systems by hav-ing higher average Occurrence, but lower average Severity (first column in Table 5 ). Furthermore, the Reference had a higher number of Inconsistence occurrences, but with lower Severity. This means that most of these Inconsitencies were not actual errors. This is expected, as careful word choice variation improves the style and requires having an overview of previously used terms in the document. Over-translation occurred rarely and in those cases, mostly in names (example shown in Figure 3 ). Other grammar manifested itself most severely in gender choice when translating sentences with person names without any gender indication from English to Czech. Similarly, Style was marked mostly in direct speech translation. The system used informal singular form addressing instead of plural. These two phenomena are shown in Figure 4 . Source & Reference: Karol\u00edna \u010cern\u00e1 Translation: Caroline Black Source: \"How dare you?\" Thunberg's U.N. speech inspires Dutch climate protesters Reference: \"Jak se opova\u017eujete?\" projev Thunbergov\u00e9 v OSN inspiroval nizozemsk\u00e9 protestuj\u00edc\u00ed proti zm\u011bn\u00e1m klimatu Translation: \"Jak se opova\u017euje\u0161?\" Thunberg\u016fv projev OSN inspiruje nizozemsk\u00e9 klimatick\u00e9 demonstranty Noteworthy is the correlation between phenomena across systems. The highest values were between Sense and Terminology (0.89), Terminology and Inconsistency (0.83) and Sense and Other grammar (0.82). There is no straightforward explanation of this correlation except the obvious that a good system is good across all phenomena. The correlation in the last phenomena pair suggests that the Other grammar category is too coarse and contains other subcategories. Markable Phenomena and Domains The results of markable phenomena across different domains is shown in Table 6 . The second to last column is the correlation (across systems) between Occurrence\u00d7Severity and the BLEU score. The last column in Table 6 shows the correlation (across systems) between the two human scores: Occurrence\u00d7Severity and Fluency\u00d7Adequacy from the first phase of this experiment. Since both BLEU and Fluency\u00d7Adequacy are positive metrics (the higher the score, the better the performance) and Occurrence\u00d7Severity is an error metric (the higher the number, the worse the performance), high negative correlations mean, that the metrics are mutually good performance predictors. The strongest correlations are: Conflicting (-0.58), Non-translated (-0.55) and Semantic role (-0.41). Except for Non-translated, the reason is clear: BLEU is unable to check grammatical relations and never looks across sentences. We find the fact, that BLEU result was in agreement with error marking for these phenomena, to be positive. Positive correlations (i.e. mismatches) were reached for Disappearance (0.28) and Overtranslated (0.33), which is somewhat surprising because here BLEU has a chance to spot these errors from the technical point of view: shorter output could fire brevity penalty and missing terms where the exact wording is clear because they appear already in the source should decrease BLEU score. The overall correlation between Occurrence\u00d7Severity and Fluency\u00d7Adequacy is more significant than the correlation with BLEU. The most correlating variables are: Sense (-0.84), Other grammar (-0.84), Terminology (-0.81) and Inconsistency (-0.59). Interesting is the markable phenomena Disappearance and Sense because of their high difference in correlations between BLEU and human score correlations. Annotator Agreement We would like to bring attention to inter-annotator agreement for the second annotation phase. Table 7 lists the following metrics, which are computed pairwise and then averaged: Plain inter-annotator agreement (IAA) reports the percentage of pairs of annotations where the two annotators agree that a given phenomenon was or was not present. IAA shows high numbers in all cases but it is skewed by the heavily imbalanced class distribution: most often, a phenomenon is not present; see the left sides of squares in the leftmost column in Table 6 for distribution reference. Cohen's Kappa (Kappa), measured also pairwise, isolates the effect of agreeing by chance and reveals that a good agreement is actually reached only in the cases of Disappearance, Terminology and Overtranslated, which are less ambiguous to annotate.  pas, but we speculate that it is due to insufficient attention of the annotators: they would perhaps agree much more often that an error occurred but they were overloaded with the complexity of the annotation task and failed to notice on their own. Plain Pearson Correlation (Corr.) was measured on Severities in instances where both annotators marked the phenomenon as present. This, however, disregards the disagreement in cases one annotator did not mark the phenomenon. For this, we also computed Corr.+, which examines all pairs in which at least one annotator reported Severity and replaces the other with zero. We observe a big difference in the correlations. In cases where both annotators agreed that there was an error they tend to agree on the severity of the mistake, except Terminology and Inconsistency. If the cases where only one annotator marked the error are included, then the agreement on Severity is non-existent, except Over-translation and Conflicting translation. Translation Direction We also examined how the language translation directions affect the results. Most notable is CUNI-DocTransformer, which performs worse when translating into Czech. With only 0.01% higher Occurence of markable phenomena, the Severity increased by 20.81%. This is not something which we observed in other systems. The translation into Czech brought on average 0.01% higher Occurrence, but the Severity on average dropped by 3.99% when switching from English\u2192Czech to Czech\u2192English. The explanation supported by the data is that in translation into English, CUNI-DocTransformer did not make any mistakes (or native Czech annotators did not detect them) and in translating into Czech, more issues were detected. Since the average Severity is measured across all phenomena, then the higher Severity in specific markable cases (Over-translated, Sense, Style and Disappearance) raised the overall average. Annotation Examples In the following figures (Figure 5 , Figure 6 and Figure 7 ) we show annotated excerpts with BLEU, Fluency, Adequacy and markable phenomena severities. References are here to convey the Czech source segment meanings. They were not shown to the annotators. Examined markables are underlined. The example in Figure 5 focuses on intentional, key information duplication (for clarity and security reasons) of the number of signed copies. This duplication was however omitted in the translated output. The output is otherwise fluent and even received higher fluency than the Reference, which has an average fluency of 0.8. Reference Noteworthy is also another markable visible in the same figure, namely the referred section name: Appendix 1 Online-B and CUNI-DocTransformer made good and consistent lexical choices. SRPOL made good lexical choices but switched between them. In this instance, this would not be an error, because consistency is not vital for interpreting the text. The translation by CUNI-T2T-2018 in Figure 6 is not wrong only because of this markable translation choice, but also by poor fluency. The BLEU score, however, does not suggest, that there is anything fundamentally wrong with the translated segment despite the meaning being distorted. Reference: In Art. III of the Sublease agreement, entitled \"Term of the Lease,\" the tenant and the lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee agreed that the apartment in question would be rented to the tenant for a fixed period from 13th May 2016 to 31st December 2018. Translation: In art. III of the apartment lease agreement, called \"sublease period\", the tenant and the tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant agreed that the apartment in question will be left to the tenant for use for a fixed period from 13. 5. 2016 to 31. 12. 2018. BLEU: 31.95%, Fluency: 0.7, Adequacy: 0.5 Terminology: 0.5, Sense: 0.25, Conflict: 1, Other grammar: 0.25 Figure 7 : Example sentence markable (lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee) annotation from Czech News document, translated by Online-G. The last example, in Figure 7 , concerns itself with conflicting markables. In this case, two distinct markables (tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant and lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee lessee) were merged into one translation tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant tenant. This is a very fundamental error because, in the Lease agreement, these two markables refer to the two parties, which enter the contract. Again, the BLEU does not suggest that anything is wrong with the translation. It could be even higher (51.06%) were it not for the localized date format in the Reference. Conclusion In this article, we compared three approaches to document translation evaluation. We saw that nonexpert annotators rate most MT systems higher than Reference with Fluency and Adequacy, but Reference ranks better than most of them when inspecting markable phenomena and their Severity. Inspecting specific instances in detail, we found out that MT systems made errors in terms of markables, which no human translator would do. Relating the current observation with the impression last year, we conclude that annotators lacking in-depth domain knowledge are not reliable for annotating on the rather broad scales of Fluency and Adequacy but they are capable of spotting term translation errors in the markable style of evaluation. This is important news because expert annotators can not be always secured. Unfortunately, the inter-annotator agreement remains generally low, possibly due to a high cognitive load with many systems annotated. We further examined these markable phenomena and showed that especially Sense, Other grammar and Terminology kinds of errors negatively influence the Fluency and Adequacy the most. For BLEU the variables of highest importance were Non-translated and Conflicting errors. In future work, we would like to examine more of the kinds of markable errors in modern MT systems and their influence on the translation quality. This description could then help researches focus on specific parts of their MT systems. Furthermore, we would like to explore possible automated metrics, which would help in determining whether the document meaning remained intact with respect to markables. Annotating markables appears to be easier for human annotators and more reliable for non-expert ones, and the results gave us more insight into the systems' performance than the Fluency-Adequacy method. Acknowledgement This study was supported in parts by the grants H2020-ICT-2018-2-825460 (ELITR) and Czech Science Foundation (grant n. 19-26934X, NEUREM3). Appendix 1). Word variability (i.e. inconsistency) is often used to make the text more interesting, but in this context, it is vital that the term is translated consistently. Most of the systems, which outperformed even the Reference, made a severe error in this case. Reference: The most expensive item to be paid before the Grand Prix is the annual listing listing listing listing listing listing listing listing listing listing listing listing listing listing listing listing listing fee. This year, the fee was around 115 million Czech crowns. \"Masses of people who come to Brno to see the Grand Prix spend money here for their accommodation, food and leisure activities, which should more or less balance out the cost associated with the organization of the event, including the listing listing listing listing listing listing listing listing listing listing listing listing listing listing listing listing listing fee,\" economist Petr Pelc evaluated the situation. Translation: The most expensive item is a breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown breakdown fee every year before the Grand Prize. This year was about a hundred fifteen million crowns. \"Mass of people who will come to Brno at the Grand Prix will spend money on accommodation, food or entertainment, which should more or less balance the costs associated with organizing the event, including the unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying unifying fee,\" the economist Petr Pelc assessed. BLEU: 26.59%, Fluency: 0.6, Adequacy: 0.4 Terminology: 1, Sense: 1, Inconsistency: 1",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions support from grants H2020-ICT-2018-2-825460 (ELITR) and Czech Science Foundation (grant n. 19-26934X, NEUREM3). The H2020 program is a European Union funding program for research and innovation, which classifies as a research agency. The Czech Science Foundation is a government-funded organization that supports scientific research, which also classifies it as a research agency. There is no mention of funding from defense, corporate entities, foundations, or an indication that there were no other sources of funding.",
    "abstract": "Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases, we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News, Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools.",
    "countries": [
        "Czech Republic"
    ],
    "languages": [
        "Dutch",
        "Czech",
        "English"
    ],
    "numcitedby": 8,
    "year": 2020,
    "month": "November",
    "title": "{WMT}20 Document-Level Markable Error Exploration"
}