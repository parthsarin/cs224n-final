{
    "article": "Factors for success and failure in Machine Translation are extracted from an analysis of the 50 years history of MT. Situations are different in various types of MT (MT for watchers, for revisors, for translators, for authors). Success or failure may be judged at several levels, conceptual, engineering, operational, commercial, communicative, which concern more specifically researchers, developers, users, vendors and funders. For each category, we propose a set of rules for success and highlight pitfalls leading to almost certain failure. It is important to note that political factors linked with funding and decision-making processes may heavily influence the perception of MT in general and of specific systems or tools as successful or not. To sum up, MT is a complex scientific technology. Success can come only if man and machine are put in their right place considering the translational situation, if researchers and developers prefer pragmatic, hybrid solutions over strict adherence to elegant but simplistic theoretical designs, and if unrelated efforts using MT only as an umbrella or as a testbed are excluded from our consideration. Introduction When speaking of successes and failures in MT, we should distinguish between different types of MT, and between different criteria for judging systems. It is also often forgotten that these first attempts led to systems used operationally, such as the GAT system at military sites in the US and at the Ispra European nuclear facility in Italy, where a 92% satisfaction rate was reported around 1970-72. Reasons for this operational success include: -Perseverance and accumulation of knowledge: the Georgetown group fully realized the limitations of the early mockups and prototypes, and set up to build large dictionaries and to experiment on large quantities of real texts. -Adequate usage: the satisfaction rate was high because the system was used by specialists for accessing information in Russian, and not by translators to produce polished translations. -Adequate funding: as there was a real need for that type of system, funding for development was not cancelled, and steady improvement ensued. -Developers were greatly helped by very fast technical progress in computers, which allowed for storing large dictionaries and offered new programming tools. For example, when P. Toma left the GAT project and found the Latsec company, he used IBM's brand new 360 macroassembler language to program a new MT system, Systran. First attempts in the Soviet Union There were also very early attempts in the Soviet Union during the same period: Lyapunov and Bagrinovskaya at Novosibirsk, Tseitin and Lejkina at Leningrad, Kulagina, Mel'\u010duk and Rosenzweig at Moscow, and many others in associated republics. Considering available computer resources, these attempts were conceptual as well as engineering successes. A remarkable factor there was the close cooperation between very good linguists and very good mathematicians turned computer specialists. For example, Gladkij & Mel'\u010duk [3] produced a very powerful and elegant model for tree transformations. There was no operational success or failure, however, because researchers realized that, contrary to what was happening in the US, computers did not become powerful enough, not to speak of their accessibility to non-military researchers, and stopped working on more than mockups for a while. . 2 The second wave: two main trends (1960-80) The second wave began in 1960, actually before the end of the first, with the creation of several MT laboratories in Europe. We let it end in 1980, because COLING-80 marked the revival of MT in Japan (and, by ricochet, in Europe two years later, and in the US five years later). The \"refoundation\" trend In England and Japan, academic research in MT started about ten years after it had started in the US. Let us only mention here the laboratories at Cardiff and Manchester (M.Masterman), and the work of Sugita, who wrote his thesis on a prototype of English-Japanese MT in 1968 [2] . It is sad to remember that these lines of research, which were quite interesting, died out because of the communicative failure in the US and the dependence of these two countries on US views at the time. Beware of trends! In France, the CETA was established in 1960, first in Paris, then at Paris and Grenoble, and finally only at Grenoble, when the main researchers of the Paris branch estimated that the problem of FAHQMT 2 was unsolvable and decided to turn to more fundamental studies and not to become \"MT cheaters\". At Grenoble, B. Vauquois decided to persist, but to pursue the less ambitious goal of experimenting with new techniques to produce not perfect results, but \"best possible\" results, or at least results better than those of existing systems, and to evaluate their usability on large corpora. That enterprise was certainly a big conceptual, engineering and communicative success. The design of the CETA system deeply influenced that of later systems around the world. There were four very important ingredients in that success: -the analogy with compilers, which led to the introduction of specialized languages for linguistic programming (SLLP), or \"metalanguages\", and to the adaptation and improvement of methods based on the theory of formal languages and automata (e.g., attributed context-free grammars were introduced and used many years before theoretical linguists began to use them in the GPSG formalism). -the first ever introduction of non-deterministic computations in the analysis phase: instead of choosing one solution as soon as an ambiguity appeared (this is still done today in the offsprings of first generation systems such as Systran, Alps, GlobalLink, etc.), several solutions were pursued in parallel, filters were applied, and only at the end of analysis was a unique solution selected, by using a preference method. -the use of an advanced linguistic methodology, including a \"hybrid pivot\" where the geometry, features and relations of the abstract structures were based on Tesnieres' deep dependency theory, while the lexical units were morpho-syntaxo-semantic derivational families based on Mel'\u010duk's lexicon organization. -an intense international cooperation (recall that B. Vauquois was one of the founders of ICCL, and its president until 1984). Although the Russian-French system developed with the CETA software and methodology was improved with reference to a corpus of more than 1M words, and tested against about 60% of that corpus as well as on new documents from the same sort (scientific articles in physics, nuclear sciences, linguistics, etc.), it was never used operationally. The main reasons are that: -in France, contrary to the US, it was at that time quite unimaginable for researchers to create a start-up company and transform a laboratory prototype into a commercial product in the way P. Toma had done a few years before. -DRME, the funding arm of the ministry of Defense, was not itself a potential client, and did not push other parts of the ministry to start a development program. As a matter of fact, it seems that research labs cannot reach operational and commercial success. A distinct organization has to take care of operational aspects. The quality of the rough Russian-French translations produced by CETA in 1970 was considerably better than those produced by all other systems on Russian-English at the time. This certainly demonstrated the advantages of the new techniques introduced. But the researchers thought they could do far better by specializing the systems to particular kinds of texts, such as signaletic bulletins of VINITI's \"referativnyij zhurnal\". Still with the MT-W paradigm in mind, B. Vauquois and his team introduced several new notions, such as \"heuristic linguistic programming\", \"multilevel interface structures\", and \"multilevel transfer\", and set up to develop new specialized languages, based on transducers rather than analyzers, so that unexpected inputs could be handled in a robust way instead of being simply rejected. While refining the concepts and building the tools, the first version of which were integrated into Ariane-78, the first ever \"generator of MT systems\", or \"MT development shell\", a new largescale Russian-French prototype was developed, and at the same time many experiments were conducted on other languages (German, French, Portuguese). The first complete translation results base on Ariane were obtained in 1977. At the same time, computer screens became available, and two new paradigms emerged: MT-W+ (fully automatic MT followed by a quick \"brush-up\") for information gathering, and MT-R for information dissemination. This was the time of the famous congress on \"overcoming the language barrier\" organized by the DG-XIII of the Commission of the European Communities, and, through the Leibnitz group, the beginning of concerted efforts by several European scientists to produce a European system for the EC. This led to the Eurotra project (see below). This period was certainly successful at the conceptual, engineering and communicative levels. Laboratory prototypes were obtained not only by GETA, but by other groups using (parts of) the Ariane system (Saarbr\u00fccken, Nancy, Campinas), and a new awareness of the domain emerged in Europe, which probably also was a factor for the revival of MT in Japan. Obviously, nothing can be said about success or failure at the operational and commercial levels at that period, because prototypes were not mature enough for being put into service. The \"application\" trend In the US, a few companies and laboratories persisted in believing in MT after the ALPAC report. Latsec has already been mentioned. Systran was installed and gradually improved at many sites, language pairs were added, subsidiaries were created in other countries. Whatever can be and has been said on the firm's marketing and advertising practices, is should be recognized that applications of Systran in MT-W were and are quite successful. Reasons are, as before, persistence and \"elbow grease\". Applications to MT-R are another story (see below). Logos was also founded around the time of the ALPAC report. The goal was also immediate application, and the first versions of the Logos system seem to have been English-Vietnamese and Vietnamese-English. The developers reused many ideas from GAT, and some from CETA's early work. For lack of better information, we have called its initial design \"1 1/2 generation\". It seems that these versions were really operational and commercial successes. But they had to be abandoned at the end of the Vietnam war in 1973. Ensued a difficult time, where attempts to develop a concurrent Russian-English system for the Apollo-Soyuz operation ended disastrously with Latsec proposing its system for free 3 . Development then turned to Farsi, again with support from the military, but the demise of the Shah in 1978 halted it. Since that time, Logos seems to do only MT-R, with no operation in MT-W. Some laboratories continued to do research in MT, notably at Berkeley on English-Chinese (Wang), at Austin on German-English (Lehman), and at Saskatoon (Booth). But funding dwindled, there was almost no academic recognition, no young researchers joined, and these efforts died out. It is a pity for the Berkeley group, where very interesting and innovative work was done. Only at Austin was to be a revival, in the MT-R paradigm, thanks to funding by Siemens from 1981 onwards. The only lesson here is that applied research without adequate funding cannot go very far! In the USSR and in Eastern Europe, several laboratories continued to work on MT, with very limited computational means. The \"application\" trend was represented by VCP (all-union center of translators), were systems of quite primitive design were developed, used, and forcibly imposed on translators. Several personal communications lead us to believe it was never a success on any count. . 3 The third wave: servicing more users (1980-) We see a third wave in MT-W since around 1980. Technological progress has made it possible to reach more users in more varied ways: through networks, on workstations or PCs, and as a service. This is happening in Japan, but not only there. Networking has actually been pioneered by Systran SA on the French minitel since around 1980. Access to Japanese data bases from Europe using commercial MT systems has followed a few years later [28] . Since around 1990, JICST (Tokyo) is also offering access to its data base of Japanese and English abstract data base in both languages through the MAJESTIC system. This last operation is not as cost effective as it could be because, at least in our opinion, too much revision is done on the machine output (see I.2.4 below). Toshiba (AS-Transac [50] ) was the first firm to offer MT on a small hardware, around 1983. Since then many Japanese companies have offered MT on portable workstations with optional OCR input, first on proprietary architectures, then on Unix-based architectures, and for some of them on PCs. Workstation-based systems are almost always presented as good for watchers (J-E) as well as for revisors (E-J), while PC-based systems seem to be used almost only for MT-W, or as a kind of extended dictionary help for personal translation into a foreign language. Until now, sales of workstation-based systems did not reach expectations, never paid for past R&D, and only in a few cases do they pay for on-going support. The main reason may be that the same design cannot be good for MT-W and MT-R: it will always be too sophisticated and expensive for one and not good enough for the other. By contrast, PC-based systems such as PC-Translator or GlobalLink are considerably less sophisticated and less expensive, and have been commercially successful. Finally, MT-W+ may be a valuable service. Since 1989, the EC has switched Systran's usage from MT-R, where it was a complete failure (no more than 2K pages per year in 1988, over a total of 1M pages translated), to MT-W+, where it reached 40K pages (over 1.2M) the following year. The EC also bought a Japanese-English MT-W+ service from Fujitsu (Japinfo). The factor for operational and commercial success here is that, contrary to MT-R, there is only an obligation of means (e.g., a brush-up of 5mn per page), and no obligation of producing very high quality final results, which could require 30 mn per page or more. As this kind of brush-up considerably increases the user's satisfaction rate, the real future in MT for watchers may well be conditioned by the availability on telematic \"brush-up\" services. I.2 MT for revisors (1970-) The first attempts: from dismal failure to the biggest success The first attempts at MT-R were done during the 1970-76+ period. Vendors of general MT systems for watchers tried to apply them to this new problem. This led to dismal failures. At the time, we heard many stories about how Systran was forced on professional translators who could only trash the outputs and do the translation again from scratch. However, this did not prevent the CEE from acquiring Systran in 1976 and wasting an enormous amount of manpower and money during the following 13 years or so. It seems that the reason behind these dismal failures is the inherent impossibilty to build FAHQMT systems for a large variety of texts. As a matter of fact, versions of Systran have later been specialized to a controlled language and used at Xerox to translate from English in several languages in a satisfactory way. During the same period, the TAUM group at the University of Montreal, where A. Colmerauer had developed the Q-systems, a simple but very powerful rule-based specialized language, was asked by a translator of the Canadian meteorological office to try and put and end to their ordeal. Because of the official policy of bilingualism, all weather bulletins from all meteorological stations had to be translated from English into French. Although this seems to be a simple task, it took translators several weeks to become proficient at it, and was so boring and ungratifying that they quit as soon as possible. The first version of the TAUM-METEO system was developed in little more than one year. It relied on a very careful analysis of the sublanguage of weather bulletins. The grammar was in effect a \"semantic grammar\", using categories such as \"meteorological event\". First experiments were very encouraging, and showed the potential of the sublanguage approach. But the system was not really put into operation before one year or so, when two researchers left TAUM and started to fine-tune the lingware and the software in such a way that the revision rate 4 went quickly from 40% to 20% to 15%. After a few more years, J. Chandioux had overhauled all parts of the system, and the revision rate went down to less than 5% [30] . He later put it on a microcomputer, prepared a French-English version, and the two versions combined currently translate about 80000 words a day. Apart of reasons previously mentioned, this remarkable success is due to: -the smallness of the sublanguage. -the quantity and ephemeral character of the texts needing to be translated. -the existence of a permanent development and tuning effort. Despite all later investigations, no such favourable sublanguage has been found ever since. All other MT-R systems, including knowledge-based systems relying on domain ontologies [43, 52] concern larger sublanguages and don't reach this incredible quality level. Second phase: a few prototypes showing the worth of the sublanguage approach (1977-81) During the second phase, 1977-81, academic MT research was still extremely reduced, and concentrated on the sublanguage approach. TAUM prepared a new system, TAUM-Aviation [13] , for the maintenance manuals of a plane which was to be stationed in Quebec. It was certainly a success on conceptual and engineering grounds: -the Q-systems were improved in very interesting ways to better handle morphology, in particular idioms. -a powerful transducer of extended Q-graphs, REZO, was built as an improvement over the ATN model. -a component was added to check for the well-formedness of abstract analysis trees. -the quality of translation obtained was very high. Nevertheless, this effort was stopped in 1981, and MT research in Canada ended for four years. The main reasons seem to be that: -the quest for quality led to a system too much dependent on one small sublanguage of the whole sublanguage of the maintenance manuals, that of the hydraulics of the landing gear. Because the lingware encoded too many features specific of that micro domain, it became very difficult to adapt it to other parts of the plane's documentation. -the design of the transfer dictionary component was too complex. Lexicographers had to work not only on lexical elements, but on subtrees of a structure being transformed, so that building transfer dictionaries became quite difficult and too expensive. -the plane was finally not stationed in Canada, so that the need for translating the intended texts ceased to exist. In its second phase, from 1977 to 1981, GETA also resolutely turned to MT-R. Development of the Russian-French system was reoriented in this framework, and Ariane-78 was extended with a subenvironment for revisors [11] . B. Vauquois and his team set up to test and improve the new linguistic and computational methodology on languages of other families. The multilingual character of the approach, its overall balance, and the emerging concept of \"lingware engineering\", supported by an \"MT shell\" (Ariane-78) were instrumental in preparing a project of technological transfer to industry and in helping set up the framework for the future Eurotra project. In Germany, the SFB99 in Heidelberg worked more on concepts and tools (SALAT [5] ) than on prototypes. A very interesting idea was to use the same augmented context-free formalism for morphological and syntactic analysis. No operational usage or transfer to industry was intended, in particular because that type of project has a limited time. By contrast, the SFB100 in Saarbr\u00fccken had started on a study of Systran, and then embarked on realizing better systems (SUSY [8] ). However, research there was dominated by research in computer science and in linguistics, MT itself remaining a sort of \"parent pauvre\", despite the dedication of a few researchers. These laboratories projects may be considered to have been successes, on conceptual, engineering and communicative counts. They in effect paved the way for later projects of \"enabling technology\". Third wave: enabling technology projects and first products (1982-87) Projects for enabling technology We propose to distinguish a third wave in 1982-87. Why these dates? They correspond to the beginning and end of two national projects (Japan, France) and of the Eurotra project per se, all aiming at triggering such products. They also almost coincide with the development period of METAL. Finally, the first MT Summit was held in Hakone in 1987 and was the occasion to see many Japanese products together for the first time. The French MAT national project (projet national TAO [25] ) was clearly a conceptual and engineering success: -lingware engineering improved on the grammatical level (\"static grammars\" [17] as a specification level) and on the lexical level (\"neutral\" lexical database). -Ariane-78 was extended to Ariane-85 and then Ariane-G5, and made accessible from a PC for submitting translations from Word\u2122 and developing terminological dictionaries. -The first version of an industrial French-English system for aviation maintenance manuals gave very impressive results. However, the project was a commercial and communicative failure. This is because of: -bad project set-up: the direction of the project was given by the funding agency, ADI, to SG2, a firm with no experience at all in NLP, and in bad financial situation, instead of CAP-Sogeti, with which a pilot project had just been conducted. -inadequate financing: funding amounted to only about 25 man-years, was to be negotiated year by year or even phase by phase, and came always with considerable delay. -bad technical decisions: SG2 implemented the revisor's workbench on an expensive and specific equipment (Questar 400) instead of going for PCs. -no sufficient duration: as ADI was disbanded by the conservative government in late 1986, the last year of the project was not obtained. This is in sharp contrast with the Japanese MU project [21] , who was excellently organized under the direction of Pr. Nagao at Kyoto University, got a secure funding for full 4 years, and included at least 15 companies (instead of giving a monopoly to one as in France). Note the existence of a French-Malaysian project [20] during the same period, which, using a minimal amount of funding, successfully produced a large-scale laboratory prototype in 1985-86. This clearly shows the critical importance of non-scientific and non-technical factors. Eurotra was officially launched in December 1982 after several years of preparation [12] . Many things have already been said about that project. Although there were some conceptual successes, notably experimental studies of linguistic phenomena across European languages, it was quite a failure on engineering, operational and communicative levels. There are several reasons for this: -bad project set-up: instead of creating an entity to which partners would delegate personal, there were one or several partners in each country; the \"central software development team\" was in effect for many years spread over Luxemburg, Geneva, Utrecht, and Manchester, with not enough developers; and there was no participation of potential users and industrial firms. -mixed and unclear goals: the project had at the same time to pursue fundamental research and to build a working system, without any specification of the exact kind of texts and intended usage. -committee approach to R&D: a good thing was to create new research teams in countries with no tradition in MT or even NLP, but scientific and technical decisions were taken at the majority rule, leading to a quite inadequate system architecture, to an everchanging succession of software \"frameworks\" and lingware \"legislations\", and to an almost complete disregard for the lexicon-oriented parts 5 . First products The 1982-87 period is marked by the apparition of the first real products in MT-R. The table below, while not complete and surely not perfectly accurate, gives an idea of the systems, makers, users, platforms, starting dates and success/failure at various levels (our appreciation). Some of these systems have achieved conceptual success. For example, Fujitsu and NEC have boldly chosen the interlingua approach, more elegant but more costly than the transfer approach. Toshiba has presented very interesting improvements to an otherwise quite classical semantic transfer design at several conferences. B'Vital/SITE has used the multilevel transfer approach for the first time in an industrial system. METAL [18] , although linguistically not very advanced (syntactic transfer with no clear separation between transfer and generation), was entirely programmed in a Lisp environment, with a very advanced user and developer's interface and quite a few interesting innovations in software design (levels of analysis CF grammars, external specification and internal organization of dictionaries). DUET-II made it possible for experienced users to control the style of the output to a certain extent. A remarkable point is that commercial success was achieved only when users were allowed to become co-developers. The PAHO systems [36] are a case in point. It seems that the \"renting approach\" to MT-R does not work, because clients require development services. But these systems are analogous with expert systems 6 , so that neither clients nor vendors can be satisfied. There are two solutions here: -develop and use a system in-house, with close coooperation between developers and revisors, as is done at IBM-Japan or PAHO; sell a kernel system, and sell training in all components of the system, including grammars, but never insist on maintaining and expanding each client's system. Fourth wave: new products and experiments with new paradigms (1988-) Since 1988, there seems to be a fourth wave, which again overlaps with previous waves. In parallel, new, large-scale operational systems have been developed. Second, new paradigms have been introduced and experimented with. The new products are all based on the same symbolic, structure-oriented, rule-based designs as before (LBMT, for \"linguistic-based MT systems\"). -MAJESTIC, operational at JICST (Japan Information Center for Science and Technology) since about 1990, is the operational version of the MU system and works in both directions between Japanese and English. Although the translations are quite satisfactory for readers interested in accessing information in the other language, that is for MT-W or MT-W+, this system has been operated in the MT-R framework. This lead to an obligation of results (professional quality) instead of to an obligation of means, hence to high operational costs, which in turn have limited the percentage of translated titles and abstracts. -AT IBM-US (and associated IBM groups in Europe), LMT (IBM) has long been a research project. It seems to emerge as an operational system since a few years and has even been announced as an \"MT-engine\" coupled with Translation Manager\u2122. LMT integrates many interesting design features (Prolog-based, slot grammars, factorizing trees, metric on trees...). It is however not clear whether it is really used operationally, even in-house as SHALT-I is. -Several systems have also appeared in Taiwan. They also use a classical LBMT design, but some are augmented with functions to compute weights on trees from weights on grammar rules and lexical elements, and methods to tune them stochastically as time goes on. The new trend has been to try new paradigms: -Statistics-Based MT (SBMT) at IBM-US (CANDIDE system): the initial hope to replicate the success achieved in speech recognition has by far not been achieved. Given the amount of effort and initial claims, this project can only be said to have been a failure, both conceptually and operationally, even if it gave rise to some engineering feats. After years of human efforts and high-speed computer time, the results on the Hansard corpus, on which the system has been trained, are still inferior to those on Systran, not developed for that kind of corpus. Also, there are combinatorial limits: the number of words per sentence must be less than 10-12, and the correspondence between words can only be 1\u2192n (n\u22643?), but not m->n (m, n>l). This \"NLP without linguistics\" approach seems to have been abandoned 8 in favor of an approach mixing symbolic and stochastic techniques. -Example-Based MT (EBMT), based on \"analogies\" between aligned translations, seems a much more promising approach, or class of approaches, pioneered essentially in Japan, at Kyoto & Nara universities (Nagao), IBM and ATR [59] . They are usually integrated in a classical LBMT design. For instance, JETS at IBM-Japan) [48] uses subtree pairs only at transfer time. Although there remain several problems, success has been clearly achieved at the conceptual and engineering levels. -Knowledge-Based MT (KBMT) has finally been demonstrated to be possible by CMU with the KBMT-89 prototype [38, 43] . The KANT industrial system [52] has also shown the potential and limits of this approach: the translations are very good, but the language is controlled interactively at input time to fit into a small sublanguage, and the creation of the \"ontology\" (formal description of the domain) is more costly than that of the linguistic knowledge base, based on an interlingua. The linguistic knowledge base itself is in turn costlier than that of less ambitious semantic transfer systems. Although this is clearly a conceptual success, it is not yet clear whether it will be judged as operationally and commercially successful. The conclusion here is that it is certainly worthwhile to experiment with new paradigms, but not to try to build systems based on a single \"ideology\". As S. Nirenburg has put it, researchers should rather try to work in heterogeneous frameworks where they can integrate a variety of \"microtheories\". I.3 MT for translators (1975-) Tools for individual translators have been available since the beginning of office automation. Note that ALPS has proposed a \"groupware\", local network environment very early. Operational and commercial success has been met only by systems running on standard platforms and integrated with several text processors. The case of TWS and MTX [7, 10] , both produced by Linguatech, is a good example. SISKEP (English-Malay) offers helps in the target language, Malay, because it aims at translators who are native speakers of Malay, but may be terminologically more competent in English. Since 1988, quite a few tools for professional teams have appeared on the market. Here are some of them. A remark here is that the most recent tools include translation memories. They have been quite successful operationally: tests have always shown a significant decrease in translation time. Commercial success is still doubtful, and reasons for that are not so clear. For Eurolang Optimizer\u2122, perhaps the product is too recent to say much, but its very nice design and integration in Word\u2122 seem to have made it recently preferred by industrial users. It seems that translators really prefer to use the same text processor they use for other tasks, even if, as in the case of Translation Manager\u2122, the specific editor has many filters to and from external formats. Tight direct integration of MAHT tools in commercial applications has long been an engineering nightmare. This is one of the reasons why Eurolang Optimizer\u2122 has not yet been integrated with more text or document processors. But modern programming techniques (OLE, Apple Events, OpenDoc) will soon make tight indirect integration possible. I.4 MT for authors (1973-) \"Interactive MT\" has actually been envisaged around 1967 by M. Kay and R. Kaplan in the MIND system [4] . The first implementation of the concept seems to have been ITS at BYU. Interaction was however done both during analysis and during transfer, and not by authors, but by specialists of the system and language(s). All other systems or prototypes are intended for authors. TITUS [31] , used operationally for many years at the Textile Institute of France, relied on a controlled language and interactive input. The DLT project [41] gave rise to interesting demonstrators and publications. It is quite surprising that the management of BSO, an industrial firm, has allowed researchers to embark on a completely different paradigm (EBMT) in 1988, instead of starting an experimental phase and trying to put the system in use. R&D there has been abandoned around 1992. N-Tran [34] was an Alvey project. The basic idea of writing in Japanese without knowing it was extremely interesting, but not enough attention was paid to the intended users: implementation was done on workstations, and dialogues were both modal (the user is under the control of the system and must answer) and used specialized linguistic terminology. ITS-2 and LIDIA are also research products, and try to show how to build disambiguation dialogues understandable by general users. Much more development is needed before operational use can be envisaged. Ambassador\u2122, available in English-Japanese, English-French, English-Spanish and French-Japanese seems to be the only product on the market. It is a special case, because it relies on \"canned\" bitexts: the user may choose some words or expressions from fixed menus or by typing them (e.g., proper names). Last year, it offered about 200 \"templates\" of letters and forms, and 450 textual \"forms\" (of sentence or paragraph size). Until now, JETS is the only MT-A system for natural texts which could be put into operational use. But writers of technical documentation at IBM-Japan are perhaps not the best possible users for such a system, because they are accustomed send their production to a translation service and don't see the need for spending time interacting with a system. In short, there have been no operational successes yet in MT-A, but the designs are becoming increasingly user-friendly and geared towards the right kind of potential users, people needing to produce translations, preferably into several languages, and not having easy or cheap access to good translation services. II. Factors for success and failure by types of persons involved As types of success or failure in MT correspond to types of persons involved, let us summarize here the main lessons gained from the previous analysis for each type. II.1 Researchers Goals of research in MT Research MT, not linguistics or computer science: don't be \"MT cheaters\"! MT is a scientific technology: its goals are to advance some knowledge, not of what or why, but of how (know-how), and to produce useful artefacts. Take research subjects from practice! Although this is applied, and not fundamental research, many fundamental questions are suggested by practice. Choosing to work on them rather than on theory-motivated questions is as interesting and more fruitful for the domain. Beware of angelism: set realistic goals! Finally, the goals should be realistic. Human performance in this task is not perfect anyway, and MT is there not to replace humans, but to assist them or to perform tasks they don't want or cannot perform. Composition of research teams Research teams should include computer scientists and language specialists, and consult regularly with potential users Fundamental research in linguistics or computer science should not be pursued in such teams, or only in a peripheral manner. Engineering-oriented people should lead such teams. Often, this implies that they are led by computer scientists, but... Computer scientists must cooperate with real language specialists, and even be at their service. It is very important to recognize the specific competence of linguists, and to build tools for them rather than to try to supplant them. Don't think you can get away without linguists (developers)! Organization of work Have computer scientists and linguists specify the whole systems together with intended users! Build good tools for linguist developers! Let computer scientists and linguists work together for certain tasks! 1.4 How to approach the \"formalisms\" Linguistic formalisms often aim at expressing one particular theory in an elegant way, and have almost never been tested extensively. For MT, useful formalisms are rather more general, not limited to one particular theory, of class of theories, and geared towards lingware engineering and efficiency. But... Don't take the bottle for the wine! What is really important is not the formalism(s), but the linguistic knowledge expressed in them. Another remark here is that too much attention is paid even to application-oriented formalisms. After a few years of work, it doesn't really matter whether one uses ATNs, extended CFGs, tree transformation systems, etc. Results are comparable in their content. Objects rather than processes are important! By \"objects\", we mean here the structures representing units of translations (titles, sentences, or paragraphs) at crucial points in the translation process. For example, the choice of \"factorizing trees\" over \"multiple trees\" has deep consequences. Use what you can from several linguistic theories! To \"fill the bottle\", one should not hesitate to borrow from various linguistic schools. This goes in parallel with the need to work \"in breadth\" rather than \"in depth\", and the remark that MT formalisms should be tools rather than theory-oriented languages. How to present results and perspectives Beware of the mockup effect! Be honest about results and significance! Beware of overoptimistic promises! II.2 Developers Clearly specify the final product! Developing a product is not the same as \"cleaning up\" a laboratory prototype. One should start to specify it from user needs, not from research-oriented goals, and then re-incorporate improved versions of elements of the prototype. Design a complete operational system! An MT shell and associated utilities cannot be added as afterthoughts. Their importance and complexity cannot been underestimated. They must provide many complex functions, among them: -Lingware management (listing, editing, compiling, ensuring coherence...), -Test corpora management (segmentation, intermediate results, revisions...), -Pre-and post-processing of texts (segmentation, markups, formulas, tables, figures...), -Format conversions. Don't try to oversimplify! Although MT-W systems may be and perhaps have to be less sophisticated than MT-R systems, they are quite complex. Previous attempts by ALPS and Weidner at oversimplification (of Systran's design), motivated by the desire to run systems on small PCs, led to failure: in 1985, a study of the Canadian Bureau of Translations showed a decrease of 40% in overall productivity it translators used Weidner CAT with MT on rather than off. In particular, the lexicon organization is intrinsically complex if good or even medium good results are required. Choose an appropriate technology! There is no universal MT solution, or paradigm. Various translational situations call for various technologies, while several different designs can also be adequate for the same translational situation. Adapt software development techniques to lingware engineering! Lingware engineering is a complex task. An MT system is considerably larger than a compiler for a large formal language, even if the lexicon is not counted. Methods from software engineering should be adapted and strictly followed during specification, development and maintenance. In particular, one should be very careful about strategy, tools and costs of dictionary building. Don't use specific hardware! Although the idea of using special hardware for MT is very old, it has always led to failures, simply because, as in the case of Lisp machines, universal architectures benefit from enormous investments and quickly beat specific (and very expensive) designs. II.3 Users Before embarking on MT Determine first which type of MT you need, if any! Still now, many users are simply not ready to use any kind of M(A)T, because the texts to be translated are in many heterogeneous formats, or only typed on paper, or even scribbled by hand 9 . Don't buy a \"closed\" MT-R system! While using MT Don't expect more than what you have paid for! Apply your system only on texts it is designed to handle! We have already said that trying to use an MT-W system in MT-R mode is a sure way to disappointment. But the same will also happen if one tries to use a well-tuned MT-R system on texts in another typology. It takes time and money to adapt a system from one typology to another, in many cases a delay of several months and a cost of several man-years. Accept and even require to be a co-developer-for all types of MT! II.4 Vendors Be honest! Far too many vendors these days promote their systems with overoptimistic figures, without even saying how they are obtained. Arguments using the number of words per hour are also quite misleading in that they rarely mention the kind of hardware used, and never the fact that the exact speed is rarely important: what does it change if 30000 words rather than 10,000 are translated in an hour? A human cannot read more than a few dozen pages an hour, or revise more than three or four, and computer time is one of the cheapest commodities nowadays. Don't sell MT-W when MT-R is required! For example, systems offered on networks should clearly warn users that, while they may be very satisfactory to gather information from texts written in foreign languages, they cannot produce reliable translations into other languages without a good dose of professional human revision. In MT-R, sell a kernel system and complete training, not service! An MT-R system is not a compiler or a photocopy machine! Accept to sell training and to transfer competence, not only on the \"easy\" parts (e.g., noun dictionaries), but on all parts, including grammars. II.5 Funders and decision-makers Beware of inadequate project organizations! One center where partners delegate personal for at least 3 years is the ideal. Set clear goals to projects, either research or development, but not both! Don't fund fundamental research and expect operational systems! Give projects a sufficient life-span and adequate, continuous and on-time funding! Beware of \"MT cheaters\"! Conclusion To sum up, MT is a complex scientific technology. Success can come only if researchers and developers prefer pragmatic, hybrid solutions over strict adherence to elegant theoretical designs, if developers adhere to pragmatic, user-oriented solutions without oversimplifying the task, if vendors are honest and accept to sell \"open\" systems, and finally if funders try to understand the issues as well as possible, don't start nonsensical projects, set clear goals, and make sure the projects really tackle MT and are not diverted by \"MT cheaters\". Contrary to a largely shared belief, there have been and are quite a few successes in MT, at all levels, conceptual, engineering, operational, commercial and communicative. But there have also been many failures, especially at the operational and commercial levels. We hope that the lessons from the past can be used in the future to increase the number of successes in MT, at all levels.",
    "abstract": "Factors for success and failure in Machine Translation are extracted from an analysis of the 50 years history of MT. Situations are different in various types of MT (MT for watchers, for revisors, for translators, for authors). Success or failure may be judged at several levels, conceptual, engineering, operational, commercial, communicative, which concern more specifically researchers, developers, users, vendors and funders. For each category, we propose a set of rules for success and highlight pitfalls leading to almost certain failure. It is important to note that political factors linked with funding and decision-making processes may heavily influence the perception of MT in general and of specific systems or tools as successful or not. To sum up, MT is a complex scientific technology. Success can come only if man and machine are put in their right place considering the translational situation, if researchers and developers prefer pragmatic, hybrid solutions over strict adherence to elegant but simplistic theoretical designs, and if unrelated efforts using MT only as an umbrella or as a testbed are excluded from our consideration.",
    "countries": [
        "France"
    ],
    "languages": [
        "Russian",
        "Japanese",
        "English",
        "French"
    ],
    "numcitedby": "0",
    "year": "1995",
    "month": "July 10-13",
    "title": "Factors for success and failure in {MT}"
}