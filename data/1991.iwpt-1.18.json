{
    "article": "To combine the advantages of probabilistic gram mars and generalized LR parsing, an algorithm for constructing a probabilistic LR parser given a prob abilistic context-free grammar is needed. In this pa per, implementation issues in adapting Tomita's gen eralized LR parser with graph-structured stack to per form probabilistic parsing are discussed. Wrig__ ht_ and-_ Wrigley (1989) has proposed a probabilistic L\ufffdle construction algorithm for non-left-recursive contextlree grammars. To account for left recursions, a method for comput1ng item probabilities using the '_generation o sy m-s-nftirre'ar equa ions 1s presen ea: The notion of e erre pro a 1ties is proposed as a means for dealing with similar item sets with differing probability assignments. Introduction Probabilistic grammars provide a formalism which accounts for certain statistical aspects of the lan guage, allows stochastic disambiguation of sen tences, and helps in the efficiency of the syntactic analysis. Generalized LR parsing is a highly effi cient parsing algorithm that has been adapted to handle arbitrary context-free grammars. To com bine the advantages of both mechanisms, an algo rithm for constructing a generalized probabilistic LR parser given a probabilistic context-free gram mar is needed. In Wright and Wrigley (1989) , a probabilistic LR-table construction method has been proposed for non-left-recursive context-free grammars. However, in practice, left-recursive context-free grammars are not uncommon, and it is often necessary to retain this left-recursive grammar structure. Thus, a method for handling left-recursions is needed in order to attain proba bilistic LR-table construction for general context free grammars. In this paper , we concentrate on incorporat ing probabilistic grammars with generalized LR parsing for efficiency. Stochastic information from probabilistic grammar can be used in making sta tistical decision during runtime to improve per formance. In Section 3, we show how to adapt Tomita's(1985 Tomita's( , 1987) ) generalized LR parser with *This research was supported in part by National Science Foundation under contract IRI-8858085. 154 graph-structured stack to perform probabilistic parsing and discuss related implementation issues. In Section 4, we describe the difficulty in comput ing item probabilities for left recursive context free grammars. A solution is proposed in Sec tion 5, which involves encoding item dependencies in terms of a system of linear equations. These equations can then be solved by Gaussian Elim ination (Strang 1980) to give the item probabili ties, from which the stochastic factors of the cor responding parse actions can be computed as de scribed in Wright and Wrigley (1989) . We also introduce the notion of deferred prob ability in Section 6 in order to prevent creating excessive number of duplicate items which a.re sim ilar except for their probability assignments. Background Probabilistic LR parsing is based on the notions of probabilistic context-free grammar and prob abilistic LR parsing table, which are both aug mented versions of their nonprobabilistic counter parts. In this section, we provide the definitions for the probabilistic versions. Probabilistic CFG A probabilistic context-free grammar (PCFG) (Suppes 1970 , We therall 1980 , Wright and Wrigley 1989) G, is a 4-tuple (N, T, R, S) where N is a set of non-terminal symbols including S the start symbol, T a set of terminal symbols, and R a set of probabilistic productions of the form <A--+ a, p > where A E N, a E (N U T) * , an d   p the production probability. The probability p is the conditional probability P(alA) , which is the probability that the non-terminal A which appears during a derivation process is rewritten by the se quence a. Clearly if there are k A-productions with probabilities P i, ... ,Pk, then I::= l Pi = 1, since the symbol A must be rewritten by the right hand side of some A-production. The production probabilities can be estimated from the corpus as outlined in Fu and Booth(1975) or Fuj isaki (1984) . It is assumed that the steps of every derivation in the PCFG are mutually independent, meaning that the probability of applying a rewrite rule de- (1) S-+ NP VP ::!. 1 (2) s-+ s pp 1 (3) NP -+ n \ufffd (4) NP -+ det n f (5) NP -+ NP PP 10 (6) PP -+ prep NP 1 (7) VP -+ V NP 1 pends only upon the presence of a given nonter minal symbol ( the premis) in a derivation and not upon how the premis was generated. Thus, the probability of a derivation is simply the product of the production probabilities of the productions in the derivation sequence. Figures 1, 2 and 3 show three example PCFGs GRAl, GRA2 and GRA3 respectively. Inci dentally, GRAl is non-left recursive, GRA2 and GRA3 a.re both left-recursive, although GRA3 is \"more\" left-recursive than GRA2. GRA2 is said to have simple re cursion since there is only a fi nite number of distinct left-recursive loops 1 in the grammar. GRA3, on the other hand, is said to have massive left re cursions because of the inter- ! \\ mi nglcd left 1 - \u2022ecursions, _\\\\_'._hich _.:_ e su _ _ It in infinit\ufffd (. ossibJy unc<? _un\ufffda ble )_ number \ufffdf _ _ d!st!n\ufffd\ufffd !\ufffd E.C-lu: s.L V. . .e QQR \u00a7_ in the grammar . 1 A Os a derivation cycle in which the first and last p1\ufffdions used in the derivation sequence are the same and occur now here else in the sequence. Figu re 3: GRA3 :A Massively Left-recursive PCFG (1) S-+ S a1 1 ! (2) S-+ B a2 \ufffd (3) S-+ C a3 I (4) B-+ S a3 I (5) B-+ B a2 f (6) B-+ C a1 1 (7) C-+ S a2 j (8) C-+ B a3 \\5 (9) C-+ C a1 1 y (10) C-+ a3 B l ( 11) C-+ a3 1,;, 155 Probabilistic LR Parse Table Generalized Probabilistic LR Parsers for Arbitrary PCFGs In this section, we describe how the efficient gen eralized LR parser with graph-structured stack in (Tomita 1985 (Tomita , 1987) ) can be adapted to parse prob abilistically using the augmented parsing table . In particular, we discuss how to maintain consistent runtime stochastic products base on three key no tions of the graph-structured stack: merging, lo cal ambiguity packing and splitting. We assume that the state number and the respective runtime stochastic product are stored at each stack node. Merging Merging occurs when an element is being shifted onto two or more of the stack tops. Figure 7 (giving p3 as P1 q 1 + p2q2) if we adopt strict prob abilistic approach, or the maximum of the prod ucts (ie, p3 = max (p1q1 , p2q2)) if we adopt the  maximum likelihood approach . Note that although the maximum likelihood approach is in some sense less \"accurate\" than the strict probabilistic ap proach, it is a reasonable approximate and has an added advantage when the stochastic factors are represented in logarithms, in which case the stochastic \"products\" of the parse stack can be maintained using only addition and subtraction operators( assuming, of course, that additions and subtractions are \"cheaper\" computationally than multiplications and divisions). Local Ambiguity Packing Local ambiguity packing occurs when two or more branches of the stack are reduced to the same non terminal symbol. To be precise, this occurs when the parser attempts to create a GOTO state node (after a reduce action, that is) and realize that the parent already has a child node of the same state. In this case there is no need to create the ). This is equivalent to the merging of shift nodes, and can be handled similarly: the runtime product of the child node is modified to the new \"merged\" product ( either by summation or max imalization). This modification should be propa gated accordingly to the successors of the packed child node, if any. prep re3, 1 (sh6, ,1 0 } (sh6, \u00bc} re4, 1 re5, 1 rel, l _re2, 1> (re6, 1 \ufffd0} (sh6, ,1 0 } (re7, fa} (sh6, ,1 0 ) $ re3, 1 (ac e, \ufffd} re4, 1 re5, 1 rel, l1 Figure 6: Probabilistic Parsing Table for \u2022GRA3 State ACTION GOTO a1 a2 0 1 (rell,\ufffd} 2 (sh9, \u00bd) (sh8, ,, ,j \ufffd ) 3 (shll, :\ufffd ) 4 (sh13, m) 5 (sh9, \ufffd) (sh8 , ? 6 ._ 6 0 ) 6 (1'e10, 27{4 ) (sh1 5, \ufffd11 ) 7 (sh1 6, {;\ufffd) 8 :re7, 11 g c rel, 11 Tre l, l} 10 1 re4, 1 1 {re4, 1 } 11 (re2, t\ufffd ) (re2, t\ufffd ) (re5, \ufffd7} (re5, '.'\\7) 12 re8, 1 13 (re6, to\\) (re6, too7 ) (re9, \ufffd} 14 re3, 1 {re3, 1 } 15 (1\u2022e2, w) (re2, w) (re5, 11 \ufffd } (re5, 11 '.'\\) 16 (re \ufffd , \\\ufffd\ufffd ) (re6, \ufffd\ufffd\ufffd) (re9, T?R) Splitting Split. ti ng occurs when there is an action conflict. Th is can be handled straightforwardly by creat ing corresp onding new nodes for the new resulting states with the respective runtime products (such as the product of the parent's stochastic prod uct with the action's stochastic factor). Splitting can also occur when reducing (popping) a merged node. In this case, the parser needs to recover the original runtime product of the merged com ponents, which can be obtained with some math ematical manipulation from the runtime products recorded in the merged node's parents. Figure 8 illustrates a simple situation in which a merged node is split into two. In the figure, a reduce action ( of which the corresponding production is of unit length) is applied at Node 3, and the GOTO's for Nodes 1 and 2 are states 4 and 5 respectively. In the case that strict probabilis tic approach is used in merging (see above), we get p 4 = P i7+ P2 p3 q and Ps = P i7+ P2 p3q. If the maximum likelihood approach is used, then p 4 = m ax f; 1 ,1':!) p 3q and ]Js = m a x f; 1 ,p 2) p3q. Further more, if the stochastic fa ctors have been expressed in lognrit.l1ms, t.hen p . . , = J)3 -max (p1 , P2 ) + PI+ q and 71\" = JJ::1 -max (71 1 ,pJ+p:2 +q (notice that only a3 $ s B C shl, 1 \u2022 2 3 4 (rell, J) 5 6 7 (shl, \ufffd) (shl O, '> bn; } (ac e, \ufffd) (sh12, :\u00bc\u00bd ) (sh14, 1\ufffd ) (shlO, \ufffd ) (rel O, \ufffd4 ) (sh12, 1 \ufffd ) (sh14, ,f\ufffd ) (re7, 1 rel, 1 Tre l, l} 1 re4, 1 (re2, t\ufffd ) (re2, \ufffd; ) (re5, \ufffd7) re8, 1 (re6, \\Ob7) (re9, \ufffd) re3, 1 Tre3, 1} -(re2, w) (re2, /1\\ ) (re5, 11 \ufffd } (re6, \\\ufffd\ufffd ) (re9, 1\ufffd } addition and subtraction are needed, as promised) . 157 --+ P2 In general, there may be more than one splitting corresponding to a reduce action (ie, we may have to pop more than one merged nodes). For every split node, we must recover the runtime products of its parents to obtain the appropriate stochas tic products for the resulting new branches. This can be tricky and is one of the reasons why a tree-structured stack ( described below) instead of graphs might perform better in some cases. Using Stochastic Product to Guide Search The main point of maintaining the runtime stochastic products is to use it as a good indicator function to guide search. In practical situation, the grammar can be highly ambiguous, resulting in many branches of ambiguity in the parse stack. As discussed before, the runtime stochastic prod uct reflects the likelihood of that branch to com plete successfully. In To mita's generalized LR parser, processes are synchronized by performing all the reduce actions before the shift actions. In this way, the processes are made to scan the input at the same rate, which in turn allows the unification of processes in the same state. Thus, the runtime stochastic products can be a good enough indicator of how promising each branch (ie. partial derivation) is, since we are comparing among partial derivations of same in put length. We can perform beam search by prun ing away branches which are less promising. If instead of the breadth-first style beam search approach described above we employ a best first ( or depth-first) strategy, then not all of the branches will correspond to the same input length. Since the measure of runtime stochastic product is biased towards shorter sentences, a good heuris tic would have to take into account of the num ber of input symbols consumed. Even so, han dling best-first search can be tricky with To mita's graph-structured stack without the process-input synchronization, especially with the merging and packing of nodes. Presumably, we can have ad ditional data structure to serve as lookup table of the nodes currently in the graph stack: for in stance, an n by m matrix ( where n is the num ber of states in the parse table and m the in put length) indexed by the state number and the input position storing pointers to current stack nodes. With this lookup table, the parser can check if there is any stack node it can use before creating a new one. However, in the worst case, the nodes that could have been merged or packed might have already been popped of the stack be fore it can be re-used. In this case, the parser degenerates into one with tree-structured stack (ie, only splitting, but no merging and packing) and the laborious book-keeping of the stochastic products due to the graph structure of the parse stack seems wasted. It might be more productive then to employ a tree-structured stack instead of a graph-structured stack, since the book-keeping of runtime stochastic products for trees is much simpler: as each tree branch represents exactly one possible parse, we can associate the respec tive runtime stochastic products to the leaf nodes (instead of every node) in the parse stack, and up dating would involve only multiplying ( or adding, in the logarithmic case) with the stochastic fac tors of the corresponding parse actions to obtain the new stochastic products. The major draw back of the tree-stack version is that it is merely a. slightly compacted form of stack list (Tomita 1987 )which means that the tree can grow un manageably large in a short period, unless suitable pruning is done. Hopefully, the runtime stochastic product will serve as good heuristic for pruning the branches; but whether it is the case that the sim plicity of the tree implementation overrides that of the representational efficiency of the graph version remains to be studied. Problem with Left Recursion The approach to probabilistic LR table construc tion for non-left recursive PCFG , as proposed by Wright and Wrigley(1989) , is to augment the stan dard SLR table construction algorithm presented in Aho and Ullman(1977) to generate a proba bilistic version. The notion of a probabilistic item (A -+ o:\u2022/3, p) is introduced, with (A -+ o: \u2022 /3) being an ordinary LR(O) item, and p the item probabil ity, which is interpreted as the posterior probabil ity of the item in the state. The major extension is the computation of these item probabilities from which the stochastic factors of the parse actions can be determined. Wright and Wrigley(1989) have shown a direct method for computing the item probabilities for non-left recursive grammars. The probabilistic parsing table in Figure 4 for the non-left recursive grammar GRAl is thus con structed. Since there is an algorithm for removing left re cursions from a context-free grammar (Aho and Ullman 1977) , it is conceivable that the algo rithm can be modified to convert a left-recursive PCFG to one that is non left-recursive. Given a left-recursive PCFG, we can apply this algo rithm, and then use Wright and Wrigley( 1989) 's table construction method on the resulting non left-recursive grammar to create the parsing ta ble. Unfortunately, the left-recursion elimination algorithm destructs the original grammar struc ture. In practice, especially in natural language processing, it is often necessary to preserve the original grammar structure. Hence a method for constructing a parse table without grammar con version is needed. For grammars with left recursion, the computa tion of item probabilities becomes nontrivia. l. First of all, item probability ceases to be a \"probabil ity\" , as an item which is involved in left recursion is effectively a coalescence of an infinite number of similar items along the cyclic paths, so its as sociated stochastic value is the sum of posteriori probabilities of these packed items. For instance, if starting from item (A -+ a: \u2022 B/3, p) we derive the item ( C -+ \u2022 B,, p x p B), then by left recursion we must also have the items (C -+ \u2022B,, p x P k ) for i = 1, ... oo. The probabilistic item (C -+ \u2022B,, q) , being a coalescence of these items, would have item probability q = I::\ufffd 1 p x p\ufffd = \ufffd' and there is no guarantee that q \ufffd l. This is un derstandable since (C '----+--B,, q) is a coalescence of items which are not necessarily mutually ex clusive. However, we need not be alarmed as the stochastic values of the underlying items are still legitimate probabilities. Owii1g to this coalescence of infinite items into one single item in left recursive grammars, the computation of the stochastic values of items in volves finding infinite sums of the items' stochastic values. For grammars with simple left recursion (that is, there are only finitely many left recursion loops) such as GRA2, we can still figure out the sum by enumeration, since there is only a finite number of the infinite sums corresponding to the left recursion loops. With massive left recursive gramma.rs like GRA3 in which there is an infinite number of (intermingled) left recursion loops, the enumeration method fails. We shall illustrate this effect in the following sections. Simple Left Recursion For grammars with simple left recursion, it is pos sible to derive the stochastic values by simple cycle clct.ect.ion. For instance, consider the following set of LR(0) items for GRA2 in Figure 9 . Suppose the kernel set contains only 1 0 , with So = \u00a5. Let V be a partial derivation before seeing the input symbol v. At this point, the possible derivations which ,vill lead to item Ji are: 1 'D\ufffd VP --.v-N P\ufffdNP-+\u2022n \u2022 v \ufffd VP ..:..... v-NP .Jb_ NP -+ -NP VP \ufffd NP -+ \u2022n .L ..1.. i 'D \ufffd VP --. v \u2022 NP \u00be NP -+ -NP VP \u00be ... :\u00bc- NP --n The sum of the posterior probabilities of the above possible partial derivations are: S1 = (So X \u00bd)+(So X ft x \u00bd)+(So X t/ X \u00bd) + .. Massive Left Recursion For grammars with intermingled left recursions such as GRA3, computation of the stochastic val ues of the items becomes a convoluted task. Con-159 sider the start state for GRA3, which is depicted in Figure 10 . ]5 : [B --Ba2, Ss] 16 : [B --Cai, S6] h : [C --Sa2 , S 1] ls : [C -\u2022Baa, Ss] ]9 : [C --Cai , -Sg] 110 : [C -\u2022aaB, S1 0 ] \u2022 111 , : [C -\u2022aa, Su] Consider the item 11 . In an attempt to write down a closed expression for the stochastic value S1 , we discover in despair that there is an infi nite number of loops to detect, as S is immedi ately reachable . by all n_ on-terminals, and so are the other nonterminals themselves. This intermin gling of the _loops renders it impossible to write down closed expressions for S 1 through Su . Probabilistic Parse Table Construction for Left Recursive Grammars In this section, we describe a way of computing item probabilities by encoding the item depen dencies in terms of systems of linear equations and solving them by Gaussian Elimination (Strang 1980) . This method handles arbitrary context free grammar including those with .left recursions. We incorporate this method with Wright and W:rigley's(1989) algorithm for computing stochas tic \u2022 factors for the parse actions to obtain a ta ble construction algorithm which handles general PCFG. A formal description of the complete table construction algorithm is in the Appendix . In the following \u2022discussion of the algorithm, lower case greek characters such as a and /3 will denote strings in (N U Tt' and upper case alpha bets like A and B denote symbols in N unless mentioned otherwise. Stochastic Values of Kernel Items For completeness, we mention briefly here how the stochastic values of items in the kernel set can be computed as proposed by Wright and Wrigley(1989) : The stochastic value of the kernel item [S' - Dependency Graph The inter-dependency of items within a state can be represented most straightforwardly by a depen dency forest. If we label each arc by the proba bility of the rule represented by that item the arc is pointing at, then the posterior probability of an item in a dependency forest is simply the total product of the root item's stochastic value and the arc costs along the path from the root to the item. This dependency forest can be compacted into a dependency graph in which no item occurs in more than one node. That is, each graph node represents a stochastic item which is a coalesce of all the nodes in the dependency forest represent ing that particular item. The stochastic value of such an item is thus the sum of the posterior prob abilities of the underlying items. Figure 11 depicts the graphical relations of the items in the example state of GRA2 in Figure 9 . We shall not attempt to depict the massively cyclic dependency graph of the start state for GRA3 (Figure 10 ) here. Generating Linear Equations Rather than attempting to write down a closed expression for the stochastic value of each item, we resort to creating a system of linear equations in terms of the stochastic values which encapsu late the possibly cyclic dependency structure of the items in the set. Consider a state \\JI with k items, m of which are kernel items. That is, \\JI is the set of items { I j 11 :S j :S k} such that I i is a kernel item if 1 :S j :S rn.. Again, let S i be a variable represent ing the stochastic value of item I j . The values of 160 S 1 , ... , Sm are known since they can be computed as outlined in Section 5 .1. Consider a non-kernel item I j , m < j :S k. Let { Ii1 1 \u2022 \u2022 \u2022 , lj n, } be the set of items in 'P from which there is an arc into I j in the dependency graph for 'Ill. Also, let P j i denote the arc cost of the arc from item I i i to I j . Then, the equation for the stochastic value of I j , namely S j , would be: n' S j = L P i i X S j i i = l (1) Note that Equation ( 1 ) is a linear equation of at most (k -m) unknowns, namely S m + 1 , ... , Sk . This means that from 1 we have a system of (k-m) linear equations with (k -m) unknowns. This can be solved using standard algorithms like simple Gaussian Elimination (Strang 1980) . The task of generating the equations can be fur ther simplified by the following observations: 1. The cost of any incoming arc of a non kernel item Ii = [Ai \ufffd \u2022ai, Si] is the produc tion probability of the production (Ai -+ Cl'i, P r ) -In other words, P j i = Pr for i = 1 ... n'. Equation ( 1 ) can then be simplified n' to S j = Pr X L i = l S j ;\u2022 2. Within a state, the non-kernel items repre senting any X-production have the same set of items with arcs into them. Therefore, these npn-kernel items have the same value for L;= l S r,, (which is similar to the Sx in Section 5 .1). Thus, Equation ( 1 ) can be further simplified n' . as S j = P r X SA j where SA j = L x= l S r,, . With that, the system of linear equations for each state can be generated efficiently without having to con struct explicitly the item dependency graph . Examples The system of linear equations for the state de picted in Figures 9 and 11 for grammar G RA2 is as \u00a3 11 . So = f (Given) S2 = \u00be(S0 + S3 ) 0 ows . S 1 = 2 (50 + S3 ) S3 = k (So + S3 ) On solving the equations, we have S 1 = 2 5 1, S2 = 2 4 1 and S3 = l 1 , which is the same solution as the one obtained by enumeration (Section 4.1). Similarly, the following system of linear equa tions is obtained for the start state of massively left recursive grammar GRA3: On solvinp; the equations, we have the solutions 29 11 6 s\ufffd 64 3 2 96 \ufffd 1 l 2 and 1. for the 1, 77 , 77 , 77 , 77 , 77 , 77 , 7, 7, 7, 7 ' 7 . stochastic variables S o through Su respectively. Solving Linear Equations with Gaussian Elimination The systems of linear equations generated during table construction can be solved using the popular method Gaussian Elimination which can be found in many numerical analysis or linear algebra text books (for example, Strang 1980) or linear pro gramming books (such as Vasek Ch \ufffd atal, 1983) . The basic idea is to eliminate the variables one by one by repeated substitutions. For instance, if we have the following set of equations: (1) S1 = a11S1 + a12S2 + ... + a1 n S n (n) Sn = a.n 1S1 + a n 2S2 + \u2022 \u2022 \u2022 + annSn . We can eliminate S 1 and remove equation (1) from the system by substituting, for all oc \ufffd ur rences of S1 in equations ( 2 ) through (n), the right hand side of equation ( 1 ). We repeatedly remove variables S1 through Sn -1 in the same way, until we are left with only one equation with one vari able S n . Having thus obtained the value for S n , we perform back substitutions until solutions for S 1 through S n are obtained. Complexity-wise, Gaussian elimination is a cu bic algorithm (Vasek Chvatal, 1983) in terr !1 s of t \ufffd e number of variables (ie, the number of items m the closure set) . The generation of linear equa tions per state is also polynomial since we only need to find the stochastic sum expressions the SA . 's, for the nonterminals (Point 2 of Sec tion 5.3). These expressions can be obtc1:ined _ by partitioning the items in the state set accordm _ g to their left hand sides. There are 0( mn) possi ble LR(O) items (hence the size of each state is O( mn)) and 0(2 mn ) possible sets where n is the number of productions and m the length of the longest right hand side. Hence, asymptotically, the computation of the stochastic values would not affect the complexity of the algorithm, since it has only added an extra polynomial amount of work for ea. eh of the exponentially many possible sets. Of course, we could have used other methods for solving these linear equations, for example, by finding the inverse of the matrix representing the equations (Vasek Chvatal, 1983) . It is also plausi ble that particular characteristics of the equations generated by the construction algorithm can be exploited to derive the equations' solution more efficiently. We shall not discuss further here. Sto chastic Factors Since the stochastic values of the terminal items in a parse state are basically posterior probabili-161 ties of that item given the root (kernel) item, the computation of the stochastic factors for the pars ing actions, which is as presented in Wright a _n d Wrigley(1989), is fairly straightfor ': ard. For sh ! ft action say from State i to State z + 1 on seemg the in\ufffdut symbol x, the corresponding stochas tic factor for this action would be Sr, the sum of the stochastic values of all the leaf items in State i which are expecting the symbol x. For reduce-action, the stochastic factor is simply the stochastic value S i of the item representing the re duction, namely [Ai \ufffd Oi \u2022 , Si ] if the red \ufffd ction is via production Ai \ufffd Oi . For accept-action, the stochastic factor is the stochastic value S n of the item [S' \ufffd S\u2022, S n ], since acceptance can be trea \ufffd ed as a final reduction of the augmented production S' \ufffd S, where S' is the system-introduced start symbol for the grammar. Deferred Probabilities The introduction of probability created a new cri terion for equality between two sets of items: not only must they contain the same items, they mu \ufffd t have the same item probability assignment. It 1s thus possible that we have many (possibly infi nite) sets of similar items of differing probability assignments. This is especially s \ufffd when there a \ufffd e loops amongst the sets of items (1e, the states) _ m the automaton created by the table construct10n algorithmthere is no guarantee that \ufffd he differ ing probability assignments of the recurrmg states would converge. Even if they do converge even \ufffd u ally, it is still undesirable to have a huge parsmg table of which many states have exactly the same underlying item set but differing probabilities. To remedy this undesirable situation, we in troduce a mechanism called defe rred probability which will guarantee that the item sets converge without duplicating too many of the states. Thus far we have been precomputing item's stochas tic ' values in an eager fashion -propagating the probabilities as early as possible. Deferred _ proba bility provides a means to defer propagatmg cer tain problematic probability assignments ( Pr ? b lematic in the sense that it causes many s1m1lar states with differing probability assignments) un til appropriate. In the extreme case, probabilities are deferred until reduction time, ie, the stochas tic factors of REDUCE actions are the respec tive rule probabilities and all other parse actions have unit stochastic factors. A reasonable post ponement, however, would be to defer propagating the probabilities of the kernel items (kernel prob abilities) until the following state. By forcing the differing item sets to have some fixed predefined probability assignment (while deferring the pro \ufffd agation of the \"real\" probabiliti : s until \ufffd . pp \ufffd opri ate times), we can prevent excessive duplication of similar states with same items but different prob abilities. To allow for deferred probabilities, we extend the original notion of probabilistic item to contain an additional field q which is the deferred proba bility for that item. That is, a probabilistic item would have the form (A -a \u2022 /3, p, q). The de fault value of q is 1, meaning that no probability has been deferred. If in the process of construct ing the closure states the table-construction pro gram discovers that it is re-creating many states with the same underlying items but with differing probabilities or when it detects a non-converging loop, it might decide to replace that state with one in which the original kernel probabilities are deferred. That is, if the item (A -a \u2022 /3, p, q) is a kernel item, and /3 =f. f , we replace it with a deferred item (A -a\u2022 {3, p', \ufffd ) and proceed to compute the closure of the kernel set as before (ie, ignoring the deferred probabilities). In essence we have reassigned a kernel probability of p' to the kernel items temporarily instead of its origi nal probability. It is important that this choice of assignment of p' be fixed with respect to that state. For instance, one assignment would be to impose a uniform probability distribution onto the deferred kernel items, that is, let p' be the prob ability Number of iern el items . Another choice is to assign unit.probability to each of the kernel items, which allows us to simulate the effect of treating each of the kernel items as if it forms a separate state. Although in theory it is possible to defer the kernel probabilities until reduction time, in prac tice it is sufficient to defer it for only one state transition. That is, we recover the deferred prob abilities in the next state. We can do this by enabling the propagation of the deferred proba bilities in the next state, simply by multiplying back the deferred probabilities q into the kernel probabilities of the next state. In other words, as in Section 5.1, if [Ai -ai \u2022 X/3i,Si,q] is in State m -1, then the corresponding kernel item in State m would be [Ai -aiX \u2022 /3i , \ufffd ' 1]. Concluding Remarks In this paper, we have presented a method for deal ing with left recursions in constructing probabilis tic LR parsing tables for left recursive PCFGs. We have described runtime probabilistic LR parsers which use probabilistic parsing table. The table construction method, as outlined in this paper and more formally in the appendix, has been imple mented in Common Lisp. The two versions of run time parsers described in this paper have also been implemented in Common Lisp, and incorporated with various search strategies such as beam-search and best-first search ( only for the tree-stack ver-sion) for comparison. The programs run success fully on various small toy grammars, including the ones listed in this paper. In future, we hope to ex perime:qt with larger grammars such as the one in Fuj isaki(1 984). Appendix A. Table Construction Algorithm A full algorithm for probabilistic LR parsing table construction for general probabilistic context-free grammar is presented here. The deferred proba bility mechanism as described in Section 6 is em ployed, the chosen reassignment of kernel proba bility being the unit probability. A.1 Auxiliary Functions A.1.1 CLOSURE CLOSURE takes a set of ordinary nonproba bilistic LR(0) items and returns the set of LR(0) items which is the closure of the input items. A standard algorithm for CLOSURE can be found in Aho and Ullman(1977) . A.1.2 PROB-CLOSURE Output: A set of probabilistic items which is the closure of the input probabilistic items. Each probabilistic item in the output set carries a stochastic value which is the sum of the posterior probabilities of that item given the input items. Method: Step 1: Step 2: Suppose k' is the size of C. Let Ii be the i-th item [Ai -ai./3i] in C, 1 \ufffd i \ufffd k'. Also, for each item Ii, let Si be a variable denoting its stochastic value. 1. For 1 \ufffd i \ufffd k, Si := P i; 2. Let &B be the set of items in C that are expecting B as the next symbol on the stack. That is, &B is the set set Bi := P r X SA ; , where P r is the probability of the production Ai -/3i . Step 3: Solve the system of linear equations gen' erated by Step 2 , using any stan dard algorithm such as simple Gaussian Elimination (Strang 1980) . Step 4: Return {[Ai --+ a \u2022 /3, Si , qi] 11:::; i :::; k'}, where qi = 1 for k\ufffd i \ufffd k'. A.1.3 GOTO Another useful function in table construction is GOTO( {!1 ... I n }, X), where the first argument { Ii ... I n } is a set of n probabilistic items and the second argument X a grammar symbol in ( N U T) . Suppose the probabilistic items in { Ii . .. I n } are such that those with symbol X after When k = 0, GOTO( {Ii }, X) is undefined. A.1.4 Sets-of-Items Construction Let U be the canonical collection of sets of prob abilistic items for the grammar G' . U can be con structed as described below . Initially U := PROB-CLOSURE({[S' --S, 1]}). Repeat the process of applying the GOTO func tion (as defined in Step k and .Bi ;/:-\u00a3}. The process stops when no new set can be gen erated. Note that equality between two sets of proba bilistic items here requires that they contain the same items with equal corresponding stochastic values, as well as deferred probabilities. A.2 LR Table Construction The algorithm is very similar to standard LR ta ble construction (Aho and Ullman 1977) except for the additional step to compute the stochastic factor for eac;h action (shift , reduce , or accept). Given a grammar G = (N, T, R, S ) , we de fine a corresponding grammar G' with a system generated start symbol S': (N U {S'}, T, R U { < S' --+ S, 1 > }, S'). 1. If [A --+ a\u2022 a,B, q a ] is in 'Wi, a ET, and GOTO(W'i,a) = '11; , set ACTION[i,a] to ( \"shift j\" , P a) where P a is the sum of q a 's -that is the stochastic values of items in 'Wi with symbol a after the dot. ACTION[i, a]  to ( \"reduce A --+ a\" , p) for every a E FOLLOW(A). If [ 3. If [S' --+ S\u2022, p] is in 'Wi , set ACTION[i, $] ($ is an end-of-input marker) to ( \"accept'' , p ) . The goto transitions for state i are con structed in the usual way: 4. If GOTO(Ii,A) = I; , set GOTo[i, A] = j All entries not defined by rules ( 1 ) through ( 4 ) are made \"error\" . The FOLLOW table can be constructed from G \u2022 by a standard algorithm in Aho and Ullman(1977) .",
    "abstract": "To combine the advantages of probabilistic gram mars and generalized LR parsing, an algorithm for constructing a probabilistic LR parser given a prob abilistic context-free grammar is needed. In this pa per, implementation issues in adapting Tomita's gen eralized LR parser with graph-structured stack to per form probabilistic parsing are discussed. Wrig__ ht_ and-_ Wrigley (1989) has proposed a probabilistic L\ufffdle construction algorithm for non-left-recursive contextlree grammars. To account for left recursions, a method for comput1ng item probabilities using the '_generation o sy m-s-nftirre'ar equa ions 1s presen ea: The notion of e erre pro a 1ties is proposed as a means for dealing with similar item sets with differing probability assignments.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "12",
    "year": "1991",
    "month": "February 13-25",
    "title": "Probabilistic {LR} Parsing for General Context-Free Grammars"
}