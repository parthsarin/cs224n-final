{
    "framing_question": "What social implications does this paper address, if any?",
    "response": "The paper addresses several social implications:\n\n- **Access to education:** The system can potentially improve access to science education by providing students with personalized feedback and resources to address their misconceptions, even if they do not have access to traditional educational resources.\n- **Equity in education:** The system can help to reduce educational inequities by providing all students with the opportunity to identify and address their misconceptions, regardless of their socioeconomic status or background.\n- **Empowerment of students:** The system can empower students by giving them the tools to take control of their learning and identify areas where they need improvement, fostering self-directed learning and a growth mindset.\n- **Teacher support:** The system can support teachers by providing them with data and insights into student misconceptions, helping them to tailor their instruction and identify areas where students need additional support.\n- **Personalized learning:** The system can promote personalized learning by providing students with resources and feedback that are tailored to their individual needs and learning styles.\n- **Critical thinking skills:** The system can help students develop critical thinking skills by encouraging them to evaluate the accuracy and validity of information, identify inconsistencies, and construct arguments based on evidence.\n- **Scientific literacy:** The system can contribute to scientific literacy by providing students with accurate and engaging information about science, helping them to understand the scientific process and make informed decisions about scientific issues.\n- **Engagement in science:** The system can increase student engagement in science by making learning more interactive and personalized, fostering a sense of curiosity and exploration.",
    "article": "We present initial steps towards an interactive essay writing tutor that improves science knowledge by analyzing student essays for misconceptions and recommending science webpages that help correct those misconceptions. We describe the five components in this system: identifying core science concepts, determining appropriate pedagogical sequences for the science concepts, identifying student misconceptions in essays, aligning student misconceptions to science concepts, and recommending webpages to address misconceptions. We provide initial models and evaluations of the models for each component. Introduction Students come to class with a variety of misconceptions present in their science knowledge. For example, science assessments developed by the American Association for the Advancement of Science (AAAS) 1 showed that 49% of American 6th-8th graders believe that the Earth's tectonic plates are only feet thick (while in fact they are miles thick) and that 48% of American 6th-8th graders believe that atoms of a solid are not moving (while in fact all atoms are in constant motion). A key challenge for interactive tutoring systems is thus to identify and correct such student misconceptions. In this article, we develop an interactive essay writing tutor that tries to address these challenges. The tutor first examines a set of science webpages to identify key concepts (Section 4) and attempts to order the science concepts in a pedagogically appropriate learning path (Section 5). Then the tutor examines a student essay and identifies misconception sentences (Section 6) and aligns these misconceptions to the true science concepts (Section 7). Finally, the tutor suggests science webpages that can help the student address each of the misconceptions (Section 8). The key contributions of this work are: \u2022 Demonstrating that a summarization approach can identify core science concepts \u2022 Showing how a learning path model can be bootstrapped from webpages with grade metadata \u2022 Developing models for misconception identification based on textual entailment techniques \u2022 Presenting an information retrieval approach to aligning misconceptions to science concepts \u2022 Designing a system that recommends webpages to address student misconceptions Related work Interactive tutoring systems have been designed for a variety of domains and applications. Dialog-based tutoring systems, such as Why2-Atlas (VanLehn et al., 2002) , AutoTutor (Graesser et al., 2004) and MetaTutor (Azevedo et al., 2008) , interact with students via questions and answers. Student knowledge is judged by comparing student responses to knowledge bases of domain concepts and misconceptions. These knowledge bases are typically manually curated, and a new knowledge base must be constructed for each new domain where the tutor is to be used. Essay-based tutoring systems, such as Summary Street (Wade-Stein and Kintsch, 2004) or CLICK (de la Chica et al., 2008b) , interact with students who are writing a summary or essay. They compare what the student has written to domain knowledge in the form of textbooks or webpages. They typically do not require a knowledge base to be manually constructed, instead using natural language processing techniques to compare the student's essay to the information in the textbooks or webpages. The current work is inspired by these essay-based tutoring systems, where interaction revolves around essay writing. However, where Summary Street relies primarily upon measuring how much of a textbook a student essay has \"covered\", we aim to give more detailed assessments that pinpoint specific student misconceptions. CLICK targets a similar goal to ours, but assumes that accurate knowledge maps can be generated for both the domain knowledge and for each student essay. Our approach does not require the automatic generation of knowledge maps, instead working directly with the sentences in the student essays and the webpages of science domain knowledge. System overview Our system is composed of five key components. First, a core concept identifier examines domain knowledge (webpages) and identifies key concepts (sentences) that describe the most important pieces of knowledge in the domain. Second, a concept sequencer assigns a pedagogically appropriate order in which a student should learn the identified core concepts. Third, a misconception identifier examines the student essay and identifies sentences that describe misconceptions the student has about the domain. Fourth, a misconception-concept aligner finds a core concept that can be used to correct each misconception. Finally, a recommender takes all the information about core concepts and student misconceptions, decides what order to address the misconceptions in, and identifies a set of resources (webpages) for the student to read. To assemble this system, we draw on a variety of existing datasets (and some data collection of our own). For example, we use data from an annotation study of concept coreness to evaluate our model for identifying domain concepts, and we use data from science assessments of the American Association for the Advancement of Science to train and evaluate our model for identifying misconceptions. We use this disparate data to establish baseline models for each of the tutor's components. In the near future, this baseline tutoring system will be used to collect student essays and other data that will allow us to develop more sophisticated model for each component. Identifying core concepts This first module aims at automatically identifying a set of core concepts in a given set of digital library resources or webpages. Core concepts in a subject domain are critical ideas necessary to support deep science learning and transfer in that domain. From a digital learning perspective, availability of such concepts helps in providing pedagogical feedback to learners to support robust learning and also in prioritizing instructional intervention (e.g., deciding the order in which to treat student misconceptions). A concept can be materialized using different levels of linguistic expressions (e.g. phrases, sentences or paragraphs), but for this work, we focus only on individual sentences as expressions of concepts. We used COGENT (de la Chica et al., 2008a) , a multi-document summarization system to extract concepts (i.e. sentences) from a given set of resources. In the following two subsections, we describe the COGENT system, discuss how we used it for core concept extraction and report the results of its evaluation of effectiveness. Model COGENT is a text summarizer that builds on MEAD (Radev et al., 2004) , a multidocument summarization and evaluation platform . MEAD was originally developed to summarize news articles. COGENT aims to generate pedagogically useful summaries from educational resources. COGENT extends MEAD by incorporating new features in the summarization process. MEAD uses a set of generic (i.e. domain-independent) features to evaluate each sentence in the given set of documents. These features include the length of the sentence, the distance from the sentence to the beginning of the document, etc. Individual scores of a sentence along these dimensions are combined to assign a total score to the sentence. After removing redundant sentences, MEAD then generates a summary using the sentences that had the highest scores. A user-specified parameter determines the number of sentences included in the summary. COGENT extends this framework by incorporating new domain-general and domain-specific features in the sentence scoring process. The domain-general features include a document structure feature, which takes into account a sentence's level in terms of HTML headings, and a content word density feature, which computes the ratio of content words to function words. The domain-specific features include an educational standards feature, which uses a TF-IDF based textual similarity score between a sentence and nationally recognized educational goals from the American Association for the Advancement of Science (AAAS) Benchmarks (Project2061., 1993) and the associated National Science Education Standards (NRC, 1996) , and a gazetteer feature, which scores sentences highly that mention many unique names from a gazetteer of named entities. While in the past, COGENT was used primarily as a summarization system, in the current work, we evaluate its utility as a means of identifying core concepts. That is, are the top sentences selected by COGENT also the sentences describing the key science concepts in the domain? Evaluation We evaluate the core concept extraction module by assessing the extracted concepts against human expert annotations. We ran an annotation study where two human experts assigned \"coreness\" ratings to a selected set of sentences collected from digital resources in three science domains: Plate Tectonics, Weather and Climate, and Biological Evolution. These experts had been recruited based on their training and expertise in the selected subject domains. First, a set of digital resources was selected from the Digital Library for Earth System Education (DLESE) 2 across the three subject domains. Then COGENT was used to extract the top 5% sentences for each domain. The experts then annotated each extracted sentence with its coreness rating on a scale Extraction % 0.5% 1.0% 2.5% 5.0% Plate Tectonics 3.33 3.27 3.00 2.81 Weather and Climate 3.13 2.97 3.07 2.99 Biological Evolution 2.00 2.13 2.46 2.25 Table 1 : Average coreness of sentences extracted at different percentages in each domain of 1 to 4, 4 being the highest. Human annotation is a time-consuming process and this is why we had to limit the number of extracted sentences to a moderate 5% (which is still more than 400 sentences). 17% of the sentences were double annotated and the inter-rater reliability, measured by Spearman's rho, was 0.38. These expert ratings of sentences form the basis of our evaluation. Table 1 shows the average coreness assigned by the experts to sentences extracted by COGENT in each domain, for different extraction percentages. For example, if COGENT is used to extract the top 1% of sentences from all the Plate Tectonics resources, then the average of their coreness ratings (as assigned by the experts) is 3.27, representing a high level of coreness. This is essentially a measure of the precision of COGENT at 1% extraction. Note that we cannot calculate a measure of recall without asking experts to annotate all of the domain sentences, a time consuming task which was outside of the scope of this study. The performance of COGENT was the best in the Plate Tectonics domain since the domain-aware features (e.g. the gazetteer features) used to train CO-GENT were selected from this domain. In the \"near domain\" of Weather and Climate, the performance is still good, but performance falls in the \"far domain\" of Biological Evolution, because of the significant differences between the training domain and the test domain. In the two latter domains, the performance of COGENT was also inconsistent in that with an increase in the extraction percentage, the average coreness increased in some cases and decreased in others. This inconsistency and overall degradation in performance in the two latter domains are indicative of the importance of introducing domain-aware features into COGENT. It is evident from the values in Table 1 that the core concepts extraction module does a decent job, especially when trained with appropriate domainaware features. Sequencing core concepts The goal of this next component is to take a set of core science concepts (sentences), as produced by the preceding module, and predict an appropriate sequence in which those concepts should be learned by the student. Some concepts serve as building blocks for other concepts, and thus it is essential to learn the basic concepts first (and address any misconceptions associated with them) before moving on to other concepts that depend on the basic concepts. For example, a student must first understand the concept of tectonic plates before they can understand the concept of a convergent plate boundary. The sequence of core concepts that results from this module will serve as input for the later module that prioritizes a student's misconceptions. There may exist several different but reasonable concept sequences (also known as learning paths)the goal of this component is to recommend at least one of these. As a first step, we focus on generating a single concept sequence that represents a general path through the learning goals, much like textbooks and curriculums do. Models Our model for concept sequencing is a pair-wise ordering model, that takes two concepts c 1 and c 2 , and predicts whether c 1 should come before or after c 2 in the recommended learning path. Formally, SEQUENCE(c 1 , c 2 ) = 0 if c 1 < c 2 1 if c 1 \u2265 c 2 To generate a complete ordering of concepts, we construct a precedence table from these pair-wise judgments and generate a path that is consistent with these judgments. We learn the SEQUENCE model as a supervised classifier, where a feature vector is extracted for each of the two concepts and the two feature vectors, concatenated, serve as the input to the classifier. For each word in each concept, we include the following two features: \u2022 local word count -the number of times the word appeared in this concept \u2022 global word count -the log of the ratio between the number of times the word occurred in the concept and the number of times it occurred in a background corpus, Gigaword (Graff, 2002) These features are motivated by the work of Tanakaishii et al ( 2010 ) that showed that local and global word count features were sufficient to build a pairwise readability classifier that achieved 90% accuracy. For the supervised classifier, we consider naive Bayes, decision trees, and support vector machines. Evaluation To evaluate our concept sequencing model, we gathered learning paths from experts in high school earth science. Using the model from Section 4, we selected 30 core concepts for the domain of plate tectonics. We asked two earth science experts to each come up with two learning paths for these core concepts, with the first path following an evidence or research based and second path following a traditional learning path. An evidence or research based learning path, is a pedagogy where students are encouraged to use the scientific method to learn about a phenomena, i.e they gather information by observing the phenomena, form a hypothesis, perform experiment, collect and analyze data and then interpret the data and draw conclusions that hopefully align with the current understanding about the phenomena. A teacher that uses this learning path acts as a guide on the side. A traditional learning path on the other hand, is the pedagogy where teachers are simply trying to pass on the correct information to students rather than letting the students discover the information themselves. In a classroom environment, a teacher using this learning path would be seen as the classical sage on stage. We used the learning paths collected from the experts to form two test sets, one for the evidence-based pedagogy, and one for the traditional pedagogy. For each pedagogy, we asked which of all the possible pair-wise orderings our experts agreed upon. For example, if the first expert said that A < B < C and the second expert said that A < C < B, then both experts agreed that A < B and A < C, while they disagreed on whether B < C or C < B. Note that we evaluate pair-wise orderings here, not a complete ranking of the concepts, because the experts did not Pedagogy Pairs (%)  produce a total ordering of the concepts, only a partial tree-like ordering. The experts put the concepts in levels, with concepts in the same level having no precedence relationship, while a concept in a lower level preceded a concept in a higher level. c 1 < c 2 c 1 \u2265 For our test sets, we selected only the pairs on which both experts agreed. Table 2 shows that experts agreed on 68-70% of the pair-wise orderings. Table 2 also shows the percentage of each type of pair-wise ordering (c 1 < c 2 vs. c 1 \u2265 c 2 ) present in the data. Note that even though all concepts are paired with all other concepts, because the experts do not produce complete orderings, the number of agreements for each type of ordering may not be the same. Consider the case where expert E 1 says that concepts A and B are on the same level (i.e., A = B) and expert E 2 says that concept A is in a lower level than concept B (i.e., A < B). Then for the pair (A, B), they disagree on the relation (E 1 says A \u2265 B while E 2 says A < B) but for the pair (B, A) they agree on the relation (they both say B \u2265 A). As a result, the c 1 \u2265 c 2 class is slightly larger than the c 1 < c 2 class. Since these data sets were small, we reserved them for testing, and trained our pair-wise classification model using a proxy task: ordering sentences by grade. In this task, the model is given two sentences s 1 and s 2 , one written for middle school and written for high school, and asked to decide whether s 1 < s 2 (i.e. s 1 is the middle school sentence) or s 2 < s 1 (i.e. s 2 is the middle school sentence). We expect that a model for ordering sentences by grade should also be a reasonable model for ordering concepts for a pedagogical learning path. And importantly, getting grade ordering data automatically is easy: the Digital Library for Earth System Education (DLESE) contains a variety of earth science resources with metadata about the grade level they were written for. To construct the training data, we searched the DLESE website for text resources that contained the words earthquake or plate tectonics. We col-  We then used this proxy-task training data to train our models. We extracted 1702 unique nonstopwords from the training data, resulting in 3404 features per concept, and 6808 features per concept pair (i.e. per classification instance). On the grade-ordering task, we evaluated three models using WEKA 3 , a naive Bayes model, a decision tree (J48) model, and a support vector machine (SVM) model. Using a stratified 50/50 split of the training data, we found that the naive Bayes and SVM models both achieved an accuracy of 80.2%, while the decision tree achieved only 62%. So, we selected the naive Bayes and SVM models for our real task, concept sequencing. Table 3 shows the performance of the two models on the expert judgments of concept sequencing. We find that the naive Bayes model produces more expertlike concept sequences than would be generated by chance and also outperforms the SVM model on the concept sequencing task. For the final output of the module, we combine the pair-wise judgments into a complete concept sequence, breaking any ties in the pair-wise judgments by preferring the order of the concepts in the output of the core concept identifier. Identifying student misconceptions The previous components have focused on analyzing the background knowledge -finding core concepts in the domain and selecting an appropriate learning sequence for these concepts. The current component focuses on the student essay, using the collected background knowledge to help analyze the essay and give feedback. Given a student essay, the goal of this component is to identify which sentences in the essay are most likely to be misconceptions. The task of misconception identification is closely related to the task of textual entailment (Dagan et al., 2006) , in which the goal is to predict if a hypothesis sentence, H, can be reasonably concluded given another sentence, T. In misconception identification, the goal is to predict if a student sentence can be concluded from any combination of the sentences in the domain knowledge, similar to a textual entailment task with a single H but many Ts. A student sentence that can not be concluded from the domain knowledge is likely a misconception. Models We developed two models for identifying student misconceptions, inspired by work in textual entailment that showed that a model that simply counts the words in H that appeared in T, after expanding the words in T using WordNet, achieves state-of-the-art performance (Shnarch et al., 2011) 4 . The Coverage model scores a student sentence by counting the number of its words that are also in some domain sentence. Low-scoring sentences are likely misconceptions. Formally: SCORE(s) = |s \u2229 d| |s| d = s \u2208D EXPAND(s ) where s is a student sentence (a list of words), D is the set of domain sentences, and EXPAND performs lexical expansion on the words of a sentence. The Retrieval model indexes the domain sentences with an information retrieval system (we use where s, D and EXPAND are defined as before, and SCORE lucene is a cosine over TF-IDF vectors 6 . For both the Coverage and Retrieval models, we consider the following lexical expansion techniques for defining the EXPAND function: \u2022 tokens -words in the sentence (no expansion) \u2022 tokens, synsets -words in the sentence, plus all lemmas of all WordNet synsets of each word \u2022 tokens, synsets expanded -words in the sentence, plus all lemmas of all WordNet synsets of each word, plus all lemmas of derived forms, hyponyms or meroynms of the WordNet synsets \u2022 tokens, synsets expanded\u00d74 -words in the sentence, plus all lemmas of all WordNet synsets of each word, plus all lemmas of WordNet synsets reachable by a path of no more than 4 links through derived forms, hyponyms or meroynms Evaluation We evaluate the quality of our misconception identification models using data collected from the American Association for the Advancement of Science's Project 2061 Science Assessment Website 7 . This website identifies the main ideas in various topics under Life Science, Physical Science and Earth Science, and for each idea provides several sentences of description along with its individual concepts and common student misconceptions. We used 3 topics (17 ideas, averaging 6.2 description sentences, 7.1 concept sentences and 9.9 misconception sentences each) as a development set: CE Cells AM Atoms, Molecules, and States of Matter PT Plate Tectonics We used 11 topics (64 ideas, averaging 5.9 description sentences, 9.4 concept sentences and 8.6 misconception sentences each) as the test set: For the evaluation, we provide all of the idea's description sentences as the domain knowledge, and combine all of an idea's concepts and misconceptions into a \"student essay\" 8 . We then ask the system to rank the sentences in the essay, placing misconceptions above true concepts. Accuracy at placing misconceptions at the top of the ranked list is then measured using mean average precision (MAP) and precision at the first item (P@1). Model MAP P@ The models were compared to a chance baseline: the expected MAP and P@1 if the concept and misconception sentences were ordered randomly. Table 4 shows that on the development set, while all models outperformed the random ordering baseline's MAP (0.607), only models with lexical expansion from 4-link WordNet chains outperformed the baseline's P@1 (0.607). The Coverage and Retrieval models using this expansion technique had comparable MAPs Model MAP P@1 Randomly ordered 0.487 0.487 Coverage -tokens, synsets expanded\u00d74 0.603 0.578 Retrieval -tokens, synsets expanded\u00d74 0.644 0.625 (0.690 vs. 0.684), but the Coverage model had a higher P@1 (0.706 vs. 0.647). These top two misconception identification models were evaluated on the test set. Table 5 shows that both models again outperformed the random ordering baseline, and the Retrieval model outperformed the Coverage model (0.644 vs. 0.603 MAP, 0.625 vs. 0.578 P@1). Aligning misconceptions to concepts The goal of this component is to take the misconception sentences identified in a student essay and align them to the core science concepts identified for the domain. For example, a student misconception like Earth's plates cannot bend would be aligned to a science concept like Mountains form when plate material slowly bends over time. Models The model for misconception-concept alignment takes a similar approach to that of the Retrieval model for misconception identification. The alignment model applies lexical expansion to each word in a core science concept, indexes the expanded concepts with an information retrieval system, and scores each concept for its relevance to a student misconception by querying the index with the misconception and returning the index's score for that concept. Formally: SCORE(c) = SCORE lucene (m, EXPAND(c)) where m is the query misconception, c is the science concept, and EXPAND and SCORE lucene are defined as in the Retrieval model for misconception identification. The concept with the highest score is the concept that best aligns to the student misconception according to the model. For lexical expansion, we consider the same definitions of EXPAND as for misconception identification: tokens; tokens, synsets; tokens, synsets expanded ; and tokens, synsets expanded\u00d74 . Model Evaluation We again leverage the AAAS Science Assessments to evaluate the misconception-concept alignment models. In addition to identifying key science ideas, and the concepts and common misconceptions within each idea, the AAAS Science Assessments provide links between the misconceptions and the concepts. Usually there is a single concept to which each misconception is aligned, but the AAAS data aligns as many as 16 concepts to a misconception in some cases. For the evaluation, we give the system one misconception from an idea, and the list of all concepts from that idea, and ask the system to rank the concepts 9 . If the system performs well, the concepts that are aligned to the misconception should be ranked above the other concepts. Accuracy at placing the aligned concepts at the top of the ranked list is then measured using mean average precision (MAP) and precision at the first item (P@1). The models were compared to a chance baseline: the expected MAP and P@1 if the concept and misconception sentences were ordered randomly. Table 6 shows that on the development set, all models outperformed the random ordering baseline. Lexical expansion with tokens and synsets achieved the highest performance, 0.813 MAP and 0.734 P@1. This model was evaluated on the test set, and Table 7 shows that the model again outperformed the random ordering baseline, achieving 0.704 MAP and 0.611 P@1. Overall, these are promising results -given a student misconception, the model's first choice for a concept to address the misconception is helpful more than 60% of the time. Model MAP P@1 Randomly ordered 0.259 0.259 Alignment -Tokens, synsets 0.704 0.611 Recommending resources The goal of this component is to take a set of student misconceptions, the core science concepts to which each misconception is aligned, and the pedagogical ordering of the core science concepts, and recommend digital resources (webpages) to address the most important of the misconceptions. For example, a student that believes that water evaporates into the air only when the air is very warm might be directed to websites about evaporation and condensation. The recommended resources are intended to help the student quickly locate the concept knowledge necessary to correct each of their misconceptions. Models The intuition behind our model is simple: sentences from recommended resources should contain the same or lexically related terminology as both the misconception sentences and their aligned concepts. As a first approach to this problem, we focus on the overlap between recommended sentences and the misconception sentences, and use an information retrieval approach to build a resource recommender. First, the user gives the model a set of domain knowledge webpages, and we use an information retrieval system (Lucene) to index each sentence from each of the webpages. (Note that we index all sentences, not just core concept sentences.) Given a student misconception, we query the index and identify the source URL for each sentence that is returned. We then return the list of the recommended URLs, keeping only the first instance of each URL if duplicates exist. Formally: SCORE(url) = max s\u2208url SCORE lucene (m, s) where url is a domain resource, s is a sentence from a domain resource and m is the student misconception. URLs are ranked by score and the top k URLs are returned as recommendations. Evaluation As a preliminary evaluation of the resource recommendation model, we obtained student misconception sentences that had been aligned to concepts in a knowledge map of plate tectonics (Ahmad, 2009) . The concepts in the knowledge map were originally drawn from 37 domain webpages, thus each concept could serve as a link between a student misconception and a recommended webpage. For evaluation, we took all 11 misconceptions for a single student, where each misconception had been aligned through the concepts to on average 3.4 URLs. For each misconception, we asked the recommender model to rank the 37 domain URLs in order of their relevance to the student misconception. We expect the final interactive essay writing system to return up to k = 5 resources for each misconception, so we evaluated the performance of the recommender model in terms of precision at five (P@5). That is, of the top five URLs recommended by the system, how many were also recommended by the experts? Averaging over the 11 student misconception queries, the current model achieves P@5 of 32%, an acceptable initial baseline as randomly recommending resources would achieve only P@5 of 9%. Discussion In this article, we have presented our initial steps towards an interactive essay writing system that can help students identify and remedy misconceptions in their science knowledge. The system relies on techniques drawn from a variety of areas of natural language processing research, including multi-document summarization, textual entailment and information retrieval. Each component has been evaluated independently and demonstrated promising initial performance. A variety of challenges remain for this effort. The core concept identification system performs well on the plate tectonics domain that it was originally developed for, but poorer on more distant domains, suggesting the need for more domain-independent features. The model for sequencing science concepts pedagogically uses only the most basic of word-based features, and could potentially benefit from features drawn from other research areas such as text readabil-ity. The misconception identification and alignment models perform well on the AAAS science assessments but have not yet been evaluated on real student essays, which may require moving from lexical coverage models to more sophisticated entailment models. Finally, the recommender model considers only information about the misconception sentence (not the aligned core concept nor the pedagogical ordering of concepts) and recommends entire resources instead of directing students to specifically relevant sentences or paragraphs. Perhaps the most important challenge for this work will be moving from evaluating the components independently to a whole-system evaluation in the context of a real essay writing task. We are currently designing a study to gather data on students using the system, from which we hope to derive information about which components are most reliable or useful to the students. This information will help guide our research to focus on improving the components that yield the greatest benefits to the students.",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 1.0564275197633144e-05,
        "foundation": 0.0,
        "none": 0.9999971839107652
    }
}