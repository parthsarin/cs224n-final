{
    "framing_question": "If this paper introduces a dataset, what information is included about people, if any?",
    "response": "The provided text does not mention any information about people. Therefore, I cannot answer this question.",
    "article": "Massively multilingual language models (MMLMs) have become a widely-used language representation method, and multiple large MMLMs were proposed in recent years. A trend is to train MMLMs on larger text corpora or with more layers. In this paper we set out to evaluate recent popular MMLMs on detecting semantically ambiguous and complex named entities with an academic GPU budget. Our submission of a single model for 11 languages on the SemEval Task 11 MultiCoNER shows that a fine-tuned XLM-R large outperforms the more recent RemBERT, ranking 9th from 26 submissions in the multilingual track. Compared to Rem-BERT, the XLM-R model has the additional advantage to fit on a slice of a multi-instance GPU. As contrary to expectations and recent findings, we found RemBERT to not be the best MMLM, we further set out to investigate this discrepancy with additional experiments on multilingual Wikipedia NER data. While we expected RemBERT to have an edge on that dataset as it is closer to its pre-training data, surprisingly, our results show that this is not the case. Introduction Pre-trained language models have revolutionized the field of Natural Language Processing (NLP) in recent years (Peters et al., 2018; Devlin et al., 2019; Zhuang et al., 2021) . Especially for cross-lingual transfer learning or creating a single multilingual model, pre-trained massively multilingual language models (MMLMs) have become a de-facto standard (Conneau et al., 2020; Chung et al., 2021) . MMLMs such as BERT and XLM-R share the same underlying idea: multilingual representations are obtained by learning from large text collections in multiple languages and are trained using a language modeling objective. MMLMs, in contrast to alternative cross-lingual transfer strategies, thus do not rely on explicit alignment via parallel data and explicit transfer via translations with e.g. annotation projection. MMLMs, together with the pretraining and fine-tuning paradigm, have enabled impressive results (Conneau et al., 2020; Lauscher et al., 2020; M\u00fcller-Eberstein et al., 2021) . This paper describes our submission to Task 11 on Multilingual Complex Named Entity Recognition (MultiCoNER) (Malmasi et al., 2022b,a) . We evaluate several recent MMLMs in a fine-tuning regime to answer the following main research question (RQ1): To what extent are more recent larger LMs outperforming earlier MMLMs for the task of multilingual complex NER? To do so, we test four MMLMs (mBERT, XLM-R base and large and the most recently proposed RemBERT). As other NER datasets exists, albeit with different labels, we further explore multi-task learning (RQ2): To what extent can we improve upon MultiCoNER by using cross-lingual cross-domain NER data as auxiliary data? Finally, as the MMLMs were pre-trained on different kinds of data, we include experiments on a second NER dataset, to answer RQ3: To what extent does RemBERT outperform XLM-R when the pre-training data is a closer match? Our contributions are: \u2022 We train a single multilingual model on Mul-tiCoNER, which to our knowledge, is the largest multilingual NER evaluation campaign to date in terms of manually-annotated multilingual training data availability. We compare four MMLMs for the task and examine task performance and GPU budget (here: 20gb). \u2022 Surprisingly, we find that RemBERT does not work well. Experimental Setup This section describes the model, all data sets and the multilingual language models used in this work. Transformer-CRF We use a transformer-CRF model for NER, implemented in the MaChAmp toolkit (van der Goot et al., 2021) (v0.3 beta). The toolkit enables easy exchange of pre-trained LM for fine-tuning as well as multi-task learning. Our model is a single MMLM fine-tuned with a single CRF decoder. To train MaChAmp, we use the proposed default parameters (van der Goot et al., 2021) , which have shown to work well across tasks with a learning rate of lr = 0.0001. We train the model for 20 epochs and select the best checkpoint using the provided dev data (multi_dev). MMLMs As our main RQ is to test the effect of using different pre-trained massively multilingual language models, we opted for four well-known variants: multilingual BERT (Devlin et al., 2019) , XLM-R with both the base (XLM-R b ) and large (XLM-R l ) version (Conneau et al., 2020) , and the more recently introduced RemBERT (Chung et al., 2021) . An overview of the MMLMs is provided in Table 1, and summarized as follows: \u2022 mBERT: trained using both Masked-Language Model (MLM) and next-sentence prediction tasks on Wikipedia data and trained with an exponentially decaying smoothing distribution over languages with \u03b1 = 0.7, i.e., to down-scale high-resource languages and up-scale lower-resource languages; 12 layers. \u2022 XLM-R: trained using only MLM on Com-monCrawl data and trained with an exponentially decaying smoothing distribution with \u03b1 = 0.3 (more aggressive smoothing); with 12 (base) or 24 (large) layers. \u2022 RemBERT: trained using only MLM 1 on Wikipedia and CommonCrawl data, with an exponentially decaying smoothing distribution \u03b1 = 0.5; trained with decoupled input and output embeddings and parameters redistributed over 32 layers. We observe that the MMLMs are trained on 100, 104 and 110 languages, where RemBERT is the MMLM with the largest amount of language coverage (110). We note that for MultiCoNER, all 11 target languages are included in the the pre-training material of all four MMLMs. What differs amongst others is the vocabulary size, the number of layers, the pre-training method, the pre-training data and the number of parameters. XLM large and Rem-BERT are closest in terms of total number of parameters, with RemBERT having an additional 9M parameters over XLM-R large ; mBERT and XLM-R base are a fifth and half of the number of parameters, respectively. What stands out is the type of pre-training data, as the language variety that these MMLMs were trained on differs. While the ones created by researchers at Google opted mainly for Wikipedia data (mBERT, RemBERT), the XLM-R models from FAIR research are trained on a cleaned CommonCrawl dump (Wenzek et al., 2020) . MultiCoNER Data The training dataset provided by MultiCoNER organizers is one of the largest and most diverse multilingual NER datasets available to date in terms of training/dev/test sizes and language/domain coverage (presumably manually annotated). 2  In contrast, the Panx (WikiAnn) dataset (Pan et al., 2017; Rahimi et al., 2019) which include further challenges such as detecting named entities on search queries and code-mixed data (Meng et al., 2021; Fetahu et al., 2021) . Mul-tiCoNER contains 6 entity types (person, location, product, corporation, groups but also complex entities) for the following 11 languages: English (en), Spanish (es), Dutch (nl), Russian (ru), Turkish (tr), Korean (ko), Persian/Farsi (fa), German (de), Chinese (zh), Hindi (hi), and Bangla (bn). While we focus on the multilingual track, and the data provided for it (multi), the shared task further features monolingual tracks and a code-mixed track containing data of some of these languages. Table 2 provides an overview of the Multi-CoNER multilingual data. It is a very large training set of 168.3k sentences, 2.7M tokens and over 237k entities spanning 11 languages. Table 6 in Table 2 shows the size of the dev and test portions (note the very large test data with 472k sentences) and the distribution over the 6 entity types in the training and dev data. Location (LOC) and person names (PER) constitutes the largest portion of entities, followed by CW, corporate names (CORP) and group names (GRP) and the least frequent entity type is product names (PROD). The appendix lists sizes of individual language test files. Auxiliary or Matching Data? For RQ2, we use a multi-task learning setup, and model the MultiCoNER task as the main task with a CRF output decoder, and add a second CRFdecoder that predicts NER types from the union of three cross-lingual cross-domain datasets. In particular, we use German data (Benikova et al., 2014) and two recently proposed derivatives annotated on top of Danish (Plank et al., 2020) and EWT-NNER on top of the English Web Treebank (Plank, 2021) . While all three data sources were annotated for nested NERs (a two-level annotation scheme, which annotates e.g., a location for \"Birmingham\" inside \"University of Birmingham\"), we here use only their inner layer entities. In addition, these data contain slightly different entity types with finer-grained subsets: location, person, organization and miscellaneous entities with two additional suffixes that add derivations (e.g., adjectival forms like \"Brasilian\" and partial NERs like \"Nintendobased\"). The total auxiliary training data (we take the union of English, Danish and German data) consists of a total of 21.4k sentences (roughly 8% of the MultiCoNER main task data) with 42.6k entity annotations. We train the multi-task model jointly by full-parameter sharing, no loss weighting and selecting the best checkpoint using the sum over both main and auxiliary task development span-F1. For RQ3, we compare the two best MMLMs on a NER benchmark derived from Wikipedia (Pan et al., 2017) (WikiAnn/Panx). We follow the setup of Lauscher et al. (2020) and use 12 languages: Indian (in), English (en), Arabic (ar), Finnish (fi), Hindi (hi), Japanese (ja), Russian (ru), Turkish (tr), Basque (eu), Hebrew (he), Italian (it), Korean (ko), Swedish (sv) and Chinese (zh), which use the same splits as RemBERT (Chung et al., 2021) , which were provided by Rahimi et al. (2019) . NER evaluation: macro-F1 vs micro-F1 For evaluation and model selection, we use the CoNLL span-F1 which is a micro-F1 over spanbased entity F1 scores. We note that some earlier work report Accuracy, which can be misleading for NER due to the high number of non-entity tokens typical for the task. The MultiCoNER organizers opted for span-based macro F1. While both micro and macro F1 consider entities correct only if both the entity boundaries and the labels match, the Mul-tiCoNER macro entity-based F1 is typically lower and hence a more conservative measure, particularly when entity types are unbalanced, which is the case for MultiCoNER. Hence, we adapted a Python-version of the Conlleval scorer to include macro-F1. 3 We report macro-F1 for the aggregated evaluation measures. Results Table 3 provides the main results of a single model trained on the MultiCoNER multi_train data and evaluated on the multi_dev portion. Bigger is not always better From the MMLM comparison in Table 3 we first observe that mBERT is outperformed substantially by more recent MMLMs. As expected, XLM-R l outperforms XLM-R b . However, regarding RQ1 our results show that bigger is not always better: XLM-R l outperforms the more recent and even larger Rem-BERT model. Surprisingly, this is consistently the case over all target languages. XLM-R l is more space efficient While XLM-R l is the best MMLM in terms of F1 on multilingual complex NER, for an academic GPU budget XLM-R l is also more efficient: it still just fits a single 20gb GPU slice (NVIDIA A100 40GB GPU split into two slices), at a cost of slightly longer training duration. This means we can fine-tune 2 XLM-R large models in parallel, while only one RemBERT. Further details are given in Table 3 . Test set results Table 4 provides the results on the MultiCoNER test set (last column, multi_test). The results confirm the findings from dev: XLM-R l results in the best model, and substantially outperforms RemBERT. This model ranks 9th in the multilingual track out of 26 participating systems. The auxiliary task (RQ2) fails to provide additional signal, which we hypothesize might be due to the high-resource setup (already a high amount of training data exists), but this would need further investigation. While multi_test already contains test data from all 11 languages, the shared task provides further monolingual (and code-mixed) test sets, which is not equal to the sum in multi_test. We submitted runs of the best models on these individual test sets which confirm that XLM-R l remains the strongest MMLM for NER on MultiCoNER. Discussion In this section, we provide a deeper analysis of our results, adding an additional experiment on data outside of the shared task. First, we examine the per-class F1 score on MultiCoNER, to rule out that the strong results of XLM-R l are purely due to strong results on a few frequent entity types. Second, we compare RemBERT vs XLM-R l on a Wikipedia NER dataset to answer RQ3. Previous findings suggest that RemBERT is a stronger model for multilingual NER (Chung et al., 2021) by testing it on on Wikipedia data, which is also closer to its pre-training data. So we test whether this is the case with our model as well.  Bigger and better because of better pre-training match? Contrary to our findings, previous work suggests that RemBERT outperforms XLM-R on NER (Chung et al., 2021) . We hypothesize that this is not the case on MultiCoNER due to the more Table 5 show the results on 13 languages (averaged over 3 runs). The results show that RemBERT has a slight advantage on 3 out of the 13 languages (EN, TR, IT), but overall and on 9 out of the 13 languages XLM-R l performs substantially better. This additional experiment surprisingly dis-confirms our hypothesis that RemBERT would have an advantage on this Wikipedia/Panx NER data due to the better pre-training data match. While this is in contrast to previous findings on Panx (Chung et al., 2021) , the reason is less clear. We studied the literature and found a study on another task (quality estimation) that similarly reports negative results: replacing the XLM-R decoder with a RemBERT decoder performs better only for 1 language pair out of 4 in quality estimation (Treviso et al., 2021) . We conclude that XLM-R l remains the best MMLM for multlingual NER, as tested across two benchmarks (MultiCoNER and WikiAnn). Test set results per entity Limitations We provided a study on four MMLMs for transformer-based multilingual complex NER to shed lights on MMLMs and GPU usage. Our study is limited in number of MMLMs tested, and the fact that we provide only approximate GPU memory consumption figures. Conclusions We test four massively multlingual language models as encoders for multilingual complex NER. Our results show that XLM-R l results in the overall best model, and surprisingly outperforms the more recently proposed RemBERT, also in terms of GPU memory consumption. While auxiliary-task training did not further prove promising, we additionally studied the discrepancy between RemBERT and XLM-R on a second benchmark (PANX/WikiAnn data). While we hypothesized that RemBERT would outperform XLM-R, our results show that this is not the case. Overall, a bigger model might not be the best choice, especially not for an academic GPU budget. Code, scripts and shared task prediction files (labels only) available at: https://github. com/bplank/multiconer2022 Acknowledgements I would like to thank Rob van der Goot for feedback on earlier drafts of this paper. Thanks to my DFF project number 9063-00077B for providing the GPUs used in this work. ",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.0003572089556838076,
        "foundation": 0.0023969165968836803,
        "none": 0.9999995679800934
    }
}