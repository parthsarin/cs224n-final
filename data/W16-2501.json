{
    "article": "The quality of word representations is frequently assessed using correlation with human judgements of word similarity. Here, we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks. We study the correlation between results on ten word similarity benchmarks and tagger performance on three standard sequence labeling tasks using a variety of word vectors induced from an unannotated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. Introduction The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015) . Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, good models are simply those that provide good performance in the downstream task according to task-specific metrics. Intrinsic evaluations are typically faster and easier to perform and they are often used to estimate the quality of representations before using them in downstream applications. The underlying assumption is that intrinsic evaluations can, to some degree, predict extrinsic performance. In this study, we demonstrate that this assumption fails to hold for many standard datasets. We generate a set of word representations with varying context window sizes and compare their performance in intrinsic and extrinsic evaluations, showing that these evaluations yield mutually inconsistent results. Among all the benchmarks explored in our study, only SimLex-999 (Hill et al., 2015) is a good predictor of downstream performance. This may be related to the fact that it stands out among other benchmark datasets in distinguishing highly similar concepts (male, man) from highly related but dissimilar ones (computer, keyboard). Materials and Methods Word Vectors We generate word representations using the word2vec implementation of the skip-gram model (Mikolov et al., 2013a) , which can be efficiently applied to very large corpora and has been shown to produce highly competitive word representations in many recent evaluations, such as sentence completion, analogy tasks and sentiment analysis. (Mikolov et al., 2013a; Mikolov et al., 2013b; Fern\u00e1ndez et al., 2014) . We induce embeddings with varying values of the context window size parameter ranging between 1 and 30, holding other hyper-parameters to their defaults. 1 lists the text sources and their sizes. We extract raw text from the Wikipedia dump using the Wikipedia Extractor 2 ; the other sources are textual. We pre-process all text with the Sentence Splitter and the Treebank Word Tokenizer provided by the NLTK python library (Bird, 2006) . In total, there are 3.8 billion tokens (19 million distinct types) in the processed text. Intrinsic evaluation We perform intrinsic evaluations on the ten benchmark datasets presented in Table 2 . We follow the standard experimental protocol for word similarity tasks: for each given word pair, we compute the cosine similarity of the word vectors in our representation, and then rank the word pairs by these values. We finally compare the ranking of the pairs created in this way with the gold standard human ranking using Spearman's \u03c1 (rank correlation coefficient). Downstream Methods We base our extrinsic evaluation on the seminal work of Collobert et al. (2011) on the use of neural methods for NLP. In brief, we reimplemented the simple window approach feedforward neural network architecture proposed by Collobert et al., which takes as input words in a window of size Name #Tokens (Train/Test) PTB 337,195 / 129,892 CoNLL 2000 211,727 / 47,377 CoNLL 2003 203,621 / 46,435 Table 3 : Extrinsic evaluation datasets five, followed by the word embedding, a single hidden layer of 300 units and a hard tanh activation leading to an output Softmax layer. Besides the index of each word in the embedding, the only other input is a categorical representation of the capitalization pattern of each word. 3  We train each model on the training set for 10 epochs using word-level log-likelihood, minibatches of size 50, and the Adam optimization method with the default parameters suggested by Kingma and Ba (2015) . Critically, to emphasize the differences between the different representations, we do not fine-tune word vectors by backpropagation, diverging from Collobert et al. and leading to somewhat reduced performance. We use greedy decoding to predict labels for test data. Extrinsic evaluation To evaluate the word representations in downstream tasks, we use them in three standard sequence labeling tasks selected by Collobert et al. (2011) : POS tagging of Wall Street Journal sections of Penn Treebank (PTB) (Marcus et al., 1993) , chunking of CoNLL'00 shared task data (Tjong Kim Sang and Buchholz, 2000) , and NER of CoNLL'03 shared task data (Tjong Kim Sang and De Meulder, 2003) . We use the standard train/test splits and evaluation criteria for each dataset, evaluating PTB POS tagging using token-level accuracy and CoNLL'00/03 chunking and NER using chunk/entity-level F -scores as implemented in the conlleval evaluation script. Table 3 shows basic statistics for each dataset. Results Tables 4 and 5 present the results of the intrinsic and extrinsic evaluations, respectively. While the different baselines and the small size of some of the datasets make the intrinsic results challenging to interpret, a clear pattern emerges when holding the result for word vectors of window size 1 as the zero point for each dataset and examining average differences: the intrinsic evaluations show higher  overall results with increasing window size, while extrinsic performance drops (Figure 1 ). Looking at the individual datasets, the preference for the smallest window size is consistent across all the three tagging tasks (Table 5 ) but only one out of the eight intrinsic evaluation datasets, Simlex-999, selects this window size, with the majority clearly favoring larger window sizes (Table 4). To further quantify this discrepancy, we ranked the word vectors from highest-to lowestscoring according to each intrinsic and extrinsic measure and evaluated the correlation of each pair of these rankings using \u03c1. The results are striking (Table 6 ): six out of the eight intrinsic measures have negative correlations with all the three extrinsic measures, indicating that when selecting among the word vectors for these downstream tasks, it is better to make a choice at random than to base it on the ranking provided by any of the six intrinsic evaluations. Discussion Only two of the intrinsic evaluation datasets showed positive correlation with the extrinsic evaluations: MTurk-287 (\u03c1 0.27 to 0.37) and SimLex-999 (\u03c1 0.85 to 1.0). One of the differences between the other datasets and the high-scoring Simlex-999 is that it explicitly differentiates similarity from relatedness and association. For example, in the MEN dataset, the nearly synonymous pair (stair, staircase) and the highly associated but non-synonymous pair (rain, storm) are both given high ratings. However, as Hill et al. (2015) argue, an evaluation that measures semantic similarity should ideally distinguish these relations and credit a model for differentiating correctly that (male, man) are highly synonymous, while (film, cinema) are highly associated but dissimilar. This distinction is known to be relevant to the effect of the window size parameter. A larger window not only reduces sparsity by introducing more contexts for each word, but is also known to affect the tradeoff between capturing domain similarity Window Size Dataset 1 2 4 5 8 16 20 25 30 WS-Rel 0.5430 0.5851 0.6021 0.6112 0.6309 0.6510 0.6551 0.6568 0.6514 WS-Sim 0.7465 0.7700 0.7772 0.7807 0.7809 0.7885 0.7851 0.7789 0.7776 Table 7 : Intrinsic evaluation results for WS-Rel and WS-Sim (\u03c1) vs. functional similarity: Turney (2012) notes that with larger context windows, representations tend to capture the topic or domain or a word, while smaller windows tend to emphasize the learning of word function. This is because the role/function of a word is categorized by its proximate syntactic context, while a large window captures words that are less informative for this categorization (Turney, 2012) . For example, in the sentence Australian scientist discovers star with telescope, the context of the word discovers in a window of size 1 includes scientist and star, while a larger context window will include more words related by topic such as telescope (Levy and Goldberg, 2014) . The association of large window sizes with greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015) . This phenomenon provides a possible explanation for the preference for representations created using larger windows exhibited by many of the intrinsic evaluation datasets: as these datasets assign high scores also to word pairs that are highly associated but dissimilar, representations that have similar vectors for all associated words (even if not similar) will score highly when evaluated on the datasets. If there is no need for the representation to make the distinction between similarity and relatedness, a large window has only benefits. On the other hand, the best performance in the extrinsic sequence labeling tasks comes from window size 1. This may be explained by the small window facilitating the learning of word function, which is more important for the POS tagging, chunking, and NER tasks than topic. Similarly, given the emphasis of SimLex-999 on capturing genuine similarity (synonyms), representations that assign similar vectors to words that are related but not similar will score poorly. Thus, we observe a decreasing trend with increasing window size for SimLex-999. To further assess whether this distinction can explain the results for an intrinsic evaluation dataset for representations using small vs. large context windows, we studied the relatedness (WS-Rel) and similarity (WS-Sim) subsets (Agirre et al., 2009) of the popular WordSim-353 reference dataset (included in the primary evaluation). Table 7 shows the performance of representations with increasing context window size on these subsets. In general, both show higher \u03c1 with an increasing context window size. However, the performance in the relatedness subset increases from 0.54 to 0.65 whereas that in similarity only increases from 0.74 to 0.77. Thus, although the similarity subset did not select a small window size, the lesser preference for a large window compared to the relatedness subset lends some support to the proposed explanation. Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect for biomedical domain tasks and word vectors (Chiu et al., 2016) . We further considered the differentiation between relatedness (association) and similarity (synonymy) as an explanatory factor, noting that the majority of intrinsic evaluation datasets do not systematically make this distinction. Our results underline once more the importance of including also extrinsic evaluation when assessing NLP methods and resources. To encourage extrinsic evaluation of vector space representations, we make all of our newly introduced methods available to the community under open licenses from https://github.com/ cambridgeltl/RepEval-2016. Acknowledgments This work has been supported by Medical Research Council grant MR/M013049/1",
    "abstract": "The quality of word representations is frequently assessed using correlation with human judgements of word similarity. Here, we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks. We study the correlation between results on ten word similarity benchmarks and tagger performance on three standard sequence labeling tasks using a variety of word vectors induced from an unannotated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "99",
    "year": "2016",
    "month": "August",
    "title": "Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance"
}