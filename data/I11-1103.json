{
    "article": "Community-based Question Answering services contain many threads consisting of a question and its answers. When there are many answers for a question, it is hard for a user to understand them all. To address this problem, we focus on logical relations between answers in a thread and present a model for identifying the relations between the answers. We consider that there are constraints among the relations, such as a transitive law, and that it might be useful to take these constraints into account. To consider these constraints, we propose the model based on a Markov logic network. We also introduce super-relations to give additional information for logical relation identification into our model. Through the experiment, we show that global constraints and super-relations make it easier to identify the relations. Introduction Community-based Question Answering services, such as Yahoo! Answers 1 , OKWave 2 and Baidu Zhidao 3 , have become popular web services. In these services, a user posts a question and other users answer it. The questioner chooses one of the answers as the best answer. These services have many threads consisting of one question and a number of answers, and the number of threads grows day by day. The threads are stored and anyone can read them. When a user has a question, if there is a similar question in the service, he or she can refer to the answers to the similar question. Herefrom, these services are useful for not only the questioner but also other users having a similar question. When more answers get posted for a question, the answers in the thread might become more diverse. Some of these answers will be similar or oppositional to each other. Also, when a question tends to have various answers, e.g. the questioner asks for opinions (e.g. \"What are your song recommendations?\"), it is insufficient to read only the best answer. Generally, as the only one best answer is chosen from the answers, a user may miss other beneficial answers. When a user checks these services with a mobile device, its small display is inefficient to browse all answers. To alleviate these problems, it would be useful to get an overview of answers in a thread, such as by identifying the relations between the answers or by summarizing them (Jimbo et al., 2010; Liu et al., 2008) . The purpose of this study is to identify logical relations between answers with a high degree of accuracy, as a basis of these methods. We propose an identification model with global constraints on logical relations between answers. Among the relations, there are some constraints like a transitive law. To this end, it is necessary to identify relations in a thread at once, and identified relations need to satisfy as many of these constraints as possible. Our model is based on a Markov logic network and incorporates these constraints as formulas of first order logic. Also, we group logical relations on the basis of semantic similarity and transitivity and call these grouped relations \"coarse relations\" and \"transi-tive relations\", respectively. We consider that these relations might be useful for identification of logical relations and that identification of these relations is easier than that of logical relations. Thus, we incorporate identification of these superrelations into our model. We briefly describe our related work in section two. Then, we show the logical relations between answers in section three and present our model with global constraints using a Markov logic network in section four. We explain the experiment and the results in section five and conclude our paper in section six. Related Work The growing popularity of Community-based Question Answering services has prompted many researchers to investigate their characteristics and to propose models for applications using them. Question search and ranking answers are an important application because there are many threads in these services. Jeon et al. discussed a practical method for finding existing question and answer pairs in response to a newly submitted question (Jeon et al., 2005) . Surdeanu et al. proposed an approach for ranking the answers retrieved by Yahoo! Answers (Surdeanu et al., 2008) . Wang et al. proposed the ranking model for answers (Wang et al., 2009) . Wang et al. proposed a model based on a deep belief network for the semantic relevance of question-answer pairs (Wang et al., 2010) . The user's qualifications affect the quality of his or her answer. For example, an IT expert may provide a good answer to a question about computers. Jurczyk and Agichtein proposed a model to estimate the authority of users as a means of identifying better answers (Jurczyk and Agichtein, 2007) . Pal and Konstan proposed the expert identification model (Pal and Konstan, 2010) . Each user has a background. If a user is an amateur in some field, he or she cannot understand a difficult question of the field. For a user-oriented question ranking, Chen and Kao proposed a model to classify a question as easy or difficult (Chen and Kao, 2010) . When there are many answers in a thread, multianswer summarization is a good way to understand the answers. Liu et al. proposed taxonomies for a question and answers, and automatic summarization model for answers (Liu et al., 2008) . Their best answer taxonomy is based on reusability for similar questions, factuality and so on, and their question type taxonomy is based on the expected answer. Achananuparp et al. proposed a model to extract a diverse set of answers (Achananuparp et al., 2010) . Their approach is based on a graph whose edges have weight about similarity and redundancy. Meanwhile, identification of discourse relations in meetings or dialogs was tackled by some researchers. Hillard et al. demonstrated that automatic identification of agreement and disagreement is feasible by using various textual, durational, and acoustic features (Hillard et al., 2003) . Galley et al. described a statistical approach for modeling agreements and disagreements in conversational interaction, and classified utterances as agreement or disagreement by using the adjacency pairs and features that represent various pragmatic influences of previous agreements or disagreements to the target utterance (Galley et al., 2004) . Jimbo et al. proposed a model of relation identification for Community-based Question Answering services (Jimbo et al., 2010) . Their model identified relations using Support Vector Machines with various features. We think considering constraints among relations might contribute to improve the performance of identifying relations. Therefore, we realize it with a Markov logic network. Relation identification is considered as a problem to find labeled edges between pairs of nodes, where a node is an answer in a thread. Structured output learning is a method to predict such a structure (Tsochantaridis et al., 2004; Crammer et al., 2006) . Morita et al. proposed a model based on structured output learning to identify agreement and disagreement relations in a discourse (Morita et al., 2009) . Yang et al. used structured Support Vector Machines to extract contexts and answers for questions in threads of online forums (Yang et al., 2009) . Logical Relations between Answers A thread consists of a question and some answers and the answers are sorted in order of posted time. Thus, in this paper, we try to identify to which preceding answer and in what relation an answer is related. However, some answers are irrelevant to a question and these answers might be unnecessary for an overview of a thread. Therefore, we con-sider not only answer-answer relations, but also question-answer relations. We consider two relations for question-answer pairs, and seven relations for answer-answer pairs. Relations for Question-Answer Pairs The relations for question-answer pairs are \"answer\" and \"unrelated\". The \"answer\" relation is that the answer answers the question directly and is beneficial for the questioner. The \"unrelated\" relation is that the answer has no relation with the expected answer for the question. The reason why we consider the \"unrelated\" relation is that some answers are replies to other answers or questions to the original questioner to ask for further details. Relations for Answer-Answer Pairs We define the logical relations for answer-answer pairs according to Radev's work that defines 24 types of relations between texts for multidocument summarization (Radev, 2000) . Table 1 shows the relations we consider. Figure 1 shows an example of a thread. Answers (a1) and (a2) include the same content, i.e. they recommend XXX, while answer (a3) expresses a different opinion. Answer (a4) mentions about XXX as well as answers (a1) and (a2), but contains the opposite opinion. Hence, the relation between (a1) and (a2) is \"equivalence\" and the relations between (a1) and (a3) and between (a2) and (a3) are \"unrelated\". The relations between (a1) and (a4) and between (a2) and (a4) are \"contradiction\" and the relation between ( a3 ) and (a4) is \"unrelated\". Question (q). Can anyone recommend me a good internet provider? Answer 1 (a1). XXX is good, though I use only this provider. Answer 2 (a2). I prefer XXX. It is cheap and fast. Answer 3 (a3). I use YYY and it is no problem. Answer 4 (a4). The customer support of XXX is the worst. a2 ) is \"equivalence\" and the relation between ( a1 ) and (a4) is \"contradiction\", we expect that the relation between ( a2 ) and (a4) will be the same as the one between (a1) and (a4). This type of constraint is what we incorporate into the model. Relation Identification Model with Global Constraints We propose a joint identification model of logical relations between answers in a thread. We consider that there are some constraints between logical relations. However, since not all relations satisfy a same constraint, we group logical relations into two types of super-relations on the basis of two kinds of commonality; transitivity and semantic similarity. To incorporate constraints between relations, we try to identify relations for all pairs in a thread jointly. For these purposes, we take an approach with a Markov logic network. Super-Relations for Answer-Answer Relations We consider two kinds of super-relations for answer-answer relations; coarse relations and transitive relations. Coarse relations are based on semantic similarity and transitive relations are based on transitivity, that is, whether a transitive law is satisfied for relations. Tables 2 and 3 show the correspondences between coarse relations and logical relations and between transitive relations and logical relations, respectively. It might be easier to identify these two superrelations than logical relations, because these relations are coarser than logical relations. Furthermore, these information might be useful for identification of logical relations. Markov Logic Network A Markov logic network (MLN, in short) is a model combining first order logic and a Markov network (Richardson and Domingos, 2006) . In this framework, we can take account of domain knowledge as formulas of first order logic. In first order logic, if there is at least one predicate that violates formulas in a possible world 4 , the world is not valid. Therefore, a model with first order logic can only have strict constraints. On the other hand, a MLN assigns a weight to each formula and can tolerate violation of the formula. A MLN L is defined as a set of tuples ({F i , w i }), where F i is a formula of first order logic and w i is its weight. The distribution for a possible world x is as follows: P (X = x) = 1 Z exp ( \u2211 i w i n i (x) ) where n i (x) is the number of F i 's ground formulas which are true on x, and Z is a normalization factor. A ground formula is a formula whose terms are constants. The advantage of this model is that it can treat not only multiple relation identification tasks but also constraints between relations. In this paper, according to the terminology of markov thebeast 5 (Riedel, 2008) , which is one of the implementations of a MLN, we call the predicates that indicate aspects obtained from input data as observed predicates, the predicates that indicate aspects to be estimated as hidden predicates. The observed predicate corresponds to a feature in a general machine learning framework, and the hidden predicate corresponds to a label. In addition, we call the formulas that express relations between observed predicates and a hidden predicate local formulas and formulas that express relations between hidden predicates global formulas. Proposed Model To identify the logical relation, our model consists of five subtasks: identification of questionanswer relations, identification of whether two answers have a relation, identification of coarse relations, transitive relations, and logical relations. Table 4 shows the hidden predicates corresponding to these subtasks. For the question-answer re- Answer j replies question i directly. hasaarelation(i, j) There is a relation between answer i and answer j. coarserelation(i, j, c) The coarse relation between i and j is c. transrelation(i, j, t) The transitive relation between i and j is t. aarelation(i, j, l) The logical relation between i and j is l. lation, the relation between question i and answer j is \"answer\" if the predicate hasqarelation(i, j) is true, and \"unrelated\" otherwise. For the answer-answer relation, the term l of the predicate aarelation(i, j, l) corresponds to the logical relation to be identified originally. In our model, when an answer is \"unrelated\" to the question, we exclude it from the identification of the answer-answer relation. Local Formulas In a MLN, there is only one hidden predicate in a local formula. We describe the relation between observed predicates and a hidden predicate on local formulas. For observed predicates, based on Jimbo et al.'s model (Jimbo et al., 2010) , we consider thread features (e.g. order of answers and question type), n-gram features (e.g. word unigram and word bigram), semantic features, named-entity features, and similarity features. Table 5 lists some of the observed predicates. Since our model uses these features both for question-answer relations and answer-answer relations, we use a term \"article\" for a question and an answer in this section. The question type of the thread is q. f irst(i) Article i is the first answer in the thread. neighbor(i, j) Article j adjoins article i. longer(i, j) Article i is longer than article j. timegap(i, j, t) A time interval between article i and j is t. antonym(i, j) Article i and j contain antonyms. sameurl(i, j) Article i and j include a same URL. unigram(i, u) Article i contains word unigram u. bigram(i, b) Article i contains word bigram b. questionf ocus(i) Article i contains the questionfocus. samef ocus(i, j) Article i and j contain the same question-focus. f ocusedneclass(i) Article i contains a word of the focused NE class. namedentity(i) Article i contains a named entity. samene(i, j) Article i and j contain the same named entities. samequoted(i, j) Article i and j contain the same quoted expression. scosine (i, j, c) Cosine similarity between article i and j in terms of sentences is c For identification of relations between articles, the information obtained from the question, such as a class of an expected answer for a question, might be useful. A question usually consists of multiple sentences such as in Figure 2 . In this example, the es-Question. I'm planning a journey to Hokkaido. Can you suggest some good sightseeing places? Figure 2 : Example of a question sential question is the latter sentence. Therefore, we extract the core sentence and the questionfocus and estimate the question type, on the basis of Tamura et al.'s model (Tamura et al., 2005) . The core sentence is the most important sentence, that is, the one requiring an answer in the question. Usually, the core sentence is an interrogative one such as \"Where is the most famous place in Hokkaido?\". But questioners sometimes ask questions without an interrogative form, such as \"Please tell me some good sightseeing places in Hokkaido.\". To cope with this diversity, a machine learning approach is taken to extract the core sentence. The question-focus is then extracted from the core sentence. The question-focus is the word that determines the answer class of the question. For example, the question-focus in Figure 2 is \"places\". We extract the question-focus according to the following steps 6 : step 1. Find the phrase including the last verb of the sentence or the phrase with \"?\" at the end. step 2. Find the phrase that modifies the phrase found in step 1. step 3. Output the nouns and the unknown words in the phrase found in step 2. Question types are categorized in terms of the expected answer into the thirteen types in table 6. We use a model based on Support Vector Machines (SVM, in short) to identify the question type for the question. In this model, the feature vector is obtained from the core sentence. The question types whose answers are nouns require a specific class of named entity for the answer, e.g. the class of named entity, PERSON for the question type, Person. We call this class focused NE class. Table 7 shows the correspondence between a question type and a named entity class. For n-gram features, we also consider unigram and bigram for the first five words and the last five words in each article. For similarity features, we also consider cosine similarities in terms of word unigram, word bigram, phrase unigram, noun unigram, and noun category unigram. The score c for sentence-based cosine similarity is calculated as follows: c = max s im \u2208S i ,s jn \u2208S j sim cos (s im , s jn ) where s im is a m-th sentence of article i and S i is a set of sentences in article i. sim cos (x, y) means a cosine similarity between sentence x and y. For the predicate about similarity like scosine(i, j, c), we do not use the score itself for c but rather a value from 0 to 1 divided up into tenths. For t in the predicate timegap(i, j, t), we choose the one from {\"less than an hour\", \"an hour \u223c 3 hour\", \"3 hour \u223c 6 hour\", \"6 hour \u223c 12 hour\", \"12 hour \u223c 24 hour\", \"24 hour \u223c 48 hour\", \"48 hour \u223c 72 hour\", \"more than 72 hour\"} for the actual time gap between article i and j. We consider the same pattern of local formulas for each hidden predicate. Some examples of local formulas are: question(i) \u2227 f irst(j) \u21d2 hasqarelation(i, j) (1) bigram (i, +w1) \u2227 bigram(j, +w2) \u21d2 hasaarelation(i, j) (2) samequoted(i, j) \u21d2 coarserelation(i, j, +c) scosine(i, j, +v) \u21d2 transrelation(i, j, +t) (4) lastunigram(i, +u1) \u21d2 aarelation(i, j, +l) (5) where \"+\" indicates that the weight of the formula depends on each constant to be grounded. For example, formula (1) has one weight in spite of values for i and j, but formula (3) has a separate weight for each value of c. Global Formulas Global formulas have more than one hidden predicates, and we can use these formulas to incorporate constraints between hidden predicates into the model. Figure 3 shows some global formulas that we consider. There are two kinds of formulas in a MLN: hard constraints and soft constraints. Hard constraints are formulas that must be satisfied in a possible world. This kind of constraint is realized by assigning a huge value for its weight. For a possible world where a formula of hard constraints is false, its probability is almost zero. In our model, formulas from (6) to ( 12 ) are hard constraints. We describe preconditions for each hidden predicate (formulas ( 6 )-( 10 )) and correspondences between super-relations and logical relations (formulas ( 11 ) and ( 12 )) as hard constraints. For example, formula (10) represents that if there is any transitive relation between answer i and j, there needs to be any logical relation between the answers. Soft constraints are formulas that are allowed to be false in a possible world. It is obvious that a possible world where soft constraints are satisfied is more probable than a world where soft constraints are not satisfied. Thus, the model identifies relations to satisfy as many of these constraints as possible. In our model, formulas from ( 13 ) to ( 16 ) are soft constraints. We describe soft constraints for relations among three answers i, j, and k. Formula (13) represents a semantic relevancy and formula ( 14 ) represents a transitive law. As shown in the example in Figure 1 , when a relation between two answers is \"equivalence\", it is reasonable to assume that logical relations from these answers to other answers are identical with each other. Formula (15) represents this situation, and formula ( 16 ) represents the opposite direction of formula (15), that is, relations from other answers to these answers. Experiment To evaluate our model, we conducted an experiment with annotated question-answers threads in Japanese. Experimental Settings We used 299 threads of three genres from Yahoo! Chiebukuro 7 , which is a Japanese Communitybased Question Answering service. Table 8 shows the statistics of the data we used. For each question-answer pair and answeranswer pair, five annotators annotated a relation. In annotating relations, answers whose questionanswer relation had been annotated with \"unre-\u00achasqarelation(i, j) \u21d2 \u00achasaarelation(j, k) (6) \u00achasqarelation(i, k) \u21d2 \u00achasaarelation(j, k) (7) hasaarelation(i, j) \u21d2 \u2203t(transrelation(i, j, t)) (8) hasaarelation(i, j) \u21d2 \u2203c(coarserelation(i, j, c)) (9) transrelation(i, j, t) \u21d2 \u2203l(aarelation(i, j, l)) (10) coarserelation(i, j, \"similar\") \u21d2 \u00ac(aarelation(i, j, \"contradiction\") \u2228 aarelation(i, j, \"unrelated\")) ( 11 ) transrelation(i, j, \"transitive\") \u21d2 \u00ac(aarelation(i, j, \"partial\") \u2228 aarelation(i, j, \"contradiction\") \u2228 aarelation(i, j, \"unrelated\")) ( 12 ) coarserelation(i, j, \"similar\") \u2227 coarserelation(j, k, \"similar\") \u21d2 coarserelation(i, k, \"similar\") ( 13 ) transrelation(i, j, \"transitive\") \u2227 transrelation(j, k, \"transitive\") \u21d2 transrelation(i, k, \"transitive\") ( 14 ) aarelation(i, j, \"equivalence\") \u21d2 aarelation(i, k, +l) \u2227 aarelation(j, k, +l) (15) aarelation(j, k, \"equivalence\") \u21d2 aarelation(i, j, +l) \u2227 aarelation(i, k, +l) Figure 3 : Some of the global formulas lated\" were excluded from the annotation of the answer-answer relation. We considered only relation labels that more than two annotators agreed on the experiment. The number of pairs that we used and the distributions of relations are shown in Tables 9 and 10 , respectively.  To acquire the semantic category for nouns, we utilized a Japanese thesaurus, Nihongo-Goi-Taikei (Ikehara et al., 1997) . For antonyms, we used the Japanese dictionary, Kadokawa-Ruigo-Shin-Jiten (Ohno and Hamanishi, 1981) . For dependency parsing, we used Japanese dependency parser CaboCha 8 . For named entities, we utilized CaboCha's output. We used SVM light9 for the implementation of 8 http://chasen.org/\u02dctaku/software/cabocha/ 9 http://www.cs.cornell.edu/people/tj/svm light/ SVM and markov thebeast for a MLN. Results For each genre, we performed 10-fold cross validation and evaluated the F-value. The baseline model was one using SVM based on (Jimbo et al., 2010) . In this model, we used the observed predicates for our model as features and trained a binary classifier for the questionanswer relation and a one-versus-rest classifier for the answer-answer relation. The algorithm for the baseline model is as follows: step 1. Identify the question-answer relation between the question and each answer. step 2. For answers to be identified as an \"answer\", identify the answer-answer relation. Our work is different from Jimbo et al.'s work with respect to the number of relations. We considers seven relations for answer-answer pairs, while they consider four relations. Also, we used a different data from their experiment. Therefore, we could not conduct an accurate comparison experiment between our model and their model. Table 11 shows the results. Bold face indicates that F-value of our model was higher than the baseline model and symbols * * (p < 0.01) and * (p < 0.05) indicate the F-value was significantly different from the baseline with a sign test. Compared with the baseline model, our model was better for most relations. performed the SVM model is that these constraints worked well. Furthermore, the MLN model is better than the w/o-super model. Because super-relations focus on transitivity and semantic similarity, identifying these relations is easier than the logical relations and the information about these relations is useful for identifying the logical relations. In our model, there are some constraints between the predicate aarelation and the other predicates. When the performances for the identifications of auxiliary relations (i.e. hasaarelation, coarserelation, transrelation) are worse, the performance of the identification of logical relations would be worse too. Therefore, in order to improve the performance of logical relation identification, it is necessary to improve the performance of identifying these auxiliary relations. Conclusion We proposed a logical relation identification model with a Markov logic network. There are constraints between relations and we incorporated them into the model. These constraints may be violated and a MLN permits violation of them. Through the experiment, we showed that our model is better than a baseline model using SVM and that incorporating super-relations improves the performance. However, since the accuracy was not so high, we need to improve our model. The relation between answers is the effective information for understanding the overview of a thread. Our future work is to propose answers summarization model and thread visualization model, based on these logical relations. While the baseline model considered only the target pair in identification, our model considers the relations of all pairs at the same time and identifies relations to satisfy as many constraints as possible. These constraints on relations contribute to improve the performance for the identification of relations. Also, to evaluate the effectiveness of introducing the super-relations, we evaluated the model without coarse relations and transitive relations (w/o-super). Table 12 shows the results for the data for PC. Bold face indicates the best value for each relation. For most relations, the w/o-super model was 16 )) as well as the MLN model. We consider that the reason why the w/o-super model out-",
    "abstract": "Community-based Question Answering services contain many threads consisting of a question and its answers. When there are many answers for a question, it is hard for a user to understand them all. To address this problem, we focus on logical relations between answers in a thread and present a model for identifying the relations between the answers. We consider that there are constraints among the relations, such as a transitive law, and that it might be useful to take these constraints into account. To consider these constraints, we propose the model based on a Markov logic network. We also introduce super-relations to give additional information for logical relation identification into our model. Through the experiment, we show that global constraints and super-relations make it easier to identify the relations.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "1",
    "year": "2011",
    "month": "November",
    "title": "Identification of relations between answers with global constraints for Community-based Question Answering services"
}