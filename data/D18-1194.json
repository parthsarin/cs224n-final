{
    "article": "We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F 1 score. Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012) , AMR (Banarescu et al., 2013) , UCCA (Abend and Rappoport, 2013) , and UDS (White et al., 2016) , as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014) , or Discourse Representation Theory (Kamp, 1981; Heim, 1988) . Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017) , ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995) , multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad\u00f3 and Lapata, 2005) , among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource languages (Strassel and Tracey, 2016) , it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often without researcher involvement, 1 and even when not available, it may be easier to find bilingual speakers that can translate a text, than it is to find experts that will create in-language semantic annotations. In addition, we are simply further along in being able to automatically understand English than we are other languages, resulting from the bias in investment in English-rooted resources. Therefore, we propose the task of cross-lingual decompositional semantic parsing, which aims at transducing a sentence in the source language (e.g., Chinese sentence in Fig. 1b ) into a decompositional semantic analysis derived based on English, via bitext. The efforts of decompositional semantics (White et al., 2016) focus on approaches to annotating meaning based on finegrained scalar judgments which reflect the ambiguity of language, and the underspecification of meaning in context. Our contributions include: (1) A form of decompositional semantic analysis allowing systems to target varying levels of structural complexity. (2) An evaluation metric to measure the similarity between system and reference semantic analysis. (3) An encoder-decoder model for cross-lingual decompositional semantic parsing. With a coreference annotating mechanism, the model solves intra-sentential coreference explicitly. (4) The first evaluation dataset for cross-lingual decompositional semantic parsing. 2  Experiments demonstrate our model achieves 38.78% F 1 , outperforming strong baselines. O P t 2 / 8 S A q D r v v t 5 B Y W l 5 Z X 8 q u F t f W N z a 3 i 9 s 6 t C W P N o c 5 D G e q m z w x I o a C O A i U 0 I w 0 s 8 C U 0 / O H l x G / c g T Y i V D c 4 i q A d s L 4 S P c E Z W q l T P P I k U 3 0 J H s I D J v e g g W q I Q o 3 Q H X e m 4 m D s 6 T T T K Z b c s p u C z p N K R k o k Q 6 1 T / P K 6 I Y 8 D U M g l M 6 Z V c S N s J 0 y j 4 B L G B S 8 2 E D E + Z H 1 o W a p Y A K a d p E e N 6 Y F V u r Q X a v s U 0 l T 9 P Z G w w J h R 4 N t k w H B g Z r 2 J + J / X i r F 3 1 k 6 E i m I E x a e L e r G k G N J J Q 7 Q r N H C U I 0 s Y 1 8 L + l f I B 0 4 y j 7 b F g S 6 j M n j x P 6 s f l 8 7 J 7 f V K q X m R t 5 M k e 2 S e H p E J O S Z V c k R q p E 0 4 e y T N 5 J W / O k / P i v D s f 0 2 j O y W Z 2 y R 8 4 n z + Y g 6 A B < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 e 6 m l u m A B M Y y G j x u T O 6 7 G f G u 0 d w = \" > A A A C E 3 i c b V A 9 S w N B F N y L X z F + R S 1 t F o M g C O E i g t o F b S w j G B P I h b C 3 e U m W 7 O 0 d u + / U c O R H 2 P h X b C x U b G 3 s / D d u L l d o 4 s D C M D O P t 2 / 8 S A q D r v v t 5 B Y W l 5 Z X 8 q u F t f W N z a 3 i 9 s 6 t C W P N o c 5 D G e q m z w x I o a C O A i U 0 I w 0 s 8 C U 0 / O H l x G / c g T Y i V D c 4 i q A d s L 4 S P c E Z W q l T P P I k U 3 0 J H s I D J v e g g W q I Q o 3 Q H X e m 4 m D s 6 T T T K Z b c s p u C z p N K R k o k Q 6 1 T / P K 6 I Y 8 D U M g l M 6 Z V c S N s J 0 y j 4 B L G B S 8 2 E D E + Z H 1 o W a p Y A K a d p E e N 6 Y F V u r Q X a v s U 0 l T 9 P Z G w w J h R 4 N t k w H B g Z r 2 J + J / X i r F 3 1 k 6 E i m I E x a e L e r G k G N J J Q 7 Q r N H C U I 0 s Y 1 8 L + l f I B 0 4 y j 7 b F g S 6 j M n j x P 6 s f l 8 7 J 7 f V K q X m R t 5 M k e 2 S e H p E J O S Z V c k R q p E 0 4 e y T N 5 J W / O k / P i v D s f 0 2 j O y W Z 2 y R 8 4 n z + Y g 6 A B < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 e 6 m l u m A B M Y y G j x u T O 6 7 G f G u 0 d w = \" > A A A C E 3 i c b V A 9 S w N B F N y L X z F + R S 1 t F o M g C O E i g t o F b S w j G B P I h b C 3 e U m W 7 O 0 d u + / U c O R H 2 P h X b C x U b G 3 s / D d u L l d o 4 s D C M D O P t 2 / 8 S A q D r v v t 5 B Y W l 5 Z X 8 q u F t f W N z a 3 i 9 s 6 t C W P N o c 5 D G e q m z w x I o a C O A i U 0 I w 0 s 8 C U 0 / O H l x G / c g T Y i V D c 4 i q A d s L 4 S P c E Z W q l T P P I k U 3 0 J H s I D J v e g g W q I Q o 3 Q H X e m 4 m D s 6 T T T K Z b c s p u C z p N K R k o k Q 6 1 T / P K 6 I Y 8 D U M g l M 6 Z V c S N s J 0 y j 4 B L G B S 8 2 E D E + Z H 1 o W a p Y A K a d p E e N 6 Y F V u r Q X a v s U 0 l T 9 P Z G w w J h R 4 N t k w H B g Z r 2 J + J / X i r F 3 1 k 6 E i m I E x a e L e r G k G N J J Q 7 Q r N H C U I 0 s Y 1 8 L + l f I B 0 4 y j 7 b F g S 6 j M n j x P 6 s f l 8 7 J 7 f V K q X m R t 5 M k e 2 S e H p E J O S Z V c k R q p E 0 4 e y T N 5 J W / O k / P i v D s f 0 2 j O y W Z 2 y R 8 4 n z + Y g 6 A B < / l a t e x i t > hin Biloxi h i < l a t e x i t s h a 1 _ b a s e 6 4 = \" l t F J r q f k 4 d w G r N R k z X A o E Q J t t 4 Q = \" > A A A C D 3 i c b V A 9 T w J B F N z z E / E L t b T Z S I x W 5 D A m a k e w s c T E E x K O k L 3 l A R v 2 9 i 6 7 7 w y E 8 B N s / C s 2 F m p s b e 3 8 N y 7 H F Q p O s s l k 5 r 2 8 n Q l i K Q y 6 7 r e z t L y y u r a e 2 8 h v b m 3 v 7 B b 2 9 u 9 N l G g O H o 9 k p B s B M y C F A g 8 F S m j E G l g Y S K g H g + u p X 3 8 A b U S k 7 n A U Q y t k P S W 6 g j O 0 U r t w 4 k u m e h J 8 h C G O h a J V I a O h m L R n Q n / i 6 9 R v F 4 p u y U 1 B F 0 k 5 I 0 W S o d Y u f P m d i C c h K O S S G d M s u z G 2 x k y j 4 B I m e T 8 x E D M + Y D 1 o W q p Y C K Y 1 T g N N 6 L F V O r Q b a f s U 0 l T 9 v T F m o T G j M L C T I c O + m f e m 4 n 9 e M 8 H u Z c v G j B M E x W e H u o m k G N F p O 7 Q j N H C U I 0 s Y 1 8 L + l f I + 0 4 y j 7 T B v S y j P R 1 4 k 3 l n p q u T e n h c r 1 a y N H D k k R + S U l M k F q Z A b U i M e 4 e S R P J N X 8 u Y 8 O S / O u / M x G 1 1 y s p 0 D 8 g f O 5 w / u 6 p 3 / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" l t F J r q f k 4 d w G r N R k z X A o E Q J t t 4 Q = \" > A A A C D 3 i c b V A 9 T w J B F N z z E / E L t b T Z S I x W 5 D A m a k e w s c T E E x K O k L 3 l A R v 2 9 i 6 7 7 w y E 8 B N s / C s 2 F m p s b e 3 8 N y 7 H F Q p O s s l k 5 r 2 8 n Q l i K Q y 6 7 r e z t L y y u r a e 2 8 h v b m 3 v 7 B b 2 9 u 9 N l G g O H o 9 k p B s B M y C F A g 8 F S m j E G l g Y S K g H g + u p X 3 8 A b U S k 7 n A U Q y t k P S W 6 g j O 0 U r t w 4 k u m e h J 8 h C G O h a J V I a O h m L R n Q n / i 6 9 R v F 4 p u y U 1 B F 0 k 5 I 0 W S o d Y u f P m d i C c h K O S S G d M s u z G 2 x k y j 4 B I m e T 8 x E D M + Y D 1 o W q p Y C K Y 1 T g N N 6 L F V O r Q b a f s U 0 l T 9 v T F m o T G j M L C T I c O + m f e m 4 n 9 e M 8 H u Z c v G j B M E x W e H u o m k G N F p O 7 Q j N H C U I 0 s Y 1 8 L + l f I + 0 4 y j 7 T B v S y j P R 1 4 k 3 l n p q u T e n h c r 1 a y N H D k k R + S U l M k F q Z A b U i M e 4 e S R P J N X 8 u Y 8 O S / O u / M x G 1 1 y s p 0 D 8 g f O 5 w / u 6 p 3 / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" l t F J r q f k 4 d w G r N R k z X A o E Q J t t 4 Q = \" > A A A C D 3 i c b V A 9 T w J B F N z z E / E L t b T Z S I x W 5 D A m a k e w s c T E E x K O k L 3 l A R v 2 9 i 6 7 7 w y E 8 B N s / C s 2 F m p s b e 3 8 N y 7 H F Q p O s s l k 5 r 2 8 n Q l i K Q y 6 7 r e z t L y y u r a e 2 8 h v b m 3 v 7 B b 2 9 u 9 N l G g O H o 9 k p B s B M y C F A g 8 F S m j E G l g Y S K g H g + u p X 3 8 A b U S k 7 n A U Q y t k P S W 6 g j O 0 U r t w 4 k u m e h J 8 h C G O h a J V I a O h m L R n Q n / i 6 9 R v F 4 p u y U 1 B F 0 k 5 I 0 W S o d Y u f P m d i C c h K O S S G d M s u z G 2 x k y j 4 B I m e T 8 x E D M + Y D 1 o W q p Y C K Y 1 T g N N 6 L F V O r Q b a f s U 0 l T 9 v T F m o T G j M L C T I c O + m f e m 4 n 9 e M 8 H u Z c v G j B M E x W e H u o m k G N F p O 7 Q j N H C U I 0 s Y 1 8 L + l f I + 0 4 y j 7 T B v S y j P R 1 4 k 3 l n p q u T e n h c r 1 a y N H D k k R + S U l M k F q Z A b U i M e 4 e S R P J N X 8 u Y 8 O S / O u / M x G 1 1 y s p 0 D 8 g f O 5 w / u 6 p 3 / < / l a t e x i t > h30 people h i < l a t e x i t s h a 1 _ b a s e 6 4 = \" W V u M u 6 7 T S B q N v m n a q a Y 1 Q F K V H z Q = \" > A A A C D 3 i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I z F a k U N N 1 I 5 o Y 4 m J C A l 3 I X v L A B v 2 9 i 6 7 c 0 Z y 4 S f Y + F d s L N T Y 2 t r 5 b 1 z g C g V f M s n L e z O 7 M y + I p T D o u t 9 O b m F x a X k l v 1 p Y W 9 / Y 3 C p u 7 9 y Z K N E c 6 j y S k W 4 G z I A U C u o o U E I z 1 s D C Q E I j G F y N / c Y 9 a C M i d Y v D G P y Q 9 Z T o C s 7 Q S u 3 i o S e Z 6 k n w E B 4 w P X F p D F E s Y d S e C v 2 R p y d + u 1 h y y + 4 E d J 5 U M l I i G W r t 4 p f X i X g S g k I u m T G t i h u j n z K N g t v 3 C 1 5 i I G Z 8 w H r Q s l S x E I y f T g 4 a 0 Q O r d G g 3 0 r Y U 0 o n 6 e y J l o T H D M L C d I c O + m f X G 4 n 9 e K 8 H u u Z 8 K F S c I i k 8 / 6 i a S Y k T H 6 d C O 0 M B R D i 1 h X A u 7 K + V 9 p h l H m 2 H B h l C Z P X m e 1 I / L F 2 X 3 5 r R U v c z S y J M 9 s k + O S I W c k S q 5 J j V S J 5 w 8 k m f y S t 6 c J + f F e X c + p q 0 5 J 5 v Z J X / g f P 4 A Z K W d q Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" W V u M u 6 7 T S B q N v m n a q a Y 1 Q F K V H z Q = \" > A A A C D 3 i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I z F a k U N N 1 I 5 o Y 4 m J C A l 3 I X v L A B v 2 9 i 6 7 c 0 Z y 4 S f Y + F d s L N T Y 2 t r 5 b 1 z g C g V f M s n L e z O 7 M y + I p T D o u t 9 O b m F x a X k l v 1 p Y W 9 / Y 3 C p u 7 9 y Z K N E c 6 j y S k W 4 G z I A U C u o o U E I z 1 s D C Q E I j G F y N / c Y 9 a C M i d Y v D G P y Q 9 Z T o C s 7 Q S u 3 i o S e Z 6 k n w E B 4 w P X F p D F E s Y d S e C v 2 R p y d + u 1 h y y + 4 E d J 5 U M l I i G W r t 4 p f X i X g S g k I u m T G t i h u j n z K N g t v 3 C 1 5 i I G Z 8 w H r Q s l S x E I y f T g 4 a 0 Q O r d G g 3 0 r Y U 0 o n 6 e y J l o T H D M L C d I c O + m f X G 4 n 9 e K 8 H u u Z 8 K F S c I i k 8 / 6 i a S Y k T H 6 d C O 0 M B R D i 1 h X A u 7 K + V 9 p h l H m 2 H B h l C Z P X m e 1 I / L F 2 X 3 5 r R U v c z S y J M 9 s k + O S I W c k S q 5 J j V S J 5 w 8 k m f y S t 6 c J + f F e X c + p q 0 5 J 5 v Z J X / g f P 4 A Z K W d q Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" W V u M u 6 7 T S B q N v m n a q a Y 1 Q F K V H z Q = \" > A A A C D 3 i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I z F a k U N N 1 I 5 o Y 4 m J C A l 3 I X v L A B v 2 9 i 6 7 c 0 Z y 4 S f Y + F d s L N T Y 2 t r 5 b 1 z g C g V f M s n L e z O 7 M y + I p T D o u t 9 O b m F x a X k l v 1 p Y W 9 / Y 3 C p u 7 9 y Z K N E c 6 j y S k W 4 G z I A U C u o o U E I z 1 s D C Q E I j G F y N / c Y 9 a C M i d Y v D G P y Q 9 Z T o C s 7 Q S u 3 i o S e Z 6 k n w E B 4 w P X F p D F E s Y d S e C v 2 R p y d + u 1 h y y + 4 E d J 5 U M l I i G W r t 4 p f X i X g S g k I u m T G t i h u j n z K N g t v 3 C 1 5 i I G Z 8 w H r Q s l S x E I y f T g 4 a 0 Q O r d G g 3 0 r Y U 0 o n 6 e y J l o T H D M L C d I c O + m f X G 4 n 9 e K 8 H u u Z 8 K F S c I i k 8 / 6 i a S Y k T H 6 d C O 0 M B R D i 1 h X A u 7 K + V 9 p h l H m 2 H B h l C Z P X m e 1 I / L F 2 X 3 5 r R U v c z S y J M 9 s k + O S I W c k S q 5 J j V S J 5 w 8 k m f y S t 6 c J + f F e X c + p q 0 5 J 5 v Z J X / g f P 4 A Z K W d q Q = = < / l a t e x i t > hwas hit h i < l a t e x i t s h a 1 _ b a s e 6 4 = \" j / C o K z N 5 X W g o Y y r X x T P / r z p a F B E = \" > A A A C D X i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I y G x I o c x U T u i j S U m I i Q c I X v L A B v 2 9 i 6 7 c y q 5 3 C + w 8 a / Y W K i x t b f z 3 7 j A F Q q + Z J K 3 7 8 1 k Z 5 4 f S W H Q d b + d 3 N L y y u p a f r 2 w s b m 1 v V P c 3 b s 1 Y a w 5 N H g o Q 9 3 y m Q E p F D R Q o I R W p I E F v o S m P 7 q c + M 0 7 0 E a E 6 g b H E X Q C N l C i L z h D K 3 W L Z U 8 y N Z D g I T x g c s 8 M H Q p M u 7 P n M P X 0 1 O 0 W S 2 7 F n Y I u k m p G S i R D v V v 8 8 n o h j w N Q y C U z p l 1 1 I + w k T K P g E t K C F x u I G B + x A b Q t V S w A 0 0 m m 5 6 S 0 b J U e 7 Y f a l k I 6 V X 9 P J C w w Z h z 4 t j N g O D T z 3 k T 8 z 2 v H 2 D / r J E J F M Y L i s 4 / 6 s a Q Y 0 k k 2 t C c 0 c J R j S x j X w u 5 K + Z B p x t E m W L A h V O d P X i S N 4 8 p 5 x b 0 + K d U u s j T y 5 I A c k i N S J a e k R q 5 I n T Q I J 4 / k m b y S N + f J e X H e n Y 9 Z a 8 7 J Z v b J H z i f P 4 J Z n T 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" j / C o K z N 5 X W g o Y y r X x T P / r z p a F B E = \" > A A A C D X i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I y G x I o c x U T u i j S U m I i Q c I X v L A B v 2 9 i 6 7 c y q 5 3 C + w 8 a / Y W K i x t b f z 3 7 j A F Q q + Z J K 3 7 8 1 k Z 5 4 f S W H Q d b + d 3 N L y y u p a f r 2 w s b m 1 v V P c 3 b s 1 Y a w 5 N H g o Q 9 3 y m Q E p F D R Q o I R W p I E F v o S m P 7 q c + M 0 7 0 E a E 6 g b H E X Q C N l C i L z h D K 3 W L Z U 8 y N Z D g I T x g c s 8 M H Q p M u 7 P n M P X 0 1 O 0 W S 2 7 F n Y I u k m p G S i R D v V v 8 8 n o h j w N Q y C U z p l 1 1 I + w k T K P g E t K C F x u I G B + x A b Q t V S w A 0 0 m m 5 6 S 0 b J U e 7 Y f a l k I 6 V X 9 P J C w w Z h z 4 t j N g O D T z 3 k T 8 z 2 v H 2 D / r J E J F M Y L i s 4 / 6 s a Q Y 0 k k 2 t C c 0 c J R j S x j X w u 5 K + Z B p x t E m W L A h V O d P X i S N 4 8 p 5 x b 0 + K d U u s j T y 5 I A c k i N S J a e k R q 5 I n T Q I J 4 / k m b y S N + f J e X H e n Y 9 Z a 8 7 J Z v b J H z i f P 4 J Z n T 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" j / C o K z N 5 X W g o Y y r X x T P / r z p a F B E = \" > A A A C D X i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I y G x I o c x U T u i j S U m I i Q c I X v L A B v 2 9 i 6 7 c y q 5 3 C + w 8 a / Y W K i x t b f z 3 7 j A F Q q + Z J K 3 7 8 1 k Z 5 4 f S W H Q d b + d 3 N L y y u p a f r 2 w s b m 1 v V P c 3 b s 1 Y a w 5 N H g o Q 9 3 y m Q E p F D R Q o I R W p I E F v o S m P 7 q c + M 0 7 0 E a E 6 g b H E X Q C N l C i L z h D K 3 W L Z U 8 y N Z D g I T x g c s 8 M H Q p M u 7 P n M P X 0 1 O 0 W S 2 7 F n Y I u k m p G S i R D v V v 8 8 n o h j w N Q y C U z p l 1 1 I + w k T K P g E t K C F x u I G B + x A b Q t V S w A 0 0 m m 5 6 S 0 b J U e 7 Y f a l k I 6 V X 9 P J C w w Z h z 4 t j N g O D T z 3 k T 8 z 2 v H 2 D / r J E J F M Y L i s 4 / 6 s a Q Y 0 k k 2 t C c 0 c J R j S x j X w u 5 K + Z B p x t E m W L A h V O d P X i S N 4 8 p 5 x b 0 + K d U u s j T y 5 I A c k i N S J a e k R q 5 I n T Q I J 4 / k m b y S N + f J e X H e n Y 9 Z a 8 7 J Z v b J H z i f P 4 J Z n T 0 = < / l a t e x i t > hdead h i < l a t e x i t s h a 1 _ b a s e 6 4 = \" c J 9 C 5 + 6 S T P g p C b J n h Q M f v B s 0 L C 8 = \" > A A A C C n i c b Z B N S 8 N A E I Y 3 f t b 6 V f X o J b Q I n k o q g n o r e v F Y w d h C U 8 p m M 2 m X b j Z h d y K W k L s X / 4 o X D y p e / Q X e / D d u 0 x 6 0 d W D h 4 X 1 n m J 3 X T w T X 6 D j f 1 t L y y u r a e m m j v L m 1 v b N b 2 d u / 0 3 G q G L g s F r H q + F S D 4 B J c 5 C i g k y i g k S + g 7 Y + u J n 7 7 H p T m s b z F c Q K 9 i A 4 k D z m j a K R + p e o J K g c C P I Q H z A K g Q d 6 f 8 j D 3 V G H 1 K z W n 7 h R l L 0 J j B j U y q 1 a / 8 u U F M U s j k M g E 1 b r b c B L s Z V Q h Z w L y s p d q S C g b 0 Q F 0 D U o a g e 5 l x S 2 5 f W S U w A 5 j Z Z 5 E u 1 B / T 2 Q 0 0 n o c + a Y z o j j U 8 9 5 E / M / r p h i e 9 z I u k x R B s u m i M B U 2 x v Y k G D v g C h i K s Q H K F D d / t d m Q K s r Q x F c 2 I T T m T 1 4 E 9 6 R + U X d u T m v N y 1 k a J X J I q u S Y N M g Z a Z J r 0 i I u Y e S R P J N X 8 m Y 9 W S / W u / U x b V 2 y Z j M H 5 E 9 Z n z 8 y w 5 v 9 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" c J 9 C 5 + 6 S T P g p C b J n h Q M f v B s 0 L C 8 = \" > A A A C C n i c b Z B N S 8 N A E I Y 3 f t b 6 V f X o J b Q I n k o q g n o r e v F Y w d h C U 8 p m M 2 m X b j Z h d y K W k L s X / 4 o X D y p e / Q X e / D d u 0 x 6 0 d W D h 4 X 1 n m J 3 X T w T X 6 D j f 1 t L y y u r a e m m j v L m 1 v b N b 2 d u / 0 3 G q G L g s F r H q + F S D 4 B J c 5 C i g k y i g k S + g 7 Y + u J n 7 7 H p T m s b z F c Q K 9 i A 4 k D z m j a K R + p e o J K g c C P I Q H z A K g Q d 6 f 8 j D 3 V G H 1 K z W n 7 h R l L 0 J j B j U y q 1 a / 8 u U F M U s j k M g E 1 b r b c B L s Z V Q h Z w L y s p d q S C g b 0 Q F 0 D U o a g e 5 l x S 2 5 f W S U w A 5 j Z Z 5 E u 1 B / T 2 Q 0 0 n o c + a Y z o j j U 8 9 5 E / M / r p h i e 9 z I u k x R B s u m i M B U 2 x v Y k G D v g C h i K s Q H K F D d / t d m Q K s r Q x F c 2 I T T m T 1 4 E 9 6 R + U X d u T m v N y 1 k a J X J I q u S Y N M g Z a Z J r 0 i I u Y e S R P J N X 8 m Y 9 W S / W u / U x b V 2 y Z j M H 5 E 9 Z n z 8 y w 5 v 9 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" c J 9 C 5 + 6 S T P g p C b J n h Q M f v B s 0 L C 8 = \" > A A A C C n i c b Z B N S 8 N A E I Y 3 f t b 6 V f X o J b Q I n k o q g n o r e v F Y w d h C U 8 p m M 2 m X b j Z h d y K W k L s X / 4 o X D y p e / Q X e / D d u 0 x 6 0 d W D h 4 X 1 n m J 3 X T w T X 6 D j f 1 t L y y u r a e m m j v L m 1 v b N b 2 d u / 0 3 G q G L g s F r H q + F S D 4 B J c 5 C i g k y i g k S + g 7 Y + u J n 7 7 H p T m s b z F c Q K 9 i A 4 k D z m j a K R + p e o J K g c C P I Q H z A K g Q d 6 f 8 j D 3 V G H 1 K z W n 7 h R l L 0 J j B j U y q 1 a / 8 u U F M U s j k M g E 1 b r b c B L s Z V Q h Z w L y s p d q S C g b 0 Q F 0 D U o a g e 5 l x S 2 5 f W S U w A 5 j Z Z 5 E u 1 B / T 2 Q 0 0 n o c + a Y z o j j U 8 9 5 E / M / r p h i e 9 z I u k x R B s u m i M B U 2 x v Y k G D v g C h i K s Q H K F D d / t d m Q K s r Q x F c 2 I T T m T 1 4 E 9 6 R + U X d u T m v N y 1 k a J X J I q u S Y N M g Z a Z J r 0 i I u Y e S R P J N X 8 m Y 9 W S / W u / U x b V 2 y Z j M H 5 E 9 Z n z 8 y w 5 v 9 < / l a t e x i t > hby a storm surge h i < l a t e x i t s h a 1 _ b a s e 6 4 = \" W d n t t y K + X U b 5 2 j v c P i d w A D L Q 8 Y 8 = \" > A A A C F n i c b V A 9 S w N B F N y L X z F + R S 1 t F o N g F S 4 i q F 3 Q x j K C Z w K 5 E P Y 2 L 8 m S v b 1 j 9 5 1 4 H P k X N v 4 V G w s V W 7 H z 3 7 h J r t D E g Y V h Z h 5 v 3 w S x F A Z d 9 9 s p L C 2 v r K 4 V 1 0 s b m 1 v b O + X d v T s T J Z q D x y M Z 6 V b A D E i h w E O B E l q x B h Y G E p r B 6 G r i N + 9 B G x G p W 0 x j 6 I R s o E R f c I Z W 6 p a r v m R q I M F H e M A s S C m j B i M d U p P o A Y y 7 M 3 0 4 9 v U 0 1 i 1 X 3 K o 7 B V 0 k t Z x U S I 5 G t / z l 9 y K e h K C Q S 2 Z M u + b G 2 M m Y R s E l j E t + Y i B m f M Q G 0 L Z U s R B M J 5 v e N a Z H V u n R f q T t U 0 i n 6 u + J j I X G p G F g k y H D o Z n 3 J u J / X j v B / n k n E y p O E B S f L e o n k m J E J y X R n t D A U a a W M K 6 F / S v l Q 6 Y Z R 1 t l y Z Z Q m z 9 5 k X g n 1 Y u q e 3 N a q V / m b R T J A T k k x 6 R G z k i d X J M G 8 Q g n j + S Z v J I 3 5 8 l 5 c d 6 d j 1 m 0 4 O Q z + + Q P n M 8 f U h O g 3 g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" W d n t t y K + X U b 5 2 j v c P i d w A D L Q 8 Y 8 = \" > A A A C F n i c b V A 9 S w N B F N y L X z F + R S 1 t F o N g F S 4 i q F 3 Q x j K C Z w K 5 E P Y 2 L 8 m S v b 1 j 9 5 1 4 H P k X N v 4 V G w s V W 7 H z 3 7 h J r t D E g Y V h Z h 5 v 3 w S x F A Z d 9 9 s p L C 2 v r K 4 V 1 0 s b m 1 v b O + X d v T s T J Z q D x y M Z 6 V b A D E i h w E O B E l q x B h Y G E p r B 6 G r i N + 9 B G x G p W 0 x j 6 I R s o E R f c I Z W 6 p a r v m R q I M F H e M A s S C m j B i M d U p P o A Y y 7 M 3 0 4 9 v U 0 1 i 1 X 3 K o 7 B V 0 k t Z x U S I 5 G t / z l 9 y K e h K C Q S 2 Z M u + b G 2 M m Y R s E l j E t + Y i B m f M Q G 0 L Z U s R B M J 5 v e N a Z H V u n R f q T t U 0 i n 6 u + J j I X G p G F g k y H D o Z n 3 J u J / X j v B / n k n E y p O E B S f L e o n k m J E J y X R n t D A U a a W M K 6 F / S v l Q 6 Y Z R 1 t l y Z Z Q m z 9 5 k X g n 1 Y u q e 3 N a q V / m b R T J A T k k x 6 R G z k i d X J M G 8 Q g n j + S Z v J I 3 5 8 l 5 c d 6 d j 1 m 0 4 O Q z + + Q P n M 8 f U h O g 3 g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" W d n t t y K + X U b 5 2 j v c P i d w A D L Q 8 Y 8 = \" > A A A C F n i c b V A 9 S w N B F N y L X z F + R S 1 t F o N g F S 4 i q F 3 Q x j K C Z w K 5 E P Y 2 L 8 m S v b 1 j 9 5 1 4 H P k X N v 4 V G w s V W 7 H z 3 7 h J r t D E g Y V h Z h 5 v 3 w S x F A Z d 9 9 s p L C 2 v r K 4 V 1 0 s b m 1 v b O + X d v T s T J Z q D x y M Z 6 V b A D E i h w E O B E l q x B h Y G E p r B 6 G r i N + 9 B G x G p W 0 x j 6 I R s o E R f c I Z W 6 p a r v m R q I M F H e M A s S C m j B i M d U p P o A Y y 7 M 3 0 4 9 v U 0 1 i 1 X 3 K o 7 B V 0 k t Z x U S I 5 G t / z l 9 y K e h K C Q S 2 Z M u + b G 2 M m Y R s E l j E t + Y i B m f M Q G 0 L Z U s R B M J 5 v e N a Z H V u n R f q T t U 0 i n 6 u + J j I X G p G F g k y H D o Z n 3 J u J / X j v B / n k n E y p O E B S f L e o n k m J E J y X R n t D A U a a W M K 6 F / S v l Q 6 Y Z R 1 t l y Z Z Q m z 9 5 k X g n 1 Y u q e 3 N a q V / m b R T J A T k k x 6 R G z k i d X J M G 8 Q g n j + S Z v J I 3 5 8 l 5 c d 6 d j 1 m 0 4 O Q z + + Q P n M 8 f U h O g 3 g = = < / l a t e x i t > hin one block h of flatsi < l a t e x i t s h a 1 _ b a s e 6 4 = \" z W G 6 M O c b a H g s 7 h i C L c n 4 I d S X 2 q g = \" > A A A C I X i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z S q C d l d 0 4 7 K C t Y V O K Z n 0 T h u a S Y b k j l j K f I s b f 8 W N C 5 X u x J 8 x f S y 0 9 U L I 4 Z x 7 S M 4 J E y k s + v 6 X t 7 K 6 t r 6 x m d v K b + / s 7 u 0 X D g 4 f r E 4 N h z r X U p t m y C x I o a C O A i U 0 E w M s D i U 0 w s H N R G 8 8 g r F C q 3 s c J t C O W U + J S H C G j u o U K o F k q i c h Q H j C k V B U K 6 C h 1 H y Q d W Z c P 5 v d O q K R Z G i z w E w d n U L R L / n T o c u g P A d F M p 9 a p z A O u p q n M S j k k l n b K v s J t k f M o O A S s n y Q W k g Y H 7 A e t B x U L A b b H k 0 j Z v T U M V 0 a a e O O Q j p l f z t G L L Z 2 G I d u M 2 b Y t 4 v a h P x P a 6 U Y X b V d 8 C R F U H z 2 U J R K i p p O + q J d Y Y C j H D r A u B H u r 5 T 3 m W E c X a t 5 V 0 J 5 M f I y q J + X K i X / 7 q J Y v Z 6 3 k S P H 5 I S c k T K 5 J F V y S 2 q k T j h 5 J q / k n X x 4 L 9 6 b 9 + m N Z 6 s r 3 t x z R P 6 M 9 / 0 D w h G l 4 Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z W G 6 M O c b a H g s 7 h i C L c n 4 I d S X 2 q g = \" > A A A C I X i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z S q C d l d 0 4 7 K C t Y V O K Z n 0 T h u a S Y b k j l j K f I s b f 8 W N C 5 X u x J 8 x f S y 0 9 U L I 4 Z x 7 S M 4 J E y k s + v 6 X t 7 K 6 t r 6 x m d v K b + / s 7 u 0 X D g 4 f r E 4 N h z r X U p t m y C x I o a C O A i U 0 E w M s D i U 0 w s H N R G 8 8 g r F C q 3 s c J t C O W U + J S H C G j u o U K o F k q i c h Q H j C k V B U K 6 C h 1 H y Q d W Z c P 5 v d O q K R Z G i z w E w d n U L R L / n T o c u g P A d F M p 9 a p z A O u p q n M S j k k l n b K v s J t k f M o O A S s n y Q W k g Y H 7 A e t B x U L A b b H k 0 j Z v T U M V 0 a a e O O Q j p l f z t G L L Z 2 G I d u M 2 b Y t 4 v a h P x P a 6 U Y X b V d 8 C R F U H z 2 U J R K i p p O + q J d Y Y C j H D r A u B H u r 5 T 3 m W E c X a t 5 V 0 J 5 M f I y q J + X K i X / 7 q J Y v Z 6 3 k S P H 5 I S c k T K 5 J F V y S 2 q k T j h 5 J q / k n X x 4 L 9 6 b 9 + m N Z 6 s r 3 t x z R P 6 M 9 / 0 D w h G l 4 Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z W G 6 M O c b a H g s 7 h i C L c n 4 I d S X 2 q g = \" > A A A C I X i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z S q C d l d 0 4 7 K C t Y V O K Z n 0 T h u a S Y b k j l j K f I s b f 8 W N C 5 X u x J 8 x f S y 0 9 U L I 4 Z x 7 S M 4 J E y k s + v 6 X t 7 K 6 t r 6 x m d v K b + / s 7 u 0 X D g 4 f r E 4 N h z r X U p t m y C x I o a C O A i U 0 E w M s D i U 0 w s H N R G 8 8 g r F C q 3 s c J t C O W U + J S H C G j u o U K o F k q i c h Q H j C k V B U K 6 C h 1 H y Q d W Z c P 5 v d O q K R Z G i z w E w d n U L R L / n T o c u g P A d F M p 9 a p z A O u p q n M S j k k l n b K v s J t k f M o O A S s n y Q W k g Y H 7 A e t B x U L A b b H k 0 j Z v T U M V 0 a a e O O Q j p l f z t G L L Z 2 G I d u M 2 b Y t 4 v a h P x P a 6 U Y X b V d 8 C R F U H z 2 U J R K i p p O + q J d Y Y C j H D r A u B H u r 5 T 3 m W E c X a t 5 V 0 J 5 M f I y q J + X K i X / 7 q J Y v Z 6 3 k S P H 5 I S c k T K 5 J F V y S 2 q k T j h 5 J q / k n X x 4 L 9 6 b 9 + m N Z 6 s r 3 t x z R P 6 M 9 / 0 D w h G l 4 Q = = < / l a t e x i t > AW A RE N ES S SE N TI EN T IN ST IG AT IO N \u2026 0 0 0. 9 \u2026 AW AR EN ES S SE NT IE NT IN ST IG AT IO N \u2026 0.0 0.0 0.1 \u2026 AR G A R G A R G A R G A R G A R Semantic Analysis The goal of cross-lingual decompositional semantic parsing is to provide a semantic analysis which can be used for various types of deep and shallow processing on the target language side. Many forms of semantic analysis are potentially suitable for this goal, e.g., AMR (Banarescu et al., 2013) , UCCA (Abend and Rappoport, 2013) , and Universal Decompositional Semantics (White et al., 2016) . Here we choose Universal Decompositional Semantics (UDS), but note that our approach is applicable to other potential graph semantic formalisms. The reasons for choosing UDS are three-fold: (1) Compatibility: UDS relates to Robust Minimal Recursion Semantics (RMRS) (Copestake, 2007) , aiming for a maximal degree of semantic compatibility. With UDS, shallow analysis, such as predicate-argument extraction (Zhang et al., 2017a) , can be regarded as producing a semantics which is underspecified and reusable with respect to deeper analysis, such as lexical semantics and inference (White et al., 2016) . (2) Robustness and Speed: There exists a robust framework, PredPatt (White et al., 2016) , for automatically creating UDS from raw sentences and their Universal Dependencies. PredPatt has been shown to be fast and accurate enough to process large volumes of text (Zhang et al., 2017c) . ( 3 ) Cross-lingual validity: PredPatt is based purely on non-lexical and linguistically well-founded patterns from Universal Dependencies, which is designed to be cross-linguistically consistent. There are three forms to represent UDS: flat, graph, or linearized representations. They are created for different purposes, and are inter-convertible. Flat representation relates to RMRS (Copestake, 2007) , and we defer its description to Appendix A. Graph Representation The graph representation as shown in Fig. 1a is developed to improve ease of readability, parser evaluation, and integration with lexical semantics. The structure of the graph representation is a tuple G = V, E : a set of variables V (e.g., p 1 and x), and a set of edges E. There are 3 types of edges: (1) Argument edges describe argument relations between variable pairs. Deeper analysis such as Semantic Proto-Role (SPR) properties (Reisinger et al., 2015) edges describe instances of variables in the target language. The subscript \"h\" indicates the syntactic head of an instance. (3) Attribute edges are unary, which describe various attributes of variables, such as event factuality (Saur\u00ed and Pustejovsky, 2009) and word senses (Miller, 1995) . The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) , and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017) . There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006) . Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol \"\u2022\" and add a COREF link between \"\u2022\" and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol \"\u2022\" are all considered as tokens in this representation. The COREF links are drawn as a visual convenience, and the actual linearized representation achieves this via co-indexing, and is thus fully linear. We describe the procedure of converting graph representation to linearized representation in Appendix B. Related Work Our work synthesizes two strands of research, semantic analysis and cross-lingual learning. The semantic analysis targeted in this work is akin to that of Hobbs (2003) , but our eventual goal is to transduce texts from arbitrary human languages into a \"...broad, language-like, inferenceenabling [semantic representation] in the spirit of Montague...\" (Schubert, 2015) . Unlike efforts such as by Schubert and colleagues that directly target such an analysis, we are pursuing a strategy that incrementally increases the complexity of the target analysis in accordance with our ability to fashion models capable of producing it. 3 Embracing underspecification in the name of tractability is exemplified by MRS (Copestake et al., 2005; Copestake, 2009) , the so-called slacker semantics, and we draw inspiration from that work. Analyses such as AMR (Banarescu et al., 2013 ) also make use of underspecification, but usually this is only implicit: certain aspects of meaning are simply not annotated. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005) , CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004) , and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017) . See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001) ; Pad\u00f3 and Lapata (2009) ; Evang and Bos (2016) ; Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008) ; Ganchev et al. (2009) ; McDonald et al. (2011); Naseem et al. (2012) ; Wang and Manning (2014) enabled model transfer by shar-ing features or model parameters for different languages. Sudo et al. (2004) ; Zhang et al. (2017a,b) ; Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. Evaluation Metric S UDS can be represented in three forms. Evaluating such forms is crucial to the development of parsing algorithms. However, there is no method directly available for evaluation. Related methods come from semantic parsing, whose results are mainly evaluated in three ways: (1) task correctness (Tang and Mooney, 2001) , which evaluates on a specific NLP task that uses the parsing results; (2) whole-parse correctness (Zettlemoyer and Collins, 2005) , which counts the number of parsing results that are completely correct; and (3) Smatch (Cai and Knight, 2013) , which computes the number of exactly matched edges between two semantic structures. Nevertheless, our task needs an evaluation metric that can be used regardless of specific tasks or domains, and is able to differentiate two UDS graph representations with similar instances, SPR analysis, or attributes. We design an evaluation metric S that computes the similarity between two graph representations. As described in Section 2.1, the graph representation is a tuple G = (V, E). For two graphs G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ), we define the score S as the maximum soft edge matching score between G 1 and G 2 : S(G 1 , G 2 ) = max m\u2208M (e (i) 1 ,e 2 )\u2208P f T (e (i) 1 , e (j) 2 ) where m is a mapping from variables in V 1 to variables in V 2 . Given a mapping m, P is a set of edge pairs: for each pair (e (i) 1 , e 2 ), variables(s) in e The precision and recall are computed by S(G 1 , G 2 )/|E 1 |, and S(G 1 , G 2 )/|E 2 | respectively. In this work, f ARG = f ATTR = e \u2212MAE , where MAE computes the mean absolute error between two set of scores s 1 and s 2 : (Papineni et al., 2002) which compute the BLEU score of an instance pair. 4  Finding an optimal variable mapping m that yields the highest S is NP-complete. We instead adopt a strategy used in Smatch (Cai and Knight, 2013) that does a hill-climbing search with smart initialization plus 4 random restarts, and has been shown to give the best trade-off between accuracy and speed. Smatch for evaluating semantic structures can be considered as a special case of S, where f T = \u03b4, the Kronecker delta. n i |s (i) 1 \u2212 s (i) 2 |/n. f INST = BLEU Model We formulate the task of cross-lingual decompositional semantic parsing as a joint problem of sequence-to-sequence learning, coreference resolution and decompositional semantic analysis. The input is a sentence X in the source language, e.g., the Chinese sentence in Fig. 1b . The output is a UDS linearized representation (Y, C, D) based on the target language: Y is a sequence of tokens; C is a set of COREF links; and D is a set of scores for decompositional analysis, such as SPR and factuality. The goal is to learn a conditional probability distribution P (Y, C, D|X) whose most likely configuration, given the input sentence, outputs the true UDS linearized representation with decompositional analysis. While the standard encoderdecoder framework shows the state-of-the-art performance in sequence-to-sequence learning (Choe and Charniak, 2016; Jia and Liang, 2016; Barzdins and Gosko, 2016) , it cannot directly solve intrasentential conference and decompositional semantic analyses in our task. To achieve this goal, we propose an encoder-decoder architecture incorporated with a coreference annotating mechanism 5 and decompositional analysis. As illustrated in Fig. 3 , Encoder transforms the input sequence into hidden states; Decoder reads the hidden states, and then at each time step generates a token and creates its COREF link; Decompositional Analysis, based on the decoder output, performs SPR analysis for predicate-argument pairs, and factuality analysis for predicates. COREF Decompostional Analysis Figure 3 : Illustration of the model architecture. Encoder The encoder employs a bidirectional recurrent neural network (Schuster and Paliwal, 1997) with LSTM units (Hochreiter and Schmidhuber, 1997) . It encodes the input X = x 1 , . . . , x N 6 into a sequence of hidden states h = h 1 , . . . , h N . Each hidden state h i is a concatenation of a left-to-right hidden state \u2212 \u2192 h i and a right-to-left hidden state \u2190 \u2212 h i , Decoder Given the encoder hidden states, the decoder predicts the linearized representation (as shown in Fig. 2 ) according to the conditional probability P (Y, C | X) which is decomposed as a product of the decoding probabilities at each time step t: P (Y, C | X) = M t=1 P (y t , c t | y <t , c <t , X) (1) where y t is the decoded token at time step t, and c t is the source of the COREF link for y t , i.e., the antecedent of y t . The set of possible antecedents of y t is A(t) = { , y 1 , . . . , y t\u22121 }: a dummy antecedent and all preceding tokens. represents a scenario, where the token is not a special symbol \"\u2022\", and it refers to none of the preceding tokens. y <t and c <t are the preceding tokens and their antecedents. We omit y <t and c <t from the notation when the context is unambiguous. The decoding probability at each time step t is decomposed as P (y t , c t ) = P (y t )P (c t |y t ) (2) where P (y t ) is the token generation probability, and P (c t |y t ) is the antecedent probability. Token Generation: The probability distribution of the generated token y t is defined as P (y t ) = softmax(FFNN g (s t , a t )) (3) where FFNN g is a two-layer feed-forward neural network over the decoder hidden state s t and the attention-weighted vector a t . s t is computed by s t = RNN(y t\u22121 , s t\u22121 ), (4) where RNN is a recurrent neural network using LSTM. a t is computed by the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) , a t = N i \u03b1 t,i h i , (5) \u03b1 t,i = exp (s t (W a h i + b a ))) N j=1 exp (s t (W a h j + b a )) , (6) where W a is a transform matrix and b a is a bias. Coref Link: The probability of y t referring to the preceding token y k , i.e., c t = y k , is defined as P (c t = y k |y t ) = exp (SCORE(y t , y k )) y k \u2208A(t) exp (SCORE(y t , y k )) , (7) SCORE(y t , y k ) is a pairwise score for a COREF link from y k to y t , defined as: SCORE(y t , y k ) = s c (y t ) + s p (y k ) + s a (y t , y k ) (8) There are three factors in this pairwise score, which is akin to Lee et al. (2017) : (1) s c (y t ), whether y t should refer to a preceding instance; (2) s p (y k ), whether y k shoud be a candidate source of such a coreference; and (3) s a (y t , y k ), whether y k is an antecedent of y t . Fig. 4 shows the details of the scoring architecture. At the core of the three factors are vector representations \u03b3(y t ) for each token y t , which is described in detail in the following section. Given the currently considered token y t and a preceding token y k , the scoring functions above are computed via standard feed-foward neural networks: s c (y t ) =w c \u2022 FFNN c (\u03b3(y t )) (9) s p (y k ) =w p \u2022 FFNN p (\u03b3(y k )) (10) s a (y t , y k ) =w a \u2022 FFNN a [\u03b3(y t ), \u03b3(y k , \u03b3(y t ) \u2022 \u03b3(y k )]) (11) where \u2022 denotes dot product, \u2022 denotes elementwise multiplication, and FFNN denotes a two-layer feed-foward neural network over the input. The input of FFNN a is a concatenation of vector representations \u03b3(y t ) and \u03b3(y k ), and their explicit elementwise similarity \u03b3(y t ) \u2022 \u03b3(y k ). Token representations: To accurately predict COREF link scores as well as decompositional analysis (which is described in the following section), we consider three types of information in each token representation \u03b3(y t ): (1) the token itself y t , (2) on the decoder side, the preceding context y <t , and (3) on the encoder side, the input sequence X = x 1 , . . . , x N . The lexical information of the token itself y t is represented by its word embedding e t . The preceding context y <t is encoded by the decoder RNN in Equation ( 4 ). We use the decoder hidden state s t to represent the preceding context information. The encoder-side context is represented by an attention-weighted weight a t defined in Equation (6). All the above information is concatenated to produce the final token representation \u03b3(y t ): \u03b3(y t ) = [e t , s t , a t ] (12) Decompositional Analyses The decompositional analyses D contains scores for Semantic Proto-Role (SPR) properties D SPR , and scores for event factuality D FACT . SPR: Given a predicate-argument pair (y i , y j ), we denote the score for SPR property p as D (y i ,y j ) SPRp . As shown in Fig. 3 , we concatenate the token representations of predicate and argument head tokens \u03b3(y i ) and \u03b3(y j ) as the input to a SPR module. We employ the state-of-the-art SPR module in Rudinger et al. (2018a) , defined as: FACT . As shown in Fig. 3 , we take the token representation of predicate head token \u03b3(y k ) as the input to the state-of-the-art factuality module (Rudinger et al., 2018c) : D(y i ,y j ) SPRp = W SPRp ReLU(W shared [\u03b3(y i ), \u03b3(y j )]) ( D(y k ) FACT = V 2 ReLU (V 1 \u03b3(y k ) + b 1 ) + b 2 , (14) where V 1 and V 2 are weight matrices, and b 1 and b 2 are biases. The log-likelihood of factuality score is defined as negative of the Huber loss (Huber, 1964) with \u03b4 = 1. We assume conditional independence among decompositional analysis: P (D|X, Y, C) = (y i ,y j ) p (D (y i ,y j ) SPRp |X, Y, C) y k P (D (y k ) FACT |X, Y, C) (15) Learning Given the input sentence X, the output sequence of tokens Y , and the COREF links C, and the decompositional analysis D, the objective is to minimize the below negative log-likelihood: L = \u2212 log P (Y, C, D|X) = \u2212 M t=1 [\u00b5 1 log P (y t ) + \u00b5 2 log P (c t |y t )]\u2212 \u00b5 3 log P (D|X, Y, C) To increase the convergence rate, we pretrain the model by setting the weights \u00b5 1 = 1 and \u00b5 2 = \u00b5 3 = 0 to only optimize the token generation accuracy. After the model converges, we set \u00b5 2 = \u00b5 3 = 1 and lower \u00b5 1 = 0.1. S Experiments We now describe the evaluation data, baselines, and experimental results. Hyperparameter settings are reported in Appendix C. Data We choose Chinese as the source language and English as the target language. For test, we selected 270 sentences from the Universal Dependencies (UD) English Treebank (Silveira et al., 2014) test set, which have human-annotated SPR (White et al., 2016) and factuality (Rudinger et al., 2018c) analyses. We then created linearized representations for these sentences using PredPatt based on their gold UD syntax. Meanwhile, the Chinese translations of these sentences were created by crowdworkers on Amazon Mechanical Turk. The test dataset will be released upon publication. For training, we first collected about 1.8M Chinese-English sentence bitexts from the GALE project (Cohen, 2007) , then tokenized Chinese sentences with Stanford Word Segmenter (Chang et al., 2008) . We created linearized representations for English sentences using PredPatt based on automatic UD syntax generated by SyntaxNet Parser (Andor et al., 2016) , and added SPR and factuality annotations using the state-of-the-art models (Rudinger et al., 2018b,c) trained on SPR v2.x and It-happened v2.0 respectively. 7 We hold out 20K training sentences for validation and indomain test. Table 2 reports the dataset statistics. Variants We evaluate our model described in Section 5 and three variants: (a) We replace the coreference annotating mechanism by randomly choosing an an-  tecedent from all preceding instances. (b) We preprocess the data by replacing the special symbol \"\u2022\" with the syntactic head of its antecedent. During training and testing, we replace the coreference annotating mechanism with a heuristic that solves coreference by randomly choosing an antecedent among preceding instances which have the same syntactic head. (c) We remove the decoder-side information in the token representation \u03b3(y t ) defined in Equation ( 12 ) and only keep the encoder-side information a t . We also include a Pipeline approach where Chinese sentences are first translated into English by a neural machine translation system (Klein et al., 2017) and are then annotated by a UD parser (Andor et al., 2016) . The UDS linearized representation of Pipeline are created by PredPatt based the automatic UD parses. Overall, our proposed model outperforms the variants in every aspect. Variants (a) and (b) use simple heuristics to solve coreference, and achieve reasonable results: they both employ sequence-tosequence models to predict graph representations, which can be considered a replica of state-ofthe-art approaches for structured prediction (Choe and Charniak, 2016; Barzdins and Gosko, 2016; Peng et al., 2017) . Compared to our model which employs the coreference annotating mechanism, these two variants suffer notable loss in the precision of S metric. As a result, their performance drops on the other metrics. Variant (c) only uses the encoder-side information for token representation, resulting in significant loss in MAE SPR and MAE FACT . In the pipeline approach, each component is trained independently. Coreference occurs 589 times in the test set. To evaluate the coreference accuracy of our model, we force the decoder to generate the reference target sequence, and only predict coreference via the copy mechanism, or its variants. In Table 3 , we report the precision, recall, and F 1 for the standard MUC using the official coreference scorer of the CoNLL-2011/2012 shared tasks (Pradhan et al., 2014) . Since coreference in our setup occurs at the sentence level, our model achieves high performance. Variant (a) randomly choosing antecedents performs poorly, whereas variant (b), which solves coreference only based on syntactic heads, achieves a relatively high score. Variant (c) demonstrates that only using encoder-side information in the coreference annotating mechanism leads a significant performance drop. Since our model and the state-of-the-art monolingual SPR model (Rudinger et al., 2018c) use the same test set, we are able to compare the performance of our model against the monolingual model by forcing the decoder and the coreference mechanism to create the reference graph representation and only predicting the SPR property scores. Table 4 shows the Pearson coefficient of each SPR property. While our model only has the access to the sentence in the source language during the encoding stage, 8 the performance is comparable to the state-of-the-art monolingual model. Results Conclusions We introduce the task of cross-lingual decompositional semantic parsing, which maps content provided in a source language into decompositional analysis based on a target language. We present: UDS graph/linearized representations as the target semantic interface, the S metric for evaluation, and the Chinese-English decompositional semantic parsing dataset. We propose an end-toend learning approach with a coreference annotating mechanism which outperforms three strong baselines. We separately evaluate the coreference mechanism and SPR prediction, showing promising results. The representations for cross-lingual decompositional semantics, the evaluation metric, and the evaluation dataset provided in this work will be beneficial to the increasing interests in semantic analysis and cross-lingual applications. Acknowledgments Thank you to the anonymous reviewers for their feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI and AIDA. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government.",
    "abstract": "We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F 1 score.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "19",
    "year": "2018",
    "month": "October-November",
    "title": "Cross-lingual Decompositional Semantic Parsing"
}