{
    "article": "We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs judge the quality or the semantic similarity of text by using retrieved relevant training data as interpretants for reaching shared semantics. We derive features measuring the closeness of the test sentences to the training data via interpretants, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource. Semantic Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov and Zesch, 2014) . RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014) , achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014) , and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a) . Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic \u00b8ici, 2008) . Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. We describe the tasks we participated below. Section 2 describes the RTM model and the features used. Section 3 presents the training and test results we obtain on the three tasks we competed and the last section concludes. Task 1 Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment (SRE) (Marelli et al., 2014a) : Given two sentences, produce a relatedness score indicating the extent to which the sentences express a related meaning: a number in the range [1, 5] . We model the problem as a translation performance prediction task where one possible interpretation is obtained by translating S 1 (the source to translate, S) to S 2 (the target translation, T). Since linguistic processing can reveal deeper similarity relationships, we also look at the translation task at different granularities of information: plain text (R for regular) and after lemmatization (L). We lowercase all text. Task 3 Cross-Level Semantic Similarity (CLSS) (Jurgens et al., 2014) : Given two text from different levels, produce a semantic similarity rating: a number in the range [0, 4] . CLSS task targets semantic similarity comparisons between text having different levels of granularity and we address the following level crossings: paragraph to sentence, sentence to phrase, and phrase to word. We model the problem as a translation performance prediction task among text from different levels. Task 10 Multilingual Semantic Textual Similarity (MSTS) (Agirre et al., 2014) Given two sentences S 1 and S 2 in the same language, quantify the degree of similarity: a number in the range [0, 5]. MSTS task addresses the problem in English and Spanish (score range is [0, 4] ). We model the problem as a translation performance prediction task between S 1 and S 2 . Referential Translation Machine (RTM) Referential translation machines provide a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic \u00b8ici, 2011; Bic \u00b8ici and Yuret, 2014) as interpretants for reaching shared semantics (Bic \u00b8ici, 2008) . RTMs are a language independent approach and achieve top performance when predicting the quality of translations (Bic \u00b8ici, 2013; Bic \u00b8ici and Way, 2014) and when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014) , and good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014a) , as an automated student answer grader (Bic \u00b8ici and van Genabith, 2013b) , and when judging the semantic similarity of sentences (Bic \u00b8ici and van Genabith, 2013a; Agirre et al., 2014) . We improve the RTM models by: \u2022 using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 instance selection model (Bic \u00b8ici et al., 2014) , \u2022 better modeling of the language in which \u2022 using a general domain corpus to select interpretants from, \u2022 increased feature set for also modeling the structural properties of sentences, \u2022 extended learning models. We use the Parallel FDA5 (Feature Decay Algorithms) instance selection model for selecting the interpretants (Bic \u00b8ici et al., 2014; Bic \u00b8ici and Yuret, 2014) this year, which allows efficient parameterization, optimization, and implementation of FDA, and build an MTPP model (Section 2.1). We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012).  Our encouraging results in the semantic similarity tasks increase our understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the semantic similarity of text. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable in different domains and tasks. RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller and more relevant set of interpretants. The Machine Translation Performance Predictor (MTPP) MTPP (Bic \u00b8ici et al., 2013) is a state-of-the-art and top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPP measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP Features for Translation Acts MTPP feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, representing the grammatical information instantiated in the training and test data. We extend the MTPP model we used last year (Bic \u00b8ici, 2013) in its learning module and the features included. Categories for the features (S for source, T for target) used are listed below where the number of features are given in brackets for S and T, {#S, #T}, and the detailed descriptions for some of the features are presented in (Bic \u00b8ici et al., 2013) . The number of features for each task differs since we perform an initial feature selection step on the tree structural features (Section 2.3). The number of features are in the range 337\u2212437. \u2022 Coverage {56, 54}: Measures the degree to which the test features are found in the training set for both S ({56}) and T ({54}). \u2022 Perplexity {45, 45}: Measures the fluency of the sentences according to language models (LM). We use both forward ({30}) and backward ({15}) LM features for S and T. \u2022 TreeF {0, 10-110}: 10 base features and up to 100 selected features of T among parse tree structures (Section 2.3). \u2022 Retrieval Closeness {16, 12}: Measures the degree to which sentences close to the test set are found in the selected training set, I, using FDA (Bic \u00b8ici and Yuret, 2011a) and BLEU, F 1 (Bic \u00b8ici, 2011) , dice, and tf-idf cosine similarity metrics. \u2022 IBM2 Alignment Features {0, 22}: Calculates the sum of the entropy of the distribution of alignment probabilities for S ( s\u2208S \u2212p log p for p = p(t|s) where s and t are tokens) and T, their average for S and T, the number of entries with p \u2265 0.2 and p \u2265 0.01, the entropy of the word alignment between S and T and its average, and word alignment log probability and its value in terms of bits per word. We also compute word alignment percentage as in (Camargo de Souza et al., 2013) and potential BLEU, F 1 , WER, PER scores for S and T.    Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002) , NIST (Doddington, 2002) , or F 1 (Bic \u00b8ici and Yuret, 2011b) for q. \u2022 LIX {1, 1}: Calculates the LIX readability score (Wikipedia, 2013; Bj\u00f6rnsson, 1968) for S and T. 1 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg 1 LIX= A B + C 100 A , where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of \".\", \":\", \"!\", \"?\" similar to (Hagstr\u00f6m, 2012) . depthB), number of brackets on the right branches over the number of brackets on the left (R/L) 2 , average right to left branching over all internal tree nodes (avg R/L). The ratio of the number of right to left branches shows the degree to which the sentence is right branching or not. Additionally, we capture the different types of branching present in a given parse tree identified by the number of nodes in each of its children. Table 1 depicts the parsing output obtained by CCL for the following sentence from WSJ23 3 : Many fund managers argue that now 's the time to buy . We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( \u00c2ngelo Mendonc \u00b8a, 2011) 4 . We use the Stanford POS tagger (Toutanova et al., 2003) We use ridge regression (RR), support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch\u00f6lkopf, 2004) , and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009) , both of which are described in (Bic \u00b8ici et al., 2013) . We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic \u00b8ici et al., 2013; Bic \u00b8ici et al., 2014) . We optimize the learning parameters by selecting \u03b5 close to the standard deviation of the noise in the training set (Bic \u00b8ici, 2013) since the optimal value for \u03b5 is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998) . At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We obtain the confidence scores using support vector classification (SVC). Task 1: Semantic Relatedness and Entailment MSTS contains sentence pairs from the SICK (Sentences Involving Compositional Knowledge) data set (Marelli et al., 2014b) Table 7 lists the results along with their ranks for r P and r S , Spearman's correlation, out of CHECK submissions. The baseline in Table 7 is normalized longest common substring (LCS) scaled in the range [0, 4] . Top individual rank row lists the ranks in each subtask. We present the results for both our official and late (about 1 day) submissions including word to sense (W2S) results 6 . RTM-DCU is able to obtain the top result in Par2S in the CLSS task. Task 10: Multilingual Semantic Textual Similarity MSTS contains sentence pairs from different domains: sense definitions from semantic lexical resources such as OnWN (from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) ) and FNWN (from FrameNet (Baker et al., 1998) RTM results on the MSTS challenge test set are provided in Table 9 along with the RTM results in STS 2013 (Bic \u00b8ici and van Genabith, 2013a) . Table 10 and (Bic \u00b8ici and van Genabith, 2013a) . MSTS Spanish. The performance difference between MSTS English and MSTS Spanish may be due to the fewer training data available for the MSTS Spanish task, which may be decreasing the performance of our supervised learning approach. RTMs Across Tasks and Years We compare the difficulty of tasks according to the RAE levels achieved. RAE measures the error relative to the error when predicting the actual mean. A high RAE is an indicator that the task is hard. In Table 12 , we list the RAE obtained for different tasks and subtasks, also listing RTM results in STS 2013 (Bic \u00b8ici and van Genabith, 2013a) and RTM results (Bic \u00b8ici and Way, 2014) on the quality estimation task (QET) (Bojar et al., 2014) where post-editing effort (PEE), human-targeted transla- The best results are obtained for the CLSS Par2S subtask, which may be due to the larger contextual information that paragraphs can provide for the RTM models. For the SRE task, we can only reduce the error with respect to knowing and predicting the mean by about 35%. Prediction of bilingual similarity as in quality estimation of translation can be expected to be harder and RTMs achieve state-of-the-art performance in this task as well (Bic \u00b8ici and Way, 2014) . Conclusion Referential translation machines provide a clean and intuitive computational model for automatically measuring semantic similarity by measuring the acts of translation involved and achieve to be the top on some semantic similarity tasks at SemEval-2014. RTMs make quality and semantic similarity judgments possible based on the retrieval of relevant training data as interpretants for reaching shared semantics. (Bic \u00b8ici and van Genabith, 2013a) and results from quality estimation task of translation (Bojar et al., 2014; Bic \u00b8ici and Way, 2014) . Acknowledgments This work is supported in part by SFI (07/CE/I1142) as part of the CNGL Centre for Global Intelligent Content (www.cngl.org) at Dublin City University, in part by SFI (13/TIDA/I2740) for the project \"Monolingual and Bilingual Text Quality Judgments with Translation Performance Prediction\" (www.computing.dcu.ie/\u02dcebicici/ Projects/TIDA RTM.html), and in part by the European Commission through the QT-LaunchPad FP7 project (No: 296347). We also thank the SFI/HEA Irish Centre for High-End Computing (ICHEC) for the provision of computational facilities and support.",
    "abstract": "We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs judge the quality or the semantic similarity of text by using retrieved relevant training data as interpretants for reaching shared semantics. We derive features measuring the closeness of the test sentences to the training data via interpretants, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource.",
    "countries": [
        "Ireland"
    ],
    "languages": [
        "Spanish",
        "English"
    ],
    "numcitedby": "15",
    "year": "2014",
    "month": "August",
    "title": "{RTM}-{DCU}: Referential Translation Machines for Semantic Similarity"
}