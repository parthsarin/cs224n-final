{
    "article": "Different languages might have different word orders. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages. Introduction Cross-lingual transfer, which transfers models across languages, has tremendous practical value. It reduces the requirement of annotated data for a target language and is especially useful when the target language is lack of resources. Recently, this technique has been applied to many NLP tasks such as text categorization (Zhou et al., 2016a) , tagging (Kim et al., 2017) , dependency parsing (Guo et al., 2015 (Guo et al., , 2016) ) and machine translation (Zoph et al., 2016) . Despite the preliminary success, transferring across languages is challenging as it requires understanding and handling differences between languages at levels of morphology, syntax, and semantics. It is especially difficult to learn invariant features that can robustly transfer to distant languages. Prior work on cross-lingual transfer mainly focused on sharing word-level information by leveraging multi-lingual word embeddings (Xiao and Guo, 2014; Guo et al., 2016; Sil et al., 2018) . However, words are not independent in sentences; their combinations form larger linguistic units, known as context. Encoding context information is vital for many NLP tasks, and a variety of approaches (e.g., convolutional neural networks and recurrent neural networks) have been proposed to encode context as a high-level feature for downstream tasks. In this paper, we study how to transfer generic contextual information across languages. For cross-language transfer, one of the key challenges is the variation in word order among different languages. For example, the Verb-Object pattern in English can hardly be found in Japanese. This challenge should be taken into consideration in model design. RNN is a prevalent family of models for many NLP tasks and has demonstrated compelling performances (Mikolov et al., 2010; Sutskever et al., 2014; Peters et al., 2018) . However, its sequential nature makes it heavily reliant on word order information, which exposes to the risk of encoding language-specific order information that cannot generalize across languages. We characterize this as the \"order-sensitive\" property. Another family of models known as \"Transformer\" uses self-attention mechanisms to capture context and was shown to be effective in various NLP tasks (Vaswani et al., 2017; Liu et al., 2018; Kitaev and Klein, 2018) . With modification in position representations, the self-attention mechanism can be more robust than RNNs to the change of word order. We refer to this as the \"order-free\" property. In this work, we posit that order-free models have better transferability than order-sensitive models because they less suffer from overfitting  language-specific word order features. To test our hypothesis, we first quantify language distance in terms of word order typology, and then systematically study the transferability of ordersensitive and order-free neural architectures on cross-lingual dependency parsing. We use dependency parsing as a test bed primarily because of the availability of unified annotations across a broad spectrum of languages (Nivre et al., 2018) . Besides, word order typology is found to influence dependency parsing (Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016; Aufrant et al., 2016) . Moreover, parsing is a low-level NLP task (Hashimoto et al., 2017) that can benefit many downstream applications (McClosky et al., 2011; Gamallo et al., 2012; Jie et al., 2017) . We conduct evaluations on 31 languages across a broad spectrum of language families, as shown in Table 1 . Our empirical results show that orderfree encoding and decoding models generally perform better than the order-sensitive ones for crosslingual transfer, especially when the source and target languages are distant. Quantifying Language Distance We first verify that we can measure \"language distance\" base on word order since it is a significant distinctive feature to differentiate languages (Dryer, 2007) . The World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013) provides a great reference for word order typology  and can be used to construct feature vectors for languages (Littell et al., 2017) . But since we already have the universal dependency annotations, we take an empirical way and directly extract word order features using directed dependency relations (Liu, 2010) . We conduct our study using the Universal Dependencies (UD) Treebanks (v2.2) (Nivre et al., 2018) . We select 31 languages for evaluation and analysis, with the selection criterion being that the total token number in the treebanks of that language is over 100K. We group these languages by their language families in Table 1 . Detailed statistical information of the selected languages and treebanks can be found in Appendix A 1 . We look at finer-grained dependency types than the 37 universal dependency labels 2 in UD v2 by augmenting the dependency labels with the universal part-of-speech (POS) tags of the head and modifier 3 nodes. Specifically, we use triples \"(ModifierPOS, HeadPOS, DependencyLabel)\" as the augmented dependency types. With this, we can investigate language differences in a finegrained way by defining directions on these triples (i.e. modifier before head or modifier after head). We conduct feature selection by filtering out rare types as they can be unstable. We defer the results in 52 selected types and more details to Appendix C. For each dependency type, we collect the statistics of directionality (Liu, 2010; Wang and Eisner, 2017) . Since there can be only two directions for an edge, for each dependency type, we use the relative frequency of the left-direction (modifier before head) as the directional feature. By concatenating the directional features of all selected triples, we obtain a word-ordering feature vector for each language. We calculate the wordordering distance using these vectors. In this work, we simply use Manhattan distance, which works well as shown in our analysis (Section 4.3). We perform hierarchical clustering based on the word-ordering vectors for the selected languages, following \u00d6stling (2015) . As shown in Figure 1 , the grouping of the ground truth language families is almost recovered. The two outliers, German (de) and Dutch (nl), are indeed different from English. For instance, German and Dutch adopt a larger portion of Object-Verb order in embedded clauses. The above analysis shows that word order is an important feature to characterize differences between languages. Therefore, it should be taken into consideration in the model design. Models Our primary goal is to conduct cross-lingual transfer of syntactic dependencies without providing any annotation in the target languages. The overall architecture of models that are studied in this research is described as follows. The first layer is an input embedding layer, for which we simply concatenate word and POS embeddings. The POS embeddings are trained from scratch, while the word embeddings are fixed and initialized with the multilingual embeddings by Smith et al. (2017) . These inputs are fed to the encoder to get contextual representations, which is further used by the decoder for predicting parse trees. For the cross-lingual transfer, we hypothesize that the models capturing less language-specific information of the source language will have better transferability. We focus on the word order information, and explore different encoders and decoders that are considered as order-sensitive and order-free, respectively. Contextual Encoders Considering the sequential nature of languages, RNN is a natural choice for the encoder. However, modeling sentences word by word in the sequence inevitably encodes word order information, which may be specific to the source language. To alleviate this problem, we adopt the self-attention based encoder (Vaswani et al., 2017) for cross-lingual parsing. It can be less sensitive to word order but not necessarily less potent at capturing contextual information, which makes it suitable for our study. RNNs Encoder Following prior work (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) , we employ k-layer bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) on top of the input vectors to obtain contextual representations. Since it explicitly depends on word order, we will refer it as an order-sensitive encoder. Self-Attention Encoder The original selfattention encoder (Transformer) takes absolute positional embeddings as inputs, which capture much order information. To mitigate this, we utilize relative position representations (Shaw et al., 2018) , with further simple modification to make it order-agnostic: the original relative position representations discriminate left and right contexts by adding signs to distances, while we discard the directional information. We directly base our descriptions on those in (Shaw et al., 2018) . For the relative positional selfattention encoder, each layer calculates multiple attention heads. In each head, the input sequence of vectors x = (x 1 , . . . , x n ) are transformed into the output sequence of vectors z = (z 1 , . . . , z n ), based on the self-attention mechanism: z i = n j=1 \u03b1 ij (x j W V + a V ij ) \u03b1 ij = exp e ij n k=1 exp e ik e ij = x i W Q (x j W K + a K ij ) T \u221a d z Here, a V ij and a K ij are relative positional representations for the two position i and j. Similarly, we clip the distance with a maximum threshold of k (which is empirically set to 10), but we do not discriminate positive and negative values. Instead, since we do not want the model to be aware of directional information, we use the absolute values of the position differences: a K ij = w K clip(|j\u2212i|,k) a V ij = w V clip(|j\u2212i|,k) clip(x, k) = min(|x|, k) Therefore, the learnable relative postion representations have k+1 types rather than 2k+1: we have w K = (w K 0 , . . . , w K k ), and w V = (w V 0 , . . . , w V k ). With this, the model knows only what words are surrounding but cannot tell the directions. Since self-attention encoder is less sensitive to word order, we refer to it as an order-free encoder. Structured Decoders With the contextual representations from the encoder, the decoder predicts the output tree structures. We also investigate two types of decoders with different sensitivity to ordering information. Stack-Pointer Decoder Recently, Ma et al. (2018) proposed a top-down transition-based decoder and obtained state-of-the-art results. Thus, we select it as our transition-based decoder. To be noted, in this Stack-Pointer decoder, RNN is utilized to record the decoding trajectory and also can be sensitive to word order. Therefore, we will refer to it as an order-sensitive decoder. Graph-based Decoder Graph-based decoders assume simple factorization and can search globally for the best structure. Recently, with a deep biaffine attentional scorer, Dozat and Manning (2017) obtained state-of-the-art results with simple first-order factorization (Eisner, 1996; Mc-Donald et al., 2005) . This method resembles the self-attention encoder and can be regarded as a self-attention output layer. Since it does not depend on ordering information, we refer to it as an order-free decoder. Experiments and Analysis In this section, we compare four architectures for cross-lingual transfer dependency parsing with a different combination of order-free and ordersensitive encoder and decoder. We conduct several detailed analyses showing the pros and cons of both types of models. Setup Settings In our main experiments 4 (those except Section 4.3.5), we take English as the source language and 30 other languages as target languages. We only use the source language for both training and hyper-parameter tuning. During testing, we directly apply the trained model to target languages with the inputs from target languages passed through pretrained multilingual embeddings that are projected into a common space as the source language. The projection is done by the offline transformation method (Smith et al., 2017) with pre-trained 300d monolingual embeddings from FastText (Bojanowski et al., 2017) . We freeze word embeddings since fine-tuning on them may disturb the multi-lingual alignments. We also adopt gold UPOS tags for the inputs. For other hyper-parameters, we adopted similar ones as in the Biaffine Graph Parser (Dozat and Manning, 2017) and the Stack-Pointer Parser (Ma et al., 2018) . Detailed hyper-parameter settings can be found in Appendix B. Throughout our experiments, we adopted the language-independent UD labels and a sentence length threshold of 140. The evaluation metrics are Unlabeled attachment score (UAS) and labeled attachment score (LAS) with punctuations excluded 5 . We trained our cross-lingual models five times with different initializations and reported average scores. Systems As described before, we have an order-free (Self-Attention) and an order-sensitive (BiLSTM-RNN) encoder, as well as an order-free (Biaffine Attention Graph-based) and an ordersensitive (Stack-Pointer) decoder. The combination gives us four different models, named in the format of \"Encoder\" plus \"Decoder\". For clarity, we also mark each model with their encoderdecoder order sensitivity characteristics. For example, \"SelfAtt-Graph (OF-OF)\" refers to the model with self-attention order-free encoder and graph-based order-free decoder. We benchmark our models with a baseline shift-reduce transitionbased parser, which gave previous state-of-theart results for single-source zero-resource crosslingual parsing (Guo et al., 2015) . Since they used older datasets, we re-trained the model on our datasets with their implementation 6 . We also list the supervised learning results using the \"RNN-Graph\" model on each language as a reference of the upper-line for cross-lingual parsing. Results The results on the test sets are shown in Table 2 . The languages are ordered by their order typology distance to English. In preliminary experiments, we found our lexicalized models performed poorly on Chinese (zh) and Japanese (ja). We found the main reason was that their embeddings were not well aligned to English. Therefore, we use delexicalized models, where only POS tags are used as inputs. The delexicalized results 7 for Chinese and Japanese are listed in the rows marked with \"*\". Overall, the \"SelfAtt-Graph\" model performs the best in over half of the languages and beats the runner-up \"RNN-Graph\" by around 1.3 in UAS and 1.2 in LAS on average. When compared with \"RNN-Stack\" and \"SelfAtt-Stack\", the average difference is larger than 1.5 points. This shows that models capture less word order infor- 7 We found delexicalized models to be better only at zh and ja, for about 5 and 10 points respectively. For other languages, they performed worse for about 2 to 5 points. We also tried models without POS, and found them worse for about 10 points on average. We leave further investigation of input representations to future work. mation generally perform better at cross-lingual parsing. Compared with the baseline, our superior results show the importance of the contextual encoder. Compared with the supervised models, the cross-lingual results are still lower by a large gap, indicating space for improvements. After taking a closer look, we find an interesting pattern in the results: while the model performances on the source language (English) are similar, RNN-based models perform better on languages that are closer to English (upper rows in the table), whereas for languages that are \"distant\" from English, the \"SelfAtt-Graph\" performs much better. Such patterns correspond well with our hypothesis, that is, the design of models considering word order information is crucial in cross-lingual transfer. We conduct more thorough analysis in the next subsection. Analysis We further analyze how different modeling choices influence cross-lingual transfer. Since we have not touched the training sets for languages other than English, in this subsection, we evaluate and analyze the performance of target languages using training splits in UD. Performance of English is evaluated on the test set. We verify that the trends observed in test set are similar to those on the training sets. As mentioned in the previous section, the bilingual embeddings for Chinese and Japanese do not align well with English. Therefore, we report the results with delexicalizing. In the following, we discuss our observations, and detailed results are listed in Appendix E. Encoder Architecture We assume models that are less sensitive to word order perform better when transfer to distant languages. To empirically verify this point, we conduct controlled comparisons on various encoders with the same graph-based decoder. Table 3 shows the average performances in all languages. To compare models with various degrees of sensitivity to word order, we include several variations of self-attention models. The \"SelfAtt-NoPosi\" is the self-attention model without any positional information. Although it is most insensitive to word order, it performs poorly possibly because of the lack of access to the locality of contexts. The self-attention model with absolute positional embeddings (\"SelfAtt-Absolute\") also does not perform well. In the case of parsing, relative positional representations may be more useful as indicated by the improvements brought by the directional relative position representations (\"SelfAtt-Relative+Dir\") (Shaw et al., 2018) . Interestingly, the RNN encoder ranks between \"SelfAtt-Relative+Dir\" and \"SelfAtt-Absolute\"; all these three encoders explicitly capture word order information in some way. Finally, by discarding the information of directions, our relative position representation (\"SelfAtt-Relative\") performs the best (significantly better at p < 0.05). Figure 2 : Evaluation score differences between Order-Free (OF) and Order Sensitive (OS) modules. We show results of both encoder (blue solid curve) and decoder (dashed red curve). Languages are sorted by their word-ordering distances to English from left to right. The position of English is marked with a green bar. One crucial observation we have is that the patterns of breakdown performances for \"SelfAtt-Relative+Dir\" are similar to those of RNN: on closer languages, the direction-aware model performs better, while on distant languages the non-directional one generally obtains better results. Since the only difference between our proposed \"SelfAtt-Relative\" model and the \"SelfAtt-Relative+Dir\" model is the directional encoding, we believe the better performances should credit to its effectiveness in capturing useful context information without depending too much on the language-specific order information. These results suggest that a model's sensitivity to word order indeed affects its cross-lingual transfer performances. In later sections, we stick to our \"SelfAtt-Relative\" variation of the self-attentive encoder and focus on the comparisons among the four main models. Performance v.s. Language Distance We posit that order-free models can do better than order-sensitive ones on cross-lingual transfer parsing when the target languages have different word orders to the source language. Now we can analyze this with the word-ordering distance. For each target language, we collect two types of distances when comparing it to English: one is the word-ordering distance as described in Section 2, the other is the performance distance, which is the gap of evaluation scores 8 between the target language and English. The performance distance can represent the general transferability from En- Figure 3 : Analysis on specific dependency types. To save space, we merge the curves of encoders and decoders into one figure. The blue and red curves and left y-axis represent the differences in evaluation scores, the brown curve and right y-axis represents the relative frequency of left-direction (modifier before head) on this type. The languages (x-axis) are sorted by this relative frequency from high to low. glish to this language. We calculate the correlation of these two distances on all the concerned languages, and the results turn to be quite high: the Pearson and Spearman correlations are around 0.90 and 0.87 respectively, using the evaluations of any of our four cross-lingual transfer models. This suggests that word order can be an important factor of cross-lingual transferability. Furthermore, we individually analyze the encoders and decoders of the dependency parsers. Since we have two architectures for each of the modules, when examining one, we take the highest scores obtained by any of the other modules. For example, when comparing RNN and Self-Attention encoders, we take the best evaluation scores of \"RNN-Graph\" and \"RNN-Stack\" for RNN and the best of \"SelfAtt-Graph\" and \"SelfAtt-Stack\" for Self-Attention. Figure 2 shows the score differences of encoding and decoding architectures against the languages' distances to English. For both the encoding and decoding module, we observe a similar overall pattern: the order-free models, in general, perform better than order-sensitive ones in the languages that are distant from the source language English. On the other hand, for some languages that are closer to English, order-sensitive models perform better, possibly benefiting from being able to capture similar word ordering information. The performance gap between order-free and order-sensitive models are positively correlated with language distance. Performance Breakdown by Types Moreover, we compare the results on specific dependency types using concrete examples. For each type, we sort the languages by their relative frequencies of left-direction (modifier before head) and plot the performance differences for encoders and decoders. We highlight the source language English in green. Figure 3 shows four typical example types: Adposition and Noun, Adjective and Noun, Auxiliary and Verb, and Object and Verb. In Figure 3a , we examine the \"case\" dependency type between adpositions and nouns. The pattern is similar to the overall pattern. For languages that mainly use prepositions as in English, different models perform similarly, while for languages that use postpositions, order-free models get better results. The patterns of adjective modifier (Figure 3b ) and auxiliary (Figure 3c ) are also similar. On dependencies between verbs and object nouns, although in general order-free models perform better, the pattern diverges from what we expect. There can be several possible explanations for this. Firstly, the tokens which are noun objects of verbs only take about 3.1% on average over all tokens. Considering just this specific dependency type, the correlation between frequency distances and performance differences is 0.64, which is far less than 0.9 when considering all types. Therefore, although Verb-Object ordering is a typical example, we cannot take it as the whole story of word order. Secondly, Verb-Object dependencies can often be difficult to decide. They sometimes are long-ranged and have complex interactions with other words. Therefore, merely reducing modeling order information can have complicated effects. Moreover, although our relativeposition self-attention encoder does not explicitly encode word positions, it may still capture some positional information with relative distances. For example, the words in the middle of a sentence will have different distance patterns from those at the beginning or the end. With this knowledge, the model can still prefer the pattern where a verb is in the middle as in English's Subject-Verb-Object ordering and may find sentences in Subject-Object-Verb languages strange. It will be interesting to explore more ways to weaken or remove this bias. Analysis on Dependency Distances We now look into dependency lengths and directions. Here, we combine dependency length and direction into dependency distance d, by using negative signs for dependencies with leftdirection (modifier before head) and positive for right-direction (head before modifier). We find a seemingly strange pattern at dependency distances |d|=1: for all transfer models, evaluation scores on d=-1 can reach about 80, but on d=1, the scores are only around 40. This may be explained by the relative frequencies of dependency distances as shown in Table 4 , where there is a discrepancy between English and the average of other languages at d=1. About 80% of the dependencies with |d|=1 in English is the left direction (modifier before head), while overall other languages have more right directions at |d|=1. This suggests an interesting future direction of training on more source languages with different dependency distance distributions. We further compare the four models on the d=1 dependencies and as shown in Figure 4 , the familiar pattern appears again. The order-free models perform better at the languages which have more d=1 dependencies. Such finding indicates that our model design of reducing the ability to capture word order information can help on shortranged dependencies of different directions to the source language. However, the improvements are still limited. One of the most challenging parts of unsupervised cross-lingual parsing is modeling cross-lingually shareable and language-unspecific information. In other words, we want flexible yet powerful models. Our exploration of the orderfree self-attentive models is the first step. Transfer between All Language Pairs Finally, we investigate the transfer performance of all source-target language pairs. 9 We first generate a performance matrix A, where each entry (i, j) records the transfer performance from a source language i to a target language j. We then report the following two aggregate perfor-mance measures on A in Figure 5 : 1) As-source reports the average over columns of A for each row of the source language and 2) As-target reports the average over rows of A for each column of the target language. As a reference, we also plot the average word-order distance between one language to other languages. Results show that both As-source (blue line) and As-target (red line) highly are anti-correlated (Pearson correlation coefficients are \u22120.90 and \u22120.87, respectively) with average language distance (brown line). Related Work Cross-language transfer learning employing deep neural networks has widely been studied in the areas of natural language processing (Ma and Xia, 2014; Guo et al., 2015; Kim et al., 2017; Kann et al., 2017; Cotterell and Duh, 2017) , speech recognition (Xu et al., 2014; Huang et al., 2013) , and information retrieval (Vuli\u0107 and Moens, 2015; Sasaki et al., 2018; Litschko et al., 2018) . Learning the language structure (e.g., morphology, syntax) and transferring knowledge from the source language to the target language is the main underneath challenge, and has been thoroughly investigated for a wide variety of NLP applications, including sequence tagging (Yang et al., 2016; Buys and Botha, 2016) , name entity recognition (Xie et al., 2018) , dependency parsing (Tiedemann, 2015; Agi\u0107 et al., 2014) , entity coreference resolution and linking (Kundu et al., 2018; Sil et al., 2018) , sentiment classification (Zhou et al., 2015 (Zhou et al., , 2016b)) , and question answering (Joty et al., 2017) . Existing work on unsupervised cross-lingual dependency parsing, in general, trains a dependency parser on the source language and then directly run on the target languages. Training of the monolingual parsers are often delexicalized, i.e., removing all lexical features from the source treebank (Zeman and Resnik, 2008; Mc-Donald et al., 2013) , and the underlying feature model is selected from a shared part-of-speech (POS) representation utilizing the Universal POS Tagset (Petrov et al., 2012) . Another pool of prior work improves the delexicalized approaches by adapting the model to fit the target languages better. Cross-lingual approaches that facilitate the usage of lexical features includes choosing the source language data points suitable for the target language (S\u00f8gaard, 2011; T\u00e4ckstr\u00f6m et al., 2013) , transferring from multiple sources (Mc-Donald et al., 2011; Guo et al., 2016; T\u00e4ckstr\u00f6m et al., 2013) , using cross-lingual word clusters (T\u00e4ckstr\u00f6m et al., 2012) and lexicon mapping (Xiao and Guo, 2014; Guo et al., 2015) . In this paper, we consider single-source transfer-train a parser on a single source language, and evaluate it on the target languages to test the transferability of neural architectures. Multilingual transfer (Ammar et al., 2016; Naseem et al., 2012; Zhang and Barzilay, 2015) is another broad category of techniques applied to parsing where knowledge from many languages having a common linguistic typology is utilized. Recent works (Aufrant et al., 2016; Wang and Eisner, 2018a,b) demonstrated the significance of explicitly extracting and modeling linguistic properties of the target languages to improve crosslingual dependency parsing. Our work is different in that we focus on the neural architectures and explore their influences on cross-lingual transfer. Conclusion In this work, we conduct a comprehensive study on how the design of neural architectures affects cross-lingual transfer learning. We examine two notable families of neural architectures (sequential RNN v.s. self-attention) using dependency parsing as the evaluation task. We show that order-free models perform better than order-sensitive ones when there is a significant difference in the word order typology between the target and source language. In the future, we plan to explore multisource transfer and incorporating prior linguistic knowledge into the models for better cross-lingual transfer. Acknowledgments We thank anonymous reviewers for their helpful feedback. We thank Robert \u00d6stling for reaching out when he saw the earlier arxiv version of the paper and providing insightful comments about word order and related citations. We are grateful for the Stanford NLP group's comments and feedback when we present the preliminary results in their seminar. We thank Graham Neubig and the MT/Multilingual Reading Group at CMU-LTI for helpful discussions. We also thank USC Plus Lab and UCLA-NLP group for discussion and comments. This work was supported in part by National Science Foundation Grant IIS-1760523.",
    "abstract": "Different languages might have different word orders. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "88",
    "year": "2019",
    "month": "June",
    "title": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing"
}