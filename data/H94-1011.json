{
    "article": "This paper reports results obtained in benchmark tests conducted within the ARPA Spoken Language program in November and December of 1993. In addition to ARPA contractors, participants included a number of %olunteers\", including foreign participants from Canada, France, Germany, and the United Kingdom. The body of the paper is limited to an outline of the structure of the tests and presents highlights and discussion of selected results. Detailed tabulations of reported \"official\" results, and additional explanatory text appears in the Appendix. WSJ-CSR TESTS New Conditions All sites participating in the WSJ-CSR tests were required to submit results for (at least) one of two \"Hub\" tests. The Hub tests were intended to measure basic speaker-independent performance on either a 64K-word (Hub 1) or 5K-word (Hub 2) read-speech test set, and included required use of either a \"standard\" 20K trigram (Hub 1) or 5K bigram (Hub 2) grammar, and also required use of standard training sets. These requirements were intended to facilitate meaningful cross-site comparisons. The \"Spoke\" tests were intended to support a number of different ehaUenges. Spokes 1, 3 and 4 supported problems in various types of adaptation: incremental supervised language model adaptation (Spoke 1), rapid enrollment speaker adaptation for \"recognition outliers\" (i.e., non-native speakers) (Spoke 3), incremental speaker adaptation (Spoke 4). [There were no participants in what had been planned as Spoke 2.] Spokes 5 through 8 supported problems in noise and channel compensation: unsupervised channel compensation (Spoke 5), \"known microphone\" adaptation for two different microphones (Spoke 6), unsupervised channel compensation for 2 different environments (Spoke 7), and use of a noise compensation algorithm with a known alternate microphone for data collected in environments when there is competing \"calibrated\" noise (radio talk shows or music) (Spoke 8). Spoke 9 included spontaneous \"dictation-style\" speech. Additional details are found in Kubala, et al. [1], on behalf of members of the ARPA Continuous speech recognition Corpus Coordinating Committee (CCCC). WSJ-CSR Summary Highlights The design of the \"Hub and Spoke\" test paradigm, was such that opportunities abounded for informative contrasts (e.g., the use of bigram vs. trigram grammars, the enablement/disablement of supervised vs. unsupervised adaptation strategies, ete). There were nine participating sites in the Hub I tests and five sites participating in the Hub 2 tests, and some sites reported results for more than one system or research team. The lowest word error rate in the Hub 1 baseline condition was achieved by the French CNRS-LIMSI group [2, 3] . Application of statistical significance tests indicated that the performance differences between this system and a system INTRODUCTION Benchmark tests were implemented within the ARPA Human Language Technology research program during the period November 1993 -January 1994. As in tests conducted last year, the large-vocabulary continuous speech recognition technology tests made use of Wall Street Journal-based Continuous Speech Recognition (WSJ-CSR) corpus material which was collected at SRI International (SRI) under contract to the Linguistic Data Consortium (LDC). Spoken language understanding technology tests made use of ARPA Air Travel Information System (ATIS) material collected at several sites, processed at NIST, annotated at SRI, and provided to participating members of the LDC. developed by Cambridge University Engineering Department using the \"HMM Toolkit\" approach [4] [5] [6] , were not significant. The Cambridge University HMM Toolkit approach also yielded excellent results for the smallervocabulary Hub 2 tests. The lowest word error rate for an ARPA contractor on the Hub 1 test data, for the C1 condition permitting valid cross-site comparisons, was reported by the group at CMU [7] [8] [9] . The CMU results were not significantly different from the corresponding results for the Cambridge University HMM Toolkit system. The lowest word e:rror rate for an ARPA contractor for the (less constrained) P0 condition was reported by the group at BBN. R is difficult to summarize results of the spoke tests, except to note that there were results reported for 8 different \"spoke conditions\", with from 1 to 3 participants and systems typically involved in each spoke. Details are presented in the Appendix. WSJ-CSR Discussion In NIST's analyses of the results, displays of the range of reported word error rates for each speaker across all systems are sometimes informative. These displays tend to draw attention to particularly problematic speakers or systems. Figure 1 shows data for the 10 speakers and 11 systems participating in the required Hub 1 C1 test. The speakers have been ordered from low error rate at the top of the figure to high error rate at the bottom. The length of the plotted line indicates the range in word error rate reported over all systems, and the one.standard-deviation points about the mean are indicated with a \"+\" symbol. Note that three speakers (40h, 40j, and 40t) have unusually high error rates relative to the other seven in this test set. In previous tests involving the Resource Management Corpus, it was noted that high error rates seemed to be correlated, at least indirectly, with unusually fast or slow rate of speech. To see if this was the case for the present test data, NIST obtained estimates of the average speaking rate (words/minute) for each of the test speakers. These estimates were based solely on the total number of words uttered and the total duration of the waveform files, and more sophisticated measures would be desirable. Figure 2 shows a plot of the word error rate vs. speaking rate for the 10 speakers and 11 systems in the Hub 1 C1 test. This figure, like Figure 1 , indicates that speakers 40h, 40j and 40f not only have unusually high error rates relative to the other speakers in this test set, but it also indicates that for these speakers, the speaking rate is markedly higher than for the other seven. Whereas the speaking rate for the seven speakers ranges from approximately 115 to 145 words/minute, for the three speakers with high error rate, the speaking rate ranges from 165-175 words/minute. There are at least two factors that may contribute to higher error rates at these fast speaking rates: within-word and across-word eoarticulatory effects (e.g., phone deletions) associated with fast (possibly better described as \"careless\" or \"easuar') speech, and possible under-representation of these effects in the training material. Chase, et al. [9] , at CMU, noted that for the 4 speakers in Spoke 7 (40g, 40h, 40i, and 40j), two (40g and 40i) could be subjectively characterized as \"careful speaker[s]\", but that 40h was characterized as a \"pretty fast speaker, [with] very low gain\", and 40j as a \"very, very fast speaker\". These \"fast speakers\" appear in a number of the test sets. NIST's analyses of the distributions of rate of speech for two sets of training material for the Hub 1 test (each consisting of approximately 30,000 utterances: \"short-term\" and \"longterm\" speakers) indicate that the distributions are rather broad, with the short-term speakers' distribution peaking at 130 words/minute, with a standard deviation of 30 words/minute, and the long-term speakers' distribution peaking at 145 words/minute, with an associated standard deviation of 30 words/minute. Note that speaking rates for the 3 \"fast-talking\" speakers fall just outside the \"plus one standard deviation region\" range relative to the peak of the distribution for the \"short-term speaker\" training set, and just inside the corresponding region relative to the \"long-term\" training set. Because a number of the measured performance differences between systems were small, and the results of the pairedcomparison significance tests validated the relevant null hypotheses, it has been observed that, in general, the use of larger test sets, especially for the Hub tests, would have been more informative, especially with regard to the results of significance tests requiring larger speaker populations (i.e., the Sign and Wileoxon Signed-Rank tests). With larger populations of test speakers, it would be less likely to have such disproportionately large representation of\"fast speakers\" in the test sets. Two spokes made use of microphones other than the \"standard\" Sennheiser close-talking microphone. (See, for example, the discussion in the Appendix of this paper for Spokes 5 and 6.) Too other spokes dealt with the issue of performance degradations that were presumably due to degradations in the signal-to-noise ratio. (See, for example, the discussion for Spokes 7 and 8.) For the test data of Spokes 5-7, subsequent to the completion of the tests, NIST performed signal-to-noise ratio (SNR) analyses, using three different bandwidth (signal preprocessing) conditions: broadband, A-weighted, and 300 Hz-3000 kHz passband \"telephone bandwidth\". The filtered SNR's are generally higher than the broadband values. Figure 3 shows the results of these SNR analyses. Figure 3 (a) indicates, the SNRs measured for the data of Spoke 5, which includes 10 \"unknown\" microphones in addition to the simultaneously collected reference Sennheiser dose-talking microphone data for each data subset, collected in the normal data collection environment. SRrs \"normal offices for recording\" speech data have A-weighted sound level values in the 46.-48 dB range, There were 2 \"tieelip\" or lapel microphones, 5 stand-mounted microphones, a surfaceeffect microphone, a speakerphone, and a cordless telephone in this set of 10 test microphones. Note that the SNR values for the Sennheiser microphone are typically about 45 dB for the both the broadband and Aweighted conditions, indicating that there is little lowfrequency energy in the spectrum of the noise in the Sennheiser microphone data. Sennheiser microphone data typically yield values of 50 dB for the telephone-bandwidth condition. For the alternate microphones, the broadband SNR's range from about 23 dB (for the Audio-Teehnica stand-mounted microphone) to 45 dB (for the GE cordless telephone). With filtration the SNR's are higher, as expected. Note that nearly all of the microphones provide at least a 30 dB telephone-bandwidth SNR, and that the AT Pro 7a lapel-mounted microphone provides approximately 40 dB. The test set data for Spoke 7, shown in Figure 3 (c), involved use of two different microphones (an Audio-Teehniea standmounted microphone and a telephone handset in addition to the usual \"reference\" Sennheiser dose-talking microphone), in two different noise environments, with background Aweighted noise levels of 58-68 dB. In the quieter of the two \"noisy\" environments, a computer laboratory with a reported A-weighted sound level in the 58-59 dB range, the broadband SNR was approximately 34-36 dB for the Sennheiser microphone, and 35 dB for the telephone handset data, but only 17 dB for the Audio-Techniea microphone. Spectral analyses of the Audio-Teehniea background noise data demonstrate the presence of significant low frequency energy as well as the presence of harmonic components with an approximately 70 Hz fundamental. These components may have originated in some rotating machinery (e.g., a cooling fan or disc drive). In the noisier environment, a room containing machinery with conveyor belts for sorting packages, with a reported A-weighted sound level in the 62-68 dB range, the broadband SNR ratio for the Sennheiser data degraded to 27-29 dB (a decrease of approximately 7 dB), and 27 dB for the telephone handset data, and the Audio-Techniea to 16 dB (a decrease of only 1 dB). With A-weighting, in the quieter environment, the SNR for the Sennheiser improved very slightly (less than 1 dB, relative to the broad band values), and for the Audio-Techniea it was 25 dB, 8 dB higher than the broad band value. In the noisier environment, the A-weighted S/N ratio for the Sennheiser data was approximately 29 dB, and the Audio-Techniea 20 dB. For the telephone handset data, both the telephonebandwidth-filtered and the A-weighted SNRs were higher than, but typically within one or two dBs, of the unweighted values, as might be expected. In summary, for the quieter of the two environments used in collecting the data of Spoke 7, none of the data subsets in Spoke 7 had an average filtered SNR worse than about 25 dB, and in the noisier environment, the worst average filtered SNR for any data subset was approximately 20 dB. These SNR values would not ordinarily be regarded as indicative of severe noise-degradation. Spoke 8 involved data collected in the presence of competing noise --music and talk radio broadcasts. For the case of competing music, the broadband SNR for the reference Sennheiser microphone ranged from 44 DB for the so-called \"20 dB\" condition, to 36 dB for the \"10 dB\" condition, and 29 dB for the \"0 dB\" condition. For the Audio-Technica microphone, corresponding measured valueswere 25, 17, and 11 dB. NISTs measurements of SNR for the data containing competing speech were inconclusive because of the difficulty of distinguishing between the spoken test material and the competing talk radio. ATIS TESTS New Conditions Recent ATIS tests were similar in many respects to previous ATIS tests --the primary difference consisting of expansion of the size of the relational air-travel-information database to 46 cities, and use of a body of newly collected and annotated data using this relational database [I0]. As in prior years, tests included spontaneous speech recognition (SPREC) tests, natural language understanding (NL) tests and spoken language understanding (SIS) tests. For the first time, data collected at NIST was included in the test and training data. The NIST data was collected using systems provided to NIST by BBN and SRL In previous years, results for NL and SLS tests were presented and discussed in terms of a \"weighted error\" percentage, which was computed as twice the percentage of incorrect answers plus the percentage of \"No Answer\" responses. The decision to weight 'kvrong answers\" twice as heavily as \"no answer\" responses was reconsidered within the past year by the ARPA Program Manager, and this year only unweighted NL and SLS errors are reported (i.e., incorrect answers count the same as \"No Answer n responses). For most system developers, this change of policy has appeared to result in changed strategies for system responses, so that in this year's reported results, little use was made of the \"No Answer\" response. Summary Highlights For the recent ATIS tests, results were reported for systems at seven sites. Lowest error rates were reported by the group at CMU [11] . The magnitude of the differences between systems is frequently small, and the significance of these small differences is not known. As in previous years, error rates for \"volunteers n are generally higher than for ARPA eontraetors, possibly reflecting a lesser level-of-effort. Additional details about the test paradigm, and comments on some aspeets by individual partieipants, are found in another paper in this Proceedings, by Dahl, et al., on behalf of members of the ARPA Multi-site ATIS Data COllection Working (MADCOW) Group [10] . Details about the technical approaches used by the partieipants, and their own analyses and comments, are to be found in references [11, [23] [24] [25] [26] [27] [28] . ATIS Discussion This year, 46% of the utterances were classified as Class A and 34% in Class D, so that 80% of the test utterances were \"answerable\" (i.e., Class A or D). Last year's test set had about the same percentage of Class A queries (43%), but somewhat fewer classified as Class D (i.e., 25%), so that last year only 67% were answerable. One possible reason for this change (other than the test-set-to-test-set fluctuations) may be that the Principles of Interpretation document is continually being extended to cover phenomena that would have otherwise resulted in eategorization of some queries as \"unanswerable\", and therefore Class X. For text input (NL test), for last year's test material, the lowest unweighted NL error rate was 6.5% for the Class A+D subset, 6.5% for Class A, and 6.4% for Class D, in contrast with this year's corresponding figures of 9.3%, 6.0% and 13.8%. Note that this year's test set apparently had \"more diffieult\" Class D queries, and that there was a larger fraction of the queries that were classified as Class D than last year (34% vs. 25%). For speech input (SLS test), and for last year's unweighted test material, the unweighted SLS error rate was 11.0% for the Class A+D subset, 10.2% for Class A, and 12.5% for Class D, in contrast with this year's corresponding figures of 13.2%, 8.9% and 17.5%. Note that while the lowest error rate for Class A queries is smaller this year (i.e., 8.9% vs. 10.2%), this year's best Class D error rate was substantially higher than last year's. It may be the ease that this is related to the extended coverage provided by the current Principles of Interpretation document, so that queries that in previous years would have been classified as unanswerable, are now judged to be answerable, although context-dependent. ACKNOWLEDGEMENTS The \"Hub and Spokes\" Test paradigm could not have been developed, specified, or implemented without the tireless and effective efforts of Francis Kubala, as Chair of the ARPA continuous speech recognition Corpus Collection Coordinating Committee (CCCC). The tests would also not have been possible without the dedicated efforts of Denise Danielson and her colleagues at SRI in collecting an exceptionally large and varied amount of CSR data for CCSR system training and test purposes. In the ATIS community, Debbie Dahl served as Chair of the MADCOW group, and it is to her credit that new data was collected at several sites with the 46 eity relational database and that participating sites reached agreement on the details of the current tests. Kate Hunicke-Smith and her colleagues at SRI International were again responsible for annotation of ATIS data and for assisting NIST in the adjudication process following preliminary scoring. It is a pleasure to acknowledge Kate's thoughtful and cheerful interactions with our group at NIST. As in previous years, the cooperation of many participants in the ARPA data and test infrastructure --typically several individuals at each site --is gratefully acknowledged. [21] (a) Rosenfeld, R., \"A Hybrid Approach to Adaptive Statistica.l Language Modelling\", in Proceedings of the Human Language Technology Workshop, March 1994 (Weinstein, C..I., ed.), and (b) Chase, L, Mosur, R., and Rosenfeld, R., \"Language Model Adaptation in the CSR Evaluaticm\", oral Presentation at the Spoken Language Technology Workshop, March 6-8, 1994, Princeton, NJ. [22] (a) IJu, EH., Moreno, P.J., Stem, R.M., and Aeero, A., \"Signal Processing for Robust Speech Recognition\", in Proeeedi:ngs of the Human Language Technology Workshop, March 1994 (Weinstein, C.J., ed.) and (b) Stem, R.M., Liu, F,H., arid Moreno, P., \"Robust Speech Recognition: Research at CMU\", Oral Presentation at the Spoken Language Technology Workshop, March 6-8,1994, Princeton, NJ. [23] Boechieri, E., \"The ATT ATIS System: March 94 Report\", Oral Presentation at the Spoken Language Technology Workshop, March 6-8, 1994, Princeton, NJ. [24] (a) Stallard, D., et al., \"Recent Work in Spoken Language Understanding in the BBN SLS Project\", and (b) Miller, S., et al., \"Statistical Language Processing Using Hidden Understanding Models\", Oral Presentations at the Spoken Language Technology Workshop, March 6-8, 1994, Princeton, NJ. [25] Normandin, Y., \"CRIM's December 1983 ATIS System\", Oral Presentation at the Spoken Language Technology Workshop, March 6-8, 1994, Princeton, NJ. [26] \"The MIT ATIS System: March 1994 Progress Report\", Oral Presentation at the Spoken language Technology Workshop, March 6-8, 1994, Princeton, NJ. [27] Moore, R. and Cohen, M. et al., \"SRI's Recent Progress on the ATIS Task\", Oral Presentation at the Spoken Language Technology Workshop, March 6-8,1994, Princeton, NJ. [28] Dahl, D., Linebarger, M., Nguyen, N. and Norton, L, \"Unisys Acth,Sties in Spoken Language Understanding\", Oral Presentation at the Spoken Language Technology Workshop, March 6-8, [29] Digilakis, V., et al., \"SRI November 1993 CSR Hub evaluation\", Oral Presentation at the Spoken Language Technology Workshop, March 6.-8, 1994, Princeton, NJ. [30] Weintraub, M., et. al., \"SRI November 1993 CSR Spoke Evaluation', Oral Presentation at the Spoken Language Technology Workshop, March 6-8, 1994, Princeton, NJ. NOTICE Throughout this paper, a number of references are provided in order to refer readers to relevant papers and oral presentations by researchers at the indMdual sites participating in the tests. In some of these papers, results are cited that differ by small amounts from those tabulated in this paper. In some cases the authors cite unofficial or preliminary, \"pre-adjudieation\" results. In other eases, the authors cite other unofficial test results conducted after the \"official\" test period dosed. The views expressed in this paper are those of the author(s). The results presented are for local, system-developerimplemented tests. NIST's role in the tests is one of selecting and distributing the test materials, implementing scoring software, and uniformly tabulating the results of the tests. The views of the author(s) and these results are not to be construed or represented as endorsements of any systems or official findings on the part of NIST, ARPA or the U.S. Government. APPENDIX: \"BENCHMARK TEST RESULTS\" A.1. WSJ-CSR November 1993 Test Material The 1993 WSJ-CSR tests make use of newly-collected training material, a new compressed waveform file format, new test paradigms, and new test sets. The new training material for the WSJ-CSR task includes a substantial amount of data (31 CD-ROMs containing training and developmental test data) collected at SRI International under contract to the Linguistic Data Consortium (LDC). In a collaborative effort involving NIST, Tony Robinson at Cambridge University's Engineering Department, and the LDC, the newly collected waveform data was processed with an \"embedded\" version (i.e., the file's SPHERE-format header is uncompressed, but the bulk of the file is compressed) of a lossless waveform compression algorithm (\"shorten\") using the NIST SPHERE file header convention, to reduce the storage requirements for this data by a factor of approximately 50% [12] . The CSR test material was released in November. A.2. WSJ-CSR Test Scoring and Adjudication The CSR tests were conducted in November and December. Test and scoring protocols were similar to last year. However, new to the CSR benchmark tests this year was the addition of an official adjudication period. Following a preliminary scoring of recognition results, sites participating in the tests were permitted to submit requests for adjudication to NIST. Adjudication requests in the CSR domain contained requests for transcription modifications due to transcription errors, alternative transcriptions, etc. A total of 22 bug reports were received from 6 sites. The bug reports contained requests for changes to 199 (151 unique) utterance transcriptions in all WSJ-CSR test sets. The NIST adjudicators carefully evaluated each request and ultimately revised transcriptions of 83 utterances (55% of the ones in question.) Of the transcriptions that were revised, most were the result of judgements by the adjudicators that the transcriptions contained words which could have multiple orthographic representations (e.g., compound words, variant orthographic representations, etc.) or which were lexically ambiguous. In many of these eases, both the original transcription and an alternative transcription were permitted. This was implemented by mapping alternate word forms to a single form in both the transcriptions and the recognized strings. The remaining revisions were the correction of simple transcription errors. A.3. WSJ-CSR Test Participants United States participants in the WSJ-CSR tests included: BBN Systems and Technologies (BBN) [13], Boston University (BU) [14] , Carnegie Mellon University (CMU) [7] [8] [9] , Dragon Systems [15], the International Computer Science Institute (ICSI) at Berkeley [16] , Massachusetts Institute of Teehnology's Lincoln Laboratory (MIT/LL) [17], and SRI International (SRI) [29, 30] . Foreign participants included two British groups at Cambridge University's Engineering Department, one pursuing eonnectionist approaches (CU-CON) [18], and another, developers of the HMM Toolkit (CU-HTK) [4] [5] [6] , a French group at CNRS-LIMSI (LIMSI) [2, 3] , and a German group at the Philips GmbH Research Laboratories in Aachen [20] . BU collaborated with BBN, making use of the N-best outputs of a BBN system, using an N-best reseoring formalism, a stochastic segment modelling approach, and the use of both BU and BBN knowledge sources. A.4. WSJ-CSR Benchmark Test Results AA.1. Hub 1: 64K Baseline. The intention of the two \"Hub\" tests was \"to improve basic [speaker independent] performance on clean [read speech] data\". For Hub 1, test data consisted of 200 utterances --20 from each of 10 speakers, using the primary (Sennheiser series HMD 410) microphone as used in prior tests. All sites were required to provide results for a static (i.e., non-adaptive) Speaker-Independent (SI) baseline system that would permit cross-site comparisons, which would use the standard 20K word trigram \"open vocabulary\" grammar and use standardized training sets. The results of that baseline system are tabulated in the column labelled \"Contrast CI\" in Table 1 . Results for (optional) use of the same system training, but with the 20K bigram grammar, are shown in the column labelled \"Contrast C2\". These 'eontrastive' results were intended for comparison with results for optional 'primary' systems. The priraary systems could use \"any grammar or acoustic training\", and these results are shown in the column labelled \"P0\". In most cases, data from each site shows on a single line. The three BU \"C1\" systems each represent different N-best rescofing formalisms using the BU stochastic segment model recognition system in combination with the BBN Byblos system, using different knowledge sources to re-rank the Nbest hypotheses. The two different CMU systems are different in many ways, so that comparisons are non-trivial. For the baseline \"el\" systems, word error rates ranged from 19.0% to 11.7%, with the lowest error rate reported for the LIMSI System. In this table, and others of this sort in this paper, the results of contrastive comparisons are shown in the boxes labelled \"COMPARISONS AND SIGNIFICANCE TESTS\". The results of use of the NIST statistical significance tests that have been used in previous tests are also shown. To illustrate interpretation of some of the tabulated results, note that BBN and MIT/LL achieved reductions in error rate of 13.9% and 9.8%, respectively, for their P0 systems when compared to the C1 baseline systems. In most cases, these reductions were shown to be significant. Refer to [13] and [17] for discussion of factors contributing to these reduction error rate. When contrasting use of trigram and bigram grammars, a number of sites achieved reductions in error rate of from approximately 12% to 23% for the ease of use of the trigram grammar. Table 2 shows a matrix tabulation of the results of cross-site and, in some eases, within-site, paired comparison statistical significance tests for the baseline H1-C1 systems. A.4.2. Hub 2: 5K Baseline. Because run times for full 20K systems were in some cases regarded as prohibitive, a second baseline Hub test, requiring only a 5K lexicon, was permitted. For Hub 2, the required static SI baseline C1 system made use of a standard 5K bigram dosed vocabulary grammar and either of two smaller training sets, consisting of approximately 7200 sentence utterances. As for Hub 1, the Hub 2 test data consisted of 200 utterances --20 from each of 10 speakers, using the primary microphone. Not surprisingly, error rates for the 5K systems were lower than for the 20K systems. Table 3 shows that for the baseline C1 systems, error rates ranged from 17.7% to 8.7%, with the lowest error rate reported by the Cambridge University's HTK research group [4] [5] [6] . For the P0 systems, for which \"any grammar or acoustic training\" were permissible, lower error rates were to be expected, and were achieved, typically with reductions in error rate of from 25% to almost 50%. In this case, also, one of the HTK configurations achieved the lowest word error rate: 4.9%. Table 4 shows a matrix tabulation of the results of cross-site and, in some eases, within-site, paired comparison statistical significance tests for the baseline H2-C1 systems. A.4.3. Spoke 1: Language Model Adaptation. The stated goal for this language model adaptation spoke was \"to evaluate an incremental supervised language model (LM) adaptation algorithm on a problem of sublanguage adaptation\". The sole participant was Rosenfeld et al. at CMU [21] . Test data consisted of read speech data from four speakers, each reading 1 to 5 articles consisting of approximately 20-25 sentence utterances, with the Sennheiser microphone. NIST's scoring was done on four successive 5sentence utterance blocks throughout the articles (i.e., utterances 1-5, 6-10, 11-15, and 16+). Use of the statistical significance tests was not thought to be appropriate since these tests assume independence of errors across sentences, and this assumption is probably not valid when using an adaptive language model. Table 5 presents the results for Spoke 1. The column labelled P0 shows results with ineremental unsupervised adaptation enabled: word error rates vary from 16.5% on the first block of 5 sentences to 18.2% on the last block. In contrast, with language model adaptation disabled, the word error rates correspondingly vary from 20.5% to 21.1%. Comparisons between P0 and C1, ir~olving enabling/disabling of supervised LM adaptation, indicate reductions in word error rate of between 9.8% to 19.4%, with lesser reductions for the P0:C2 comparisons involving unsupervised LM adaptation. A.4A. Spoke 3: SI Recognition Outliers. 'Hae stated goal for this spoke was \"to evaluate a rapid enrollment speaker adaptation algorithm on difficult speakers (e.g., non-native speakers of American English)\". The sole participant was BBN [13] . Test data consisted of read speech from ten speakers, each reading 40 sentence utterances, with the Sennheiser microphone. For each speaker, the 40 \"rapid enrollment\" utterances were available for use with the \"rapid enrollment\" speaker adaptation. Table 6 presents the results for Spoke 3. The column labelled P0 shows results with rapid enrollment adaptation enabled: word error rate for the 400 utterance test set is 14.5%. In contrast, with adaptation disabled, the word error rate is 32.0%. Alternatively, the P0:C1 contrast indicates a reduction in error rate 54.7%, which was shown to be significant using all of the significance tests applied by NIST. A.4.5. Spoke 4: Incremental SpoakerAdaptatlon. The stated goal for this spoke was \"to evaluate an incremental speaker adaptation algorithm\". Two sites participated: Dragon [15] and MIT/LL [17] . In this spoke, there were only four test speakers, with 100 sentence utterances for each. NISTs scoring was done on four successive 25-sentence utterance blocks (i.e., utterances 1-25, 26-50, 51-75, and 76+). Table 7 presents the results for Spoke 4. For the Dragon results, word error rates for the P0 condition (with incremental unsupervised adaptation enabled) range from 15.5% to 14.3%. For MIT/LL, the corresponding variation is 10.9% to 11.1%. There is evidence of significant reductions in error of the order of 20% to 30% for the P0:C1 contrasts for the Dragon results (e.g., note the reduction of from 19.4% to 15.5% for the first block of 25 utterances). For the corresponding MIT/LL results, the magnitudes of the reductions are not as large. For both sites, the incremental changes in error rates between the P0 and C2 eases, involving unsupervised/supervised adaptation, in most eases are not shown to be significant, and range from approximately 4% to 16%. A.4.6. Spoke 5: \"Microphone Independence\". The stated goal of this spoke was to \"evaluate an unsupervised channel compensation algorithm\". The different \"channels\" in this ease were different microphones --each of the ten speakers in this test set used a different (unknown) microphone. Similar, but not identical, microphones had been incorporated in training and development material. For the 200 utterances in each portion of this test set, both the unknown microphone data (in \"wv2\" data files) and corresponding Sennheiser microphone data (in \"wvl\" files) were available. Both CMU [22] and SRI [30] participated in this spoke. With unsupervised channel compensation enabled, the CMU system achieved an error rate of 15.1%, in contrast to 20.9% with compensation disabled --a 27.8% reduction in word error rate. SRI achieved a comparable reduction of 24.2%, and with slightly lower error rates. With compensation enabled, the CMU system achieved 9.7% word error for the corresponding Sennheiser data, while the SRI system achieved 6.6% word error. Enabling/disabling the channel compensation made essentially no difference for the case of the Sennheiser data subset, as might be suspected. A.4.7. Spoke 6: Known Alternate Microphones. The stated goal of this spoke was to \"evaluate a known microphone adaptation algorithm\". There were two different microphones --an Audio Techniea stand-mounted microphone, and a telephone handset which was to be connected to the data collection apparatus \"over external lines\", in addition to the Sennheiser (wvl) data. Two-channel microphone adaptation data --for each of the two microphones and the (reference) Sennheiser microphone was provided fi'om \"devtest data\". There were ten speakers for the data for each of the two microphones, with 20 sentence utterances per speaker. In NIST's analysis of the results, data are separately tabulated for the Audio-Teehniea (at) data, and for the telephone handsets (th). Three sites participated: BBN [13], Dragon [15], and SRI [301. Table 9 presents the results for Spoke 6. For the case of the microphone adaptation disabled (C1), for the Audio-Technica microphone's data, word error rates were 6.4% for the SRI system, 10.4% for the BBN system, and 18.5% for the Dragon results. For telephone handset data, the SRI system had 19.1%, the BBN system had 29.3% and Dragon 65.4%. These results for the telephone handset data were probably somewhat worse than might have been expected because of inadvertent channel differences between development test and evaluation test sets. Considering the adaptation enabled/disabled P0:C1 contrast, BBN and Dragon achieved 9.4% and 11.7% reductions in word error rate for the Audio-Teehniea microphone, and 57.4% (from 29.3% to 12.5% word error) and 11.7% for BBN and Dragon, respectively. On corresponding Sennheiser data, the BBN and SRI systems with adaptation disabled achieved word error rates ranging from 5.9% to 8.4%, while the Dragon results were 13.8% and 14.6%. A.4.8. Spoke 7: \"Noisy Envlronments\". The stated goal of this spoke was to \"evaluate a noise compensation algorithm with known alternate microphones\" in two different datacollection environments with background A-weighted sound level of from 55 to 68 dB. Two different microphones were used, the same microphones as were used for Spoke 6, (the Audio-Teehniea and a telephone handset). Utterances for the microphone/channel adaptation (Sennheiser to known alternate microphone) were available from development test data, and there were files with background noise (but no speech) for each microphone-noise-environment-speaker condition. The two noise environments (\"el\" and \"e2\") consisted of computer laboratory (el), and a room with package sortation machinery in operation (\"e2\"). The sole participant in this spoke was SRI [30] . Table 10 presents the results for Spoke 7. As might be expected, the word error rate was smallest for the lower of the two noise conditions with the alternate highquality (but not close-talking) Audio-Technica microphone (8.5%) (for which the A-weighted S/N ratio was approximately 26 dB), and markedly higher for both alternate microphones in the higher noise environment (17.4% and 28.8%). For corresponding data from the close-talking Sennheiser microphone, in the two different noise environments, error rates of from 6.3% to 9.1% were obtained. A.4.9. Spoke 8: \"Calibrated Noise Sources\". The stated goal of this spoke was to \"evaluate a noise compensation algorithm with a known alternate microphone on data corrupted with calibrated noise sources\". Data was collected using the Audio-Technica microphone, which was also used in Spokes $6 and $7, in the presence of competing noise (from a \"boom box\" radio-tape player situated nearby). The competing noise was either a variety of musical selections (\"mu\") or talk radio (\"tr\"). The competing noise was \"calibrated\" in the sense that the level of the competing noise was intended to be set so as to be 20 or 10 dB less than the speech peak level, or equal to (or potentially greater than) the speech peak level, the \"0 dB condition\". Note however that NIST's measurements of SNR do not agree well with these desiderata, as discussed in Section 2.3 of this paper except ha some qualitative sense. CMU [212] was the sole participant in this spoke. Table 11 presents the results for Spoke 8. Data were submitted for the 3 competing noise conditions, both microphones (Sennheiser and Audio-Teehniea), and with noise compensation enabled and disabled --a total of 24 conditions, permitting many cross-comparisons. With compensation disabled, there were reductions in error rate with use of the close-talking, noise cancelling Sennheiser microphone when comparing results for the two different microphones (C3:C1). With compensation enabled, and again comparing the two different microphones (C3:P0), the differences in error rate are reduced, but are still significant in most cases. There is evidence of significant reductions in error rate when considering compensation enabled/disabled (P0:C1) for both music and talk radio at the 10 dB and 0 dB conditions. Further, enabling compensation appears to be beneficial for much of the data obtained with the close talking Sennheiser microphone (see, for example the C3:C2 comparisons). AA.10. Spoke 9: Spontaneous WSJ Dictation. The stated goal of this spoke was to \"improve basic performance on spontaneous dictation-style speech\". There were 10 speakers (all journalists, but with varying experience in dictation), each dictating 20 spontaneous Wall Street Journal-like sentence utterances, and using the Sermheiser microphone. BBN [13] was the sole participant in this spoke. Table 12 presents the results for Spoke 9. Using the same system as used for the C1 condition in Hub 1 (which achieved a word error rate of 14.2% on the Hub 1 test data), a word error rate of 24.7% was achieved on the $9 data, indicating that the spontaneous dictation $9 test set is substantially more challenging. BBN's $9 system achieved an error rate of 19.1% on the $9 data, a significant reduction in word error rate of 22.8% over the H1-C1 system. test set for number of subjects or the difficulty of scenarios per collection site. No \"pre-filtering\" of the test data was performed except to attempt to exclude subject-scenarios with mostly repetitive queries. The ATIS test material was released in November, 1993. A.6. ATIS Scoring and Adjudication The ATIS scoring and adjudication process took place in December and early January. ATIS test and scoring protocols were similar to those of previous benchmark tests. After the scored ATIS results were released in December 1993, approximately 140 adjudication requests (\"bug reports\") were sent to NIST. NISTworked in conjunction with SRI to resolve the requests, about 10 of which were duplicates. The majority of the bug reports dealt with transcription issues, in some cases pointing to limitations in our community's procedures for transcribing ATIS-domain spontaneous speech. One utterance, in particular, which was classified as Class X (and thus did not affect the NL or SLS scores), but was included in the ATIS SPREC scoring, included low-level remarks by the experimenter, as a result of an inadvertent \"open mike\" condition. Originally, this block of speech was transcribed as \"unintelligible\", but in adjudication, it was fully transcribed, partially because a number of sites had objected to having been scored with significant numbers of insertion errors. ARer adjudication, most sites continued to do very poorly on this one utterance, but were now penalized for substitutions and deletions as well. It alone accounts for an increment of approximately 0.3% in the Class A+D+X word error for most sites, and a substantially larger fraction of the Class X error rate. In retrospect, it is clear that this problematic utterance (and the entire subject-scenario) ought not to have been included in the test set because of the \"open mike\" condition. Besides the recurrent complaints of bad transcriptions, a problem involving fare IDs or flight IDs not appearing in the maximal reference answer fdes (the \"rf2s\") (which came to be known as \"Joe's Fare Bug\") was brought to our attention. This bug was attributed to about 21 of the test utterances before scoring. The bug was fred by SRI and new .rf2s were generated prior to rescoring. A.5. ATIS November 1993 Test Material The final, adjudicated set of test material consisted of 965 test utterances and was collected at 5 sites --BBN, CMU, MIT, NIST and SRI. As in previous years, it was selected by NIST staff from set-aside material previously collected within the MADCOW community [10] . The test set was selected so as to balance the number of utterances per data collection site (-200 utterances per site.) Because of differences in the scenarios and data collection systems used at the different collection sites, it was not possible to balance the AT&T collaborated with CMU, using an AT&T-developed ATIS..domain speech recognition system and the CMU ATIS natural language system, and Unisys collaborated with BBN, using a set of N-best outputs for a BBN ATIS-domain speech recognition system as input for Unisys-developed natural language technology. A.8. ATIS Benchmark Test Results A.8.1. SPontaneous speech RECognition (SPREC) Tests. Table 13 presents the results for the SPREC tests for all systems and subsets of the ATIS test data, using the Sennheiser close-talking microphone. For the case of the subset of all answerable queries, Class A+D, the word error rates ranged from 3.3% to 9.0%. Table 14 presents a matrix tabulation of the ATIS SPREC results for the Class A+D subset. The overall word error rate across all tested systems for the data from the several collecting sites (\"Overall Totals\" row along the bottom of the Table ) ranges from 3.6% for the CMU-eolleeted data to 6.8% for the NIST-eollected data, reflecting differences in subject populations and other factors. Table 15 presents the results, in matrix form, of the application of 4 paired-comparison significance tests for the SPREC systems for the Class A+D subset. Among other things, note that the performance differences between the BBN and the CMU systems are not shown to be significant, and that the differences between the MIT, SRI and one of the Unisys systems are also not shown to be significant. Note also that significant differences are shown between the BBN results and those for the two Unisys systems, which make use of BBN-provided N-best results. A.8.2. Natural Language (NL) Understanding Tests. Table 16 presents a tabulation of the results for the NL tests for all systems and all sets of \"answerable\" ATIS queries, Class A+D, Class A and Class D. For the set of all answerable queries, Class A+D, the unweighted error rate (\"UW. Err.\") ranges from 43.1% to 9.3%. For Class A queries, the range is 28.6% to 6.0%, and for Class D, the range is 63.1% to 13.8%. In each ease (and as in last year's results), the lowest error rates were reported by the CMU system. As noted in Section A9 of this paper, the AT&T NL system was the results of a collaborative agreement with CMU, thus it is not surprising that the performance is nearly identical to that of the CMU system. There are, in some cases, more than one set of results submitted by individual sites, corresponding to different systems. The differences between systems were specified in the \"Systems Descriptions\" provided to NIST at the time results were submitted. Space limitations prohibit discussion of these differences in this paper. After preliminary scoring had been completed, Moore at SRI advised NIST that a bug had been found in the code that produced results submitted to NIST for the SRI NL and SLS systems, with the effect of reporting results that were \"essentially the output of [the SRI] system with the robust processing component turned off\", because a \"No_~Answer\" response over-wrote the answer produced by the robust processing component (a \"template mateher\"). With the permission of the ARPA Coordinating Committee, SRI later resubmitted results for the debugged systems, and these SRI results are shown as \"late, debugged\" results. Table 17 presents a matrix tabulation of the official NL results for the several subsets of test material. There is some indication of varying degrees of difficulty presented by the different subsets of data from the different sites, subjectscenarios, and subject populations: note that the unweighted error rates reported in the \"Overall Totals\" row ranges from 28.1% to 16.0%, but also note that both these values were obtained with BBN systems --one at BBN, and the other at NIST. These differences probably are not significant since the numbers of speakers in the individual test sets is small. A.8.3. Spoken Language System (SLS) Understanding Tests. Table 18 presents a tabulation of the results for the SLS tests for all systems and all sets of \"answerable\" NTIS queries, Class A+D, Class A and Class D. For the set of all answerable queries, Class A+D, the unweighted error rate (\"UW. Err.\") ranges from 46.8% to 13.2%. For Class A queries, the range is 33.5% to 8.9%, and for Class D, the range is 65.2% to 17.5%. For the Class A+D and Class A results, the lowest error rates were obtained by the CMU system, but for the Class D results, the lowest error rates were obtained by the MIT/LCS system. Table 19 presents a matrix tabulation of the official SIS results for the several subsets of Class A+D test material from different sites. Note that there is some evidence of \"local adaptation\" to locally collected data (e.g., error rates for the CMU system are substantially lower for the CMUcollected data). Note also that some sites (typically the \"volunteers\") continued to use the \"No_~swer\" option more frequently than others, which would be a beneficial strategy in a system in which \"wrong answers\" were penalized more heavily than \"no answer\". In some eases, use of this option was more prevalent for data from some originating sites than others, perhaps reflecting differences between subject populations or subject-scenario subsets. I Environn~ent 1 i;~i;i~i:i;i:i:i:i:i:i:i:i;i:iii~:~i:i:i:i:i:i:i:i:i:i:::i:i:i:i:!;i~i~i~i~i~i~i~i;i!i~i!i!i!i:i:i:i;i:i;i;i~i~i~i~i~i~i!i:i?i:i:i:i:!;!:i;!:i         Nt~mbers printed at the top of the matrix columns indicate the number of utterances in the Test Data (sub)set from the corresponding mite. \"Overall Totals\" (colttmn) present results for the entire Class A+D Subset for the system corresponding to that matrix row. \"Foreign Coll. Site Totals\" present results for \"foreign slte\" data (i.e., excluding ]ccelly collected data) for the Class A\u00f7D Subset. \"Overall Totals\" (row) present res~ Its accumulated over all systems corresponding to the Test Data (sub)set corresponding to that i~atrix column. \"Foreign System Totals\" present results acc~Imulated over \"foreign systens\" (i.e., excluding results for the system(s) developed at the site responsible for collection of that Test Data s~ibset.)     Matrlx columns present results for Test Data S~absets collected at several sites, and matrix rows present results for dlfferemt systems. i I I Iiii IIii IIII Illl Iiii \"!!!i\" \u00f7------\u00f7-- ~,~, ~ -- \u00f7----\u00f7----\u00f7------ \u00f7----\u00f7 ..... ~ 1222o JJJJ '''~' -- \u00f7------+----+ ....... I --\u00f7------ + ------ + ---- II~, ~, IIII iiii 1 R \u00f7----D + ------ i,ii i i m +__--m~ .... i t 4J Numbers printed at the top of the matrix columns indicate the number of \u00f7valuable utterances in the Test Data (sub)set from the corresponding site. \"Overall Totals\" (column) present results for the entire Class (A\u00f7D) Subset for the system corresponding to that matrix row. \"Foreign Coil. Site Totals\" present results for \"foreign site\" data (i.e., excluding locally collected data) for the Class (A\u00f7D) Subset. \"Overall Totals\" (row) present results accumulated over all systems corresponding to the Test Data (sub)set corresponding to that raatrlx column. \"Foreign System Totals\" present results acclLm~lated over \"foreign systems\" (i.e., excluding results for the system(s) developed at the site responsible for collection of that Test Data subset.) ** Late and for a debugged system.  Matrix columns present results for Test Data Subsets collected at several sites, and matrix rows present results for different systems. Numbers printed at the top of the matrix columns indicate the number of evaluable utterances in the Test Data (sub)set from the corresponding site. \"Overall Totals\" (column) present results for the entire Class (A\u00f7D) Subset for the system corresponding to that matrix row. \"Foreign Coll. Site Totals\" present results for \"foreign site\" data (i.e., excluding locally collected data) for the Class (A\u00f7D) Subset. \"Overall Totals\" (row) present results accuxnulated over all systems corresponding to the Test Data (sub)set corresponding to that matrix column. \"Foreign System Totals\" present results accumulated over \"foreign systems\" (i.e., excluding results for the system(s) developed at the site responsible for collection of thet Test Data subset.) ** Late and for a debugged system. ;;---;---;-i-~;;--~;---;rl-~;J;;---;-\u00f7-;;i--;;---; -",
    "abstract": "This paper reports results obtained in benchmark tests conducted within the ARPA Spoken Language program in November and December of 1993. In addition to ARPA contractors, participants included a number of %olunteers\", including foreign participants from Canada, France, Germany, and the United Kingdom. The body of the paper is limited to an outline of the structure of the tests and presents highlights and discussion of selected results. Detailed tabulations of reported \"official\" results, and additional explanatory text appears in the Appendix. WSJ-CSR TESTS New Conditions All sites participating in the WSJ-CSR tests were required to submit results for (at least) one of two \"Hub\" tests. The Hub tests were intended to measure basic speaker-independent performance on either a 64K-word (Hub 1) or 5K-word (Hub 2) read-speech test set, and included required use of either a \"standard\" 20K trigram (Hub 1) or 5K bigram (Hub 2) grammar, and also required use of standard training sets. These requirements were intended to facilitate meaningful cross-site comparisons. The \"Spoke\" tests were intended to support a number of different ehaUenges. Spokes 1, 3 and 4 supported problems in various types of adaptation: incremental supervised language model adaptation (Spoke 1), rapid enrollment speaker adaptation for \"recognition outliers\" (i.e., non-native speakers) (Spoke 3), incremental speaker adaptation (Spoke 4). [There were no participants in what had been planned as Spoke 2.] Spokes 5 through 8 supported problems in noise and channel compensation: unsupervised channel compensation (Spoke 5), \"known microphone\" adaptation for two different microphones (Spoke 6), unsupervised channel compensation for 2 different environments (Spoke 7), and use of a noise compensation algorithm with a known alternate microphone for data collected in environments when there is competing \"calibrated\" noise (radio talk shows or music) (Spoke 8). Spoke 9 included spontaneous \"dictation-style\" speech. Additional details are found in Kubala, et al. [1], on behalf of members of the ARPA Continuous speech recognition Corpus Coordinating Committee (CCCC). WSJ-CSR Summary Highlights The design of the \"Hub and Spoke\" test paradigm, was such that opportunities abounded for informative contrasts (e.g., the use of bigram vs. trigram grammars, the enablement/disablement of supervised vs. unsupervised adaptation strategies, ete). There were nine participating sites in the Hub I tests and five sites participating in the Hub 2 tests, and some sites reported results for more than one system or research team. The lowest word error rate in the Hub 1 baseline condition was achieved by the French CNRS-LIMSI group [2, 3] . Application of statistical significance tests indicated that the performance differences between this system and a system",
    "countries": [
        "United States"
    ],
    "languages": [
        "French",
        "German"
    ],
    "numcitedby": "184",
    "year": "1994",
    "month": "",
    "title": "1993 Benchmark Tests for the {ARPA} Spoken Language Program"
}