{
    "article": "In this paper, we introduce an extension of our previously released TUKE-BNews-SK corpus based on a semi-automatic annotation scheme. It firstly relies on the automatic transcription of the BN data performed by our Slovak large vocabulary continuous speech recognition system. The generated hypotheses are then manually corrected and completed by trained human annotators. The corpus is composed of 25 hours of fully-annotated spontaneous and prepared speech. In addition, we have acquired 900 hours of another BN data, part of which we plan to annotate semi-automatically. We present a preliminary corpus evaluation that gives very promising results. The initial research related to Broadcast News (BN) transcription for the Slovak language started in recent years, when the BN data collection was performed. This activity resulted in the first Slovak TUKE-BNews-SK corpus, introduced in (Pleva and Juh\u00e1r, 2014) . It consists of 265 hours of spontaneous speech, dialogues and live coverage with different background conditions. It was manually annotated, because there were no other available resources supporting the annotation process at that time. It is still the only Slovak annotated corpus for building knowledge sources for the BN transcription task. In this paper, we introduce a new extension of the TUKE-BNews-SK corpus, which stems from the current needs of our laboratory, and whose creation was partially initiated by demand in Slovak research. The reason for building the corpus lies in the fact that the modern and leading trends in building resources for large vocabulary continuous speech recognition (LVCSR) applications focus on fully-automatic annotation of speech (Wessel and Ney, 2005; Novotney et al., 2009) , without any additional human annotation effort. This fact motivated us to follow this trend. Unfortunately, we still do not have sufficient acoustic resources to perform automatic annotation, because the previous corpus did not provide enough data to build accurate acoustic models (AMs) for that purpose. With our increasing effort to build more precise and robust speech transcription system for error free speech transcription, the need for more data of high-quality become stronger and resulted recently in a cooperation with one of the Slovak commercial TV broadcasters. Moreover, with increasing demand driven by the growing population of deaf and hearing impaired people (approx. 5 \u2212 7% of Slovak population), we started recently with this broadcaster to support more TV broadcasting with Slovak closed captions generated automatically through modern speech recognition technologies. The above mentioned facts really motivated us to create a new corpus, which would meet the conditions and requirements for building modern LVCSR systems suitable for automatic annotation purposes. In the first phase, the presented annotated corpus was considered as initial data intended to bootstrap our speech transcription system and to find its optimal configuration. The initial data were employed also for gender detection, speaker diarization and identification, for preparing the adaptation data for acoustic modeling and finally, for the preliminary evaluation in terms of word error rate (WER) for different BN categories. The corpus was considered to be sufficient after we had collected a reasonable amount of acoustic data with respect to the adequate speaker coverage. It finally amounted to 25 hours of fully-transcribed high quality multi-track acoustic data originally provided to us by the broadcaster. So far, we have collected 900 hours of raw, non-annotated acoustic data to be annotated, including primarily prime time broadcast and sports news, social programs and TV shows. Corpus Design There are several significant differences between the previously released corpus and its extension. The data we have collected are composed of multi-channel audio recordings (see Sec. 2.1.), whereas the TUKE-BNews-SK corpus gathers single-channel recordings. Each channel has a separated content (in-studio anchors, external reporters and interviewees). The audio data from the in-studio anchors were captured directly in the TV studio by lavalier microphones. The voices of the external reporters were captured by handheld microphones mixed with the signal from noise-cancelling camera microphone. For annotation purposes the source audio channels were simply joined together. On the other hand, the multi-channel mode permits to employ advanced techniques for precise speech processing, noise suppression and acoustic modeling. Furthermore, the new speech database covers current events and hot topics in Slovakia, which is a suitable ground for domain-based modeling and topic extraction tasks. The current speaker inventory is expanded because of the presence of a number of new speakers. Some specific speakers remained the same and hence their database was successfully extended. Last, but not least, we cannot omit the fact  that speaking style has changed slightly during the last few years, which should be taken into account in acoustic and language modeling. Apart from the annotated corpus, we have collected other related acoustic data to be annotated, which comprise the following three blocks: 1. prime time evening TV News with Sports News (330 hours) -covered by three in-studio anchors and many reporters mainly outside the studio; 2. one-hour block of afternoon program (191 hours)it consists of Telephone Lottery contest, TV News with similar, but reduced content of prime time TV News, and social program, called \"Reflex\"; 3. the \"Morning\" TV shows (380 hours) -live broadcast often from outside, where two anchors lead semiprepared discussions with more than 10 guests about cooking, health, lifestyle, fashion, etc. There is a lot of background noise or music in the background. Data Acquisition and Archiving The source data are acquired as video tracks in MP4 container format and also as multi-channel audio tracks of sampling frequency f s = 48kHz in 24bit linear PCM (L24) format. Since the AMs were originally trained on speech data with f s = 16kHz, the audio data have to be downsampled to equal frequency. We utilized a multi-channel speech recognition operated through a web user interface (Koct\u00far et al., 2015) . For that purpose, we store the data in multimedia container Matroska 1 in separate audio streams to one file without audio/video compression (Sta\u0161 et al., 2015) . 1 https://www.matroska.org/ Basic Corpus Statistics The presented corpus consists overall of 60 hours of annotated content, including the silence and other malformed audio content. Semi-Automatic Corpus Annotation The absence of acoustic resources needed for the automatic annotation led us to use a middle ground solution, which lies in semi-automatic annotation. This concept is closely related to lightly supervised acoustic model training, where a well-trained LVCSR system is firstly used for initial transcription (Lamel et al., 2002; Li et al., 2013) . It is a wellestablished way to tackle the lack-of-data problem, especially for languages that do not belong to the group of major EU languages. Thus, we firstly used our server-based LVCSR system (Sta\u0161 et al., 2015) to transcribe the speech as well as possible. Secondly, we employed a number of trained human annotators for the subsequent manual postannotation at word level. The speech recognition server is based on large vocabulary recognition engine Julius (Lee et al., 2001) that was modified to support multi-threaded parallel speech recognition and sharing acoustic and language models (LMs) among all instances for memory space saving purposes (Lojka et al., 2014) . The acoustic model has been trained on genderbalanced acoustic databases that included: \u2022 250 hours of annotated speech recordings from TUKE-BNews-SK corpus (Pleva and Juh\u00e1r, 2014); \u2022 250 hours of annotated speech recordings of judicial readings, read phonetically rich sentences, newspaper articles and spelled items, recorded in conference rooms (Rusko et al., 2014 ); \u2022 90 hours of annotated speech recordings realized in the main hall of the Slovak Parliament (Darjaa et al., 2011; Rusko et al., 2014) ; \u2022 80 hours of annotated speech recordings from TV shows named \"Court Room\" (Rusko et al., 2014) . The trigram Slovak LM has been created using the SRILM toolkit (Stolcke, 2002) , restricted to the vocabulary size of 416k unique words and smoothed by the Witten-Bell algorithm. The LM adapted to the domain of broadcast news, has been trained on preprocessed and classified text corpora of more than 2, 150M tokens contained in 120M Slovak sentences (Hl\u00e1dek et al., 2014; Sta\u0161 and Juh\u00e1r, 2015) . The speech transcription system further employed Table 3 : Experimental results of the extended TUKE-BNews-SK corpus evaluation gender detection, speaker diarization and identification, multi-channel speech segmentation, and multi-pass sequential speech recognition with hypothesis combination (Lojka and Juh\u00e1r, 2014; Kiktov\u00e1 and Juh\u00e1r, 2015; Sta\u0161 et al., 2015) . Since the transcribed acoustic data were not accurate (approx. 21.40% WER), the generated annotations were completed by annotators using the Transcriber tool (Barras et al., 2001) with respect to the corrections and by adding gender and speaker labels, acoustic condition labels and focus conditions. The background sounds, such as music, background speech, different noises, telephone speech, etc., were also manually annotated (see Tab. 2.), because we are planning to build acoustic models that are robust towards these noises through multi-condition training. The annotations permit full speaker-adaptive training. In the second phase, we are planning to append the fullyannotated data from that corpus to the current training data in order to retrain the present acoustic and language models. The retrained sources will be used to perform further automated transcription and manual correction of a different set of acquired BN data. Consequently, the new acoustic data will be appended again to the already expanded training data and the process of LVCSR system retraining and semi-automatic annotation will iteratively proceed until a sufficient amount is reached (usually hundreds of hours). Naturally, the amount of training data will grow rapidly, and the transcription accuracy at each iteration should consistently increase. From this point, we hope that we will be able to move to the automatic annotation. Experimental Results We report here the preliminary results (see The WER values of the last two categories are considerably higher (31. 70%-39.19%) . This is caused by the fact that non-native speakers frequently occur in the utterances or the speech is mostly spontaneous and moreover, the acoustic conditions are often degraded by background or traffic noise or by other sounds (music, shouting, sirens, wind, etc.) . Note that these acoustic conditions have not been included in the acoustic modeling yet. Regarding the gender-dependent evaluation, we can state that it can bring a further reduction of WER (except three cases) in the range from \u22120.50% to \u22121.19% absolutely. We have carried out also 58 speaker-dependent evaluations for in-studio anchors and external reporters (see Conclusions and Future Intentions In this paper, we introduced a new extension of our previously released TUKE-BNews-SK corpus. We were motivated by the modern trends in corpora design and we employed a semi-automatic annotation procedure to generate error-free transcriptions. Our current intention is focused on the semi-automatic acoustic data annotation, so far. We expect and hope that we will be able in the near future to move to fully-automatic annotation of any amount and kind of new data without the need for human annotation effort. Our other serious interest is focused on the DNN-based LVCSR for Slovak using Kaldi (Povey et al., 2011) . Therefore, we would like to replace the current speech recognition engine Julius by the WFST-based Kaldi engine. We are also interested in real-time BN subtitling and focus on improving the subtitling performance to generate closed captions for deaf and hearing impaired people. Acknowledgements The research presented in this paper was partially supported by the Ministry of Education, Science, Research and Sport of the Slovak Republic under the project VEGA 1/0075/15 (50%) and the Research and Development Operational Programme funded by the ERDF under the project implementation University Science Park TECHNICOM for Innovation Applications Supported by Knowledge Technology, ITMS code: 26220220182 (50%).",
    "abstract": "In this paper, we introduce an extension of our previously released TUKE-BNews-SK corpus based on a semi-automatic annotation scheme. It firstly relies on the automatic transcription of the BN data performed by our Slovak large vocabulary continuous speech recognition system. The generated hypotheses are then manually corrected and completed by trained human annotators. The corpus is composed of 25 hours of fully-annotated spontaneous and prepared speech. In addition, we have acquired 900 hours of another BN data, part of which we plan to annotate semi-automatically. We present a preliminary corpus evaluation that gives very promising results.",
    "countries": [
        "Slovakia"
    ],
    "languages": [
        "Slovak"
    ],
    "numcitedby": "9",
    "year": "2016",
    "month": "May",
    "title": "An Extension of the {S}lovak Broadcast News Corpus based on Semi-Automatic Annotation"
}