{
    "article": "This paper presents our submission to task 10, Structured Sentiment Analysis of the SemEval 2022 competition. The task aims to extract all elements of the fine-grained sentiment in a text. We cast structured sentiment analysis to the prediction of the sentiment graphs following (Barnes et al., 2021) , where nodes are spans of sentiment holders, targets and expressions, and directed edges denote the relation types between them. Our approach closely follows that of semantic dependency parsing (Dozat and Manning, 2018). The difference is that we use pre-trained language models (e.g., BERT and RoBERTa) as text encoder to solve the problem of limited annotated data. Additionally, we make improvements on the computation of cross attention and present the suffix masking technique to make further performance improvement. Substantially, our model achieved the Top-1 average Sentiment Graph F1 score on seven datasets in five different languages in the monolingual subtask. Introduction SemEval 2022 task 10 is a structured sentiment analysis task, aiming to predict all of the opinion tuples in a text. Each opinion O is a tuple (t, h, e, p), where h is a holder who expresses a polarity p towards a target t through a sentiment expression e. In practical, the task of structured sentiment analysis can help machines understand how people perceive ideas, policy etc. This paper describes the system developed by the team ZHIXIAOBAO for SemEval-2022 Task 10. We follow the work of (Barnes et al., 2021) to cast the task as dependency graph parsing problem. The predicted opinion tuples are denoted by a directed graph, G = (V, E), for each sentence. As shown in Figure 1 , all tokens in a sentence are presented as nodes and there are directed edges between the nodes to represent their relations. Each node in V can point to multiple nodes, and can have multiple incoming edges too. Sentiment expressions are regarded as roots in the structured sentiment graph (e.g., the token \"5\" and \"don't\" in Figure 1 ). Notice that not all nodes connect to the other nodes (e.g., the token \"give\" in Figure 1 ). The isolated tokens are the none-sentiment elements in the sentence, thus we should be able to predict the edge type of \"null\" in our model. Original edge types are defined as holder, target, and expression. Following the work of (Barnes et al., 2021) , we also tried the \"+inlabel\" style of definition, where none-\"null\" edge types consist of holder in , holder out , target in , target out , expression in , expression out . Foot markers in, out denotes the in-span and out-span edges respectively. For example, the edge from \"5\" to \"some\" belongs to holder out , while the edge from \"Some\" to \"others\" belongs to holder in . As demonstrated in previous work (Barnes et al., 2021) , formulating the task as a graph structure prediction problem is superior to that of solving it by the span extraction and relation prediction approaches. The former can better extract overlapping spans than the latter. Thus, our model mainly follows the solution of dependency parsing to directly predict between-word relations. The model consists of a text encoder to extract contextual features of tokens, and a classifier to predict edges between each pair of tokens. A bilinear or biaffine cross attention is applied in the classifier layer to make multiplicative interactions between the features of a pair of tokens. In our model, pre-trained language models (e.g., BERT and RoBERTa) are used as the text encoder. We discover that finetuning the pre-trained language model brings huge enhancements in our experiments. In addition, as the meaning of in-span and out-span edge types are totally different, we leverage two cross attention for in-span edge types and out-span edge types prediction respectively. We also present a suffix mask- Task Description SemEval task 10 focuses on predicting all elements of the structured sentiment in a text, represented by opinion tuples (t, h, e, p), where h is a holder who expresses a polarity p towards a target t through a sentiment expression e. The evaluation is on seven datasets in five languages, the statistics of which are shown in Table 1 . NoReC Fine (\u00d8vrelid et al., 2019) is the largest structured sentiment multi-domain dataset with professional reviews in Norwegian. MultiB EU and MultiB CA (Barnes et al., 2018) are hotel reviews datasets in Basque and Catalan, respectively. OpeNER EN and OpeNER ES (Agerri et al., 2013) are polarity-enhanced datasets with customer reviews in Spanish and English respectively. MPQA (Wiebe et al., 2005) annotates news articles in English from the world press. Finally, DS Unis (Toprak et al., 2010) is an annotated English reviews dataset of online universities and e-commerce. Previous shared tasks on Aspect-Based Sentiment Analysis (ABSA) focus on extracting sentiment targets and classifying the polarity directly. Most previous methods follow the information ex-traction pipeline, which firstly extract the span of holders, targets, expressions and subsequently predict the relations. However, splitting structured sentiment analysis into subtasks may cause the error propagation problem. We follow the work that solving the problem by dependency graph parsing (Barnes et al., 2021) to achieve better performance in our model. System Overview The overview of our system is shown in Figure 1 . We use a pre-trained language model to extract text information as the node features in the graph. Then, a cross attention layer is used to compute the predicted score of each edge type. After we get the edge score for each token pair, a graph parsing algorithm is presented to transform the predicted score to opinion tuples. Text Encoder We use the pre-trained language models, BERT and RoBERTa, to generate the contextual features of the text in multiple languages. Both of them are Transformer-based language models using a huge amount of text with a masked language model objective. These pre-trained language models have shown great superiority in a low-resource scenario like this task. Compared with BERT, RoBERTa removes next sentence prediction(NSP) loss and applies larger batch size and sequence length during the pre-training step, leading to a better performance in most cases. We tried the monolingual version of RoBERTa LARGE (Liu et al., 2019a) and BERT LARGE (Devlin et al., 2019) for each dataset, as our feature extractor. Our experimental results demonstrate that RoBERTa LARGE performs better than BERT LARGE in all the datasets. Discrete Cross Attention After extracting the text features, we can then use bilinear or biaffine attention to produce a score for each pair of tokens. The score includes multiplicative interactions among pairs, and can be used to predict edge types. Different from the previous work (Barnes et al., 2021) , we model heads and dependents of in-span and out-span separately, because we think it is better to cast the in-span and out-span label prediction as two \"different\" tasks. According to our observation, they have different properties in the corpus. Thus, inspired by the multi-task learning framework, we propose to use Discrete Cross Attention (DCA) to make them share same bottom features in the text encoder, but have non-shared parameters in the computation of cross attention. The contextual features C extracted from text encoder are processed with four layers of the feedforward neural networks(FNN), FNN in head , FNN in dep , FNN out head and FNN out dep creating representations of potential heads and dependents for in-span and out-span respectively. And then a bilinear score is computed for each kind of edge types using a trainable parameter matrix A. The discrete cross attention can be formulated as bellow, h in i = F N N in head (c i ) (1) d in i = F N N in dep (c i ) (2) score in ij = h inT i A in d in j (3) h out i = F N N out head (c i ) (4) d out i = F N N out dep (c i ) (5) score out ij = h outT i A out d out j ( 6 ) score ij = sof tmax(score in ij \u2295 score out ij ) (7) score ij represents the final score list for each edge type, which is the softmax score of the concatenation of in-span edge scores score in ij and outspan scores score out ij . Graph Parsing We set a threshold \u03b8 to determine whether the edge exists, i.e., if max(score ij ) > \u03b8, we set the predicted edge type to be argmax(score ij ), or we make the predicted edge to be \"null\". We set \u03b8 = 0.5 in our experiments. We use two kinds of graph parsing representations, head-first and head-final, following (Barnes et al., 2021) . For head-first, we use the first token in the target/holder/expression spans as the head of the span and the other tokens within the span as the dependent. For head-final, we take the opposite way, i.e., set the final token of the target/holder/expression spans as the heads. The algorithm of converting structured sentiment graph to opinion tuples (H, T, E) is in shown in Algorithm 1. H, T , E denote holder, target and expression respectively. ExpT ypes, HolT ypes, T rgT ypes are the edge type sets for expression, holder, target respectively. e i,j denotes the predicted edge type between token i and token j, and r is the root nodes. F indSpan(\u2022) is a function to find the complete span for a certain edge type, which can be simply implemented by merging linked tokens with the same edge type. As shown in Algorithm 1, we should first find the expressions, and then add the linked holders and targets for each expression to the opinion tuples. Notice that if we cannot find holder or target spans for an expression, we shall append an empty token into H set or T set . Suffix Masking Trick In both training and predicting procedure, a sentence is first tokenized by byte-pair encoding (BPE) before it is inputted into the text encoder, i.e., BERT and RoBERTa. Some words are splitted into prefixes and suffixes in the procedure. For example, \"universities\" is tokenized to \"universiti\", \"##es\" as shown in Figure 2 . We think these suffixes are often noises and provide few supervision signals for the edge prediction, since they are shared by many distinguished words. Inspired by this intuition, we mask these suffixes in the computation of edge scores. As shown in Figure 2 , we mask the suffix, \"##es\" and \"##tory\", before the prediction of edges. In this way, the edges only exist between the pair of (< cls >, satisf ac) and (universiti, satisf ac). Experiments The experiments details and main results are shown in this section. Experiment Details The implementation of our model depends on pytorch and huggingface. In our experiments, BERT LARGE represents the monolingual version for each dataset, which all can be found in the huggingface website. As for RoBERTa LARGE , the version of xlm-roberta-large (Conneau et al., 2019) is used on models for MultiB CA , MultiB EU and OpeNER ES , and roberta-large-en-cased (Liu et al., 2019b) is used on the English datasets, i.e., MPQA, OpeNER EN and DS Unis . We use Adam as our optimizer with the learning rate to be 1e-5 for the fine-tuning of pre-trained language models and 1e-4 for the other parameters in the model. The batch size is set to 12 with the gradient accumulation steps to be 48. The dropout rate is 0.3 and the hidden state size of FNN layers is set to 256. Our models are run on a maximum of 1000 epochs. We train all the models with 5 different seeds on the training set released by the orginizer and choose the best results based on the performance on development datasets. The training run on two Tesla V100 GPUs with 32G memory. It has to be noted that we add a \"<cls>\" token when encoding the sentences and set the \"<cls>\" token as the root of the sentiment graph. In Figure 1 , there actually exists an edge between <cls> token and the head of expression spans. Metrics To measure how well a system is able to capture the full sentiment graph, submitted systems are evaluated on sentiment graph F 1 (SF 1 ) following (Barnes et al., 2021) . A true positive is defined as an exact match at graph-level, weighting the overlap in predicted and gold spans for each sentiment element, averaged across all three kinds of spans, i.e., expression, holder, target. For precision we weight the number of correctly predicted tokens divided by the total number of predicted tokens (for recall, we divide instead by the number of gold tokens). Main Results The main experimental results are shown in Table 2 . It can be seen that using monolingual BERT LARGE pre-trained on larger language-specific corpus as text encoder is better than the multilingual BERT base used in (Barnes et al., 2021) , and finetuning the pre-trained language models brings more improvements than freezing the parameters. An in- Table 2 : Main results. mBERT base denotes the multilingual BERT (Xu et al., 2019) . \"+lstm\" denotes adding an LSTM layer after the text encoder. \"+inlabel\", \"DCA\" and \"mask\" denote the \"+inlabel\" style of edge types, the discrete cross attention and the suffix masking technique presented in last section. teresting discovery is that adding an LSTM layer between text encoder and cross attention leads the decreasing of SF 1 score. Thus, we remove the LSTM layer in our final submitted models. In addition, we can see that the \"+inlabel\" style definition of edge types is indeed helpful in this task. Furthermore, the presented discrete cross attention and suffix masking technique significantly improve the performance of our model. The results prove the effectiveness of finetuning RoBERTa in this task. As shown in the Table 2 , methods with RoBERTa LARGE as the text encoder on six datasets achieve the best performance. The best SF 1 scores on MPQA, OpeNER EN and DS Unis are 44.7, 76.0, and 49.4, where roberta-large-en-cased is used in the model. For MultiB CA , MultiB EU and OpeNER ES , xlmroberta-large (Conneau et al., 2019) outperforms BERT LARGE and we have 72.8 on MultiB CA , 73.9 on MultiB EU and 72.2 on OpeNER ES . The results demonstrate that a pre-trained language model with more parameters and trained on the larger corpus performs very well in downstream tasks as feature extractor. For NoReC Fine , we use nbbert-large (Kummervold et al., 2021) and the SF 1 score is 52.9. We do not get results on NoReC Fine in the last line of the table because monolingual RoBERTa LARGE for Norwegian is not available at the time of our experiments. We guess that the model on NoReC Fine using RoBERTa LARGE will achieve a better result. There are some failed attempt during the period of competition, too. We tried to enrich the contextual features of a sentence with word embedding, POS tag embedding, lemma embedding by using tools like SpaCy (Honnibal et al., 2020) , Stanza (Qi et al., 2020) and UDPipe (Straka and Strakov\u00e1, 2017) . But, we find that it is not superior to directly fine-tuning the pre-trained language model. We also tried to pre-train the RoBERTa-large on a Norwegian corpus, then the SF 1 score continuously grows with the increasing of training steps. However, due to the limitation of computation resources and time, we only trained the model with 2M steps. The final version does not outperform that using BERT LARGE because of the inadequate training. Conclusion In this paper, we have presented the implementation of the ZHIXIAOBAO system submitted to the SemEval-2022 Task 10. We propose an enhanced dependency parsing model for sentiment graph analysis. We leverage the fine-tuning technique of pre-trained language models, BERT LARGE and RoBERTa LARGE to increase the ability of model generalization. Furthermore, we present the discrete cross attention and suffix masking technique to achieve a significant performance improvement. Our model ranked 1st out of 32 participating teams on the monolingual subtask with the highest SF 1 score on 4 datasets. Acknowledgements We would like to thank the anonymous reviewers for their constructive comments.",
    "abstract": "This paper presents our submission to task 10, Structured Sentiment Analysis of the SemEval 2022 competition. The task aims to extract all elements of the fine-grained sentiment in a text. We cast structured sentiment analysis to the prediction of the sentiment graphs following (Barnes et al., 2021) , where nodes are spans of sentiment holders, targets and expressions, and directed edges denote the relation types between them. Our approach closely follows that of semantic dependency parsing (Dozat and Manning, 2018). The difference is that we use pre-trained language models (e.g., BERT and RoBERTa) as text encoder to solve the problem of limited annotated data. Additionally, we make improvements on the computation of cross attention and present the suffix masking technique to make further performance improvement. Substantially, our model achieved the Top-1 average Sentiment Graph F1 score on seven datasets in five different languages in the monolingual subtask.",
    "countries": [
        "China"
    ],
    "languages": [
        "Basque",
        "Spanish",
        "Catalan",
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "{ZHIXIAOBAO} at {S}em{E}val-2022 Task 10: Apporoaching Structured Sentiment with Graph Parsing"
}