{
    "article": "In this paper we present, describe, and evaluate SentiEcon, a large, comprehensive, domain-specific computational lexicon designed for sentiment analysis applications, for which we compiled our own corpus of online business news. SentiEcon was created as a plug-in lexicon for the sentiment analysis tool Lingmotif, and thus it follows its data structure requirements and presupposes the availability of a general-language core sentiment lexicon that covers non-specific sentiment-carrying terms and phrases. It contains 6,470 entries, both single and multi-word expressions, each with tags denoting their semantic orientation and intensity. We evaluate SentiEcon's performance by comparing results in a sentence classification task using exclusively sentiment words as features. This sentence dataset was extracted from business news texts, and included certain key words known to recurrently convey strong semantic orientation, such as \"debt\", \"inflation\" or \"markets\". The results show that performance is significantly improved when adding SentiEcon to the general-language sentiment lexicon. Introduction The analysis of public opinion on globally relevant entities has consistently been a matter of great import, especially since the advent of market research, starting in the second half of the 20th century. As the number and variety of information resources has grown exponentially due to the consolidation of the Web 2.0 in the last decade, we face the challenge of transforming the richness of this vast amount of information into computationally tractable data. From an economic point of view, there has always been a demand for information on opinions related to financial institutions such as central banks, or the appraisal of the economic state-of-affairs, such as the Monthly Michigan Index of Consumer Sentiment in the USA. Generally funded by either public or private institutions, these studies were carried out employing traditional surveying techniques, with data analysis methods that are not always transparent and, in many cases, without the possibility to access the original data, which would allow replication of results or find alternative interpretations. A newer trend, however, has been used in recent years which employs data and text mining techniques. Sentiment Analysis can no doubt be seen as an alternative to traditional surveying methods, and has been considered as a more dynamic and robust solution to the analysis of online text, such as news, user-generated content (UGC) or financial reports (Batra and Rao, 2010; Yu et al., 2013) . Despite the formidable body of research generated on the topic of sentiment analysis (see Liu (2015) for a concise state-of-the-art), SA strategies are still far from perfect as systems fall short of accounting for the vast complexity of human speech. Traditionally, approaches to sentiment analysis employ machine learning (ML) techniques, sentiment lexicons or, more commonly, a combination of both. The success of machine learning techniques draws on the well-known fact that the expression of sentiment depends is heavily dependent on domain specificity, that is, words and phrases tend to have the same sentiment within the same genre, but may vary across domains (Aue and Gamon, 2005; Ding and Liu, 2007; Pang and Lee, 2008; Choi et al., 2009) . ML algorithms excel at capturing systematicity when trained on a large corpus of similar documents, where the same lexical items are used repeatedly to express the same sentiment. Conversely, lexicon-based approaches rely on the existence of sentiment lexica. Such resources can be automatically created (Lu et al., 2011; Tai and Kao, 2013) or crafted manually. As Perrie et al. (2013) discuss, the advantage of the former is a faster creation process, whereas the latter tend to be more reliable. Either way, the availability of high quality sentiment lexical resources is of utmost importance for high performance. In this paper we describe the semiautomatic process of building a sentiment lexicon in the financial/economic domain, which assumes the existence of a general-language sentiment lexicon. Thus, we exploit the concept of a \"plug-in\" lexicon, compiled to be used in combination with general domain sentiment lexicons. To create this resource, we developed a corpus-driven method in order to extract candidate terms from a corpus of business news designed ad-hoc. These candidates were then matched against our existing general-language polarity lexicon, Lingmotiflex (Moreno-Ortiz and Perez-Hernandez, 2018) , to compile the final product. Finally, we evaluate its performance by running a sentiment classification task using several general-purpose sentiment dictionaries versus using them together with SentiEcon. The Economic-financial Domain in Sentiment Analysis The language of economics and finance is highly specialized, as has been recognized repeatedly by researchers in the field (Sinha et al., 2019; Loughran and McDonald, 2011; Loughran and McDonald, 2014; Loughran and McDonald, 2016; Krishnamoorthy, 2018) . The notion of domain dependence is particularly relevant in SA (Costa et al., 2016) , as it impacts every level of analysis and often requires expert judgment to determine the polarity of a given text. At the lexical level, in particular, the polarity of one word may vary across domains. For example, Loughran and McDonald (2011) observed that in the economic-financial domain, the word \"liability\" is considered neutral, whereas it usually conveys a negative polarity in general language. This makes it difficult to reuse SA lexicons across domains and accentuates the necessity to develop domain-specific resources. On the other hand, opinions recurrently shake market dynamics: good news may lift markets and bad news may sink them. Behavioral economics, e.g., Thaler (2015) , considers that emotions trespass the boundaries of rationality and govern, together with other factors, the decisionmaking of investors. Even before the advent of automatic sentiment analysis techniques, applying textual analysis to financial markets was a well-established practice. For instance, Niederhoffer's pioneering research (Niederhoffer, 1971) proposed the development of computational text analysis techniques to obtain greater objectivity in the results. He analyzed manually the semantics of two decades of headlines in the business section of The New York Times. Similarly, Klein and Prestbo (1974) supported the intrinsic connection between markets and news after observing how bad news influenced stocks. It is therefore not surprising that sentiment analysis, as understood today, was regarded, since its advent, as a powerful decision-making toolkit for analyzing market behaviour (Shiller, 2000) . Financial sentiment analysis emerged in the mid-2000s and focused mainly on the prediction of stock market movements employing external information such as press headlines, e.g., Meyer et al. (2017) , corporate reports, e.g., Hajek et al. (2014) , social media (mainly Twitter and Stocktwits), e.g., Gaillat et al. (2018a) , or expert sentiment towards different firms and their movements (O'Hare et al., 2009; Bar-Haim et al., 2011) . This sentiment-finance correlation has been consistent through the ages; Garcia (2013) observed the effect of sentiment on asset prices during a century-long period (1905 to 2005) from two daily financial columns from the New York Times. His lexicon-based SA analysis evidenced that economic recessions correspond with times of heightened expression of sentiment in the news. In a very recent study, Shapiro and Wilson (2019) analyzed transcripts of meetings of the Federal Open Market Committee in order to estimate the institution's objective function; they concluded that text-based sentiment analysis of news returns similar results to survey-based consumer sentiment measures. Within Sentiment Analysis, as understood today, it is common to distinguish corpus-based approaches from lexicon-based approaches. Although a combination of both methods can be found in the literature (Riloff et al., 2006) , lexicon-based approaches are usually preferred for sentence-level classification (Andreevskaia and Bergler, 2007) , whereas corpus-based, statistical approaches are preferred for document-level classification. Lexicon-based approaches to sentiment classifi-cation rely mostly on a dictionary that contains sentiment words with their associated polarity. But sentiment dictionaries have also been shown to improve performance in corpus-based approaches, and many researchers will use them as available or expand them to adapt them to their data sets. Although the application of general-domain lexicons tend to misclassify financial texts with a negative impact on performance, some influential financial research projects have used this type of lexicons, especially the Harvard General Inquirer (Stone and Hunt, 1963) . Tetlock (2007) used a sentiment dictionary to observe the influence of negative words in financial media in order to predict the earnings of companies and stock returns. The application of general-domain lexicons, such as the HGI, to the financial domain has been widely criticized. In this particular case, Loughran and McDonald (2011) point out that as much as as 73.8 percent of HGI's negative words does not have a negative sense in financial documents. They went on to re-examine this lexicon and produced a new one from documents published by the U.S. Securities and Exchange Commission. Entries in their dictionary may belong to one of the following categories: positive, negative, litigious, uncertain, strong modal, and weak modal. Our approach is similar, although it is based on the idea of a plugin domain-specific lexicon, where words that show a different polarity in the general sentiment dictionary are given the correct polarity in the plug-in dictionary, and words and phrases that do no exist are included. Other similar resources include Malo et al. (2014) , who trained classifiers to conduct sentence-level SA of financial news. Their Financial Phrase Bank provides a set of 5,000 sentences, manually annotated by 16 subject experts. It features specific domain concepts, an extensive list of verbs and expressions that help to detect the direction of financial indicators and a classification of the potentiality of some directionality-dependent terms. The resulting compilation is sensitive to the directionality of entities through the parsing of sentences. This resource was updated by Sinha et al. (2019) , who also released an entity-annotated news dataset containing over 12,000 headlines and their related financial sentiment. Financial microblogs are another relevant field of research. Oliveira et al. (2016) produced a stock market sentiment lexicon, which includes 20,551 items extracted automatically from microblogs (StockTwits and Twitter). In other languages, several resources have also been described. Van de Kauter et al. (2015) adapted a generaldomain lexicon in Dutch language featuring 3,187 items. Mao et al. (2011; Mao et al. (2014) built a financial lexicon in Chinese from a large news corpus trained on stock returns. Gaillat et al. (2018b) compiled the multilingual SSIX Corpora that classified 2,886 microblogging messages and its different entities in Spanish, German and English. Corpus compilation and lexicon extraction As mentioned in section 1, SentiEcon is designed to be used in combination with a general domain lexicon. Our plugin approach (see Moreno-Ortiz and Fern\u00e1ndez-Cruz ( 2015 ) for more details) aims to compile only those particular entries for which their domain-specific polarity differs or is not recorded in the general domain lexicon. It is designed, thus, to override the general polarity of lexical items with the one assigned in the plug-in lexicon, when they are used in the economic-financial domain and to add those sentiment words and phrases specific of the domain. Data Sources To extract term candidates, terminology extraction software requires a domain-specific corpus as the main source to extract specialized knowledge and a general-domain data collection to be used as a reference corpus. For our purposes, the Esmeraldas Great Recession News Corpus (GRNC) served as the main data source. This corpus consists of 41,000 news articles from the Business section of two major English-language daily newspapers: The Guardian and The New York Times. In total it contains 24 million words. It covers most releases published between January 2007 and December 2015. The general language reference corpus used was the English Web 2015 (EnTen-Ten15) (Jakub\u00ed\u010dek et al., 2013) , of approximately 15.7 billion words. Lexicon Creation Process The compilation of SentiEcon followed a 3-step process: (1) extracting candidate terms automatically, (2) matching candidates against our general-language polarity database, and (3) refining results through corpus work and manual assigning sentiment scores by domain experts. We used The Sketch Engine's KeyWords utility (Kilgarriff et al., 2014) to extract a set of some 30,000 single-word and multi-word term candidates. All term candidates were matched against our general-language polarity database (Lingmotif-lex). At this point it is worth remarking that our intention is not to extract a complete set of financial-economic terms, but rather to obtain a list of words and phrases that convey positivity or negativity when their orientation diverges from the sentiment they carry in other domains or in the general language or when the semantic orientation is not present in it. For example, terms like \"sale\" or \"bank\" are irrelevant here, as they are neutral in both the specific and the general domain. However, some single word terms such as \"asset\" or \"fix\" are included in the general lexicon as positive, but need to be recategorised in the plug-in lexicon as neutral, as their polarity differs: \"The currency fixes are based on actual trades\" vs. \"There is no easy fix: removing the wires can be dangerous\". In addition, all non-terms, neutral terms and terms that did not convey different semantic orientation in the financial domain were discarded. Once a complete list of candidates was compiled, we refined these results by matching candidates against the corpus and assigned sentiment scores with the support of a domain expert. The process was carried out as follows: 1. Initial speculative term selection and sentiment score assignment for clearly identified polarity words. 2. Checking intuitive assignment against corpus data by searching for alternative uses and local grammar patterns. 3. Enhancement of term set by adding lexical entries manually, in case, for instance, of obvious misses identified in the previous step. For example, a nonapparent term candidate found in the list was \"haircut\", which was rather frequent, especially related to the European debt crisis: \"The Troika want haircuts of over 50% for wealthy savers (100k+ euro) at Bank of Cyprus.\" 4. Assignment of final sentiment and intensity scores according to the lexicon's format, which is compiled in a tab-separated UTF-8 file (e.g., money <launder> VB neg 3). Description of SentiEcon Each lexicon entry consists of four data fields, as described in Table 1 . A more extensive description of the lexicon's format can be found in Moreno-Ortiz and Perez-Hernandez (2018). Data field Example SentiEcon includes both single words and multiword expressions, as they convey an important part of the semantic orientation of a text. Certain financial terms that reflect the results of a company's activity are technically considered neutral when isolated (e.g., \"unemployment\"), but undoubtedly convey a strong semantic orientation, which is revealed when combined with directional lexical elements. These elements are marked as neutral with intensity in the lexicon, and their semantic orientation when combined with directionality elements is expressed as multiword templates. Such cases are very common in the economic and financial domain, where variance figures are more important than absolute values. These economic indicators are characterized by not having inherent sentiment but will be positive or negative as they increase or decrease. The polarity of these terms will depend on the expected direction of events (e.g., the results are positive when they are expected to increase, but neutral or negative when they decline). Directionality elements, mainly verbs or nouns (e.g., \"rise\", \"rocket\", \"fall\", \"collapse\") mark both the direction and intensity of their respective performance indicators. For example, \"debt is rising\" conveys a negative polarity. This is easily implemented in our lexicon through a set of nouns and verbs (e.g. \"drop\", \"dip\", \"double\", \"rise\") that are redundantly combined with different positive or negative indicators, within a span of three words. Table 2 shows several examples. The final version of our economic/financial sentiment plugin lexicon contains 6,843 entries, distributed as shown in Table 3 . As is obvious from the single-word vs. multiword MWE pattern Example <debt> all neu 2 \"Holders of senior debt would be repaid in full.\" <debt> 3 <rise> nn neg 2 \"Carinthia's total debt will rise to euro 3.7 billion in 2014 (...)\" <debt> 3 <fall> nn pos 2 \"Household debt has fallen from its peak.\" unemployment 3 <dip> vb pos 2 \"But the unemployment rate dipped to 5.1% and hourly wages were steady (...)\" unemployment 3 <grow> vb neg 2 \"(...) with 24% unemployment also set to grow.\" Evaluation Methods In order to evaluate the performance of SentiEcon, we developed a manually annotated gold standard dataset consisting of 1,000 sentences. We decided to use sentences rather than paragraphs or full documents because it has been shown in the literature that document-level SA in the financial domain does not generally account for the relevance of text segments, as individual sentences in financial news typically focus on different aspects which may express different sentiments. For example, after analyzing 1,000 random sentences from financial announcements, Lutz et al. (2019) concluded that an accurate classification of sentences would perform more fine-grained explanatory analyses on financial texts and also improve pre-existent prediction systems. Sentence-level classification was also used in other relevant works, such as Malo et al. (2014) and Sinha et al. (2019) . The length of the news articles was also a deciding factor, since such texts are rather long articles where different concepts, events and entities are described and contrasted, which introduces a number of extraneous variables and unnecessarily complicates things for our purposes. Next, the 1,000 sentences sample was extracted randomly from our corpus. Then two domain experts annotated the dataset by classifying each sentence as belonging to one of three categories: POSITIVE, NEGATIVE, and NONE. They were instructed to take into account only the information available in the sentences and to annotate sentences. Annotation was carried out independently and then they were asked to reach a consensus in differing cases. Similarly to Malo et al. (2014) , our annotators were instructed to consider the following main guidelines while annotating the phrases: \u2022 There are no fixed rules about how particular words should be annotated. \u2022 Avoid bias based on prior knowledge about the company or institution. Thus, each sample sentence should be annotated by using the information that is explicitly available. \u2022 Be as consistent as possible with respect to your own annotations. This dataset was split into training and test sets in 80/20 percentages. The only features used to train the classifiers were the number of sentiment items (positive and negative) found in the sentences, the hypothesis being that the addition of a high-quality domain-specific lexicon such as Sen-tiEcon should have a strong impact on the performance of the classifiers. Even though some of the lexicons include neutral lexical items, we decided not use this as a training feature for any. We carried out 24 experiments in total, using three different lexicons with and without the addition of our plug-in lexicon and, in order to take into account the impact of various classification algorithms, we trained and tested four different ML classifiers: logistic regression, Naive Bayes, Random Forest, and SVM. We tested the performance of SentiEcon with the following sentiment lexicons: \u2022 The General Inquirer (Stone and Hunt, 1963) . This is one of the oldest sentiment lexicons publicly available. It is based on work in cognitive psychology and content analysis. This resource offers syntactic, semantic, and pragmatic information to part-ofspeech tagged words, with 1915 positive and 2291 negative words. Lexical items for \"yes\" and \"no\" (in the sense of refusal) are grouped in separate categories and further semantic dimensions, such as strength or active/passive orientation, are also included 1 . \u2022 MPQA (Multi-Perspective Question Answering) Subjectivity Lexicon (Wilson et al., 2005) . This resource contains 2,718 positive and 4,912 negative words drawn from a combination of sources, including the General Inquirer lists, the output of the system created by Hatzivassiloglou and McKeown (1997) and a bootstrapped list of subjective clues (Riloff and Wiebe, 2003) , hand-labeled for sentiment. The lexicon also includes labels for reliability (strongly subjective or weakly subjective) and four polarity tags: positive, negative, both and neutral. The majority of words are marked as having either positive (33.1 percent) or negative (59.7 percent) polarity, whereas only a small number of clues (0.3 percent) are marked as having both positive and negative, and 6.9 percent of the clues in the lexicon are marked as neutral 2 . \u2022 Lingmotif-lex (Moreno-Ortiz and Perez-Hernandez, 2018) . A manually-curated, wide-coverage, domain-neutral lexicon for sentiment analysis in English, developed for the sentiment analysis tool Lingmotif (Moreno-Ortiz, 2017a; Moreno-Ortiz, 2017b) . It contains over 28,000 single-word forms and over 38,000 multi-word expressions. All lexicons were adapted to the minimum shared features, but their performance was ultimately determined by the coverage and quality of their contents, as well as their relevance to the financial domain. In the following section we describe and discuss the results we obtained. Improvements are significant for every lexicon in terms of accuracy, across all categories. Accuracy is thus improved by nearly 8 percent on average by employing SentiEcon. Results Next we offer the performance results thrown by the different combinations of classifiers and lexicons in terms of precision and recall for each of the classes in the dataset (POS, NEG and NONE). Tables 5, 6 , and 7 show the results of the experiments performed with each of the lexicons without and with the SentiEcon plug-in lexicon. In general, performance figures show that the plug-in improves precision and recall for all categories (positive, negative and none), although some differences in degree are apparent. The precision, recall, and F1 figures in these tables are given using a \"no plugin/with plugin\" notation. the \"F1 Diff.\" column summarizes the difference in performance for each of the classes. The \"Avg\" row displays the macro-average values for each metric. which refers to sentences where no polarity is present, is, by far, the most difficult to classify correctly. This is because the presence of polarity lexical items does not always determine the polarity of the whole sentence, of course. The addition of the plug-in lexicon, where specialized lexical entries are able to \"neutralize\" a significant number of otherwise polarized words and phrases proves to be essential for the correct classification of these \"none\" cases. Class The data also show that scores are higher for recall of negative items in the MPQA and General Inquirer and, consequently, the addition of the plug-in lexicon greatly improves their metrics, although not as significantly as in the case of positive items. Special mention deserves the fact that neutral (\"none\") sentences are misclassified by the General Inquirer lexicon on its own, which is why there is such a big difference when the plug-in lexicon is added. Of course it also improves results in the other categories. Examples of misclassified NONE sentences are the following: \u2022 UBS said it would receive a direct injection of government money worth some $5.3 billion, while Credit Suisse said it had raised $8.8 billion from \"a small group of major global investors\" including the Qatari authorities. \u2022 The increase is higher than Next predicted only two months ago when it said prices would rise between 5% and 8%. \u2022 That is worth about $363.63 a share, or 16 percent higher than Allergan's closing price on Friday. \u2022 But the movable feast comes at a cost: pollution -especially carbon dioxide, the main global warming gas -from transporting the food. \u2022 Johnson had also met AstraZeneca and GSK, whose headquarters are in Britain, while O'Brien had been to the Paris headquarters of the French firm, Sanofi-Aventis. In contrast to Lingmotif-lex and MPQA's accuracy in the detection of neutral sentences, HGI's poor performance seems to be motivated by much lower polarity prediction figures in the lexicon-based classification. For instance, Lingmotif-lex detected 133 neutral sentences accurately and failed to do so 112 times. By contrast, HGI only succeeded detecting polarity 44 times, while failing to do so 201 times. Table 8 summarizes results for the category NONE in terms of identified positive and negative items. Again, domain specificity in the sentiment lexicon appears to be a key determining factor to assign the correct polarity, as some neutral items (e.g., \"authorities\" or \"feast\") have been tagged as positive while they are neutral in the target domain. Lexicon Conclusions In this paper we have presented SentiEcon, a wide coverage, domain-specific lexicon for the analysis of economic and financial texts in English. We have described our approach to the compilation and labeling process of this resource according to the requirements of the Lingmotif SA system, although the resource can be used in combination with any general language sentiment dictionary. Three main features characterize SentiEcon: its careful manual curation, the strong emphasis placed on multi-word expressions, and a simple implementation of a directionality system for economic indicators that accounts for sentiment assignation by context. Even if the system requires labour-intensive terminographical sessions with heavy reliance on domain experts, results already offer enough evidence as to the high quality and flexibility of our resource. Our evaluation results evidence that, in combination with a high-quality general-language sentiment lexicon, such as Lingmotif-lex, SentiEcon achieves very high performance levels that may encourage us to carry out further performance tests, different applications and the compilation of other domain-specific lexicons. Future research paths will also take into account the influence of specialized languages variation, as lexicons must adapt to different levels of expertise along with the observation of cross-domain interference. Acknowledgements This research was supported by Spain's MINECO through the funding of project Lingmotif2 (FFI2016-78141-P) and and PUCE Esmeraldas 2016 Research Fund. Special thanks to Mr. Ander Mugica for his assistance as a domain expert during the development stages of the lexicon.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 3.128162811005808e-07,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions that the research was supported by Spain's MINECO through the funding of project Lingmotif2 (FFI2016-78141-P) and the PUCE Esmeraldas 2016 Research Fund. MINECO is a government department in Spain, indicating research agency funding. There is no mention of defense, corporate, or foundation funding, and since there is specified funding, it cannot be classified as none.",
    "abstract": "In this paper we present, describe, and evaluate SentiEcon, a large, comprehensive, domain-specific computational lexicon designed for sentiment analysis applications, for which we compiled our own corpus of online business news. SentiEcon was created as a plug-in lexicon for the sentiment analysis tool Lingmotif, and thus it follows its data structure requirements and presupposes the availability of a general-language core sentiment lexicon that covers non-specific sentiment-carrying terms and phrases. It contains 6,470 entries, both single and multi-word expressions, each with tags denoting their semantic orientation and intensity. We evaluate SentiEcon's performance by comparing results in a sentence classification task using exclusively sentiment words as features. This sentence dataset was extracted from business news texts, and included certain key words known to recurrently convey strong semantic orientation, such as \"debt\", \"inflation\" or \"markets\". The results show that performance is significantly improved when adding SentiEcon to the general-language sentiment lexicon.",
    "countries": [
        "Spain"
    ],
    "languages": [
        "English",
        "French"
    ],
    "numcitedby": 5,
    "year": 2020,
    "month": "May",
    "title": "Design and Evaluation of {S}enti{E}con: a fine-grained Economic/Financial Sentiment Lexicon from a Corpus of Business News",
    "values": {
        "novelty": " In this paper we present, describe, and evaluate SentiEcon, a large, comprehensive, domain-specific computational lexicon designed for sentiment analysis applications, for which we compiled our own corpus of online business news.   These economic indicators are characterized by not having inherent sentiment but will be positive or negative as they increase or decrease. The polarity of these terms will depend on the expected direction of events (e.g., the results are positive when they are expected to increase, but neutral or negative when they decline).  Our evaluation results evidence that, in combination with a high-quality general-language sentiment lexicon, such as Lingmotif-lex, SentiEcon achieves very high performance levels that may encourage us to carry out further performance tests, different applications and the compilation of other domain-specific lexicons.",
        "performance": "In this paper we present, describe, and evaluate SentiEcon, a large, comprehensive, domain-specific computational lexicon designed for sentiment analysis applications, for which we compiled our own corpus of online business news. We evaluate SentiEcon's performance by comparing results in a sentence classification task using exclusively sentiment words as features. Improvements are significant for every lexicon in terms of accuracy, across all categories. Accuracy is thus improved by nearly 8 percent on average by employing SentiEcon."
    }
}