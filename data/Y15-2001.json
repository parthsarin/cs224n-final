{
    "article": "Dialog systems are generally categorized into two types: task oriented and non task oriented systems. Recently, the study of non task oriented dialog systems or chat systems becomes more important since robotic pets or nursing care robots are paid much attention in our daily life. In this paper, as a fundamental technique in a chat system, we propose a method to identify if a speaker displays sympathy in his/her utterance. Our method is based on supervised machine learning. New features are proposed to train a classifier for identifying the sympathy in user's utterance. Results of our experiments show that the proposed features improve the F-measure by 3-4% over a baseline. Introduction Dialog systems could be broadly divided into two categories. One is a task oriented dialog system. It focuses on a specific task such as guidance on sightseeing, hotel reservation or promotion of products, and communicates with a user to achieve a goal of the task. The other is a non task oriented dialog system or chat system. It does not suppose any specific tasks but can handle a wide variety of topics to freely chat with the user. Most of the past researches focus on task oriented dialog systems. In recent years, however, non task oriented dialog systems become more important since robotic pets or nursing care robots are paid much attention (Libin and Libin, 2004) . One of important characteristics in free conversation is sympathy of a speaker for the topics in the conversation (Anderson and Keltner, 2002; Higashinaka et al., 2008) . The topics in free conversation are not fixed but could be changed by the speakers at any time. To make the conversation natural and smooth, however, a non task oriented dialog system can not arbitrarily change the topics. It is uncomfortable for the user if the system would suddenly change the topic when the user wants to continue to talk on the current topic, or if the system would keep the same topic when the user is bored and does not want to talk on the topic any more. If the system fails to shift the topic at appropriate time, the user may break the conversation. The sympathy of the user is one of the useful clues to guess good timing for changing the topic. If the user shows the sympathy for the current topic, the system should continue the conversation with the same topic. On the other hand, if the user does not display the sympathy, the system should provide other topics. Therefore, it is essential for the chat system to guess the sympathy of the user. This paper proposes a method to automatically judge whether the user displays the sympathy in his/her utterance as a fundamental technique in a non task oriented dialog system. In this paper, we define 'sympathetic utterance' as the utterance where the speaker expresses the sympathy or approval especially when he/she replies to subjective utterance of the other participant. Note that the utterance just showing agreement is not defined as sympathetic. Various kinds of clues could be applicable for identification of the sympathy, such as facial expressions, gesture or the contents of the utterance. Since we focus on a text based chat system, our method only considers the content and detects the user's sympathy in a transcript of the utterance. In addition to ordinary n-gram features, new features for the sympathy identification are introduced. The effectiveness of our proposed features will be proved via empirical evaluation. The remaining parts of this paper are organized as follows. Section 2 discusses related work for the sympathy identification. Section 3 presents our proposed method. Section 4 reports results of evaluation experiments. Finally, Section 5 concludes the paper. Related work A considerable number of studies have been made on an automatic tagging of utterance in a dialog corpus. That is, each utterance in the dialog is automatically annotated with some useful information such as dialog acts. Hereafter we call it 'dialog tag'. Supervised machine learning is often used for automatic identification of dialog tags. Since the sympathy of the speaker is also regarded as a kind of dialog tags, we introduce several related work automatically classifying utterance into dialog tags including the sympathy 1 .  Xioa et al. (2012) proposed a method to estimate the sympathy speech using the language model learning tool SRILM (Stolcke, 2002) . In their method, n-gram of words were used as the features to classify if the utterance indicated the sympathy of the speaker. They reported that bi-gram was the most effective feature and the accuracy of the sympathy identification was around 60%. A set of 29 dialog acts including 'empathy' was proposed toward an open-ended dialog system (Minami et al., 2012) . They performed the automatic recognition of them using a weighted finite-state transducer with the words in the utterance. Sekino et al. (2010) tried to identify the dialog acts using Conditional Random Fields (CRF). SWBD-DAMSL tag set (Jurafsky et al., 1997) were used as a set of dialog acts. Note that the tag 'sympathy' is included in SWBD-DAMSL. The features used for training CRF were the tag of the previous utterance, the number of content words in the utter-ance, the length of the utterance and so on. To identify the dialog acts of the sentences in microblogging, semantic category patterns were introduced as the feature of Support Vector Machine (SVM) classifier (Meguro et al., 2013) . The words in the utterance were converted into their semantic categories (or abstract concepts) using a thesaurus, then n-gram of not words but semantic categories is used as the feature. Results of this study showed that n-gram of the semantic categories was more effective than word n-gram. This study also applies supervised learning for automatic identification of the sympathy. Especially, we investigate what are the useful features to infer the sympathy in the utterance. Therefore, we focus on identification of the sympathy only, although many previous work handled the sympathy as one of the dialog acts. Several studies reported that characteristics of the sympathy could be found in an expression at the end of the utterance (Itoh and Nagata, 2007; Huifang, 2009) . In addition, there might be more linguistic features indicating the sympathy of the speaker. The main contribution of the paper is that new features for the sympathy identification are proposed through manual analysis of a free conversation corpus. Furthermore, the effectiveness of these features is empirically evaluated by experiments. Note that the target language in this study is Japanese. Proposed method Our system accepts a text of utterance in free conversation as an input, then guesses whether it indicates the speaker's sympathy. Support Vector Machine (SVM) (Chih-Chung and Chih-Jen, 2001 ) is applied to train a binary classifier to judge if the given utterance is sympathetic 2 . Feature We design the following 9 features for sympathy identification. Note that all features are binary, that is, the weight in the feature vector is 1 if it is present in the utterance, 0 otherwise. F ng : Word n-gram The word n-gram (n=1,2,3) is used as the feature, since it represents the content of the utterance. This is the basic feature widely used for identification of the dialog tags in the previous work. Since the content of the previous utterance is also important, we use the word n-gram of both the current and previous utterance. F len : Length of utterance Since the sympathetic utterance tends to be short, the length of the utterance (the number of characters) is considered. In the simple approach, the length feature is defined according to intervals, such as '1 \u223c 5', '6 \u223c 10' and 'more than 10'. However, it is rather difficult to determine the optimum intervals. In this study, the length features are defined as in ( 1 ) and ( 2 ) f (i) len : if l u is in [i \u2212 2, i + 2] (1) f (long) len : if l u \u2265 20 (2) , where l u stands for the length of the utterance. We use 17 length features f (i) len (3 \u2264 i \u2264 19 ) as well as an extra feature f (long) len indicating the utterance is long. This approach enables us to incorporate the information of the length of utterance into SVM more flexibly. F tu : Turn taking In our conversation corpus, the speakers may give two or more utterance in one turn. This feature indicates the presence of turn taking, i.e. whether the speaker of the current and previous utterance is the same. F rw1 : Repetition of word (1) The speakers often show their sympathy by repeating a word in a previous utterance of the other. For example, in the simple conversation below 3 , the speaker B repeats the word '\u5091\u4f5c (fine work)' to agree with A's comment. A: \u3042\u306e (that) / \u6620\u753b (movie) /\u306f/ \u5091\u4f5c (fine work) / \u3060 (be) (That movie is a fine work.) B: \u5091\u4f5c (fine work) / \u3060 (be) /\u306d (It is a fine work.) We introduce a feature indicating if the same word appears in the current and previous utterance. F rw2 : Repetition of word (2) Repetition of the words does not always indicates the sympathy. Let us consider the following example. A: \u6d77\u8349\u985e (seaweed) / \u5acc\u3044 (dislike) /\u306a/\u306e/? (Do you dislike seaweed?) B: \u305d\u3046 (so) /\u3067/\u3082/\u306a\u3044 (not) /\u3088/\u3001/ \u6d77\u8349 (seaweed) (Not so much, seaweed.) The speaker B repeats the word '\u6d77\u8349 (seaweed)', but his/her utterance does not show the sympathy. This feature is similar to F rw1 , but more strictly checks the presence of repetition of the content words. The feature F rw2 is activated if either condition below is fulfilled: \u2022 The last predicative word in the previous utterance is also found in the current utterance. \u2022 There is only one content word in the current utterance and it also appears in the previous utterance. \u2022 The semantic class of the last predicative word in the previous utterance is also found in the current utterance. \u2022 There is only one content word in the current utterance and its semantic class also appears in the previous utterance. F F da : Dialog act Dialog act is also a useful feature to identify the sympathy. When we hear the other's assertion or opinion, we sometimes show our sympathy with it. However, we seldom express the sympathy for a simple yes-no question. In this study, we define a set of dialog acts in free conversation as in Figure 1 . self-disclosure, question(yes-no), question(what), response(yes-no), response(declarative), backchannel, filler, confirmation, request We manually annotate the conversation corpus with the dialog acts and use them as the features. In future, the dialog acts should be automatically identified to derive this feature. F end : End of sentence The speakers often show their sympathy in an expression at the end of their utterance. For example, in Japanese, \"\u3060 [da] / \u306d [ne]\" or \"\u3088 [yo] / \u306d [ne]\" 4 at the end of the sentence strongly indicates the sympathetic mood of the speaker. Based on the above observation, the expression at the end is introduced as the feature. In this paper, it is represented by a sequence of function words at the end of each sentence in the utterance. Combination features In the preliminary experiment, we investigated several types of kernels of the SVM classifier: linear kernel, polynomial kernel, radial basis function and so on 5 . We found that the kernels except for the linear kernel performed very poorly on our data set. Therefore, we chose the linear kernel. However, the individual features are regarded as independent each other in the SVM with the liner kernel, although the dependency between the features should be considered. To tackle this problem, we introduce a feature composed by combination of the existing features. When a feature set F = {. . . f i . . .} is derived from one utterance, where f i is one of the features described in Subsection 3.1, all possible pairs of features [f i , f j ] (i \u0338 = j) are also added to the feature set. Hereafter, [f i , f j ] is referred to as a combination feature. The combination features enable the classifier to consider the dependency between two features. Since the number of this feature are increased combinatorially, feature selection is applied as described in the next subsection. Feature selection A simple feature selection procedure is introduced. We apply the feature selection only for the word n-gram feature (F ng ) and the combination feature, since the numbers of these features are extremely high. The correlation between a sympathy class and a feature f i is measured by \u03c7 2 value. The features are discarded when \u03c7 2 value is less than a threshold. We denote the threshold of \u03c7 2 value for the n-gram and combination feature as T ng and T comb , respectively. In the experiment in Section 4, these thresholds will be optimized with a development data. Filtering of negative samples In supervised machine learning, it is inappropriate that the numbers of positive and negative samples in the training data are extremely imbalanced, since the trained classifier may display strong bias for the majority class. In general, however, the sympathetic utterance does not frequently appear in free conversation. Actually, the ratio of the sympathetic utterance is 1.1% in our conversation corpus as will be shown in Table 1 . To tackle this problem, a filtering process to remove the negative samples is introduced to correct imbalance of the training data. The basic idea of our filtering method is that we try to remove redundant negative samples. Here 'redundant' sample stands for a sample that is similar to other samples in the training data. Similar negative samples might be redundant and could be removed from the training data without any significant loss of the classification performance. The similarity between two samples (utterance) is measured by cosine similarity of the vector consisting of the word n-gram feature only. It is time consuming to calculate the similarity between all possible pairs of the utterance in the training data. Instead, we reduce the computational cost by constructing clusters of the utterance as the prepossessing. First, the clusters are constructed from the set of the negative samples. A fast clustering algorithm 'Repeated Bisections' is used, where the number of the cluster is set to 1000 6 . For each cluster, the redundant negative samples are detected by Algorithm 1. Given a set of utterance in a cluster U , the algorithm divides the utterance into a set of non-redundant utterance U k to be kept and redundant utterance U d to be deleted. For each utterance u i , if the similarity between u i and the rest of the utterance u j is greater than the threshold S f il , u j is added to the set U d . Then u i is added to U k . Intuitively, if several similar utterance are found, only the first appeared one is remained in the training data. Note that the threshold S f il controls the number of the removed negative samples. It is optimized on a development data. Evaluation This section reports experiments to evaluate our proposed method. In this experiment, the systems are evaluated and compared by the precision, recall and Input : U = {u 1 , u 2 , \u2022 \u2022 \u2022, u n } Output: U k , U d U k \u2190 \u2205, U d \u2190 \u2205 for i \u2190 1 to n do if u i \u2208 U d then next end for j \u2190 i + 1 to n do sim \u2190 cos(u i , u j ) if sim \u2265 S f il then U d \u2190 U d \u222a {u j } end end U k \u2190 U k \u222a {u i } end Algorithm 1: Search for redundant negative samples F-measure of the identification of sympathetic utterance. Data Meidai conversation corpus 7 is used to train and evaluate our proposed method. It is a collection of transcription of actual conversation or chat in Japanese. Two to four participants joined free conversation. Dialogs where the number of the participants is two are chosen from the corpus, then each utterance is manually annotated with 'sympathy tag' indicating whether it expresses the sympathy of the speaker or not 8 . We randomly divide the conversation corpus into three sets: 80% training, 10% development and 10% test set. Table 1 shows the number of the dialogs, sympathetic utterance (sym) and non-sympathetic utterance (non-sym) in each data. The ratio of the positive and negative samples stands at 1 to 86, that is, the number of sympathetic utterance is much fewer than non-sympathetic. A balanced data in- Results and discussion Parameter optimization First, the parameter T ng for selection of n-gram feature was optimized on the development data. Figure 2 shows a change in precision(P), recall(R) and F-measure(F) on the development data. We chose T ng = 0.9 as the best parameter where the precision, recall and F-measure were the highest. In this case, 4378 features, which are 1% of all n-gram features, were selected. Another parameters T comb and S f il were also optimized. The details will be reported later. Results We define the baseline as the classifier with the word n-gram feature only. Table 2 reveals the performance of the baseline and our proposed method on the test data, while Table 3 shows the results on the balanced test data. In these tables, the filtering of the negative samples is not applied. Our proposed method outperformed the baseline on the whole, although the precision was comparable on the balanced data. In the imbalanced test data, the F-measure was not so high. This is because the sympathetic utterance does not frequently appear in the conversation corpus. Since the participants of some dialogs were strangers, they might hesitate to express their sympathy. On the other hand, in the balanced test data, the results were reasonably high. If our method is applied for the conversation between close friends where they frequently show their sympathy, it will achieve better performance than the results in Table 2 . Effectiveness of features Next, to investigate the effectiveness of our proposed features, the models with several feature sets are compared. We train the classifiers with the basic word n-gram feature and one of the other features (denoted as F ng + F * ), and compared it with the baseline model (F ng ). We also compare the classifier with all features (denoted as F ALL ). Table 4 and 5 show the results on the imbalanced and balanced test data. Note that the combination features are not used in this experiment. On the imbalanced test data, adding the feature F len , F rc2 , F da and F end caused a decline of the Fmeasure. Furthermore, the classifier using all features were comparable with the baseline. However, on the balanced data, almost all types of the features contributed to gain the F-measure. In addition, precision, recall and F-measure of F ALL were better than the baseline. From the results in Table 4 and 5, turn taking (F tu ) and repetition of word (F rw1 and F rw2 ) seem the most effective features. Since the increase or decrease caused by adding one feature is inconsistent for several features on the imbalanced and balanced data, however, the effectiveness of them are rather unclear. Effectiveness of combination feature In this subsection, we evaluate the combination feature. Two sets of the features are investigated: the word n-gram feature F ng and all proposed features F ALL . For each feature set, the combination features are added to the feature vector of the utterance. Recall that we introduce feature selection for the combination feature. The parameter T comb was optimized on the development data. T comb was set as 140 for both feature sets F ng and F all on the imbalanced data. While, it was set as 280 and 260 for F ng and F all on the balanced data, respectively. Table 6 and 7 compare the classifiers with and without the combination feature in the imbalanced and balanced test data, respectively. In Table 6 , the combination feature improves both precision and recall in F ALL feature set. While, combination of the n-gram features increases the precision but decreases recall and F-measure. Therefore, the combination of our proposed features worked well, but the combination of n-gram not. In the balanced test data (Table 7 ), the models with and without the combination feature are comparable. Comparing F ng +COM B and F ALL +COM B in Table 6 , incorporation of the proposed features improved the F-measure with a loss of the precision. In the same comparison in Table 7 , all three criteria were improved by using the proposed features. Therefore, it can be concluded that our proposed features are effective for identification of the sympathy, especially when the dependency between two features is considered. Evaluation of filtering of negative samples The method of negative sample filtering was evaluated using the imbalanced data set. First, the parameter S f il was optimized as 0.5 that achieved the highest F-measure on the development data. Three methods are compared in this experiment: a model without the negative sample filtering (w/o Filtering), a model with the filtering by our proposed method (Proposed Filtering) and a model where the negative samples are randomly removed (Random Filtering). In Proposed Filtering, 25,174 negative samples were removed from the training data. In Random Filtering, the same number of the negative samples were randomly removed. We repeated the training of the classifier with random filtering five times and compared the average with the other methods. 8 reveals the results of three methods. By the filtering, the recall was improved, while the precision declined. It is natural because the classifier tends to judge the utterance as sympathetic (positive) when the number of the negative samples in the training data is reduced. Since F-measure was improved, the filtering of the negative samples seems to contribute toward improvement of the performance. However, our proposed filtering method was worse than the random sampling. It is still uncertain why the idea to remove the redundant negative samples is inappropriate in this task. In future, we will investigate the reason and refine the algorithm of the negative sample filtering. Error Analysis We have conducted an error analysis to find major causes of the errors. First, we found many false positives (the sympathetic utterance is wrongly classified as non-sympathetic) and false negatives (the non-sympathetic utterance is wrongly classified as sympathetic) when the previous utterance was long. In such cases, the previous utterance consisted of many sentences, but only one sentence was usually related to the current utterance. Although many features were derived from the previous utterance, the most of them were irrelevant. Such noisy features might cause the classification error. To overcome this problem, the coherence between the current and previous utterance should be considered. In other words, it is required to introduce a method to choose only the sentence relevant to the current utterance from long previous utterance. Many errors were also found when both the current and previous utterance were too short. We guessed that the classification errors were caused by the lack of the features. Due to the feature selection, even the word n-gram feature was sometimes not extracted from short utterance. One of the solutions is to apply feature selection only for bi-gram and trigram while remaining all uni-gram features, in order to prevent from extracting no n-gram feature. We also found that several false negatives were caused by the feature F end . Some of the expressions at the end of the sentence indicate the speaker's sympathy, but not always. Let us suppose such an expression appeared in non-sympathetic utterance and the lengths of both current and previous utterance were short. In such cases, since only a few features were extracted, the end of the sentence feature strongly worked to classify the utterance as the sympathetic. The way to incorporate the end expression into the classifier should be refined. Conclusion This paper proposed a method to identify the sympathetic utterance in the free conversation. The main contribution of the paper is to propose novel features for sympathy identification. Results of the experiments indicate that (1) the proposed features are effective, especially when the pairs of these features are considered as the additional features, (2) among the proposed features, turn taking and repetition of the content words show strong correlation with the sympathetic utterance, and (3) the filtering of negative samples is important to improve the F-measure. F-measure of the proposed method was still low in the extremely imbalanced positive and negative sample data. We proposed the filtering method to remove the redundant negative samples, but it was worse than the random filtering. However, since the results on the balanced data were promising, we believe that the filtering of negative samples is a right way to improve the performance. In future, we will continue to explore a better way of negative sample filtering.",
    "abstract": "Dialog systems are generally categorized into two types: task oriented and non task oriented systems. Recently, the study of non task oriented dialog systems or chat systems becomes more important since robotic pets or nursing care robots are paid much attention in our daily life. In this paper, as a fundamental technique in a chat system, we propose a method to identify if a speaker displays sympathy in his/her utterance. Our method is based on supervised machine learning. New features are proposed to train a classifier for identifying the sympathy in user's utterance. Results of our experiments show that the proposed features improve the F-measure by 3-4% over a baseline.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "1",
    "year": "2015",
    "month": "October",
    "title": "Identification of Sympathy in Free Conversation"
}