{
    "framing_question": "To what extent do the authors engage with the communities that are the subject of their paper, and do they claim a participatory approach?",
    "response": "The authors do not engage with the communities that are the subject of their paper, and they do not claim a participatory approach.",
    "article": "The growth of open-access technical publications and other open-domain textual information sources means that there is an increasing amount of online technical material that is in principle available to all, but in practice, incomprehensible to most. We propose to address the task of helping readers comprehend complex technical material, by using statistical methods to model the \"prerequisite structure\" of a corpus -i.e., the semantic impact of documents on an individual reader's state of knowledge. Experimental results using Wikipedia as the corpus suggest that this task can be approached by crowdsourcing the production of ground-truth labels regarding prerequisite structure, and then generalizing these labels using a learned classifier which combines signals of various sorts. The features that we consider relate pairs of pages by analyzing not only textual features of the pages, but also how the containing corpora is connected and created. Introduction and Motivation Nicholas Carr has argued in his recent popular book \"The Shallows\" that existing Internet technologies encourage \"shallow\" processing of recent and popular information, at the expense of \"deeper\", contemplative study of less immediately-accessible information (Carr, 2011) . While Carr's hypothesis is difficult to formalize rigorously, it seems intuitively plausible. For instance, user-generated content from Twitter and Facebook is mainly comprised of short, shallow snippets of information. Most current research in AI (and more broadly in computer science) does not seem likely to reverse this trend: e.g., work in crowdsourcing has concentrated on tasks that can be easily decomposed into small pieces, and much current NLP research aims at facilitating short-term \"shallow\" goals, such as answering well-formulated questions (e.g., (Kwok et al., 2001) ) and extracting concrete facts (e.g., (Etzioni et al., 2006; Yates et al., 2007; Carlson et al., 2010) ). This raises the question: what can AI do to facilitate deep, contemplative study? In this paper we address one aspect of this larger goal. Specifically, we consider automation of a novel task-using AI methods to facilitate the \"deep comprehension\" of complex technical material. We conjecture that the primary reason that technical documents are difficult to understand is lack of modularity: unlike a self-contained document written for a general reader, technical documents require certain background knowledge to comprehend-while that background knowledge may also be available in other on-line documents, determining the proper sequence of documents that a particular reader should study is difficult. We thus formulate the problem of comprehending technical material as a probabilistic planning problem, where reading a document is an operator that will probabilistically change the state of knowledge K(u, t) of a user u at time t, in a manner that depends on u's prior knowledge K(u, t \u2212 1). Solving this task requires, among other things, understanding the effect of reading individual documents dspecifically, the concepts that are explained by d, and the concepts that are prerequisites for comprehending d. This paper addresses this problem. In particular, we consider predicting whether one page in Wikipedia is a prerequisite of another. More generally, we define the \"prerequisite struc-  ture\" for a corpus as a graph, where nodes are concepts to comprehend, and a directed edge d \u2192 d corresponds to the assertion \"understanding d is a prerequisite to understanding d\". For Wikipedia, we assume a one-to-one correspondence between document titles and concepts explicated by (i.e., postconditions of) these documents. Figure 2 presents a small example of a prerequisite structure, and indicates how it might be used to construct a plan for comprehending a specific concept. Focusing on Wikipedia has several advantages. First, it is densely linked, and hence a document d will likely be linked directly to any prerequisite page d . (However, not all hyperlinks will indicate a prerequisite.) Second, Wikipedia's standardized format makes textual analysis easier. Finally, there is a great deal of social information available about how documents are used by the Wikipedia community. These properties make it easy for us to explore the informativeness of different types of information with respect to predicting prerequisite structure. Our overall plan for producing a prerequisite structure for a corpus is first, to use crowdsourc-ing approaches to obtain a subset of the prerequisite structure; and second, to extrapolate this structure to the entire corpus using machine learning. Below, we first describe datasets that we have collected, based on five technical concepts in Wikipedia from five different fields. We then outline the specifics of our procedure for annotating prerequisite structure, using Amazon's Mechanical Turk, and demonstrate that meaningful signals about prerequisite structure can be obtained using a classifier that exploits several sources: graph analysis of Wikipedia's link graph; graph analysis of a bipartite graph relating Wikipedia pages to Wikipedians that have edited these pages; and textual analysis. We complete our experimental analysis of the prerequisite-structure prediction task by discussing and evaluating the degree to which prerequisite-structure prediction is domain-independent, and the degree to which different subareas of Wikipedia (e.g., biology vs computer science) require different predictors. After discussing related work, we return in the concluding remarks to the overarching goal of facilitating comprehension, and discuss the relation- ship of the current study to these goals. Specifically we note that facilitating comprehension also requires understanding a user's goals, and her initial state of knowledge, in addition to understanding the prerequisite structure of the corpus. We also discuss the relationship between planning and prerequisitestructure prediction and suggest that use of appropriately robust planning methods may lead to good comprehension plans, even with imperfectly predicted prerequisite structure. Experiments As discussed above, we focus in this paper on predicting prerequisite structure in Wikipedia. While most Wikipedia pages are accessible to a general reader, there are many pages that describe technical concepts, such as \"conditional random fields\", \"cloud radiative forcing\", and \"Corticotropin-releasing factor\". Most of these technical pages are not self-contained: for instance, to read and comprehend the page on \"conditional random fields\", one will have to first understand \"graphical model\", and so on, as suggested by Figure 1. In this section, we evaluate the following questions: \u2022 Can we train a statistical classifier for prerequisite classification in a target domain, where the classifier is trained on out of domain (i.e., non-target domain) data annotated using Amazon Mechanical Turk service? \u2022 What are the effects of different types of signals on the performance of such a classifier? \u2022 How does out of domain training compare to in domain training? Experimental Setup For our experiments, we choose five targets from differing areas for experimentation, listed in Table 1 . Several of the techniques we used are based on graph analysis. The full graphs associated with Wikipedia are unwieldy to use for experimentation because of their size: therefore, for each target concept, we extracted a moderate-sized low-conductance subgraph of Wikipedia's link graph containing the target, using a variant of the PageRank-Nibble algorithm (Andersen et al., 2006) . 1 . As parameters we used \u03b1 = 0.15 and = 10 \u22127 , yielding graphs with approximately 15-20,000 nodes and 350-500,000 edges each. We also collected the edit history for each page in every subgraph forming a second graph for each sub-domain 2 . On average, each page from these subgraphs had been edited about 20 times, by about 8 unique editors. Details are given in Table 1 . For classification, we used a Maximum Entropy (MaxEnt) classifier. Given a pair of Wikipedia pages x = (d, d ) connected by a directed edge (hyperlink) from d to d , the classifier will predict with probability p(+1|x) whether the main concept in page d is a prerequisite for the main concept in page d. The classifier has the form p(y|x) = exp(w \u2022 \u03c6(x, y)) y \u2208Y exp(w \u2022 \u03c6(x, y )) , y \u2208 Y = {\u22121, +1} where \u03c6(x, y) is a feature function which represents the pair of pages x = (d, d ) in a high dimensional space, and w is the parameter vector of the classifier which is estimated from training data. We use the Mallet package 3 to train and evaluate classifiers. For the experiments in this paper, we shall exploit the following types of features: WikiHyperlinks: Features include the random walk with restart (RWR) score (Tong et al., 2006) In order to evaluate different prerequisite classification systems and also to train the MaxEnt classifier, we collected gold prerequisite decisions using Amazon Mechanical Turk (AMT). Since preparing annotated gold data for entire graphs in Table 1 would be prohibitively expensive, we used the following strategy to sample a smaller subgraph from the larger domain-specific subgraph, which in turn will be used for training and evaluation purposes. Preliminary investigation suggested that most of the pages in the prerequisite structure rooted at a target 4 Amazon Mechanical Turk: http://mturk.amazon.com concept d are connected to d via many short hyperlink paths. Hence, for each target domain, we first selected the top 20 nodes with highest RWR scores, relative to the target concept, in the subgraph for that target concept (as listed in Table 1 .) We then sampled a total of 400 edges from these selected nodes, with outgoing edges from a node sampled with a frequency proportional to its RWR score. Thus, using this strategy, we selected up to 400 pairs of pages (3) the two pages are unrelated; (4) Don't know. Subsequently, based on the feedback from the workers, a fifth option was also added: the two concepts are related, but they don't have any prerequisite relationship between them. Based on the available workers and turnaround time, the number of assignments per HIT (i.e., number of unique workers assigned to a particular HIT) was either 3 or 5; and the number of HITs used was either 200 or 400. Depending on the hardness of domain and availability of workers opting to work on a domain, reward per HIT assignment was varied from $0.02 (for Global Warming and Newton's Laws) to $0.08 (for Public-key Cryptography, Meiosis and Parallel Postulate). This data collection stage spanning all five domains was completed in about a week at a total cost of $278. Statistics about the data are presented in Table 2 5 . Starting with the AMT data collected as above, we next created a binary-labeled training dataset, where each instance corresponds to a pair of pages. We ignored all \"Don't Know\" labels, treated option (1) above as vote for the corresponding prerequisite edge, and treated all other options as votes against. We then assigned the final label for a node pair using majority vote (breaking ties arbitrarily). Consistency of labels In contrast to standard setup of gold data preparation where a single annotator is guaranteed to provide feedback on every instance, the situation in case of Mechanical Turk-based annotation is different, as the workers are at liberty to choose the HITs (or instances) they want to work on. This makes standard \u03ba statistics-based inter-annotator computation (Fleiss, 1981) inapplicable in the current setting. We circumvented this problem by first selecting all workers with at least 100 feedbacks, and then computing pairwise \u03ba statistics between all pairs of these frequent workers. These \u03ba statistics were averaged across each domain, and also averaged across all domains. The results, also shown in Table 2 , show moderate agreement (recall that \u03ba = 0 indicates no correlation). We are encouraged to observe that moderate level of agreement is possible even in this setting, where there is no control over worker background and quality. We next explore whether this level of agreement is sufficient to train statistical classifiers. Prerequisite Classification In this section, we explore whether it is possible to train a MaxEnt classifier to determine prerequisite structure in a target domain, with the training performed in \"leave one domain out\" manner, where the training data originates from domains other than the target domain. For example, for classifications in the target domain, say \"Global Warming\", we train the classifier with annotated data from the remaining four domains (or whatever domains are available). We note that training on \"out of domain\", if it is successful, has several benefits. First, a successful training strategy in this setup removes any need to have labeled data in each target domain of interest, which is particularly relevant as labeled data is expensive to prepare. Second, a classifier trained just once can be repeatedly used across multiple domains without requiring retraining. Accuracies of MaxEnt classifiers trained using the \"leave one domain out\" strategy are shown in Figure 3 ; we report the test accuracy on each target domain, as well as the average across domains. Performance of a random classifier is presented as a baseline. Classes in the train and test sets were balanced by oversampling the minority class. From Figure 3 , we observe that it is indeed possible to train prerequisite classifiers in an out of domain setting, using data from the Amazon Mechanical Turk service; on average, the classifier outperforms the random baseline with 8.6% absolute improvement in classification accuracy. We also experimented with other rule-based classifiers 6 , and in all cases, the trained MaxEnt classifier outperformed these baselines. Although more sophisticated training strategies and more clever feature engineering would likely yield further improvements, we find it encouraging that even a relatively straightforward classification technology along with a basic set of features was able to achieve significant improvement in performance on the novel task of prerequisite prediction. Feature Ablation Experiments The MaxEnt classifier evaluated in the previous section had access to all three types of features: WikiEdits, WikiHyperLinks, and WikiPageContent, as described in the beginning of this section. In order to evaluate the contribution of each such signal, we created ablated versions of the full Max-Ent classifier which uses only one of these three subsets. We call these thee variants: MaxEnt-WikiEdits, MaxEnt-WikiHyperLinks, and MaxEnt-WikiPageContent, respectively. Average accuracies across all five domains comparing these three variants, in comparison to the Random baseline and the full classifier (MaxEnt-Full, as in previous section) are presented in Table 3 . From this, we observe that all three variants perform better than the random baseline, with maximum gains achieved by the MaxEnt-WikiPageContent classifier, which uses page content-based features exclusively. We  also note that the full classifier MaxEnt-Full, is able to effectively combine three types of signals improving performance even further. In Table 4 , we present a per-domain breakdown of the gains achieved by these four classifiers over the random baseline. From this, we observe that the MaxEnt-WikiEdits classifier outperforms the random baseline only in 2 out of 5 domains. This might be due to the fact that the MaxEnt-WikiEdits uses uses only one feature-the RWR score of the target page relative to the source page on the Wikipedia edits graph. We hope that use of more discriminating features should further help this classifier. From Table 4 , we also observe that MaxEnt-WikiHyperLinks is able to outperform the random baseline in 4 out of 5 cases, and the MaxEnt-WikiPageContent (as well as the full classifier) outperforms the random baseline in all 5 domains, sometimes with large gains (as in the case of Public-key Cryptography domain). From this we observe that good generalization performance is possible even when there is no in domain training data available. Effect of Out of Domain Training All the classifiers evaluated in previous sections were trained in an out of domain setting, i.e., the training data originated from domains outside the domain in which the classifier is applied and evaluated. This has several benefits, as noted above. An alternative and more standard way to train classifiers is to have the training and evaluation data be from the same domain (below, the in-domain setting). While such a classifier will require labeled training from each domain of interest, it is nonetheless of interest to compare in-domain and out-ofdomain learning. If there are substantive differences, this could be used to improve prerequisite-structure predictor in a subdomain (e.g., biology), or may suggest alternative training methods (e.g., involving transfer learning). Motivated by this, for each domain, we compare the performances of the out-of-domain and indomain classification performances. The results are shown in Figure 4 . On average, we observe that the out-of-domain classifier is able to achieve 93% of the performance of the in-domain classifier. We note that this is encouraging for domain-independent prerequisite-structure prediction, as this suggests that for the prerequisite classification task, close to optimal (i.e., in-domain performance) is possible when the classifiers are trained in an out-of-domain setting. Related Work We believe the task of prerequisite structure prediction to be novel; however, it is clearly related to a number of other well-studied research problems. In light of our emphasis on Wikipedia, a connection can be drawn between identifying prerequisites and measuring the semantic relatedness of concepts using Wikipedia's link structure (Yeh et al., 2009) . We consider here a related but narrower question, namely whether an inter-page link will improve comprehension for a specific reader. In the area of intelligent tutoring and educational data mining, recent research has looked at enriching textbooks with authoritative web content (Agrawal et al., 2010) . Also, the problem of detecting prerequisite structure from differential student performance on tests has been considered (e.g., (Pavlik et al., 2008; Vuong et al., 2011) ). Our proposal considers discovering prerequisite structure from text, rather than from exercises, and relies on different signals. Research in adaptive hypermedia (surveyed elsewhere (Chen and Magoulas, 2005) ) has goals similar to ours. Most adaptive hypermedia systems operate in narrow domains, which precludes use of some of the crowd-based signals we consider here. In this literature, a distinction is often made between \"adaptability\" (the ability for a user to modify a presentation of hypermedia) and \"adaptivity\" (the ability of a system to adapt to a user's needs.) In this framework, our project focuses on adding \"adaptivity\" to existing corpora via a prerequisite structure, and our principle contribution to this area is identifying techniques that learn to combine textual features and social, crowd-based signals in order to usefully guide comprehension. Another related area is data-mining logs of Web usage, as surveyed by Pierrakos et al (Pierrakos et al., 2003) . Our focus here is on facilitating a particular type of Web usage, comprehension, rather than more commonly-performed tasks like site navigation and purchasing. A number of \"open education\" resources exist, in which information can be organized into sharable modules with known prerequisites between them (e.g., Connexions (Baraniuk, 2008) ). We focus here on discovering prerequisite structure with machine-learning methods rather than simply encoding it. Similarly, a Wikimedia project 7 has developed infrastructure allowing a user to manually assemble Wikipedia pages into e-books. Our focus is on guiding the process of finding and ordering the sections of these books, not the infrastructure for generating them. We also note that one widely-used way for complex technical concepts to be broadly communicated is by writers or teams of writers, and previous researchers have investigated understanding how collaborative writers work (No\u00ebl and Robert, 2004) , and even developed tools for collaborative writing (Zheng et al., 2006) . Our work focuses on tools to empower readers, rather than writers. Conclusion In this paper, we motivated the goal of \"crowdsourcing\" the task of helping readers comprehend complex technical material, by using machine learning to predict prerequisite structure from not only document text, but also crowd-generated data such as hyperlinks and edit logs. While it is not immediately obvious that this task is feasible, our experiments suggest that relatively reliable features to predict prerequisite structure exist, and can be successfully combined using standard machine learning methods. To achieve the broader goal of facilitating comprehension, predicting prerequisite structure is not enough. Another important subproblem is using predicted prerequisites to build a feasible plan. As part of ongoing work, we are exploring use of modern optimization methods (such as Integer Linear Programming) to compute \"reading plans\" that minimize a weighted linear combination of expected user effort and probability of plan \"failure\" 8 . We also plan to explore another major subproblem associated with facilitating comprehensionpersonalizing a reading plan. Clearly, even if d is a prerequisite for d, a user interested in d need not first read a page explaining d , if she already understands d ; instead, a reading plan based on prerequisite structure should be adjusted based on what is believed about the user's prior knowledge state. In the context of Wikipedia comprehension, one possible signal for predicting an individuals' prior knowledge is the Wikipedia edit log: if we assume that editors tend to edit things they know, the edit log indicates which concepts tend to be jointly known, and hence collaborative-filtering methods might be able to more completely predict a user's knowledge given partial information about her knowledge-just as collaborative-filtering is often used now to extrapolate user preference's from knowledge of others' joint preferences. Besides contributing to the goal of facilitating comprehension, we believe that the specific problem of predicting prerequisite structure in Wikipedia is a task of substantial independent interest. Prerequisite structure can be thought of as a sort of explanatory discourse structure, which is overlaid on a hyperlink graph; hence, scaling up our methods and applying them to all of Wikipedia would identify a canonical broad-coverage instance of such explanatory discourse. This could be re-used for other tasks much as lexical resources like WordNet are: for instance, consider identifying explanatory discourse in an external technical text (e.g., a textbook) by soft-matching it to the Wikipedia structure, using existing techniques to match the external text to Wikipedia (Agrawal et al., 2010; Mihalcea and Csomai, 2007; Milne and Witten, 2008) . Acknowledgments This research has been supported in part by DARPA (under contract number FA8750-09-C-0179), and Google. Any opinions, findings, conclusions and recommendations expressed in this paper are the authors' and do not necessarily reflect those of the sponsors. We are thankful to the anonymous reviewers for their constructive comments",
    "funding": {
        "military": 0.9996846064470069,
        "corporate": 0.40733385987215487,
        "research agency": 0.00014894136713305972,
        "foundation": 5.51223498068687e-07,
        "none": 1.8624621656027074e-06
    }
}