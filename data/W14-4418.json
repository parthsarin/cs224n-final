{
    "article": "In this paper we describe a method for generating a procedural text given its flow graph representation. Our main idea is to automatically collect sentence skeletons from real texts by replacing the important word sequences with their type labels to form a skeleton pool. The experimental results showed that our method is feasible and has a potential to generate natural sentences. Introduction Along with computers penetrating in our daily life, the needs for the natural language generation (NLG) technology are increasing more and more. If computers understand both the meaning of a procedural text and the progression status, they can suggest us what to do next. In such situation they can show sentences describing the next instruction on a display or speak it. On this background we propose a method for generating instruction texts from a flow graph representation for a series of procedures. Among various genres of procedural texts, we choose cooking recipes, because they are one of the most familiar procedural texts for the public. In addition, a computerized help system proposed by Hashimoto et al. (2008) called smart kitchen is becoming more and more realistic. Thus we try to generate cooking procedural texts from a formal representation for a series of preparation instructions of a dish. As the formal representation, we adopt the flow graph representation (Hamada et al., 2000; Mori et al., 2014) , in which the vertices and the arcs correspond to important objects \u2021 His current affiliation is Cybozu Inc., Koraku 1-4-14, Bunkyo, Tokyo, Japan. or actions in cooking and relationships among them, respectively. We use the flow graphs as the input and the text parts as the references for evaluation. Our generation method first automatically compiles a set of templates, which we call the skeleton pool, from a huge number of real procedural sentences. Then it decomposes the input flow graph into a sequence of subtrees that are suitable for a sentence. Finally it converts subtrees into natural language sentences. Recipe Flow Graph Corpus The input of our LNG system is the meaning representation (Mori et al., 2014) for cooking instructions in a recipe. A recipe consists of three parts: a title, an ingredient list, and sentences describing cooking instructions (see Figure 1 ). The meaning of the instruction sentences is represented by a directed acyclic graph (DAG) with a root (the final dish) as shown in Figure 2 . Its vertices have a pair of an important word sequence in the recipe and its type called a recipe named entity (NE) 1 . And its arcs denote relationships between them. The arcs are also classified into some types. In this paper, however, we do not use arc types for text generation, because we want our system to be capable of generating sentences from flow graphs output by an automatic video recognition system 2 or those drawn by internet users. Each vertex of a flow graph has an NE composed of a word sequence in the text and its type such as food, tool, action, etc. Recipe Text Generation The problem in this paper is generating a procedural text for cooking (ex. Figure 1 ) from a recipe flow graph (ex. Figure 2 ). Our method is decomposed into two modules. In this section, we explain them in detail. Skeleton Pool Compilation Before the run time, we first prepare a skeleton pool. A skeleton pool is a collection of skeleton sentences, or skeletons for short, and a skeleton is a sentence in which NEs have been replaced with NE tags. The skeletons are similar to the so-called templates and the main difference is that the skeletons are automatically converted from real sentences. The following is the process to prepare a skeleton pool. 1. Crawl cooking procedural sentences from recipe sites. 2. Segment sentences into words by a word segmenter KyTea (Neubig et al., 2011) . Then recognize recipe NEs by an NE recognizer PWNER (Mori et al., 2012) . 3. Replace the NE instances in the sentences with NE tags.  We store skeletons with a key which is the sequence of the NE tags in the order of their occurrence. Sentence Planning Our sentence planner produces a sequence of subtrees each of which corresponds to a sentence. There are two conditions. Cond. 1 Each subtree has an Ac as its root. Cond. 2 Every vertex is included in at least one subtree. As a strategy for enumerating subtrees given a flow graph, we choose the following algorithm. 1. search for an Ac vertex by the depth first search (DFS), 2. each time it finds an Ac, return the largest subtree which has an Ac as its root and contains only unvisited vertices. 3. set the visited-mark to the vertices contained in the returned subtree, 4. go back to 1 unless all the vertices are marked as visited. In DFS, we choose a child vertex randomly because a recipe flow graph is unordered. Evaluation We conducted experiments generating texts from flow graphs. In this section, we report the coverage and the sentence quality. Experimental Settings The recipe flow graph corpus (Mori et al., 2014) contains 200 recipes. We randomly selected 40 flow graphs as the test data from which we generate texts. The other 160 recipes were used to train the NE recognizer PWNER (Mori et al., 2012) with 200 more recipes that we annotated with NE tags. To compile the skeleton pool we crawled 100,000 recipes containing 713,524 sentences (see Table 1 ). Skeleton Pool Coverage First we counted the numbers of the skeletons that matches with a subtree (Step 1 in Subsection 3.3) for all the subtrees in the test set by changing the number of the recipe sentences used for the skeleton pool compilation. Table 2 shows the numbers of subtrees that do not have any matching skeleton in the pool (uncovered subtrees) and the average number of skeletons in the pool for a subtree. From the results shown in the table we can say that when we use 100,000 recipes for the skeleton compilation, our method can generate a sentence for 98.4% subtrees. And the table says that we can halve the number of uncovered subtrees by using about four times more sentences. The average number of the skeletons says that we have enough skeletons in average to try more sophisticated scoring functions. Text Quality To measure the quality of generated texts, we first calculated the BLEU (N = 4) (Papineni et al., 2002) with taking the original recipe texts as the references. The unit in our case is a sequence of sentences for a dish. Table 2 shows the average BLEU for all the test set. The result says that the more sentences we use for the skeleton pool compilation, the better the generated sentences become. The absolute BLEU score, however, does not tell much about the quality of generated texts. As it is well known, we can sometimes change the instruction order in dish preparation. Therefore we conducted a subjective evaluation in addition. We asked four evaluators to read 10 texts generated from 10 flow graphs and answer the following questions. Q1. How many ungrammatical two-word sequences does the text contain? Q2. How many ambiguous wordings do you find in the text? Then we show the evaluators the original recipe text and asked the following question. Q3. Will the dish be the same as the original recipe when you cook according to the generated text? Choose the one among 5: completely, 4: almost, 3: partly, 2: different, or 1: unexecutable. Table 4 shows the result. The generated texts contain 14.5 sentences in average. The answers to Q1 tell that there are many grammatical errors. We need some mechanism that selects more appropriate skeletons. The number of ambiguous wordings, however, is very low. The reason is that the important words are given along with the subtrees. The average of the answer to Q3 is 3.05. This result says that the dish will be partly the same as the original recipe. There is a room for improvement. Finally, let us take a look at the correlation of the result of three Qs with BLEU. The numbers of grammatical errors, i.e. the answers to Q1, has a stronger correlation with BLEU than those of Q2 asking the semantic quality. These are consistent with the intuition. The answer to Q3, asking overall text quality, has the strongest correlation with BLEU on average among all the questions. Therefore we can say that for the time being the objective evaluation by BLEU is sufficient to measure the performance of various improvements. Related Work Our method can be seen a member of template-based text generation systems (Reiter, 1995) . Contrary to the ordinary template-based approach, our method first automatically compiles a set of templates, which we call skeleton pool, by running an NE tagger on the real texts. This allows us to cope with the coverage problem with keeping the advantage of the template-based approach, ability to prevent from generating incomprehensible sentence structures. The main contribution of this paper is to use an accurate NE tagger to convert sentences into skeletons, to show the coverages of the skeleton pool, and to evaluate the method in a realistic situation. Among many applications of our method, a concrete one is the smart kitchen (Hashimoto et al., 2008) , a computerized cooking help system which watches over the chef by the computer vision (CV) technologies etc. and suggests the chef the next action to be taken or a good way of doing it in a casual manner. In this application, the text generation module make a sentence from a subtree specified by the process supervision module. There are some other interesting applications: a help system for internet users to write good sentences, machine translation of a recipe in a different language represented as a flow graph, or automatic recipe generation from a cooking video based on CV and NLP researches such as (Regneri et al., 2013; Yamakata et al., 2013; Yu and Siskind, 2013) . Conclusion In this paper, we explained and evaluated our method for generating a procedural text from a flow graph representation. The experimental results showed that our method is feasible especially when we have huge number of real sentences and that some more sophistications are possible to generate more natural sentences. Acknowledgments This work was supported by JSPS Grantsin-Aid for Scientific Research Grant Numbers 26280084, 24240030, and 26280039.",
    "abstract": "In this paper we describe a method for generating a procedural text given its flow graph representation. Our main idea is to automatically collect sentence skeletons from real texts by replacing the important word sequences with their type labels to form a skeleton pool. The experimental results showed that our method is feasible and has a potential to generate natural sentences.",
    "countries": [
        "Japan"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "16",
    "year": "2014",
    "month": "June",
    "title": "{F}low{G}raph2{T}ext: Automatic Sentence Skeleton Compilation for Procedural Text Generation"
}