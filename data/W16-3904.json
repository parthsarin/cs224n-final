{
    "article": "In this paper we present a series of experiments on discriminating between private and corporate accounts on Twitter. We define features based on Twitter metadata, morphosyntactic tags and surface forms, showing that the simple bag-of-words model achieves single best results that can, however, be improved by building a weighted soft ensemble of classifiers based on each feature type. Investigating the time and language dependence of each feature type delivers quite unexpecting results showing that features based on metadata are neither time-nor language-insensitive as the way the two user groups use the social network varies heavily through time and space. Introduction Due to popularity of Twitter and its proactive content harvesting policy, automatic identification of latent user attributes has become a hot research topic in the past couple of years. Tasks range from discriminating between different types of users (Mislove et al., 2011) , behaviour (Pennacchiotti and Popescu, 2011; Rao et al., 2010) , location (Hecht et al., 2011) , gender (Burger et al., 2011) , age (Nguyen et al., 2013) , occupation (Hu et al., 2016) , social class (Borges et al., 2014) and personality type (Quercia et al., 2011) . In this paper we present a series of experiments on discriminating between corporate and private accounts on Twitter. We investigate language-independent features, features extracted from morphosyntactic annotations and simple bag-of-words features. We additionally investigate the time and space dependence of specific feature sets by measuring how well specific features handle training on one time span or closely-related language and testing on another time span or other closely-related language. Related work Automatic classification of different types of users is an active area of research and has been attempted at various degrees of granularity and for diferent communicative goals. Naaman et al. (2010) , for example, distinguish two types of users: those who disseminate information (informers) and those who share what they are doing and how they are feeling (meformers). De Choudhury et al. (2012) , on the other hand, distinguish between the accounts of individual persons, organizations, news media and other miscellaneous categories. Kim et al. (2010) use the same set, only adding the celebrities category, while (Wu et al., 2011) , suggest a two-stage approach where they first classify users into two broad categories of elite and ordinary users, and then further classify the elite users into more fine-grained groups of media, celebrities, organizations and bloggers. Similarly different both in terms of number and complexity are the features used for classification. Rao et al. (2010) use 2 categories of over 20 features that are easy to obtain from the Twitter timeline: the network structure (follower-following ratio, follower frequency, following frequency) and the communication behavior of individuals (response frequency, retweet frequency, tweet frequency), showing that tweeting behavior information is not useful for most classification tasks. Therefore, in addition to profile features, tweeting behaviour and social network, (Pennacchiotti and Popescu, 2011) , show that, rich linguistic features (prototypical words and hashtags, generic and domain-specific LDA, sentiment words) yields consistently better results. Kucukyilmaz et al. (2008) employ a more traditional apparoach to author characterization by relying on a set of term-based features in users' messages and a set of stylistic features (character usage, message length, word length, punctuation usage, punctuation marks, stopword usage, stopwords, smiley usage, smileys, vocabulary richness). While our work has noticeable overlap with existing research that was carried out on English, it extends that research, beyond the language shift, by (1) performing a more detailed analysis of a broader list of features, and inspecting (2) the stability of those features in different time periods and (3) their portability to a closely-related language. Dataset The dataset used in this paper consists of 7.5 million Slovene tweets collected from June 2013 to January 2016 with the TweetCaT tool (Ljube\u0161i\u0107 et al., 2014) . Each of the 7778 users in the collection was manually annotated as private or corporate. We performed our experiments on 5842 users for whom we had at least 100 tweets at our disposal. Out of these 5842 users, 4382 were annotated as private and 1460 as corporate. Features We discriminate between two basic types of features: language-independent (LI) and languagedependent (LD) ones. We calculate our language-independent features both from the text of a tweet and its additional metadata. We use two types of language-dependent features: focused (FLD) and bagof-words (BoW) features. While the bag-of-words features are simply types found in the text of all the training tweets, the focused ones are mostly based on morphosyntactic annotation of the text. It should be stressed here that we do not pressupose the language-independent features to work well across languages for our task, but just that these features can be found in different languages. All the features are calculated on user level as we perform our predictions on each Twitter user. Language independent features Among the language-independent features we discriminate between the following feature types: \u2022 avg -average number of tweets satisfying a condition \u2022 mean -mean of a continuous tweet-level variable \u2022 med -median of a continuous tweet-level variable \u2022 var -variance of a continuous tweet-level variable \u2022 user -user-level metadata In Table 1 we give an overview of all language-independent features, sorted in descending order by the p-value obtained with the two-sided Mann-Whitney U test (Mann and Whitney, 1947) on the variable in question regarding the distribution in private and corporate accounts. The null hypothesis of this test is that the two samples (variable measurements on private and corporate accounts) come from the same distribution. Beside the feature description and its univariate p-value, we report the effect size as the difference in the median between private and corporate accounts. The variables reporting a positive effect size, like the average_inreply variable, therefore have a higher median in private tweets while the variables with a negative effect size, like the average_http variable, have a higher median among corporate tweets. Our univariate feature analysis shows, among many other things, that private accounts are most specific for replying to tweets, mentioning other users, favoring other user's tweets, tweeting in various hours of the day, producing tweets of variable length and posting more frequently during weekends. On the other 2 : An overview of the language-dependent focused features, sorted by their p-value hand, corporate accounts use more URLs, post more during working hours, produce longer tweets and have a higher followers / friends ratio. All the engineered features, except for the median of number of favorites (because it is most frequently 0) and the average number of truncated tweets (most frequently 0 as well) have shown to be statistically significant with p < .05. Language-dependent features In Table 2 we give an overview of the focused language-dependent features, with a statistical analysis identical to the one of the language-independent features. All the variables are user-level probabilities of specific linguistic phenomena. Again, the effect size is the difference in the median between private and corporate accounts, i.e. a positive effect size points at higher values among private users. This univariate analysis shows, among other things, for private accounts to use more singular / dual demonstrative pronouns, singular / dual verbs, negative pronouns and supines (short verb infinitive forms). On the other hand, corporate accounts use more adjectives, verbs in plural, numbers, superlatives and content words. In Table 3 we give an overview of the 20 strongest word features identified through the coefficent value of the linear discriminative classifier that will be described in the Experiments section. As the table shows, private users prefer pronouns and verbs in 1st person singular (I, me, my, I did, I wish, I hope), nouns and verbs depicting thoughts and activities carried out in their spare time (wish, grill, slim), while 3rd and 2nd person plural forms (we, our, we did, we can, you check, we congratulate), nouns describing sem(did-1st prs sg) i moj(my) cerar http://t.co/h7jwauc0rv me my bravo \u017eelim(wish-1st prs sg) zalogi(stock) mesto(city) upam(hope-1st prs sg) zoya #delajvitko(#workslim) ko(when) http://t.co/qcfz6ii342 pod(under) salesforce \u017earu(grill) @delo tv zaradi(because) smo(did-3rd prs pl) teden(week) klik(click) \u010destitamo(congratulations) evrov(euro) tf2 pi\u0161e(writes) festival preveri(check-2nd prs sg) we people ste(did-2nd prs pl) na\u0161(our) kamala #volitve14(#election14) lahko(can) novo(new) akcija(sale) facebook Experiments We run three batches of experiments. In the first batch we experiment with the three types of features presented in the previous section. Additionally, we build an ensemble classifier which uses the output of the three feature-type classifiers. In the second batch we investigate the time sensitivity of the presented feature types by training on data from one time span and testing on a later one. Finally, in the third batch we investigate the portability of two feature types, namely language-independent features and bag-ofwords features, to a closely-related language. As our weak baseline we use the most-frequent-class (MFC) baseline. Our classifier of choice on all feature types are support vector machines (SVM). In case where the number of features is smaller than the number of instances we use the RBF kernel while in the opposite case we use a linear kernel. In all the experiments we 5-fold with stratification over all our data, optimising in each iteration the classifier via grid search on the development data and evaluating it on the test data via weighted F1. In case of the RBF kernel we optimise the C (2 n , n \u2208 {\u22125, \u22123, \u22121, ...15}) and \u03b3 (2 n , n \u2208 {\u221215, \u221213, \u221211, ...3}) hyperparameters while for the linear kernel we optimise the C parameter in the same range as with the RBF kernel. Feature type comparison We present the results of training a classifier on each type of features, the language independent (LI), focused language dependent (FLD) and bag-of-words (BoW), in Table 4 . We additionally compare these classifiers to our weak most-frequent-class (MFC) baseline and an weighted soft ensemble of all three classifier outputs. The weighted soft ensemble classifier uses the per-class probability output of each classifier, weights each class distribution with the weighted F1 of that classifier estimated on the development data, averages over all the distributions and picks the class with maximum probability. We report F1 on each class and an overall weighted F1. We additionally report a p-value obtained through the approximate randomisation test (Yeh, 2000) with R = 1000 for each classifier combination. Approximate randomisation estimates the p-value as p = r R where r is the number of iterations where randomly switching responses of system 1 and system 2 has produced a greater or equal difference in the evaluation metric than the original difference between system 1 and system 2, while R is the number of iterations over the responses. The results show that each of the feature types improves over the weak MFC baseline by a large margin. While all the three feature types come quite close to each other, the weakest one are the languageindependent features, mostly relying on tweet and user metadata. We find it quite surprising that the bagof-words model improves over the focused language-dependent model which uses a language abstraction in form of morphosyntactic descriptions. 1 Finally, the weighted soft ensemble improves over each of the classifiers. 2 The difference between each classifier combination has proven to be statistically significant. The performance of each classifier on specific classes did not yield any surprises. The private class, which is more represented, performs regularly better. The best-performing ensemble classifier yields weighted F1 of 0.89 on the corporate and 0.96 on the private class. Regarding the number of features, the single best performing classifier is also the heaviest one with 3,978,030 features. On the other hand the LI classifier works with 38 features, while the FLD classifier works with 27 features. Time sensitivity In this set of experiments we investigate the impact of training on one time span and evaluating on another one. We again experiment with our three types of features. In each iteration of cross-validation, the development set consists only of tweets published before 2015, removing all users for which after filtering we do not have 100 tweets available. We repeat the same process on the test set by keeping only tweets published 2015 onward. Along with our new time dependence evaluation results, we repeat the results from the previous batch of experiments which presents the performance of the feature type in the same time span. The results of this experimental batch are presented in the left hand side of Table 5 . The results are somewhat surprising showing that the LI features are actually the least time insensitive. Probably due to the way the Twitter social network was used before and after 2015 the loss reaches 7% of weighted F1. On the other hand, the FLD and BoW classifiers measure a minor loss of 1%. Portability to a similar language We perform a final set of experiments investigating the portability of the language independent (LI) and the bag-of-words (BoW) features to a closely related language. As our closely related language we chose Croatian as the two countries are strongly interconnected, expecting therefore that Twitter is used in a comparable manner and that a usable overlap in surface forms among the two closely-related languages should occur. This lexical overlap has already been used in previous work on bilingual lexicon extraction (Fi\u0161er and Ljube\u0161i\u0107, 2011) and identification of false friends (Ljube\u0161i\u0107 and Fi\u0161er, 2013) . We discard the focused language dependent (LFD) features from these experiments as they require the texts of the tweets to be morphosyntactically annotated with the same tagset which would require a tagset mapping. In these experiments we use the full Slovene data for development and evaluate on a set of 101 Croatian users that were annotated by hand, using the same criteria applied while annotating Slovene data. In the Croatian dataset 50 users were annotated as private and 51 as corporate. The results are presented in the right hand side of Table 5 . While both classifiers outperform the weak baseline by a wide margin, interestingly, the language independent features prove to be the less stable feature type when a shift in the target domain / language occurs. While, naturally, the BoW feature set experiences a heavy loss of 22% when evaluating on Croatian data, the loss of 30% on the LI features is even greater, in addition to the initial lower results. Error analysis We conclude the experiments with an error analysis of the instances misclassified by the enseble classifier. Among those instances we specifically analyse the cases where the classifier based on languageindependent features (LI) and the bag-of-words classifier (BoW) disagree. We performed a detailed qualitative analysis of the tweeting behaviour and language of 10 random accounts per class misclassified by each system in order to gain more insight into why the misclassification might have occured. In total, 4 scenarios were analysed: (1a) corporate and (1b) private accounts misclassified by LI but correctly classified by BoW, and (2a) corporate and (2b) private accounts misclassified by BoW but correctly classified by LI. As far as the corporate accounts misclassified by the LI approach but correctly classified by the BoW approach (1a) are concerned, they were smaller / early-stage societies / NGOs / civil initiatives, popular radio stations, records companies, event agencies and bloggers who (probably deliberately) share many tweeting characteristics of private accounts in terms of tweeting amount, time, retweets, mentions, favourites and hashtags. Among the private accounts misclassified by the LI approach (1b), 9 out of 10 examined accounts belong to journalists, politicians and professionals who do not explicitly disclose their professional status or affiliation in their account profiles but nevertheless tweet for professional purposes and demonstrate all the behaviour of the corporate accounts. This suggests that in addition to straightforward binary categories, there is a grey zone of borderline, difficult to define account types which call for more detailed annotation guidelines. Interestingly, there was only one corporate account that was misclassified by the BoW approach but correctly classified by the LI one (2a), which is a fair-trade store that raises awareness for fair-trade home products, clothes and food in a communicative, conversational, friendly way. The topics covered and vocabulary used is therefore closely related to our every-day lives, not unlike the topics and vocabulary present in the tweets published by private accounts. Similar results were obtain with the qualitative analysis of 10 private accounts erroneously classified by BoW but correctly by LI (2b). They show that 9 of them are borderline because they are accounts of journalists, managers and small business owners whose tweets, published with the typical corporate dynamics, cover topics conveyed by the linguistic features that are tipically associated with people's private lives, such as health and lifestyle, food, cosmetics, hairstyle, sports and charity. The error analysis clearly shows that the erroneously classified accounts are indeed outliers, either by tweeting behaviour, or by tweeting content and / or language style. This suggests that manual annotation guidelines should be refined to include advice how to treat such potentially difficult cases and / or extend the classification beyond a binary approach. Conclusion In this paper we have presented a series of experiments on discriminating between private and corporate Twitter accounts among Slovene Twitter users. In the first part of the paper we have analysed a series of features, casting some light on the way the two groups use our medium of interest. In our classification experiments we have shown that the simple bag-of-words model outperforms the language-independent and focused language-dependent features. Building a classifier ensemble yielded a statistically significant improvement, showing that these feature sets are at least partially complementary. We have shown that language-independent features based mostly on Twitter metadata are actually very unreliable, showing bad performance both when different time spans or different languages are used for training and evaluation. A big surprise was that the bag-of-words model was more portable to a closely related language than the feature set based on metadata. These results point at significant differences in the way the social network is used across time and space. Overall we have achieved very good results when combining all classifiers in a weighted soft ensemble. Given that most researchers are interested in removing corporate accounts from their Twitter collections, we can expect that our classifier will be very useful for Slovene as F1 on the private class is more than 96%.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 5.51223498068687e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the conducted research. Therefore, based on the provided text, it appears there was no disclosed funding."
}