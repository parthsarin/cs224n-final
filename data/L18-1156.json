{
    "article": "Distributional hypothesis has been playing a central role in statistical NLP. Recently, however, its limitation in incorporating perceptual and empirical knowledge is noted, eliciting a field of perceptually grounded computational semantics. Typical sources of features in such a research are image datasets, where images are accompanied by linguistic tags and/or descriptions. Mainstream approaches employ machine learning techniques to integrate/combine visual features with linguistic features. In contrast to or supplementing these approaches, this study assesses the effectiveness of social image tags in generating word embeddings, and argues that these generated representations exhibit somewhat different and favorable behaviors from corpus-originated representations. More specifically, we generated word embeddings by using image tags obtained from a large social image dataset YFCC100M, which collects Flickr images and the associated tags. We evaluated the efficacy of generated word embeddings with standard semantic similarity/relatedness tasks, which showed that comparable performances with corpus-originated word embeddings were attained. These results further suggest that the generated embeddings could be effective in discriminating synonyms and antonyms, which has been an issue in distributional hypothesis-based approaches. In summary, social image tags can be utilized as yet another source of visually enforced features, provided the amount of available tags is large enough. Introduction Virtually, all the methods for generating distributional/distributed word representations (Baroni et al., 2014) rely on the notion of distributional hypothesis (Firth, 1957) . These approaches enable word representations to properly capture the distributional hypothesis by measuring the commonality of the linguistic contexts of word occurrences. Although these approaches are proven effective in various semantic tasks, they are limited in terms of the incorporation of perceptual and empirical knowledge: perceptually or empirically obvious objects have not been necessarily well verbalized in a corpus of written texts (Bruni et al., 2014 ). Yet another issue with the distributional hypothesis-based methods is that they often run into trouble when discriminating synonyms from antonyms or more vaguely related words (Hill et al., 2015) . Recently, motivated by these issues, several research works that try to incorporate human perceptual/empirical knowledge into linguistically derived representations have emerged. Most typically, such approaches combine/integrate visual features achieved from visual resources with linguistic features (word embeddings) by applying machine learning/deep learning techniques. To enable this line of research, a visual resource in which an image is accompanied by linguistic descriptions is generally required. Although these methods compensated/improved purely linguistic representations, the source of visual features cannot be limited to image data. That is, if a content of any modality is described with a substantial amount of linguistic tags and/or descriptions, the linguistic co-occurrence observed around the content can be utilized as a source of semantic features. Once resting on this notion, the so-called social media can be exploited as an attractive resource. When using a social media service, the user assigns tags to her/his contents so that they may be easily searched and located by other users. Sometimes, this process is referred to as folksonomy, as the tags are not constrained by pre-defined controlled keywords and/or ontology terms. Despite the nature that users can freely choose tags, it is exemplified that the vocabulary of tags in a social media service has converged and become stabilized over time (Halpin et al., 2007) . Moreover, if a target social media is popular enough and maintains a huge amount of content, the set of tags can be considered as a type of corpus where a similar set of tags would be assigned to similar content. These facts validate the use of social tags as a source of semantic features. Furthermore, the media type of social media content is not necessarily limited to images, admitting the possibility of incorporating other types of modality. In the present work, we utilize the YFCC100M (Thomee et al., 2016 ) dataset 1 , which is a social media-originated dataset. We generate word embeddings by statistically processing the co-occurrences of linguistic tags. The empirical results of semantic similarity/relatedness tasks may allow us to conclude that social image tags can be utilized as yet another source of visually enforced features, provided the amount of available image tags is large enough. Related work The present research is inspired by the work on multimodal semantic representations (section 2.1). As most of the work in this direction deals with image features, image datasets (section 2.2) as a source of visual features are of crucial importance. Multimodal semantic representation Theoretically supported by the concept of grounded cognition (Barsalou, 2008) and technically endorsed by the progress of machine learning techniques, work on distributed word representation (word embeddings) has extended its research scope to multimodal semantic representation in which perceptual information, such as visual features, is combined with or integrated into corpus-derived linguistic embeddings (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela and Bottou, 2014; Kiela et al., 2016) . Mainstream approaches employ deep learning techniques to integrate/combine visual features with linguistic features (Lazaridou et al., 2015; Kodirov et al., 2017; Hasegawa et al., 2017) . The achieved results in standard semantic similarity/relatedness tasks are generally promising, suggesting that corpus-derived word embeddings can be successfully enhanced by visual features. Source of image/visual features As far as a method for inducing multimodal semantic representation relies on image features, the role of the source image dataset is crucial. Image datasets can be classified in terms of the different types of collected images, linguistic annotations, and originators (who tagged images). Table 1 contrasts representative image datasets with YFCC100M, which is the central ingredient of the present work. ImageNet (Krizhevsky et al., 2012) has been playing a leading role in improving visual object recognition techniques. The ESP-Game (von Ahn and Dabbish, 2004) dataset is often employed in the work on multimodal semantic representations. These two datasets are contrastive in a sense: ImageNet images clearly portray a focused object, whereas ESP-Game images often depict more natural scenes, showing multiple objects and the relations among them. This means that the ESP-Game images are noisier in terms of visual object recognition (Kiela et al., 2016) . MS COCO (Lin et al., 2014) , however, has been heavily employed in caption generation research. YFCC100M (Thomee et al., 2016) collects images and the associated metadata from a social media service Flickr, which is a Web-based service for sharing visual contents. This dataset is different from others in that the tags attached to a posted image is given by the contributor. This nature makes a difference when it is utilized as a source of semantic features, as discussed in the rest of this paper. In Flickr, each image is annotated with a variety of tags, including the name of a depicted object, the place where the picture is taken, and the emotional feeling expressed by the contributor. The amount of data made possible by the popularity of Flickr is also a crucial factor; Thus meaningful co-occurrence statistics can be collectively obtained from this huge dataset. Generating word embeddings from social media data As described earlier, we aim to construct word semantic representations (word embeddings) by exploiting a social media service as a source of visually enforced semantic features. More specifically, we generate word embeddings, first by constructing a tag co-occurrence matrix, and then converting the raw counts to more effective quantities, and finally applying a dimensionality reduction technique to the co-occurrence matrix. It should be emphasized here that we only employ textual tags, meaning that we have never applied any visual feature extraction to the maintained images. This process assigns each tag word a dense and lowdimensional vector, which can be utilized as a word embedding vector. The rationale behind this approach is that visual cooccurrences of objects could be naturally captured by the co-occurrence of image tags. We further suppose that the intention of a contributor who wants to disseminate her/his photo to a broader audience may be reflected in the attached tags. Therefore, the tags attached to an image can be considered as a proxy to the image that may partake social implications. Constructing a tag word co-occurrence matrix: We constitute a tag word co-occurrence matrix M , where M i,j counts the number of times that tag word w i and tag word w j are attached to the same image. The shape of the matrix M is N \u00d7 N if the number of word types equals to N . Transforming the matrix: As the raw counts do not properly dictate the strength of co-occurrence, we transform the co-occurrence matrix by computing positive pairwise mutual information (PPMI) (Church and Hanks, 1990) , which is formulated as follows. M i,j = max ( 0, log P (w i , w j ) P (w i )P (w j ) ) As this formulation suggests, PPMI alleviates the influence of high frequency tags, allowing us to properly measure the strength of tag co-occurrence. Dimensionality reduction: As the number of tag types easily increases with the size of a dataset, the matrix M would generally be sparse. We thus apply singular value decomposition (SVD), and reduce the matrix from N \u00d7 N to d \u00d7 d, where d \u226a N . M = U \u2022 \u03a3 \u2022 V T (2) \u2248 U (d) \u2022 \u03a3 (d) \u2022 V T (d) (3) This gives us the word embedding vector for word w t as d) . As frequently argued, dense and lowdimensional representations may yield the benefit of data reduction as well as the effect of data abstraction. v wt = U (d) wt \u2022 \u221a \u03a3 ( Experimental settings We evaluate the efficacy of tag-originated word embeddings (henceforth, tag embeddings) in standard semantic similarity/relatedness tasks. This section describes the experimental settings, and the following section discusses the results. Source of social image tags We used the Yahoo Flickr Creative Commons 100M (YFCC100M) dataset (Thomee et al., 2016) . This dataset collects almost 100 M images and approximately 700 K movies posted on Flickr. Each image is described by a set of metadata, including image titles, user-generated tags, and machine-generated tags. We constructed word semantic representations by feeding only the user-generated tags (tags that are given by the contributors of contents) to the process described in the previous section. The number of user-generated tags amounts to approximately 69M, among which 68.5M tags are assigned to images, and the rest 420K are assigned to movies. In average, approximately seven tags are assigned to each instance of the content. Figures 1 (a Figure 1 (a) portrays a cat, which is further detailed by the hyponyms \"kitten\" and \"kitty,\" as well as the hypernym \"pet.\" Figure 1 (b) artistically shows a scenery for which abstract words like \"calm\" and \"quiet\" are attached. Moreover tag words like \"summer\" (time) or \"favignana\" (place) are being assigned, which would not be annotated even by state-of-the-art computer vision techniques. In the experiments, a word co-occurrence matrix was constructed for the selected 20,943 words that were used for describing more than 1,500 images. We excluded multiword tags and numbers, and the remaining words were converted to lowercase. The total number of tag instances counts at a value of 10 M. In the dimensionality reduction by SVM, the dimensionality d of word embeddings is set to 300. Evaluation tasks and the datasets We evaluated the efficacy of constructed word embeddings with word similarity/relatedness tasks in which the predicted scores were compared against the gold data given in the following test datasets. The Spearman's rank correlation coefficient was employed as the performance measure of the experiment that uses one of the datasets. \u2022 YP130 (Yang and Powers, 2006) : This dataset that maintains 130 verb pairs was built for the evaluation of verb similarities. \u2022 WordSim353 (Finkelstein et al., 2002) : This dataset contains 353 word pairs for which semantic relatedness scores are assigned. Note that semantic similarity that essentially measures the degree of synonymy can be considered as a subclass of semantic relatedness. \u2022 SimLex999 (Hill et al., 2015) : SimLex999 provides word similarity (rather than relatedness or association) judgments for 999 word pairs. Note that the parts of speech of compared words are always the same. \u2022 USF Assoc (Nelson et al., 2004) : This dataset, University of South Florida Free Association Norms (abbreviated as USF Assoc), collects the free association scores for 5,019 stimulus words. In the experiments, we used the pairs of words included in the SimLex999 dataset. Needless to say, free association relations include a wider range of semantic relationships. \u2022 MEN (Bruni et al., 2014) : This dataset presents semantic relatedness scores for 3,000 word pairs. This dataset was specially made to evaluate multimodal representations. The parts of speech of compared words are not necessarily the same. The words are biased to concrete concepts, as they are chosen from the tags in the ESP-Game and Flickr data. \u2022 SemSim / VisSim (Silberer et al., 2016) : This is a dataset of 7,576 word pairs, each of which is annotated using not only semantic similarities (SemSim) but also visual similarities (VisSim); therefore, the user can compare the performances of her/his model in predicting different types of similarities. Experimental results Major results Table 2 compares the major experimental results (in Spearman's correlations), where the YFCC column shows the results with the tag embeddings that were generated from the tag co-occurrence matrix which records 10 M tag instances. Wiki or GNews displays the results with corpusderived word embeddings. By applying the Word2Vec Skip-Gram model, we derived 300-dimensional word embeddings both for Wikipedia 2009 dump 2 (Wiki) and GoogleNews 3 (GNews). Notice that the dimensionalities are equalized with those of tag embeddings. As shown in the table, the tag embeddings achieved the highest correlation of 0.81 in the MEN relatedness task, demonstrating that social image tags are good sources of visually enforced features for concrete concepts. Furthermore, the tag embeddings achieved an acceptable result of 0.45 in the SimLex999 similarity task, which is worse than GNews-originated embeddings, but better than Wikioriginated embeddings. This could be good news, as a similarity task is generally considered to be more difficult when compared to a relatedness task. The table presents that the degradations in USF Assoc score compared to that of SimLex999 are evident in all the embedding types. However, the difference in YFCC (tag embedding) is larger than the other two types. This may be due to the fact that the tags attached to an image are tightly associated with the image, whereas linguistic contexts, or context windows, are more generous to include weakly associated words. A surprise result is a correlation of 0.47 achieved by the tag embeddings in the YP130 verb similarity task. It could be unfortunately unreliable, as the coverage is as low as 16% (shown in the second column). This insists that verbs are not frequently assigned as a social image tag. In summary, the tag embeddings could achieve comparable performances with corpus-originated embeddings in a variety of similarity/relatedness tasks.   The amount of data versus performances: It is often desired to know the necessary/sufficient amount of data to achieve a reasonable performance. Figure 2 displays the saturation of correlation coefficients with the increase in the amount data. As the graph shows, the performance of all datasets does not significantly improve when increased to more than 10 M contents 4 , showing a limit to the effective number of tags. Do social image tags make a difference? An expectation to multimodal semantic representations is to address issues inherent to the purely linguistic distributional hypothesis. This expectation also applies to the tag embeddings proposed in the present work. To assess whether this could be attained, we conducted a small experiment by using WordNet semantic relationships. More specifically, for each of the selected 1,928 words that have tag embeddings, we retrieved k-nearest words in WordNet, and investigated the ranks of their antonyms, synonyms, hypernyms, and hyponyms.  The most prominent fact presented in the table is that the MRR for antonyms with YFCC embedding is far lower than that of the other two embedding types. This confirms that the proposed method could be effective in excluding antonyms from the other semantically similar/related words. Note that YFCC embedding ranked synonyms, hypernyms, and hyponyms are relatively higher than other two embedding types. This may endorse the fact that a content contributor tends to add hypernyms and/or hyponyms as tags, probably for the purpose of increasing the probability of the posted image being retrieved. To sum up, the resulting semantic representations exhibit somewhat different and favorable behaviors from corpusoriginated representations. Concluding remarks This paper proposed to exploit social image tags as a source of features for generating word embeddings, and demonstrated that the generated representations exhibit somewhat different and favorable behaviors compared to the corpusoriginated representations. These results highlight that social media could be exploited as yet another source of semantic features. This insight may open up a new way of meaning representation that optimally integrates verbal, perceptual, and social features upon a given semantic task. Other benefits potentially attained from the use of social media are dynamics and multilinguality. Social tagging would provide opportunities to capture new definitions for existing words or new words themselves. Tags given in multiple languages can be exploited to develop cross-lingual/multilingual semantic representations. Acknowledgments The present research was supported by JSPS KAKENHI Grant Number 17H01831 and 15K12873.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 1.9361263126072004e-07,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article explicitly mentions support from JSPS KAKENHI Grant Number 17H01831 and 15K12873. JSPS (Japan Society for the Promotion of Science) is a major government-funded organization in Japan that provides grants for scientific research, categorizing it as a research agency. There is no mention of funding from defense, corporate entities, foundations, or an indication that no funding was received."
}