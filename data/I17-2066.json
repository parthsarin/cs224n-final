{
    "article": "In dialogue systems, conveying understanding results of user utterances is important because it enables users to feel understood by the system. However, it is not clear what types of understanding results should be conveyed to users; some utterances may be offensive and some may be too commonsensical. In this paper, we explored the effect of conveying understanding results of user utterances in a chatoriented dialogue system by an experiment using human subjects. As a result, we found that only certain types of understanding results, such as those related to a user's permanent state, are effective to improve user satisfaction. This paper clarifies the types of understanding results that can be safely uttered by a system. Introduction Current dialogue systems often convey the understanding results of user utterances for confirmation and for showing understanding. Taskoriented dialogue systems repeat information provided by users by using understanding results of user utterances to confirm the content of user utterances (Litman and Silliman, 2004; Raux et al., 2005) . Chat-oriented dialogue systems also need to confirm the content of user utterances and to show understanding so that the systems can be more affective. However, some of the understanding results should not be conveyed to users. For instance, some utterances (e.g. \"You are stubborn.\") may be offensive and some (e.g. \"It is summer.\") may be too commonsensical. To create a dialogue system which conveys one's understanding results, we need to know what types of the results can be used as system utterances. In this paper, focusing on chat-oriented dialogue systems, we investigate the effects of conveying understanding results of user utterances. Specifically, we investigate the types of results that can be conveyed to users without lowering user satisfaction. For this purpose, we first prepared various types of understanding results. Then, by a subjective experiment, we examined their individual effects on user satisfaction. Note that, in this paper, we focus on the effects of system utterances that convey understanding results \"as they are\"; that is, utterances are literally the same as understanding results. As a result of the experiment, we found that user's temporary states during dialogue should not be conveyed and user's permanent states and information irrelevant to users themselves can be conveyed safely as system utterances. Our results are useful for creating a dialogue system that conveys understanding results. Data of Understanding Results For our investigation, we need to prepare understanding results categorized by their types. For this purpose, we use a corpus of PerceivedInfo collected in our previous work (Mitsuda et al., 2017) . In this corpus, user utterances in chat-oriented dialogue are associated with the information that can be perceived/inferred by humans from these utterances. Such information is called Perceived-Info (perceived information). Figure 1 shows an example of a chat-oriented dialogue and their PerceivedInfo in the corpus. As stimuli for collecting PerceivedInfo, a Japanese chat-oriented dialogue corpus (Higashinaka et al., 2014) was used. PerceivedInfo was written by multiple annotators using natural sentences with regard to each utterance in the dialogue. Table 1 shows the classification of Perceived-Info created in our previous work. These types were determined by manual clustering. The classification was evaluated by inter-annotator agreement among three annotators using 3,000 instances of PerceivedInfo with \"Level 3\" types. Fleiss' \u03ba showed substantial agreement (0.69), indicating the validity of the classification. In this work, we use the PerceivedInfo in this corpus as understanding results and investigate the effects of system utterances that convey Perceived-Info. We also investigate how the effects are different depending on the types of PerceivedInfo in the classification. Experiment Using PerceivedInfo, we evaluated the effects of system utterances conveying the understanding results in an experiment. Below, we explain the procedure to create the utterances for the experiment and how we evaluate them. Figure 2 shows the flow of preparation and evaluation. We first select pairs of PerceivedInfo and a user utterance used for writing that Perceived-Info from the corpus. The writers rewrite or refer Types of System Utterances Table 2 shows the four types of system utterances prepared for evaluation. Utterances from PerceivedInfo are compared with those of three other types; namely, \"Automatic,\" \"Repetition,\" and \"Human.\" \"PerceivedInfo\" is described below. Table 2: Types of system utterances prepared for evaluation. \"Example\" column shows system utterance when user utterance is \"I'll visit Mt. Fuji if I feel up to it.\" (utterance U 13 in Figure 1 ). PerceivedInfo This utterance simply conveys PerceivedInfo in the form of confirmation. The utterance ends with a tag question form to confirm the content of PerceivedInfo. Rewriting PerceivedInfo is done manually. The symbols A and B that indicate speakers are changed to \"You\" or \"I\". Types of Utterances for Comparison We prepared three other types of system utterances for comparison. \"Automatic\" emulates the utterance of a chat-oriented dialogue system that is currently available. \"Repetition\" represents a simple repetition of the content of a user utterance. \"Human\" is an utterance conceived by human. Automatic This utterance is an automatic response from a chat-oriented dialogue system that generates an utterance on the basis of keywords extracted from user utterances. To prepare responses, we use a Japanese chatoriented dialogue system by NTT DOCOMO (Onishi and Yoshimura, 2014) . Repetition This utterance is a repetition of a predicate argument structure in a user utterance (Higashinaka et al., 2014) . It ends with a tag question form (in Japanese, \"desu ne\") to show that the system understands the content of a user utterance. The utterance is manually created by extracting and rewriting a predicate argument structure from the user utterance. Human This utterance is a human-level utterance (i.e., upper bound). We prepare it by having writers manually write an appropriate response to a keyword in the user utterance. Writers are instructed to select their favorite keyword in the utterance and use it to create a response that would satisfy users. Preparation and Evaluation of Utterances To clarify the difference of effects caused by the types of PerceivedInfo, we randomly selected approximately the same number of Perceived-Info from each type in \"Level 3\" shown in Table 1 . In total, we prepared 500 instances of Perceived-Info; that is, 500 PerceivedInfo and user utterances associated with PerceivedInfo. Using the 500 PerceivedInfo and utterances, \"PerceivedInfo\" and \"Repetition\" were written by a single writer and two versions of \"Human\" were written by two writers (Writer 1 and Writer 2 ) independently. We evaluated both utterances of \"Human\" written by the writers, because the quality of \"Human\" may depend on the writer. \"Automatic\" were generated from the chat-oriented dialogue system by NTT DOCOMO using the utterance as an input to the system. Following this experimental set-up, we prepared five types of utterances (including two versions of \"Human\") for each pair of PerceivedInfo and a user utterance, totalling 2,500 utterances, for evaluation. To evaluate how each utterance is usable as a system utterance in dialogue, we annotated \"naturalness\" to the utterances. Raters were instructed to evaluate how natural the response was in the chat-oriented dialogue and to annotate an absolute score for each utterance in one of seven grades from one (very unnatural) to seven (very natural). They evaluated the five types of utterances at the same time. They could see not only a user utterance and system utterance but also the context before the user utterance. Three raters worked independently. Results of Subjective Evaluation Table 3 shows the results of the evaluation, where the average scores annotated by three raters to the five types of utterances are listed. The results show that \"Human by Writer 1 \" and \"Human by Writer 2 \" were ranked the highest by all raters, with \"Automatic\" ranked as the lowest. The order of the evaluated scores tended to be consistent in all raters (\"Human by Writer 1 ,\" \"Human by Writer 2 ,\" \"Repetition,\" \"PerceivedInfo,\" and \"Automatic\"). Spearman's rank correlation coefficient between two annotators averages at 0.56. \"Per- Table 3 : Naturalness scores of system utterances annotated by three raters ceivedInfo\" was evaluated as being more natural than \"Automatic,\" but less natural than \"Repetition.\" From this result, we can say that using only Per-ceivedInfo as system utterances is not an effective method. However, since there may be a difference among the types of PerceivedInfo, we further investigated the evaluation scores in each type of PerceivedInfo. Figure 3 shows the averaged naturalness scores annotated for each type of \"PerceivedInfo.\" The scores were clearly divided into three ranges: 1-2, 2-4, and 4-5, and defined as Low-rate type, Mid-rate type, and High-rate type, respectively. We investigated what PerceivedInfo exist in each type and the reasons for their high or low rating. For reference, we list examples and scores of Per-ceivedInfo in each type in Table 4 . Low-rate type An utterance in the Low-rate type mainly refers to user's temporary states, such as thoughts, emotion, or behavior during dialogue (e.g., \"You want me to agree, don't you?\"). Even an utterance that includes a positive expression (e.g., \"You like me, don't you?\") tends to be evaluated as unnatural. This can be partly explained by the politeness theory (Brown and Levinson, 1987) . Utterances in the Low-rate type that mention a user's temporary state would create the need for the user to explain. Thus, a user's negative face, the desire to be left free to act as he or she chooses, can be threatened. Mid-rate type An utterance in the Mid-rate type generally refers to user's permanent states, such as favorites, experience, or profiles (e.g., \"You like cool cars, don't you?\"). Such an utterance tends to be evaluated as natural as \"Repetition.\" However, an utterance including a negative expression (e.g., \"You are stubborn, aren't you?\") or a part of profiles (e.g. \"You are a woman, aren't you?\") tends to be evaluated as unnatural. This means that a mention of something negative or private about the user is not a good option. This can also be explained by the politeness theory as a violation of a user's positive face; that is a desire to keep self-image approved. High-rate type An utterance in the High-rate type generally refers to the content that does not directly relate to users, such as general facts (e.g., \"A trip abroad is expensive, isn't it?\"). Many utterances are evaluated as more natural than \"Repetition\" and as natural as \"Human.\" This may be because the utterances in the High-rate type do not threaten a user's face because the content of the utterances has no direct relation to users. From the experiment, we found that utterances created from specific types of PerceivedInfo are evaluated as more natural than others. Our results conform to the politeness theory and further provide quantitative evaluation of utterances that convey PerceivedInfo. One interesting thing is that the violation of the negative face has more impact on the naturalness when compared to that of the positive face. It is of great interest that, although much PerceivedInfo occurs during a dialogue, only a part of it can be uttered. Although further investigation is needed, our results are useful for providing a guideline for creating system utterances that convey understanding results. Table 4 : Best and worst three utterances conveying perceived information on each type in Table 1 . \"Score\" column shows average score and each score annotated by three raters. Related Work Although there has been no studies that explored the effect of utterances conveying system's understanding results to users, there have been several that have explored what linguistic behavior can be used or how to utter contents in dialogue systems from the viewpoints of social aspects (especially on the politeness theory). For example, Gupta et al. constructed a taskoriented dialogue system in the cooking domain in which utterance generation is performed on the basis of the politeness theory (Gupta et al., 2007) . Wang et al. estimated the politeness of each utterance in a task-oriented dialogue system by using various features, such as insults or criticisms (Wang et al., 2012) . Danescu et al. constructed a corpus in which politeness is annotated in online community data and constructed a model for estimating politeness using linguistic features, such as gratitude expressions or positive and negative lexicons (Danescu-Niculescu-Mizil et al., 2013) . Conclusion In this paper, we investigated what types of understanding results can be used as system utterances. Using the corpus of PerceivedInfo (perceived information), we manually created and evaluated the utterances that convey PerceivedInfo. We found that certain types of PerceivedInfo, especially those related to a user's permanent state and information irrelevant to users themselves, are usable. For future work, we want to construct a dialogue system that conveys the understanding results in the way we proposed. For this purpose, we need to create an automatic estimator of Per-ceivedInfo. In this work, we used the understanding results as they were; however, we can create various system utterances from PerceivedInfo, and, in such a case, other types of Perceived-Info can be effectively used. We want to further pursue how we can make use of PerceivedInfo in dialogue systems.",
    "abstract": "In dialogue systems, conveying understanding results of user utterances is important because it enables users to feel understood by the system. However, it is not clear what types of understanding results should be conveyed to users; some utterances may be offensive and some may be too commonsensical. In this paper, we explored the effect of conveying understanding results of user utterances in a chatoriented dialogue system by an experiment using human subjects. As a result, we found that only certain types of understanding results, such as those related to a user's permanent state, are effective to improve user satisfaction. This paper clarifies the types of understanding results that can be safely uttered by a system.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "0",
    "year": "2017",
    "month": "November",
    "title": "Investigating the Effect of Conveying Understanding Results in Chat-Oriented Dialogue Systems"
}