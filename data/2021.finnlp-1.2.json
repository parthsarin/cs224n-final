{
    "article": "The COVID-19 pandemic produced uniquely unstable financial conditions that permeated many sectors of the economy, creating complex challenges for existing financial models. In this investigation we tackle a subset of these challenges, seeking to determine the relationship between Reddit posts and actual economic time series during the economically tumultuous year of 2020. More specifically, we compute correlation between language used in Reddit's /r/personalfinance forum and initial or continuing unemployment claims in the United States throughout the year. We collect a novel dataset for the task, complete with annotations distinguishing between unemploymentspecific and general employment inquiries. We also train a Convolutional Neural Network (CNN) based deep learning model to distinguish between these categories, achieving a maximum F1 score of 0.857. Finally, we compute Pearson correlation coefficients between these model predictions and realworld financial data relative to a number of reasonable baselines, yielding correlations of up to 0.796 for initial claims and 0.747 for continuing claims. Introduction The ever-increasing amount of online social media data, paired with recent advances in deep learning and natural language processing, enables a new generation of analysis previously unattainable for analyzing public response to a global pandemic and consequent financial crisis. In this investigation, we seek to shed light on how a specific subset of social media data can be used to draw parallels with real world economic trends. We select unemployment data for our case study. Our work leverages a deep learning model to discover strong correlations between Reddit's 1 /r/personalfinance posts and actual initial and continuing unemployment metrics in the United States during the anomalous, turbulent COVID-19 year of 2020. Our contributions are as follows: \u2022 We create a novel dataset of 667 posts from r/personalfinance annotated with fine-grained categories corresponding to types of employment inquiries. \u2022 We build a CNN-based model to discriminate between unemployment categories, achieving a binary classification F1 score of 0.857. \u2022 We compute correlation scores between our model predictions and real-world employment data, achieving strong Pearson's correlation scores of 0.796 and 0.747 for initial claims and continuing claims, respectively. To enable replication and encourage additional follow-up work, we make our data available upon request. The remainder of this paper is organized as follows. We begin by presenting a brief summary of related work which served as inspiration. We then describe our data collection and annotation procedure, as well as define and discuss training procedures for a CNN classification model. Predictions from the top performing model are subsequently extended to the full year of data, enabling us to compute correlations with real unemployment economic time series, focusing on both initial and continuing claims in the USA. We analyze and discuss these results before commenting on intriguing future work directions. Related Work Although a historically less utilized platform than Twitter in academic works, Reddit contains a wealth of data in the modern online social network age. Reddit reportedly has around 430 million active users, 2 with just under 50% of these users being based in the United States, 3 our target country for this investigation. The primary advantage of using Reddit data for analysis and modeling in the context of the work presented here is that users can opt in to various topic-focused discussion forums, ranging from general topics like /r/food or /r/music, to specific topic forums like /r/whitesox or /r/CHIcubs, giving researchers a significant amount of flexibility to investigate topics. Within natural language processing research settings, the use of Reddit has been slowly increasing in popularity, with recent studies leveraging Reddit data for mental health discourse analysis [Choudhury and De, 2014; Valizadeh et al., 2021] , psychological personality prediction of users [Gjurkovi\u0107 and \u0160najder, 2018] , and even recent COVID-19 symptom and sentiment detection [Murray et al., 2020] . From an economic or financial perspective, one clear and prevalent use of social media text in academic research has been that of analyzing crowd sentiment for investment and trading strategies, whether related to equities or other asset classes. One recent example of this is the work of Wooley et al. (2019) , where the authors focus on a number of research questions revolving around cryptocurrency trading prices and related Reddit discussion boards. The results and discussion from these and other works support our notion of using Reddit as a data source in our case study of unemployment analysis. The /r/personalfinance sub-reddit specifically has indeed seen some limited mention in academic works; however, these works have largely been geared towards psychological mental health assessment of online users. Two such works are those of Shen and Rudzicz (2017) and Low et al. (2020) , both which directly reference data from /r/personalfinance in their work -but as a control sub-reddit, rather than a target for determining anxiety or mental health. Dataset We systematically collected data from Reddit since no existing datasets were available for our specific needs, and we utilized existing unemployment data from official government sources. The following subsections explain this process in greater detail. Text Data Collection Reddit post data was collected using the PushShift library [Baumgartner et al., 2020] in Python 3 from the /r/personalfinance sub-reddit using a search query from January 1 to December 31 of 2020. Reddit's API guidelines 4 allow for the collection and use of this data. While this API returns a sample of all posts, we filtered out any posts which had been removed from the site for any reason in order to respect the decision and privacy of users and site moderators. Similarly, we made the choice to not analyze any personal information related to the posters themselves. This yielded a total of 99,282 posts. One of the aspects which makes this sub-reddit especially interesting and useful for analysis is that a vast majority of posts are user-tagged with pre-defined categories, like \"investing\" or, more relevantly in this work, \"employment\" (4935 posts). Annotation For our predictive model and ultimate unemployment time series correlation, we labeled a sample of posts which were poster-tagged with \"employment.\" Our annotation scheme is as follows: \u2022 Unemployment (U): Individuals who are obviously unemployed, recently laid-off, and who have questions about their unemployment (e.g., regarding government benefits or otherwise To compute inter-annotator agreement, a shared set of 100 data samples of the \"employment\" tagged posts were doubleannotated by two U.S. born, English speaking, graduate-level college-educated individuals, which we determined was sufficient background to accurately interpret English social media posts. One of the annotators was an author of this paper, and the other was an external volunteer. Following a training phase that involved iterating on the annotation guide to improve clarity, particularly with respect to vague edge cases, a Cohen's Kappa score of 0.88 was obtained, which denotes substantial agreement [Landis and Koch, 1977] . Given this quantitative measure of confidence in the annotation guide, the remaining instances were single-annotated. The final dataset comprised 667 instances.  The most frequent words appearing in /r/personalfinance posts during the year 2020, after removing stopwords. Unemployment Data Exploratory Data Analysis We conducted exploratory, descriptive analyses of our data and outline a few findings in this subsection. Figure 1 shows post frequency over time on /r/personalfinance during our data collection range. We note the somewhat downward trajectory of this data, with a notable lower volume of posts around November. Considering post title and text, we observe word count distributions (min, max, mean) for titles as (1, 41, 6.24) and texts as (0, 2897, 83.92), after removing stopwords using the NLTK stopwords list [Bird et al., 2009] . Also excluding stopwords, we report the most frequent post text words in Table 1 . We note that some common words among those that are most frequent are indicative of the underlying question/answer style of the posts, like \"i'm,\" \"like,\" \"know,\" \"want,\" etc. Finally, we implemented a series of functions to compute the post frequency with title or text matching a set of query words.  It is interesting to visually note the steadily decreasing frequency of COVID-19 words after the initial spike -and overall relative infrequency of these keywords with a maximum of around 12% of posts -despite what we know to be the actual prevalence of COVID-19 cases in the United States, 6 shown in Figure 3 . Methods & Models For the task of correlating our /r/personalfinance posts with initial and continuing unemployment claims, we experiment with different iterations of a CNN deep learning model. Our model structure is influenced heavily by Semeval 2017's Task 5 [Cortis et al., 2017] , which focused on sentiment scoring of financial headlines. More specifically, we draw inspiration from the works of Mansar et al. (2017) and Kar et al. (2017) , which experimented with various deep learning models and word embeddings to achieve the top two scoring results. The primary advantage of using a simple CNN for our prototype model is that these models can work effectively on   All models were trained and evaluated using randomly assigned 90/10 train/test splits. We compared six conditions, three of which were classical machine learning models (considered as baseline alternatives) and three of which were variations of our CNN architecture with different training or preprocessing settings: \u2022 TB1: A random forest classification model [Ho, 1995] . \u2022 TB2: A logistic regression classification model [McCullagh and Nelder, 1989 ]. \u2022 TB3: A linear support vector machine (SVM) classification model [Boser et al., 1992] . \u2022 CNN1: Our CNN model with no extra preprocessing steps. \u2022 CNN2: Our CNN model with stopword removal, as well as regular expression-based removal of special charac-ters. \u2022 CNN3: Our CNN model with all of the preprocessing steps applied in CNN2. Additionally, post-level VADER [Hutto and Gilbert, 2014] sentiment scores were concatenated to the output of the max pooling layer. These conditions are further described in Table 2 , and additional feature details are provided in \u00a74.2. Scikit-Learn [Pedregosa et al., 2011] was the primary package for implementing the classical machine learning models (TB1, TB2, and TB3), and Keras [Chollet and others, 2015] was the primary package for implementing the CNN models (CNN1, CNN2, and CNN3). CNN models were trained using stochastic gradient descent and a learning rate of 0. Model Evaluation Results of our model comparison are shown in Table 2 . We evaluate models using macro and weighted F1 scores (harmonic mean of precision and recall), computed using Scikit-Learn. Non-parenthesized scores report model performance on a simplified two-class labeling scheme where we combined U+C and E+O posts into two distinct categories. The motivation behind this decision was an unfortunate and significant class imbalance -our dataset distribution was ultimately 19.8% U, 12.2% C, 50.1% E, and 17.1% O. While clearly not empty, there simply were not as many posts in our target categories of interest, U and C, as we expected. Four-class classification scores are also reported for our top performing model in row CNN2, as well as our baseline models, enclosed in parentheses. All four-class scores are notably lower than the two-class scores. Ablation Studies In our CNN conditions, we experimented with a variety of settings to assess the utility of preprocessing steps and lexicon-based features (i.e., sentiment scores), with these settings represented as CNN1, CNN2, and CNN3. To incorporate sentiment scores into the model, we concatenated postwise VADER sentiment lexicon scores [Hutto and Gilbert, 2014] to the output of the max pooling layer, similar to that  done by Mansar et al. (2017) . Contrary to our expectations, this did not consistently improve model performance, yielding a weighted F1 score of 0.844 which is lower than the performance without VADER scores (CNN2). However, it was clear from our comparison that the task benefited from some basic text preprocessing steps. In CNN2, stopwords were removed using NLTK [Bird et al., 2009] and regular expressions were utilized to remove special characters such as commas and dashes, whereas in CNN1 no preprocessing steps were performed. CNN2 outperformed CNN1; moreover, these actions lowered the number of missing words encountered by our pretrained GloVe embedding model (13.56% originally, reduced to 1.50%). Results & Discussion Extending the predictions of our top performing model to the entire set of \"employment\" /r/personalfinance posts, we computed correlation with official initial and continuing unemployment claims to assess sub-reddit posting behaviors with respect to real-world financial trends. Pearson correlation coefficients (r) were computed using the Stats module of Scipy [Virtanen et al., 2020] and are shown in Table 3 . Item 1 in the table corresponds to the condition used to predict (un)employment We observe that the strongest correlation with initial claims stems from the simple keyword search (described in \u00a73.4) at a value of 0.796. This does not necessarily indicate poor performance from our model, which achieved a lower score at 0.691. Rather, we speculate that the way in which categories were defined, mixed with the actual sub-reddit post content, simply does not as closely reflect initial unemployment metrics. On the other hand, our CNN model predictions of the Unemployment/Cut merged class correlate better with continuing unemployment claims than the alternative approaches, at a value of 0.747. The correlations between our CNN model predictions and real-world unemployment trends are illustrated through the plots in Figure 4 . Visually, it is clear that the elongated peak of the CNN predictions around April and May (shown in blue) match up better with continuing claims (red) than with initial claims (orange). Qualitatively, this behavior appears reasonable given the context of the online forum -in fact, with almost all posts being question/answer focused, we observed this same pattern while annotating posts. Individuals seemed relatively unlikely to post immediately after being laid-off with questions about filing for unemployment. Instead, for unemployment related inquiries we observed notably higher frequency of individuals asking questions about events within the last month or so. These posts frequently referenced mistakes made on unemployment filing forms, for example by asking how long they needed to wait before they received their first assistance payment after filing. Summary, Conclusion & Future Work In this work, we successfully obtained strong correlation with official unemployment statistics during the economically anomalous year of 2020. We contributed a new dataset of r/personalfinance posts labeled with Unemployment, Cut/Furlough, Employment, and Other designations. We then trained a number of machine learning models on this data, including both classical baselines (Random Forest, Logistic Regression, Linear SVM) as well as a more advanced CNN model, which achieved a best weighted F1 score of 0.857 on two-class classification of posts. These predictions were extended to the full slice of user \"employment\" tagged posts. We then computed correlation between these predictions and initial and continuing unemployment claims, sourced from the St. Louis Federal Reserve, relative to two baselines (frequency of posts tagged with \"employment,\" and unemployment keyword percent search). Our experiments yielded top correlations of 0.796 for initial claims and 0.747 for continuing claims. We make our data and source code available to the research community to foster additional work in this area and to facilitate replication. With respect to future work directions, on a micro level, there are clear next steps which could be taken to encourage improved performance. Our current models serve as a prototype, with the primary objective being to establish proof of concept rather than achieve state of the art performance. While our methodology for training the CNN model was efficient and F1 scores indicate that the model structure and inputs were enough to achieve strong performance measures, extending model input to include longer word sequences or further expanding our labeled dataset could similarly help to achieve higher performance. This could also enable the use of more advanced machine learning models like gated recurrent units (GRUs), long short-term memory networks (LSTMs), or BERT/Transformer-based models [Devlin et al., 2019] , which we largely avoided due to the small size of our labeled data sample. On a macro level, this study provides an interesting case study towards harnessing natural language in Reddit posts for economic time series correlation, specifically focused on unemployment and the COVID-19 pandemic in the USA. However, further work could consider including a larger time frame to encompass non-pandemic phenomena as well. Even within the context of our specific sample of /r/personalfinance data, there is still ample room to explore and analyze altogether different time series, as there is no shortage of user-tagged posts covering a diverse range of topics including \"retirement,\" \"housing,\" \"investing,\" \"auto,\" and others, offering a productive starting point for follow-up work."
}