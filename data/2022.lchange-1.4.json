{
    "article": "Contextual word embedding techniques for semantic shift detection are receiving more and more attention. In this paper, we present What is Done is Done (WiDiD), an incremental approach to semantic shift detection based on incremental clustering techniques and contextual embedding methods to capture the changes over the meanings of a target word along a diachronic corpus. In WiDiD, the word contexts observed in the past are consolidated as a set of clusters that constitute the \"memory\" of the word meanings observed so far. Such a memory is exploited as a basis for subsequent word observations, so that the meanings observed in the present are stratified over the past ones. Introduction The use of contextual embedding techniques is receiving more and more attention in the field of semantic shift detection. In particular, pre-trained models like BERT (Hu et al., 2019; Martinc et al., 2020a) , ELMo (Kutuzov and Giulianelli, 2020; Rodina et al., 2020) , and XLM-R (Cuba Gyllensten et al., 2020; Rother et al., 2020) , are being proposed as promising solutions to capture the different meanings of a target word according to the different contexts in which the word appears throughout a considered diachronic corpus. Such solutions generally employ clustering techniques to aggregate embeddings of a specific word into clusters (Martinc et al., 2020a; Karnysheva and Schwarz, 2020) . The idea is that each cluster denotes a specific word meaning that can be recognized in the considered documents. In this way, it is possible to analyze the shift of a word meaning/sense by exploiting the evolution of a cluster over time. For instance, an increasing number of elements in a cluster denotes that the associated word meaning is getting frequently adopted. On the opposite, a cluster with a decreasing number of elements over time refers to a word meaning that is getting obsolete. Usually, the corpus is static, meaning that all the documents of the considered time periods are available as one whole, and a single clustering activity is performed over the entire corpus, generating clusters of word meaning with documents of different time periods (Kutuzov et al., 2018; Tahmasebi et al., 2018 Tahmasebi et al., , 2021)) . As a result, the time period in which a document is added to the corpus is not taken into account for cluster composition, and this is not completely satisfactory for an appropriate recognition of meaning changes over time. When a dynamic corpus is considered, namely time periods and documents can be progressively added, scalability issues also arise, since the clusters of word meanings need to be re-calculated or updated. As a possible solution, some recent works propose to perform clustering separately for each time period. In this case, the resulting clusters need to be aligned in order to recognize similar word meanings in different, consecutive time periods (Kanjirangat et al., 2020; Montariol et al., 2021) . However, solutions based on clustering alignment are not satisfactory as well, since they do not capture the possible evolution pattern of a meaning across different time periods. A recent work proposes an average-based approach to track semantic shift via continuously evolving embeddings (Horn, 2021) computed as a weighted running average (Finch, 2009) of embeddings generated by a contextual model. This method is suitable to be applied on stream data and it is far more scalable than typically cluster-based methods. Nevertheless, it does not allow to analyse which meanings are actually changed. In this paper, we present What is Done is Done (WiDiD), an incremental approach to semantic shift detection based on incremental clustering techniques and contextual embeddings to capture the changes over the meanings of a target word along a diachronic corpus. In WiDiD, we work under the assumption that the documents of the corpus become available as a stream and they are segmented in a sequence of time periods. The word contexts observed in past time periods are consolidated as a set of clusters that constitute the \"memory\" of the word meanings observed so far. Such a memory is then exploited as a basis for subsequent word observations in the current time period. The idea of WiDiD is that the clusters of word meanings previously created cannot be changed (what is done is done), and the word meanings that are observed in the present must be stratified/integrated over the past ones. To enforce scalability, incremental clustering techniques are employed in WiDiD, so that the word embeddings extracted from the documents of the current time period are compared and assimilated into the set of consolidated clusters coming from the past time periods. A comparative evaluation of the proposed WiDiD approach against a reference benchmark is discussed in the paper according to multiple configurations characterized by different clustering algorithms and embedding methods. In particular, we present experiments based on a pretrained BERT model as well as results obtained from a trained Doc2Vec model, which has been adapted to provide pseudo-contextual word embeddings to extend the conventional static word representations of context-free embedding techniques. As a further contribution of WiDiD, different metrics for semantic shift evaluation of word meanings are defined in the paper and experimental results are provided to discuss their effectiveness. The paper is organized as follows. In Section 2, the relevant literature is discussed. In Section 3, we present the WiDiD approach. Incremental clustering techniques and semantic shift measures of WiDiD are illustrated in Sections 4 and 5, respectively. Experimental results are discussed in Section 6. Section 7 finally provides our concluding remarks. Related work Works related to WiDiD are about the use of word embeddings for semantic shift detection by leveraging the idea that semantically-related words are close to each other in the embedding space (Mikolov et al., 2013) . In approaches relying on context-free embeddings, independent word vectors defined over different \"temporal\" vector spaces can be compared after applying an alignment mechanism (Hamilton et al., 2016) such as the Procrustes (Sch\u00f6nemann, 1966) . Moreover, recent contextualised architectures are proposed, like ELMo (Peters et al., 2018) , BERT (Devlin et al., 2019) , and XLM-R (Conneau et al., 2020) , which generate dynamic word embeddings according to the use of the words in the input sequences, thus enabling the recognition of different meanings by comparing the context in which words are used throughout the text. The solution proposed in Hu et al. (2019) is one of the first examples based on BERT embeddings to track changes in word meanings and it requires lexicographic supervision, like the use of a reference dictionary (e.g., the Oxford dictionary for the English language) to list the possible word meanings beforehand, thus it is hardly applicable to low-resource languages. A number of unsupervised approaches based on contextual embeddings are proposed to sidestep the need of lexicographic resources (Schlechtweg et al., 2020; Tahmasebi et al., 2021) . In general, these kinds of approaches follow a three-step scheme: i) extraction of embeddings for each occurrence of a target word from a contextual model such as BERT (Hu et al., 2019; Martinc et al., 2020a) , ELMo (Kutuzov and Giulianelli, 2020; Rodina et al., 2020) , or XLM-R (Cuba Gyllensten et al., 2020; Rother et al., 2020) ; ii) aggregation of the embeddings with a clustering algorithm like K-Means (Giulianelli et al., 2020; Cuba Gyllensten et al., 2020) , Affinity Propagation (Martinc et al., 2020a; Kutuzov and Giulianelli, 2020) , or DBSCAN (Rother et al., 2020; Karnysheva and Schwarz, 2020) ; iii) comparison of the vector distribution over clusters according to time by using a semantic distance measure, like Jensen-Shannon divergence (Martinc et al., 2020a) , Entropy Difference (Giulianelli et al., 2020) , or Wasserstein Distance (Montariol et al., 2021) . The main limitation of applying clustering to word embeddings is the scalability issues about memory consumption and time. As a recent contribution, in Montariol et al. (2021) , a scalable and interpretable method is proposed based on merging of similar embeddings to reduce the number of representations to consider for a given word and time slice. Further solutions to overcome scalability issues are provided by Rodina et al. (2020) and Laicher et al. (2021) . In particular, they propose to limit the number of embeddings by randomly sampling sentences from each period. The intrinsic time-complexity issues of applying clustering algorithms to embeddings are also addressed in Rother et al. (2020) by reducing the embedding dimensionality. In Martinc et al. (2019) , the contextual embeddings of a word are averaged to generate a single word representation for each time period. In Giulianelli et al. (2020) , the average pairwise distance between embeddings of different time periods is calculated. Even if these solutions are more efficient and scalable than clustering, they provide uninterpretable results since multiple word occurrences are collapsed into a single representation, like in context-free embeddings. Most of the cluster-and average-based approaches estimate the magnitude of semantic shifts ignoring the uncertainty of their estimations. As a result, estimations can be erroneously inflated since the irregularities of word frequencies over time can negatively affect the stability of word embeddings (Zhou et al., 2021; Wendlandt et al., 2018) . In this respect, Liu et al. (2021) propose a solution based on the combination of BERT embeddings with permutation-based statistical test and term-frequency thresholding. Original contribution of WiDiD. With respect to the above solutions, the WiDiD approach is based on incremental clustering techniques applied to contextual word embeddings. In WiDiD, the \"memory\" of word meanings observed in the past is consolidated in a set of clusters that is not re-calculated in subsequent time periods. As a result, only the word embeddings of the current time period are analyzed with the aim to measure the change with respect to the clusters of past word meanings. This way, it is possible to compare specific word meanings also from a qualitative point of view (i.e., interpretable results) without requiring any alignment mechanisim across time periods. In other words, the stratified layers of clusters over time allow to reconstruct not only the quantity of semantic shift but also the evolution of a word meaning. The WiDiD approach Consider a diachronic document corpus C = C 1 \u222a C 2 where C 2 denotes a set of documents of the time t and C 1 denotes a set of documents cumulatively collected in the t \u2212 n time periods prior to t. Given a target word w, the goal of semantic shift detection is to measure how much the meaning(s) of w is changed from C 1 to C 2 . The WiDiD approach relies on a contextual embedding model to represent each occurrence of the target word w in a corpus C j (either C 1 or C 2 ). We keep track of the word embedding representations collected for w over time by relying on the embedding model E 1 that contains the word vectors computed over C 1 . Given this input, we process the new documents in C 2 as follows (see Figure 1 ). Document selection. In this step, we select the subset of documents C w,2 \u2286 C 2 that are relevant for the word w. C w,2 is composed by the documents containing the word w. As an alternative, any information retrieval technique suitable for finding relevant documents for a given target can be exploited for the composition of C w,2 . Fine tuning. In this step, the model E 1 used to generate the word vectors over C 1 can be optionally updated/fine-tuned into a new model E 2 to take into account the new documents in C 2 (Kim et al., 2014; Giulianelli, 2019) . When the observed time t is the initial one, the model E 1 is trained on C 2 or a pre-trained model is used. The WiDiD approach is compatible with any technique for contextual word embedding, that is any method that produces a vector embedding the meaning of a word in a specific document. Embedding extraction. In this step, we isolate the embedding vectors representing the contextual meaning of the word w. The contextualised embedded representation of the word w in the k-th document of a corpus C w,j is denoted by e j w,k . Then, the representation of the word w in the corpus C j is defined as: \u03a6 j w = {e j w,1 , . . . , e j w,m }, with m being the number of documents in C w,j . As the final output of this step, we have two sets of embedding vectors: \u03a6 1 w that is produced in the previous iterations of the WiDiD approach over the corpus C w,1 and \u03a6 2 w , produced at the current time t for the corpus C w,2 . Clustering. In this step, vectors in \u03a6 1 w \u222a \u03a6 2 w are clustered in order to group vectors representing similar meanings. The set of clusters produced in this step is denoted K 2 and the i-th cluster in K 2 is denoted \u03d5 w,i . A distinguishing feature of WiDiD is to perform also the clustering step in an incremental fashion, by updating the clusters K 1 computed in the previous iterations of WiDiD. A more detailed description of the incremental clustering techniques used in WiDiD is given in Section 4. The clusters of K 2 can be classified in three types (see Figure 2 ). Cluster types (A) and (C) contain vectors that derive from a single corpus, either the past (i.e., C 1 ) or the current one (i.e., C 2 ). The cluster type (B) is t-1 t t-2 1 \u2026 \u2026 Past corpora Corpus at time t Word w DOCUMENT SELECTION C w,2 E 1 Embedding model FINE TUNING EMBEDDING EXTRACTION E 2 \u03a6 w 2 K 2 CLUSTERING CLUSTER REFINEMENT K' 2 SEMANTIC SHIFT MEASURING Score(w) K 1 Clustering scheme C 1 C 2 Figure 1: The WiDiD approach (A) (B) (C) Embedding vector for corpus C 1 Embedding vector for corpus C 2 Vectors mean (C 1 ) Vectors mean (C 2 ) Figure 2 : Types of clusters in K 2 a mixture of vectors from the past (corpus C 1 ) and vectors from the present time (corpus C 2 ). For each cluster, we compute also the mean \u00b5 i of the vectors that are associated with the same time period (i.e., the same corpus). Cluster refinement. The cluster set K 2 may contain poorly-informative clusters, such as clusters containing a single vector, or aged information, namely clusters that contain only vectors representing a word meaning observed a long time ago. In order to get rid of poor or aged information, in Wi-DiD, it is (optionally) possible to perform a cluster refinement step to drop the undesired clusters. We note that this step is also useful to reduce the information available about the past in view of a subsequent execution of WiDiD for the next time period t + 1. With regard to poorly-informative clusters, we enforce standard cluster pruning techniques that are typically based on a threshold over the cluster size or the average distance of vectors from the cluster centroid (Raskutti and Leckie, 1999) . For aged information, the idea of WiDiD is that each cluster is associated with an aging index that measures how recently the cluster has been updated during the incremental clustering process. This index is updated each time a cluster in the cluster set K 1 is upgraded by adding vectors of \u03a6 2 w (i.e., vectors deriving from the corpus C 2 ). A threshold over the aging index is then used to decide when an aged cluster should be pruned from K 2 . As a result, this is a mechanism to regulate how much memory the WiDiD will keep about the past. The final pruned cluster set is denoted K \u2032 2 and will be the basis of the clustering step in the next iteration of WiDiD. Semantic shift measuring. To evaluate whether a word w exhibits a semantic change between the two corpora C 1 and C 2 , we measure the distance between the sets \u03a6 1 w and \u03a6 2 w using the clusters in K \u2032 2 . Further details on how to measure semantic shift are provided in Section 5. Incremental clustering In WiDiD, we rely on incremental clustering to aggregate contextual embedding vectors that represent similar word meanings into the same cluster. We propose an incremental extension of Affinity Propagation (AP) (Frey and Dueck, 2007) , called Affinity Propagation a Posteriori (APP) (see Algorithm 1). Let's call X and X 1 , and L and L 1 the embeddings and the cluster labels at time t and t\u22121, respectively. At time t = 1 the standard AP clustering is performed. At each time t > 1, for each existing cluster computed at time t \u2212 1, the data points x i \u2208 X 1 are packed into a single average representation, i.e. the centroid \u00b5 of each cluster. The set of the centroids for X 1 is denoted \u00b5X 1 . Then, the standard AP algorithm is executed on \u00b5X 1 \u222a X, with the aim to obtain a new set of temporary labels L 2 , i.e., the new assignment of data points to Algorithm 1 The APP algorithm Input t: time step X: data at time step t X1: data at time step t \u2212 1 L1: labels at time step t \u2212 1 \u03b3: trim factor Output L, X: at time step t 1: if t == 1 then 2: L \u2190 AP (X) 3: L, X \u2190 T rim(L, X, \u03b3) 4: yield L, X 5: 6: else if t > 1 then 7: \u00b5X1 \u2190 P ack(L1, X1) 8: L2 \u2190 AP ( \u00b5X1 \u222a X ) 9: \u00b5L1, L \u2190 Split(L2) 10: L1 \u2190 U npackAndU pdate(\u00b5L1, \u00b5X1, L1, X1) 11: L, X \u2190 T rim( L1 \u222a L, X1 \u222a X, \u03b3) 12: yield L, X 13: end if clusters. Such labels are then split in two subsets, \u00b5L 1 and L, which contain labels for each average representation in \u00b5X 1 and for each data point in X, respectively. Given \u00b5L 1 , \u00b5X 1 , L 1 , X 1 , we unpack the centroids of \u00b5L 1 into the corresponding data points X 1 mapping the previous labels L 1 into the new labels of their respective centroids \u00b5L 1 . Intuitively, clusters from time step t \u2212 1 can't be changed, in the sense that each point from t \u2212 1 remain in the same cluster after running AP at time step t. However, each cluster from t \u2212 1 can be updated with points from t, and new clusters can be created at time step t containing no points from t \u2212 1. Finally, APP returns L 1 \u222a L, which is the union of the unpacked and updated L 1 and L. APP includes the notion of aging index to use for cluster refinement, implemented through a trim factor \u03b3. In our current implementation the idea of \u03b3 is that clusters containing less than \u03b3 percent of the whole set of embeddings \u03a6 1 w \u222a \u03a6 2 w at time t are assumed to be poorly-informative and thus they are dropped. Semantic shift measuring Clustering contextual word embeddings for a word w at time t results in a set of k clusters K 2 = \u03d5 w,1 , ..., \u03d5 w,k where \u03d5 w,i \u2286 \u03a6 1 w \u222a \u03a6 2 w . In particular, we denote as \u03d5 1 w,i , \u03d5 2 w,i the set of embeddings from \u03a6 1 w , and \u03a6 2 w respectively, enclosed in the i\u2212th cluster; formally we define \u03d5 1 w,i = \u03d5 w,i \u2229 \u03a6 1 w and \u03d5 2 w,i = \u03d5 w,i \u2229 \u03a6 2 w . According to this, in WiDiD, we propose three different aggregation measures to estimate semantic change. Borrowing from Giulianelli (2019), we employ the Jensen-Shannon divergence to measure semantic change leveraging cluster distributions. In addition, we adapt the methods of Martinc et al. (2019) and Kutuzov (2020) for scenarios where embeddings are clustered. Jensen-Shannon divergence (JSD). The Jensen-Shannon divergence quantify the similarity between two probability distributions using a symmetrization of the Kullback-Leibler divergence. JSD(p 1 w , p 2 w ) = H 1 2 p 1 w + p 2 w \u2212 1 2 H(p 1 w ) \u2212 H(p 2 w ) (1) To quantify changes between word senses we create two time-specific cluster distributions p 1 w , p 2 w as the relative number of cluster members for t \u2212 n, and t, respectively (Hu et al., 2019) . Intuitively, we compute the value related to the i\u2212th cluster as: p j w,i = |\u03d5 j w,i | |\u03a6 j w | (2) where j \u2208 {1, 2}. Distance between prototype embeddings (PDIS). Recent work used the term word prototype to indicate a 'prototypical' representation of the word computed by averaging all its embeddings in a specific temporal sub-corpus (Rodina et al., 2020; Kutuzov, 2020; Martinc et al., 2019) . In contrast to this definition, we compute (i) sense prototypes \u00b5 1 w,i , \u00b5 2 w,i as the average embedding for each cluster partition \u03d5 1 w,i , \u03d5 2 w,i , respectively; and (ii) word prototypes M 1 w , M 2 w as the average embedding of all sense prototypes \u00b5 1 w,i , and \u00b5 2 w,i respectively. The idea is that computing the average of a smaller set of more significant embeddings, i.e., the sense prototypes, can be beneficial to reduce noise in clusters. The average-based method by Martinc et al. (2019) consists in computing the cosine similarity between the global average embeddings of all embeddings from t \u2212 n and t, respectively. We extend this method by computing the cosine distance between M 1 w and M 2 w . P DIS(M 1 , M 2 ) = 1 \u2212 M 1 \u2022 M 2 \u2225M 1 \u2225 \u00d7 \u2225M 2 \u2225 (3) Difference between prototype embedding diversities (PDIV). The method proposed by Kutuzov (2020) relies on the notion of \"embedding diversity\" for word prototypes (DIV). We extend this method considering sense prototypes. In particular, we estimate the degree of ambiguity for w in C 1 , C 2 as the mean cosine distance d between sense prototypes \u00b5 j w,i and the relative word prototype M j w . The final result is the absolute difference between the relative coefficients. For the sake of simplicity, let's denote as \u03a8 1 w and \u03a8 2 w the set of sense prototypes of \u00b5 1 w,i , and \u00b5 2 w,i respectively. P DIV (\u03a8 1 w , \u03a8 2 w ) = \u00b5 1 w,k \u2208\u03a8 1 w d(\u00b5 1 w,k , M 1 w ) |\u03a8 1 w | \u2212 \u00b5 2 w,k \u2208\u03a8 2 w d(\u00b5 2 w,k , M 2 w ) |\u03a8 2 w | (4) Evaluation of WiDiD For evaluation of WiDiD, we rely on the Task 1 framework of SemEval-2020. SemEval is a series of international NLP workshops based on a collection of shared tasks in which computational semantic analysis systems designed by different teams are presented and compared. In particular, we focus on SemEval-2020 Subtask 2 where the goal is to consider texts from two distinct time periods and to evaluate the degree of semantic shift of a set of target words (Schlechtweg et al., 2020) . In SemEval-2020, the semantic shift degree is measured by the Spearman's rank-order correlation between the semantic shift index (i.e., the ground truth) and the semantic shift assessment computed by a model for each target word in the evaluation set. Our evaluation is performed over the English and Latin corpora of SemEval-2020. A summary view of the considered corpora is provided in Table 1. As proposed in Montariol et al. (2021) , in the English corpus, we removed POS tags from both the corpus and the evaluation set. Experimental setup In the evaluation, the following configurations of WiDiD have been adopted. Word representations. Pre-trained BERT and trained Doc2Vec models are exploited as embedding models. We use the Transformers library by HuggingFace to extract contextual word embeddings from pre-trained BERT models without performing any fine-tuning stage (Wolf et al., 2020) . We use a specific model for each language, namely bert-base-uncased 1 for English and bertbase-multilingual-uncased 2 for Latin. The models are base versions of BERT with 12 attention layers and a hidden layer of size 768. The only model available for Latin is a multilingual BERT model trained on 104 languages, including Latin. The acquisition of contextual embeddings is done by feeding the models with text sequences from the corpora in which the target words occur. Sequence embeddings are generated one sequence at a time by summing the last 4 encoder output layers according to Devlin et al. (2019) . Finally, given a sequence of size sequence length \u00d7 embeddings size, we cut it into pieces to get a separate contextual embedding for each token in the sequence. In this way, we extract token embeddings for each occurrence of a target word in a corpus. Due to the byte-pair input encoding scheme employed by BERT models, some tokens may not correspond to words but rather to word pieces (Sennrich et al., 2016; Wu et al., 2016) . Therefore, if a word is split into more than one token, we build a single word embedding by concatenating them. Pseudo-Word Representations. While BERTlike models generate dynamic embeddings for a word according to their belonging sequences (i.e., documents), Doc2Vec (Le and Mikolov, 2014) produces a static lookup table of word and sequence embeddings only for words and sequences seen during training. We exploit Doc2Vec by computing pseudo-contextual word embeddings under the assumption that word occurrences belonging to similar sequences have the same meaning. This means that, given a target word w in the corpus C j we consider as \u03a6 j w the set of sequence embeddings related to sequences where w occurs. For training Doc2Vec models, we use the Gensim library (Rehurek and Sojka, 2011). In particular, we trained word and sequence embeddings of size 100 for 15 epochs, with a window size of 10. Clustering of embeddings. For the evaluation of WiDiD, we exploit the APP clustering algorithm described in Section 4. Since APP is an extension of the Affinity Propagation (AP) clustering algorithm, we compared the results of APP against the results of AP in the clustering step of the WiDiD approach. In addition, we tested a further incremental extension of AP called IAPNA. IAPNA is an incremental version of AP that has been proposed by Sun and Guo (2014) and it is based on the idea of computing a reasonable assignment for all the data points at the same status. Then, when new points are available, the relationships between the new points and the other points are assigned referring to their nearest neighbors and by updating the responsibility and availability indexes for those points. In particular, we use the scikit-learn (Pedregosa et al., 2011) implementation for standard AP, that we extended for implementing both IAPNA and APP. Experiments. The following experiments have been executed. We apply the semantic shift measures illustrated in Section 5 (i.e., JSD, PDIS, PDIV) to the clusters of contextual embeddings obtained by using AP, IAPNA, and APP, respectively. Since PDIS and PDIV are extensions of the CD (Cosine Distance over Word Prototypes) and DIV (Difference between Token Embedding Diversities) measures proposed by Martinc et al. (2019) and Kutuzov (2020), we also consider them as baselines. Experimental results The results of our evaluation are shown in Table 2 3 . Surprisingly, Doc2Vec proved to be a suitable model for semantic shift detection, in both incremental and non-incremental clustering contexts. It performs well, while being smaller and faster than contextual models. In particular, Doc2Vec-based methods achieve the highest result in our experiments on both Latin and English, with correlation coefficient of 0.512 and 0.514, respectively. APP provides top results on both Latin and English, although AP has a slightly higher performance on English. On average, both incremental clustering algorithms IAPNA and APP perform well in semantic shift detection compared to the conventional AP clustering. We note that IAPNA and APP have opposite behavior on Latin and English: IAPNA has higher results with BERT embeddings on Latin and Doc2Vec embeddings on English, while APP has higher results with Doc2Vec embeddings on Latin and BERT embeddings on English, respectively. The fact that IAPNA and APP perform differently on different languages is consistent with the literature results (Kutuzov and Giulianelli, 2020) . As a further remark, we note that APP produces a smaller and more reasonable number of clusters compared to both AP and IAPNA. For instance, we observed situations where both AP and IAPNA produce more than 100 clusters, that is rather unrealistic if we assume that a cluster represents a word meaning. On the opposite, in our experiments, the number of APP clusters generally varies between 0 and 30. We also note that APP is sensitive to the aging index. In Table 2 , we present the top results obtained with two different values of the aging index (i.e., 0 and 5). Removing clusters containing less than 5% of the embeddings has a positive impact just in some experiments with English, but not with Latin. We plan to further investigate the effects of the aging index in our future work. About our proposed measures for semantic shift detection (i.e., JSD, PDIS, PDIV), we note that they always perform better than the baselines CD and DIV. We also note that the CD baseline does not work well on Doc2Vec embeddings, while DIV does not work well in all our experiments. On Latin, the highest results are achieved by JSD on both Doc2Vec and BERT embeddings. On English, the top JSD and PDIS results are on Doc2Vec and BERT embeddings, respectively. More experiments are required on PDIV since it performs very differently in the various experiments we performed, and it achieves statistical significance only in four out of twelve experiments (six on Latin, six on English). Finally, Table 3 provides the best results obtained by other literature approaches for semantic shift detection based on contextual word embeddings over the English and Latin corpora of SemEval-2020. We note that both IAPNA and APP are competitive when compared to the considered literature approaches. The WiDiD scores are above average and slightly below the maxi-  mum scores (in bold). We stress that we obtained these results without fine-tuning, confirming that the idea of using incremental clustering is promising. Compared to other literature approaches based on pre-trained models without fine-tuning, we note that incremental clustering algorithms achieve the highest scores on Latin (0.512 with APP and 0.411 with IAPNA for Doc2Vec and BERT, respectively). Our results on the English corpus come second in the pre-trained ranking (0.512 with APP and 0.499 with IAPNA for Doc2Vec and BERT, respectively) after Laicher et al. (2021) . All in all, excluding Laicher et al. (2021) and Kutuzov (2020), our results are the highest of all the considered literature works of Table 3 , both on Latin and English. Concluding remarks In this paper, we presented the WiDiD approach characterized by incremental clustering techniques and contextual word embedding methods. Ongoing work is about the fine-tuning of adopted embedding models to further improve the quality of results. Moreover, we are working on defining cluster analysis techniques. The idea is to exploit the results of semantic shift measures to interpret possible trend patterns over clusters along the time, such as a broad meaning that forks into narrower ones, or a meaning that increases its popularity and vice versa. Further work is about the specification of aging policies to manage the memory of aged embeddings in the cluster evolution. Acknowledgments This paper is partially funded by the RECON project within the UNIMI-SEED research programme and by the PSR-UNIMI programme."
}