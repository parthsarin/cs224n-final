{
    "article": "In this paper, we attempt to improve upon the state-of-the-art in predicting a novel's success by modeling the lexical semantic relationships of its contents. We created the largest dataset used in such a project containing lexical data from 17,962 books from Project Gutenberg. We utilized domain specific feature reduction techniques to implement the most accurate models to date for predicting book success, with our best model achieving an average accuracy of 94.0%. By analyzing the model parameters, we extracted the successful semantic relationships from books of 12 different genres. We finally mapped those semantic relations to a set of themes, as defined in Roget's Thesaurus and discovered the themes that successful books of a given genre prioritize. At the end of the paper, we further showed that our model demonstrate similar performance for book success prediction even when Goodreads rating was used instead of download count to measure success. Introduction Since its publication in 1868, approximately 1.78 million copies of Louisa May Alcott's Little Women have been sold, which equates to about 1,000 copies a month for 152 years. Every publisher in the industry hopes to find a manuscript that can sell even 10,000 copies in its lifetime. This begs the question: what makes Little Women a timeless success? Recently, researchers have attempted to use machine learning and natural language processing to answer this question, among others. Predicting the success of a novel by analyzing its content is a challenging research problem. Thousands of new books are published every year, and * Both authors contributed equally to this research. Figure 1 : This figure represents the lexical production rules of context free grammar observed in both successful and unsuccessful books classified by Goodreads ratings. We present the count of 20 lexical rules, each normalized with respect to it's highest occurrence in any book, for 10 successful and 10 unsuccessful books from CHILDREN genre. We see that certain lexical rules occur more frequently in successful books than unsuccessful books. only a fraction of them achieve wide popularity. Therefore, the ability to predict a book's success prior to publication would be exceptionally useful to the publishing industry and enable editors to make better decisions. Many factors contribute to a book's success including, but not limited to plot, setting, character development, etc. Additionally, there are some other factors that contribute to a book's popularity that an author and publisher cannot control like the time when a book is published, the author's reputation, and the marketing strategy. In this paper, we only focus on the content of the book to predict its popularity. In this paper, we explore whether novels in a spe-cific genre have certain dominant themes in common based on stylometric features, and if so, what meaning we can attribute to those themes as it relates to a book's success. To attain this objective, we investigated ways to enable using the entire book's content for stylistic modeling using frequencies of lexical production rules of CFGs for each novel (see Figure 1 ) followed by semantic word association of those rules to Roget's Categories and Themes for better interpretation. In this work, we followed widely used feature reduction technique for SVM modeling to reduce the large lexical feature space for lengthy novels and experiment with different techniques using POS, Unigram, Word-Net, and lexical production rules. In this article we present the following contributions: \u2022 We built the largest dataset containing a total of 17,962 books. We included books from 4 additional genres and reclassified 2 of the genres included in (Ashok et al., 2013) as follows: Mystery\u2192Detective; Love\u2192Romance. \u2022 We introduced our feature reduction methods to greatly improve prediction performance with our best model achieving 94% accuracy for success prediction. \u2022 We mapped both WordNet's semantic word relations and context free grammar rules to a set of themes, as defined in Roget's Thesaurus. With these mappings, we discovered the themes that successful books of a given genre prioritize. Related Works Roget's Thesaurus is a widely used Englishlanguage thesaurus. A British lexicographer, Peter Mark Roget (1779 -1869) , created the thesaurus in 1805. The first version of the thesaurus comprised of nearly 15,000 words and was released to the public on 29 April 1852 (Roget and Roget, 1886) . Since then each successive edition was improved with more words, with the most recent edition containing more than 100,000 words. In previous work, Jarmasz and Szpakowicz (2004) showed that Roget's is an excellent resource for measuring semantic similarity and Roget's word clusters have higher correlation than many other prominent word groups e.g., Wordnet (Miller, 1998; Jarmasz, 2012) . Syntactic features, such as CFG productions have been found to be very effective in different NLP tasks. Raghavan et al. (2010) used CFGs for authorship attribution achieving very high accuracy such as 96%. Rayson et al. (2002) presented systematic analyses based on lexical and syntactic features for genre detection of a literary works showing that novels involve more use of verbs and adverbs. On the other hand, Douglas and Broussard (2000) showed that informative writing tend to use nouns, prepositions, determiners and coordinating conjunctions more. CFGs were also used in several other works, such as gender attribution by tracing stylometric evidence by (Sarawgi et al., 2011) , and native language detection by exploiting parse structures (Wang and Zong, 2011) . In the earlier work, Ashok et al. ( 2013 ) used stylistic approaches, such as unigram, bigram, part-of-speech distribution, grammatical rules, constituents, sentiment, and connotation as features and used Liblinear SVM (Fan et al., 2008) for the book success classification task. They used books from 8 genres, and they were able to achieve an average accuracy of 73.50% across all genres. Maharjan et al. ( 2017 ) used a set of hand-crafted features in combination with a recurrent neural network and generated feature representation to predict success. They obtained an average F1-score of 73.50% for 8 genres. In a more recent work by Maharjan et al. (2018a) , they used the flow of emotion throughout a book for success prediction and obtained an F1-score of 69%. In this paper, we used widely used feature reduction technique for SVM modeling. Guyon et al. (2002) used SVM weights for assigning ranks in the feature selection process. They verified that the topranked genes found by SVM have biological relevance to cancer and the SVM classifier with SVM selected features worked better than other classifiers in determining the relevant features along with the classification task. 3 Dataset Construction Original Dataset The original dataset from Ashok et al. (2013) is quite small as it only includes the first 1,000 sentences from 800 books split into 8 different genres, which are further split into successful and unsuccessful classes, each having 50 books. Additionally, many of the files included have less than 1,000 sentences, or contain automatically generated text from Project Gutenberg instead of the text from the proper novel. Finally, the books included are prelabeled with their successful/unsuccessful class where download counts are absent, which limits further testing. Considering these issues, we decided to build upon (Ashok et al., 2013) by creating a cleaner and more complete dataset. Additionally, we present multiple models that are both more accurate and more general than the best performing model in (Ashok et al., 2013) , unigram. From these models, we discovered more interesting and revealing qualities that separate successful from non-successful books. New Dataset We downloaded and used 17,962 English novels from Project Gutenberg: an online catalog of over 60,000 books, which are available to download for free in various formats (Gutenberg). We filtered the 60k books as follows: a) only English books, and b) only fiction books. We used a bash script 1 to harvest the novels from Project Gutenberg according to the webmaster's guidelines 2 . After downloading the books, we used the NLTK API for data processing (Bird et al., 2009) . For each book, we extracted the unigram and bigram frequencies, the part-of-speech (POS) tag using the Stanford CoreNLPParser frequencies, the lexical and non-lexical context free grammar production rules also using the Stanford CoreNLPParser, the Roget's Thesaurus Category frequencies, and the WordNet Synset frequencies (Roget, 1852; Princeton University, 2010; Zhu et al., 2013) . Like the authors of (Maharjan et al., 2018a) , we also extracted the NRC Emotional Lexicon features and the Linguistic Inquiry and Word Count (LIWC) features from each book (Mohammad and Turney, 2013; Pennebaker et al., 2015) . These emotional word mappings are highly valuable for some tasks, but the resulting models were not effective in our tests, and therefore not presented in this article. Like in (Ashok et al., 2013) , we also used the download count of each book to define success. In addition to predicting success classification for books in 12 unique genres, we also tested prediction performance independent of genre across the entire dataset. In both settings, we found an upper (\u03c5 + ) and lower (\u03c5 \u2212 ) download count threshold for classifying books of that genre as \"successful\" (with approx. more than 60% download count) or \"not successful\" (with approx. less than 40% download count) ensuring a balanced dataset (Table 1 ). We further collected Goodreads rating of 7,541 books out of 17,962 books that we discuss at the end of this paper. Methodology Linguistic Models We utilized 12 linguistic models for our quantitative analysis. 6 of the models are our own implementation of models used in (Ashok et al., 2013) . Our 6 additional models have not been used to make these types of qualitative conclusions until now. These models include WordNet (Princeton University, 2010), Roget's Thesaurus (Roget, 1852) , two models that map WordNet to different levels of Roget's Thesaurus, and two models that map context free grammar rules to Roget's Thesaurus. Mapping examples are given in Table 2 and explained below. Unigram: The frequency of unique words in text. Part-of-Speech Distribution: The authors of Ashok et al. ( 2013 ) demonstrated the value of PoS tag distribution in success prediction, and Koppel et al. (2006) presented the relationship between PoS tagging and genre detection and authorship attribution. Therefore, we reevaluated the application of PoS tag distribution for success prediction. Context Free Grammar Rule Distribution: We also reevaluate the analysis of CFG rule distribution as presented in (Ashok et al., 2013) , and use the same four categories: \u2022 \u0393: lexical production rules (productions where the right-hand symbol (RHS) is a terminal symbol (word)). \u2022 \u0393 G : lexical production rules prepended with the grandparent node. \u2022 \u03b3: nonlexical production rules (productions where the RHS is a non-terminal symbol). \u2022 \u03b3 G : nonlexical production rules prepended with the grandparent node. WordNet Implementation We used the sci-kit learn implementation of Lib-Linear SVM with 5-fold cross validation for class prediction (Pedregosa et al., 2011; Fan et al., 2008) . To tune the weighted linear SVM parameter C, we used the tool gridsearchCV (Pedregosa et al., 2011) and performed a search over the values ranging 1e(\u22124to3). Part-of-speech tag features are scaled with unit normalization, while all other features are scaled using tf-idf. We used two strategies for the class prediction task: predicting class by genre and predicting class independent of genre. We chose this model over neural models as it gives us better scope to interpret book success with hand crafted features. After the initial training and testing of each model, we employed an exhaustive feature reduction method, similar to our success labeling process, to maximize performance (see Figure 2 ). For a given model, we start with the mean feature weight learned during training. We remove all features from the dataset with |weight| less than the |mean| feature weight. Next, we train and test the model on this reduced fea- ture set and record the accuracy. For each subsequent test, starting at a step value of 0.25, we take only the features with weights greater than or equal to M ean(OriginalW eights) + (StdDev(OriginalW eights) * Step). This process continues, increasing the step value by 0.25 after each iteration, until one of the following conditions is met: 100% classification accuracy is achieved, maximum accuracy is found (determined if multiple consecutive subsequent feature sets produce decreasing performance), or the number of features is reduced to less than 1% of the original number of features. Additionally, as explained previously, the processes of mapping WordNet to Roget's Thesaurus is a feature reduction technique in its own right. Table 3 illustrates the degree of feature reduction when WordNet and \u0393 G are mapped. Experimental Results Using the original small dataset (Ashok et al., 2013) , the prediction accuracy for each model by genre is presented in Table 8 at Appendix A1, and highlights another primary reason for increasing the size of the dataset. As each of the models was found to be achieving 100% accuracy in success prediction, we were convinced that those  models were overfitting the dataset. This observation further motivated us to build a much larger dataset. For the classification task on newly constructed dataset, we applied the 5-fold cross validation method on all the genre specific datasets for evaluating each machine learning model. While for the feature reduction task, features were reduced using training weights from the training set, then tested on the test set. We had continued reducing the training set until the resulting features did not improve performance on the remaining test set. The prediction accuracy for each model across all books, and each model by genre, both before and after feature reduction are shown in Table 4 and  Table 5 , respectively. As illustrated in both settings, the performance of nearly every model improved after we reduced the features with \u03b3 G showing the largest improvement of an average of 24.3% when reduced by genre and WordNet improving the most by 16.1% when reduced independent of genre. in Table 4 and Table 5 . When predicting novel success by genre and independent of genre, \u0393 G R shows the best results predicting a book's success class with an accuracy of 94.0% and 80.1%, respectively. Furthermore, when predicting success by genre, \u0393 G R achieves the highest accuracy for each genre except DETECTIVE. For DETECTIVE novels, \u03b3 G R outperforms all models with 100% accuracy. Figure 2 illustrates the pattern of performance improvement that each model exhibits through the feature reduction process both by genre and independent of genre. As the number of features is reduced, the average accuracy for success prediction increases until the algorithm finds the best set of features and achieves peak performance. Then accuracy sharply drops as the feature set is reduced further. The fact that each model demonstrates such behavior validates the effectiveness of our feature reduction method. Interpreting Book Success Prediction While our reduced \u0393 G and WordNet models display excellent performance in both test settings (by genre and independent of genre), the resulting feature sets are not self-explanatory. In other words, the respective lexical production rules and Synsets that the models deem most important do not necessarily highlight some interesting aspect of successful books. This is where Roget's Thesaurus proves most valuable. We figured that if we looked up the Roget Theme of the RHS for each lexical production rule and the Roget Theme for each WordNet Synset we would find that the successful and unsuccessful books prioritize different Themes. With this hypothesis in mind, we mapped the reduced WordNet and reduced \u0393 G models to new Roget models by first looking up the Roget Category of each Synset and RHS, respectively, from the reduced feature sets, and then summing the frequencies in each group of Sysnets/symbols. As we did with each previous model, we reduced the new WNRC and \u0393 G RC models. From the WNRC R and \u0393 G RC R models we mapped again, this time from Roget Categories to the 23 Roget Themes, which produced the WNRT and \u0393 G RT models. Mapping examples are given in Table 2 and its outcome is detailed in the Figure 3 . We did not expect the performance of the WNRC and \u0393 G RC models, since they were conceived strictly as intermediary maps between WordNet/\u0393 G and Roget Themes. \u0393 G RC produced the highest baseline results of all the models without any feature reduction used in our experiments with 81.9% average accuracy by genre. Furthermore, \u0393 G RC R accurately predicts success classification per genre at an average rate of 88.4%. What's impressive about the accuracy of \u0393 G RC R , when compared to that of \u0393 G , is the large difference in number of features used in each model as shown when predicting DETECTIVE novels in Table 3 . With these impressive results from \u0393 G RC R , we expected \u0393 G RT and \u0393 G RT R to follow suit despite learning with a feature set of at most 13 features. However, this was not the case as \u0393 G RT R predicts the success of a book by its genre with an average accuracy of only 69.9%. As previously stated, the motivation for the construction of WNRT and \u0393 G RT was strictly to find a common thread between successful novels in each genre. Therefore, the poor performance of the WNRT R and \u0393 G RT R models does not undercut the reasoning behind its conception, and the high accuracy of WordNet R , WNRC R , \u0393 G R , and \u0393 G RC R supports our claim that each is a general model that can reveal underlying characteristics of successful books. Additionally, WNRT and \u0393 G RT do not improve performance after feature reduction when classifying independent of genre. This outcome also supports our original hypothesis as it shows that the models require each of the 23 Roget Themes in order to make the most accurate prediction. The lack of improvement in WNRT R and \u0393 G RT R when Figure 3 : This heatmap presents how the mapping of \u0393 G to RT helps to interpret success of ADVENTURE books. The plot presents both +ve/\u2212ve Roget Themes on the row, and successful/unsuccessful books on the column. Each cell represents the relative frequency of observing \u0393 G in an RT . We observe that authors of successful books used certain CFGs that result in higher frequency in +veRT cells, while the unsuccessful books have higher frequency in the \u2212veRt cells. predicting success class independent of genre also demonstrates the relationship between a novel's genre and its prioritization of certain Themes.    After mapping the resulting feature weights of our WordNet R and \u0393 G R models to Roget Themes, we were able to highlight the most important Themes when classifying the success of a novel given its genre. Table 6 gives the most important themes in predicting the success of CHILDREN'S novels and the successful and unsuccessful semantic word groups within those themes. These results clearly identify words associated with \"school\" and \"grammar\" as key contributors to unsuccessful CHIL-DREN'S novels, while words like \"secret,\" \"enthusiastic,\" and \"selfishness\" contribute to successful CHILDREN'S novels. Successful Categories and Themes for a Genre The indicated Themes align with intuitive expectations for CHILDREN'S books, especially the presence of FORMATION OF IDEAS and MORAL. To verify these results, we looked at the most downloaded CHILDREN'S book, Little Women. We ranked each book in the CHILDREN'S genre according to the frequency of each prioritized Theme listed in Table 7 . Then, we looked to see where Little Women ranked for each of the Themes. Little Women's use of the top Themes matches up as expected, as it ranks in the top three for four of the five most important Themes, and eighth for the fifth as shown in Table 7 . The opposite is true for the least downloaded books, which all rank at the bottom for use of the most important Themes. Our Thematic observations hold true for each genre, but there is not one Theme shared by all 12 genres. This adheres to the observation we made about WNRT and \u0393 G RT and each model's lack of improvement after feature reduction for predicting success across all books independent of genre. Experiments with Goodreads Rating The discoveries made in our research are just the beginning of what can be done with our dataset. In addition to the data utilized for this project, we also extracted Goodreads Rating 3 as proposed in (Maharjan et al., 2019) . We could collect the rating for 7,541 books from a total of 17,962 books scraped from Project Gutenberg, where each book has been rated by at least 5 readers. We labeled all the books having average rating >= 3.5 as successful, and < 3.5 as unsuccessful (presented in Table 10 at Appendix A2). In Appendix A1, Table 9 demonstrates the performances of previous and our models, respectively. When predicting novel success by genre, \u0393 G shows the best results predicting a book's success class with an average weighted F1-score of 92.2% outperforming previous stateof-the-art results(75% for the genre attention with RNN method (Maharjan et al., 2018b )) as well. This result validates the applicability of our proposed model for book success prediction. Conclusion We created the largest dataset for evaluating book success, and presented a novel study of how context free grammar rules and semantic word association of influence a book's success. Our empirical results demonstrate that our large dataset combined with our feature reduction technique can predict a book's success with better accuracy than the current stateof-the-art methods. The analysis performed in this project shows the relationship between thematic word groups and a book's popularity, with our best model that uses context free grammar lexical production rules (\u0393 G R ) achieving a prediction accuracy of 94.0%. Finally, we illustrated that readers expect certain themes to be prioritized over others based on a book's genre, and the proper use of those themes directly contributes to a book's popularity. Appendix A1 Results MODEL Here, GR stands for Goodreads, while SB, UB and GRC stands for successful books unsuccessful books and Goodreads count, respectively. We could collect a total of 7,541 book ratings from Goodreads, as opposed to the total 17,962 downloaded books from the Project Gutenberg website.",
    "abstract": "In this paper, we attempt to improve upon the state-of-the-art in predicting a novel's success by modeling the lexical semantic relationships of its contents. We created the largest dataset used in such a project containing lexical data from 17,962 books from Project Gutenberg. We utilized domain specific feature reduction techniques to implement the most accurate models to date for predicting book success, with our best model achieving an average accuracy of 94.0%. By analyzing the model parameters, we extracted the successful semantic relationships from books of 12 different genres. We finally mapped those semantic relations to a set of themes, as defined in Roget's Thesaurus and discovered the themes that successful books of a given genre prioritize. At the end of the paper, we further showed that our model demonstrate similar performance for book success prediction even when Goodreads rating was used instead of download count to measure success.",
    "countries": [
        "United States",
        "Bangladesh"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2021",
    "month": "September",
    "title": "Syntax and Themes: How Context Free Grammar Rules and Semantic Word Association Influence Book Success"
}