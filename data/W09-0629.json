{
    "article": "The TUNA-REG'09 Challenge was one of the shared-task evaluation competitions at Generation Challenges 2009. TUNA-REG'09 used data from the TUNA Corpus of paired representations of entities and human-authored referring expressions. The shared task was to create systems that generate referring expressions for entities given representations of sets of entities and their properties. Four teams submitted six systems to TUNA-REG'09. We evaluated the six systems and two sets of human-authored referring expressions using several automatic intrinsic measures, a human-assessed intrinsic evaluation and a human task performance experiment. This report describes the TUNA-REG task and the evaluation methods used, and presents the evaluation results. Introduction This year's run of the TUNA-REG Shared-Task Evaluation Competition (STEC) is the third, and final, competition to involve the TUNA Corpus of referring expressions. The TUNA Corpus was first used in the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge (Belz and Gatt, 2007) which took place between May and September 2007; and again for three of the shared tasks in Referring Expression Generation (REG) Challenges 2008, which ran between September 2007 and May 2008 (Gatt et al., 2008) . This year's TUNA Task replicates one of the three tasks from REG'08, the TUNA-REG Task. It uses the same test data, to enable direct comparison against the 2008 results. Four participating teams submitted 6 different systems this year; teams and their affiliations are shown in Table 1 . Data Each file in the TUNA corpus 1 consists of a single pairing of a domain (a representation of 7 entities and their attributes) and a human-authored description for one of the entities (the target referent). Some domains represent sets of people, some represent items of furniture (see also Table 2 ). The descriptions were collected in an online elicitation experiment which was advertised mainly on a website hosted at the University of Zurich Web Experimentation List 2 (a web service for recruiting subjects for experiments), and in which participation was not controlled or monitored. In the experiment, participants were shown pictures of the entities in the given domain and were asked to type a description of the target referent (which was highlighted in the visual display). The main condition 3 manipulated in the experiment was +/\u2212LOC: in the +LOC condition, participants were told that they could refer to entities using any of their properties (including their location on the screen). In the \u2212LOC condition, they were discouraged from doing so, though not prevented. The XML format we have been using in the TUNA-REG STECs, shown in Figure 1 , is a variant of the original format of the TUNA corpus. The root TRIAL node has a unique ID and an indication of the +/ \u2212 LOC experimental condi-tion. The DOMAIN node contains 7 ENTITY nodes, which themselves contain a number of ATTRIBUTE nodes defining the possible properties of an entity in attribute-value notation. The attributes include properties such as an object's colour or a person's clothing, and the location of the image in the visual display which the DOMAIN represents. Each ENTITY node indicates whether it is the target referent or one of the six distractors, and also has a pointer to the image that it represents. The WORD-STRING is the actual description typed by one of the human authors, the ANNOTATED-WORD-STRING is the description with substrings annotated with the attributes they realise, while the ATTRIBUTE-SET contains the set of attributes only. The ANNOTATED-WORD-STRING and ATTRIBUTE-SET nodes were provided in the training and development data only, to show how substrings of a human-authored description mapped to attributes. Apart from differences in the XML format, the data used in the TUNA-REG Task also differs from the original TUNA corpus in that it has only the singular referring expressions from the original corpus, and in that we have added to it the files of images of entities that the XML mark-up points to. The test set, which was constructed for the 2008 run of the TUNA-REG Task, consists of 112 items, each with a different domain paired with two human-authored descriptions. The items are distributed equally between furniture items and people, and between both experimental conditions (+/ \u2212 LOC). In the following sections, the two sets of human descriptions will be referred to as HUMAN-1 and HUMAN-2. 4 The numbers of files in the training, development and test sets, as well as in the people and furniture subdomains, are shown in Table 2 . The TUNA-REG Task Referring Expression Generation (REG) has been the subject of intensive research in the NLG community, giving rise to substantial consensus on the problem definition, as well as the nature of the inputs and outputs of REG algorithms. Typically, such algorithms take as input a domain, consisting of entities and their attributes, together with an indication of which is the intended referent, and output a set of attributes true of the referent which distinguish it from other entities in the domain. The TUNA-REG task adds an additional stage (realisation) in which selected attributes are mapped to a natural language expression (usually a noun phrase). Realisation has received far less attention among REG researchers than attribute selection. The TUNA-REG task is an 'end-to-end' referring expression generation task, in the sense that it takes as input a representation of a set of entities and their properties, and outputs a word string which describes the target entity. Participating systems were not constrained to have attribute selection as a separate module from realisation. In terms of the XML format, the items in the test set distributed to participants consisted of a DOMAIN node and ATTRIBUTE-SET, and participating systems had to generate appropriate WORD-STRINGs. As with previous STECs involving the TUNA data, we deliberately refrained from including in the task definition any aim that would imply assumptions about quality (as would be the case if we had asked participants to aim to produce, say, minimal or uniquely distinguishing referring expressions), and instead we simply listed the evaluation criteria that were going to be used (described in Section 5). Participating Teams and Systems This section briefly describes this year's submissions. Full descriptions of participating systems can be found in the participants' reports included in this volume. IS: The submission of the IS team, IS-FP-GT, is based on the idea that different writers use different styles of referring expressions, and that, therefore, knowing the identity of the writer helps generate REs similar to those in the corpus. The attribute-selection algorithm is an extended fullbrevity algorithm which uses a nearest neighbour technique to select the attribute set (AS) most similar to a given writer's previous ASs, or, in a case where no ASs by the given writer have previously been seen, to select the AS that has the highest degree of similarity with all previously seen ASs by any writer. If multiple ASs remain, the algorithm first selects the shortest, then the most representative of the remaining REs, then the AS with the highest-frequency attributes. Individualised statistical models are used to convert the selected AS into a surface-syntactic dependency tree which is then converted to a word stirng with an existing realiser. GRAPH: The GRAPH team reused their existing graph-based attribute selection component, which represents a domain as a weighted graph, and uses a cost function for attributes. The team developed a new realiser which uses a set of templates derived from the descriptions in the TUNA corpus. In order to build templates, certain subsets of attributes were grouped together, individual attributes were replaced by their type, and a preferred order for attributes was determined based on frequencies of orderings. During realisation, if a matching template exists, types are replaced with the most frequent word string for each given attribute; if no match exists, realisation is done by a simple rule-based method. NIL-UCM: The three systems submitted by this group use a standard evolutionary algorithm for attribute selection where genotypes consist of binary-valued genes each representing the presence or absence of a given attribute. Realisation is done with a case-based reasoning (CBR) method which retrieves the most similar previously seen ASs for an input AS, in order of their similarity to the input. (Sub)strings are then copied from the preferred retrieved case to create the output word string. One system, NIL-UCM-EvoCBR uses both components as described above. The other two systems, NIL-UCM-ValuesCBR and NIL-UCM-EvoTAP, replace one of the components with the team's corresponding component from REG'08. USP: The system submitted by this group, USP-EACH, is a frequency-based greedy attribute selection strategy which takes into account the +/ \u2212 LOC attribute in the TUNA data. Realisation was done using the surface realiser supplied to participants in the ASGRE'07 Challenge. Evaluation Methods and Results We used a range of different evaluation methods, including intrinsic and extrinsic, 5 automatically computed and human-evaluated, as shown in the overview in Table 3 . Participants computed automatic intrinsic evaluation scores on the development set (using the teval program provided by us). We performed all of the evaluations shown in Table 3 on the test data set. For all measures, results were computed both (a) overall, using the entire test data set, and (b) by entity type, that is, computing separate values for outputs in the furniture and in the people domain. Evaluation methods for each evaluation type and corresponding evaluation results are presented in the following three sections. Automatic intrinsic evaluations Humanlikeness, by which we mean the similarity of system outputs to sets of human-produced reference 'outputs', was assessed using Accuracy,  string-edit distance, BLEU-3 and NIST-5. Accuracy measures the percentage of cases where a system's output word string was identical to the corresponding description in the corpus. Stringedit distance (SE) is the classic Levenshtein distance measure and computes the minimal number of insertions, deletions and substitutions required to transform one string into another. We set the cost for insertions and deletions to 1, and that for substitutions to 2. If two strings are identical, then this metric returns 0 (perfect match). Otherwise the value depends on the length of the two strings (the maximum value is the sum of the lengths). As an aggregate measure, we compute the mean of pairwise SE scores. BLEU-x is an n-gram based string comparison measure, originally proposed by Papineni et al. (2001; 2002) for evaluation of Machine Translation systems. It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs. Setting x = 4 (i.e. considering all n-grams of length \u2264 4) is standard, but because many of the TUNA descriptions are shorter than 4 tokens, we compute BLEU-3 instead. BLEU ranges from 0 to 1. NIST is a version of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent n-grams, which are taken to be more informative. The maximum NIST score depends on the size of the test set. Unlike string-edit distance, BLEU and NIST are by definition aggregate measures (i.e. a single score is obtained for a peer system based on the entire set of items to be compared, and this is not generally equal to the average of scores for individual items). Because the test data has two human-authored reference descriptions per domain, the Accuracy and SE scores had to be computed slightly differently to obtain test data scores (whereas BLEU and NIST are designed for multiple reference texts). For the test data only, therefore, Accuracy expresses the percentage of a system's outputs that match at least one of the reference outputs, and SE is the average of the two pairwise scores against the reference outputs. Results: Table 4 is an overview of the selfreported scores on the development set included in the participants' reports (not all participants report Accuracy scores). The corresponding scores for the test data set as well as NIST scores for the test data (all computed by us), are shown in Table 5 . The table also includes the result of comparing the two sets of human descriptions, HUMAN-1 and HUMAN-2, to each other using the same metrics (their scores are distinct only for non-commutative measures, i.e. NIST and BLEU). We ran 6 a one-way ANOVA for the SE scores. There was a main effect of SYSTEM on SE (F = 10.938, p < .001). A post-hoc Tukey HSD test with \u03b1 = .05 revealed a number of significant differences: all systems were significantly better than the human-authored descriptions, and GRAPH was furthermore significantly better than NIL-UCM-EvoCBR. We also computed the Kruskal-Wallis H value for the systems' individual Accuracy scores, using a chi square test to establish significance. By this test, the observed aggregate difference among the seven systems is significant at the .01 level (\u03c7 2 7 = 20.169). Human intrinsic evaluation The TUNA'09 Challenge was the first TUNA shared-task competition to include an intrinsic evaluation involving human judgments of quality. Design: The intrinsic human evaluation involved descriptions for all 112 test data items from all six submitted systems, as well as from the two sets of human-authored descriptions. 7 Thus, each of the 112 test set items was associated with 8 different descriptions. We used a Repeated Latin Squares design which ensures that each subject sees descriptions from each system and for each domain the same number of times. There were fourteen 8 \u00d7 8 squares, and a total of 896 individual judgments in this evaluation, each system receiving 112 judgments (14 from each subject). Procedure: In each of the 112 trials, participants were shown a system output (i.e. a WORD-STRING), together with its corresponding domain, displayed as the set of corresponding images on the screen. 8 The intended (target) referent was highlighted by a red frame surrounding it on the screen. They were asked to give two ratings in answer to the following questions (the first for Adequacy, the second for Fluency): 1. How clear is this description? Try to imagine someone who could see the same grid with the same pictures, but didn't know which of the pictures was the target. How easily would they be able to find it, based on the phrase given? 2. How fluent is this description? Here your task is to judge how well the phrase reads. Is it good, clear English? We did not use a rating scale (where integers correspond to different assessments of quality), because it is not generally considered appropriate to apply parametric methods of analysis to ordinal data. Instead, we asked subjects to give their judgments for Adequacy and Fluency for each item by manipulating a slider like this: The slider pointer was placed in the center at the beginning of each trial, as shown above. The position of the slider selected by the subject mapped to an integer value between 1 and 100. However, the scale was not visible to participants, whose task was to move the pointer to the left or right. The further to the right, the more positive the judgment (and the higher the value returned); the further to the left, the more negative. Following instructions, subjects did two practice examples, followed by the 112 test items in random order. Subjects carried out the experiment over the internet, at a time and place of their choosing, and were allowed to interrupt and resume the experiment. According to self-reported timings, subjects took between 25 and 60 minutes to complete the experiment (not counting breaks). Participants: We recruited eight native speakers of English from among post-graduate students currently doing a Masters degree in a linguisticsrelated subject. 9  We recorded subjects' gender, level of education, field of study, proficiency in English, variety of English and colour vision. Since all subjects were native English speakers, had normal colour vision, and had comparable levels of education and academic backgrounds, as indicated above, these variables are not included in the analyses reported below.  over People and Furniture Items. On Adequacy, there were main effects of SYSTEM (F (7, 880) = 7.291, p < .001) and DOMAIN (F (1, 880) = 29.133, p < .001), with a significant interaction between the two (F (7, 880) = 15.30, p < .001). On Fluency, there were main effects of SYSTEM (F (7, 880) = 18.14) and of DOMAIN (F (7, 880) = 17.20), again with a significant SYSTEM \u00d7 DOMAIN interaction (F (7, 880) = 5.60), all significant at p < .001. Post-hoc Tukey comparisons on both dependent measures yielded the homogeneous subsets displayed in Table 7 . Results: Extrinsic task-performance evaluation As for earlier shared tasks involving the TUNA data, we carried out a task-performance experiment in which subjects have the task of identifying intended referents. Design: The extrinsic human evaluation involved descriptions for all 112 test data items from all six submitted systems, as well as from the two sets of human-authored descriptions. We used a Repeated Latin Squares design with fourteen 8\u00d7 8 squares, so again there were a total of 896 individual judgments and each system received 112 judgments, however this time it was 7 from each subject, as there were 16 participants; so half the participants did the first 56 items (the first 7 squares), and the other half the second 56 (the remaining 7 squares). Procedure: In each of their 5 practice trials and 56 real trials, participants were shown a system output (i.e. a WORD-STRING), together with its corresponding domain, displayed as the set of corresponding images on the screen. In this experiment the intended referent was not highlighted in the onscreen display, and the participants' task was to identify the intended referent among the pictures by mouse-clicking on it. 10  In previous TUNA identification experiments (Belz and Gatt, 2007; Gatt et al., 2008) , subjects had to read the description before identifying the intended referent. In ASGRE'07 both description and pictures were displayed at the same time, yielding a single time measure that combined reading and identification times. In REG'08, subjects first read the description and then called up the pictures on the screen when they had finished reading the description, which yielded separate reading and identification times. This year we tried out a version of the experiment where subjects listened to descriptions read out by a synthetic voice 11 over headphones while looking at the pictures displayed on the screen. Stimulus presentation was carried out using DMDX, a Win-32 software package for psycholinguistic experiments involving time measurements (Forster and Forster, 2003) . Participants initiated each trial, which consisted of an initial warning bell and a fixation point flashed on the screen for 1000ms. Following this, the visual domain was displayed, and the voice reading the description was initiated after a delay of 500ms. We recorded time in milliseconds from the start of display to the mouse-click whereby a participant identified the target referent. This is hereafter referred to as the identification speed. The analysis reported below also uses identification accuracy, the percentage of correctly identified target referents, as an additional dependent variable. Trials timed out after 15, 000ms. Participants: The experiment was carried out by 16 participants recruited from among the faculty and administrative staff of the University of Brighton. All participants carried out the experiment under supervision in the same quiet room on the same laptop, in the same ambient conditions, with no interruptions. All participants were native speakers, and we recorded type of post, whether they had normal colour vision and hearing, and whether they were left or right-handed. Timeouts and outliers: None of the trials reached time-out stage during the experiment. Outliers were defined as those identification times which fell outside the mean \u00b12SD (standard deviation) range. 44 data points (4.9%) out of a total of 896 were identified as outliers by this definition; these were replaced with the series mean (Ratliff, 1993) . The results reported for identification speed below are based on these adjusted times. Results: Table 8 displays mean identification speed and identification accuracy per system. A univariate ANOVA on identification speed revealed significant main effects of SYSTEM (F (7, 880) = 4.04, p < .001) and DOMAIN (F (1, 880) = 11 We used the University of Edinburgh's Festival speech generation system (Black et al., 1999) in combination with the nitech us slt arctic hts voice, a high-quality female American voice. 11.53, p < .001), with a significant interaction (F (7, 880) = 6.02, p < .001). Table 9 displays homogeneous subsets obtained following pairwise comparisons using a post-hoc Tukey HSD analysis. We treated identification accuracy as an indicator variable (indicating whether a participant correctly identified a target referent or not in a given trial). A Kruskal-Wallis test showed a significant difference between systems (\u03c7 2 7 = 44.98; p < .001). Correlations Table 10 displays the correlations between the eight evaluation measures we used. The numbers are Pearson product-moment correlation coefficients, calculated on the means (1 mean per system on each measure). As regards the human-assessed intrinsic scores, there is no significant correlation between Adequacy and Fluency. Among the automatically computed intrinsic measures, the only significant correlation is between Accuracy and BLEU. For the extrinsic identification performance measures, there is no significant correlation between Identification Accuracy and Identification Speed. As for correlations across the two types (human-assessed and automatically computed) of intrinsic measures, the only significant correlations are between Fluency and Accuracy, and between Adequacy and Accuracy. So, a system with a higher percentage of human-like outputs (as measured by Accurach) also tends to be scored more highly in terms of Fluency and Adequacy by humans. We also found significant correlations between intrinsic and extrinsic measures: there was a strong and significant correlation between Identification Accuracy and Adequacy, implying that more adequate system outputs allowed people to identify target referents more correctly; there was also a significant (negative) correlation between Fluency and Identification Speed, implying that more fluent descriptions led to faster identification. While these results differ from previous findings (Belz and Gatt, 2008) , in which no significant correlations were found between extrinsic measures and automatic intrinsic metrics, it is worth noting that significance in the results reported here was only observed between human-assessed intrinsic measures and the extrinsic ones. Concluding Remarks The three editions of the TUNA STEC have attracted a substantial amount of interest. In addition to a sizeable body of new work on referring expression generation, as another tangible outcome of these STECs we now have a wide range of different sets of system outputs for the same set of inputs. A particularly valuable resource is the pairing of these outputs from the submitted systems in each edition with evaluation data. As this was the last time we are running a STEC with the TUNA data, we will now make all data sets, documentation and evaluation software from all TUNA STECs available to researchers. We are planning to add to these as many system outputs as we can, so that other researchers can perform evaluations involving these. We are also planning to complete our evalua-tions of the evaluation methods we have developed. Among such experiments will be direct comparisons between the results of the three variants of the identification experiment we have tried out, and a direct comparison between different designs for human-assessed intrinsic evaluations (e.g. comparing the slider design reported here to preference judgments and rating scales). Apart from the technological progress in REG which we hope the TUNA STECs have helped achieve, perhaps the single most important scientific result is strong evidence for the importance of extrinsic evaluations, as these do not necessarily agree with the results of much more commonly used intrinsic types of evaluations. Acknowledgments We thank our colleagues at the University of Brighton who participated in the identification experiment, and the Masters students at UCL, Sussex and Brighton who participated in the quality assessment experiment. The evaluations were funded by EPSRC (UK) grant EP/G03995X/1.",
    "abstract": "The TUNA-REG'09 Challenge was one of the shared-task evaluation competitions at Generation Challenges 2009. TUNA-REG'09 used data from the TUNA Corpus of paired representations of entities and human-authored referring expressions. The shared task was to create systems that generate referring expressions for entities given representations of sets of entities and their properties. Four teams submitted six systems to TUNA-REG'09. We evaluated the six systems and two sets of human-authored referring expressions using several automatic intrinsic measures, a human-assessed intrinsic evaluation and a human task performance experiment. This report describes the TUNA-REG task and the evaluation methods used, and presents the evaluation results.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "104",
    "year": "2009",
    "month": "March",
    "title": "The {TUNA}-{REG} {C}hallenge 2009: Overview and Evaluation Results"
}