{
    "article": "Knowledge Base Completion infers missing facts from existing ones in knowledge bases. As recent Open Information Extraction systems allow us to extract ever larger (yet incomplete) open-domain Knowledge Bases from text, we seek to probabilistically extend the limited coverage we get from existing facts, to arbitrary queries about plausible information. We propose a simple baseline, based on language modeling and trained with off-the-shelf programs, which gives competitive results in the previously defined protocol for this task, and provides an independent source of signal to judge arbitrary fact plausibility. We reexamine this protocol, measure the (large) impact of the negative example generation procedure, which we find to run contrary to the belief put forward in previous work. We conduct a small manual evaluation, giving insights into the rudimentary automatic evaluation protocol, and analyse the shortcomings of our model. Introduction Much recent effort has been put into building Knowledge Bases (KBs), either manually curated (Freebase (Bollacker et al., 2008) , Cyc (Lenat, 1995) ) or automatically produced (YAGO (Suchanek et al., 2007) , Knowledge Vault (Dong et al., 2014) ), ranging from logically consistent linked-data in OWL (SUMO (Pease et al., 2002) ) to little-structured sets of textual relations extracted from text (NELL (Mitchell et al., 2015) ) with Open IE systems (Reverb, Ollie (Mausam et al., 2012) , ClausIE (Del Corro and Gemulla, 2013) , Stanford Open IE (Angeli et al., 2015) , CSD-IE (Bast and Haussmann, 2013) ). However large they may be, typical KBs are largely incomplete, and many relevant facts are missing (West et al., 2014) . Because an exhaustive coverage of the information that ought to be part of the KB is a very desirable feature, KB completion (inference of missing facts from known ones) is a rapidly growing field (West et al., 2014; Nickel et al., 2015; Wang et al., 2015; Toutanova et al., 2016) . In the context of Open Information Extraction (OIE), our aim is to assign a score to an arbitrary unseen query fact 1 , judging its plausibility as a member of the KB. This task is important for three reasons : first, it extends the coverage of the existing KB probabilistically to any query, greatly improving upon the closed-world assumption that facts not known to be true are false. Second, as the extraction of information in the open domain is a relatively noisy process, a confidence score helps detecting extraction errors, and makes for higher-quality automatically generated KBs. Last, adjusting the confidence threshold of extracted facts allows to tune as desired the trade-off between precision and recall of the extraction process. The task has attracted little attention since it was introduced in (Angeli and Manning, 2013) , most that we know in (Li et al., 2016) . The authors propose to assign high KB-membership probability to facts that have support facts existing in the KB, which are similar to them (based on 1 A \"fact\" is a relation phrase linking two or more argument phrases. The arguments need not be named entities. common phrases and word similarity). We propose a new baseline for this task, in the form of a language model. Whereas the method of (Angeli and Manning, 2013) is fairly complicated to implement, and requires indexing the KB in various ways for intermediate computations, a trained language model is very compact, straightforward to implement and train, and fast to process requests at use time. We train the model on automatically extracted facts (including some noise) from the same corpus, i.e. the knowledge base, taken as a list of sentences. We experiment with language model features and show that a linear classifier gives good results at the task of recognising actual extracted facts, perhaps unsurprisingly. We then go back over the experimental protocol proposed by (Angeli and Manning, 2013) and consider the way negative examples are automatically generated. We find that this procedure has a significant impact on the difficulty of the task. At last, we go back to the goal of improving existing extractions by picking out the noise. Instead of an automatically generated test set, we measure the ability of the models to identify the remaining wrong extractions in the RV-15M high-quality dataset (presented in section 4.1.). Related work Knowledge Base Completion Much work in Knowledge Base Completion (KBC) has been done in recent years (Bordes et al., 2013; Riedel et al., 2013; Garc\u00eda-Dur\u00e1n et al., 2015; Feng et al., 2016; Trouillon et al., 2016) , on tasks very similar to ours, mostly focusing on Freebase, and other such large manually curated KBs (WordNet, NCI-PID (Schaefer et al., 2008), etc.) . The major difference between our approach and most KBC work is the predefined schema of the KB. The arguments of the relations curated in Freebase are mostly named entities, and the relations to be gathered were defined when building the KB. FB15k, a popular Freebase dataset, covers 1345 predicates, though only 401 have more than 100 occurences (Yang et al., 2014) . NELL captures about 150 relations, and WordNet about 20. By contrast OIE seeks to extract all the relations expressed in text, resulting in hundreds of thousands of relation predicates (even though many are synonyms). The RV-15M dataset used in this work has 660k distinct relation strings. (Toutanova et al., 2015) embed surface textual patterns in the same vector space as the KB relations, which is similar to the implicit embedding of all predicates in the same vector space as we do. Yet their work only predicts relations based on the 237 predicates of the FB15k-237 dataset, whereas we predict confidence scores for all relations, including predicates never seen during training. 2.2. Angeli and Manning (2013) In this work we reexplore in depth the task set up in (Angeli and Manning, 2013) , and it is their work that is most similar to ours. They seek to probabilistically extend a KB to arbitrarily any query fact, in the sense that any candidate fact has a KB-membership probability (or confidence score). Indeed, this is a sensible way of considering a knowledge base system that must perform inferences. To this aim, they compare a query fact to a set of candidate support facts from the KB. The candidate facts need to share 2 phrases (arguments or relation) with the query fact, and is allowed to differ by the third part. The query fact has a high KB-membership score if it is similar enough to its closest support facts (that is, the differing part is similar enough). The algorithm is as follows: \u2022 Gather candidate facts that can support the query fact: candidates must share two of (argument 1, relation, argument 2) with the query, these phrases having at least the same head word. Stricter criteria are used as long as there is a sufficient number of candidates. \u2022 Compute the distances between the query fact and each of its supporting facts, using 11 distance metrics, based on both distributional similarity and the Word-Net thesaurus -cosine, Jaccard, etc. \u2022 The highest similarities are used as weights in a linear classifier, whose job is to agreggate the similarity values across candidates and distance metrics. OIE Systems Confidence Contrarily to most OIE systems in which the confidence score is often an afterthought, Fader et al. (2011) went to great lengths to develop Reverb's confidence function (see their Section 4.2). They manually labeled the extractions from 1000 Web sentences as correct or incorrect, and trained a classifier using features about the original sentence to assign a confidence score to the extraction process. The confidence function of Ollie is based on the frequency of the syntactic pattern that was used to extract a given fact. ClausIE simply returns the confidence score of the underlying dependency parser, as its rules rely directly on it. In contrast to our work, the confidence scores produced by OIE systems rely on the original sentence from which they were extracted, and on how well the extraction procedures could handle the input sentences. Our goal is to judge the quality of query facts as they stand, regardless of the sentences they come from. For instance, with the Chomsky sentence Colorless green ideas sleep furiously as input, past OIE evaluation would consider (ideas, sleep, furiously) to be a correct extractions, whereas the goal of our system is to reject any \"fact\" coming from that sentence on the grounds that they do not make sense. Revisiting the Task Setup Our end goal is to improve the OIE process by pruning out the erroneous or empty facts produced. We frame this as a classification problem, seeking to distinguish correct useful facts from ill-constructed or void statements. Task Protocol A KB is constructed by running an open information extractor over a textual corpus. Even though there is some noise, extracted facts are assumed to be correct, and samples of them are set aside from the KB to constitute positive examples of unseen facts for the classification task. For negative examples, artificial facts are constructed by replacing one part of a genuine extraction with that of another. Let (a 1 , r, a 2 ) and (a 1 , r , a 2 ) be two genuine facts from the KB, then one negative example is picked between (a 1 , r, a 2 ), (a 1 , r , a 2 ) and (a 1 , r, a 2 ). We will show in Section 4.3. that this choice is a very significant parameter of the experimental setup. Classifiers are trained to discriminate the positive from the negative examples. The performance metric is the classification accuracy. Approaches ArgSim is a weak baseline for the task, measuring the cosine similarity between a 1 and a 2 in a fact. Arguments (effectively bag-of-words) are represented by the average of their individual words' embeddings. This performs well on ConceptNet, per (Li et al., 2016) , but captures little information on OIE facts. This is both reported by Angeli and Manning (2013) and replicated in our experiments. The full algorithm of (Angeli and Manning, 2013) , presented in Section 2.2., is denoted AM-system in Table 1 . We reimplemented the count and cos methods used in their evaluation, which are both simplified versions of their approach (the former coarse, and the latter close in principle to the full-fledged scoring function). As has been noted by (Stanovsky et al., 2015) , OIE output can be used as training material for other tasks such as text comprehension, word similarity and analogy. This is because OIE produces a distinctive intermediate representation of the sentence, from which complementary features (to that of dependency parse or lexical representations) can be extracted. Moreover, in the confidence scoring function of Reverb, several features capture how completely the extraction covers the sentence's tokens. In short, the most typical correct extractions look like short declarative sentences, like (Hudson, was born in, Hampstead), or (Hampstead, is a suburb of, London). Then, it seems natural to train a language model on confidently extracted facts, and to expect from correct unseen extractions that they fit well and cause low perplexity. This implements the assumption that an unseen fact is plausible iff it resembles a short and naturalsounding sentence. The most basic implementation of this idea (LM-basic) is to straightforwardly use the probability of concatenated (a 1 .r.a 2 ) as its score. Next, we notice that given the way negative facts are constructed, all three argument and relation spans are probable as they stand (since they come from genuine extractions). What makes the fact incorrect is that the parts do not fit together. Therefore, we use as a score the (log) probability of the whole fact minus that of each individual part. We call this model LMjunctions. With {a 1 , r, a 2 } the fact expressing that the relation r holds between the two arguments a 1 and a 2 , and p the language model probability function : score({a 1 , r, a 2 }) = log p(a 1 .r.a 2 ) \u2212 log p(a 1 ) \u2212 log p(r) \u2212 log p(a 2 ). We trained the language models with KenLM (Heafield, 2011) , using the knowledge base itself as a corpus, each fact being considered as a sentence. We trained 5-gram models (the default), doing as little parameter tuning as possible. Further, we train a linear classifier based on linguistic modeling-based features (LM-SVM). We implemented the SVM with scikit-learn (Pedregosa et al., 2011) 2 . It uses 20 features such as the individual log-probabilities of the various parts, the log-probabilities of various bigrams and trigrams focused on the argument-relation junctions, and arithmetic operations over those values. The classifier is trained on 10k genuine extractions as positive examples, and 10k artificially constructed ones as negative examples, counts picked to be the same as those in (Angeli and Manning, 2013) . Experiments Dataset We used Reverb-15M, a shared 3 dataset of high-quality binary assertions extracted by Reverb on the ClueWeb09 corpus. In order to obtain a high-precision dataset, its authors filtered the extractions by Reverb's confidence function (with a 0.9 threshold), stopwords, and frequency, along with certain syntactic criteria. 4 We used the normalized (lemmatized) version of the tuples. Taken as a text corpus, the RV-15M dataset is 98M tokens long. Angeli and Manning (2013) used a similar set of extractions : the authors ran Reverb over ClueWeb09 themselves, filtered out extractions scoring under 0.5 per Reverb's confidence function, and retained the first billion extractions, which results in a KB of 500 million unique facts. Their dataset is thus larger, and noisier, than the one we used. Classification Table 1 shows the performance of our approach, along with the methods count and cos of (Angeli and Manning, 2013) . The most elementary language modeling idea demonstrably captures some useful signal for the task. By focusing on the probability of the argument-relation junctions, the language model improves to near state-of-the-art performance. Impact of Negative Examples Sampling Method One important task parameter is the way negative examples are constructed. With (a 1 , r, a 2 ) and (a 1 , r , a 2 ) two genuine extractions, then one of (a 1 , r, a 2 ), (a 1 , r, a 2 ), or (a 1 , r , a 2 ) will be used as a negative example. We examine the impact of choosing one of the former two (changing an argument) versus picking the latter (changing the relation 5 ). In Figure 1 , we vary the fraction of relation changes over argument changes (a \"knob\" of the task setup), and measure the ensuing precision of systems. Angeli and Manning (2013) use a particular scheme : out of the 3 negative example candidates, they pick the one that has the largest cosine similarity with the original fact (a 1 , r, a 2 ). This is with the stated purpose of training the classifier to discriminate between more similar examples, supposedly a better learning setup. In practice, this means changing the relation in about 75% of cases, similarly to setting the knob at 0.75 in Figure 1 6 . In practice, the fact most similar to the original tends to be the one with the swapped relation, because relational phrases are more often similar to each other than argument phrases. This in turn, we suppose, is because phrases are treated as bag of words and represented by their average word embedding, and relation phrases often share common verbs such as be, make, etc. and prepositions, whereas argument phrases are more distinct. Between d(a 1 , a 1 ), d(r, r ) and d(a 2 , a 2 ), d(r, r ) is the largest of the three in only 7% of cases (this choice of negative examples is the column of values at 0.07 on the x-axis). Looking at Figure 1 , we can see that swapping the relation phrase makes the task easier : except for AM-cos, all systems perform better, in a linear fashion. From a language modeling perspective, this is easy to explain : swapping the relation introduces two breaks inside the short sentence, where words may not fit together, instead of just one when an argument is replaced. When building artificial facts in this way, we recommend picking one of the 3 candidate negatives at random (i.e. setting the knob on 0.33). For more difficult learning tasks 7 , further research could leverage other sources of information to produce better distractors as negative examples. This could be argument types, or using the fact that certain relations have only one correct value (e.g. <person>, be born on, <date>), so that differing values are known to be false. Results on Manually Annotated Tuples Our classification task up to now assumed all extracted facts to be correct, and all randomly-generated facts to be wrong. In practice this is not always the case, as some noise remains in the high-quality Reverb dataset, and some randomly assembled facts turn out to have interpretations that make them right, at least plausible. For instance (a knight, (Stanovsky and Dagan, 2016) . We annotated 430 tuples manually as: \u2022 good extractions (45%) : capturing some information, and at least sometimes true. Some examples are (Blackberry picking, is a great introduction to, foraging), (A new computer ; only costs around ; 500$), and (Blood pressure, is influenced by, dietary fibre). \u2022 unsure (30%) : typically only true in their original context which is lost, neither wrong nor useful in a vacuum. For instance (Alfred, was against, garages), (Archaeology, answers this question with, confidence), or (Access ; is limited to ; official business). \u2022 incorrect extractions (25%) : nonsense or false, often due to upstream parser errors or noisy source text. E.g. (5 Mts, Walk From, Wembley Stadium), (Atomic Kitten, released, the) and (A whole day ; set aside for ; literary pursuits). We annotate the tuples regardless of the sentences from which the facts were extracted : we label the facts as they stand and not the extraction process. The unsure labels (covering 25% of examples) were ignored and models had to classify correct and incorrect extractions. The negative examples were oversampled to balance the dataset. The models used are the same as in Section 4.2., except that the SVM cannot be trained, as there is no training data (all facts are positive examples in the automatic task setup). Results are presented in Table 2 . Among genuine extractions (all scored highly by Reverb's confidence function), current models have a hard time recognizing ill-formed or nonsensical extractions, and perform worse than on the automatically generated test set. Some incorrect facts are scored highly because some particular pattern was often repeated over the web and systematically misinterpreted by the OIE system. One example is Cross listed with <another class>., occuring frequently on certain university curriculum pages, from which Reverb extracts (Cross, listed with, e.g. AIST2340). This is correct from a language modeling perspective, even though it's a wrong fact. Human performance In an attempt to gauge the impact of false positives (genuine extractions by ReVerb that turn out to be erroneous) and false negatives (artificially assembled facts that turn out to be plausible), two human annotators, both NLP practitioners and including one author, manually performed the task on 200 facts. Half of them were not lemmatised. As in Table 1 , the negative sampling method was that of (Angeli and Manning, 2013) . Both judges achieved just 80% accuracy at discriminating genuine and artificial facts, on both the lemmatised and unlemmatised versions of the task. Agreement was also 80%. Examples of highly ambiguous facts on which both annotators were mistaken include: \u2022 (Zaire ; will be maimed by ; betrayal) \u2022 (Jean ; is a native of ; New York) \u2022 (Peas ; here take advantage of ; ringtones) \u2022 (Cooking school ; have changed a bit in ; the Los Angeles area) The first and third are genuine extractions (positive examples in the automated task) while the second and fourth were assembled at random (negative examples). Such facts constitute 10% of the test set. Examples of facts on which annotators disagreed (one being mistaken) are: \u2022 (shimer college ; be establish in ; mt) \u2022 (the north node ; will be in ; pisces) \u2022 (Specific attention ; will be given to ; THE MAN) \u2022 (Zoroastrianism ; is in even ; worse shape) \u2022 (Kara ; is vice president of ; buying) Out of those five, only the third is artificial, and others are genuine extractions (the first two being lemmatised, as in the automatic evaluation). Such facts constitute 20% of the test set. Overall, it is as if 60% of the automatically generated testset was reliably recognisable as genuine or artificial, humans performing no better than chance on the remaining 40% (hence the resulting 80% performance). Conclusion We revisit the task of judging the plausibility of a new candidate fact to extend a knowledge base, in the context of OIE -arbitrary relations between unrestricted noun phrases. Correctly assessing the validity of an unknown fact is highly valuable, both as a way to refine KBs built automatically, and to implicitly enhance finite stored knowledge for it to answer an order of magnitude more queries. We propose a new baseline for this task, based on language modeling, which achieves state of the art performance. Indeed, archetypal correctly extracted information resembles short declarative sentences. We show that the way artificial negative examples are sampled has a large and robust impact on the difficulty of the task. We manually find genuine yet incorrect extractions and show that while our system does capture some useful signal, picking up wrong extractions from a high quality dataset remains a challenging task. We examine the sort of facts that our model gets \"right\" despite the test set generation method being wrong, and the sort of facts on which it performs poorly. Future work extending the language model beyond off-the-shelf programs with named-entity recognition would improve its performance. Acknowledgments We would like to thank Fabrizio Gotti for his continuous help with annotation, alongside ever helpful comments. This work was supported by the Nuance Foundation.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 5.51223498068687e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, or foundations. Therefore, based on the provided text, there is no evidence to suggest funding from these sources.",
    "abstract": "Knowledge Base Completion infers missing facts from existing ones in knowledge bases. As recent Open Information Extraction systems allow us to extract ever larger (yet incomplete) open-domain Knowledge Bases from text, we seek to probabilistically extend the limited coverage we get from existing facts, to arbitrary queries about plausible information. We propose a simple baseline, based on language modeling and trained with off-the-shelf programs, which gives competitive results in the previously defined protocol for this task, and provides an independent source of signal to judge arbitrary fact plausibility. We reexamine this protocol, measure the (large) impact of the negative example generation procedure, which we find to run contrary to the belief put forward in previous work. We conduct a small manual evaluation, giving insights into the rudimentary automatic evaluation protocol, and analyse the shortcomings of our model.",
    "countries": [
        "Canada"
    ],
    "languages": [],
    "numcitedby": 2,
    "year": 2018,
    "month": "May",
    "title": "Revisiting the Task of Scoring Open {IE} Relations",
    "values": {
        "building on past work": " In the context of Open Information Extraction (OIE), our aim is to assign a score to an arbitrary unseen query fact 1 , judging its plausibility as a member of the KB. This task is important for three reasons : first, it extends the coverage of the existing KB probabilistically to any query, greatly improving upon the closed-world assumption that facts not known to be true are false. Second, as the extraction of information in the open domain is a relatively noisy process, a confidence score helps detecting extraction errors, and makes for higher-quality automatically generated KBs. Last, adjusting the confidence threshold of extracted facts allows to tune as desired the trade-off between precision and recall of the extraction process.  The task has attracted little attention since it was introduced in (Angeli and Manning, 2013) , most that we know in (Li et al., 2016) .  We propose a new baseline for this task, in the form of a language model. Whereas the method of (Angeli and Manning, 2013) is fairly complicated to implement, and requires indexing the KB in various ways for intermediate computations, a trained language model is very compact, straightforward to implement and train, and fast to process requests at use time. We train the model on automatically extracted facts (including some noise) from the same corpus, i.e. the knowledge base, taken as a list of sentences. We experiment with language model features and show that a linear classifier gives good results at the task of recognising actual extracted facts, perhaps unsurprisingly.",
        "ease of implementation": "a trained language model is very compact, straightforward to implement and train, and fast to process requests at use time.",
        "novelty": " We propose a simple baseline, based on language modeling and trained with off-the-shelf programs, which gives competitive results in the previously defined protocol for this task, and provides an independent source of signal to judge arbitrary fact plausibility.   We propose a new baseline for this task, in the form of a language model.",
        "performance": "Both judges achieved just 80% accuracy at discriminating genuine and artificial facts"
    }
}