{
    "article": "We present a study of a dataset of tables from biomedical research publications. Our aim is to identify characteristics of biomedical tables that pose challenges for the task of extracting information from tables, and to determine which parts of research papers typically contain information that is useful for this task. Our results indicate that biomedical tables are hard to interpret without their source papers due to the brevity of the entries in the tables. In many cases, unstructured text segments, such as table titles, footnotes and non-table prose discussing a table, are required to interpret the table's entries. Introduction Automation of information extraction (IE) from biomedical literature has become an important task (Shatkay and Craven, 2012) . In particular, biomedical IE enables the semi-automation of tasks such as document indexing (Aronson et al., 2004) and database curation, e.g., (Donaldson et al., 2003; Karamanis et al., 2008) . Most research in biomedical IE has concentrated on information extraction from prose. However, much important data, such as experimental results and relations between biomedical entities, often appear only in tables (Ansari et al., 2013) . This insight was confirmed experimentally for the task of mutation database curation. In particular, Wong et al. (2009) showed that for a sample of research articles used to populate the Mismatch Repair database (Woods et al., 2007) , tables served as a sole source of information about mutations for 59% of the documents. Yepes and Verspoor (2013) reported that a text mining tool applied to full articles and their supplementary material, used to catalogue mutations in the COS-MIC (Bamford et al., 2004) and InSiGHT (Plazzer et al., 2013) databases, could recover only 3-8% of the mutations if only prose was considered. An additional 1% of the mutations was extracted from tables in the papers, with an improvement of mutation coverage to about 50% when supplementary material (mostly tables) was considered. Information extraction from tables (Table IE ) comprises various tasks, such as (1) classification of table entries or columns into a set of specific classes (Quercini and Reynaud, 2013; Wong et al., 2009) ; (2) association of table entries or columns with concepts from a domain vocabulary (Assem et al., 2010; Yosef et al., 2011); and (3) extraction of relations, defined in a vocabulary, between entities in tables -usually done with Task 2 (Hignette et al., 2009; Limaye et al., 2010; Mulwad et al., 2013; Venetis et al., 2011) . These tasks are often performed by consulting external knowledge sources. However, despite the intuition that unstructured text accompanying tables often provides helpful information, little use has been made of such text. Examples of such usage are the works of Yosef et al. (2011) , who performed collective named entity normalisation in Web texts and tables; Hignette et al. (2009) , who employed table titles to improve relation extraction from Web tables; and Govindaraju et al. (2013) , who improved performance in extracting a few predefined relations from papers in Economics, Geology and Petrology by processing jointly the text and tables in the papers. This paper describes the first step of a project that aims to automatically perform Tasks 2 and 3 on biomedical tables. In this step, we manually analyse a dataset of tables from the biomedical literature to identify characteristics of biomedical tables that pose challenges for column annotation, and determine the parts of a research paper that typically contain information which is useful for interpreting tables. Our results show that tables in biomedical research papers are generally hard to interpret without their source papers due to the brevity of the entries in the tables. Further, in many cases, un- Analysis Design The dataset used in our analysis comprises a set of biomedical research papers discussing genetic variation. To build the dataset, we randomly sampled five articles from each of the three datasets used in (Wong et al., 2009) and (Yepes and Verspoor, 2013) . The resulting sample contains 39 tables, with a total of 280 columns. We manually analysed the dataset to collect statistics regarding typical data types in the tables (Section 3.1). Columns in the tables were annotated with Semantic Types (STs) from the Unified Medical Language System (UMLS), which has 133 STs in total. To assign a label to a column in a table, the annotator first located a specific UMLS concept corresponding to a fine-grained type of the entities listed in the column (e.g., \"[C0009221] Codon (nucleotide sequence)\" for Columns 3-7 in Figure 1 ), after which the ST corresponding to the selected concept was assigned to the column (e.g., \"Nucleotide Sequence [T086]\"). Individual data entries were not annotated due to insufficient coverage of specific values (e.g., mutations) in UMLS, and the predominantly numerical nature of the data (Section 3.1). On the basis of our annotation, we gathered statistics regarding issues that may influence the performance of an automatic Table IE system, e.g., the consistency of the data types in tables (Section 3.2), and the sources of information that are useful for concept annotation (Section 3.3). It is worth noting that the annotator (first author of the paper) had little background in biomedical science at the time of annotation, and employed external sources such as NCBI databases 1 and Wikipedia to assist with the annotation. This lack of biomedical background may have affected the accuracy of the disambiguation of biomedical entities. However, we posit that the obtained results provide more relevant insights into the use of non-table components in automatic Table IE than those obtained from expert annotation. Results Content of the Data Entries We analysed our dataset to determine which data types are typically contained in biomedical ta-1 http://www.ncbi.nlm.nih.gov/ bles. It was previously noted that, in general, table entries contain very little text, which often does not provide enough context for entity disambiguation (Limaye et al., 2010) . Unlike the interpretation of noun phrases, interpreting numerical data is the biggest challenge for Table IE , because numbers are highly ambiguous (in principle they could be assigned most of the UMLS STs). Another significant challenge in both general and biomedical IE is the use of abbreviations. In light of the above, our analysis shows that biomedical tables are very difficult to interpret: \u2022 42% of the columns in our sample contain numbers, and 3% contain numerical expressions (e.g., 45/290 and 45 \u00b1 6), both representing information such as statistical data, percentages, times, lengths, patient IDs and DNA sequences (e.g., codons 175, 176 and 179 in Column 3 in Figure 1 ). \u2022 32% of the columns comprise abbreviated entries (e.g., MSI, N and A in Figure 1 ) and symbolic representations (e.g., \u00b1 for heterozygote). \u2022 7% of the columns contain free text. \u2022 Only 12% of the columns comprise biomedical terms as entries. \u2022 The remaining 4% of the columns contain a mixture of abbreviations, free text, and numerical expressions. Our study shows that numerical and abbreviated entries can be interpreted correctly if they are appropriately expanded using mentions from table titles, footnotes and prose. For example, in the table in Figure 1 , the abbreviations MSI, N and A can be expanded using the table footnote; and codon mentions in Columns 3-7 can be expanded using the prose describing the table (highlighted). 2 Quality of the Column Headers We analysed our dataset to determine whether it is possible to identify types of biomedical table entries based only on the content of column headers. To do so, we first identified the number of cases where column headers were sufficient for column type identification during the manual table annotation phase (Section 2). We determined that although 97% of the columns in our sample have headers, in many cases they are too ambiguous to be used as the only evidence for the column type. Figure 1 : An example of a biomedical table and prose discussing the table. Source: (Oda et al., 2005) In fact, only 34% of the columns in our sample could be annotated without referring to parts of the documents other than the column entries and their headers. In 57% of the cases, additional information was required to confirm the type of a column (e.g., Columns 3-7 in Figure 1 ), and in 9% of the cases, headers were not helpful in column type identification (Table 1 ). This finding agrees with observations in the Web domain, e.g., (Limaye et al., 2010; Venetis et al., 2011) . We then compared the labels (STs) assigned to table columns to the STs of the entities in the corresponding headers. The comparison showed that in only 53% of the cases a header was labeled with the same ST as the entries in the column. For instance, Columns 3-7 in Figure 1 contain entities of the class \"Codon\" (ST \"Nucleotide Sequence [T086]\"), while the headers, which designate exons, have the ST \"Nucleic Acid, Nucleotide, or Nucleotide [T114]\" or \"Biologically Active Substance [T123]\". We therefore conclude that, in general, headers in isolation are insufficient, and often misleading, for column type identification. Sufficiency and Criticality of Information Sources for Column Annotation We analysed the dataset to determine the contribution of different sources of information in a table and its source article to the identification of the types of biomedical table entries. To this effect, we found it useful to consider the following information sources for each column: (1) the content of the data entries in the column, (2) the header of the column, (3) the headers of other columns, (4) the title of the table, (5) table footnotes, and ( 6 ) prose describing the content of the table (referred to as \"prose\" for simplicity). We distinguish between two aspects of these sources: sufficiency and criticality. \u2022 The sufficiency categories are: (1) Sufficient, if the source on its own was enough to identify the column label; (2) Insufficient, if the source allowed the formulation of a hypothesis about the column label, but required information from other sources to confirm the hypothesis; and (3) Non-indicative, if the source did not contribute to the column labelling. \u2022 The criticality categories are: (1) Critical, if disregarding the source is very likely to lead to an annotation error; (2) Probably Critical, if disregarding the source may lead to an annotation error; and (3) Non-critical, if the source could be disregarded without causing an error. Criticality was assigned to each information source in an incremental manner depending on the sufficiency of the source: if some \"cheap\" sources of information were sufficient for column type identification, more \"expensive\" sources were not considered to be critical. The cost of a source was based on the complexity of the methods required to locate and process this source, increasing in the following order: column header, other headers, table title, table footnotes and prose. To illustrate these ideas, consider Column 3 (concept \"Codon\") in Figure 1 . The other headers, table title and footnotes were classified as Nonindicative, and hence Non-critical, since they do not contain any explicit information regarding the column type (\"codon\" is mentioned in the footnote in a sentence about formatting, which is not considered at present). The header and prose were classified as Insufficient, because each merely suggests the column class, and Critical, because both were required to label the column. When annotating Column 8 (\"Codon change\", ST \"Genetic Function [T045]\"), the title was classified as Probably Critical, because there was no direct correspondence with any UMLS concept -the mapping was performed intuitively, and the title confirmed the chosen hypothesis. The results of our analysis are summarised in Tables 1 and 2 , which respectively show statistics regarding the sufficiency and criticality of various sources of information. The results in Table 1 indicate that none of the information sources were sufficient for each table column in our dataset when taken in isolation. However, it was possible to label every column when all the sources were considered jointly. It is worth noting that the combination of the information sources that enabled labelling all the columns of a single table varied from table to table. As seen in Table 2 , each type of unstructured text associated with tables (i.e., table titles, footnotes and prose) was characterised as critical or probably critical in a substantial number of cases. In addition, we observed that in 59.3% of the cases, a table title or prose segments were characterised as critical or probably critical; and in 70.9% of the cases a table title, footnotes or prose were critical or probably critical. Table footnotes represent an important source of information for abbreviation expansion: 97% of the tables in our sample have footnotes in the form of unstructured text, and about 62% of the footnotes introduce at least some of the abbreviations in the tables. Further, about 72% of the footnotes contain remarks associated with column headers or data entries. No other uses of footnotes were identified. The prose that was required to interpret the tables during annotation was found in referencing paragraphs (i.e., containing descriptors such as \"(  cases the prose was found elsewhere in the sections containing referencing paragraphs; and in 8% of the cases it was found elsewhere in the source document. Our analysis shows that table titles, footnotes and prose tend to be complementary and, in general, none of them can be disregarded during annotation (Tables 1 and 2 ). For example, although all the tables in our sample have titles, on average only 40% of the columns in each table are represented in the titles -column \"representatives\" are either not mentioned in the titles, or their entity types in the titles differ from the types of the columns. We therefore conclude that all unstructured text associated with biomedical tables (i.e., table titles, footnotes and prose) is vital for interpreting them. Conclusion In this paper, we presented an analysis of a dataset of tables from biomedical research papers performed from the perspective of information extraction from tables. Our results show that tables in biomedical research papers are characterised by an abundance of numerical and abbreviated data, for which existing approaches to Table IE We conclude that considering unstructured text related to tables -in particular, combining existing techniques for the interpretation of stand-alone tables with IE from unstructured text -will improve the performance of Table IE . In the near future, we propose to develop techniques for locating table descriptions in the full text of source articles, and incorporating text processing techniques into approaches to Table IE . Acknowledgments We would like to thank the anonymous reviewers for their very detailed and insightful comments. NICTA is funded by the Australian Government through the Department of Communications and by the Australian Research Council through the ICT Centre of Excellence Program.",
    "abstract": "We present a study of a dataset of tables from biomedical research publications. Our aim is to identify characteristics of biomedical tables that pose challenges for the task of extracting information from tables, and to determine which parts of research papers typically contain information that is useful for this task. Our results indicate that biomedical tables are hard to interpret without their source papers due to the brevity of the entries in the tables. In many cases, unstructured text segments, such as table titles, footnotes and non-table prose discussing a table, are required to interpret the table's entries.",
    "countries": [
        "Australia"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "2",
    "year": "2014",
    "month": "November",
    "title": "Challenges in Information Extraction from Tables in Biomedical Research Publications: a Dataset Analysis"
}