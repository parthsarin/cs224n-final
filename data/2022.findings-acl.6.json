{
    "article": "What can pre-trained multilingual sequenceto-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title's question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data 1 . Introduction Pre-trained multilingual sequence-to-sequence (PMSS) models, such as mBART (Tang et al., 2021) and mT5 (Xue et al., 2021) , are pre-trained on large general data, then fine-tuned to deliver impressive results for natural language inference, question answering, and text simplification (Hu et al., 2020) . Their performance on machine translation shows promise for translating low-resource languages (Liu et al., 2021b; Adelani et al., 2021; Thillainathan et al., 2021) , which remains an open challenge (Lopez and Post, 2013; Koehn and Knowles, 2017; Mager et al., 2021; Ranathunga et al., 2021) . When can mBART and mT5 succeed in translating a low-resource language? Despite their promise, the specific conditions for their practical application are not yet clear. Understanding their sensitivities is crucial to guide data acquisition efforts and apply PMSS models to new languages. 1 Code is available at https://github.com/LRLNMT/ LRLNMT We introduce a framework for assessing datadependency of performance of machine translation systems. We then apply it in a large-scale study of mBART's viability for low-resource machine translation on 10 typologically and geographically varied languages. Eight languages are low-resource, and four are unseen by mBART during pre-training. Through our results, we gauge the importance of five dimensions of the training data: 1. Amount of fine-tuning data 2. Noise in fine-tuning data 3. Amount of pre-training data 4. Domain mismatch 5. Language typology The closest work to ours (Liu et al., 2021b) considers only the first two. For the seen languages, mBART reaches acceptable performance with either 10k high-quality, indomain sentence pairs or 100k noisy ones. However, mBART's BLEU score for unseen languages is often below 3.0-far below usability. For these unseen, low-resource languages, the fact that even mBART-which has already seen billions of sentences-cannot succeed in virtually any of our conditions speaks to the need for appropriate indomain data. Therefore, the analytical framework in our experimental design can help to target new data acquisition efforts. 2 Models and Data mBART and mT5 are PMSS models that rely on the encoder-decoder Transformer architecture (Vaswani et al., 2017) trained on Common Crawlderived data with variants of a monolingual autoencoding objective: they must recreate the input text that they are provided. Neither is trained with an explicit objective encouraging similar tokens or sentences to have similar representations. After model weights have been learned, the models can be fine-tuned on parallel text for translation. The ideal fine-tuning scenario would be vast, clean data matching the language and domain of interest. Because this scenario is unlikely for low-resource languages, we test the relaxation of these assumptions for PMSS models. In a preliminary experiment comparing mBART and mT5, mBART performed better than mT5 on 11 of the 18 translation directions, especially the EN\u2192xx directions (Table 1 ), corroborating Liu et al. (2021b) . Because mBART performed better both in number of translation directions and average BLEU, we focus hereafter on it. Languages To assess mBART's translation ability, we selected a set of high-and low-resource languages with high typological and geographical diversity (Table 2 ). Five of the ten languages do not use the Latin script, so that we can evaluate mBART's generalization to non-Latin scripts (see Pires et al., 2019) . Eight are considered low-resource languages by Joshi et al. (2020) , while two high-resource languages (FR and HI) give a skyline of performance. 2 Four are unseen during mBART's pre-training. Together, these languages let us probe the effects of pre-training data size and language typology on translation. Corpora Selecting suitable parallel corpora enables us to probe the remaining three factors: amount of finetuning data, noise in the fine-tuning data, and domain mismatch. For each of our 10 languages, we use three training corpora: data from Common Crawl, the Bible, and one other domain-specific dataset (Table 3; complete details in Appendix A).    Crawl is large and open-domain, while the others are smaller curated translations. We use FLORES (which is also open-domain) and the two domainspecific corpora for testing. Comparing on these lets us assess the impact of domain mismatch. To evaluate consistently across differently sized corpora, we sampled fixed-size training sets from each corpus. For the Common Crawl data, we used two sizes: 25k and 100k sentence pairs. For the Bible, we used a 1k-sentence-pair sample. Finally, for each language's other domain-specific dataset, depending on the amount of parallel text available, we used up to four sizes (1k, 10k, 50k, 100k). The Common Crawl datasets are large opendomain parallel corpora, but their construction by automatic alignment invites substantial noise. This problem is especially severe for low-resource languages (Kreutzer et al., 2022) . Noisy data often harm translation models (Khayrallah and Koehn, 2018) , but it is possible to use them effectively (McCarthy et al., 2020a) . This raises the question of whether mBART can do so. Among our experiments, we can see whether and when a smaller, clean parallel corpus would be preferable. Experimental Setting We fine-tune mBART models on each of the training corpora and sizes listed above, and we evaluate their performance using the development and test sets from the domain-specific corpora and FLORES. We additionally train a standard Transformer baseline (Vaswani et al., 2017) to compare pretraining versus training from scratch. We score translations with SacreBLEU (Post, 2018) . Details of training and evaluation are given in Appendix B. Results and Analysis The results of our empirical study are given in Table 4, with FR given in Table 5 . By contrasting specific groups of rows, we probe our five factors. Amount of fine-tuning data To assess this dimension, we compare the Transformer and mBART models trained on varying sizes of the same corpus with their corresponding open-domain and domain-specific evaluation sets. In the open-domain case (training on Common Crawl), for languages seen during pre-training, mBART fine-tuned with 25k sentence pairs outperforms the Transformer trained with 100k parallel sentences; this pattern holds for 18 of the 20 language directions. This indicates that pre-trained mBART is at least four times as data-efficient. Although it also outperforms the Transformer on unseen languages in terms of BLEU, the scores are often below 3.0-a far cry from even the BLEU score needed for gisting. On the other hand, we observe a similar trend when training with domain-specific datasets (JW300, Gov't, and DGT). For the governmentdomain dataset, mBART trained with 10k sentences of SI or TA achieves a higher BLEU than the Transformer trained with 50k sentences (+3.4 to +6.8); this suggests at least a fivefold data efficiency. The exception is SI\u2192EN, where the difference in scores is 0.1 BLEU. For JW300, mBART trained with 10k parallel sentences outperforms the Transformer trained with 100k for some translation tasks tenfold. Further, mBART trained with 50k sentences outperforms the Transformer model for all languages by a large margin 3 . Of note, YO begins to perform well in-domain on JW300 with tens of thousands of sentences. When do we reach diminishing returns on finetuning size? Figure 1 shows how fine-tuning size affects translation of JW300 into EN from AF, XH, and YO. Although training with more data improves BLEU, the gain saturates as the dataset size reaches approximately 50k sentence pairs. Liu et al. attribute this to the limit of the model's capacity: that the pre-trained weights are \"washed out\" (2020) when fine-tuning with more parallel data. Noise in fine-tuning data At what point is a small-but-clean corpus more useful than an automatically mined one like from Common Crawl? Comparing mBART trained on Common Crawl versus domain-specific data, we see that for several languages both in and not in mBART, 10k high-quality in-domain sentences leads to better performance than 100k sentences from Common Crawl. Amount of pre-training data The improvement of mBART over the Transformer is more prominent for languages with more pretraining data. The correlation between BLEU and number of pre-training sentences is R 2 = 0.31 for open-domain (Figure 2 ), and the effect in the domain-specific case is similar. This shows that mBART effectively leverages the pre-training data. Taken with the results of \u00a74.1, the contrasting behavior between seen and unseen languages belies a \"rich-get-richer\" phenomenon. Domain mismatch This section compares the performance of models when trained and tested on matching versus mismatched domains. Unsurprisingly, taking a training set from the same domain as the test set consistently yields higher BLEU than a mismatched training set. This pattern repeats across domains and directions. Of greater interest is that Common Crawltrained models often do better on domain-specific test sets than open-domain test sets. For languages with JW300 or Gov't, testing BLEU on these was higher than on the open-domain FLORES data. Further, for SI and TA, mBART trained on 10k sentences achieved higher BLEU than the Transformer trained on 100k data, suggesting the pretraining gain was able to compensate the lack of in-domain data. This may indicate that mBART is valuable for domain-specific translation with low amounts of high-quality data. Results for FR on DGT and the Bible and HI on PMI show that mBART can excel with even 1k parallel sentences for languages with sufficient pretraining. If data from a different domain is available in sufficient quantities, an acceptable translation can be expected, as evident from the Gov't 50k and JW300 100k settings. Noticeably, issues related to domain difference and fine-tuning dataset size are less pronounced for FR (see results for 1k Bible data and 1k DGT). This reiterates the impact of language coverage in the mBART model. Language typology This analysis relates properties of the languages to their performance. Foremost, AF regularly achieves the highest BLEU among low-resource languages used to pretrain mBART. This observation is consistent with Zhou and Waibel (2021) . We attribute this to AF's relationship with EN: both are Germanic and share the Latin script, with large lexical overlap. Multilingual machine translation systems can learn shared representations for linguistically similar languages (Dabre et al., 2017; Neubig and Hu, 2018; Kudugunta et al., 2019; Hokamp et al., 2019) ; we expect that mBART taps into this relationship. Further, a smaller token set may help explain this improved generalization (Arivazhagan et al., 2019) . For unseen languages that share the Latin script with English, explaining mBART's performance is less trivial, so we turn to a computational analysis. GA reaches lower BLEU than YO, despite being Indo-European like most of mBART's training data. It could be a result of its rare VSO word order (Liu et al., 2021a) , its initial consonant mutations, or other rare syntactic phenomena. To explain the divergent behavior of AF and GA, we use syntactic features estimated by the k nearest neighbors (Littell et al., 2017) of their WALS features (Dryer and Haspelmath, 2013) . Figure 3 shows the syntactic similarities between AF, GA, and four highresource languages (EN, DE, FR, and NL). This confirms that AF is more syntactically similar to these high-resource languages than GA is. Finally, we consider the interplay of translation direction and BLEU. Translating into EN regularly outperforms translating from EN, which we may attribute to mBART and the Transformer learning a strong EN language model in the decoder (Voita et al., 2021) . But it may also come from BLEU's ignorance of subword phenomena. When translating into a morphologically rich language like SI or TA, no partial credit is awarded for partially correct sets of morphemes. We see this as bolstering the movement toward character-aware metrics (Popovi\u0107, 2015; Mager et al., 2021) . Conclusion We have assessed the value of PMSS models like mBART for low-resource machine translation. We designed a reusable framework of experiments, capturing mBART's sensitivity to five facets of data. Consistently, mBART fails in learning to translate new under-resourced languages-those unseen in the pre-trained model. For languages used in monolingual pre-training, we find four-to tenfold data efficiency over a from-scratch Transformer, plus robustness to domain differences. For domain-specific datasets, mBART might outperform standard Transformers by an efficiency of five to ten times; future work can pinpoint the saturation size. Fine-tuned mBART is robust to domain differences, while the Transformer flounders for out-domain datasets. However, the performance on unseen languages is generally not indicative of usable translation system. Taken in tandem, these results point to the paramountcy of monolingual pre-training for the bilingual task of translation. The biggest open issue, though, is not how to tune PMSS models on limited data; instead, greater data acquisition is the hope for truly low-resource machine translation. A Supplementary Material on Corpora Here we give details of the corpora used in our study. Bible. The JHU Bible Corpus (McCarthy et al., 2020b ) is a recently released corpus of Bible translations in over 1600 languages. In several lowresource languages, the Bible is the only available text parallel with another language; moreover, its verse structure makes it multi-parallel across thousands of languages. It has been used to assess multilingual translation at massive linguistic scale (Mueller et al., 2020) , develop new morphological tools (Nicolai et al., 2020) , and fine-tune pretrained language models to new low-resource languages (Ebrahimi and Kann, 2021) . Gov't. The government document corpus of Fernando et al. ( 2020 ) is a multilingual corpus for Sinhala, Tamil, and English. It contains official Sri Lankan government documents: annual reports, crawled content from government institutional websites, committee reports, procurement documents, and acts. PMI. PMIndia (Haddow and Kirefu, 2020 ) is a parallel corpus of news updates for English and 13 other languages in India, extracted from the Prime Minister of India's website. JW300. The JW300 corpus (Agi\u0107 and Vuli\u0107, 2019) is another parallel corpus, spanning 343 languages. It is obtained from jw.org and includes Jehovah's Witness magazines like Awake and Watchtower. The domain is highly religious, but it includes other societal topics such as reports about persecution of their disciples around the world. While JW300 was automatically aligned, Abbott and Martinus (2019) and Alabi et al. (2020) have verified its quality for African languages. For languages with non-Latin scripts in our study, the alignment has been judged to be poor by native speakers. DGT. The European Commission's Directorate-General for Translation-Translation Memory (Tiedemann, 2012) covers 25 languages and corresponds to the 'Summaries of EU legislation'. They are short explanations of the main acts passed by the European Union. The legislation included in the dataset includes directives, regulations, decisions, and international agreements. Common Crawl. CCAligned (El-Kishky et al., 2020) and CCMatrix (Schwenk et al., 2021) are web-scraped corpora that were automatically aligned using LASER sentence embeddings (Schwenk, 2018) . CCAligned is newer, and it has more text in low-resource languages. The dataset, albeit noisy (Kreutzer et al., 2022) , has been used to develop highly multilingual machine translation models like M2M100 (Fan et al., 2021) and mBART multilingual MT (Tang et al., 2021) ; a modified version is used to train mT5 (Xue et al., 2021) . Data splits For FLORES and the Bible, we always use 1000 sentence pairs for development (see Kann et al., 2019) and 1000 sentence pairs for test. For the second in-domain dataset, the size varies between 1000 and 2000 sentence pairs based on availability. B Supplementary Material on Experimental Setup mBART and mT5. We compared mBART50 and mT5-base because they have comparable numbers of parameters. For both the mBART50 and mT5-base models (Tang et al., 2021) , we train up to 3 epochs with a learning rate of 5\u00d710 \u22125 , dropout of 0.1, maximum lengths of 200 for the source and target, and a batch size of 10. We decode using beam searh with a beam size of 5. We use the implementations in the HuggingFace Transformers library, and we leverage hardware-level parallelism by training on NVIDIA Tesla V100 GPUs. We perform bilingual fine-tuning on the 10 selected language pairs. For each language direction, we initialize the encoder-decoder model's parameters from the pre-trained mBART model's corresponding encoder and decoder. After initialization, we continue training. Because mBART requires a target language to be specified during decoding from amongst those that the model has seen, we follow past work in selecting languages related to our target languages for unseen languages (Madaan et al., 2020; Cahyawijaya et al., 2021) . Considering syntactic and phylogenic closeness of languages (Dryer and Haspelmath, 2013; Littell et al., 2017) , we chose BN for AS, TE for KN, FR for GA, and SW for YO. mT5. Considering memory bottlenecks, we use the mT5-base model. It supports over 100 languages, including five of the six from our prelimi-nary experiment. Because Irish (GA) is not among these, we use the French language code for finetuning the model. Transformer. We train Transformer models implemented in FAIRSEQ using the same datasets as we used for fine-tuning mBART. We use two Transformer architectures, depending on the data size. When there are fewer than 10k parallel sentences, the model consists of 3 encoder layers and 3 decoder layers, with embedding dimension of 512 and 2 attention heads. When there are 10k or more parallel sentences, we instead use a model that consists of 6 encoder layers and 6 decoder layers, with an embedding dimension of 256 and 2 attention heads. In each case, we have an initial learning rate of 1 \u00d7 10 \u22123 , a weight decay of 1 \u00d7 10 \u22124 , dropout of 0.4, and batch size of 32. We use early stopping based on the validation loss. We train the models from scratch with segmentation into subword tokens performed by SentencePiece. When decoding, we use beam search with a beam size of 5. Evaluation. To ease the comparison of future work with ours, we report that the SacreBLEU settings we use are represented by the signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.5.0. Acknowledgments This project has been supported by the ICLR Co-Submitting Summer (CSS) program 2022 initiated by ICLR DEI co-chairs Rosanne Liu and Krystal Maughan. David Adelani acknowledges the support of the EU funded Horizon 2020 project ROXANNE under grant agreement No. 833635. Lastly, we thank the Spoken Language Systems Chair, Dietrich Klakow at Saarland University for providing GPU resources to train the models.",
    "abstract": "What can pre-trained multilingual sequenceto-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title's question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data 1 .",
    "countries": [
        "United States"
    ],
    "languages": [
        "Sinhala",
        "Tamil",
        "English"
    ],
    "numcitedby": "2",
    "year": "2022",
    "month": "May",
    "title": "Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?"
}