{
    "article": "The paper presents the HW-TSC's pipeline and results of Offline Speech to Speech Translation for IWSLT 2022. We design a cascade system consisted of an ASR model, machine translation model and TTS model to convert the speech from one language into another language(en-de). For the ASR part, we find that better performance can be obtained by ensembling multiple heterogeneous ASR models and performing reranking on beam candidates. And we find that the combination of context-aware reranking strategy and MT model fine-tuned on the in-domain dataset is helpful to improve the performance. Because it can mitigate the problem that the inconsistency in transcripts caused by the lack of context. Finally, we use VITS model provided officially to reproduce audio files from the translation hypothesis. Introduction In this year, there is only one track in the speech to speech translation task which is the English to German translation (En-De) (Anastasopoulos et al., 2022) . The audio files in English are given in the dataset, and we are required to produce the audio files in German. In recent research of speech to speech task, there are basically two paradigms with respect to the system architecture, which are cascade and end-to-end. And the cascade pipeline composed by an ASR model, a MT model and a TTS model is commonly used, because this system is more mature than end-to-end one. The advantage of this pipeline is that each module of the system can be a state-of-the-art one trained on sufficient independent corpora. It also allows us to perform experiments with different combinations of ASR models, MT models and TTS models. But compared to end-to-end system, this cascade system may not capture all information like accent of speakers, emotion, etc. End-to-End system like S2UT is introduced in (Lee et al., 2021) , which can be directly trained on speech to speech dataset with the help of text generation as the auxiliary task. However, we didn't adopt this approach due to the insufficiency of available corpora. For the ASR model, we tried Conformer (Gulati et al., 2020) , S2T Transformer (Synnaeve et al., 2019) and U2 (Zhang et al., 2020) , and obtained three types of ASR results. In translation, inconsistency of translation of same words in the context is a common difficulty. This is caused by the flaw of conventional translation that treats each sentence independently in a documents, ignoring surrounding contexts. For example, a family name in English can be translated in different ways in Chinese. Because Chinese transcripts comes from transliteration, and there are lots of words share same pronunciation but different spelling. This may cause the ambiguity in transcripts, which is hard for readers to understand. To solve the problem, we propose the context-aware reranking strategy in translation, essentially an approach to adapt sentence-level MT models into document-level translation scenarios. It aims to generate the best candidate by taking previous contexts into account and reranking with scores estimated by all models. Method Data Preprocessing We consider five datasets as our training set of ASR models, which are MuST-C V2 (Cattoni et al., Language WMT Bilingual In-domain Text En-De 79M 459K En-Zh 96M 590K En-Ja 42M 552K Table 2 : Data statistics of our MT corpora 2021), LibriSpeech (Panayotov et al., 2015) , TED-LIUM 3 (Hernandez et al., 2018) , CoVoST (Wang et al., 2020) and IWSLT. The statistical description is shown in Table 1 . The CoVoST dataset has the longest duration and the largest number of utterances. In the first step, we load the waveform of audio files as tensors and extract the 80-dimensional filter bank features of them. Because the encoder and decoder of a Transformer (Vaswani et al., 2017) model can only process limited size of sequences, we restrict the frame size of input speeches to the range of 50 to 3000, and the number of tokens should be no more than 150. At the same time, we calculate the speed of the speech by length of references and frame size of each sample. This metric could help us find those speech with small frame size but large number of tokens, or vice versa, which should be considered as outliers. So we choose the speech with the speed within \u00b5(\u03c4 )\u00b14\u00d7 \u03c3(\u03c4 ), where \u03c4 = # frames # tokens . Through these process pipeline in fine-grained level, we obtain the cleaned training set. For the test set, we use the official dataset provided audios in the task. We also use the MuST-C dev, tst-COMMON and tst-HE set to evaluate our model so that they can be compared easily with other approaches. For the training set of MT models, we follow the configuration and preprocessing procedures as (Wei et al., 2021) , and the scale of the dataset is shown in Table 2 . Automatic Speech Recognition We apply Conformer (Gulati et al., 2020) and S2T-Transformer (Synnaeve et al., 2019) to predict the fundamental results in an ensemble approach, and clean the predicted candidates with the U2 model (Zhang et al., 2020) . All of these models are trained on the united dataset with the domain controlled training/generation (Wang et al., 2021) . We ensemble the ASR result of the two models, and some results have been corrected in i \u0338 = |S| \u2212 1 do \u0176 , P f \u2190 F(u i , k): propose candidates P g \u2190 G(u i , \u0176 ): scoring with M T \u2032 if i < N then P q \u2190 Q( \u0176 , C) else P q \u2190 Q( \u0176 , C [\u2212N :] ) end if \u0177 * \u2190 arg max \u0177 m \u2208 {f, g, q}w m log P m C \u2190 C \u222a {\u0177 * } i \u2190 i + 1 end while return C the post-processing. Sometimes both Conformer and S2T-Transformer makes errors in the recognising process, except the errors appeared in different position. For example, in a same sentence, the Conformer would recognise the \"ex-boyfriend\" as \"next boyfriend\" incorrectly, and the S2T-Transformer may misidentify \"the cuss words\" as \"the cusp words\". Through ensembling, these errors can be eliminated and results can be improved. We proved that the ensembling of these heterogeneous ASR models can in some what extent improve the possibility of choosing the correct answer. Meanwhile, we find that two autoregressive models both have the drawback of producing meaningless sentences when acoustic inputs are applause or laughing from the audience. In this situation, U2 presents the stability and robustness in predicting those audio without real utterances. So, we use U2 as the criteria to filter the ensemble results comes from Conformer and S2T-Transformer. It means, for each sample, we predict with U2 first and see if the prediction is a blank line, if it is, we directly use it as the output, otherwise, we predict the sample again with the ensembled model mentioned above. This is the key to apply U2, but it would not change any other prediction of ensemble results. After the cleaning process of U2, results are more anti-interference to the sample that filled with laughter or meaningless natural noise. Translation Models We use the WMT21 news corpora to train the MT model in En-De direction, then, use the combination of MuST-C and IWSLT dataset to fine-tune the pretrained model. Context-aware MT reranking Following the work in (Yu et al., 2020) that utilises the noisy channel model (Brown et al., 1993) in document-level translation, we adopted similar strategy to improve the translation with longer context information. However, we make some simplification on the decoding process and the scoring function. More specifically, we restrict the context to a sliding window that only taking a fixed size of sentences into account when applying the LM scoring: O(x, y \u2212N : , y i ) =w MT log p MT (y i |x i ) + w LM log p LM (y i |y \u2212N : ) + w MT' log p MT \u2032 (x i |y i ) (1) where N is the context length, w are weights for each component. The decoding process is also simplified into a greedy search instead of sentencelevel beam search as described in Algo 1. During inference, we find that the test set is exactly same as the tst2022-en-de used in the offline, therefore, we manually regroup ASR outputs back to documents and translate them with this approach. Text to Speech In a cascade speech to speech translation system, text to speech (TTS) is the final module to convert translations into speech. We use the pretrained VITS (Kim et al., 2021) 3 Experiments Setup In the training of our ASR models, we use the sentencepiece model (Kudo and Richardson, 2018) for tokenization with vocab size=20000. Configurations of ASR models are exactly same to our offline submission. We follow the recipe of (Wei et al., 2021) to train our NMT models in both directions, as well as the language model. All MT models are also fine-tuned on in-domain corpora for additional 10 epochs. We implemented all models with fairseq (Ott et al., 2019) . The automatic evaluation of our S2S system is achieved by calculating metrics on the retranscribed outputs from our system. Specifically, an officially assigned ASR model: \"wav2vec2large-xlsr-53-german\" (Baevski et al., 2020) is used to transcribe the TTS generated audio files back to texts first. Then, they are used for the evaluation with automatic tools performed in text-level. This significantly reduces the difficulty of evaluation but still preserves the fairness. We use BLEU (Papineni et al., 2002) , ChrF (Popovic, 2015) and TER (Snover et al., 2006) as evaluation metrics in our experiments. Results Because the speech cannot be directly compared to transcripts, we have to convert the speech into transcripts by the Wav2vec ASR model. We tested the score of BLEU, ChrF and TER by evaluating the translation outputs of MT model and the retranscribed results of final outputs from TTS. And those scores can be seen in Table 3 . Note that before computing evaluation metrics, we applied some normalizing process to make the results of Oracle and TTS comparable. More specifically, since the re-transcribed text from the wav2vec model is lower-cased and has no punctuation, we also perform lower-casing and removing of punctuation for Oracle hypothesis and the references. Finally, we evaluate metrics on Oracle and TTS hypothesis towards the normalized references. From the experimental results on three sub sets of MuST-C, we have some interesting findings. Through the process of TTS and re-ASR, the BLEU score and ChrF score has both decreased by about 7+ and 0.05+, and the TER score increased by 0.07+. This trend appears in both three test sets, demonstrating that there might be serious information loss in this process. However, further conclusions can only be drawn from the human evaluation. Ablation Effectiveness of domain controlled generation We test whether the domain tag prefix is useful for the performance of model, and the results are shown in Table 4 . There are four domain tag used in our new dataset, including \"<MC>\", \"<LS>\", \"<TL>\" and \"<CV>\". All these prefix represents the abbreviations of each dataset. Compared with the results of model fed by dataset without using any domain prefix tags, the model trained on the tagged dataset has the better performance. This essentially benefits from the extra prior information provided by the domain prefix tags. In detail, domain tags provides more latent information that cannot be easily captured in raw audios, making the generation more deterministic. Meanwhile, this allows us to control the generation style in our demanded domain, being closer to the reference. So, the domain tag prefix effectively improves the performance of our model. Conclusion In the paper, we elaborate the cascade system for this Speech to Speech task. There are several strategies we applied to improve the system, including domain-tag prefix and the context-aware reranking strategy. We did some experiments to verify the reliability of those strategies for a cascade system, and we also made some analysis from the theoretical level. In the future, we are going to explore the feasibility of the end-to-end system, since it might reduce the negative impact of information loss on system performance.",
    "abstract": "The paper presents the HW-TSC's pipeline and results of Offline Speech to Speech Translation for IWSLT 2022. We design a cascade system consisted of an ASR model, machine translation model and TTS model to convert the speech from one language into another language(en-de). For the ASR part, we find that better performance can be obtained by ensembling multiple heterogeneous ASR models and performing reranking on beam candidates. And we find that the combination of context-aware reranking strategy and MT model fine-tuned on the in-domain dataset is helpful to improve the performance. Because it can mitigate the problem that the inconsistency in transcripts caused by the lack of context. Finally, we use VITS model provided officially to reproduce audio files from the translation hypothesis.",
    "countries": [
        "China",
        "Australia"
    ],
    "languages": [
        "English",
        "German",
        "Chinese"
    ],
    "numcitedby": "1",
    "year": "2022",
    "month": "May",
    "title": "The {HW}-{TSC}{'}s Speech to Speech Translation System for {IWSLT} 2022 Evaluation"
}