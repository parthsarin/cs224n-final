{
    "article": "Computational linguistics has a long tradition of lexicalized grammars, in which each grammatical rule is spe cialized for some individual word. The earliest lexicalized rules were word-specific subca. tegoriza. tion frames. It is now common to find fully lexicalized versions of many grammatical formalisms, such as context-free and tree-adjoining grammars [Schabes et al. 1988] . Other formalisms, such as dependency grammar [Mel'cuk 1988] \u2022 and head-driven phrase-structure grammar [Pollard & Sag 1994] , are explicitly lexical from the start. Lexicalized grammars have two well-known advantages. Where syntactic acceptability is sensitive to the quirks of individual words, lexicalized rules are necessary for linguistic description. Lexicalized rules a. re also computationally cheap for parsing written text: a parser may ignore those rules that do not mention any input words. More recently, a third advantage of lexicalized grammars has emerged. Even when syntactic acceptabilit: I) is not sensitive to the particular words chosen, syntactic distribution may be [Resnik 1993]  . Certain words may be able but highly unlikely to modify certain other words. Such facts can be captured by a. probabilistic lexicalized grammar, where they may be used to resolve ambiguity in favor of the most probable analysis, and also to speed parsing by avoiding (\"pruning\" ) unlikely search paths. Accuracy and efficiency can therefore both benefit. Recent work along these lines includes [Charniak 1995, Collins 1996, Eisner 1996b, Collins 1997], who reported state-of-the-art parsing accuracy. Related models are proposed without evaluation in [Lafferty et al. 1992 , Alsha. wi 1996]  . This recent flurry of probabilistic lexicalized parsers has focused on what one might call bilexical granuuars, in which ea. eh grammatical rule is specialized for not one but two individual words. 1 The central insight is that specific words subcategorize to some degree for other specific words: ta: r is a good object for the verb ra ise. Accordingly, these models estimate, for example, the probability that ( a. phrase headed by) word y modifies word x, for any two words x, y in the vocabulary V . At first blush, probabilistic bilexica. l grammars appear to carry a. substantial computational penalty. Chart parsers derived directly from CKY or Earley's algorithm take time O(n 3 min(n, 11 1 1 ) 2 ), which amounts to O(n 5 ) in practice. Such algorithms implicitly or explicitly regard the grammar as a context-free grammar in which a noun phrase headed by tiger bears the special nonterminal NP ti g er \u2022 Such :::::::: O( n 5 ) algorithms are explicitly used by [Alshawi 1996, Collins 1996]  , and almost certainly by [Charniak 1995]  as well. The present paper formalizes an inclusive notion of bilexica. l grammars, and shows that they can be parsed in time only O(n 3 g 3 t' 2 m, ):::::::: O(n 3 ), where g, t, and m are bounded by the grammar and are typically small. (g is the maximum number of senses per input word, t measures the degree of lexical interdependence that the grammar allmvs among the several children of a. word, and rn bounds the number of modifier relations that the parser need distinguish for a given pair of words.) The new algorithm also reduces space requirements to 0(11 2 g 2 The new \ufffd O(n 3 )-time algorithm has been implemented, and was used in the experimental work of [Eisner 1996a , Eisner 1996b] , which compared various bilexica. l probability models. The algorithm also applies to the head-automaton models of [Alsha. wi 1996] and the phrase-structure models of [Collins 1996 , Collins 1997 ], allowing \ufffd O(n 3 )-time rather than \ufffd O(n 5 )-time parsing, granted the (}inguistica.lly sensible) restrictions that the number of distinct X-ba. r levels is bounded and that left and right adjuncts a.re independent of ea. eh other. Formal Definit ion of Bilexical Grammars Unweighted Bilexical Grammars A bilexica. l grammar consists of the fo llowing elements: \u2022 A set V of v.rords, called the vocabulary, which contains a distinguished symbol ROOT. The elements of V may be used to represent word senses: so V can contain separate elements bank 1 , bank 2 , and bank 3 to represent the various meanings of bank. For parsing to be efficient, the maximum number of senses per word, g, should be small. \u2022 A set A1 of one or more n1odifier roles. 1'1 does not affect the set of sentences generated by the grammar, but it affects the structures assigned to them. In these structures, elements of lvf will be used to annotate syntactic or semantic relations among words. For example, John might not merely modify loves, but modify it as SUBJECT or OBJECT, or as AGENT or PATIENT; these a. re roles in kl. For parsing to be efficient , 1' , J , should be small. (While a semantic theory may posit a great many modifier roles, it need not be the parser's job to make the finer distinctions.) More precisely, what should be small is m, the maximum number of modifier roles available for connecting two given words, such as John and loves. \u2022 For each word w, a. pair of deterministic finite-state automata l\\ v and 1\u2022w . Each automaton accepts some set of strings over the alphabet V x M . .C w specifies the possible sequences of left dependents ( arguments and adjuncts) for w. 1'w specifies the possible sequences of right dependent.s for ' W . By convention, the first element in such a. sequence is closest to 'W in the surface string. Thus, the possible dependent sequences are specified by ,C( C w ) R and ,C( 1'w ) respectively. Note that the collection of automata in a grammar may be implemented as a pair of functions C and r, such that C(w, s, iv', \u00b5 ) returns the destination state of the (w', \u00b5, )-la.beled arc that leaves state s of automaton C w , and similarly for r (w, s, w', \u00b5. ) . These fu nctions may be computed in any convenient way. For parsing to be efficient, the maximum number of states per automaton, t, should be small. However, without penalty there may be arbitrarily many distinct automata. (one per word in V), and each automaton state may have arbitrarily many arcs (one per possible dependent in V), so IVI does not affect performance. We must now define the language generated by the grammar, and the structures that the grammar assigns to sentences of this language. Let a dependency tree be a rooted tree whose nodes (internal and external ) are labeled with words from V, and whose edges are labeled with modifier roles from 1'1 . The children ( ''dependents\" ) of a node are ordered with respect to each other and the node itself, so that the node has both left children that precede it and ri g ht children that follow it. Figure la. illustrates a. dependency tree all of whose edges a.re labeled with the empty string. Given a bilexical grammar, a leaf a: of a dependency tree may be expanded to modify the tree as follows. Let w be the word that labels x, a be any string of left dependents accepted by . C w , and /3 be any string of right dependents accepted by 1\\ v . These strings are in ( V x M)*. Add lal left children and l/31 right children to the leaf x. Label the left children and their attached edges with the symbol pairs in a (from right to left ); similarly label the right children and their attached edges with the symbol pairs in /3 (from left to right). A dependency parse tree is a dependency tree generated from the grammar by starting with a single node, labeled with the special symbol ROOT E V, and repeatedly expanding leaves until every node has been expanded exactly once. Figure la. may be so obtained: one typical expansion step gives the leaf plan the child sequences Cl' = the, J3 = of, ra ise. Another such step expands the, choosing to give it no children at all (a = f3 = e). A string w E \\/* is generated by the grammar, with analysis T, if T is a dependency parse tree and listing the node labels of Ti n infix order yields the string w followed by ROOT. w is called the frin g e of T . The term bilexical refers to the fa ct that (i) each w E 1, \u2022 may specify a ,:vholly different choice of automata l\\ v and r w , and furthermore (ii) each such automaton may make distinctions among individual words that are appropriate to serve as children of w. Thus the grammar is sensitive to specific pairs of lexical items. For example, it is possible for one lexical verb to select for a completely idiosyncratic set of nouns as subject, and another lexical verb to select for an entirely different set of nouns. Since it never requires more than a two-state automaton ( though with many arcs! ) to specify the set of possible subjects for a verb, there is no penalty for such behavior in the parsing algorithm to be described here. We ight ed Bilexical Grammars The ability of a verb to subcategorize for an idiosyncratic set of nouns, as above, can be used to implement black-and-white ( \"hard\" ) selectional restrictions. Where bilexical grammars are really useful, however, is in capturing gradient (''soft'' ) selectional restrictions. A weighted bilexical grammar can equip each verb with an idiosyncratic probability distrib1tiion over possible obj ect nouns, or indeed possible dependents of any sort. vVe now formalize this notion. A wei g hted finite-state automaton A is a finite-state automaton that associates a real-valued wei g ht with each arc and each final state. Following heavily-weighted arcs is intuitively \"good,\" \"probable,'' or \"cqmmon\" ; so is stopping in a heavily-weighted final state. Each accepting path through A is automatically assigned a ,veight, namely, the sum of all a. re weights on the path and the final-state weight of the last state on the path. Ea.eh string accepted by A is assigned the weight of its accepting path. Now, we may define a weighted bilexical grammar as a bilexica. l grammar in which all the automata. f w and 1,w are ,:veighted automata.. The grammar assigns a weight to ea. eh c). ependency parse tree: namely, the sum of the weights of all strings of dependents, a and ,8, generated while expanding nodes to derive the tree. (This weight is well-defined: it does not depend on the order in which leaves were expanded. ) The goal of the parser is to determine whether a string w E l'* can be generated by the grammar, and if so, to determine the highest-weighted analysis for that sentence. It is convenient to set up the weighted automata. so that ea.eh automaton formally accepts all strings in l'* , but assigns a weight of -(x:, to any that a.re not permitted by the competence grammar. Then a .sentence is rejected as ungrammatical if its highest-weight analysis has ,:veight only -oo. The unweighted case is therefore a special case of the weighted case. Lexical Selection The above fo rmalism must be extended to deal with lexical selection. Regrettably, the input to a parser is typically not a string in l T *. Rather, it contains ambiguous tokens such as bank, whereas the \"words\" in V are word senses such as bank 1 , bank 3 , and bank 3 , or part-of-speech-tagged words such as bank /N and bank /V. One would like a parser to resolve these ambiguities as well as structural ambiguities. ,, Ve may modify the formalism as follows. Consider the unweighted case first . Let O be the real input-a string not in \\/* but rather in P( V)*, where P denotes powerset. Thus the ith symbol of O is a confusion set of possibilities for the ith word of the input, e.g., {bank 1 , bank 2 , bank 3 }. 0 is generated by the grammar, with analysis T, if some string w E \\/* is so generated, where w is formed by replacing ea.eh set in O with one of its elements. Note that w is the fringe of T . For the weighted case, each confusion set in the input string f2 assigns a weight to ea. eh of its members. Again, intuitively, the heavily-weighted members a. re the ones that are commonly correct, so the noun bank / N would be weighted more highly than the verb bank/V. ,, 1 e score dependency parse trees as before, except that now we also add to a tree's score the ,veights of all its fringe words, as selected from their respective confusion sets. Formally, we say that f2 = VV1 H 1 2 ... H 7 n E P(V)* is generated by the grammar, with analysis T and weight p + q1 + \u2022 \u2022 \u2022 + q n , if some string w = w1w2 ... w 11 E V* is generated with analysis T and weight p, and lwl = lfll = n and for each 1 \ufffd i:::; n, Wi appears in the set Hli with weight qi . Adding String-Local Constraints An extension is to allow the grammar to specify a list of excluded bi g rams. If a tree's fringe contains an excluded bigram ( two-word sequence), the tree is not considered a valid dependency parse tree, and is given score -oc,. \u00a73.8 shmvs how this extension lets the scoring model consider such fa ctors as the probability of the tag k-grams that appear in the parse ( even for k > 2), as proposed by [Lafferty et al. 1992] . Consideration of such factors has been shown useful for probabilistic systems that simultaneously optimize tagging and parsing [Eisner 199Gb ] . Uses of Bilexical Grammars Bilexical grammars, and the new parsing algorithm for them, are not limited to dependency-style structures. They a. re flexible enough to capture a variety of grammar formalisms and probability models. This section will illustrate the point with some key cases. vVe begin with the dependency case, and progress to phrase-structure grammars. A Simple Case: Monolexical Dependency Grammar It is straightforward to encode dependency grammars such as those of [Gaifman 1965 ] . w\u2022 e focus here on the case that [Milward 1994 ] calls Lexicalized Dependency Grammar (LDG ). Milward demonstrates a parser for this case that requires O(n 3 g 3 t 3 ) \ufffd O(n 3 ) time and O(n 2 g 2 ( 1 ) \ufffd O(n 2 ) space, using a left-to-right algorithm that maintains its state as au acyclic directed graph. Here t is taken to be the maximum number of dependents on a. ,vord. m does not appear because Milward takes tree edges to be unlabeled, i.e., m = 1. LDG is defined to be only monolexical. Ea. eh word sense entry in the lexicon is for a word tagged with the type of phrase it proj ects. An entry for helped /S, which appears as head of the sentence Nurses helped John wash, may specify that it wants a. left dependent sequence of the form wif N and a right dependent sequence of the form w2/N, w3/V. However, under LDG it cannot constrain the lexical content of \u2022 w 1 , w2 , or W3 , either discretely or probabilistically. 2  By encoding a monolexical LDG as a. bilexical grammar, and applying the algorithm described below in \u00a74, we can improve parsing time and space by a fa ctor of t. The encoding is straightforward. To capture the preferences for helped/S as above, we define th d p ed/ S to be a two-state automaton that accepts exactly the set of nouns, and 1'h tlp ed/ S to be a three-state automaton that accepts exactly those word sequences of the form (noun, verb) . Obviously, l\\.eip td/ S includes a great many arcs-one arc for every noun in V. This does not however affect parsing performance, which depends only on the number of states in the automaton. Optional and Iterated Dependents The use of automata makes the bilexical grammar considerably more flexible than its LDG equivalent. In the example of \u00a73.1, 1'h d p cd/ S can be trivially modified so that the dependent verb is optional (Nurses helped John). LDG can accomplish this only by adding a new lexical sense of helped/S, increasing g. Similarly, under a bilexical grammar, t 1rnrsu /N can be specified to accept dependent sequences of the form (adj , a. dj , adj , ... adj , ( <let) ). Then nurses may be expanded into weary Belgian nurses. Unbounded iteration of this sort is not possible in LDG, where each word sense has a fixed number of dependent. s. In LDG, as in categorial grammars, weary Belgian nurses would have to be headed by the adjunct weary. Thus, even if LDG were sensitive to bilexicalized dependencies, it would not recognize nurses-;.helped as such a dependency. Bilexical Dependency Grammar In the example of \u00a73.1, we may arbitrarily weight the individual noun arcs of the \u00a3h el p ed automaton, according to how appropriate those nouns are as subjects of helped. (In the unweighted case, we might choose to rule out inanimate subjects altogether, by removing their arcs or assigning them the weight -oc,.) This turns the grammar from monolexical to bilexical, without affecting the cubic-time cost of the parsing algorithm of \u00a74. Probabilistic Bilexical Dependency Grammars [Eisner 199Gb] compares several probability models for dependency gram mar. Each model simultaneously eval uates the part-of-speech tags and the dependencies in a given dependency parse tree. Given an untagged input sentence, the goal is to find the dependency parse tree ,vith highest probability under the model. Each of these models can be accomodated to the bilexical parsing framework, allowing a cubic-time solution. In each case, V is a set of part-of-speech-tagged words. For simplicity, 1',f is taken to be a singleton set { c:}. Each weighted automaton Cw or 'l'w is defined so that it accepts any dependent sequence in l/* , but the automaton has 8 states, which enable interactions among successive dependents in a sequence. Any arc that accepts a noun (say) terminates in the Noun state. The 'W arc from Noun may be weighted differently than the w arcs from other states; so a given dependent word w may be more or less likely depending on whether the previous dependent in the sequence ,vas a noun. The final-state weight of Noun may also be selected freely: so the sequence of dependents might be likely or unlikely to end with a noun. 3  As sketched in [Eisner 1996a], each of Eisner's probability models is implemented as a particular scheme for weighting such an automaton. For example, model C regards tw and 1'w as Markov processes, where each state specifies a probability distribution over its exit options, namely, its outgoing arcs and the option of halting. The ,veight of an arc or a final state is then the log of its probability. Thus if 1'h dp t d/F includes an arc labeled with (bathe/\\!, c:) and this arc is leaving the Noun state, then the arc weight is (an estimate of ) log Pr(next right dependent is bathe/\\! I parent is helped/\\! and previous right dependent was a noun) The weight of a dependency parse tree under this probability model is a product of such fa ctors, which means that it estimates Pr( dependency links & input words ) according to a generative model. By contrast, model D estimates Pr( dependency links I input words), using arc ,, r eights that are roughly of the form log Pr( bathe/\\! is a right dep. of helped/\\! I both words appear in sentence and prev. right dep . was a noun) which is similar to the probability model of [Collins 1996 ]. Some of the models evaluated also rely on weighted string-local constraints, as implemented in \u00a7:3.8 below. Probabilistic Bilexical Phrase-Structure Grammar In some situations, one wishes a parser to evaluate phrase-structure trees rather than dependency parse trees. [Collins 1997 ] observes that since VP and S are both verb-headed, the dependency grammars of \u00a73.4 would falsely expect them to appear in the same environments. (The expectation is fa lse because conthwe subcategorizes for VP only. ) Phrase-structure trees address the problem by providing nonterminal labels. In addition, phrase structure trees are less flat than dependency trees: a ,,,ord 's dependents attach to it at different levels, providing an obliqueness order on the dependents of a word. Obliqueness is of semantic intere(' ,t, and is also exploited by [vVu 1995] , whose statistical translation model preserves the topology (ID, not LP) of binary-branching parses. Fortunately, it is possible to encode (headed ) phrase structure trees as dependency trees with labeled edges. One can therefore use the fast bilexical parsing algorithm of \u00a74 to generate the highest-weighted dependency tree, and then convert that tree to a phrase-structure tree. For the encoding, ,:ve expand V so that all words are tagged not with single nonterminals but with nonter m.inal chains. Let us encode the phrase-structure tree [John {contiwuedv [t o batheF himself}vp }vp }s . The word bathe is the head of V and VP constituents, while continued is the head of V, VP, and S constituents (reading from the leaves upward). In the dependency tree, we therefore tag them as bathe/V, VP and co11ti11-'lt ed/V, llP, S. The automaton 1'continut d can now require a dependent 's tag to end with VP; this captures the ungrammaticality of *John continued {Oscar to bathe himself}. Next, we put Af to be the set of nonterminals. To encode the fact that in the phrase-structure tree, himself modifies bathe by attaching at the VP level above bathe, ,:ve attach himself to bathe in the corresponding dependency tree ,,,ith a VP-labeled edge. Finally, we wish to ensure that any dependency tree the parser returns can be mapped back to a phrase structure tree. For this reason, the bilexical grammar's automata must require that the left or right dependents of a word are appropriate to the word's tag. Thus, any edges depending from contin-ued/V, VP,S must be labeled with V, VP, or S-the nonterminals that continued proj ects-and must fa ll in this order on each side. For instance, 1' cont i nue dfF,FP,S may not allow S-labeled right dependents to precede VP-labeled right dependents . . For this scheme to work, certain conditions must hold of the phrase-structure trees we are encoding. Only finitely many nonterminal chains may be available to tag a given word, and the nonterminals in a chain may not repeat . This is essentially in conformance with X-bar theory. The one difference is in the treatment of adjunct.ion: in X-bar theory, three adjuncts to a VP would require 4 VP levels. One may work around this by stipulating a single VP-ADJUNCT level above VP, to which all VP adjuncts (0 or more) attach. 4 This encoding scheme improves on that of [Collins 1996) , because any dependency tree can be converted back to a phrase-structure tree, and this tree is unique. This makes it possible to use the fa st bilexical parser, which (unlike Collins's) produces dependency trees without regard to whether they were derived from phrase structure trees. It also means that a probabilistic parsing model (unlike Collins's) need not be deficient , i.e., probability is not assigned to dependency structures that cannot be used by the phrase-structure grammar. One interesting artifact of Collins 's encoding scheme is that unary rules are impossible : every non terminal level must have at least one dependent , whether on the left or on the right. If desired, the nev;, r scheme can be made to have this property as well. VVe augment V further, so that each non terminal in a non terminal chain is annotated with either \"L'' or \"R,\" indicating that it requires children on the specified side. If VP is marked with \"L,\" then the left-child automaton for that word must rule out any left-dependent sequence that does not have at least one dependent with a VP-labeled edge, and preserve the weights on all other dependent sequences. Relationship to Head Automata It should be noted that weighted bilexical grammars are essentially a special case of head-automaton grammars [Alshawi 1996 ]. As noted in the introduction, head-automaton grammars are bilexical in spirit. However, the left and right dependent.s of a word w are generated not separately, by automata l\\ 11 and 1\\ u , but in interleaved fashion by a single weighted automaton, d w . d w assigns weight to strings over the alphabet 1/ x { +-1 --} ; each such string is an interleaving of lists of left and right dependents from l 7 . Head automata, as well as [Collins 1997 ], can model the case that \u00a73.5 cannot: where nonterminal sequences may include nonterminal cycles. [Alshawi 1996 ] poi11ts out that this makes head automata are fairly powerful. An automaton corresponding to the regular expression ((a, +-)(b, -'-) )* requires its word to have an equal number of left and right children, i.e,. a n 1ub 11 \u2022 (By contrast, a bilexical grammar or dependency grammar may be made to generate { a 11 1ub 11 : n 2: O} only by making words other than w the heads of these strings, so that the words that are allowed to interact bilexically would change.) For syntactic description, this added power is probably unnecessary. (Linguistically plausible interactions among left and right subcat frames, such as fronting, can be captured in bilexical grammars simply via multiple word senses. ) What head-automaton grammars offer over bilexical grammars is the ability for a head to specify an obliqueness order over all its dependents, including arbitrarily many adjuncts. A head-automaton parse tree for a 2 a 1 wb1 b2 is more finely detailed than in dependency grammar . It essentially gives the phrase headed by w a binary-branching analysis, such as 7 Idiom Encoding To the bilexical construction of \u00a7: 3.3, one may add detectors for special phrases. Consider the idioms (a) run scared, (b) run circles [a round NP}, and (c) run NP {i nto the ground}. (a), like most idioms, is only bilexical, so it may be captured \"for free\" : simply increase the weight of the scared arc in 1\u2022,. u n/F \u2022 .But because (b) and ( c) are trilexical , they require augmentation to the grammar. (b) requires a special state to be added to 1' ru n /F, so that the dependent sequence ( cir(:[es, around) may be recognized and weighted heavily. ( c) requires a specialized lexical entry for into; this sense is a preferred dependent of run and has gromz d as a preferred dependent. fo reach way of combining a. and b into a new weighted analysis c (if any ) 1::?. Discover( sfa.rt, end, c) 13. foreach partial analysis a. in Co ,n+1 H. if signa.ture(a.) indicates that a is a full (not partial) analysis of the sentence 1s. then print a The iterations in lines 3 and 11 have yet to be defined, but the dynamic programming idea is clear ( and fa miliar): analyses of length-I substrings can be created from the substrings themselves (line 3), and analyses of successively longer substrings can be created by gluing together shorter analyses in pairs (line 11), until we have one or more analyses of the whole sentence. In particular, the problem has the optimal substructure property: any optimal analysis of a long string can be found by gluing together just optimal analyses of shorter substrings. (An optimal analysis is defined to be a partial analysis of maximum weight for its signature; Discover() ensures that the cha.rt contains only optimal analyses.) For suppose that a. and a.' a. re partial analyses of the same substring, and ha. \ufffde the same signature, but a. has less weight than a.'. Then suboptimal a. cannot be pa. rt of any optimal analysis b in the chart-for if it were, the definition of signature ensures that we could substitute a' for a. in b to get an partial analysis b' of greater total weight than b and the same signature as b, which contradicts b's optimality. The algorithm's running time is dominated by the six nested loops, yielding time 0(n 3 S' 2 d). Here S' is the maximum number of possible signatures that may fall in a given cha.rt cell, and d is the maximum number of ways to combine two adj acent partial analyses into a larger one. Inefficient Chart Parsing of Bilexical Grammars How might we apply the above method to parsing of bilexical grammars? The obvious way is for each partial analysis to represent a subtree. More precisely, each partial analysis would represent a kind of dotted subtree that may not yet have acquired all its children. The signature of such a dotted subtree is a triple (w, se w , s r w), where w E V is an input word, se w is a state of fw , and Sr w is a state of 1'w . If both S\u00a3 w and Sr w are final states, then the signature is said to be complete. It is clear that in line 3 of the algorithm, the sole analysis a. is the triple (w, start state of l\\ v , start state of The case where a. is attached to the root of b as a new left child is similar, and may give another m values for c. \\Vhy is this method inefficient? Because there a. re too many possible signatures. The probability with which b attaches to a. depends on the roots of both a. and b. Since the root w of a could be any of the words at positions start + l, sta1't + 2, ... mid, and there may be min(n, IVI) distinct such words in the worst case, the number S' of possible signatures for a is at least min( n, IVI ). The same is true for b, whose root w' could likewise be any of many words. But then the runtime of the algorithm is O(n 3 min(n, IVl ) 2 IMI) \ufffd O(n 5 ). In a nutshell, the problem is that each chart cell may have to maintain many differently-headed analyses. Efficient Chart Parsing of Bilexical Grammars To eliminate these two additional min( n, !VI) factors, we must reduce the number of possible signatures for a partial analysis. The solution is for partial analyses to represent some kind of contiguous string other than constituents. Each partial analysis in Ci,j will be a new kind of object called a span, which consists of one or two \"half-constituents'\ufffd in a sense to be described. The headword(s) of a span in Ci,j are guaranteed to be at positions i and j in the sentence. This guarantee means that where Ci ,j in the previous section had n-fold uncertainty a.bout the correct location of the headword for the optimal analysis of VVi + 1 VVi + 2 ... H1j, here it will have only : 3 -fold uncertainty. The three possibilities are that Wi is an unattached headword, that Wj is, or that both are. Given a dependency parse tree, we know what its constituents are: a constituent is any substring consisting of a word and all its descendants. The inefficient parsing algorithm of the \u00a74.2 assembled the correct parse tree by finding and gluing together analyses of the tree's constituents in an approved way. For something similar to be possible with spans, we must define what the spans of a given dependency parse tree are, and how to glue analyses of spans together into analyses of larger spans. Not every substring of the sentence is a correct constituent, and in the same way, not every subst'ring is a correct span. Wi' l.l1 i +I ... ' Wj (j > i) of the tree's fringe, such that none of the interior ,vords of the span communicate with any words outside the span. Formally: if i < k < j, and Wk is a dependent of ' Wk' or vice-versa, then i \ufffd k' \ufffd j . Since we will build the parse by assembling analyses of spans, and the interiors of adj acent spans a. re insulated from ea.eh other, we crucially never need to know anything about the internal analysis inside a span . \\\u00a5hen we combine two adj acent spans, we never add a link from or to the interior of either. For, by the definition of span, if such a link were necessary, then the spans being combined could not be spans of the true parse anyway. There is always some other way of decomposing the true parse (itself a span ) into smaller spans so that no such links from or to interiors are necessary. Figure le shows such a decomposition. 6 Any span of greater than two words, say a.gain from 'U'i to ' Wj , can be decomposed uniquely by the following deterministic procedure. Choose i < k < j such that w1., is the rightmost word (strictly inside the span) that connects to wi ; if there is no such word, put k = i+ l. Because crossing links are not allowed, the substrings from Wi ... w1.: and w1.: ... Wj must also be spans. \\V\"e can therefore assemble the original 'Wi ... 'Wj span by concatenating the 'l.l1i . .. w1., and 'WJ., ... Wj spans, and optionally adding a link between the end words, 'l.l\\ and Wj . By construction, there is never any need to add a link between any other pair of words. Notice that when the two narrower spans a. re concatenated, w1.: gets its left children from one span and its right children from the other. The procedure for choosing k can be rephrased declaratively. To wit, the left span in the concatenation, Wi ... w1.: , must be si111ple in the following sense: it must have a direct link between ' U\\ and w1.,, or else have only two words. It is useful to note that the analysis of a span always takes one of three forms; Figure lb illustrates the first two -(la.beled \"yes\" ). In the first case, the endwords tl'i and Wj are not yet connected to each other: that is, the pat.h between them in the final parse tree will involve words outside the span. Then the span consists of two \"half-constituents\"-wi with all its right descendants, followed by Wj with all its left descendants. Wi and Wj both need parents. In the second case, Wj is a descendant of U\\ via a chain of one or more leftward links within the -span itself; then the span consists of Wi and all its right descendants to the left of Wj (inclusive ), and only Wi still needs a pa. rent. The third case is the mirror image of the second. (It is impossible for both 'l.l\\ and Wj to both have parents inside the span : for then some word interior to the span would need a pa.rent outside it.) The signature of a span does not have to state anything a.bout the internal analysis except which of these three cases holds-i.e., which of Wi , Wj need pa.rents. This is needed so that the parser knows when it is able to add a link from i or j to a more distant word after concatenation, without creating multiple parents or otherwise jeopardizing the form of the dependency parse. To determine the allowability of the dependent introduced by such a new link, or the weight associated with it, the signature of a span from Wi to Wj must also include the states of the automata. 1\\v ; and l w i . vVe can now understand the actual algorithm. It is convenient to slightly alter the definition of C i ,j , so that it stores the best analysis ( as a span) of H1d 1 Vi + 1 ... lVj .7 v V e may represent an analysis of a span as a tuple (linktype , a, b) , where linktype specifies the label (E M) and direction of the link, if any, between the leftmost and rightmost words of the span, and where a and b point to 'the narrower spans that were concatenated to obtain this one. If the span is only two words wide, then we represent an analysis of it as ( linktype , w 1 , w:\ufffd ) so that the analysis specifies the words it has chosen from the confusion set. As usual , the analyses we discover in a cell of the cha.rt a. re organized into competitions or subcells by their sig1la.tures. The signature of an analysis has the form ( L?, R?, w L, w R, S r , s c, sirnpl e ?) . Here L? and R? are boolean variables stating whether the leftmost and rightmost words have parents in the span. WL and WR are the leftmost and rightmost words of the span ( as chosen from the appropriate confusion sets). S r is the state work. The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and its scoring approach is compatible ,vith a wide variety of probability models. The obvious parsing algorithm for bilexical grammars (used by most authors) takes time 0( n 5 ). A more efficient 0( n 3 ) method is exhibited. The new algorithm has been implemented and used in a large parsing experiment [Eisner 199Gb ]. kgram Tagging One may use the string-local constraints of \u00a72.4 to score dependency parse trees according to the trigram part-of speech tagging model of [Church 1988) . Each input word w (including ROOT) is regarded as a confusion set over all tuples of the form (t\", t', t, w), where t is a tag for \u2022 w and t\", t' are tags for the two words that precede w. (Thus, V consists of such tuples. ) The weight of the tuple (t\", t', t, w) in its confusion set is log( Pr(w It)\u2022 Pr(t I t\", t')). The bigram (t?, t\ufffd, ti, wi )(t\ufffd\ufffd 1 , t\ufffd + l , ti+l, Wi + i) is an excluded bigram unless t\ufffd = t\ufffd\ufffd 1 and ti = t\ufffd + l . Because of the excluded bigrams, any dependency parse tree has a fringe (including ROOT) of the form (BOS, BOS, t1, 'W1 )(BOS, i1, i2, W2 )(t1, t2, t3, W3 ) ... (t n -1, t 11 , EOS, ROOT) Then the total vveight accruing to the tree from the confusion sets is and to maximize this total is to maximize the probability product shown, just as [Church 1988 ) does. k-gr ms for k > : 3 may be handled in exactly the same way. Since the parser maximizes the above sum of conf\u00b5sion-set \u2022 w eights and the weights accruing from the cl oice of paths through the automata, grammatical structure also helps determine the highest-weighted parse. Cubic-Time Parsing This section begins by reviewing the general idea of chart parsing, presenting a general method drawn from context-free \"dotted-rule'' methods such as [Graham et al. 1980 , Earley 1970) . Second, we will see why this method is inefficient when applied to bilexical grammars in the obvious ,vay. Finally, a more efficient ( cubic time ) algorithm is presented, which applies the general chart parsing method rather differently. Generalized Chart Parsing Method The input is a string n = VV 1 W2 ... Hl 11 of confusion sets (so each VVi \ufffd V). C (the chart ) is an (n+ 1) x (n+ 1) array. The chart cell Ci , j holds a set of partial anal y ses of the input. Each partial analysis has a weight , and also a si g nature that concisely describes its ability to combine with other partial analyses. For each signature s , the subcell Ci , j [s] holds the highest-weight partial analysis of the input substring l'l-'i+l H 7 i+'2 ... llj for vvhich s is the signature. 5  Let Discover(i,j, P) be an operation that replaces C,j [si g nat ure(P)] with the partial analysis P if P has higher weight than the partial analysis currently in C\\ ,j [si g natv1\u2022e(P)]. A basic chart-parsing method is then as follows: 1. for i := 1 to n + l fo reach -WE H'i , where H r n + l = {ROOT} (* seed c hart with members of th t confusion set *) 3. fo reach partial analysis a of the single word w 1. Discover(i -1, i, a) s. for wiclth := 1 to n + l 6. for .s tart := 0 to (n + 1) -width end := . s tart + width 5 ln a more general conception, C\\ ,j [s) holds a summary of all known partial analyses of M 1 i+l W;+2 ... 111 1 j having signature s. Summaries must be defined in such a way that if f is an operation that combines two partial analyses, and A. and H are sets of partial analyses of adjacent substrings, then s-u.mmary({f(a, b) : a EA. and b E H}) must be computable from s 1 1-rnrnary(A) and s-urnrnary(B ). ln addition. if A. and H are both sets of partial analyses of IV;+1 IV;+2 ... 11-\u2022 1 having signature s, then smnrnary(A. U H) must be computable from surnma.ry(A.) and sumrna.ry(H), so that new analyses can be added to the chart. 1n the usual case, where the ultimate goal of the parser is to find the highest-weight dependency parse tree, surnrna.ry(A.) is just the highest-weight parse tree in A. (or, more generally, a forest of all parse trees in A. that tie for the highest weight ). However, if the parser is to return something else, one might set things up so that s-u.rnrnary(A.) was a list. of the 10 highest-weight parse trees in A.-or even something more out.re. lf the parser is being used only to do language modeling for speech recognition, for instance, and the goal is to minimize the per-word error rate, then the summary might give a posterior probability distribution over the confusion set. for the kth word, for each i < k \ufffd j; this would be determined by allowing the partial analyses in A. to vote in proportion to their weight. of the leftmost word's right-dependent automaton 1'w L after the automaton has read all the leftmost word's dependents within the span, and se is similarly the state of the rightmost word's left-dependent automaton l\\ u R . Finally, simple? is true just if the span is simple, as defined above; we use this to prevent ourselves from finding the same analysis in multiple ,:va.ys. 1. 3. \ufffd- 5. 6. 8. Q. 10. ll. 12. 13. 14. 1-5. 16. 17. 18. l!:i. fo r i := 1 to n fo reach Wi El l/; , -Wi+1 E l 1 ' V i+1 such that 'Wi'Wi+ 1 is not an excluded bigram, where ll'n+1 = {ROOT} then print a Computing the signatures is fa irly straightforward. For example, if l-inktype is rightward in line 16 (making 'W s ta r t a dependent of 'W en d), then the new analysis ( linktype, a, b) has the signature (TRUE, FA LSE, sig(a). 'WL , s\u2022 ig(b).'l. liR , sig(a) .s r , 8(sig(b).sc, sig(a.).wL), TRUE) Computing weights of analyses is also fairly straightforward. In the above example, (linktype , a, b) has weight weight(a.) + weight(b) + a. 1'cweight(sig(b) .se, sig(a.).wL) + stopweight(sig(a.).se ) + stopwe-ight (sig(b) .sr ) Note the use of the final-state weights, stozn.ueight , to reflect the fa ct that the overlapping word W r nid (see line 11) can no longer get new children once it is hidden in the interior of the span. 8 For a two-word span (linktype, Wi , Wi+l ) with rightward link, as created in line 4, the weight is a1\u2022cweight(wi+ 1 .START , wi ) plus the weight of wi (only!) in its conf usion set. The algorithm requires 0( n 2 S) space for the cha.rt, where S is the maximum number of signatures per cell. The running time is a. gain dominated by the six nested loops. It is O(n 3 \u2022 S 2 \u2022 IA11). Given the definition of signatures, S = O(g 2 t' 2 ); that is, it is bounded by a constant times (max size of confusion set,) 2 times (max states per a. :utoma.ton) 2 . (Crucially, there a. re only g choices for ea.eh of 'WL and 'WR .) Thus, the grammar constant S is typically small. ,vith careful coding it is possible to improve the runtime somewhat , from O(n 3 g 4 t 4 IA11) to O(n 3 g 3 t 2 m). Perhaps only some link types a. re possible in line 15, so IA\u2022 f l can be replaced by 1n. We can save a. factor of g at line 11, because the restriction on sig(b).wL means that we only need to iterate through S/g of the signatures. Finally, we can reduce S to just O(g 2 t), which helps both time and space complexity. Instead of Discover() using a. single cha.rt to maintain the best analysis with signature (L?, R?, WL , WR , sr , se, sirnple ?), it will use two charts to maintain, respectively, the best analyses with signatures (L?, R?, WL , WR, sr ,HALTED, simple?) and (L?, R?, WL, WR ,HALTED, se, simple?). Ea.eh of these two analyses has already had one stopweight added to its weight. The algorithm should now be modifi ed to select a from the first chart and b from the second cha. rt. Conclusions This paper has introduced a. new formalism, weighted bilexical grammars, in which individual lexical items can have idiosyncratic selectional influences on ea.eh other. Such \"bilexica.lism'' has been a theme of much current",
    "abstract": "Computational linguistics has a long tradition of lexicalized grammars, in which each grammatical rule is spe cialized for some individual word. The earliest lexicalized rules were word-specific subca. tegoriza. tion frames. It is now common to find fully lexicalized versions of many grammatical formalisms, such as context-free and tree-adjoining grammars [Schabes et al. 1988] . Other formalisms, such as dependency grammar [Mel'cuk 1988] \u2022 and head-driven phrase-structure grammar [Pollard & Sag 1994] , are explicitly lexical from the start. Lexicalized grammars have two well-known advantages. Where syntactic acceptability is sensitive to the quirks of individual words, lexicalized rules are necessary for linguistic description. Lexicalized rules a. re also computationally cheap for parsing written text: a parser may ignore those rules that do not mention any input words. More recently, a third advantage of lexicalized grammars has emerged. Even when syntactic acceptabilit: I) is not sensitive to the particular words chosen, syntactic distribution may be [Resnik 1993]  . Certain words may be able but highly unlikely to modify certain other words. Such facts can be captured by a. probabilistic lexicalized grammar, where they may be used to resolve ambiguity in favor of the most probable analysis, and also to speed parsing by avoiding (\"pruning\" ) unlikely search paths. Accuracy and efficiency can therefore both benefit. Recent work along these lines includes [Charniak 1995, Collins 1996, Eisner 1996b, Collins 1997], who reported state-of-the-art parsing accuracy. Related models are proposed without evaluation in [Lafferty et al. 1992 , Alsha. wi 1996]  . This recent flurry of probabilistic lexicalized parsers has focused on what one might call bilexical granuuars, in which ea. eh grammatical rule is specialized for not one but two individual words. 1 The central insight is that specific words subcategorize to some degree for other specific words: ta: r is a good object for the verb ra ise. Accordingly, these models estimate, for example, the probability that ( a. phrase headed by) word y modifies word x, for any two words x, y in the vocabulary V . At first blush, probabilistic bilexica. l grammars appear to carry a. substantial computational penalty. Chart parsers derived directly from CKY or Earley's algorithm take time O(n 3 min(n, 11 1 1 ) 2 ), which amounts to O(n 5 ) in practice. Such algorithms implicitly or explicitly regard the grammar as a context-free grammar in which a noun phrase headed by tiger bears the special nonterminal NP ti g er \u2022 Such :::::::: O( n 5 ) algorithms are explicitly used by [Alshawi 1996, Collins 1996]  , and almost certainly by [Charniak 1995]  as well. The present paper formalizes an inclusive notion of bilexica. l grammars, and shows that they can be parsed in time only O(n 3 g 3 t' 2 m, ):::::::: O(n 3 ), where g, t, and m are bounded by the grammar and are typically small. (g is the maximum number of senses per input word, t measures the degree of lexical interdependence that the grammar allmvs among the several children of a. word, and rn bounds the number of modifier relations that the parser need distinguish for a given pair of words.) The new algorithm also reduces space requirements to 0(11 2 g 2",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "71",
    "year": "1997",
    "month": "September 17-20",
    "title": "Bilexical Grammars and a Cubic-time Probabilistic Parser"
}