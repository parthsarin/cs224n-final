{
    "framing_question": "If this paper introduces a task or benchmark, how does the paper propose that performance be measured, if at all?",
    "response": "In order to evaluate the performance of the proposed metric, the paper introduces a pairwise information ordering task. For this task, the paper proposes to measure performance in terms of accuracy, which is the proportion of text pairs for which the coherent text is correctly identified.",
    "article": "We propose a simple and effective metric for automatically evaluating discourse coherence of a text using the outputs of a coreference resolution model. According to the idea that a writer tends to appropriately utilise coreference relations when writing a coherent text, we introduce a metric of discourse coherence based on automatically identified coreference relations. We empirically evaluated our metric by comparing it to the entity grid modelling by Barzilay and Lapata (2008) using Japanese newspaper articles as a target data set. The results indicate that our metric better reflects discourse coherence of texts than the existing model. Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006) , summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004) . Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988) , into computational models for each task, ranging from heuristic rule-based to sophisticated machine learning-based approaches. Some of this research has relied on the occurrence of discourse entities (e.g. NPs and pronouns) to capture cohesion of a text for indirectly estimating discourse coherence. Barzilay and Lapata (2008) 's approach, for instance, models the transition of discourse entities appearing in adjacent sentences for capturing local discourse coherence, which is derived from the notion of Centering Theory. In their approach, the plausible transition of discourse entities in a coherent text is trained together with a set of incoherent texts by using a ranking SVM (Joachims, 2002) , making use of a grid of each discourse entity with regard to its grammatical role, called an entity grid representation. Their approach to evaluating discourse coherence is quite useful when discourse entities explicitly appear in languages such as English. In their evaluation, they reported their coherence modeling based on the entity grid representation contributes to drastically improving accuracy on the information ordering task, which is the pairwise ranking problem given a pair of coherent and incoherent texts in English. However, in languages such as Japanese and Italian, capturing the transition of discourse entities is relatively difficult due to the frequent use of ellipses. As an example of employing the entity grid model in Japanese, Yokono and Okumura (2010) directly attempted this for representing grid using typical Japanese grammatical roles (wa (topic), ga (subj), o / ni (obj / i-obj) and others). They conducted an empirical evaluation of pairwise ranking of Japanese texts, replicating the experimental settings by Barzilay and Lapata (2008) . Their result shows their model achieved around 70% in accuracy, whereas the evaluation result on the English data set reaches around 90%. This difference of performance might be caused by the frequent occurrence of ellipses. In Japanese, for example, subjects in a sentence are frequently unrealised, resulting in the less frequent occurrence of adjacent discourse entities in a same coreference, which are essential for capturing the transition of discourse entities in entity grid modelling (Barzilay and Lapata, 2008) . Against this background, we propose a metric of discourse coherence, which takes into account any pair of discourse entities in a text to capture the relationship of the entities distantly appeared in a text, which cannot not be directly exploited in the entity grid approaches. In order to evaluate discourse coherence using our metric, we utilise the outputs of a coreference resolution model (especially, the reliability of each output of the model). The assumption behind it is that one tends to appropriately utilise coreference relations when writing a coherent text, i.e. the better use of coreference relations is considered as a good indicator of coherent texts. This paper is organised as follows. Section 2 briefly reviews the previous work on automatically evaluating discourse (local) coherence. Section 3 explains the proposed metric of evaluating discourse coherence exploiting the outputs of coreference resolution and Section 4 s 1 : [John] bought [iPad2] Related work There has been an increase in recent work for evaluating discourse (local) coherence of a text (Barzilay and Lapata, 2008; Karamanis et al., 2004; Lin et al., 2011; Miltsakaki and Kukich, 2000; Higgins et al., 2004, etc.) , which strongly relates to the cohesion of discourse entities appearing in the text from the theoretical perspective mainly based on Centering Theory (Grosz et al., 1995) . For example, Karamanis et al. (2004) and Miltsakaki and Kukich (2000) proposed a metric of coherence directly utilising the transition of centers in a text, as Centering Theory does. According to the analysis by Poesio et al. (2004) , Karamanis et al. (2004) define a metric based on the numbers of missing backward-looking centers, each of which is a discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in a text based on the transition of discourse entities, which was done in a series of studies (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008) . In Barzilay and Lapata (2008) , the transition of discourse entities in adjacent discourse units (e.g. sentences) is formalised as an entity grid, which is a matrix of discourse entities and their realised grammatical roles, because a grammatical role of a discourse entity is a good indicator of its salience. For example, a given input text shown in Figure 1 , consisting of the three sentences, each discourse entity is represented in the entity grid shown in Table 1 . In the entity grid, each column is filled with the corresponding label (e.g. S (subject), O (object), X (others) and -(not realised)). In the grid, the local transition of entities with regard to the labels can be seen as a generalisation of the center transition discussed in a series of Centering studies (Walker et al., 1997; Grosz et al., 1995) . Therefore, exploiting the transition becomes a good indicator of (local) discourse coherence. In their work, the transition of each entity was used as a feature for distinguishing a coherent text from an incoherent one. As an extension of Barzilay and Lapata (2008) , Lin et al. (2011) took into account the use of discourse relations to revise the formulation of an entity grid. They used the four types of discourse relations (Temporal, Contingency, Comparison and Expansion) defined in the Penn Discourse Treebank (PDTB) instead of grammatical roles, which are automatically acquired by the discourse parser by Lin et al. (2011) . For grid representation, they calculated the tran-John iPad2 gift Lucy sound system birthday party s 1 S O X X - - s 2 - S - - X - s 3 S - X X - X Table 1 : Entity grid of the input example in Figure 1 s \u2032 1 =(s 1 ): (Joachims, 2002) . Through their empirical evaluation they reported their extension of the entity grid representation contributes to improving performance on the pairwise ordering task compared to the original entity grid model. A metric for evaluating coherence based on coreference resolution As explained in Section 2, typical approaches to modeling discourse coherence have exploited the transition of discourse entities in terms of grammatical roles or discourse relations defined in PDTB. In contrast, we estimate discourse coherence by a metric relying on the outputs of an NP coreference resolution model. For instance, from the coherent text shown in Figure 1 , the corresponding incoherent text is generated by randomly reordering sentences, one of which is as shown in Figure 2 . In this incoherent text, as the pronoun \"it\" is placed relatively far from its antecedent \"iPad2\" and a distractor \"birthday party\" is inserted between these two expressions, the interpretation of \"it\" is more difficult than the case of the coherent text. As a result, applying a typical coreference resolution model to coherent and incoherent texts gives rise to the difference in the number of correctly identified coreference relations. In addition, if there is no difference in terms of the number, there may be a difference in the reliability score (i.e. predicted probability outputted by a classifier) of the resolved relations. Based on these differences, we propose a metric for evaluating discourse coherence, which is calculated according to the following two steps: 1. a coreference (or anaphora) resolution model trained with annotated coherent texts is applied to a target text T . 2. the coherence score of T is calculated from the outputs of step1 by coherence(T ) = 1 N N j score ana (i, j), ( 1 ) where T is a target text, j is a candidate anaphor appearing in T and i is the most likely candidate antecedent of j. N is the number of candidate anaphors appearing in T . The reliability score of the coreference relation of i and j, score ana (i, j), is the output score (e.g. predicted probability) obtained after a coreference model is applied to T in step1. Note that the proposed metric can also be used as one of the features for the entity grid model because it is obtained from a different perspective from the entity grid (i.e. information of the discourse entity transition). In Section 5.3 we will also demonstrate the results of the entity grid model employing our metric as a feature. Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010) , but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002) , In the work on Japanese anaphora resolution by Iida and Poesio (2011) , they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph| j) 2 (2) where j is a candidate anaphor and i is the most likely candidate antecedent of j. P(cor e f |i, j) is calculated by a simple coreference classifier such as Ng and Cardie (2002) and P(anaph| j) is the score of anaphoricity of j, which is used to exclude typical non-anaphoric mentions such as pleonastic it. Given equation ( 2 ), their anaphora resolution model judge as anaphoric if coref(i, j) \u2265 0.5; otherwise non-anaphoric. In this work, we adopt the above approach to obtain score ana (i, j) needed in equation (1). By using cor e f (i, j) we define score ana (i, j) as follows: score ana (i, j) = \u2212 log(1 \u2212 max i coref(i, j)) (3) The feature set and detailed configuration for model creation generally follows the original work by Iida and Poesio (2011) . For creating a classifier, we used MegaM 1 , an implementation of the Maximum Entropy model, with default parameter settings. As an anaphoricity determination model (Iida et al., 2005) , we used the selection-then-classification model, which first selects a most likely candidate antecedent i and then determines the anaphoricity of candidate anaphor j referring to the information from a pair of i and j, because Iida et al. (2005) reported their model determines anaphoricity more precisely than a simple anaphoricity model (e.g. Ng and Cardie (2002) Empirical Evaluation This section first evaluates performance of NP coreference resolution on coherent and incoherent texts for exploring the possible use of these results on evaluating discourse coherence; we then conduct an empirical evaluation on ranking a pair of coherent and incoherent texts by comparing our metric with the entity grid model. Data set For our evaluation, we used the NAIST text corpus, which consists of Japanese newspaper articles containing manually annotated NP coreference relations. Because the corpus has no explicit boundary between training and test sets, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009) . Table 2 summarises the statistics of annotated coreference relations in the corpus. Because the data set contains some texts consisting of only a sentence 2 , we excluded them for our evaluation of information ordering. In line with the experiments done by Barzilay and Lapata (2008) , we created 20 different texts by randomly scrambling the order of the sentences in an original text, each of which is henceforth called an incoherent text, while the original text is called a coherent text. In this evaluation, we followed Barzilay and Lapata (2008) 's experimental setting, that is, the task of pairwise ordering, i.e., to detect a coherent text given a coherent and incoherent text pair. Experiment 1: NP coreference resolution on incoherent texts We first evaluate performance of NP coreference resolution on both coherent and incoherent texts. During the training phase, we use only coherent texts as the training instances for creating a classifier used in each model. By using only coherent texts for training, we expect that a model appropriately identifies coreference (or anaphoric) relations in coherent texts, while it is less successful in incoherent texts. Next, classifiers induced from coherent texts are applied to either coherent or incoherent texts to investigate the difference of performance on coreference resolution. Table 3 shows the results for the recall, precision and F -score of pairwise classification on NP coreference resolution on evaluating coherent or incoherent texts, where the 'coherent' which stands for the results on coherent texts, the 'incoherent:\u00b5' and 'incoherent:\u03c3' which mean the averaged score of the results on incoherent texts and its standard deviation. Table 3 demonstrates that the 'coherent' obtains better performance in F -score than 'incoherent:\u00b5' on NP coreference resolution. It indicates that the performance of NP coreference resolution strongly correlates to discourse coherence, that is, this relative difference of performance between co-Recall Precision F-score coherent 0.624 0.508 0.560 incoherent 0.538\u00b1 0.004 0.496 \u00b1 0.004 0.516 \u00b1 0.004 Table 3 : Results using NP coreference resolution herent and incoherent texts is expected to lead to better discrimination on information ordering which we discuss in Section 5.3. Experiment 2: pairwise information ordering We next investigate the effects of the metric proposed in Section 3 for the task of pairwise information ordering comparing the results with the entity grid model. As a baseline model, we use a model which randomly selects a text from two given texts. Alternative baselines are variants of the entity grid model; one captures the transition of discourse entities based on lexical chaining (i.e. NPs which have identical head strings are grouped as a cluster), and the other uses the outputs of a NP coreference resolution model for the entity grid representation instead of using lexical chaining. As for the coreference resolution model for obtaining the entity grid representation, we employed the original selection-then-classification model (SCM) described in Section 4 because it performed better in the final evaluation (i.e. pairwise ordering). This may be because the original SCM tends to accurately identify coreference relations in incoherent texts as well as coherent ones, and as a result those relations are considered as less noisy inputs to the entity grid model. For the entity grid representation in Japanese, we employed the work by Yokono and Okumura (2010) , which is based on Japanese case-makers (e.g. wa (topic), ga (subject), o (object)) to simply identify grammatical roles of discourse entities 3 . Note that we excluded the extensions of the base entity grid modeling (e.g. separating discourse entities into two classes based on the salience of each, introduced by Barzilay and Lapata (2008) ) for simplification. To create a pairwise ranker based on the entity grid modelling, we used a ranking SVM (Joachims, 2002) as Barzilay and Lapata (2008) did. In this evaluation, we also compared the entity grid models using the coherence metric based on NP coreference as a feature. The results are shown in Table 4 . These results demonstrate the entity grid models and the models based on our coherence metric achieved better accuracy than the random baseline. By comparing the entity grid models with and without coreference resolution, the results show that the former outperforms the latter. It indicates Japanese NP coreference resolution is also useful for grid representation, the same as for English coreference resolution adopted in Barzilay and Lapata (2008) . Furthermore, ranking based on our metric achieved better accuracy than the entity grid models. This is because our metric has an advantage of being able to capture the coherence and incoherence resulting from the use of long-distance coreference relations, while the entity grid model focuses on the local coherence based on discourse entities appearing in the adjacent two or three sentences. Table 4 : Results of pairwise information ordering Our metric utilises the appropriateness of anaphoric functions, one of characteristics of coherence which was not directly integrated in the entity grid model. Therefore, by combining them we can expect to see an improvement in accuracy. The last row in Table 4 shows the result of the entity grid model using coreference resolution integrated with our metric as a feature. As expected, the result ((c) in Table 4 ) obtained the best accuracy out of all the results shown in Table 4 4 . It indicates that long-distant coreference relations are also important for evaluating discourse coherence in a text. Conclusion In this paper we proposed a metric for evaluating discourse coherence based on the outputs of a coreference resolution model to reflect the idea that a writer tends to appropriately utilise anaphoric or coreference relations when writing a coherent text. In order to investigate the effects of the proposed metric, we conducted an empirical evaluation on a pairwise ordering task, taking the NAIST text corpus as a target data set. The results of our evaluation demonstrated that the metric calculated using the outputs of NP coreference resolution achieved better accuracy than the entity grid model (Barzilay and Lapata, 2008) . Moreover, the result of integrating the metric with the entity grid model shows the improvement of 7 points in accuracy. In this work, we focused on the use of NP coreference resolution as cues for evaluating discourse coherence in a text. However, even if we refer to coreference relations as indicators of discourse coherence, the relations are sometime sparse in a text, resulting in assigning an inappropriate score to it. One simple way to avoid this problem is to take into account other types of reference behaviour, such as zero anaphora and bridging anaphora, because this type of reference function can often relate distant discourse fragments (e.g. two clauses placed far from each other). In addition, although we focused on exploiting the relationship of discourse entities in terms of anaphoric functions, the (latent) topic transition in a text is another key for capturing text coherence, as discussed by Chen et al. (2009) . Therefore, one interesting issue for discourse coherence is how to integrate the above factors into existing coherence models.",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 2.5776826777512696e-06,
        "foundation": 0.0,
        "none": 1.0
    }
}