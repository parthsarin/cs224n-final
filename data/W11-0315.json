{
    "article": "Finding the right representation for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce. This paper investigates language model representations, in which language models trained on unlabeled corpora are used to generate real-valued feature vectors for words. We investigate ngram models and probabilistic graphical models, including a novel lattice-structured Markov Random Field. Experiments indicate that language model representations outperform traditional representations, and that graphical model representations outperform ngram models, especially on sparse and polysemous words. Introduction NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve strong performance. Thus, a part-of-speech (POS) tagger would traditionally use a feature like, \"the previous token is the\" to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to texts like the Web, scientific domains, and personal communications like emails, all of which have very different characteristics from traditional training corpora. Collecting labeled training data for each new target domain is typically prohibitively expensive. We investigate representations that can be applied when domain-specific labeled training data is scarce. An increasing body of theoretical and empirical evidence suggests that traditional, manually-crafted features limit systems' performance in this setting for two reasons. First, feature sparsity prevents systems from generalizing accurately to words and features not seen during training. Because word frequencies are Zipf distributed, this often means that there is little relevant training data for a substantial fraction of parameters (Bikel, 2004) , especially in new domains (Huang and Yates, 2009) . For example, word-type features form the backbone of most POS-tagging systems, but types like \"gene\" and \"pathway\" show up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features \"gene\" and \"pathway\" (Ben-David et al., 2009; Blitzer et al., 2006) . Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings. For instance, the word type \"signaling\" appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, \"Interest rates rose, signaling that . . . \" (Marcus et al., 1993) . In biomedical text, however, \"signaling\" appears primarily in the phrase \"signaling pathway,\" where it is considered a noun (NN) (PennBioIE, 2005) ; this phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates, 2010a) . Our response to these problems with traditional NLP representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples. Our approach depends on the well-known distributional hypothesis, which states that a word's meaning is identified with the contexts in which it appears (Harris, 1954; Hindle, 1990) . Our goal is to develop probabilistic lan-guage models that describe the contexts of individual words accurately. We then construct representations, or mappings from word tokens and types to real-valued vectors, from these language models. Since the language models are designed to model words' contexts, the features they produce can be used to combat problems with polysemy. And by careful design of the language models, we can limit the number of features that they produce, controlling how sparse those features are in training data. In this paper, we analyze the performance of language-model-based representations on tasks where domain-specific training data is scarce. Our contributions are as follows: 1. We introduce a novel factorial graphical model representation, a Partial-Lattice Markov Random Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (HMM) for language modeling. 2. In experiments on POS tagging in a domain adaptation setting and on weakly-supervised information extraction (IE), we quantify the performance of representations derived from language models. We show that graphical models outperform ngram representations. The PL-MRF representation achieves a state-of-the-art 93.8% accuracy on the POS tagging task, while the HMM representation improves over the ngram model by 10% on the IE task. 3. We analyze how the performance of the different representations varies due to the fundamental challenges of sparsity and polysemy. The next section discusses previous work. Sections 3 and 4 present the existing representations we investigate and the new PL-MRF, respectively. Sections 5 and 6 describe our two tasks and the results of using our representations on each of them. Section 7 concludes. Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2006) ; 2) dimensionality reduction techniques for vector space models (Deerwester et al., 1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005; Blei et al., 2003; V\u00e4yrynen et al., 2007) ; 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar-tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009) ; 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009) , some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b) . In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PL-MRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum\u00e9 III, 2007; Jiang and Zhai, 2007; Daum\u00e9 III and Marcu, 2006; Finkel and Manning, 2009; Dredze et al., 2010; Dredze and Crammer, 2008) . Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009) . Daum\u00e9 III et al. (2010) use semisupervised learning to incorporate labeled and unlabeled data from the target domain. In contrast, we investigate a domain adaptation setting where no labeled data is available for the target domain. Representations A representation is a set of features that describe instances for a classifier. Formally, let X be an instance set, and let Z be the set of labels for a classification task. A representation is a function R : X \u2192 Y for some suitable feature space Y (such as R d ). We refer to dimensions of Y as features, and for an instance x \u2208 X we refer to values for particular dimensions of R(x) as features of x. Traditional POS-Tagging Representations As a baseline for POS tagging experiments and an example of our terminology, we describe a representation used in traditional supervised POS taggers. The instance set X is the set of English sentences, and Z is the set of POS tag sequences. A traditional representation TRAD-R maps a sentence x \u2208 X to a sequence of boolean-valued vectors, one vector per Representation Feature TRAD-R \u2200 w 1[x i = w] \u2200 s\u2208Suffixes 1[x i ends with s] 1[x i contains a digit] NGRAM-R \u2200 w ,w P (w ww )/P (w) ) ] represents a set of boolean features, one for each value of a, where the feature is true iff g(a) is true. x i represents a token at position i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity}, k (and k) represents a value for a latent state (set of latent states) in a latentvariable model, y * represents the optimal setting of latent states y for x, y i is the latent variable for x i , and y i,j is the latent variable for x i at layer j. prefix(y,p) is the plength prefix of the Brown cluster y. HMM-TOKEN-R \u2200 k 1[y i * = k] HMM-TYPE-R \u2200 k P (y = k|x = w) I-HMM-TOKEN-R \u2200 j,k 1[y i,j * = k] BROWN-TOKEN-R \u2200 j\u2208{\u22122,\u22121,0,1,2} \u2200 p\u2208{4,6,10,20} prefix(y i+j , p) BROWN-TYPE-R \u2200 p prefix(y, p) LATTICE-TOKEN-R \u2200 j,k 1[y i,j * = k] LATTICE-TYPE-R \u2200 k P (y = k|x = w) word x i in the sentence. Dimensions for each latent vector include indicators for the word type of x i and various orthographic features. Table 1 presents the full list of features in TRAD-R. Since our IE task classifies word types rather than tokens, this baseline is not appropriate for that task. Below, we describe how we can learn representations R by using a variety of language models, for use in both our IE and POS tagging tasks. All representations for POS tagging inherit the features from TRAD-R; all representations for IE do not. Ngram Representations N-gram representations model a word type w in terms of the n-gram contexts in which w appears in a corpus. Specifically, for word w we generate the vector P (w ww )/P (w), the conditional probability of observing the word sequence w to the left and w to the right of w. The experimental section describes the particular corpora and language modeling methods used for estimating probabilities. HMM-based Representations In previous work, we have implemented several representations based on HMMs (Rabiner, 1989 ), which we used for both POS tagging (Huang and Yates, 2009) and IE (Downey et al., 2007b ). An HMM is a generative probabilistic model that generates each word x i in the corpus conditioned on a latent variable y i . Each y i in the model takes on integral values from 1 to K, and each one is generated by the latent variable for the preceding word, y i\u22121 . The joint distribution for a corpus x = (x 1 , . . . , x N ) and a set of state vectors y = (y 1 , . . . , y N ) is given by: P (x, y) = i P (x i |y i )P (y i |y i\u22121 ). Using Expectation-Maximization (EM) (Dempster et al., 1977) , it is possible to estimate the distributions for P (x i |y i ) and P (y i |y i\u22121 ) from unlabeled data. We construct two different representations from HMMs, one for POS tagging and one for IE. For POS tagging, we use the Viterbi algorithm to produce the optimal setting y * of the latent states for a given sentence x, or y * = arg max y P (x, y). We use the value of y i * as a new feature for x i that represents a cluster of distributionally-similar words. For IE, we require features for word types w, rather than tokens x i . We use the K-dimensional vector that represents the distribution P (y|x = w) as the feature vector for word type w. This set of features represents a \"soft clustering\" of w into K different clusters. We refer to these representations as HMM-TOKEN-R and HMM-TYPE-R, respectively. Because HMM-based representations offer a small number of discrete states as features, they have a much greater potential to combat feature sparsity than do ngram models. Furthermore, for tokenbased representations, these models can potentially handle polysemy better than ngram language models by providing different features in different contexts. We also compare against a variation of the HMM from our previous work (Huang and Yates, 2010a), henceforth HY10. This model independently trains M separate HMM models on the same corpus, initializing each one randomly. We can then use the Viterbi-optimal decoded latent state of each independent HMM model as a separate feature for a token. We refer to this language model as an I-HMM, and the representation as I-HMM-TOKEN-R. Finally, we compare against Brown clusters (Brown et al., 1992) A Novel Lattice Language Model Representation Our final language model is a novel latent-variable language model with rich latent structure, shown in Figure 1 . The model contains a lattice of M \u00d7 N latent states, where N is the number of words in a sentence and M is the number of layers in the model. We can justify the choice of this model from a linguistic perspective as a way to capture the multidimensional nature of words. Linguists have long argued that words have many different features in a high dimensional space: they can be separately described by part of speech, gender, number, case, person, tense, voice, aspect, mass vs. count, and a host of semantic categories (agency, animate vs. inanimate, physical vs. abstract, etc.), to name a few (Sag et al., 2003) . Our model seeks to capture a multidimensional representation of words by creating a separate layer of latent variables for each dimension. The values of the M layers of latent variables for a single word can be used as M distinct features in our representation. The I-HMM attempts to model the same intuition, but unlike a lattice model the I-HMM layers are entirely independent, and as a result there is no mechanism to enforce that the layers model different dimensions. Duh (2005) previously used a 2-layer lattice for tagging and chunking, but in a supervised setting rather than for representation learning. Let Cliq(x, y) represent the set of all maximal cliques in the graph of the MRF model for x and y. Expressing the lattice model in log-linear form, we can write the marginal probability P (x) of a given sentence x as: y c\u2208Cliq(x,y) score(c, x, y) x ,y c\u2208Cliq(x ,y ) score(c, x , y ) where score(c, x, y) = exp(\u03b8 c \u2022 f c (x c , y c )). Our model includes parameters for transitions between two adjacent latent variables on layer j: \u03b8 trans i,s,i+1,s ,j for y i,j = s and y i+1,j = s . It also includes observation parameters for latent variables and tokens, as well as for pairs of adjacent latent variables in different layers and their tokens: \u03b8 obs i,j,s,w and \u03b8 obs i,j,s,j+1,s ,w for y i,j = s, y i,j+1 = s , and x i = w. Computationally, the lattice MRF is preferable to a na\u00efve Factorial HMM (Ghahramani and Jordan, 1997) representation, which would require O(2 M ) parameters for an M -layer model. However, exact training and inference in supervised settings are still intractable for this model (Sutton et al., 2007) , and thus it has not yet been explored as a language model, which requires even more difficult, unsupervised training. Training is intractable in part because of the difficulty in enumerating and summing over the exponentially-many configurations y for a given x. We address this difficulty in two ways: by modifying the model, and by modifying the training procedure. Partial Lattice MRF Instead of the full lattice model, we construct a Partial Lattice MRF (PL-MRF) model by deleting certain edges between latent layers of the model (dashed gray edges in Figure 1 ). Let c = N 2 , where N is the length of the sentence. If i < c and j is odd, or if j is even and i > c, we delete edges between y i,j and y i,j+1 . The same lattice of nodes remains, but fewer edges and paths. A central \"trunk\" at i = c connects all layers of the lattice, and branches from this trunk connect either to the branches in the layer above or the layer below (but not both). The result is a model that retains most 2 of the edges of the full model. Additionally, the pruned model makes the branches conditionally independent from one another, except through the trunk. For instance, the right branch at layers 1 and 2 in Figure 1 (y 1,4 , y 1,5 , y 2,4 , and y 2,5 ) are disconnected from the right branch at layers 3 and 4 (y 3,4 , y 3,5 , y 4,4 , and y 4,5 ), except through the trunk and the observed nodes. As a result, excluding the observed nodes, this model has a low tree-width of 2 (excluding observed nodes), and a variety of efficient dynamic programming and message-passing algorithms for training and inference can be readily applied (Bodlaender, 1988) . 3 Our inference algorithm passes information from the branches inwards to the trunk, and then upward along the trunk, in time O(K 4 M N ). As with our HMM models, we create two representations from PL-MRFs, one for tokens and one for types. For tokens, we decode the model to compute y * , the matrix of optimal latent state values for sentence x. For each layer j and and each possible latent state value k, we add a boolean feature for token x i that is true iff y * i,j = k. For types, we compute distributions over the latent state space. Let y be the column vector of latent variables for word x. For each possible configuration of values k of the latent variables y, we add a real-valued features for x given by P (y = k|x = w). We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R, respectively. Parameter Estimation We train the PL-MRF using contrastive estimation, which iteratively optimizes the following objective function on a corpus X: x\u2208X log y c\u2208Cliq(x,y) score(c, x, y) x \u2208N (x),y c\u2208Cliq(x ,y ) score(c, x , y ) 2 As M, N \u2192 \u221e, 5 out of every 6 edges are kept. 3 c.f. a tree-width of min(M ,N for the unpruned model where N (x), the neighborhood of x, indicates a set of perturbed variations of the original sentence x. Contrastive estimation seeks to move probability mass away from the perturbed neighborhood sentences and onto the original sentence. We use a neighborhood function that includes all sentences which can be obtained from the original sentence by swapping the order of a consecutive pair of words. Training uses gradient descent over this non-convex objective function with a standard software package (Liu and Nocedal, 1989 ) and converges to a local maximum (Smith and Eisner, 2005) . For tractability, we modify the training procedure to train the PL-MRF one layer at a time. Let \u03b8 i represent the set of parameters relating to features of layer i, and let \u03b8 \u00aci represent all other parameters. We fix \u03b8 \u00ac0 = 0, and optimize \u03b8 0 using contrastive estimation. After convergence, we fix \u03b8 \u00ac1 , and optimize \u03b8 1 , and so on. We use a convergence threshold of 10 \u22126 , and each layer typically converges in under 100 iterations. Domain Adaptation for a POS Tagger We evaluate the representations described above on a POS tagging task in a domain adaptation setting. Experimental Setup We use the same experimental setup as in HY10: the Penn Treebank (Marcus et al., 1993) Wall Street Journal portion for our labeled training data; 561 MEDLINE sentences (9576 types, 14554 tokens, 23% OOV tokens) from the Penn BioIE project (PennBioIE, 2005) for our labeled test set; and all of the unlabeled text from the Penn Treebank WSJ portion plus a MEDLINE corpus of 71,306 unlabeled sentences to train our language models. The two texts come from two very different domains, making this data a tough test for domain adaptation. We use an open source Conditional Random Field (CRF) (Lafferty et al., 2001 ) software package 4 designed by Sunita Sarawagi and William W. Cohen to implement our supervised models. Let X be a training corpus, Z the corresponding labels, and R a representation function. For each token x i in X, we include a parameter in our CRF model for all features R(x i ) and all possible labels in Z. Furthermore, we include transition parameters for pairs of consecutive labels z i , z i+1 . For representations, we tested TRAD-R, NGRAM-R, HMM-TOKEN-R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE- 12, 16, and 20 layers) . Following HY10, each latent node in the I-HMMs have 80 possible values, creating 80 8 \u2248 10 15 possible configurations of the 8-layer I-HMM for a single word. Each node in the PL-MRF is binary, creating a much smaller number (2 20 \u2248 10 6 ) of possible configurations for each word in a 20-layer representation. NGRAM-R was trained using an unsmoothed trigram model on the Web 1Tgram corpus. To keep the feature set manageable, we included the top 500 most common ngrams for each word type, and then used mutual information on the training data to select the top 10,000 most relevant ngram features for all word types. We incorporated ngram features as binary values indicating whether x i appeared with the ngram or not. We also report on the performance of Brown clusters and Blitzer et al.'s Structural Correspondence Learning (SCL) (2006) technique, which uses manually-selected \"pivot\" words (like \"of\", \"the\") to learn domain-independent features. Finally, we compare against the self-training CRF technique from HY10. Results and Discussion For each representation, we measured the accuracy of the POS tagger on the biomedical test text. Table 2 shows the results for the best variation of each kind of model -20 layers for the PL-MRF, 7 layers for the I-HMM, and 1000 clusters for the Brown clustering. All language model representations significantly outperform the SCL model and the TRAD-R baseline. The novel PL-MRF model outperforms the previous state of the art, the I-HMM model, and much of the performance increase comes from a 11.3% relative reduction in error on words that appear in biomedical texts but not in newswire texts. Both graphical model representations significantly outperform the ngram model, which is trained on far more text. For comparison, our best model, the PL-MRF, achieved a 96.8% in-domain accuracy on sections 22-24 of the Penn Treebank, about 0.5% shy of a state-of-the-art in-domain system (Shen et al., 2007) with more sophisticated supervised learning. We expected that language model representations perform well in part because they provide meaningful features for sparse and polysemous words. from our test data, along with 296 non-polysemous word types, chosen based on POS tags and manual inspection. We further define sparse word types as those that appear 5 times or fewer in all of our unlabeled data, and non-sparse word types as those that appear at least 50 times in our unlabeled data. Table 3 shows results on these subsets of the data. As expected, all of our language models outperform the baseline by a larger margin on polysemous words than on non-polysemous words. The margin between graphical model representations and the ngram model also increases on polysemous words, presumably because the Viterbi decoding of these models takes into account the tokens in the surrounding sentence. The same behavior is evident for sparsity: all of the language model representations outperform the baseline by a larger margin on sparse words than not-sparse words, and all of the graphical models perform better relative to the ngram model on sparse words as well. Thus representations based on graphical models address two key issues in building representations for POS tagging. Information Extraction Experiments In this section, we evaluate our learned representations on a different task that investigates the ability of each representation to capture semantic, rather than syntactic, information. Specifically, we inves-  Existing set-expansion techniques utilize the distributional hypothesis: candidate noun phrases for a given semantic class are ranked based on how similar their contextual distributions are to those of the seeds. Here, we measure how performance on the set-expansion task varies when we employ different representations for the contextual distributions. Methods The set-expansion task we address is formalized as follows: given a corpus, a set of seeds from some semantic category C, and a separate set of candidate phrases P , output a ranking of the phrases in P in decreasing order of likelihood of membership in C. For any given representation R, the set-expansion algorithm we investigate is straightforward: we create a prototypical \"seed representation vector\" equal to the mean of the representation vectors for each of the seeds. Then, we rank candidate phrases in increasing order of the distance between the candidate phrase representation and the seed representation vector. As a measure of distance between representations, we compute the average of five stan-dard distance measures, including KL and Jensen-Shannon divergence, and cosine, Euclidean, and L1 distance. In experiments, we found that improving upon this simple averaging was not easy-in fact, tuning a weighted average of the distance measures for each representation did not improve results significantly on held-out data. Because set expansion is performed at the level of word types rather than tokens, it requires typebased representations. We compare HMM-TYPE-R, NGRAM-R, LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment. We used a 25-state HMM, and the same PL-MRF as in the previous section. Following previous set-expansion experiments with n-grams (Ahuja and Downey, 2010) , we employ a trigram model with Kneser-Ney smoothing for NGRAM-R. For Brown clusters, instead of distance metrics like KL divergence (which assume distributions), we rank extractions by the number of matches between a word's BROWN-TYPE-R features and seed features. Data Sets We utilized a set of approximately 100,000 sentences of Web text, joining multi-word named entities in the corpus into single tokens using the Lex algorithm (Downey et al., 2007a) . This process enables each named entity (the focus of the setexpansion experiments) to be treated as a single token, with a single representation vector for comparison. We developed all word type representations using this corpus. To obtain examples of multiple semantic categories, we utilized selected Wikipedia \"listOf\" pages from (Pantel et al., 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. In all, we had 432 examples across 16 distinct categories such as Countries, Greek Islands, and Police TV Dramas. Results For each semantic category, we tested five different random selections of five seed examples, treating the unselected members of the category as positive examples, and all other candidate phrases as negative examples. We evaluate using the area under the precision-recall curve (AUC) metric. The results are shown in Table 4 . All representations improve performance over a random baseline, equal to the average AUC over five random orderings for each category, and the graphical models outperform the ngram representation. HMM-TYPE-R performs the best overall, and Brown clustering with 1000 clusters is comparable (320 and 100 cluster perform slightly worse). As with POS tagging, we expect that language model representations improve performance on the IE task by providing informative features for sparse word types. However, because the IE task classifies word types rather than tokens, we expect the representations to provide less benefit for polysemous word types. To test these hypotheses, we measured how IE performance changed in sparse or polysemous settings. We identified polysemous categories as those for which fewer than 90% of the category members had the category as a clear dominant sense (estimated manually); other categories were considered non-polysemous. Categories whose members had a median number of occurrences in the corpus less than 30 were deemed sparse, and others non-sparse. IE performance on these subsets of the data are shown in Table 3 . Both graphical model representations outperform the ngram representation more on sparse words, as expected. For polysemy, the picture is mixed: the PL-MRF outperform ngrams on polysemous categories, whereas HMM's performance advantage over n-grams decreases. One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less well than the HMM-TYPE-R, whereas the reverse is true on POS tagging. We suspect that the difference is due to the issue of classifying types vs. tokens. Because of their more complex structure, PL-MRFs tend to depend more on transition parameters than do HMMs. Furthermore, our decision to train the PL-MRFs using contrastive estimation with a neighborhood that swaps consecutive pairs of words also tends to emphasize transition parameters. As a result, we believe the posterior distribution over latent states given a word type is more informative in our HMM model than the PL-MRF model. We measured the entropy of these distributions for the two models, and found that H(P PL-MRF (y|x = w)) = 9.95 bits, compared with H(P HMM (y|x = w)) = 2.74 bits, which supports the hypothesis that the drop in the PL-MRF's performance on IE is due to its dependence on transition parameters. Further experiments are warranted to investigate this issue. Conclusion and Future Work Our investigation into language models as representations shows that graphical models can be used to combat polysemy and, especially, sparsity in representations for weakly-supervised classifiers. Our novel factorial graphical model yields a state-of-theart POS tagger for domain adaptation, and HMMs improve significantly over all other representations in an information extraction task. Important directions for future research include models for handling polysemy in IE, and richer language models that incorporate more linguistic intuitions about how words interact with their contexts. Acknowledgments This research was supported in part by NSF grant IIS-1065397 and a Microsoft New Faculty Fellowship. ",
    "abstract": "Finding the right representation for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce. This paper investigates language model representations, in which language models trained on unlabeled corpora are used to generate real-valued feature vectors for words. We investigate ngram models and probabilistic graphical models, including a novel lattice-structured Markov Random Field. Experiments indicate that language model representations outperform traditional representations, and that graphical model representations outperform ngram models, especially on sparse and polysemous words.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "17",
    "year": "2011",
    "month": "June",
    "title": "Language Models as Representations for Weakly Supervised {NLP} Tasks"
}