{
    "article": "This paper describes our system used in the SemEval-2022 Task5 Multimedia Automatic Misogyny Identification (MAMI). This task is to use the provided text-image pairs to classify emotions. In this paper, We propose a multi-label emotion classification model based on pre-trained LXMERT. We use Faster-RCNN to extract visual representation and utilize LXMERT's cross-attention for multimodal alignment. Then we use the Bilinearinteraction layer to fuse these features. Our experimental results surpass the F 1 score of baseline. For Sub-task A, our F 1 score is 0.662 and Sub-task B's F 1 score is 0.633. The code of this study is available on GitHub 1 . Introduction In social networks, meme is mainly used to express the emotion of netizen. It usually consists of text and images. But at the same time, memes also convey some negative emotions, such as negative comments about women. SemEval-2022 Task5: Multimedia Automatic Misogyny Identification (MAMI) (Fersini et al., 2022) focuses on identifying whether meme conveys negative emotions towards women. \u2022 Sub-task A: a basic task about misogynous meme identification, where a meme should be categorized either as misogynous or not misogynous; \u2022 Sub-task B: an advanced task, where the type of misogyny should be recognized among potential overlapping categories such as stereotype, shaming, objectification, and violence. Since the Transformer (Vaswani et al., 2017 ) and BERT (Devlin et al., 2019) models were proposed, researchers have begun to work on image and text multi-modality work in recent years, in addition to using one modality such as only image or text. Nowadays, for multimodal models, they can be divided into two categories, singlestream model and dual-stream model. In the single-stream model, language information and vision information are fused at the beginning and directly input into the encoder. Some representative single-stream models include ImageBERT (Qi et al., 2020) , Unicoder VL (Li et al., 2020) , VL-BERT (Su et al., 2020) , VisualBERT (Li et al., 2019) , etc. In the dual-stream model, in addition to the LXMERT, we will introduce below, there were ViLBert (Lu et al., 2019) and UNIMO (Li et al., 2021) , etc. As for emotion recognition, in previous tasks, there are also emotion classification tasks based on multi-modal graphics and text, such as Zhu et al. (2021) used text-CNN and ALBERT to Identify the persuasion skills of Meme. Peng et al. (2020) used the adversarial learning of sentiment word representations for sentiment analysis. A tree-structured regional CNN-LSTM (Wang et al., 2020) and dynamic routing in a tree-structured LSTM (Wang et al., 2019) were used for dimensional sentiment analysis. In previous SemEval competitions, Tian et al. (2021) extracted heterogeneous visual representations (i.e., face features, OCR features, and multimodal representations) and explored various multimodal fusion strategies to combine the textual and visual representations. In addition, in multimodal analysis combining images and text, Yuan et al. (2020) proposed a parallel channel ensemble model combining BERT embedding, BiLSTM, attention and CNN, and ResNet for sentiment analysis of memes. The main difficulty of multi-modality is how to extract the two modalities' features and express the semantics more accurately, which involves the representation of multi-modality, the alignment between multi-modality, and the fusion of multi- modality. For the multi-modal task of text and image, the previous practice is to input the text and image into two different pre-training models for processing the text and image modalities respectively, and then concatenate the output features and predict the emotion. However, this method lacks the processing of the alignment relationship between modalities. The proposed model considers the above three problems in the multi-modality field. Inspired by LXMERT, we use it as the main framework of our model. We use Faster R-CNN (Ren et al., 2017) to extract image RoI features and their position. For texts, we use BERT to extract text embedding. Then our system uses LXMERT (Tan and Bansal, 2019) to deal with the multimodal alignment of text and image. After when two modalities are processed by LXMERT, we use the learnable integration mechanism Bilinearinteraction layer to fuse these features. The remainder of this paper is organized as follows. In section 2, we described LXMERT and our fusion method in detail. The experimental results are presented in section 3. Finally, a conclusion is drawn in section 4. System Overview Task A and Task B are very similar in model structure except for the output layer. Therefore, we in-troduce the model we proposed as a whole. This model can be divided into four parts. They are the embedding layer for image and text preprocessing, the encoder for multi-modal presentation and alignment, the feature fusion layer, and the final output layer. The proposed model is as shown in Figure 2 . Embedding For images, LXMERT does not simply use a convolutional neural network to output feature map but uses (Anderson et al., 2018) to extract objects from images. The image processing of LXMERT is similar to text processing inspired by BERT. The specific idea is to use Faster R-CNN to select 36 RoI (region of interest) boxes with high confidence for each image and use these boxes as the features of the image. Similar to the text processing of BERT, the model also considers the position of each box and embeds the corresponding position. 36 objects are extracted by Faster R-CNN as {o 1 , . . . , o 36 }. f j is the 2048 dimension RoI features of o j , and p j is its position. As is shown in figure 2 , the processing of these variables is as follows: where W F and W P are the trainable weights of fully connected layer in matrix format. Moreover, b F and b P are the bias of the layer. fj and pj are the output of the layer-normalization. fj = LayerNorm (W F f j + b F ) pj = LayerNorm (W P p j + b P ) v j = fj + pj /2 (1) For the text, sentences are converted into tokens whose length is equal to the length of scent according to the practice of WordPiece tokenizer (Wu et al., 2016) . For instance, when the length of the sentence is n, the word tokens are {w 1 , ..., w n }. Then word w i and its index i (the absolute position of w i ) are projected to vectors by embedding sublayers.The specific structure of embedder is shown in Figure 1 . Then added to the index-aware word embedding: \u0175i = WordEmbed (w i ) \u00fbi = IdxEmbed (i) h i = LayerNorm ( \u0175i + \u00fbi ) (2) The specific structure of embedder is shown in Figure 1 . Attention layer In this subsection, we will give a brief description of the attention mechanism. The principle of the attention mechanism is to give a request vector x and its context vector y j , then, calculate the correlation between x and each y j , and get a correlation score. The correlation score used in LXMERT is the dot product of vector x and vector y j . After calculating the scores of all relevant context vectors y j for x, LXMERT uses softmax to convert each score into a probability \u03b1 j to obtain the at-tention distribution. a j = score (x, y j ) \u03b1 j = exp (a j ) / k exp (a k ) (3) Att X\u2192Y (x, {y j }) = j \u03b1 j y j (4) The output of the layer is the weighted sum of all probabilities with y i . The self-attention layer in LXMERT is implemented in a similar way to the attention layer, except that the query vector x in self-attention comes from the context-dependent vector y i . Encoder The processing of image modality and text modality is shown in Figure 2 . After embedding two modalities, LXMERT uses the two transformer single-modality encoders. One is a text encoder and another is an image encoder. Each layer in a single-modality encoder contains a self-attention ('Self') sub-layer and a feed-forward ('FF') sublayer, where the feed-forward sub-layer is further composed of two fully-connected sub-layers. We take N L and N R layers in the language encoder and the object-relationship encoder, respectively. We add a residual connection and layer normalization (annotated by the '+' sign in Figure 2 ) after each sub-layer as in Transformer (Vaswani et al., 2017) . The features processed by a singlemodality encoder will be first sent to another encoder called the cross-modality layer. Its main function is to align the features of the two modalities. The bi-directional cross-attention sublayer contains two unidirectional cross-attention sublayers, one from image to text and the other from text to the image. LXMERT stacks them N x times, the input of k-th layer is the output of the previous(k \u2212 1)-th layer. Similarly, the query and context vectors are the outputs of the (k \u2212 1)-th layer. The method of processing the text features h k\u22121 i and the image features v k\u22121 j in unidirectional cross-attention sub-layers is as follows: \u0125k i = Cross AttL\u2192R h k\u22121 i , v k\u22121 1 , . . . , v k\u22121 m vk j = Cross AttR\u2192L v k\u22121 j , h k\u22121 1 , . . . , h k\u22121 n (5) where \u0125k i and vk j are the output of the crossattenton layer. Then LXMERT further inputs the features processed by the cross-modality sublayer to the selfattention sublayer. This method aimed to further construct the internal connection of each modality after alignment. The specific treatment is: hk i = Self AttL\u2192L \u0125k i , \u0125k 1 , . . . , \u0125k n \u1e7dk j = Self AttR\u2192R vk j , vk 1 , . . . , vk m (6) \u0125k i , vk j then processed by self-attention to hk i and \u1e7dk j , which will be further input to an 'FF' sublayer, connected through a residual, and input to the normalization to obtain the final output h k i , v k j . For each text in the data, the model will generate a Pooler output. We use the Pooler of each sentence as the output of the text modality. Fusion The method of this layer is inspired by Sina's paper FiBiNET by Huang et al. (2019) . After LXMERT outputs two modality features, we need to further process its output. The dimension of image features is 36 \u00d7 768, while the dimension of text features is 768. To better integrate the two modalities, we flatten the image features and change its dimension to 768 through a feedforward layer. Then, each modality will be normalized through layer normalization. Then, the features of each modality are sent to the Bilinearinteractive layer. The idea of the Bilinear-interactive layer is as shown in Figure 3 . We establish a k-order square matrix W , which is trainable. To fuse the information of various modalities, 768-dimensional image features will first inner product with W . Then, for text features, we use Hadamard product to multiply the previous matrix. We finally use the dropout layer to improve the generalization ability of the model. Output layer \u2022 Sub-task A: this task is a binary classification task, so in the output layer, we use a shape of 768 \u00d7 1 full connection layer and use sigmoid as the activation function to process the results. \u2022 Sub-task B: this task is a multi-label classification task. Therefore, in the output layer, we use a full connection layer whose shape is 768 \u00d7 5. Since each label classification is equivalent to binary classification, we use sigmoid as the activation function to process the results during output. 3 Experiments and Evaluation Dataset The task organizer provided 10000 pieces of data for training, including meme images with image serial numbers and text descriptions corresponding to the image. In the training dataset, there are 10000 images and an excel table to record the text corresponding to the images and supervise the learning of the corresponding labels. When analyzing the data, we found that different labels account for different proportions in the number of their respective classifications. For the misogynous tag, both 0 and 1 categories account for 50%, so the data sample tag is more balanced for a supervision task. However, for the other four labels such as sharing, the proportion of label 1 is only 12.74%. Among 10000 samples, the label of violence accounts for only 9.53%. Table 1 shows the proportion of each label in the training dataset. As shown in Table 1 , we find that the proportion of labels of different categories is very different, and there is data imbalance. This will make the model have a strong learning effect on a large classification label and easy to classify. However, for the low proportion of classification tags, it is difficult to learn and classify. Based on this, we use Focal loss by (He et al., 2016) as the loss function of our model. Experimental configuration Our model is based on TensorFlow platform version 2.5.0. The main model adopts LXMERT from the Hugging Face transformers toolkit. We first useUNC-NLP/LXMERT-base-uncased tokenizer-Fast to process our text to embeddings, and we also use UNC-NLP/LXMERT-base-uncased pretrained model as our base model LXMERT's pretrained model. The Adam optimizer (Kingma and Ba, 2015) was used to update all trainable parameters. The Hyper-parameters configuration used in the model is shown in Table 2 : We use Faster R-CNN to extract features of images, which is based on the paper by (Anderson et al., 2018) . In this task, we use an open-source docker image airsplay / bottom-up attention and use a Faster R-CNN pretraining model based on ResNet101 to extract 36 RoI feature boxes and their corresponding position. Evaluation Metrics Sub-task A Systems will be evaluated using macro-average F1-score. In particular, for each class label (i.e. misogynous and not misogynous) the corresponding F1-score will be computed, and the final score will be estimated as the arithmetic mean of the two F1-score. Sub-task B Systems will be evaluated using weighted-average F1-score. In particular, the F1-score will be com- F 1 -score = 2 \u00d7 precision \u00d7 recall precision + recall (8) F 1 -score is the harmonic average of recall and precision. Hyperparametric selection In this section, we mainly introduce the hyperparametric selection of focal loss of the model. We adjust the two hyperparameters \u03b3 and \u03b1 of Focal loss and train the model.  the performance of the model. The loss function focal loss is modified based on the standard crossentropy loss. This function can reduce the easy-to-classify samples so that the model can more focus on the samples that are difficult to classify in training. p t in cross-entropy loss function reflects the recognition ability of the model to this sample (i.e. how well the knowledge is mastered). We define p t is: p t = p if y = 1 1 \u2212 p otherwise (9) The smaller the p t is, the more difficult it is to classify, so contribution should be improved to the loss function when calculating the loss. Therefore, the specific method of Focal loss is to multiply a weight with p t before the entropy loss function. \u03b1 is balancing factor,\u03b1 \u2208 [0, 1], \u03b3 is modulating factor, \u03b3 \u2208 [0, 5]. The Focal loss is as: F ocal_loss(p t ) = \u2212\u03b1(1 \u2212 p t ) \u03b3 log(pt) (10) Thus, when \u03b1 = 1, gamma = 0, focal loss is similar to the cross-entropy loss function. By changing the values of \u03b3 and \u03b1, we found that when \u03b1 = 0.25 and \u03b3 = 3, for sub-task B, the weighted F 1 score of our model reached 0.662 and 0.633. See Figure 4 . Model comparison We compare our model to a baseline and a model that combines two pre-trained models based on ELECTRA (Clark et al., 2020) and ResNet-101 (Ren et al., 2017) in this section. ELECTRA deals with text modality and ResNet is used to deal with image modality. The methods of feature fusion are compared with the Bilinear-interactive layer and concatenate layer using direct concatenate. The specific task is based on sub-task B. See Table 3 for details. Conclusion In this task, we design an image and text multi-modality model based on LXMERT for multi-modality representation and alignment, and modality fusion based on the Bilinear-interaction layer. Compared with the traditional method of stitching two pre-training models for each modality then concatenating two features to predict emotion, this model considers the representation, alignment, and fusion of multi-modality, and achieves better results than the baseline method. At the same time, we found that after adding the Bilinear-interaction layer, the performance of the model is better than using only feature concatenate. See Table 3 . Meanwhile, when analyzing the data, we found that the background of the meme graph and some characters in the graph were not used as the target input model by Faster R-CNN, which may affect the accuracy of the model. Meanwhile, the size of the meme image is too small to include multiple targets, and the target is relatively single, which may affect the performance of the model. Acknowledgement This work was supported by the National Natural Science Foundation of China (NSFC) under Grants No. 61966038. The authors would like to thank the anonymous reviewers for their constructive comments.",
    "abstract": "This paper describes our system used in the SemEval-2022 Task5 Multimedia Automatic Misogyny Identification (MAMI). This task is to use the provided text-image pairs to classify emotions. In this paper, We propose a multi-label emotion classification model based on pre-trained LXMERT. We use Faster-RCNN to extract visual representation and utilize LXMERT's cross-attention for multimodal alignment. Then we use the Bilinearinteraction layer to fuse these features. Our experimental results surpass the F 1 score of baseline. For Sub-task A, our F 1 score is 0.662 and Sub-task B's F 1 score is 0.633. The code of this study is available on GitHub 1 .",
    "countries": [
        "China"
    ],
    "languages": [],
    "numcitedby": "1",
    "year": "2022",
    "month": "July",
    "title": "{YNU}-{HPCC} at {S}em{E}val-2022 Task 5: Multi-Modal and Multi-label Emotion Classification Based on {LXMERT}"
}