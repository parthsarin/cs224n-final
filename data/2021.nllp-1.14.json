{
    "article": "Automated Compliance Checking (ACC) systems aim to semantically parse building regulations to a set of rules. However, semantic parsing is known to be hard and requires large amounts of training data. The complexity of creating such training data has led to research that focuses on small sub-tasks, such as shallow parsing or the extraction of a limited subset of rules. This study introduces a shallow parsing task for which training data is relatively cheap to create, with the aim of learning a lexicon for ACC. We annotate a small domain-specific dataset of 200 sentences, SPAR.txt 1 , and train a sequence tagger that achieves 79,93 F1-score on the test set. We then show through manual evaluation that the model identifies most (89,84%) defined terms in a set of building regulation documents, and that both contiguous and discontiguous Multi-Word Expressions (MWE) are discovered with reasonable accuracy (70,3%). Introduction Non-compliance with building regulations has been linked to fatal incidents (Cook, 2017) . However, ensuring that a building complies with regulations is complicated and time-consuming because: \u2022 Regulations contain ambiguous and sometimes conflicting criteria (Cook, 2017; Hywel et al., 2020) . \u2022 Regulations change and are distributed over many documents (Fuchs, 2021) , e.g., over 800 documents in the U.K. with many of them behind a paywall. \u2022 Criteria often refer to entire sections in other documents, e.g., \"The emergency lighting should be installed in accordance with BS 5266: Part 1: 2016 as read in association 1 For the SCOTREG corpus, SPAR.txt dataset and code see: https://github.com/rubenkruiper/SPaR.txt with BS 5266: Part 7: 1999 (BS EN: 1838 : 2013) .\" (Scottish Government, 2020) . \u2022 Regulations differ per country, and some criteria borrowed from international regulations are not suited to the specific environment (Moon et al., 2019) . Automated Compliance Checking (ACC) could reduce the difficulty, time, costs and number of human errors made during compliance checking (Dimyadi and Amor, 2013; Preidel and Borrmann, 2018) , as well as ease customisation and innovation in the building sector (Niemeijer et al., 2014) . There exist two branches of ACC research. One focuses on tools that reason over a rule-base -often consisting of hard-coded rules (Pauwels et al., 2017; Solihin et al., 2019) . The other branch attempts semantically parse the Natural Language regulations into rules that enable reasoning -a complex task that is of interest to the wider legal domain (Wyner et al., 2012) . This study presents a novel shallow parsing task, for which the creation of training data is cheap, and an accompanying small dataset of 200 sentences. The aim is to learn a semantic lexicon for ACC, which is often an important first step for semantic parsing because it enables the grounding of information units identified in a text -such as objects, interactions and constraints (Zettlemoyer and Michael, 2005; Kollar et al., 2010; Chen, 2012) . Section 2 motivates our task and describes related work with a focus on parsing building regulations. 3 describes the task, as well as the collection of a small annotated dataset -SPAR.txt -for the task of discovering and identifying domain-specific terms, including Multi-Word Expressionss (MWE). In 4 we describe and train a sequence tagging model, which generalises well to unseen text within the same domain. 5 describes the evaluation of outputs, specifically with regards to the objects identified in a corpus of 13K sentences derived from the Scot-tish Building Regulations (Scottish Government, 2020) . Our model finds 89,84% of the terms explicitly defined in these documents. Furthermore, a significant proportion (70,3%) of these predictions matches exactly what a human annotator would consider to be an object in a given sentence context. We argue that our new task can provide a cheap approach to lexicon learning that could benefit (1) Information Extraction (IE) in support of ACC, (2) Information Retrieval (IR) in support of manual compliance checking, and (3) the mapping of unstructured text to a structured representation. 2 Related work 2.1 Semantic parsing for ACC Semantic parsing revolves around learning the meaning of Natural Language and converting it to an executable logical form, which is a hard and unsolved task (Mooney, 2007; Artzi and Zettlemoyer, 2013) . The fragmented structure of legal texts further complicates semantic parsing in legal domains (Lawsky, 2017), e.g., a clause may state that some object must comply with all of the regulations in some other section or document. Therefore, statutory reasoning requires defeasible logic representations that allow conclusions to be defeated on the basis of subsequent information (Pertierra et al., 2017) . Due to the complexity of the task, existing approaches to semantic parsing in the ACC domain often limit their scope to parsing quantitative requirements in some small sub-domain (Fuchs, 2021; Moon et al., 2021) . In the ACC domain, studies on semantic parsing rely on traditional Rule-Based Systems to extract concepts and construct logical statements, e.g., (Zhang and El-Gohary, 2016b,a; Zhou and El-Gohary, 2019; Xu and Cai, 2019) . Empirical approaches may be able to handle the combinatorial explosion caused by ambiguity in Natural Language (Wyner et al., 2012) . But training data for such systems is difficult and costly to collect (Chen, 2012; Herzig and Berant, 2017) . Unsurprisingly, Machine Learning studies in the ACC domain focus on simpler sub-tasks of semantic parsing for which training data is relatively easy to collect, such as Named Entity Recognition (Liu and El-Gohary, 2017; Moon et al., 2021) and Relation Classification (Zhong et al., 2020) . This study follows a similar strategy to limit the complexity of annotation, see 3.2 and 3.3. Shallow parsing for ACC Orthogonal research on shallow parsing in the ACC domain includes (1) the decomposition of complex sentences into parts that are easier to process (Zhang and El-Gohary, 2019) , and (2) more-or-less idiosyncratic semantic markup schemes that help identify requirements and their components in text, e.g., (Hjelseth and Nisbet, 2011) and (Zhang and El-Gohary, 2016b) . Efforts to automate such shallow parsing approaches encountered performance issues when handling MWEs (Zhang and El-Gohary, 2019; Zhang and Nora, 2020) . Although the proper handling of MWEs is a key issue in Natural Language Processing (Siskind, 1996; Sag et al., 2002; Ramisch et al., 2018) , extracting MWEs is especially relevant to IE in domains rich in technical terms (Baldwin and Kim, 2010) -such as the building regulations. Processing MWEs is a general requirement for ACC, because both single and multi-word concepts mentioned in regulations have to be aligned with components and values found in Building Information Model (BIM) models. Such BIM models rely on standards, such as the Industry Foundation Classes (IFC) data model, to facilitate amongst others compliance checking and information exchange between applications and potentially international stake-holders (Plume and Mitchell, 2007; Beetz et al., 2009; Pauwels et al., 2017) . This study aims to automatically learn a vocabulary for ACC, which entails MWE processing. Processing MWEs Processing MWE can be divided into two sub-tasks: MWE discovery and MWE identification (Constant et al., 2017) . MWE discovery aims to find new types of MWEs in text corpora and storing them in a lexicon. Unsupervised approaches are used that rely on properties of MWEs that set them apart from random combinations of words, such as word collocation frequency (Manning et al., 2002; Pecina and Schlesinger, 2006) , non-substitutability of component words (Lapata and Lascarides, 2003; Constant et al., 2017) , and non-compositionality (Frege, 1996; Riedl and Biemann, 2015) . The latter applies mostly to idiomatic expressions (Villavicencio and Idiart, 2019) , such as 'cloud nine'. MWE identification revolves around annotating MWEs in a corpus, based on a lexicon or on results from MWE discovery (Constant et al., 2017) . This enables representing MWEs as single tokens, which has been shown to improve accuracy of NLP tasks (Green et al., 2011) , such as dependency parsing (Nivre and Nilsson, 2004) . Supervised approaches are used, amongst which sequence tagging has been found to work well (Constant et al., 2017) , e.g., (Blunsom and Baldwin, 2006; Constant et al., 2012) . Sequence tagging has also been used for joint MWE identification and Part-of-Speech (POS) tagging (Constant and Sigogne, 2011) and may be amended to handle discontiguous MWEs (Schneider et al., 2014) . This study explores sequence tagging for joint MWE processing. However, this study does not aim to handle idiomatic expressions or proverbs. Beyond research on MWE processing, a related task that focuses on identifying technical terms and Named Entities is concept mining, e.g., (Rajagopal et al., 2013; Poria et al., 2014) . In contrast to these concept mining studies, we do not rely on dependency parses or external resources, such as ConceptNet (Speer et al., 2013) . MWE tagging for ACC 3.1 A building regulations corpus Many building regulations are captured in formats that are not easily processed by computers, such as PDF (Fuchs, 2021) . The Scottish Building Regulations (Scottish Government, 2020) are an exception and are openly available online. We scrape the domestic and non-domestic regulations, including text found in lists, side-notes, tables and captions. We use TextBlob 2 for word-level tokenization (Penn Treebank Tokenizer) and sentence splitting (PunktSentenceTokenizer (Kiss and Strunk, 2006) ). We will refer to the resulting corpus as SCOTREG. An overview of the size of SCOTREG in terms of sentences and tokens can be found in Table 1 , as well as the number the terms defined in the 'Terms and definitions' sections. Throughout SCOTREG defined terms provide a hyperlink to the definitions section. This enables us to count how often defined terms occur. Note these defined terms represent classes, they are expressed in various ways throughout the texts and some of the variations do not match any of the terms verbatim -even after lower-casing and lemmatizing. Problem statement The IFC schema does not comprehensively cover the terminology used in the building regulations. In \"A roof covering or roof light which forms part of an internal ceiling lining should [...]\" 3 the term 'roof light' would fall under the more generic IFC class 'window'. The problem is compounded as building regulations cover a wide range topics (and thus concepts) -from design and construction, including fire regulations and accessibility, to facility management, renovation and demolition (Pauwels et al., 2017) . Despite the more fine-grained terminology used in the SCOTREG corpus, only 128 terms are defined -neither 'roof light', 'roof covering' nor 'internal ceiling lining' are defined. Task description The task in this study is to identify low-level constituent parts of a sentence, which we will refer to as spans. We assume that contextual interdependencies between single-word and multi-word spans help tackle lexical sparsity of MWEs. Therefore, we tag both single words and groups of words in a sentence and we tag all tokens exhaustively, including punctuation. Whether spans comprise a single punctuation mark or a group of possibly discontiguous tokens, they should represent a coherent unit of grammatical meaning. 'The Building (Scotland) Act 2003' is a Named Entity that should be treated as a single span. But a regular noun-chunking approach would break on the parentheses and the cardinal number -it may even treat every capitalised word as a proper noun. \u2022 \"A roof covering or roof light which forms part of an internal ceiling lining should [...]\" A constituency parser may break the term 'roof covering' into separate words, because a POS tagger would typically assign 'covering' the tag VBG. \u2022 \"[...], a paved (or equivalent) footpath at least 900mm wide [...]\" In this case, while we would like to split 'a paved (or equivalent) footpath' into the spans: 'a paved footpath', '(', ')', 'or' and 'equivalent'. In a downstream task this would allow us to define a class for 'paved footpath', and reason over the equivalent types of footpaths. However, to extract 'a paved footpath' from the sentence above, a discontiguous span representation is required. Simplifying assumptions For our task we rely on a simplified definition of MWEs: \"possibly discontiguous combinations of at least two tokens, where tokens are separated by white-spaces or punctuation in text\" -similar to (Villavicencio and Idiart, 2019) . We also make two assumptions on the types of MWEs that we expect to find in the building regulation texts. First, we assume that the building regulations contain few to no idiomatic expressions, because these may introduce ambiguity. We justify this assumption as the building regulations use a slightly more formal syntax, albeit not as strict as other types of legal text (Chalkidis et al., 2020) . Second, we expect a relatively low variability in surface forms. Both for verbal expressions found in clauses, e.g., 'X should conform to Y, and for the surface forms of MWEs that indicate technical terms, e.g., 'insulation envelope' and 'selfcontained emergency luminaries'. A low variability in surface forms would be reflected by a relatively small vocabulary -which is thought to ease the complexity of various NLP tasks (Church, 2013) . As can be seen in Figure 1 , SCOTREG has in the order of 10K unique tokens for a total of 283K. In comparison to the more heterogeneous Brown corpus (Francis and Kucera, 1964) , which has more than 23K unique tokens for the first 283K tokens, SCOTREG indeed has a small vocabulary. Annotating SPAR.txt A domain expert annotated a random selection of 200 sentences in BRAT (Stenetorp et al., 2012) . Our assumption is that such a small dataset should suffice for achieving reasonable results on the proposed parsing task. Figure 2 exemplifies how annotations can span single words, multiple words and also indicate that two groups of words belong to a single, discontiguous span. To distinguish between verb-based and noun-based spans, as well as spans that belong to neither of these classes, we annotate the following span types: \u2022 OBJECT spans indicate either real-world objects or distinguishable concepts. They include proper nouns, compounds, multi-word terms, and multi-word Named Entities, such as 'the Target Emissions Rating', 'offensive fire-fighting' and 'BS 8000-15: 1990'. We include determiners as part of the OBJECT span during annotation, see Figure 2 . \u2022 ACTION spans may help identify whether a sentence expresses a requirement, similar to (Hjelseth and Nisbet, 2011) . We include verbs, support verbs, prepositional verbs and verbparticle constructions, e.g., 'should be maintained' and 'takes account of ', but we expect to split light-verb constructions, such as 'to take a shower' (Constant et al., 2017) into 'to take' and the OBJECT span 'a shower'. During training the sentence is tokenized and the aim is to predict the correct tags for each token, see the tagging scheme described in Section 4.1. The identifier for this sentence in the dataset is 'd_2.14.4_i3_s_0'. \u2022 FUNCTIONAL spans are modifiers that are not inherently part of an OBJECT or ACTION span. They include adverbs and adjectives, e.g., 'main' and 'principal' in 'the main or principal bedroom', as well as complex function words, e.g., 'up to'. \u2022 DISCOURSE spans include punctuation, coreference anaphora, conjunctions and disjunctions, e.g., ',', 'or' and 'this'. MWEs can blur the lines between syntax and semantics (Green et al., 2011) , with ambiguous cases leading to annotation inconsistencies (Hollenstein et al., 2016) . Because the task involves annotating both the words that make up an MWE and those surrounding it, the annotator is forced to come up with more-or-less consistent decisions. As a tool to determine which words belong together, annotators are asked to rely on standard constituency tests: \u2022 Substitution test -if you can substitute a part of a sentence with another word or group of words that belong to the same type, the part is a constituent. \u2022 Pronoun test -if you can replace a part of a sentence with 'it', the part is a constituent. \u2022 Question by repetition test -if you can repeat a part of a sentence, within a valid question, then it is a constituent. Nevertheless, annotating MWEs often requires domain-specific knowledge and remains ambiguous. Using Figure 2 as an example, one might argue that the preposition 'from' could be part of the AC-TION span 'should be entered' -considering that 'from' converts 'to enter' from an intransitive to a transitive verb. We define several loose guidelines to warrant further consistency in annotation: 1. Punctuation Unless punctuation should be part of an OBJECT span, such as the colon in the document name 'BS 800-15:1990', then punctuation should be marked as DISCOURSE. 3 shows how a list of 'Standards' share a word that is crucial to the semantics of the individual items. Similarly, conjunctions and disjunctions sometimes share a determiner, e.g., 'the size and orientation of the windows' would include the spans 'the size' and 'the orientation'. Negation The motivation is to help downstream tasks determine that both 'size' and 'orientation', here, are properties of 'the window'. Overlap We currently limit the annotation of discontiguous spans to a maximum of two parts. Therefore, overlapping spans should only occur this cannot be avoided, e.g., 'the Silver level' and 'the Gold level' in 'the Silver and Gold level' would need to exist of three single-word spans to avoid overlap. 6. Adverbs typically express a manner, place, time, frequency, degree, and so on. The expectation is that such expressions will usually be labelled as FUNCTIONAL, e.g., 'only' in Figure 2 . 7. Adjectives typically modify nouns and may or may not be part of an OBJECT span. This decision would be based on whether the modified noun is likely to constitute a separate category or not. As an example, 'structural' would be part of the OBJECT span 'the structural properties' but a separate FUNCTIONAL span in 'matters of structural concern'. 8. Quantities and units are treated as a single OBJECT span. If such a span modifies a noun, e.g., '900mm wide', then these would usually form two separate spans '900mm' and 'wide'. A second domain expert annotated 140 out of 200 sentences. The inter-annotator agreement was found to be Cohen k=0,79. We randomly divide the gold annotations into a 60/20/20 (%) split for train, development and test respectively -see Table 6 in Appendix A. Training Representing discontiguous spans Regular tagging schemes, such as BIO and BI-OUL, are unable to represent discontiguous spans (Schneider et al., 2014) . We adopt a tagging approach that can handle discontiguous pairs of spans, similar to (Muis and Lu, 2016) . Specifically, we use BH to indicate the beginning (head) of a span and IH to indicate subsequent tokens that belong to this span. If a second span exists that is part of a discontiguous MWE, then the beginning (discontiguous) of this second span is tagged BD, and subsequent tokens of that span are tagged ID. Tags are provided with a type to distinguish between OBJECT and ACTION spans etc. Figure 2 provides example tags for an annotated sentence. Notably, we assume that between the head and discontiguous spans of a given type, there exist no head-spans of the same type. Model We adopt a sequence tagging approach that has been shown to work well (Huang et al., 2015) , where an embedded text sequence is encoded by a bidirectional Long Short-Term Memory network where each of the items shares the word 'Standards'. Note that only part of the sentence is shown here, the identifier for this sentence in the dataset is 'd_0.12.2_i3_#1_s_0'. (Hochreiter and Schmidhuber, 1997 ) and a Conditional Random Field (Lafferty et al., 1999) model is used to predict a tag for each token in the sequence. We modify the implementation found in AllenNLP 4 (Gardner et al., 2018) . We embed text using pre-trained BERT embeddings (bert-basecased) (Devlin et al., 2018) and rely on a single bi-LSTM layer (hidden dim. 384). We add a selfattention layer that captures relative positional information (Shaw et al., 2018) , following (Huang et al., 2019) . The encoded and attended representations are concatenated and projected through two linear layers (hidden dim. 60) before being passed to the CRF model. Table 2 provides an overview of results. We compare the use of BERT (Devlin et al., 2018) and SPANBERT (SpanBERT/spanbert-basecased) (Joshi et al., 2020) for tokenization and embedding. Our intuition is that the masked span Language Modelling task may help capture MWE properties used for MWE discovery, see 2.3. But in our experiments SPANBERT embeddings consistently perform worse, see Table 2 . This may be due to a mismatch between the span types and sizes that SPANBERT was originally trained on -in SPAN-BERT all masked spans are contiguous -and the ones found in our training dataset. A more general issue is the imbalance of tag-types, see Table 3 for results on the test set per tag type. Evaluation Evaluating discovered lexical entries can be tricky (Constant et al., 2017) . We limit our evaluation to the processing of OBJECT spans. First, we perform an extrinsic evaluation by comparing our model output against the defined terms in the SCOTREG   corpus. Second, post hoc human judgement provides insight in the number of correctly identified OBJECT spans given the sentence context. Predicting defined terms We use the model trained on SPAR.txt to predict tags for all sentences in the SCOTREG corpus. We then compare whether each of the 128 defined terms is identified at least once by our model. A total of 115 (89,84%) defined terms were found. Table 4 lists the 13 defined terms (10,16%) that were not found. Most of these defined terms exist verbatim in the SCOTREG corpus, but our model splits these spans into multiple parts. However: \u2022 'average flush' only occurs as 'average flush volume' and our model treats 'average' as a separate FUNCTIONAL span. \u2022 'High-speed ready in-building physical infrastructure' never occurs verbatim in the text. Post hoc human judgement We collect all contiguous and discontiguous OB-JECT spans that our model predicted on the 13K sentences of the SCOTREG corpus. A total of 16,428 unique potential OBJECT spans are identified. We find that this number decreases slightly, to 15,662, if we remove determiners and lower-case the text. We randomly select 165 out of the 16,5K OBJECT spans, and then one of the sentences in which this object occurs -this sample size provides a 99% confidence level with a 10% margin of error. We exclude objects that match any of the defined terms and exclude sentences that are part of the annotated dataset. We use Doccano (Nakayama et al., 2018) for annotation. Each of these 165 samples is presented to the annotators as a combination of the OBJECT span and the corresponding sentence context. The task is to annotate whether the predicted OBJECT span is actually an object in the sentence, with a choice between the labels: (1) exact match, (2) partial match, or (3) not an object. Two domain experts annotated the 165 OBJECT spans, see Table 5 . The inter-annotator agreement was found to be Cohen k=0,79. If we take the average of their judgement, this comes down to 116 (70,3%) exact matches, 40 (23,6%) partial matches, and 9 (5,5%) non-objects. Examples of each labelled object and relevant parts of the sentence context include:  \u2022 Not an object: 'sleeping' in \"Rooms intended for sleeping should be [...]\" -should be an ACTION span here. \u2022 Not an object: 'land subject' in \"[...] development may be given approval on land subject to [...]\" -the OBJECT 'land' is modified by 'subject to'. \u2022 Not an object: 'changes 1' in \"Schedule 1changes to building types 1 and 20.\" -may be the result of overfitting on certain discontiguous patterns. Discussion Despite the small size of SPAR.txt, the trained model discovers a large number (16K) of OBJECT spans in 13K sentences. These spans cover most terms that are explicitly defined in the SCOTREG corpus (89,84%). The defined terms that the model did not identify are expressed in patterns that were never seen during training, although some do not occur verbatim in the texts. A significant proportion (70,3%) of identified OBJECT spans exactly match human judgement. Because annotation is cheap for our task, it is straightforward to create additional gold training samples and improve performance. To this end, partial matches can help identify phenomena that were not seen during training, e.g., 'Articles 15 & 16' and 'subject to' as listed above. False positive OBJECT spans provide insight in phenomena that the model currently overfits on, and may potentially help balance future iterations of our dataset. Moreover, the predicted outputs for the SCOTREG corpus are valuable to the creation of a lexicon for ACC. Conclusions Regulatory documents are an important part of the legal framework, with research on ACC methods focusing on the grand goal of semantic parsing. This study introduces a much simpler parsing task that requires few training examples, with the additional benefit that the collection of a dataset is cheap. We presented the small SPAR.txt dataset and trained a sequence tagger that can process single-word and multi-word spans. We showed that the OBJECT spans identified in the SCOTREG corpus cover most of the existing, limited set of defined terms. Moreover, the model achieves reasonable accuracy when it comes to discovering OBJECT spans, regardless of whether these are discontiguous or not. The annotation of gold training data for the presented approach is cheap, because the annotation task is simple. But the results can benefit the research on ACC, e.g., the output of our task may support more complicated semantic annotation tasks, IE and IR, as well as the development of a domainspecific lexicon. Future work will focus on clus-tering the identified spans to develop a semantic lexicon, balancing and growing the dataset, as well as using predicted outputs for IR in support of manual ACC. Finally, we will explore how well the presented approach performs in other domains with similar text characteristics. A Overview of spans and tags for SPAR.txt and SCOTREG predictions Acknowledgements This research is funded by the IC3 (International Centre for Connected Construction) of Northumbria University. The authors are grateful to Julian Vincent for his thoughts on the annotation process. We would also like to congratulate Dr. Ben Trevett for passing his viva. B Overview of examples Sect. Full sentence Source 3.2 A roof covering or roof light which forms part of an internal ceiling lining should also follow the guidance to Standard 2.5 Internal linings. 2.8.0 Introduction Domestic 3.3 The Building (Scotland) Act 2003 gives Scottish Ministers the power to make building regulations to: Introduction Domestic In order to allow unobstructed access to a domestic building for fire and rescue service personnel, a paved (or equivalent) footpath at least 900mm wide (see also Section 4 Safety) should be provided to the normal entrances, of a building. 2.12.4 Access for fire and rescue service personnel Domestic 3.4 Section 6 Energy, indicates that less demanding U-values can be adopted for the insulation envelope of certain types of limited life buildings, other than dwellings and residential buildings. Explanation Domestic In conversions for example, it may be easier to install self-contained emergency luminaries than to install a protected circuit to the existing lighting system 2.10. d_2.5.7_i5_#1_s_0 Domestic The guidance in this clause takes account of the audibility levels in adjoining rooms and the effect of smoke travelling along a ceiling. d_2.11.7_i0_s_0 Domestic The guidance in this clause takes account of the audibility levels in adjoining rooms and the effect of smoke travelling along a ceiling. d_2.11.7_i0_s_0 Domestic CO2 monitoring equipment should be provided in the apartment expected to be the main or principal bedroom in a dwelling where infiltrating air rates are less than 15m 3 /hr/m 2 @ 50 Pa. d_3.14.2_i1_s_0 Domestic Non-domestic use within dwellings -accommodation up to 50m 2 used by an occupant of a dwelling in their professional or business capacity should be considered as a part of the dwelling. d_6.9.1_i3_s_0 Domestic In the measurement of height or depth from ground which is not level the height or depth shall be taken to be the mean height or depth, except that: for the purpose of types 1, 2, 3, 4, 5, 18 or 19 of schedule 3, and for any other purpose where the difference in level is more than 2.5m the height or depth shall be taken to be the greatest height or depth. d_0.7.2_i1_#4_s_0 Domestic Other pipes should be capped at both ends and at any point of connection, to ensure rats cannot gain entry. n_3.5.5_i1_s_1 Nondomestic In order to allow unobstructed access to a domestic building for fire and rescue service personnel, a paved (or equivalent) footpath at least 900mm wide (see also Section 4 Safety) should be provided to the normal entrances, of a building. d_2.12.4_i1_s_0 Domestic The layout of a dwelling, the size and orientation of the windows, the thermal mass, level of insulation, airtightness, and ventilation can have a significant affect on the demand for heat. Materials that are susceptible to changes in their properties may be used in building work and will meet the requirements of the regulations if the residual properties, including the structural properties: d_0.8.7_i1_s_0 Domestic The collation and dissemination of information relating to matters of structural concern is a vital element of achieving safe structures. d_1.0.1_i2_s_2 Domestic 5.2 Positive input systems -mechanical input air ventilation systems have been successfully installed in existing dwellingswith the objective of overcoming problems of surface condensation and mould growth. Mechanical ventilation and systems Domestic This is to give occupants and staff the earliest possible warning of an outbreak of fire and allow time for assisting occupants in an emergency to evacuate the building or for horizontal progressive evacuation initially to an adjacent sub-compartment which leads to a compartment exit. Hospitals Nondomestic These 'bridges' commonly occur around openings such as lintels, jambs and sills and at wall/roof junctions, wall/floor junctions and where internal walls penetrate the outer fabric. Surface condensation -thermal bridging Nondomestic Normally the control and indicating equipment operates a fire alarm system and it may perform other signalling or control functions as well. Nondomestic Rooms intended for sleeping should be separated by a door that will act as a sound barrier and reduce noise transference. Doors in internal walls Nondomestic Pressure for land development may mean that development may be given planning approval on land subject to some risk of flooding. Introduction Domestic \u2022 Schedule 1 -changes to building types 1 and 20. 0.2.1 Explanation of Regulation 1 Nondomestic Table 7 : Complete overview of the examples used in the paper and their location in SCOTREG. Examples that occur in SPAR.txt have an identifier, such as n_6.C257_i0_s_0. These identifiers are created as follows: domestic (d) or non-domestic (n), the section (6.C2) -in this case concatenated with an integer (57) that is incremented each time a section name is reused, the item index (0 for first item) that indicates which item in the list of this section, and the sentence number (s_0 for first sentence).'#' in the identifier means that the sentence occurs in a list or table, with lists and tables 0-indexed for each section.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 1.9361263126072004e-07,
        "none": 0.9999974223173222
    },
    "reasoning": "Reasoning: The research is funded by the IC3 (International Centre for Connected Construction) of Northumbria University. IC3 is not described as a branch of the armed forces, a for-profit company, a government-funded research agency, or a non-profit foundation. It appears to be associated with a university, which does not fit neatly into any of the provided categories. Therefore, none of the specified funding sources directly apply based on the information given."
}