{
    "article": "\\Ve describe and evaluate experimentally a method to parse a tagged corpus without grammar modeling a natural language on context-free language. This method is based on the following three hypotheses. 1) Part-of-speech sequences on the right-hand side of a rewriting rule are less constrained as to what part-of-speech precedes and follows them than non-constituent sequences. 2) Part-of-speech sequences directly derived from the same non-terminal symbol have similar environments. 3) The most suitable set of rewriting rules makes the greatest reduction of the corpus size. Based on these hypotheses, the system finds a set of constituent-like part-of-speech sequences and replaces them with a new symbol. The repetition of these processes brings us a set of rewriting rules, a grammar , and the bracketed corpus. Introduction A standard approach to a natural language analy_sis is to characterize it with a set of rules, a grammar. Given the difficulty in developing a grammar manually, it is necessary to build a method for automatic grammar induction . One of the most promising results_ of grammar inference is based on the inside-outside algorithm, which can be used to train a stochastic context-free grammar. It is an extension of the forward-backward algorithm. [Pereira and Schabes, 1992] and [Schabes et al., 1993] proposed a method to infer the parameters of a stochastic context-free grammar from a partially parsed corpus and evaluated the results. [Brill, 1993] describes another technique for grammar induction: \"the system learns a set of ordered transformations and applies it to a new sentence.\" These two methods pr_ ofit from corpora annotated with syntactic structure, the Penn Tree bank [Marcus and Santorini, 1993] [Church , 1988] , applied to build the Penn Treebank, is reported a5 \"95-99% correct, depending on the definition of correct\" and another tagger developed by [Brill, 1992] . Corpora in the Penn Treebank have two sorts of additional data: the part-of-speech of each word and the syntactic structure of each sentence. The fi rst stage of building a corpus is an au tomatic tagging or parsing and the second the manual correc tion of errors. Since the accuracy of current parsers is not satisfactory, the manual correction of parsing results is an arduous task. As for tagging, however, the method of marks almost the same accuracy. It follows that it is worth trying to induce a grammar from corpora without syntactic structure , that is to say, to parse corpora using only part-of-speech information . Only a few attempts, however, have been made so far at grammar induction from an unbracketed corpus. [Magerman and Marcus, 1990] proposes a parsing system using mutual information statistics and a manually written \"distituent grammar,\" a list of tag pairs which cannot be adjacent within a constituent , such as (prep noun) . -[Brill and Marcus, 1992] also proposes a technique l 71 for grammar induction. The operations of extracting a grammar from a tagged corpus and applying it to the very same corpus is equal to the process of parsing the corpus without a grammar. In this paper, we propose a new method to parse a tagged corpus without a grammar based on the following Three Hypotheses In the theory of formal language, the rewriting rules determine whether a sequence of alphabets is a sentential form or not. Therefore the language, a set of sentential forms, reflects the characteristics of the rewriting rules. Since our method models natural language on context free language, a corpus is regarded as a set of sentential forms. It follows that a corpus r. eflects the characteristics of the rewriting rules of the natural language. In the following parts of this section , we present an example of the corpus we used, define some symbols for explanation and discuss the three hypotheses on the relation between a corpus and the rewriting rules derived from this point of view. Corpus and Symbol Definitions For our main experiment, we used part-of-speech (POS) sequences from the Wall Street Journal (WSJ) in the Penn Treebank. Table 1 shows its POS tagset and Figure 1 is an example Nekoosa would n't be a diversificat ion . NNP MD RB VB DT NN ( ( (N ekoosa ) would n't ( be ( a diversification ) ) ) . ) . Our method models natural language on context-free language, which is described by a grammar G = (N, T, P, S), where l\\ r is the set of non-terminal symbols, T is the set of terminal symbols, P is the set of rewriting rules and S is the start symbol. Since our method regards the Figure 1: An example sentence in WSJ \u2022 corpus as a set of POS sequences , the set of terminal symbols T is equal to the part-of-speech tagset of the Penn Treebank. Non-terminal symbols are, however, introduced by the system and they aren 't elements of the syntactic tagset of the Penn Treebank. For the subsequent explanation , we make the follO\\ving definitions: pos E T, syn E N, tag E N u T syn EN + , pas ET + , tag E (N uT) + where x + = x--{c} generally. Constituent-like Part-of-speech Sequence The first step of this method is to extract se9uences of one ore more POS from the corpus to form the right-hand side of rewriting rules. The following is the first hypothesis according to which the system extracts constituent-like POS sequences. \u2022 POS sequences on the right-hand side of a rewriting rule are less constrained as to what POS precedes and follows them than non-constituent sequences. To explain this hypothesis concretely, let us consider the following two different POS sequences: pos a which appears on the right-hand side of a rewriting rule and pos x = pos x 1 I ) _ P(pos \u2022 posi) _ f(pos \u2022 posi) p pos pos , pos -P(pos) f (pos) Therefore, n-gram statistics, the frequencies of all the symbol sequences appearing in the corpus, are applicable to compute the entropies. To calculate n-gram statistics for arbitrary n, we adopted the algorithm proposed in [Nagao and Mori, 1994] . Notice that n is more than one and does not exceed the length of the longest sentence in the corpus, because n-grams containing a symbol for final punctuation mark never appear in rewriting rules, except for ones with Son the left-hand side. 2 shows 20 of them in order of frequency. In addition to entropy, we propose another measure of constituency : the ratio of delimiters which precede or follow the POS sequence in question. In our method the delimiters are sentence-final punctuation mark and comma. To extract constituent-like POS sequences from the corpus, we set threshold values for the entropies (H m in) and for the delimiter ratios (Pd Similarity between Constituents The second step is to cluster the POS sequences extracted in the first step. The following is \u2022 \ufffd \u2022 \u2022 I. our second hypothesis, which gives a measure of this clustering. \u2022 POS sequences directly derived from the same non-terminal symbol have similar environ ments. To explain the background of this hypothesis, let us consider the case in which the Produce a graph whose nodes correspond to POS sequences and whose arcs correspond to the Euclidean distance between two POS sequences. , , \ufffd . 3 shows the result of clustering under the conditions H m i n = 3, Pd m i n = 0.05 and D m i n = 0.25. (1) Delete arcs whose value is greater than a threshold value (D m i n ) - Decompose the graph into connected components by examining connectivity. Each connected component corresponds to a cluster and the nodes to their elements . Table In this table, \"area'' is a value given to each cluster which is calculated by summing the product of the length and the frequency of each element 1 . Selecting Rewriting Rules After clustering, a new set of rewritill'g rules can be obtained from any cluster by putting each of the POS sequences on the right-hand si\ufffde of a rule and introducing a new single non terminal symbol on the left-hand sides. Next, the system applies them to reduce the corpl.ls for (2) Considering cases like this, we can conclude that changing all the clusters into rewriting rules and applying them to the corpus would disturb the appropriate extraction of rewriting rules. For this reason, the system selects only one cluster from the output of the second step. We Table 3: chose the --area\u2022' of the cluster as the measure for selecting the best cluster. This is based on the third hypothesis presented below. \u2022 The most suitable set of rewriting rules makes the greatest reduction of the corpus size. One may expect that a large value for area means that the rewriting rules of the cluster are basic symbol sequences which appear regardless of their global environment. And the larger their area is. the more their application ,v ill reduce the number of symbols in the corpus. Algorithm \\\\\"e developed a system. ba5ed on the hypotheses discussed in the previous section, which extracts a set of rewriting rules from a corpus and applies them to the corpus. The system executes the following four processes repeatedly: Compute n-gram statistics on the corpus, Extract part-of-speech sequences whose frequency is more than f m i n and which meet the conditions H min = 3 and Pd min = 0.05. Cluster the part-of.:speech sequei1ces an d select a cluster to produce a set of rewriting rules, Rewrite the corpus: applying the rewriting rules. In the experiment, the initial value of f min is equal to 2,000. If no POS sequence is extracted in the second process, f min is multiplied by 0.9 and the second process is repeated with the new f min \u2022 If f min becomes less than 50, the system stops. Below, we describe more precisely the la5t two processes. Produce Rewriting Rules As we described above, the system extracts constituent-like POS sequences and clusters them depending on their distributional similarity. Next it selects the cluster which has the largest ,:area'' in the corpus to produce rewriting rules. The last process is to select a non-terminal to put on the left side of rewriting rules. There are two cases, depending on the number of POS sequences in the cluster which consist of a single non-terminal symbol. If there is exactly one, that non-terminal symbol is put on the left-hand side to produce rewriting rules, and the rewriting rule which would have that non-terminal symbol on both sides is erased. In the other case, i.e. the number of POS sequences composed of a single non-terminal symbol is zero or more than one, the system introduces a new non-terminal symbol and puts it on the left-hand side to produce rewriting rules. Rewrite Corpus After having produced a set of rewriting rules, the system applies them to every sentence in the corpus. At this point, there are two problematic situations. The first one is that a rewriting rule is applicable in more than one way. For example, suppose that the rewriting rule is syn 1 ---+-tag 1 \u2022 tag 1 and the following symbol sequence exists in the corpus: ... tag 1 \u2022 tag 1 \u2022 tag1 ... The rule is applicable to both the first and the last tag 1 \u2022 tag1 at the same time. To handle cases like this, the system applies each rewriting rule simply from left to right. The second case is that two or more rewriting rules conflict with each other. For example, suppose that there are two rewriting rules such as and the following symbol sequence exists in the corpus: ... tag 1 \u2022 tag2 \u2022 tag3 ... ( 3) ( 4) In this case both of the rules are applicable. To avoid this conflict, the system applies rewriting rules in the order of the length of their right-hand side. In this example, only rule ( 4) is applied. Results We conducted experiments on the sentences in WSJ, which are composed of the POS tags in Table 1 . The corpus contains 24,678 sentences and 549,247 tags. The average sentence length is, therefore, 22.3 tags. Figure 3 shows the distribution by length of sentences in the corpus. We ran two experiments with different threshold values for the distance between two distri butions (Exp. 1, D m i n = 0.20 and Exp. 2, D m i n = 0.25). The output of each experiment is the final state of the corpus and the extracted rewriting rules. Accuracy of Parsing First we discuss the final state of the corpus, which can be regarded as the parsing results. Figure 4 shows an example of a parsed sentence, where the symbols starting with \"NT\" are the non-terminal symbols introduced by the system. For the evaluation of our experiments we have chosen the measure called \"crossing paren thesis accuracy\" which arose out of a parser evaluation workshop [Black et al. , 1991] , 1993 , ] [Brill: 1993]] . In addition to the difference in source inform\ufffdtion, it must be noted that those methods were tested only on relatively short sentences, while in our experiments sentence length is not limited. From this viewpoint one may say that the accuracy of ou r method is sufficiently promising. We must draw attention to another difference between our method and the others. They simply bracket sentences and are not able to introduce non-terminal symbols, while our method is able to infer non-terminal symbols without any information but a corpus. Table 5: Some rewriting rules extracted in Experiment 2 NT00l PRP$ \u2022 NNS \u2022 NNS NT005 VBN \u2022 NT002 NT00l PDT \u2022 DT \u2022 NNS \u2022 NN NT005 -VBG \u2022 NT00l NT00l -NNP -NN P-POS -NN NT005 -PRP \u2022MD\u2022 VB \u2022 NT002 NT00l -DT \u2022 VBG \u2022 NNS \u2022 NNS NT006 -VB Z \u2022 NT005 \u2022 NT005 NT00l -DT -JJ \u2022 NN NT006 -V BZ \u2022 NT005 \u2022 NT002 NT002 TO \u2022 NT00l NT006 V BZ \u2022 NT005 \u2022 NT00l NT002 TO \u2022 VB \u2022 NT00l NT006 -V BZ \u2022 NT005 NT002 ----TO \u2022 VB :1VNS NT006 --!-V BZ \u2022 NT002 \u2022 NT005 NT002 RB \u2022 TO \u2022 NT00l NT006 -VBZ \u2022 NT002 1V T002 IN \u2022 1VT00l NT006 -V BZ \u2022 N-Y00l \u2022 NT00l NT002 IN \u2022 RB \u2022 NT00l NT006 -V BZ \u2022 NT00l NT002 IN -NNS -NN NT006 -V BD \u2022 NT005 \u2022 NT005 NT002 TO \u2022 NT003 NT006 -VBD -NT002 \ufffdv roo2 RB \u2022 i\\! T002 1V T006 -MD \u2022 VB \u2022 NT00l NT002 RB \u2022 IN \u2022 NT003 NT006 -MD \u2022 VB \u2022 NT00l ST002 IN \u2022 NT003 NT007 -VBZ -VBN -VBN NT003 NNP -NNP -NNP NT007 -MD -VB :V T003 NNP -NNP NT008 -NNS -NT006 ?-.f T003 NT003 \u2022CC\u2022 NT003 NT008 N NS \u2022 V BP \u2022 NT005 1VT004 NN -lv\u2022 N NT008 -NNS -VBP -NTO0l NT004 JJ -NN-1 \\fN NT009 ----NT004 \u2022 NT00S 1VT 005 NT002 \u2022 VB N \u2022 NT002 NT009 -NT003 \u2022 J J \u2022 NT00S NT005 NT00l \u2022 NT002 NT0lO -WRB \u2022 PRP \u2022 NT006 NT005 NT00l \u2022 V BN \u2022 NT002 NT010 -V BG \u2022 NT004 \u2022 NT006 NT005 NT00l \u2022 VBG \u2022 NT002 NT010 -VBG -NT004 POS sequences on the right-hand side of a rewriting rule are less constrained as to what POS precedes and follows them than non-constituent sequences . 2. POS sequences directly derived from the same non-terminal symbol have similar environ ments. The most suitable set of rewriting rules makes the greatest reduction of the corpus size. Then, we developed an algorithm based on these hypotheses which extracts rewriting rules and parses tl\ufffde sentences at the same time. The correctness of these hypotheses has been experimentally attested by the evaluation of the extracted grammar and parsing accuracy. The method we have proposed in this paper is a bottom-up approach to grammar infer ence. On the other hand, a top-down approach such as [Magerman and Marcus, 1990] may also be important for grammar induction . Clustering techniques, e.g. [Hindle, 1990] and subcat egorization techniques, e.g. [Brent, 1991] may bridge the gap between these two approaches. Combining these techniques including tagging techniques, larger amounts of syntactic informa tion can be retrieved from unbracketed corpora or even Evaluation of Rewriting Rules The other output of our system is a set of rewriting rules. Its quantitative evaluation is so difficult that w \ufffd cqmpare them w\ufffdth a general English grammar. The interesting thing in this table is that the rewriting rules whose left-hand side is NT006 have various combinations of noun phrases and/or prepositional phrases on their right-hand side. They must be considered as verbal case frames. This indicates that it is possible to extract types of verbal case frames, which is defined a priori in various attempts\u2022 at automatic case frame acquisition [Brent and Berwick, 1991] [Brent, 1991] [Manning, 1993] . Elsewhere, however, the grammar contains some inappropriate rewriting rules, as follows: NT002 _. IN \u2022 J J R \u2022 N N S \u2022 N N S It would be better to represent this by the following rules instead. NT002 _.IN\u2022 NT00l NT00l _. JJR \u2022 N NS \u2022 N NS It is tme . that there are _some rewriting rules like this in the grammar but, as we mentioned above, almost all the non-terminal symbols are comprehensible. Since our method assumes simple \ufffdon text-free language as the model and extracts a grammar based on hypotheses deduced from its characteristics, tpese results l\ufffdad to the conclusion that structural features of langu_age ca11 be fXtracted usi 11 g simple statistical methods . It should . also be added _that it is difficult to evaluate a grammar because the evaluation depends on the language model of the parsed corpus to _be compared. Conclusions In this paper, we hav _ e described a new method to parse a tagged corpus without grammar, modeling a natural language on context-free language. We proposed the following three hy potheses. Intelligence, 1990 . [Manning, 1993] Conference on Artificial",
    "abstract": "\\Ve describe and evaluate experimentally a method to parse a tagged corpus without grammar modeling a natural language on context-free language. This method is based on the following three hypotheses. 1) Part-of-speech sequences on the right-hand side of a rewriting rule are less constrained as to what part-of-speech precedes and follows them than non-constituent sequences. 2) Part-of-speech sequences directly derived from the same non-terminal symbol have similar environments. 3) The most suitable set of rewriting rules makes the greatest reduction of the corpus size. Based on these hypotheses, the system finds a set of constituent-like part-of-speech sequences and replaces them with a new symbol. The repetition of these processes brings us a set of rewriting rules, a grammar , and the bracketed corpus.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "13",
    "year": "1995",
    "month": "September 20-24",
    "title": "Parsing Without Grammar"
}