{
    "article": "Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the endto-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework's veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage. Introduction Pre-trained language models like BERT (Devlin et al., 2019) have achieved prominent improvements in both information retrieval (IR) and natural language processing (NLP). Concurrently, researchers have raised wide awareness about the difficulty of explaining such deep learning models (Guidotti et al., 2018; Robnik-\u0160ikonja and Bohanec, 2018; Fong and Vedaldi, 2017) . Recently, many papers scrutinize BERT's behaviors in various tasks (van Aken et al., 2019; Clark et al., 2019; Tenney et al., 2019; Qiao et al., 2019; MacAvaney et al., 2020) . When it comes to token-wise analysis, most of the work study intra-layer self-attention and how it relates to various linguistic characteristics. Although these analyses yield unique insights on layer-local behavior across pairs of tokens, they do not take a global perspective of how token-wise representations exactly relate to the prediction. This is crucial for answering a fundamental question in interpretability: what hidden features and tokens contribute the most to the prediction? To faithfully compute such feature-prediction attribution maps, Schulz et al. (2020) and Jiang et al. (2020) propose to apply information bottlenecks. In this paper, we leverage this model-agnostic method to analyze passage reranking for pre-trained transformers. We first introduce the information bottleneck for attribution (IBA) method (Schulz et al., 2020) in general and elaborate its use in interpreting passage reranking. Afterwards, we compare it with two other widely adopted attribution methods to demonstrate its credibility and justify our choice. We then carry out detailed analyses on the inner mechanisms of passage reranking. BERT reranking (Nogueira and Cho, 2019) starts a new chapter in information retrieval, as it combines the dual advantages of the speed of sparse representation (BM25) and the deep contextualization of dense representation. To be specific, given a query , BM25 returns top-1,000 passages . The label is 1 if a passage \u2208 is relevant to , and 0 otherwise. For BERT, the input is [CLS] q [SEP] d [SEP] , and the output label is . After fine-tuning, we rerank based on the output probabilities of relevance. This setting is different from most NLP tasks, where positive and negative labels are provided by the dataset, and only one pair of (input, probability) is required for the final output. We use IBA to generate attribution maps for BERT-large (Devlin et al., 2019) fine-tuned on the MSMARCO dataset (Bajaj et al., 2016) in this paper. With the attribution maps, we investigate the following questions: Q1. What are the similarities and differences between BERT and BM25? For the two-stage pipeline, we wonder how BERT's ranking mechanism is similar to BM25 and what it provides that BM25 doesn't. Through crosspassage examination, we find that BERT still regards lexical matching as important to some extent, similar to BM25. BERT, furthermore, manages to capture deeper-contextualized relationships between the query and the relevant passage. Q2. How do special tokens contribute to reranking across layers? In BERT, only the [CLS] token is designed to factor into prediction. Then how do those special tokens collect information across layers to capture a contextualized relationship? We find that, different from what attention analyses show, [CLS] starts to gather the evidence for prediction primarily after layer 16, especially in layer 24. Q3. How robust is the top-ranked passage? One of the special settings of ranking is that we do not care about the absolute score, as long as the relevant passage ranks higher than irrelevant ones. We conduct experiments of token removal for the top-1 positive passage to test the robustness. We find that we can truncate up to 22.5% tokens on average, given reasonable attribution scores, of the top-ranked passage without affecting its order. Q4. Does BERT have positional bias? We then look deeper into what makes those passages rank higher. We find that BERT, after fine-tuned on MSMARCO, prefers those passages with inverted pyramid structure-that is, passages that put important information at the start. We further confirm that it has positional bias towards the start of the passage through various experiments. Related Work Generally speaking, interpretability methods are either model specific, applying to only a single architectural family, or model agnostic, covering a broad spectrum of supervised models. Since pretrained transformers represent the state of the art in NLP, for model-specific techniques we discuss those for BERT, the prototypical, most-interpreted transformer model. As this work specifically explores passage reranking, we also provide the necessary literature about recent progress. BERT specific A number of works investigate the inner mechanisms of BERT. Kovaleva et al. (2019) ; Clark et al. (2019) carefully analyze BERT's attention heads, noting positive correlation between attention heads and linguistic features, as well as special tokens. Looking at attention, Voita et al. (2018) find that BERT captures anaphora and dependence on position and length in machine translation. Pointing out some shortfalls of these papers, Jain and Wallace (2019); Brunner et al. (2019) ; Serrano and Smith (2019) argue that attentions often do not reflect how models make predictions. Another line of work analyzing BERT use probing classifier to draw the connection between vector representation and specific linguistic knowledge (Tenney et al., 2019; Hewitt and Manning, 2019; Liu et al., 2019) . Rogers et al. (2020) provide a thorough literature survey about what we already know about how BERT works and they've found different probing methods sometimes lead to contradictory interpretations. A direct remedy is to look into what BERT looks at during inference time (i.e. identify important features for prediction, also known as \"attribution methods\" in general). That's where our work focuses on. Attribution maps Although more commonly applied to convolutional neural networks in image classification, most attribution methods are model agnostic. They aim to assign weights to input features according to how the model makes predictions, with higher weights corresponding to greater contributions. The most prevalent methods are gradient-based. Intuitively, gradients reflect how small changes in the input affect the final prediction to some extent. But previous work shows that raw gradients are noisy and limited to capturing only the local \"importance\" (Smilkov et al., 2017) . To remedy this, some of them (Sundararajan et al., 2017; Smilkov et al., 2017) incorporate global importance to mitigate this problem, while others (Binder et al., 2016; Shrikumar et al., 2017; Kindermans et al., 2018) modify or extend the back-propagation algorithms directly to emphasize positive contributions with regard to prediction. However, Sixt et al. (2020) show that most of the modified back-propagation methods fail a basic sanity check: invariance to parameter randomization and label randomization. LIME (Ribeiro et al., 2016) is not even limited to differentiable models. They use interpretable models like decision trees to approximate deep neural networks, and thus can theoretically interpret any classifier. However, empirically, LIME's high demand on memory may worsen its quality compared to other methods, as we will see in the later section. Information-theoretic methods are often unconstrained by tasks and models as well, while additionally providing a unified view of how infor-mation flows across models. Guan et al. (2019) use mutual information to estimate tokens importance across layers but don't provide quantitative evaluation. Bang et al. (2019) also take advantage of information bottlenecks to interpret predictions, but they restrict the information by sampling tokens, which doesn't generate a complete attribution map for every token and limits the interpretation to be token-wise only. More recently, Schulz et al. (2020) propose the information bottleneck method for attribution, which empirically achieves the best result on multiple evaluation metrics in interpreting images. Jiang et al. (2020) further leverage this method in NLP and also surpass other modelagnostic methods on multiple datasets. Neural IR BERT is a game changer for information retrieval. Lin et al. (2020) even separate neural reranking techniques into \"pre-BERT\" and \"post-BERT\" eras. Nogueira and Cho (2019) start the post-BERT era by proposing a two-stage pipeline, using sparse representations like BM25 to generate candidates and then neural models like BERT to rerank them. More recent work explores merging the two-stage pipeline into an end-to-end dense retrieval, like DPR (Karpukhin et al., 2020) , which still use BERT as the basic building block for neural information retrieval. Therefore, understanding BERT's behavior for reranking in the original setting still helps. Toward this, a few previous works specifically analyze BERT for reranking: Qiao et al. (2019) analyze attention to see how BERT attends to stop words and regular words across layers. MacAvaney et al. (2020) does a more thorough study of various reranking models, using carefully designed textual manipulation methods. Different from them, we use a model-agnostic method to generate a token-wise attribution map, as it provides us with the flexibility to carry out a layer-wise analysis. Besides, to the best of our knowledge, no previous work has done a cross-passage analysis to see patterns across the ranks of different passages. IBA Method General Introduction to IBA The starting point of IBA is to keep only the featurelevel information that's most helpful toward the final prediction. After a given layer in the target neural network, we insert an information bottleneck, which restricts the total amount of information in the representation. Simultaneously, we maximize the amount of information important toward the final prediction. To be concrete, given an input X \u2208 R and output Y \u2208 R , an information bottleneck is an intermediate representation T that maximizes the following function: I(Y; T) \u2212 \u2022 I(X; T), (1) where I denotes mutual information, and is a hyperparameter that balances the trade-off between reconstruction I(Y; T) and information restriction I(X; T). A larger means a narrower bottleneck and hence less information through the network. Intuitively, maximizing I(Y; T) keeps information for accurate prediction, while minimizing I(X; T) filters out unnecessary information. To obtain the condensed representation T, we construct a loss function based on the intuition above. For I(Y; T), we can directly use the cross-entropy loss for classification L CE . For I(X; T), we will derive it step by step below. Formally, for a given layer of a model, let X = (H), meaning the output of each layer, where H is the input of layer . We then restrict the information by injecting noise into input X, which results in T = \u00b5 X + (1 \u2212 \u00b5) , (2) where refers to element-wise multiplication, 1 is an all-one vector, \u00b5 \u2208 R is the weighting parameter controlling the balance between signal and noise whose dimension is the same as input X. For each dimension, we constrain \u00b5 \u2208 [0, 1], setting \u00b5 = (\u03b1 i ), where is the sigmoid function, to simplify the training process. And \u03b1 i is the parameter that we are learning for each dimension. From Eq. 2 we can see that when \u00b5 = 0, that is, all the information is discarded; only noise is passed through (T = ). Taking that into account, in order to preserve the magnitude of the input for the next layer, it's desirable to keep the same mean and variance as X. Therefore, we have \u223c N ( X , 2 X ). This condition doesn't always ensure T to have exactly the same mean and covariance with X though. And the model is recovered after training the bottleneck to ensure the covariance shift doesn't affect interpreting subsequent instances. After obtaining T, we can now evaluate I(X; T). By definition, I(X; T) = E X [ [ (T|X) (T)]], (3) where means Kullback-Leibler (KL) divergence and (T|X), (T) are probability distributions. As (T) = \u222b (T|X) (X)dX, there is no analytical expression for (T). We use the standard variational approximation (T) = N ( X , 2 X ) to substitute (T). Note that we estimate each X and X empirically. The variational approximation assumes that each dimension is distributed independently and normally. The normal distribution comes from the observation that activations after linear and convolutional layers tend to be Gaussianlike (Klambauer et al., 2017; Borovykh, 2018) . The independence assumption, on the other hand, does not hold in general, but it just overestimates the mutual information, so it gives an upper bound of mutual information between X and T: I(X; T) \u2264 E X [ [ (T|X) (T)]]. ( 4 ) Proof can be found in Appendix A. An upper bound means when the approximation between X and T is 0, their mutual information is guaranteed to be 0, which is a desired property, as we expect I(X, T) to be small. Combining Eq. 4 with the cross entropy for classification, we have our loss function: L = L CE + \u2022 E X [ [ (T|X) (T)]]. (5) Note that we negate the sign for minimization. can be viewed as the gate controlling the relative importance between the two loss components. After getting T from above, we calculate how much information T still contains about X using Eq. 3. This gives us the contribution of each dimension in every token. In order to generate the token-wise attribution map, we sum over the feature-token axis to obtain an attribution score for each token. IBA for Passage Reranking Analysis The procedure of using BERT to rerank passages (Nogueira and Cho, 2019 ) can be characterized as follows: Given query , and a list of passages , \u2208 is returned by BM25. BERT then assigns the relevance score ( , ), the logits for the probability that the passage is regarded as relevant, to each pair of and . L CE in this case is the same as the cross entropy in Nogueira and Cho (2019) . We use BERT-large model fine-tuned on MSMARCO dataset for experiments. In order to get T, we optimize the learning parameter \u03b1. At the beginning of the training, we start with T \u2248 X to keep the information of X in T as much as possible. Thus, we initialize \u03b1 = 5 for each dimension as it results in \u00b5 = 0.993, which is close to 1 as desired. During optimization, we fix the training steps to 10 and repeat a sample 10 times to inject different noise, which altogether requires 100 total steps to generate an attribution map for a single instance. Another important hyperparameter is . We empirically pick \u2248 10 \u00d7 L CE L IB , as suggested in (Jiang et al., 2020) . To compare the effectiveness of IBA with other attribution methods, we carry out a degradation test. The essential idea of a degradation test is to remove the most important % tokens, excluding special tokens, identified by different attribution methods and measure the drop of the probability with respect to the given label. The initial value of is 11 and we increase until all the tokens are removed, shown as the -axis in Figure 1a . -axis means the normalized average probability drop after removing a certain percentage of tokens: \u00af ( | )\u2212 \u2212 where represents input with certain tokens removed, is the original probability before tokens removal, and is the minimum of the fully degraded instance's probability across all attribution methods. We conduct the experiment across the entire MSMARCO dev set (6980 queries). We compare the result with two other popular model-agnostic attribution methods, LIME (Ribeiro et al., 2016) and Integrated Gradients (IG) (Sundararajan et al., 2017) , each representing a different category of attribution methods: LIME uses interpretable models like decision trees and linear models to approximate the black box, while IG is a variation of using the gradient of the predicted output with respect to given input features. To provide a simple baseline, we also compare the result with \"Random,\" where tokens are removed randomly. We expect a better attribution method will have a steeper slope, meaning removing important tokens identified by the method significantly deteriorates the performance. As shown in Figure 1 , IBA outperforms all other three methods with a 61.3% probability drop comparing with second-placed IG, which makes for a 29.0% drop. The absolute probability drop value can be seen in Table 1 . Experiments and Analyses Figure 1c shows an example of important tokens in the query and the passage identified by IBA. Aside from token matching like \"pH\" and \"water\", deeper semantic relatedness like \"acid\" and \"neutral\" are Given the attribution maps, we are now able to study which tokens BERT looks at for reranking. To be specific, we exploit IBA to extract the top-20 most important tokens M for each ( , ), \u2208 , \u2208 , where and represent the query list and the passage list. We carry out our experiment under two different settings: 1. consists of 1,000 randomly selected queries from the entire MSMARCO passage reranking dev set. is composed of the human annotated relevant passages. We then apply IBA to all 24 layers to get top-20 tokens M for each ( , ). 2. consists of 105 queries from a subset of the MSMARCO passage reranking dev set, provided by Pyserini (Lin et al., 2021) . comprises top-50 passages that BERT-large retrieves for each query. For these experiments, we fix the layer that we insert the information bottleneck after. For setting 1, we aim at cross-layer analysis for relevant passages. Specifically, we identify if lower layers show different focus from higher layers. This setting is similar to GLUE-like (Wang et al., 2018) classification tasks where we want to find general patterns about BERT. The reason for using the top 20 is that, in our sampled instances, the average tokenized query length is 9.2, and we also want to see the emphasized tokens in passages. For setting 2, we perform cross-passage analysis to investigate different patterns between higher-ranked passages and lower-ranked passages. The choice of the top-50 cutoff is due to frugality: the recall@50 (0.817) is comparable to the recall@1000 (0.848), with much less computation. Passage-Level Patterns It's well known that two-stage ranking pipelines use both exact token matching and semantic relatedness (Lin et al., 2020) . As BM25 estimates relevance purely by lexical matching, we wonder if BERT still relies on exact match and what else BERT provides. Q1. What are the similarities and differences between BERT and BM25? To answer this question, we first study the correlation between higher ranking scores and higher lexical matching between queries and passages. To measure the degree of lexical matching, we use the Jaccard index under experimental setting 2: J = | \u2229 | | \u222a | , { , | \u2208 \u2229 M, \u2208 \u2229 M}, where \u2208 [1, | |], \u2208 [1, | |] , remember that M is the top-20 tokens extracted by IBA. For each query , we calculate the Jaccard index for every passage in the top-50 passages. We then average them across all queries. We choose to insert the information bottleneck after layer 16, as it is the most informative one according to our degradation test. As we see in Figure 2a , the Jaccard index decreases as the rank of the passages becomes lower. In general, the higher the rank is, the more overlapped the important tokens between the query and \"Higher\" rank actually means lower order: passages with order 1 have higher rank than passages with order 2, etc. passages are. We also calculate Spearman's correlation (Spearman, 1961) to gauge the degree of monotonic association. We find that = \u22120.98, indicating a strong monotonic relation between the Jaccard index and passages order. Does this correlation hold among all tokens between the query and passage? Figure 2b shows the Jaccard index J = | \u2229 | | \u222a | , { , | \u2208 , \u2208 } across pas- sages. We see it shows a similar trend to J, confirming that even if BM25 returns passages that have higher lexical matching with query, token matching between queries and passages still plays an important role when BERT is reranking. But using all of the tokens between the query and the passage obtains a correlation coefficient of = \u22120.71, which is lower than using important tokens only. We argue that it's because IBA interprets in a way that's more aligned with the specific tokens that BERT looks at when reranking.  We further investigate what BERT provides that BM25 doesn't. Specifically, we look into what BERT gets right but BM25 gets wrong. We notice that BERT captures more contextualized relevance between the query and passage, while the BM25returned answer has more \"superficial\" relevance -BM25 seems to talk about the topic but doesn't really answer the question. The example shown in Figure 3 demonstrates that passage returned by BM25 seems highly related to the topic-\"cognitive impairment\" but instead of explaining what the goal is, it is explaining what \"cognitive impairment\"'s definition is. On the contrary, BERT not only returns the passage related to \"cognitive impairment\" but also the goal. More discussions about semantic similarity are in Appendix F. Layer-Level Patterns Downstream tasks often rely on BERT's [CLS] vector at the last layer as input, and that's also true for Query: what is the goal for the child with a cognitive impairment BM25 ranked 1st: A cognitive impairment is a condition where your child has some problems with ability to think and learn. Children with a cognitive impairment often have trouble with such school subjects as math and reading. cognitive impairment is a condition where your child has some problems with ability to think and learn. Children with a cognitive impairment often have trouble with such school subjects as math and reading. BERT ranked 1st: Promoting optimum development. The goal for children with cognitive impairment is the promotion of optimum social, physical, cognitive, and adaptive development as individuals within a family and community. Vocational skills are only one part of that goal. The focus must also be on the family and other aspects of development. reranking. It's intriguing to know the layer at which [CLS] starts to learn the relevance. Clark et al. (2019) thoroughly analyze BERT's self-attention mechanism for each layer. While they provide insight into how tokens attend to one another, the attention weights themselves often do not correlate with measures of feature importance (Jain and Wallace, 2019) . Q2. How do special tokens contribute to reranking across layers? We insert an information bottleneck after each layer for 24-layer attribution maps. First we inspect how the [CLS] token gets emphasized across the layers. Figure 4a shows the attribution score across 24 layers in experimental setting 1, with 95% confidence intervals. Note that the score is normalized between 0 to 1 for each token but it doesn't add up to 1 for each instance. We further normalize the attribution score by dividing the sum of the attribution scores at each layer to account for different layers' scale. As we can see in the plot, the attribution score for [CLS] across layers first decreases from layers 1-7, then goes up and fluctuates between layers 7-16, until finally increasing from layer 16 to 24. This differs from what attention analysis reveals in Kovaleva et al. (2019) and Clark et al. (2019) , where they demonstrate that attention heads attend to [CLS] in earlier layers but attend to [SEP] in later layers. It's not contradictory, though, because we inspect feature importance with respect to the predicted output. Since [CLS] at the final layer is treated as a summary representation for the whole sentence to perform classification, it's intuitive that [CLS] is regarded as an important feature in the final layers. What about the [SEP] tokens? Figure 4b shows the attribution score averaged between the two present [SEP] tokens-recall that BERT inserts two for every input. They become increasingly important with a certain amount of fluctuation from  Combining the above plots and the degradation tests across layers in Figure 1b , we conjecture that the [CLS] token initially serves as a classification prior to condition the tokens in the early layers (1-7) with [SEP] increasing participation. Then, BERT gathers more general syntactic information (Hewitt and Manning, 2019) , until layer 16, after which the [CLS] token slowly aggregates class-specific information and at layer 24 becomes the most important token for classification. Figure 1b (the full 24-layer degradation test is shown in Appendix B) echos the findings from previous work (Liu et al., 2019) , demonstrating that the middle layers are the most informative ones for prediction. To be exact, layer 16 ( 2 3 of the total number of layers) is the most informative one in our experiment with BERT-large, the same fraction as what Jiang et al. (2020) Truncation Test Different from other downstream tasks, passage reranking usually involves scores for 1,000 passages to generate the final result. Instead of absolute scores for passages, we only care if relevant passages have higher scores than irrelevant passages. Recent work (Bai et al., 2020; Formal et al., 2021) starts to incorporate sparse mechanisms (adding and removing tokens) in order to elevate efficiency for the first-stage ranking. We ask how token removal affects those true positive passages. Q3. How robust is the top-ranked passage? Specifically, we want to know how many unimportant tokens we can remove before the top-1 passage falls to second place. Once again, we use the IBA-generated attribution map and then remove those tokens with lowest attribution scores, until the ranking score for the top-1 passage drops below the second one. As in the reranking setting, the input is always a query-passage pair ( , ), and we have two experimental settings: (1) removing tokens that appear in both and ; and (2) removing tokens that appear in only . We include the result under both settings and report the truncated number, as well as the percentage needed in Table 2 . Surprisingly, even if BERT assigns an extreme score to the passage, making the score close to one another (Qiao et al., 2019) , it still takes up to 22.5% tokens on average for top-1 passage to downgrade to the second place. Obviously, removing tokens from the query quickly deteriorates the ranking score. Passagesonly seem to have more redundant tokens that can be safely removed, even though sentences in the passage will become incomplete and broken after token removal. Note that this experiment removes only tokens of the top-1 passage. We attach the comprehensive results and discussion of truncating tokens in all passages in Appendix G. et al. (2020) observe that changing the sentence order has negative effects when reranking with BERT. They suggest that either the model is affected by the discourse-level signal (e.g., topics discussed earlier in passages) or the model encodes positional bias. Positional Bias MacAvaney Q4. Does BERT have positional bias? To investigate if BERT has positional bias, we first plot the position index of where { | \u2208 M \u2229 }. Specifically, we insert the bottleneck after every layer under experimental setting 1-1000 relevant pairs of ( , )-and then accumulate the count of each position index for all 24 layers (plots for each layer is also shown in Appendix H). Statistics about randomly selected ( , ) are shown in Figure 4c . As we show in Figure 5a , tokens at the start of passages (e.g., position index from 0 to 20) have significantly higher occurrences than tokens appearing later. We then conduct three controlled experiments: (1) swapping the first two sentences; (2) reversing the order of all sentences; and (3) randomizing the order of sentences. We maintain the order of withinsentence tokens in order to keep the discourse complete and coherent. The plots are shown in Figure 5 . We see that, although changing the order results in more later-appearing tokens emphasized, the start of the passages still have incomparable dominance. To quantify the effect of swapping sentences, randomizing sentences, and reversing sentences, we calculate (relevant|{original, swap, random, reverse}). We find that (relevant|original) = 0.939, (relevant|swap) = 0.920, (relevant|random) = 0.918, (relevant|reverse) = 0.897. The probability drops after every change of sentence order. The more the order changes, the more the probability drops (i.e., the negative effect is reversed order > randomized > swapped). Given that BERT assigns extreme reranking scores to most ( , ) pairs (e.g., scores are mostly either close to 0 or close to 1), it's unclear whether changing the order of sentences affects the final result. Therefore, we also conduct experiments of changing the order sentence with the subset of the MS-MARCO passage reranking dev set. We present these results in Table 3 . Swapping sentences substantially deteriorates the result; randomizing and reversing the sentences further worsens the result. The above experiments suggest that the sentence order in the passage carries high importance in reranking with BERT. Specifically, passages with the inverted pyramid structure would be preferred, as they present important information at the beginning of the passages. More discussion on positional bias can be found in Appendix E. Conclusions In this work, we leverage IBA to examine BERT for reranking. We compare ranking mechanisms between BM25 and BERT, finding that BERT still values token matching, and it also learns deeper relevance between queries and passages. We further analyze special tokens across layers and demonstrate patterns that [CLS] aggregate evidence. We then investigate the robustness of top-ranked passages. Finally, we find that BERT fine-tuned on MSMARCO has positional bias towards the start of the passage. In summary, attribution maps can explain models' predictions and serve well as an observation tool that helps us visualize patterns, resulting in improved hypothesis formulation and experimental design. A Proof of Variational Upper Bound I(X; T) = E X [ [ (T|X) (T)]] = \u222b X ( ) ( \u222b T ( | ) log ( | ) ( ) ) = \u222b X \u222b T ( , ) log ( | ) ( ) ( ) ( ) = \u222b X \u222b T ( , ) log ( | ) ( ) + \u222b X \u222b T ( , ) log ( ) ( ) = \u222b X \u222b T ( , ) log ( | ) ( ) + \u222b T ( ) ( \u222b X ( | ) ) log ( ) ( ) = E X [ [ (T|X) (T)]] \u2212 [ (T) (T)] \u2264 E X [ [ (T|X) (T)]] B All 24 Layer Degradation Result C Qualitative Anlysis Table 4 shows a few examples with highlighted important tokens. We can see top-10 most important tokens across query and passage not only show the token matching but also capture semantic relatedness. For example, \"much\" in the query of the first example is highlighted. \"number\", \"million\" and \"$\" sign, which are highly related to the concept of \"much\", are also highlighted. Similarly, in the second example, BERT identifies that the core of the question -\"same document\". In the corresponding passage, it emphasizes \"or\" as well as \"mortgage\" before that and \"trust\" after that. In the third example, the query is about \"stronger\", which is again, captured by BERT, and related tokens like \"vs\" and \"roughtly equivalent\" are highlighted. D Detailed E Further Discussion on Positional Bias We find that BERT prefers passages with important information emphasized at the beginning. But is this preference a real \"bias\"? Will it cause misjudgement because of emphasizing too much on the start of the passages? To answer this question, we design an experiment to see if passages with higher reranked scores (than the ground truth passages) also happen to get key tokens emphasized earlier. Concretely, for those instances that have incorrectly ranked negative passages higher than the positive one, we regard each token in as a query, and we find the position of corresponding token that appears in where = . Then, we calculate the mean reciprocal rank for Query Document how much did nr ##a give to congress m ##em ##bers of congress pay attention to these numbers , and they know that in the last election cycle the nr ##a spent $ 18 . 6 million on various campaigns , a says lee dr ##ut ##man , who has studied the role of gun money in politics for the sunlight foundation . is mortgage and deed of trust the same document the mortgage or deed of trust is recorded in the county land records , usually shortly after the borrow ##ers sign it . if the loan is fully paid off , the lend ##er will record a release ( or satisfaction ) of mortgage or a rec ##on ##vey ##ance of deed ( which is used in conjunction with deeds of trust ) in the county land records . all -MRR = 1 | \u2229 | | \u2229 | =1 1 position( ) . We then aggregate the MRR for all higher-ranked negative passages (HRNPs) and compare it with the MRR for the lower-ranked positive passages (LRPPs). When we aggregate by the \"max\" function, we find that, in 86.2% of cases, HRNPs have higher MRR than LRPPs. Averaging all MRRs for HRNPs gives us 0.191, while it's 0.103 for averaging LRPPs. These numbers are 63.8%, 0.129, and 0.103, respectively if we aggregate by the arithmetic mean. We cannot say that the reason for those negative passages ranking higher is due to matched tokens appearing earlier, but we do note a correlation between HRNPs and early-appearing matched tokens. Driven by this positional bias, we are also curious about how positional index correlates with the passages' ranks. We compute the average positional index for each document's top-20 most important tokens, and then average for each query. As we show in Fig. 8 , higher-ranked passages do have earlier tokens emphasized, meaning that passages with important tokens stressed earlier are preferred. When comparing the top-1 document returned by BERT BERT with the top-1 document returned by BM25 BM25 , this preference also exists. We compute the MRR across all tokens in the query and passages like we do in Section 4.4 for those passages BERT \u2260 BM25 and BERT makes the correct prediction. We find that even BM25 is almost all about term matching, with J( , BERT ) = 0.062, J( , BM25 ) = 0.074, considering the position, MRR( , BERT ) = 0.127 is still higher than MRR( , BM25 ) = 0.099. F Note on Semantic Similarity Measurement We also find that it is hard to measure the contextualized relevance between query and passages by simply calculating cosine similarity between query vectors and document vectors. We encode , BERT , BM25 and don't find that ( ( ), ( BERT )) is higher than ( ( ), ( BM25 )) when using the Universal Sentence Encoder or Sentence-BERT (Reimers and Gurevych, 2019) , denoted as , pre-trained on an NLI dataset. However, if using a Sentence-BERT pre-trained on a paraphrase corpus (specifically the model \"paraphrase-MiniLM-L6-v2\") to measure semantic similarity, ( ( ), ( BERT )) is significantly higher than ( ( ), ( BM25 )) as \"paraphrase-MiniLM-L6-v2's\" pretrained corpus includes MSMARCO triplet. Tempting as it is to conclude that BERT has indeed captured semantic similarity that BM25 hasn't, it's unfair to use a pre-trained model with prior knowledge on MSMARCO to measure the semantic similarity. Therefore, we think BERT has learned a deeper relevance between the query and document, but it cannot be simply measured by vaguely defined semantic similarity. G Truncation Test across All Passages To further measure the trade-off between compression and quality, we do truncation test for all passages. Specifically, given a % of tokens kept for every single document, we measure final ranking performance-MRR score. The result is shown in Figure 9 . From the result, we can see that truncating doc only is more robust than truncating both query and doc. On average, with 90% tokens of passage kept, we have MRR = 0.311. But for the maximum, we can get MRR = 0.392 with 90% tokens, which is very close to the original score, and that depends on what tokens we remove. H Position Index for Important Tokens across 24 Layers Shown in Figure 10 , layer 24 is the outlier, where most tokens emphasized are in the middle of the document. For other layers, it's still the start of passages that is emphasized. Acknowledgements This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada; computational resources were provided by Compute Canada. ",
    "abstract": "Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the endto-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework's veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage.",
    "countries": [
        "Canada"
    ],
    "languages": [],
    "numcitedby": "2",
    "year": "2021",
    "month": "November",
    "title": "How Does {BERT} Rerank Passages? An Attribution Analysis with Information Bottlenecks"
}