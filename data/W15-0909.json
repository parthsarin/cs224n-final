{
    "article": "In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. Introduction While the explicit identification of multiword expressions (\"MWEs\": Sag et al. (2002) , Baldwin and Kim (2009) ) has been shown to be useful in various NLP applications (Ramisch, 2012) , recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (\"IR\": Acosta et al. (2011) ) and machine translation (\"MT\": Weller et al. (2014) , Carpuat and Diab (2010) and Venkatapathy and Joshi (2006) ). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007) , they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations, based on compositionality. For example, an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the reference translation. On the other hand, a partial mismatch between traffic signal and traffic light should be rewarded, as the usage of traffic is highly compositional in both cases. That is, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the TESLA machine translation metric (Liu et al., 2010) . Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006) ; (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010) ; or (3) deeper linguistic information such as semantic roles (Gim\u00e9nez and M\u00e0rquez, 2008; Pad\u00f3 et al., 2009) . In this research, we focus on the TESLA MT eval-uation metric (Liu et al., 2010) , which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) . This is the (primary) approach we take in this paper, as outlined in Section 3.2. Methodology Using compositionality scores in TESLA In this section, we introduce TESLA and our method for integrating compositionality scores into the method. Firstly, TESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: S ms = 1 if lemma(x) = lemma(y) a+b 2 otherwise S lem (x, y) = I(lemma(x) = lemma(y)) S pos (x, y) = I(POS (x) = POS (y)) where: a = I(synset(x) \u2229 synset(y)) b = I(POS (x) = POS (y)) lemma returns the lemmatised unigram, POS returns the POS tag of the unigram, synset returns the WordNet synsets associated with the unigram, and I(.) is the indicator function. The similarity between two n-grams x = x 1,2,...,n and y = y 1,2,...,n is measured as follows: s(x, y) = 0 if \u2203i, s(x i , y i ) = 0 1 n n i=1 s(x i , y i )) otherwise TESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores over the three terms (S ms , S lem and S pos ) for all n-grams. In order to add the compositionality score to TESLA, we first identify MWEs in the MT output and reference translation. If an MWE in the reference translation aligns exactly with an MWE in the MT output, the weight remains as 1. Otherwise, we replace the computed weight computed for the noun compound with the product of computed weight and the compositionality degree of the MWE. This forces the system to be less flexible when encountering less compositional noun compounds. For instance, in TESLA, if the reference sentence contains ivory tower and the MT output contains white building, TESLA will align them with a score of 1. However, by multiplying this weight with the compositionality score (which should be very low for ivory tower), the alignment will have a much lower weight. Predicting the compositionality of MWEs In order to predict the compositionality of MWEs, we calculate the similarity between the MWE and each of its component words, using the three approaches detailed below. We calculate the overall compositionality of the MWE via linear interpolation over the component word scores, as: comp (mwe) = \u03b1comp c (mwe, w 1 ) + (1 \u2212 \u03b1)comp c (mwe, w 2 ) where mwe is, without loss of generality, made up of component words w 1 and w 2 , and comp c is the compositionality score between mwe and the indicated component word. Based on the findings of Reddy et al. (2011) , we set \u03b1 = 0.7. Distributional Similarity (DS): the distributional similarity between the MWE and each of its components (Salehi et al., 2014) , calculated based on cosine similarity over co-occurrence vectors, in the manner of Sch\u00fctze (1997) , using the 51st-1050th most frequent words in the corpus as dimensions. Context vectors were constructed from English Wikipedia. SS+DS: the arithmetic mean of DS and string similarity (\"SS\"), based on the findings of Salehi et al. (2014) . SS is calculated for each component using the LCS-based string similarity between the MWE and each of its components in the original language as well as a number of translations (Salehi and Cook, 2013) , under the hypothesis that compositional MWEs are more likely to be word-forword translations in a given language than noncompositional MWEs. Following Salehi and Cook (2013) , the translations were sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) . In Salehi and Cook (2013) , the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. Dataset We evaluate our method over the data from WMT 2013, which is made up of a total of 3000 transla-tions for five to-English language pairs (Bojar et al., 2013) . As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output, as collected by Graham et al. (2014) . We used the Stanford CoreNLP parser (Klein and Manning, 2003) to identify English noun compounds in the translations. Among the 3000 sentences, 579 sentences contain at least one noun compound. Results We performed two evaluations, based on the two sets of judgements (pairwise preference or continuousvalued judgement for each MT output). In each case, we use three baselines (each applied at the segment level, meaning that individual sentences get a score): (1) METEOR (Banerjee and Lavie, 2005) , ( 2 ) BLEU (Papineni et al., 2002) , and (3) TESLA (without compositionality scores). We compare these with TESLA incorporating compositionality scores, based on DS (\"TESLA-DS\") and SS+DS (\"TESLA-SS+DS\"). We also include results for an exact match method which treats the MWEs as a single token, such that unless the MWE is translated exactly the same as in the reference translation, a score of zero results (\"TESLA-0/1\"). We did not experiment with the string similarity approach alone, because of the high number of missing translations in PanLex. In the first experiment, we calculate the segment level Kendall's \u03c4 following the method used in the WMT 2013 shared task, as shown in Table 1 , including the results over the subset of the data which contains a compound noun in both the reference and the MT output (\"contains NC\"). When comparing TESLA with and without MWE compositionality, we observe a tiny improvement with the inclusion of the compositionality scores (magnified slightly over the NC subset of the data), but not great enough to boost the score to that of METEOR. We also observe slightly lower correlations for TESLA-0/1 than TESLA-DS and TESLA-SS+DS, which consider degrees of compositionality, for fr-en, de-en and es-en (results not shown). In the second experiment, we calculate Pearson's r correlation over the continuous-valued adequacy  judgements, as shown in Table 2 , again over the full dataset and also the subset of data containing compound nouns. The improvement here is slightly greater than for our first experiment, but not at a level of statistical significance (Graham and Baldwin, 2014) . Perhaps surprisingly, the exact compositionality predictions produce a higher correlation than the continuous-valued compositionality predictions, but again, even with the inclusion of the compositionality features, TESLA is outperformed by METEOR. The correlation over the subset of the data containing compound nouns is markedly higher than that over the full dataset, but the r values with the inclusion of compositionality values are actually all slightly below those for the basic TESLA. As a final analysis, we examine the relative impact on TESLA of the three compositionality methods, in terms of pairings of MT outputs where the ordering is reversed based on the revised TESLA scores. Table 3 details, for each language pairing, the number of pairwise judgements that were ranked correctly originally, but incorrectly when the compositionality score was incorporated (\"P\u2192N\"); and also the number of pairwise judgements that were ranked incorrectly originally, and corrected with the incorpo-ration of the compositionality judgements (\"N\u2192P\"). Overall, the two compositionality methods perform better than the exact match method, and utilising compositionality has a more positive effect than negative. However, the difference between the numbers is, once again, very small, except for the ru-en language pair. The exact match method (\"0/1\") has a bigger impact, both positively and negatively, as a result of the polarisation of n-gram overlap scores for MWEs. We also noticed that the N\u2192P sentences for SS+DS are a subset of the N\u2192P sentences for DS. Moerover, the N\u2192P sentences for DS are a subset of the N\u2192P sentences for 0/1; the same is true for the P\u2192N sentences. Discussion As shown in the previous section, the incorporation of compositionality scores can improve the quality of MT evaluation based on TESLA. However, the improvements are very small and not statistically significant. Part of the reason is that we focus exclusively on noun compounds, which are contiguous and relatively easy to translate for MT systems (Koehn and Knight, 2003) . Having said that, preliminary error analysis would suggest that most MT systems have difficulty translating non-compositional noun compounds, although then again, most noun compounds in the WMT 2013 shared task are highly compositional, limiting the impact of compositionality scores. We speculate that, for the method to have greater impact, we would need to target a larger set of MWEs, including non-contiguous MWEs such as split verb particle constructions (Kim and Baldwin, 2010) . Further error analysis suggests that incorrect identification of noun compounds in a reference sentence can have a negative impact on MT evaluation. For example, year student is mistakenly identified as an MWE in ... a 21-year-old final year student at Temple .... Furthermore, when an MWE occurs in a reference translation, but not an MT system's output, incorporating the compositionality score can sometimes result in an error. For instance, in the first example in Table 4 , the reference translation contains the compound noun cash flow. According to the dataset, the output of MT system 1 is better than that of MT sys-57 Reference This means they are much better for our cash flow. MT system 1 That is why they are for our money flow of a much better. MT system 2 Therefore, for our cash flow much better. Reference 'I felt like I was in a luxury store,' he recalls. MT system 1 'I feel as though I am in a luxury trade,' recalls soldier. MT system 2 'I felt like a luxury in the store,' he recalled the soldier. Table 4 : Two examples from the all-en dataset. Each example shows a reference translation, and the outputs of two machine translation systems. In each case, the output of MT system 1 is annotated as the better translation. tem 2. However, since the former translation does not contain an exact match for cash flow, our method decreases the alignment score by multiplying it by the compositionality score for cash flow. As a result, the overall score for the first translation becomes less than that of the second, and our method incorrectly chooses the latter as a better translation. Incorrect estimation of compositionality scores can also have a negative effect on MT evaluation. In the second example in Table 4 , the similarity score between luxury store and luxury trade given by TESLA is 0.75. The compositionality score, however, is estimated as 0.22. The updated similarity between luxury trade and luxury store is therefore 0.16, which in this case results in our method incorrectly selecting the second sentence as the better translation. Conclusion This paper described the first attempt at integrating MWE compositionality scores into an automatic MT evaluation metric. Our results show a marginal improvement with the incorporation of compositionality scores of noun compounds. Acknowledgements We thank the anonymous reviewers for their insightful comments and valuable suggestions. NICTA is funded by the Australian government as represented by Department of Broadband, Communication and Digital Economy, and the Australian Research Council through the ICT Centre of Excellence programme.",
    "abstract": "In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact.",
    "countries": [
        "Canada",
        "Australia"
    ],
    "languages": [
        "Thai",
        "Hindi",
        "Czech",
        "Chinese",
        "French",
        "Portuguese",
        "Romanian",
        "Dutch",
        "English",
        "Russian"
    ],
    "numcitedby": "15",
    "year": "2015",
    "month": "June",
    "title": "The Impact of Multiword Expression Compositionality on Machine Translation Evaluation"
}