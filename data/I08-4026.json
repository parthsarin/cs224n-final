{
    "article": "This paper briefly describes our system in The Fourth SIGHAN Bakeoff. Discriminative models including maximum entropy model and conditional random fields are utilized in Chinese word segmentation and named entity recognition with different tag sets and features. Transformation-based learning model is used in part-of-speech tagging. Evaluation shows that our system achieves the F-scores: 92.64% and 92.73% in NCC Word Segmentation close and open tests, 89.11% in MSRA name entity recognition open test, 91.13% and 91.97% in PKU part-of-speech tagging close and open tests. All the results get medium performances on the bakeoff tracks. Introduction Lexical analysis is the basic step in natural language processing. It is prerequisite to many further applications, such as question answer system, information retrieval and machine translation. Chinese lexical analysis chiefly consists of word segmentation (WS), name entity recognition (NER) and part-of-speech (POS) tagging. Because Chinese does not have explicit word delimiters to mark word boundaries like English, WS is essential process for Chinese. POS tagging and NER are just like those of English. Our system participated in The Fourth SIGHAN Bakeoff which held in 2007. Different approaches are applied to solve all the three tasks which are integrated into a unified system (ITNLP-IsLex). For WS task, conditional random fields (CRF) are used. For NER, maximum entropy model (MEM) is applied. And transformation-based learning (TBL) algorithm is utilized to solve POS tagging problem. The reasons using different models are listed in the rest sections of this paper. We give a brief introduction to our system sequentially. Section 2 describes WS. Section 3 and section 4 introduce NER and POS tagging respectively. We give some experimental results in section 5. Finally we draw some conclusions. Chinese word segmentation For WS task, NCC corpus is chosen both in close test and open test. Conditional random fields Conditional random fields are undirected graphical models defined by Lafferty (2001) . There are two advantages of CRF. One is their great flexibility to incorporate various types of arbitrary, non-independent features of the input, the other is their ability to overcome the label bias problem. Given the observation sequence X, on the basis of CRF, the conditional probability of the state sequence Y is: ( ) ( k k i-1 i k 1 p Y X = exp l f y , y , X,i Z(X) ) \u23a7 \u23ab \u23a8 \u23ac \u23a9 \u23ad \u2211 (1) ( k k i-1 i y Y k ) Z(X) = exp l f y , y , X,i \u2208 \u23a7 \u23ab \u23a8 \u23ac \u23a9 \u23ad \u2211 \u2211 (2) Z(x) is the normalization factor. ( ) 1 , , , k i i f y y X i \u2212 is the universal definition of features in CRF. Word segmentation based on CRF Inspired by Zhao (2006) The contexts window size for each character is 5: C -2 , C -1 , C 0 , C 1 , and C 2 . There are 10 feature templates used to generate features for CRF model including uni-gram, bi-gram and tri-gram: C -2 , C-1 , C 0 , C 1 , C 2 , C -1 C 0 , C 0 C 1 , C -2 C -1 C 0 , C -1 C 0 C 1 , and C 0 C 1 C 2 . For the parameters in CRF model, we only do work to choose cut-off value for features. Our experiments show that the best performance can be achieved when cut-off value is set to 2. Maximum likelihood estimation and L-BFGS algorithm is used to estimate the weight of parameters in the training module. Baum-Welch algorithm is used to search the best sequence of test data. For close test, we only used CRF to do segmentation, no more post-processing, such as time and date finding, was done. So the performance could be further improved. For open test, we just use our NER system to tag the output of our close segmentation result, no more other resources were involved. Chinese name entity recognition For NER task, MSRA is chosen in open test. Chinese name dictionary, foreign name dictionary, Chinese place dictionary and organization dictionary are used in the model. Maximum entropy model Maximum entropy model is an exponential model that offers the flexibility of integrating multiple sources of knowledge into a model (Berger, 1996) . It focuses on the modeling of tagging sequence, replacing the modeling of observation sequence. Given the observations sequence X, on the basis of MEM, the conditional probability of the state sequence Y is: The word tokens in the window i =-2, -1, 0, 1, 2 1 ( | ) exp ( , ) ( ) j j j p Y X f Y X Z X \u03bb \u239b \u239e = \u239c \u239c \u239d \u23a0 \u2211 \u239f \u239f (3) ( ) exp ( , ) j j Y j Z X f \u03bb \u239b \u239e = \u239c \u239c \u239d \u23a0 \u2211 \u2211 YX\u239f \u239f (4) T i The NE tags i = -1 C i C i-1 The bigram of C i i = -1, 1 P i The POS tags of word tokens i = -1, 0, 1 P -1 P 1 The combination of POS tags T -1 C 0 The previous tag and the current word token B C i is Chinese family name C C i is part of Chinese first name W C i is Chinese whole name F C i is foreign name S C i is Chinese first name W(C i ) O o t h e r W(C i-1 )W(C i ) The bigram of W(Ci) i = -1, 1 IsInOrgDict(C 0 ) The current word token is in organization dictionary IsInPlaceDict(C 0 ) The current word token is in place dictionary Being Similar to the definition of CRF, Z(x) is the normalization factor. ( ) , j f Y X is the universal definition of features. Name entity recognition based on MEM Firstly, we use a segmentation tool to split both training and test corpus into word-token-based texts. Characters that are not in the dictionary are scattered in the texts. NE tags using in the model follow the tags in training corpus. Other word tokens that do not belong to NE are tagged as O. Based on the segmented text, the context window is also set as 5. Inspired by Zhang's (2006) work, there are 10 types of feature templates for generating features for NER model in Table 2 . When training our ME Model, the best performance can be achieved when cut-off value is set to 1. Maximum likelihood estimation and GIS algorithm is used to estimate the weight of parameters in the model. The iteration time is 500. Chinese part-of-speech tagging For POS tagging task, NCC corpus and PKU corpus are chosen both in the close test and open test. Transformation-based learning The formalism of Transformation-based learning is first introduced in 1992. It starts with the correctly tagged training corpus. A baseline heuristic for initial tag and a set of rule templates that specify the transformation rules match the context of a word. By transformating the error initial tags to the correct ones, a set of candidate rules are built to be the conditional pattern based on which the transformation is applied. Then, the candidate rule which has the best transformation effect is selected and stored as the first transformation rules in the TBL model. The training process is repeated until no more candidate rule has the positive effect. The selected rules are stored in the learned rule sequence in turn for the purpose of template correction learning. Part-of-speech tagging based on TBL POS tagging is a standard sequential labeling problem. CRF has some advantages to solve it. Because both corpora have relative many POS tags, our computational ability can not afford the CRF model in condition of these tags. TBL model is utilized to replace with CRF. We compute the max probability of current word's POS tag in training corpus. The POS tag which has max occurrence probability for each word is used to tag its word token. By this method, we got the initial POS tag for each word. The rule templates which are formed from conjunctions of words match to particular combinations in the histories of the current position. 40 types of rule templates are built using the patterns. The cut-off value of the transformation rules is set to 3 (Sun, 2007) . For open test, our NER system is used to tag the output of our POS tagging result. Parts of NE tags are corrected. Evaluation Following the measurement approach adopted in SIGHAN, we measure the performance of the three tasks in terms of the precision (P), recall (R), and F-score (F). The WS results are listed on the Table 3 . Some errors could be caused by the annotation differences between the training data and test data. For example, \"\u963f\u73cd\" (A Zhen) was considered as a whole word in training data, while \"\u963f\u5170\" (A Lan) was annotated as two separate word \"\u963f\" (A) and \"\u5170\" (Lan) in the test data. Some post-processing rules for English words, money unit and morphology can improve the performance further, Following are such errors in our results: \"vid eo\", \"\u65e5 \u5143\" (Japan yen), \"\u4e0d \u4e09 \u4e0d \u56db\" (not three not four). Word segmentation results For open test, we hoped to use NER module to increase the OOV recall. But the NER module didn't prompt the performance very much because it was trained by the MSRA NER data in Bakeoff3. The difference between two corpora may depress the NER modules effect. Also, the open test was done on the output of close test and all the errors were passed. Name entity recognition results The official results of our NER system on MSRA corpus for open track are showed in Table 4 . As it shows, our system achieves a relatively high score on both PER and LOC task, but the performance of ORG is not so good, and the Avg1 performance is decreased by it. The reasons are: (1) The ORG sequences are often very long and our system is unable to deal with the long term, a MEMM or CRF model may perform better. (2) The resource for LOC and ORG are much smaller than that of PER. More sophisticated features such like \"W(C i )\" may provide more useful information for the system. Conclusions Chinese lexical analysis system is built for the SIGHAN tracks which consists of Chinese word segmentation, name entity recognition and part-of-speech tagging. Conditional random fields, maximum entropy model and transformation-based learning model are utilized respectively. Our system achieves the medium results in all the three tasks.",
    "abstract": "This paper briefly describes our system in The Fourth SIGHAN Bakeoff. Discriminative models including maximum entropy model and conditional random fields are utilized in Chinese word segmentation and named entity recognition with different tag sets and features. Transformation-based learning model is used in part-of-speech tagging. Evaluation shows that our system achieves the F-scores: 92.64% and 92.73% in NCC Word Segmentation close and open tests, 89.11% in MSRA name entity recognition open test, 91.13% and 91.97% in PKU part-of-speech tagging close and open tests. All the results get medium performances on the bakeoff tracks.",
    "countries": [
        "China"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "2",
    "year": "2008",
    "month": "",
    "title": "A Study of {C}hinese Lexical Analysis Based on Discriminative Models"
}