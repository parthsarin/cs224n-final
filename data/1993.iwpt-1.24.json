{
    "article": "We describe a mechanism for automatically estimating frequencies of verb subcategorization frames in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a regular grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical method enables the system to learn patterns of errors based on a set of training samples and substantially improves the accuracy of the frequency estimation. Introduction When we construct a grammar, there is always a trade-off between the coverage of the grammar and the ambiguity of the grammar. If we hope to develop an efficient high-coverage parser for unrestricted texts, we must have some means of dealing with the combinatorial explosion of syn tactic ambiguities. While a general probabilistic optimization technique such as the Inside-Outside algorithm (Baker 1979 , Lauri and Yo ung 1990 , Jelinek et al. 1990 , Carroll and Charniak 1992) can be used to reduce ambiguity by providing es timates on the applicability of the context-free rules in a grammar (for example), the algorithm does not take advantage of lexical information, including such information as verb subcategoriza tion frame preferences. Discovering or acquiring lexically-sensitive linguistic structures from large corpora may offer an essential complementary ap proach. Verb subcategorization (verb-subcat) frames represent one of the most important elements of grammatical/lexical knowledge for efficient and reliable parsing. At this stage in the computational-linguistic exploration of corpora, dictionaries are still probably more reliable than automatic acquisition systems as a source of sub categorization (subcat) frames for verbs. The Oxford Advanced Learners Dictionary ( OALD) (Hornby 1989), for example, uses 32 verb patterns to describe a usage of each verb for each meaning of the verb. However, dictionaries do not pro vide quantitative information such as how often each verb is used with each of the possible subcat frames. Since dictionaries are repositories, pri marily, of what is possible, not what is most likely, they tend to contain information about rare us age (de Marken 1992) . But without information about the frequencies of the subcat frames we find in dictionaries, we face the prospect of having to treat each frame as equiprobable in parsing. This can lead to serious inefficiency. We also know that the frequency of subcat frames can vary by domain; frames that are very rare in one domain can be quite common in another. If we could au tomatically determine the frequencies of subcat frames for domains, we would be able to tailor parsing with domain-specific heuristics. Indeed, it would be desirable to have a subcat dictionary for each possible domain. parsing with domain-specific heuristics. Indeed, it would be desirable to have a subcat dictionary for each possible domain . . This paper describes a mechanism for auto matically acquiring subcat frames and their fre quencies based on a tagged corpus. The method utilizes a tagged corpus because (i) we don't have to deal with a lexical ambiguity (ii) tagged cor pora in various domains are becoming readily available and (iii) simple and robust tagging techniques using such corpora recently have been de veloped (Church 1988 , Brill 1992 Note that in the actual implementation of the procedure, all of the redundant operations are eliminated. Our NP parser also uses a fi\ufffdite-state grammar. It is designed especially to support identification of verb-subcat frames. One of its special features is that it detects time-adjuncts such as \"yesterday\" , \"two months ago\" , or \"the following day\" , and eliminates them in the tok enization process. For example, the sentence \"He told the reporters the following day that ... \" is tokenized to \"bivnc ... \" instead of \"bivnnc ... \". Experiment on Wall Street Journal Corpus We used the above method in experiments involv ing a tagged corpus of Wall Street Journal (WSJ) articles, provided by the Penn Treebank project. Our experiment was limited in two senses. First, we treated all prepositional phrases as adjuncts. Table 2: Set of Subcategorization Frame Extraction Rules We extracted two sets of tagged sentences from the WSJ corpus, each representing 3-MBytes and approximately 300,000 words of text. One set was used as a training corpus, the other as a test corpus. Table 2 gives the list of verb subcat frame extraction rules obtained ( via exam ination) for four verbs \"expect\" \"reflect\" \"tell\" ' ' ' and \"give\" , as they occurred in the training corpus. Sample sentences that can be captured by each set of rules are attached to the list. Table 3 shows the result of the hand comparison of the automatically identified verb-subcat frames for \"give\" and \"expect\" in the test corpus. The tabu lar columns give actual frequencies for each verb subcat frame based on manual review and the tabular rows give the frequencies as determined automatically by the system. The count of each cell ([i, j] ) gives the number of occurrences of the verb that are assigned the i-th subcat frame by the system and assigned the j-th frame by man ual review. The frame/column labeled \"REST\" represents all other subcat frames, encompassing such subcat frames as those involving wh-clauses, verb-particle combinations (such as \"give up\" ), and no complements. To measure the total accuracy of the system, we randomly chose 33 verbs from the 300 most frequent verbs in the test corpus (given in Ta ble 4), automatically estimated the subcat frames for each occurrence of these verbs in the test cor pus, and compared the results to manually deter mined su beat frames. The overall results are quite promising. The total number of occurrences of the 33 verbs in the test corpus (excluding participle forms) is 2,242. Of these, 1,933 were assigned correct subcat frames by the system. (The 'correct' assignment counts always appear in the diagonal cells in a comparison table such as in Table 3 .) This indicates an overall accuracy for the method of 86%. If we exclude the subcat frame \"REST\" from our statistics, the total number of occurrences of the 33 verbs in one of the six subcat frames is 1,565. Of these, 1,311 were assigned correct sub cat frames by the system; This represents 83% accuracy. For 30 of the 33 verbs, both the first and the second (if any) most frequent subcat frames as determined by the system were correct. For all of the verbs except one (\"need\" ), the most frequent frame was correct. puting the error rate, we divide the total 'off diagonal'-cell counts, excluding the counts in the \"REST\" column, by the total cell counts, again excluding the \"REST\" column margin. Thus, the off-diagonal cell counts in the \"REST\" row, rep resenting instances where one of the six actual subcat frames was misidentified as \"REST\" , are counted as errors. This formula, in general, gives higher error rates than would result from simply dividing the off-diagonal cell counts by the total cell counts. Overall, the most frequent source of errors, again, was errors in noun phrase boundary de tection. The second most frequent source was misidentification of infinitival 'purpose' clauses, as in \"he used a crowbar to open the door\". \"To open the door\" is a 'purpose' adjunct modifying either the verb phrase \"used a crowbar\" or the main clause \"he used a crowbar\". But such ad juncts are incorrectly judged to be complements of their main verbs by the subcat frame extrac tion rules in Table 2. In formulating the rules, we assumed that a 'purpose' adjunct appears ef fectively randomly and much less frequently than infinitival complements. This is true for our cor pus in general; but some verbs, such as \"use\" and \"need\" , appear relatively frequently with 'pur pose' infinitivals. In addition to errors from pars ing and 'purpose' infinitives, we observed several other, less frequent types of errors. These, too, pattern with specific verbs and do not occur ran domly across verbs. Statistical Analysis For most of the verbs in the experiment, our method provides a good measure of subcat frame frequencies. However, some of the verbs seem to appear in syntactic structures that cannot be captured by our inventory of subcat frames. For example, \"need\" is frequently used in rela tive clauses without relative pronouns, as in \"the last thing they need\". Since this kind of rela tive clauses cannot be captured by the rules in Table 2, each occurrence of these relative clause causes an error in measurement. It is likely that there are many other classes of verbs with dis tinctive syntactic preferences. If we try to add rules for each such class, it will become increas ingly difficult to write rules that affect only the USHIODA -EVANS -GIBSON -WA IBEL target class and to eliminate undesirable rule in teractions. In the following sections, we describe a sta tistical method which, based on a set of training samples, enables the system to learn patterns of errors and substantially increase the accuracy of estimated verb-subcat frequencies. First, the system is run on the new corpus to obtain an N-dimensional contingency table. This table is considered to be an X1 -X2 -\u2022 \u2022 \u2022 -XN marginal table. What we are aiming at is the Y margins that represent the real subcat frame fre quencies of the new corpus. Assuming that the training corpus and the new corpus are homo geneous (e.g., reflecting similar sub-domains or samples of a common domain), we estimate the Y margins using Bayes theorem on the fitted val ues of the training corpus by the formula given in Table 5 . General Scheme The method described in Section 2 is wholly de terministic; it depends only on one set of subcat extraction rules which serve as filters. Instead of treating the system output for each verb to ken as an estimated subcat frame, we can think of the output as one feature associated with the occurrence of the verb. This single feature can be combined , statistically, with other features in the corpus to yield more accurate characteriza tions of verb contexts and more accurate subcat frame frequency estimates. If the other features are capturable via regular-expression rules, they can also be automatically detected in the manner described in the Section 2. For example, main verbs in relative clauses without relative pronouns may have a higher probability of having the fea ture \"nnk\" , i.e., \"(NP )(NP )(VERB)\". More formally, let Y be a response vari able taking as its value a subcat frame. Let X1 , X2 , ... , XN be explanatory variables. Each Xi is associated with a feature expressed by one or a set of regular expressions. If a feature is ex pressed by one regular expression (R), the value of the feature is 1 if the occurrence of the verb matches R and O otherwise. If the feature is ex pressed by a set of regular expressions, its value is the label of the regular expression that the oc currence of the verb matches. The set of regu lar expressions in Table 2 can therefore be con sidered to characterize one explanatory variable whose value ranges from (NP +NP ) to (REST). Now, we assume that a training corpus is avail able in which all verb tokens are given along with their subcat frames. By running our system on the training corpus, we can automatically gen erate a (N + I)-dimensional contingency table. Table 3 is an example of a 2-dimensional contin gency table with X = E(Y = k I X1 -X2 -\u2022 \u2022 \u2022 -XN marginal table of the new corpus) = LL ... LMi i 2 . . \u2022i N +P(Y = k l X1 = i1 ,X2 = i2 , \u2022\u2022 \u2022 ,XN = iN) i1 i2 i N where M 1 i2 \u2022\u2022 \u2022 i n+ is the cell count of the X1 -X2 -\u2022 \u2022 \u2022 -XN marginal table of the new corpus obtained as the system output, and Mi i i 2 ... i N k is the fitted value of the (N + I)-dimensional contingency table of the training corpus based on a particular loglinear model. Lexical Heuristics The simplest application of the above method is to use a 2-way contingency table, as in Table 3 . There are two possibilities to explore in construct ing a 2-way contingency table. One is to sum up the cell counts of all the verbs in the training corpus and produce a single (large) general ta ble. The other is to construct a table for each verb. Obviously the former approach is prefer able if it works. Unfortunately, such a table is typically too general to be useful; the estimated frequencies based on it are less accurate than raw system output. This is because the sources of errors, viz., the distribution of off-diagonal cell counts of 2-way contingency tables, differ consid erably from verb to verb. The latter approach is problematic if we have to make such a table for each domain. However, if we have a training cor pus in one domain, and if the heuristics for each verb extracted from the training corpus are also applicable to other domains, the approach may work. To test the latter possibility, we constructed a contingency table for the verb from the test corpus described in the Section 3 that was most problematic (least accurately estimated) among the 33 verbs-\"need\" . Note that we are using the test corpus described in the Section 3 as a train ing corpus here, because we already know both the measured frequency and the hand-judged fre quency of \"need\" which are necessary to construct a contingency table. The total occurrence of this verb was 75. To smooth the table, 0.1 is added to all the cell counts. As new test corpora, we extracted another 300,000 words of tagged text from the WSJ corpus (labeled \"W3\" ) and also three sets of 300,000 words of tagged text from the Brown corpus (labeled \"Bl\", \"B2\" , and \"B3\" ), Table 6: Statistical Estimation (Unit = %) for the Ve rb \"Need\" as retagged under the Penn Treebank tagset. All the training and test corpora were reviewedand judged -by hand. Table 6 gives the frequency distributions based on the system output, hand judgement, and sta tistical analysis. (As before, we take the hand judgement to be the gold standard, the actual frequency of a particular frame.) After the Y margins are statistically estimated, the least es ti\ufffdated Y values less than 1.0 are truncated to 0. (These are considered to have appeared due to the smoothing.) In all of the test\u2022 corpora, the method gives very accurate frequency distribution estimates. Big gaps between the automatically-measured and manually-determined frequencies of \"NP \" and \"REST\" are shown to be substantially re duced through the use of statistical estimation. This result is especially encouraging because the heuristics obtained in one domain are shown to be applicable to a considerably different domain. Furthermore, by combining more feature sets and making use of multi-dimensional analysis, we can expect to obtain more accurate estimations. Conclusion and Future Di rection We have demonstrated that by combining syn tactic and multidimensional statistical analysis, the frequencies of verb-subcat frames can be esti mated with high accuracy. Although the present system measures the frequencies of only six sub cat frames, the method is general enough to be extended to many more frames. Since our current focus is more on the estimation of the frequen cies of subcat frames than on the acquisition of frames themselves, using information on subcat frames in machine-readable dictionaries to guide the frequency measurement can be an interesting direction to explore. The traditional application of regular expres sions as rules for deterministic processing has self evident limitations since a regular grammar is not powerful enough to capture general linguistic phe nomena. The statistical method we propose uses regular expressions as filters for detecting specific features of the occurrences of verbs and employs multi-dimensional analysis of the features based on loglinear models and Bayes Theorem. We expect that by identifying other useful syntactic features we can further improve the ac curacy of the frequency estimation. Such features can be regarded as characterizing the syntactic context of the verbs, quite broadly. The features need not be linked to a local verb context. For ex ample, a regular expression such as \"w [-vex] \u2022k\" can be used to find cases where the target verb is preceded by a relative pronoun such that there is no other finite verb or punctuation or sentence final period between the relative pronoun and the target verb. If the syntactic structure of a sentence can be predicted using only syntactic and lexical knowl edge, we can hope to estimate the subcat frame of each occurrence of a verb using the context ex pressed by a set of features. We thus can aim to extend and refine this method for use with gen eral probabilistic parsing of unrestricted text. Acknowledgements We thank Te ddy Seidenfeld, Jeremy Yo rk, and Alex Franz for their comments and discussions with us. W3 NP+NP NP+CL NP+INF NP CL INF",
    "abstract": "We describe a mechanism for automatically estimating frequencies of verb subcategorization frames in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a regular grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical method enables the system to learn patterns of errors based on a set of training samples and substantially improves the accuracy of the frequency estimation.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "2",
    "year": "1993",
    "month": "August 10-13",
    "title": "Frequency Estimation of Verb Subcategorization Frames Based on Syntactic and Multidimensional Statistical Analysis"
}