{
    "article": "Computational Argumentation in general and Argument Mining in particular are important research fields. In previous works, many of the challenges to automatically extract and to some degree reason over natural language arguments were addressed. The tools to extract argument units are increasingly available and further open problems can be addressed. In this work, we are presenting the task of Aspect-Based Argument Mining (ABAM), with the essential subtasks of Aspect Term Extraction (ATE) and Nested Segmentation (NS). At the first instance, we create and release an annotated corpus with aspect information on the token-level. We consider aspects as the main point(s) argument units are addressing. This information is important for further downstream tasks such as argument ranking, argument summarization and generation, as well as the search for counter-arguments on the aspect-level. We present several experiments using stateof-the-art supervised architectures and demonstrate their performance for both of the subtasks. Introduction The field of computational argumentation (Slonim et al., 2016) gained a lot of interest in the last couple of years. This is noticeable from both the number of the submitted publications related to this field and also from the high volume of emerging datasets (Aharoni et al., 2014; Levy et al., 2017; Habernal et al., 2018; Stab et al., 2018; Trautmann et al., 2020a) , specific task formulations (Wachsmuth et al., 2017; Al-Khatib et al., 2020) and models (Kuribayashi et al., 2019; Chakrabarty et al., 2019) . Similar to aspect-based sentiment analysis (Pontiki et al., 2014) , we also see the possibility of breaking down arguments into smaller attributes or meaningful components in the argument mining domain. We consider these components as aspects of the arguments. Previous works already utilized aspectinformation for several subtasks within the argument mining domain (Fujii and Ishikawa, 2006; Misra et al., 2015; Gemechu and Reed, 2019) . However, these works vary significantly in the definition of aspects and do not focus on the aspect-based argument mining explicitly, e.g., employ aspects as a source of side or additional information. For instance, Fujii and Ishikawa (2006) are mainly focusing on the summarization of opinions, visualizing pro and contra arguments for a given topic. Thereby, the authors are extracting aspects, calling them points at issue, and ranking the arguments according to them. However, their approach relies on rule-based extraction solely. In Misra et al. (2015) , the authors are proposing summarization methods to recognize specific arguments and counter-arguments in social media texts, to further group them across discussions into facets (i.e., aspects) on which that issue is argued. Still, this work is limited to a couple of topics and samples. Finally, Gemechu and Reed (2019) also mention aspects as part of four functional components, where the authors interchangeably label aspects and concepts for the specific words. However, to the best of our knowledge, the authors did not publish their labeled data, making a comparative evaluation of aspect extraction methods impossible. We, in contrast, specifically address the aspect term Supporters say it is an unnecessary regulation designed to force clinics to shut down, while opponents say the prohibition protects women's health. Topic: Abortion Granted, the initial construction costs of a nuclear plant are huge, but the ongoing maintenance and fuel costs have proven to be far lower than that of other energy sources. One of the potential applications for the ABAM is the ability to search for specific subtopics within a larger controversial area. For instance, for the topic abortion, one can particularly be interested in regulation or health-related aspects (first example in Figure 1 ). Whereas for the topic of nuclear energy, one can care for solely enviromental, costor safety-related aspects (second example in Figure 1 ). By searching or filtering for the particular aspects, one has the possibility to select for specific information and, therefore, to get more fine-grained results. Another benefit is the ability to compare opposing arguments on the aspect-level. Topic: Nuclear Energy In this regard, necessary subtasks within the ABAM include the explicit Aspect Term Extraction (ATE) on token-level and the Nested Segmentation (NS) of argumentative parts along with their aspects within a given sentence. Our work is based on Trautmann et al. (2020a) , where the authors already addressed the task of argument unit segmentation. We extend their benchmark with aspect term extraction on these argument units. The ABAM task can be performed in two ways: first, as a two-step pipeline approach with argument unit recognition and classification (AURC) followed by aspect term extraction, or as an end-to-end approach in the form of the nested segmentation task. Since the argument units are already provided by Trautmann et al. (2020a) , we can use them directly for the second step in the pipeline, namely the ATE task. Whereas in the end-to-end scenario we adress both tasks (i.e., AURC and ATE) simultaneously for argumentative sentences. One of the main challenges we faced during this work was the absence of publicly available benchmarks containing the aspect terms. Existing argument mining datasets do not contain the required information and therefore could not be directly applied for Aspect-Based Argument Mining. We address this challenge by extending an existing fine-grained argument corpus (Trautmann et al., 2020a) with crowdsourced token-level aspect information. This is our focused main contribution. While annotating the corpus, we were faced multiple difficulties, including the proper definition of aspects and the creation of rules required for the aspect extraction. It is important to note, that within this work, we refer to aspects as the main point(s) arguments are addressing. Last but not least, since we are extending the existing corpus, we do not explicitly concentrate on the stance definition and its annotation. Furthermore, as stated in Trautmann et al. (2020a) , there are two main argument mining directions: closed domain discourse-level and the argument mining from the information seeking perspective. The authors of the underlying corpora follow the latter and provide the reasons for that in their work. We, therefore, adopt their vision on that point. Summarizing the abovementioned points, our contribution within this work is as follows: \u2022 We are emphasizing and presenting the task of Aspect-Based Argument Mining on its own. \u2022 We are extending an existing corpus with token-level aspect terms, making a comparative evaluation of ABAM methods possible. \u2022 We are presenting a number of strong baselines with a corresponding error analysis. Problem Statement We define the ABAM task as following: Given a list of several topic related texts (documents or paragraphs), we segment the texts into N sentences sentence i = [t 1 , t 2 , t 3 , . . . , t n ] (1) The problem is to select, if available, one (or several) span(s) span j = [t k , . . . , t l ] (2) inside each sentence i , with k >= 1, l <= n, l \u2212 k >= SEG min and l \u2212 k <= SEG max (with SEG min = 3 tokens and SEG max = n tokens in a segment), and a corresponding stance stance j \u2208 [P RO, CON ] (3) Tokens outside of argumentative spans are assigned the N ON stance label. Furthermore, regularly there is at least one aspect in every selected span with aspect j = [t p , . . . , t q ] ( 4 ) where p >= k, q <= l, q \u2212 p >= ASP min and q \u2212 p <= ASP max (with ASP min = 1 token and ASP max = 5 tokens per aspect). Related Work Regarding the abovementioned problem definition ( \u00a72), we selected three research areas as thematically closed to our task. Sentiment Analysis: The SemEval workshop organized the task of aspect-based sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016) . Its subtasks also involved the aspect term extraction, which mainly inspired our approach and definition of the aspect term. Recent works applied adversarial training of pretrained language models (Karimi et al., 2020) and a combination of contextualized embeddings and hierarchical attention (Trusca et al., 2020) for new state-of-the-art results on this tasks. Argument Mining: In our work we adopt the definition of argument facets from the previous work and adjust it for our task. For instance, Misra et al. (2015) used the information on argument facets for the summarization of arguments in social media. Furthermore, the authors used argument facets for the argument similarity task (Misra et al., 2016) . The abovementioned works were a first approach in the area of argument facet extraction and were limited to solely a couple of topics and samples. Recent work extended this approach to 28 topics and used the aspect information for the argument similarity task and argument clustering (Reimers et al., 2019) . However, the focus of Reimers et al. (2019) was on the pairwise classification of argumentative sentences and not on the aspect term extraction task itself. Lastly, the work by Bar-Haim et al. (2020) defined argument key-points to create concise summaries from a large set of arguments. Nested Named Entity Recognition: The task of nested-NER is similar to the nested segmentation task ( \u00a75.1.2) that we propose. Early work (Finkel and Manning, 2009) presented newspaper and biomedical corpora, and modeled the data by manual feature extraction. Recent works proposed recurrent neural networks (Katiyar and Cardie, 2018) and sequence-to-sequence (Strakov\u00e1 et al., 2019) approaches. The latter modeled nested labels as multilabels, a method that we also adopted for our task with overlapping stance and aspect labels. Corpus Creation The creation of the ABAM benchmark is based on the argument units from the AURC corpus (Trautmann et al., 2020a) and is divided into two main parts. The first part addresses two studies for the annotation task formulation, whereas the second part describes the final corpus creation. We outsourced the data annotation to independent (crowd-)annotators and based on their results we created the gold labels. Expert Study We conducted two expert studies on random samples of ten argument units per stance and topic, selected from the AURC corpus. The resulting sets contained 160 samples for each study. Token-Level Annotation The first expert study task was to select explicit aspect terms from a given argument unit on the tokenlevel. Two graduate domain experts performed the annotation. Experts were free to select every inputtoken which fits the following task description: \"The aspects are defined as the most important point(s) the argument unit is addressing\". After the annotation step, the Inter-Annotator Agreement (IAA) for the 160 samples was computed. We decided for Cohen's \u03ba (Cohen, 1960) as our agreement measure, that resulted in the initial score of 0.538. According to Viera et al. (2005) , this score is in the moderate agreement range. Furthermore, the primary analysis of the selected aspect terms from both annotators yielded a list of especially frequent part-of-speech (PoS) patterns for the selected tokens. To further improve the annotation process, the PoS information was employed in the second expert study. Candidates Selection The aspect candidate selection step is crucial for the correct aspect term extraction task. To select the aspect candidates for the second study, we rely on the part-of-speech information. Specifically, the PoS patterns that occurred more than twice in the previous expert study (i.e., token-level annotation) where picked, and some additional PoS patterns were defined (e.g., the singular and plural form of nouns). The tag set is based on the Part-of-Speech tags used in the Penn Treebank Project 1 and the stanza NLP library 2 . The final PoS pattern list is comprehensive and representative (includes 44 patterns, see Table 1 ), and ensures linguistically and grammatically correct candidates, without affecting the actual discourse. These PoS patterns were applied on a different set of 160 random samples to create a list of aspect term candidates for every argument unit. The total count of unique aspects for all topics is 4525, but the sum of all unique aspects per topic is 5485. This is due to some aspects appearing in several topics (c.f. Table 3 ). The annotators were asked to solve the same task as before, but now by selecting one or several options from the aspect term candidates list. If none of the aspect term candidates were appropriate, the option NONE was selected. This simplification of the task, compared to the first study, led to a raised Cohen's \u03ba of 0.790. This is considered as a substantial agreement (Viera et al., 2005) and we deem this as a viable approach for the aspect term extraction. Corpus Annotation Based on the insights from the first two studies, the annotation guidelines ( \u00a7A) were extended with clearer task formulations and examples. Additionally, the final set of PoS patterns (full list in Table 1 ) was applied on all argument units from the AURC corpus. The AURC corpus was slightly preprocessed to account for duplicates on the sentence-and segment-level, as well as on some minor errors on span boundaries. Two independent (crowd-)annotators with a linguistic background and a minimum professional working proficiency in English were recruited for the aspect term extraction task. The annotation procedure was the same as described in \u00a74.1.2. The inter-annotator agreement score for the two expert annotators resulted in a Cohen's \u03ba of 0.874 for all eight (8) topics. This is considered as an almost perfect agreement (Viera et al., 2005) . Annotation Merge For the gold standard we selected the annotations where both of the annotators agreed on the token-level. This ensured that we always had a selection of aspects if neither of the annotators selected the NONE option. Additionally, shorter aspect terms are favoured by this annotation merge. Gold Standard The final descriptive statistics of the ABAM corpus are depicted in the Table 2 . There are 12040 aspects in total and 4525 unique (lemmatized) aspects. The topic with the most segments (T8 in Table 2 ), also yielded the most total aspects (2019). Furthermore, there are 58.10% of the aspects with only one token, 32.12% with 2 tokens, 7.94% with 3 tokens, 1.73% with 4 tokens and only 0.12% with 5 tokens. Common Aspects In further aspect analysis we aggregated the most common aspects for the eight topics. The top five aspects and the absolute occurence counts per topic, are shown in Table 3 . Furthermore, three aspects (life, problem, government) appeared in all eight topics and the aspects people, cost, society, risk, law appeared in seven topics. Experimental Setup This section presents our experimental setup regarding the two tasks, the employed models and the data set splits. Table 3 : The top 5 most common aspects per topic and for aspects that appear in several topics. Tasks In this work we apply the two different, but related, sub-tasks for ABAM in the sequence labeling formulation, following Akhundov et al. (2018) . Aspect Term Extraction In the first task (ATE), we employ only the aspect term information within the segments (argument untis). This sequence labeling task is a binary classification problem per token. Nested Segmentation In the second task (NS), we utilize full argumentative sentences (like the examples in Figure 1 ) with the stance (PRO, CON, NON) and aspect (O, ASP) information for every token as our input. We extend the stance labels with the aspect information for a total set of five possible combinations ( [NON,O], [PRO,O], [PRO,ASP], [CON,O], [CON,ASP]). 3 This is a multiclass sequence labeling problem, which solves both the argument unit segmentation and the aspect term extraction tasks. Models BERT For the two subtasks, we decided for the BERT model (Devlin et al., 2019) as a recent state-ofthe-art system on a number of natural language processing tasks. We utilize the base and large versions of BERT, as well as both versions of the models with an additional CRF-Layer (Sutton et al., 2012) as the final classification layer in the architecture. Further information about hyperparameter search and computing infrastructure are in \u00a76.2, \u00a7B and \u00a7C. PoS Patterns Additionally, we applied the PoS-patterns from the aspect candidates creation step we used in \u00a74. For the ATE task we labeled all tokens that match the PoS-patterns and report the results as the lower boundary of our approaches. Evaluation As the evaluation metric, we report the macro-F1 scores 4 for both of our tasks. Further information about accuracy, precision and recall can be found in \u00a7D. Table 5 : Sample counts per set and domain for the nested segmentation task. Inner-Topic & Cross-Topic For a better understanding of the model performance, we followed the two different dataset splits (domains) as they were defined for the AURC corpus (Trautmann et al., 2020a) . In the inner-topic split we trained, evaluated and tested our models on the same set of topics (T1-T6, Table 2 ). In the cross-topic split we trained our model on T1-T5, selected the best hyperparameter from the evaluation on T6 and tested on T7 and T8. Detailed sample counts are shown in Table 4 and Table 5 for each task, domain and set. Results This section presents the results for our tasks as described in \u00a75.1. Tasks Aspect Term Extraction The best performing options are the BERT LARGE models (Table 6 ). Both of them perform similar, but the one with the CRF-layer is slightly better on the development set for inner-topic and the test set for the cross-topic. The inner-topic scores are higher compared to the more challenging cross-topic set-up, were we evaluate the models on unseen topics. All the models performed much better than the lower boundary from the PoS-Patterns Matches. However, this scores are still bellow the human performance of 0.895. The human performance on this task is based on the results from the second expert study ( \u00a74.1.2) Nested Segmentation The results for NS (Table 7 ), show that the BERT LARGE model outperforms the other listed approaches, except for the development set in the inner-topic set-up. Furthermore, the cross-topic set-up is also more challenging for this task, compared to the inner-topic setting. Table 7 : F1 results on the dev and test sets for the inner-topic (INNER) and cross-topic (CROSS) set-ups for the nested segmentation task. Hyperparameters For our experimental setup with BERT, we fine-tuned the whole (standard) base and large models, as well as both models with an additional final CRF-Layer. We selected the hyperparameters on the development sets and in particular the learning rate (range: 0.00001 -0.00009 in 0.00001 steps) and the dropout rate (range: 0 -0.5 in 0.1 steps). We used grid search, to cover all possible combinations. The model parameters were optimized with AdamW (Loshchilov and Hutter, 2018) . The training batch size was 32. Our reported results are the averages from three runs and one epoch took about 1 minute for the base models and less than 2 minutes for the large models on average. We fine-tuned for 10 epochs in the ATE task and for 20 epochs in the NS task. Detailed numbers of the final hyperparameters for each model and task can be found in the tables in the appendix \u00a7B. Error Analysis Recalling our definition of aspects: They are defined as the main point(s) argument units are addressing. Furthermore, considering our annotation guidelines in \u00a7A, the most important point is usually not equal to the given main topic. An overview of the main errors found during the evaluation of the development sets for the best performing models in the inner-and cross-topic set-ups, is given below. Aspect Term Extraction During the evaluation of ATE results, we observed a number of errors, which we grouped into the following categories: \u2022 The models tend to favour NOUNS in general. \u2022 Topic words, such as abortion or marijuana legalization, are often selected as aspects, which is in conflict with our guidelines. \u2022 Phrase constructions like thread of ... are often selected as a whole aspect by the models. For the benchmark, we, in contrast, focus on the main representative word of such constructions (e.g., suicide vs. thread of suicide). \u2022 In the case of ADJECTIVE+NOUN, we suggest to avoid general adjectives (e.g. new in new treatments), whereas focused adjectives that are part of the concept should be selected (e.g. recreational in recreational marijuana). Our observation is, that models in general could not sufficiently differentiate between such adjectives. \u2022 Models lack the understanding of domain-specific phrasems like in vitro fertilisation or life without parole and tend to select only the nominalized part of them (e.g., fertilisation, parole). Overall the inner-topic set-up achieved much better performace compared to the cross-topic set-up and both models showed significantly better results over the PoS-Patterns Matches baseline. However, in the cross-topic set-up we faced more repeated errors, such as the tendency to select topic words as aspects and not sufficient understanding of domain-specific phrasems. Nested Segmentation The typology of the main errors in the NS task is similar to the ATE task. Additionally, in the NS task, a number of errors occured due to the wrong assigment of the stance labels, especially in the cross-topic set-up. These results confirm the insight from Trautmann et al. (2020a) , where most of the errors arose due to the wrong stance classification. Apparently, the BERT-based models tend to attach to sentiment words for the stance predictions, which is not always correlated. Conclusion ABAM is a challenging task that, to the best of our knowledge, was not directly addressed before. We made two important contributions: First, we created and released a publicly available benchmark for Aspect-Based Argument Mining. Second, we showcased several baselines for the two subtasks, namely the Aspect Term Extraction and the Nested Segmentation, and performed an elaborative error analysis. We believe that these findings as well as the benchmark are of high potential for further downstream tasks, such as argument ranking, argument summarization and the search for counter-arguments on the aspect-level. For the future work, we foresee the investigation of unsupervised approaches for the Aspect Term Extraction task, since they showed promising results within the Aspect-Based Sentiment Analysis domain. Furthermore, it would be of high interest to incorporate topic-specific knowledge (e.g., understanding of phrasems) into the models to address the discussed error types. In another line of work, one could also explore distant supervision (Rakhmetullina et al., 2018) or domain adaptation methods (M\u00e4rz et al., 2019) , as well as relational approaches (Trautmann et al., 2020b) A Annotation Guidelines Annotation guidelines defined for the Aspect Term Extraction task in Aspect-Based Argument Mining. Task Description \u2022 Given a main topic and an argumentative segment (unit), please select one or several options from the aspect candidates list. \u2022 If no aspect candidate could be selected from the list, pick the option None. While selecting the aspects, please consider the following rules: \u2022 An aspect is defined as the most important/relevant point for the argument made. \u2022 The most important point is usually not equal to the given main topic. \u2022 In case of doubt, shorter aspects candidates (generic terms; e.g. \"life span\") are prefered over longer candidates (e.g. \"prolonged life span\"). General Hints \u2022 The selected aspect(s) should be related to the topic in general. \u2022 The presence of AND/OR (usually) denote multiple aspects: -If a sentence contains multiple phrases (e.g., \"abortion causes breast cancer AND it kills unborn children.\"); -If there is an enumeration and objects connected by AND/OR (e.g. \"abortion causes breast cancer, infertility and pain.\"); \u2022 In the case of ADJECTIVE+NOUN, general adjectives should be avoided (e.g. \"new\" in \"new treatments\"), whereas focused adjectives that are part of the concept should be selected (e.g. \"recreational\" in \"recreational marijuana\"). \u2022 Please, use these test-questions for yourself while annotating: -Do you want this argument to be shown to someone, if they select this aspect(s) of the topic, or are other aspect terms in this argument more relevant for the point made? -Which words make you understand the argument most? -Which words are the most relevant and mainly form the meaning of the argument made? -If you would compress the argument into a few most relevant words, which words would that be? B Hyperparameters The dropout rate of 0.1 was always the best option. The learning rates for the different models are displayed in Table 8 for the ATE task and in Table 9 for the NS task. C Compute Resources We used Kaggle's Kernels 5 for the processing of the data and Google's Colab 6 for the training (finetuning) of our models. The former service offers a single 12GB NVIDIA Tesla K80 GPU, while the latter a single 16GB NVIDIA Tesla P100 GPU. D Additional Results The additionally reported numbers for accuracy, precision and recall can be found in the Table 11 : Accuracy (acc.), precision (pre.) and recall (rec.) results on the dev and test sets for the innertopic (INNER) and cross-topic (CROSS) set-ups for the nested segmentation task (args). These are the average scores from three runs. ",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 0.0,
        "none": 0.999754187196919
    },
    "reasoning": "Reasoning: The article does not provide any specific information regarding funding sources for the research presented. Without explicit mention of support from defense, corporate entities, research agencies, foundations, or an indication of no funding, it is not possible to accurately determine the funding sources based solely on the provided text."
}