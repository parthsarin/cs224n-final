{
    "article": "Dialogue summarization helps users capture salient information from various types of dialogues has received much attention recently. However, current works mainly focus on English dialogue summarization, leaving other languages less well explored. Therefore, we present a multi-lingual dialogue summarization dataset, namely MSAMSum, which covers dialogue-summary pairs in six languages. Specifically, we derive MSAMSum from the standard SAMSum (Gliwa et al., 2019) using sophisticated translation techniques and further employ two methods to ensure the integral translation quality and summary factual consistency. Given the proposed MSAMum, we systematically set up five multi-lingual settings for this task, including a novel mix-lingual dialogue summarization setting. To illustrate the utility of our dataset, we benchmark various experiments with pre-trained models under different settings and report results in both supervised and zero-shot manners. We also discuss some future works towards this task to motivate future researches 1 . !\"#$%&'() *+,-./01234 5678%&'()* Introduction Recent years have witnessed increasing interest in dialogue summarization (Feng et al., 2021a; Tuggener et al., 2021) . It aims to distill the most important information from various types of dialogues, which can alleviate the problem of communication data overload. Towards this research direction, various datasets have been proposed to promote this task. The AMI (Carletta et al., 2005) and ICSI (Janin et al., 2003) datasets provide the initial opportunity for meeting summarization. With the advent of data-hungry neural models and pre-trained language models, Gliwa et al. (2019) come up with the first high quality large-scale dialogue summarization dataset, namely SAMSum, which resurges this  task. Then, various datasets are proposed to meet different needs and scenarios (Chen et al., 2021a; Malykh et al., 2020; Rameshkumar and Bailey, 2020; Zhong et al., 2021; Zhu et al., 2021; Chen et al., 2021b; Zhang et al., 2021; Fabbri et al., 2021) . Despite the encouraging progresses achieved, current works overwhelmingly focused on English. Meanwhile, with the help of instantaneous translation systems 2 , a dialogue involving multinational participants becomes more and more common and frequent. Therefore, it is valuable to provide them with dialogue summaries in a preferred language. To this end, we propose a multi-lingual dialogue summarization task. The practical benefits of this task are twofold: it not only provides rapid access to the salient content, but also enables the dissemination of relevant content across participants of other languages. Intuitively, to achieve this goal, we need to answer two key questions, one is Where do we get data resources for this multi-lingual research? the other is How do we perform various multi-lingual settings? For the first question, we seek for potential available resources that can support our multi-lingual research. Although creating English datasets has proven feasible, the need for dialogues and summary-written experts in different languages makes the collection of multi-lingual datasets highly costing or even intractable. To mitigate this challenge, we devote our efforts to constructing the multi-lingual dataset via sophisticated translation techniques following Zhu et al. (2019) . Firstly, we select SAMSum (Gliwa et al., 2019) as our source English dataset because of its large scale and wide domain coverage. Then, we translate it into five other official languages of the United Nations via high-performance translation API, including Chinese, French, Arabic, Russian and Spanish. Furthermore, We employ two methods: round-trip translation and textual entailment to filter out lowquality translations and ensure the factual consistency at both the dialogue-level and summary-level. Finally, we obtain our MSAMSum dataset as the data resource for this multi-lingual research. For the second question, given the wellconstructed MSAMsum dataset, we set up various settings for our multi-lingual dialogue summarization task, including ONE-TO-ONE, MANY-TO-ONE, ONE-TO-MANY and MANY-TO-MANY. The ONE-TO-ONE setting can be further divided into Mono-lingual and Cross-lingual settings. To further boost the research on multi-lingual dialogue summarization, we creatively propose one new setting, namely MIX-TO-MANY, which takes a mixlingual dialogue as input and produce summaries in different languages. This setting is in line with the real world scenario that multinational participants can use their mother tongue to communicate with each other by means of instantaneous translation systems (depicted in Figure 1 ). To sum up, we set up five settings for the research on the whole scene of multi-lingual dialogue summarization. To illustrate the utility of our MSAMSum, we conduct extensive experiments under five multilingual settings based on the current multi-lingual pre-trained model mBART-50 (Tang et al., 2020) , and evaluate it in both supervised and zero-shot manners. The results reveal the feasibility of multilingual dialogue summarization task. The case study also shows that the multi-lingual model is able to produce fluent and factual consistency summaries in different languages. We further conclude several future works to prompt future researches. Related Work Multi-lingual Summarization Multi-lingual summarization is a valuable research direction, which can benefit users from various countries (Cao et al., 2020; Wang et al., 2022) . Especially, cross-lingual summarization, which receives a document in a source language and produces a summary in a another language, has attracted lots of research attentions (Wan et al., 2010) . For a long time, pipeline systems combining both machine translation and summarization tools are used to solve this problem (Ouyang et al., 2019) . However, pipeline systems do have their own drawbacks, like error propagation and system latency. Therefore, researchers turn to end-to-end neural methods. Zhu et al. (2019) first propose two cross-lingual summarization datasets using machine translation techniques. Afterwards, various models (Zhu et al., 2020b; Xu et al., 2020; Wang et al., 2021) and datasets (Ladhak et al., 2020; Hasan et al., 2021; Varab and Schluter, 2021) are proposed for this task. These works have achieved great progresses and have proved the feasibility of end-to-end multi-lingual summarization. In this paper, for the first time, we study the dialogue summarization task under various multi-lingual settings. Dialogue Summarization The earlier publicly available meeting datasets AMI (Carletta et al., 2005) and ICSI (Janin et al., 2003) have prompted dialogue summarization for a long time. Recently, the introduction of SAMSum dataset has resurged this direction. Researchers propose various methods to tackle this problem by incorporating auxiliary information, modeling the interaction and dealing with long input sequences (Chen and Yang, 2020; Feng et al., 2021b; Zhu et al., 2020a; Feng et al., 2021c) . Additionally, various valuable datasets are carried out to meet different needs, which further accelerate the development of dialogue summarization (Zhong et al., 2021; Zhu et al., 2021; Zhang et al., 2021) . What is more, Mehnaz et al. (2021) study dialogue summarization under the Hindi-English code-switched setting and get the best performance based on multilingual pre-trained language models. Nonetheless, the current datasets and models are mainly tailored for English, which leave other languages less well explored. To mitigate this challenge, we propose the MSAMSum to study the multi-lingual dialogue summarization task. The MSAMSum Dataset In this section, we introduce our MSAMSum dataset, including (1) Why we choose SAMSum dataset? (2) How we translate the original SAM-Sum dataset? (3) How we control the translation quality? and (4) Statistics for the newly created MSAMSum dataset. The whole dataset construction process is shown in Figure 2 . Dataset Selection Current dialogue summarization datasets are mainly tailored for English (Gliwa et al., 2019; Chen et al., 2021a,b; Zhang et al., 2021) , resulting in existing works not centring on other languages. In order to support our multi-lingual research, we follow Zhu et al. (2019) , which uses state-of-the-art machine translation techniques to construct datasets in different languages. Before launching the translation of the current dataset, we first need to choose a suitable dataset. After carefully comparing several datasets, we finally choose SAMSum (Gliwa et al., 2019) as our source English dataset according to the following two reasons: (1) it is a human-labeled large-scale dataset; (2) it covers a wide range of domains. Machine Translation For each dialogue-summary pair in the selected English SAMSum dataset (shown in Figure 2 (a)), we translate the utterances and the summary to the target language (shown in Figure 2(b) ) via high-performance machine translation service 3 . To make our work more representative and generalized, we choose five other official languages of the United Nations as our translation target languages 4 . Note that for each dialogue, we perform the translation at the utterance-level since machine translation can achieve good results with utterances of moderate length. After this process, we can get dialogue-summary pairs in Chinese (Zh), French (Fr), Arabic (Ar), Russian (Ru), Spanish(ES) and also original English (En). Quality Controlling To ensure the data quality, we further leverage two quality controlling methods. First, we employ round-trip translation strategy at both dialogue and summary level to filter out low-quality translations. Second, at the summary level, we use textual entailment strategy to verify factual consistency. Round-trip Translation Round-trip translation is the process of translating a text into another language (forward translation), then translating the result back into the original language (back translation), using MT service. Given the translated dialogue-summary pair in target language (shown in Figure 2(b )), we back-translate it into the original English version (shown in Figure 2(c) ). Afterward, we follow Zhu et al. (2019) and calculate the ROUGE-1 score (Lin, 2004) between the original dialogue-summary pair and the backtranslated dialogue-summary pair (shown in Figure 2(d) ). In detail, we first calculate the ROUGE-1 score for the corresponding utterances and the sum- mary respectively, and then get the final ROUGE-1 score by averaging all scores. If the final ROUGE-1 score exceeds the pre-defined threshold, the translated dialogue-summary pair (shown in Figure 2(b) ) is retained. Otherwise, the pair will be filtered 5 . Textual Entailment Since the summary serves as the core part of dialogue summarization, it not only needs coarsegrained surface-level high quality but also finegrained factual consistency (Huang et al., 2021) . To this end, we adopt the textual entailment method to access whether the translated summary is consistent with the original summary. Specifically, we obtain the entailment score for the translated English summary and the original English summary via state-of-the-art entailment model 6 , as shown in Figure 2 (e). If the entailment score exceeds the predefined threshold, the translated dialogue-summary pair is retained. Otherwise, the pair will be filtered. Datasets Alignment and Statistics Following the above steps, we can get translated and pure datasets in different languages. Note that these datasets are of different sizes, which is caused by the quality controlling process. To unify our experiments, we get the intersection of these datasets in six languages, resulting in the final MSAMSum dataset (statistics in Multi-lingual Settings In this section, we introduce various multi-lingual dialogue summarization settings, including a newly proposed MIX-TO-MANY setting. All settings are depicted in Figure 3 . ONE-TO-ONE The ONE-TO-ONE setting can be viewed as a specific type of multi-lingual setting, where the model can merely handle the input of one language and the output of one language. According to whether the  Then, we adopt a greedy search strategy to assign each participant a language. Finally, we can get the mix-lingual dialogue associated with summaries in different languages. input and output belong to the same language, this setting can be further divided into Mono-lingual setting (shown in Figure 3 Experimental Setting: For mono-lingual experiments, we train six models based on {En\u2192En}, {Zh\u2192Zh}, {Fr\u2192Fr}, {Ar\u2192Ar}, {Ru\u2192Ru} and {Es\u2192Es} mono-lingual pairs respectively. For cross-lingual experiments, we train two models based on {En\u2192Zh} and {Zh\u2192En} cross-lingual pairs respectively. All eight models are tested in supervised manner. MANY-TO-ONE and ONE-TO-MANY MANY-TO-ONE models are able to process dialogues in various languages and output the summary in one language, as shown in Figure 3(c ). On the contrary, ONE-TO-MANY models have the ability to produce summaries in various languages given a fixed language input, as shown in Figure 3(d) . Both settings require models with multilingual capabilities. Experimental Setting: For MANY-TO-ONE experiments, we train one model based on all {En\u2192En, Zh\u2192En, Fr\u2192En, Ar\u2192En, Ru\u2192En, Es\u2192En} pairs. For ONE-TO-MANY experiments, we train one model based on all {En\u2192En, En\u2192Zh, En\u2192Fr, En\u2192Ar, En\u2192Ru, En\u2192Es} pairs. These two models are tested in supervised manner. MANY-TO-MANY As shown in Figure 3 (e), MANY-TO-MANY models can take dialogues in various languages as inputs and produce summaries in various languages. Thanks to the pre-trained multi-lingual language models (Liu et al., 2020; Tang et al., 2020) , based on which, MANY-TO-MANY models can perform zero-shot summarization even though the inputoutput language pair is not seen during the training process. Experimental Setting: For MANY-TO-MANY experiments, we train one model based on all {En\u2192En, Zh\u2192Zh, Fr\u2192Fr, Ar\u2192Ar, Ru\u2192Ru, Es\u2192Es} pairs and test it in both supervised and zero-shot manners. MIX-TO-MANY Nowadays, dialogue participants from different countries can use their mother tongue to communicate with each other based on instantaneous translation systems. To investigate the possibility of generating summaries directly from mix-lingual dialogues (utterances in different languages), we come up with an innovative new setting: MIX-TO-MANY, as shown in Figure 3 (f). To this end, we first simulate the real scenario and construct mix-lingual dialogue-summary pairs, the whole construction process is shown in Figure 4 . Given each English dialogue in MSAMSum (shown in Figure 4 (a)), we first group utterances by participants, which results in several groups for different participants (shown in Figure 4(b) ). Then, for each group, we calculate the average roundtrip translation ROUGE-1 score for each language (shown in Figure 4(c) ). Afterward, we adopt a greedy search strategy to assign each participant a language (shown in Figure 4(d) ). The goal of our strategy is twofold: choose as many languages as possible and as high-quality translations as possi- (a) The number of dialogues that contain L language, L=Zh, Fr, Ru, Es, Ar. (b) The percentage of dialogues that contain N language. N=2,3,4,5. Experiments In this section, we first introduce our model mBART-50. After, we describe the evaluation metrics. Finally, we show the implementation details. Backbone Model We employ mBART-50 (Tang et al., 2020) as our multi-lingual summarizer, which is a Transformerbased model and pre-trained on a huge volume of multi-lingual data. It is derived from mBART (Liu et al., 2020) and extends the language processing capabilities from 25 languages to 50 languages in total. The architecture of mBART-50 is based on the BART (Lewis et al., 2020) , which adopts position-wise feed-forward network, multi-head attention (Vaswani et al., 2017) , residual connection (He et al., 2016) and layer normalization (Ba et al., 2016) modules to map the source dialogue into dis-tributed representations and further generate the target summary. To handle various input and output languages, mBART-50 needs to receive inputs with language identifiers (e.g., En, Zh) at both the encoder and the decoder side. According to the practical experience, we set both the source language identifier and target language identifier at the start of the source and target sequences respectively. Evaluation Metrics The most widely used metrics for summarization are ROUGE scores (Lin, 2004) . However, the original ROUGE is specifically designed for English. To make this metric suitable for our experiments, we employ the multi-lingual ROUGE (Hasan et al., 2021) as our evaluation metrics, which takes segmentation and popular stemming algorithms for various languages into consideration 9 . Implementation Details For MSAMSum construction, we set round-trip translation ROUGE-1 threshold to 80.00 and the textual entailment threshold to 0.9. For experiments, we use the standard mBART-50 implementation provided by Huggingface/transformers 10 . For fine-tuning process, the learning rate is set to 5e-06, the dropout rate is 0.1, the warmup is set to 2000 and the batch size is 4. In the test process, beam size is 5, the minimum decoded length is 10 and the maximum length is 150. All our experiments are conducted based on the Tesla-V100-32GB GPU. Results In this section, we describe experimental results and show our analyses for different settings. ONE-TO-ONE Results Table 2 shows the results for ONE-TO-ONE setting, including both the mono-lingual and the cross-lingual experiments. According to the 52.98 ROUGE-1 score achieved by fine-tuning BARTlarge on full English SAMSum dataset (Chen and Yang, 2020) , we can see that our experiments achieve impressive results. For mono-lingual experiments, Ar\u2192Ar results perform worse than others to some extent, we attribute this to the fact that the Arabic language processing capability of the  pre-trained mBART-50 is relatively weak, which is in line with the size of original pre-training corpus (Lewis et al., 2020) . For cross-lingual experiments, surprisingly, we find that En\u2192Zh get better results compared with Zh\u2192Zh, which may due to the model's strong English comprehension ability. MANY-TO-ONE and ONE-TO-MANY Results Table 3 and table 4 show results for MANY-TO-ONE and ONE-TO-MANY settings respectively. For both settings, we find that the results of the multi-lingual model varied less between pairs compared with ONE-TO-ONE models. For the MANY-TO-ONE model, the results of En\u2192En and Zh\u2192En are slightly worse than results of corresponding single ONE-TO-ONE models. This is because the MANY-TO-ONE model needs to handle multiple languages, which may cause the parameters interference problem (Lin et al., 2021) , and is therefore inferior to a single expert model. In contrast, the En\u2192En 49.84 24.73 40.67 En\u2192Es 47.27 21.82 37.87 En\u2192Zh 43.86 18.25 35.56 En\u2192Fr 44.33 19.58 35.20 En\u2192Ru 41.26 15.76 33.00 En\u2192Ar 39.71 14.96 32.82 ONE-TO-MANY Src\u2192Tgt R-1 R-2 R-L MANY-TO-MANY Results Table 5 shows ROUGE-L results for the MANY-TO-MANY setting 11 . We test each language pair in the cartesian product of six languages, which results in two types of manners: supervised and zeroshot summarization. For the supervised manner (results in bold), almost all results show the best performance. For the zero-shot manner (results in italics), we find that despite the model is fine-tuned based on mono-lingual dialogue-summary pairs, it still has the strong ability to perform summarization across different languages. In line with previous experiments, we find the MANY-TO-MANY model that balances across various languages inevitably loses some performances compared with the ONE-TO-ONE model. Nonetheless, the MANY- TO-MANY model, which greatly reduces the deployment cost while preserving the performance, is an important research direction in the future. MIX-TO-MANY Src\u2192Tgt R-1 R-2 R-L MIX-TO-MANY Results Table 6 shows the results for the MIX-TO-MANY setting. As the first step towards this direction, we find that current multi-lingual pre-trained models can obtain encouraging results. The Mix\u2192Es, Mix\u2192Zh, Mix\u2192Fr and Mix\u2192Ru models achieve comparable results with respect to the corresponding ONE-TO-ONE model. These results verify that despite the multi-lingual model only deals with one language at a time in the pre-training progress, after fine-tuning, it can handle mix-lingual inputs concurrently. Surprisingly, the Mix\u2192Ar results even surpass the performance of singe Ar\u2192Ar model. We think this is due to the mix-lingual dialogue essentially acts as an utterance-level code-switching data, which helps the representation space of the low-resource language align with other languages. This also inspire us that it would be better to generate the low-resource language summary directly from the mix-lingual dialogue. Case Study Figure 6 shows summaries in different languages generated by the ONE-TO-MANY model for an example English dialogue. We can see that all the generated summaries achieve good ROUGE performance, with English being the highest. We find that the multi-lingual model can generate fluent summaries while preserving the important information of the dialogue. Besides, the model also has the ability to accurately express participants information (e.g., Elliot, Jordan) and keep entities' factual consistency (e.g., 8 pm) across different languages. Conclusion and Future Work In this paper, we innovatively explore the multilingual dialogue summarization task. To this end, we carefully create MSAMSum as our testbed, which covers dialogue-summary pairs in six languages, including English, Chinese, Russian, French, Arabic and Spanish. Furthermore, we systematically set up five multi-lingual settings to benchmark extensive experiments. Our results indicate that various models can achieve impressive performance based on pre-trained models. Besides, the newly proposed MIX-TO-MANY setting also shows its effectiveness in low-resource scenarios. In the future, we think several concerns need to be addressed for this task. Firstly, multi-lingual models tend to underperform mono-lingual models; Secondly, low-resource languages tend to perform poorly; Thirdly, the difficulty of aligning finegrained information in different languages. Future works should pay particular attention to these concerns to facilitate this multi-lingual dialogue summarization research direction. We thank the anonymous reviewers for their insightful comments. This work was supported by the National Key RD Program of China via grant 2020AAA0106502, National Natural Science Foundation of China (NSFC) via grant 61976073 and Shenzhen Foundational Research Funding (JCYJ20200109113441941). English Dialogue Elliot : I can't talk rn , I'm rly busy. Elliot : Can I call u back in about 2 hours? Jordan : Not really , I'm going to a funeral. Jordan : I'll call you tonight , ok? Elliot : Sure Elliot : Whose funeral is it? Jordan : My colleague's , Brad. Jordan : I told you about him , he had a liver cancer. Elliot : I'm so sorry man , I hope u're ok. Elliot : I'll call u at 8 pm. Generated Summaries (One-to-many) A Ethical Considerations As we propose a new multi-lingual dialogue summarization dataset and conduct experiments based on large pre-trained language models, we make several clarifications to address potential concerns: \u2022 Dataset: Since our MSAMSum is derived from the SAMSum (Gliwa et al., 2019) , which is a well-constructed and human-labelled dataset. Therefore, our dataset inherits the contents of SAMSum and does not contain toxic information. \u2022 Model: The experiments described in this paper are based on the mBART-50-large (Tang et al., 2020) and make use of V100 GPUs. Despite we run dozens of experiments, our results could help reduce parameter searches for future works. We also consider to alleviate such resource-hungry challenge by exploring light-weight distilled models. B Round-trip Translation ROUGE Scores Table 7 shows the average ROUGE scores between the English data in SAMSum (Gliwa et al., 2019) and the round-trip translated English data. These results indicate the overall translation quality. C The Changing of Data Size Table 8 shows how the data size changes. After quality controlling process, we can get different data size for different languages (before alignment). After taking the intersection of different languages, we get our final MSAMSum (after alignment). D Detailed MANY-TO-MANY Results Table 9 shows detailed ROUGE-1, ROUGE-2 and",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions support from the National Key R&D Program of China, the National Natural Science Foundation of China (NSFC), and Shenzhen Foundational Research Funding. These are indicative of research agency funding, as they are government-funded organizations that provide grants for research. There is no mention of defense, corporate, or foundation funding, nor is there an indication that no funding was received.",
    "abstract": "Dialogue summarization helps users capture salient information from various types of dialogues has received much attention recently. However, current works mainly focus on English dialogue summarization, leaving other languages less well explored. Therefore, we present a multi-lingual dialogue summarization dataset, namely MSAMSum, which covers dialogue-summary pairs in six languages. Specifically, we derive MSAMSum from the standard SAMSum (Gliwa et al., 2019) using sophisticated translation techniques and further employ two methods to ensure the integral translation quality and summary factual consistency. Given the proposed MSAMum, we systematically set up five multi-lingual settings for this task, including a novel mix-lingual dialogue summarization setting. To illustrate the utility of our dataset, we benchmark various experiments with pre-trained models under different settings and report results in both supervised and zero-shot manners. We also discuss some future works towards this task to motivate future researches 1 . !\"#$%&'() *+,-./01234 5678%&'()*",
    "countries": [
        "China"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": 3,
    "year": 2022,
    "month": "May",
    "title": "{MSAMS}um: Towards Benchmarking Multi-lingual Dialogue Summarization"
}