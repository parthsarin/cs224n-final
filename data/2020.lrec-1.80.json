{
    "article": "This paper deals with the annotation of dialogue acts in a multimodal corpus of first encounter dialogues, i.e. face-to-face dialogues in which two people who meet for the first time talk with no particular purpose other than just talking. More specifically, we describe the method used to annotate dialogue acts in the corpus, including the evaluation of the annotations. Then, we present descriptive statistics of the annotation, particularly focusing on which dialogue acts often follow each other across speakers and which dialogue acts overlap with gestural behaviour. Finally, we discuss how feedback is expressed in the corpus by means of feedback dialogue acts with or without co-occurring gestural behaviour, i.e. multimodal vs. unimodal feedback. Introduction A dialogue act is a speech segment, or utterance, that has a communicative function in a conversation, and is thus a type of speech act (Searle, 1969) . Dialogue acts are essential to the understanding of the dynamics and semantics of dialogues and therefore to the construction of dialogue systems. This paper deals with the annotation of dialogue acts in a multimodal corpus of first encounter dialogues, i.e. faceto-face dialogues in which two people who meet for the first time talk with no particular purpose other than just talking. Contrary to most existing corpora displaying dialogue act tags, our corpus is not shaped by a domain specific task which the participants have to solve together, but it is guided by their willingness to talk to each other exchanging general information about themselves. Therefore, the distribution of dialogue act labels in the corpus is bound to be different from that of dialogues normally classified as task-oriented. Furthermore, the corpus is multimodal. The dialogues have been video-recorded, and the gestural behaviour has been carefully annotated with respect to form, dynamics and functions of head movements, facial expressions and body posture. Therefore, the annotation of dialogue acts provides an additional layer of functional linguistic analysis which will enable analyses of how multimodal signals contribute to the structure and content of the dialogues. The annotation of dialogue acts in a multimodal corpus is also relevant for the implementation of embodied conversational agents since the annotations allow not only to model the dialogue structure, but also the gestures together with the linguistic context. The first encounters corpus dealt with in our work is particularly interesting, since it shows how subjects from a specific culture address each other when they are not acquainted and then exchange information about themselves. For this reason, first encounters have been collected and studied in projects investigating social conventions in different cultures, e.g. (Rehm et al., 2009) . This type of information is crucial for behavioural models of culturally aware virtual agents and robots, particularly when they meet people for the first time. In this paper we describe the method used to annotate dialogue acts in the corpus, including a report of interagreement test results. We provide descriptive statistics of the annotation, particularly looking at which dialogue acts tend to follow each other across speakers. We analyse the way head movements and facial expressions reinforce some of the most frequent dialogue acts occurring in the corpus. Finally, we discuss how feedback is modelled in the corpus by means of utterances and gestural behaviour considered together. Background Studies Various classifications have been proposed the past forty years to annotate and analyse dialogue acts in different domains, e.g. HCRC MapTask (Anderson et al., 1991) , DAMSL (Allen and Core, 1997), Verbmobil (Alexandersson et al., 1997) , and SWBD-DAMSL (Jurafsky et al., 1997) . Common to all these classifications is the fact that they were proposed in order to construct specific dialogue systems and to provide the necessary background for the implementation of dialogue management strategies (Allen and Core, 1997; Alexandersson et al., 1997; Jurafsky et al., 1997) . Finally, a few schemes were proposed in order to provide large annotated data for training and testing dialogue models. This was the case of the multimodal AMI corpus (Carletta, 2007) , which provided many types of linguistic annotations, including dialogue acts, as well as gestural annotations in the domain of project meetings. The interest for dialogue act annotation is also reflected in the many efforts dedicated to automatic dialogue act labelling in different corpus types and languages by means of machine learning techniques (Verbree et al., 2006; Purver et al., 2007; Milajevs and Purver, 2014; Amanova et al., 2016) . In order to facilitate the interoperability between the various annotation frameworks, an effort has been made to create a standard for dialogue act annotation that would take into account and combine categories from the previous schemes. The result of that work is the ISO 24617 standard (Bunt et al., 2010; Bunt et al., 2017) . Guidelines have also been provided to implement the ISO dialogue act annotation framework in the annotation tool for multimodal behaviour ANVIL (Bunt et al., 2012) . According to this implementation, each functional dimension of the dialogue acts is annotated in a different ANVIL track. Finally, Bunt et al. (2019) have applied the ISO 24617 standard to re-annotate dialogue acts in a number of dialogue corpora which were previously coded according to different dialogue act schemes. We decided to apply the ISO standard in our work to take advantage of the interoperability it offers. Furthermore, some of the categories implemented in the standard are similar to those used in the functional annotation of the gestural behaviour in our corpus, as will be explained below. The corpus The annotated corpus described here consists of 12 videorecorded dyadic conversations in Danish between six male and six female participants who meet each other for the first time. The corpus is about one hour long and was collected and annotated at the University of Copenhagen within the Nordic project NOMCO (Paggio et al., 2010) . The participants were asked to stand in front of each other and talk freely for about five minutes to get acquainted with one another. Three different cameras and two cardioid microphones were used to video-record the conversations. The already existing annotations follow the MUMIN framework (Allwood et al., 2007) and comprise, in addition to the orthographic transcription, labels referring to the form and functions of head movements, facial expressions and body postures, and their relation to the speech segment they are associated with (Paggio and Navarretta, 2017) . Dialogue act labels were added as described in the next section. The annotations can be made available for research purposes on request. The annotation of dialogue acts The annotation of dialogue acts was performed using the ANVIL tool (Kipp, 2004) as was also done previously for the annotation of gestural behaviour. A set of specifications was defined based on the ISO 24617-2:2012 standard 1 . The following dialogue act categories were included in the specifications: \u2022 General Purpose Dialogue Acts: Accept-Offer, Accept-Request, Accept-Suggest, Address-Offer, Address-Request, Address-Suggest, Agreement, Answer, Check-Question, Choice-Question, Confirm, Correction, Decline-Offer, Decline-Request, Decline-Suggest, Disagreement, Disconfirm, Inform, Inform-Answer, Instruct, Offer, Promise, Propos-Question, Question, Request, Set-Question, Suggest. \u2022 Interaction Structuring Dialogue Acts: AcceptApology, AcceptThanking, Apology, Confratulate, InitialGoodbye, InitialGreeting, InitialSelfIntroduction, ReturnGoodbye, ReturnGreeting, Return-SelfIntroduction, Thanking. 1 https://www.iso.org/standard/51967.html \u2022 Feedback-related Dialogue Acts: AlloFeedbackGive, AlloFeedbackElicit, AutoFeedback. \u2022 Related to Own Communication Management: OwnCommManagement, Retraction It must be noted that in the current phase of the dialogue act annotation work, we do not distinguish between the various dimensions of dialogue acts and annotate them all on the same track by only picking one of the labels at a time. Furthermore, we focus on the types from the first three dimensions, and do not, as done in the standard, distinguish between Own Communication Management and Time Management. All relevant instances were annotated using Own-CommManagement. The reason for this is that the main annotator found it difficult to apply the Time Management label since in a non task-oriented corpus, pauses mostly signal that the speakers are planning their own speech (Maclay and Osgood, 1959; Chafe, 1974; Allwood et al., 2007) . Relevant examples are pauses that occur after a retraction and precede the new speech as in the following exchange: (1) Det lyder da -retraction (It sounds actually) PAUSE \u00f8hm -planning pause (pause uhm) Jeg synes det det lyder som om du er da p\u00e5 skinner (I think it sounds as if you are actually on track) These pauses are the most common in the corpus, while there are only few stalling examples in which the speakers explicitly tell the interlocutors that they are looking for the correct wording as in the following example: (2) \u00f8h hvad hedder det PAUSE jeg arbejder (uh what is it called PAUSE I work) For the segmentation of dialogue acts, we followed the definition of a functional segment in the ISO standard: \"A functional segment is a minimal stretch of communicative behaviour that has a communicative function\" (ISO/DIS 24617-2, p.3). Examples of functional segments and the categories that have been assigned to them are shown below. Note that a dialogue act can be as short as a filler, and as long as a complex sentence: \u2022 \u00d8hm (filler) \u2192 OwnCommunicationManagement \u2022 Ja (yes) \u2192 AlloFeedbackGive \u2022 Af en eller anden grund s\u00e5 var der ikke nogen der var hjemme (for some reason, nobody was at home) \u2192 Inform The annotation procedure was as follows. First a single coder annotated the dialogue acts in one conversation, and then the annotations were checked by two other coders. All coders were experts who were well-acquainted with the annotation methodology. Disagreements and problematic cases were discussed, and a pre-final version was produced based on the common understanding of the dialogue labels achieved through discussions of the first trial conversation. Table 1 : Inter-annotator agreement results In this pre-final version, all dialogues were annotated by the same coder. The inter-coder agreement tests were run on two dialogues, which were independently annotated by two expert coders. The inter-coder agreement figures for segmentation, category assignment and overall agreement as provided by the ANVIL tool for the four speakers in the two dialogues are in Table 1 . The results for segmentation and category assignment are given in terms of Cohen's \u03ba (Cohen, 1960) and corrected \u03ba (Brennan and Predinger, 1981) , while the overall agreement is also expressed in terms of Krippendorff's \u03b1 (Krippendorff, 1970) . As can be seen, the results demonstrate high agreement. Nevertheless, the few cases of disagreement were inspected and the entire annotation revised by the second annotator. To delve deeper into the way dialogue acts are used in the first encounter corpus, we extracted all pairs of dialogue acts which follow each other at the onset by no longer than 0.5s. Using a 0.5s window is essentially an empirical choice. However, there is experimental evidence of the fact that human subjects are sensitive to temporal gaps of at least 0.5s (Giorgolo and Verstraten, 2008; Leonard and Cummins, 2010) . Extending the window beyond this measure, on the other hand, would yield a larger number of associated dialogue act pairs with the consequent danger of capturing pairs which are not directly adjacent to each other. A total of 1391 such dialogue act pairs were extracted. Figure 1 shows which interlocutor responses (on the x axis) follow at the onset dialogue acts from the other speaker. In the legend of the figure, only the dialogue acts that are most frequently followed by a response within 0.5s are mentioned explicitly, while less frequent categories are joined together under the 'Other' label. The distribution shows that there is a significant dependence between the adjacent categories (\u03c7 2 = 1870.3, df = 208, p-value < 2.2e-16). The clearest pattern is the one showing the inter-dependency between AlloFeedbackGive and Inform, which tend to follow each other. Tables 3 and 4 give frequency counts and percentages of the dialogue acts that follow AlloFeedbackGive and Inform, respectively. Interestingly, the second most frequent dialogue act type following AlloFeedbackGive is another feedback giving utterance, probably showing an alignment of the two speakers along this dimension. Following an Inform act, on the other hand, we see not only AlloFeedbackGive as already noted and as expected, but also Agreement, and dif-  These associations confirm the main characteristic of the dialogues, which is for the two speakers to create common knowledge about each other, as well as rapport through continuous feedback. The Dialogue Acts in the First Encounters Relation between dialogue acts and gestural behaviour To investigate the relation between dialogue acts and gestural behaviour, we extracted all the overlaps between dialogue acts and head movements on the one hand, and di- alogue acts and facial expressions on the other. An overlap is defined by having at least a time point in common. We did this within as well as across speakers to look at how speakers combine utterances with gestural behaviour, but also how interlocutors respond non-verbally to the utterances of the other conversation participant. Starting with overlaps between overlaps and head movements, there are a total of 3686 overlaps when we look at the same speaker (Figure 2 ), and 2389 overlaps across different speakers (Figure 3 ). While the plots show that all types of head movements occur together with many different dialogue act types, we also note that AlloFeedback-Give is mostly accompanied by Nod by the same speaker, whereas across speakers Inform mostly overlaps with Nod. These two overlap types show in fact how feedback works from two different perspectives. If we focus on multimodal expressions by one speaker, linguistic feedback expressions are often accompanied by nodding, while looking at the dynamics of the dialogue across speakers, feedback is often given while the other person is providing information. As for the overlaps between overlaps and facial expressions, there are 1677 for the same speaker (Figure 4 ), and 1731 across speakers (Figure 5 ). There does not seem to be much difference in how facial expressions overlap with dialogue acts in the two plots with the noticeable case of Retraction, which appears in the plot showing overlaps for the same speaker. This type of overlap reflects the situation in which a speaker abandons what they were saying and starts a new utterance. This is often accompanied by a facial expression, which can be considered a kind of self feedback. Analysis of Feedback In the preceding section, we described how different dialogue acts are associated with head movements and facial expressions focusing on the type of the movement. Here, we discuss the relation between the functional annotation of the data at the level of dialogue act on the one hand, and gestural behaviour on the other. For this analysis, we have chosen to focus on the feedback function, which was in fact annotated in the speech and the gestural channels separately. The gestures which are considered together with dialogue acts are head movements and facial expressions. Feedback is the phenomenon according to which speakers exchange information about being able and willing to continue having Contact with each other, to have Perception of each other, and Understanding the message that is being communicated (CPU) (Allwood et al., 1993; Bunt et al., 2010) Feedback head movements and facial expressions have been addressed in numerous studies (Yngve, 1970; Duncan, 1972; Hadar et al., 1984; McClave, 2000; Cerrato, 2007) . Moreover, the annotations in the Danish NOMCO corpus have been previously used to test whether feedback categories can be predicted automatically using the shape descriptions of head movements and facial expressions (Paggio and Navarretta, 2013) , or to annotate feedback automatically in different corpora (Navarretta, 2013) . Feedback speech expressions in a Swedish dialogue corpus were investigated by Allwood et al. (1993) while yes/no feedback speech expressions in the Danish NOMCO corpus were analysed in Paggio and Navarretta (2017) . Finally, feedback yes/no expressions and nods in the Danish NOMCO corpus have been compared to the corresponding expressions in the Swedish NOMCO corpus and to the occurrences of nods in the Finnish first encounter corpus (Navarretta et al., 2012) . In this study we analyse the occurrence of feedback categories in speech and gestural behaviour, more specifically head movements and facial expressions, using the dialogue act annotations for speech and the MUMIN functional categories for gestures. We do this by looking at how often a feedback dialogue act of a certain type co-occurs with a head movement or a facial expression encoded with the same feedback category (see Table 5 ), another feedback category or a different functional category than feedback. The same is then done starting from feedback head movements and feedback facial expressions and extracting the co-occurring dialogue acts. Only the first co-occurring event is extracted in cases where there are multiple overlaps. Moreover, we only consider co-occurrence of speech and gestures produced by the same speaker. Multiple overlaps and co-occurrences of events across speakers are left for future investigation. Even though the definition of feedback in MUMIN and in the ISO 24617 standard is the same, the names of the feedback categories vary slightly. Table 5 shows the correspondence between the feedback categories in the two annotation frameworks. Since both frameworks have a feedback category of auto or self feedback describing cases in which speakers expresses feedback to their own speech, we also included the phenomenon in our study. jor differences between the two annotations are the following. Firstly, MUMIN distinguishes between contact and perception (CP) and Contact, Perception and understanding (CPU), while both categories in the dialogue act annotations are annotated with the label AlloFeedback. This distinction, however, is not important when comparing the annotations. Secondly, the two categories Agreement and Disagreement belong to another dimension than feedback in the Dialogue Act annotation. However, for this study we considered them equivalent since they refer to the same kind of behaviours. Table 6 shows the number of occurrences of feedback dialogue acts of a) a certain feedback type which b) occur alone (FBDA), c) co-occur with a gesture (head movement and/or facial expression) encoded with the same feedback category (+=FBGesture), d) co-occur with a feedback gesture of another feedback type (+otherFBGesture), e) cooccur with a non feedback gesture (+notFBGesture), f) the total number of feedback dialogue acts (All), g) the percentage of feedback dialogue acts that co-occur with a gesture annotated with the same feedback category (%FB same), h) the percentage of feedback dialogue acts that co-occur with a gesture, independently of its function (%Multimodal). We do not show when feedback head movements and facial expressions co-occur since we are focusing on cooccurrences of feedback dialogue acts and gestures. As can be seen, there are 1263 feedback dialogue acts in the corpus and 85% of them co-occur with a head movement and/or a facial expression. Moreover, the majority of Agreement, and most of the FeedbackGive (back-channelling) dialogue acts are accompanied by a gesture annotated with the same feedback category, meaning that the two signals reinforce each other. Finally, 44% of AutoFeedback dialogue acts co-occur with a gesture that has exactly the same function. From the table it is also evident that many feedback dialogue acts co-occur with a feedback gesture of different type or a gesture with another function than feedback. In these cases speech and gesture convey different information at the same time. In Table 7 Conclusions and Future Work In this paper we have presented the annotation of dialogue acts in the multimodal NOMCO corpus, a collection of naturally occurring first encounter dialogues in Danish. The dialogue act annotation is based on the ISO 24617 standard, and the validation of the annotations shows high agreement between two coders. The only difficulty in applying the standard was the distinction between Own Communication Management and Time Management categories, which does not seem relevant in conversations that are not task oriented, and where time management appears to be always functional to the management of communication. We then presented descriptive statistics showing first how different dialogue act types follow one another, and then how they overlap with gestural behaviours, in particular head movements and facial expressions. Both analyses point at ways in which the dialogue participants create common ground and rapport by constantly checking mutual understanding and giving or eliciting feedback to and from each other. Finally, we focus on feedback behaviour by examining the functional annotation of the gestural behaviour and how it relates to the dialogue act annotation, and show that feedback gestures can constitute not only a reinforcement of the co-occurring spoken utterance, but also a complement to it. In future work, we plan to distinguish the various dimensions of dialogue acts, and to add turn management dialogue acts to the annotations. It would also be useful to compare the dialogue act annotations in this corpus with dialogue act annotations in corpora addressing domain specific tasks, such as map task dialogues, or in corpora involving people who know each other well. This would complete a preceding study of feedback expressions in first encounters and in conversations between family members and friends that confirmed the hypothesis that the number and type of feedback signals is partly related to the degree of familiarity of the participants (Navarretta and Paggio, 2012) . The role played in the expression of dialogue acts by other types of movement, such as hand gestures or body posture, is also an interesting topic for further research. Finally, we would like to experiment with the automatic annotation",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not provide specific information regarding its funding sources. It mentions the University of Copenhagen and the Nordic project NOMCO, but does not specify if these are funding sources or just affiliations and projects involved in the research. Without explicit mention of funding from defense, corporate entities, research agencies, or foundations, we cannot assume any specific funding source.",
    "abstract": "This paper deals with the annotation of dialogue acts in a multimodal corpus of first encounter dialogues, i.e. face-to-face dialogues in which two people who meet for the first time talk with no particular purpose other than just talking. More specifically, we describe the method used to annotate dialogue acts in the corpus, including the evaluation of the annotations. Then, we present descriptive statistics of the annotation, particularly focusing on which dialogue acts often follow each other across speakers and which dialogue acts overlap with gestural behaviour. Finally, we discuss how feedback is expressed in the corpus by means of feedback dialogue acts with or without co-occurring gestural behaviour, i.e. multimodal vs. unimodal feedback.",
    "countries": [
        "Denmark"
    ],
    "languages": [
        "Swedish",
        "Finnish",
        "Danish"
    ],
    "numcitedby": 2,
    "year": 2020,
    "month": "May",
    "title": "Dialogue Act Annotation in a Multimodal Corpus of First Encounter Dialogues",
    "values": {
        "building on past work": " Dialogue acts are essential to the understanding of the dynamics and semantics of dialogues and therefore to the construction of dialogue systems.  Contrary to most existing corpora displaying dialogue act tags, our corpus is not shaped by a domain specific task which the participants have to solve together, but it is guided by their willingness to talk to each other exchanging general information about themselves.  Therefore, the distribution of dialogue act labels in the corpus is bound to be different from that of dialogues normally classified as task-oriented.  The dialogues have been video-recorded, and the gestural behaviour has been carefully annotated with respect to form, dynamics and functions of head movements, facial expressions and body posture.  Therefore, the annotation of dialogue acts provides an additional layer of functional linguistic analysis which will enable analyses of how multimodal signals contribute to the structure and content of the dialogues.  The annotation of dialogue acts in a multimodal corpus is also relevant for the implementation of embodied conversational agents since the annotations allow not only to model the dialogue structure, but also the gestures together with the linguistic context.  The first encounters corpus dealt with in our work is particularly interesting, since it shows how subjects from a specific culture address each other when they are not acquainted and then exchange information about themselves.  For this reason, first encounters have been collected and studied in projects investigating social conventions in different cultures, e.g. (Rehm et al., 2009) .  This type of information is crucial for behavioural models of culturally aware virtual agents and robots, particularly when they meet people for the first time.  In this paper we describe the method used to annotate dialogue acts in the corpus, including a report of interagreement test results.  We provide descriptive statistics of the annotation, particularly looking at which dialogue acts tend to follow each other across speakers.  We analyse the way head movements and facial expressions reinforce some of the most frequent dialogue acts occurring in the corpus.  Finally, we discuss how feedback is modelled in the corpus by means of utterances and gestural behaviour considered together."
    }
}