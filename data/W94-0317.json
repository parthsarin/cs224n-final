{
    "article": "This paper deals with the reference choices involved in the generation of argumentative text. A piece of argumentative text such as the proof of a mathematical theorem conveys a sequence of derivations. For each step of derivation, the premises (previously conveyed intermediate results) and the inference method (such as the application of a particular theorem or definition) must be made clear. The appropriateness of these references crucially affects the quality of the text produced. Although hot restricted to nominal phrases, our reference decisions are similar to those concerning nominal subsequent referring expressions: they depend on the availability of the object referred to within a context and are sensitive to its attentional hierarchy. In this paper, we show how the current context can be appropriately segmented into an attentional hierarchy by viewing text generation as a combination of planned and unplanned behavior, and how the discourse theory of Reichmann can be adapted to handle our special reference problem. Introduction This paper describes how reference decisions are made in PROVERB, a system that verbalizes machine-found natural deduction (ND) proofs. A piece of argumentative text such as the proof of a mathematical theorem can be viewed as a sequence of derivations. Each such derivation is called a proof communicative act (PCA), following the viewpoint that speeches are actions. By reference choices we mean the explicitness of the verbalization of certain entities in the PCAs. Concretely, such decisions must be made for intermediate conclusions used as premises, as well as for the inference method. As an example, let us look at the PCA with the name Derive below: (Derive Derived-Formula: u * Iu = u Reasons : (unit(lu, U, *), u 6U) Method : Def-Semigroup*unit) Here, Derived-Formula is filled by a new intermediate conclusion the current PCA aims to convey, which is derivable by applying the filler of Method, with the filler of Reasons as premises. While the new conclusion will usually be handed over unchanged for verbalization, there are alternatives for referring to both the Reasons and the Method. Depending on the discourse history, the following are two of the possible verbalizations: 1. (inference method omitted): \"Since lu is the unit element of U, and u is an element of U, u* 1v = u.\" 2. (reasons omitted): \"According to the definition of unit element, u * 1v= u.\" Note that, an explicit reference to a premise or an inference method is not restricted to a nominal phrase, as opposed to the traditional subsequent references. Despite this difference, the choices to be made here have much in common with the choices of subsequent references discussed in more general frameworks [Rei85, GS86, Da192] : they depend on the availability of the object to be referred to in the context and are sensitive to the segmentation of the current context into an attentional hierarchy. Although this observation is widely agreed upon for subsequent references, no consensus about where the segment boundaries lie has been reached. In PROVERB, we attack this problem by viewing text generation as a combination of hierarchical planning [Hov88, Moo89, Reigl, Dal92] and local organization [Sib90] . Following [GS86] , moreover, we assume that every posting of a new task by the hierarchical planning mechnism creates a new attentional unit. As a consequence, the attentional hierarchy is equivalent to the plan hierarchy. Based on this segmentation of context, PRO VERB makes reference choices according to a discourse theory adapted from that of Reichman [Rei85, Hua90] . The System PROVERB PROVERB is a text planner that verbalizes natural deduction (ND) style proofs [Gen35] . Several similar attempts can be found in previous work. The system EX-POUND [Che76] is an example of direct translation: Although a sophisticated linearization is applied on the input ND proofs, the steps are then translated locally in a template driven way. ND proofs were tested as inputs to an early version of MUMBLE [McD83] , the main aim however, was to show the feasibility of the architecture. A more recent attempt can be found in THINKER [EP93] , which implements several interesting but isolated proof presentation strategies. PRO VERB therefore can be seen as the first serious attempt to devise a comprehensive computational model that produces adequate argumentative texts from ND style proofs. Most current NL text planners assume that language generation is planned behavior and therefore adopt a hierarchical planning approach [Hov88, Moo89, Da192, Rei91] . Nevertheless there is psychological evidence that language has an unplanned, spontaneous aspect as well [Och79] . Based on this observation, Sibun [Sib90] implemented a system for generating descriptions of objects with a strong domain structure, such as houses, chips and families. Once a discourse is started, local structures suggest the next objects available. From a computational point of view, a hierarchical planner elaborates recursively on the initial communicative goal until the final subgoals can be achieved by applying primitive operators. Local organization, on the other hand, chooses a part of the remaining task and carries it out. 2.1 The Planning Framework PROVERB combines both of these approaches in a uniform planning framework [Hua94c] . The hierarchical planning is realized by so-called top-down presentation operators that split the task of presenting a particular proof into subtasks of presenting subproofs. While the \u2022 overall planning mechanism follows the RST-based planning approach [Hov88, Moo89, Rei91], the planning operators resemble more the schemata in schema-based planning [McK85, Par88] . Bottom-up presentation operators are devised to simulate the unplanned aspect, where the next intermediate conclusion to be presented is chosen under the guidance of the local focus mechanism. It is called bottom-up since one new intermediate conclusion or subproof is chosen and presented, using previously presented intermediate conclusions as premises. The two kinds of presentation operators are treated differently. Since top-down operators embody explicit communicative norms, they are given a higher priority. Only when no top-down presentation operator is applicable, will a bottom-up presentation operator be chosen. The overall planning framework is realized by a function called Present. Taking as input a subproof, Present repeatedly executes a basic planning cycle until the input subproof is conveyed. Each cycle carries out one presentation operator, where Present always tries first to choose and apply a top-down operator. If impossible, a bottom-up operator will be chosen. The function Present is first called with the entire proof as the presentation task. The execution of a top-down presentation operator may generate subtasks by calling it recursively. 2.2 The Discourse Model and the Attentional Hierarchy The discourse carried out so far is recorded in a discourse model. Rather than recording the semantic objects and their properties, the intermediate conclusions of a ongoing argument or mathematical proof are stored. Therefore, our discourse model consists basically of the part of the input proof tree which has already been conveyed. The segmentation of the discourse is described in section 3. The following are some notions useful for the formulation of the presentation operators: \u2022 Task is the subproof in the input proof whose presentation is the current task. \u2022 Local focus is the intermediate conclusion last presented, the semantic objects involved in the local focus are called the focal centers. 2.3 Proof Communicative Acts PCAs are the primitive actions planned to achieve communicative goals. When enriched with reference decisions, they are called preverbal messages (PM). Like speech acts, PCAs can be defined in terms of the communicative goals they fulfill as well as their possible verbalizations. Based on analysis on proofs in mathematical textbooks, thirteen PCAs are identified and employed in PROVERB, see [Hua94b] for details. The simplist one conveying the derivation of a new intermediate conclusion is illustrated in the introduction. There are also PCAs that update the global attentional structure. These PCAs also convey a partial plan for the further presentation. For instance, the PCA (Begin-Cases Goal: Formula Assumptions: (A B)) creates two attentional units with A and B as the assumptions, and Formula as the goal by producing the verbalization: \"To prove Formula, let us consider the two cases by assuming A and B.\" Top-Down Planning Top-down presentation operators express communicative norms concerning how a proof to be presented can be split into subproofs, as well as how the hierarchically structured subproofs can be mapped onto some linear order for presentation. Because it is not the main concern of this paper, we will look at only one such operator, which handles proof segments containing cases. The corresponding schema of such a proof tree is shown in Figure 1 , where the subproof rooted at ?L4 leads to F V G, while subproofs rooted at ?L2 and ?La are the two cases proving Q by assuming F or G, respectively. Under two circumstances a writer may recognize that he is confronted with a proof segment containing cases. First, when the subproof that has the structure as given above is the current presentation task, tested by (task ?L1) 1. Second, 1 Labels stand for the corresponding nodes ). In both circumstances, a communication norm motivates the writer to first present the part leading to F V G (in the second case this subgoal has already been achieved), and then to proceed with the two cases. This norm also requires that certain PCAs be used to mediate between parts of proofs. This procedure is ,captured by the presentation operator below. Case-Implicit \u2022 Proof: as given in Figure 1 \u2022 Applicability Condition: ((task ?L1) V (local-focus ?L4)) A (not-conveyed (?L2 ?L3)) \u2022 Acts: 1. if ?L4 has not been conveyed, then present ?L4 (subgoal 1) 2. a PCA with the verbalization: \"First, let us consider the first case by assuming F.\" 3. present ?L2 (subgoal 2) 4. a PCA with the verbalization: \"Next, we consider the second case by assuming G.\" 5. present ?L3 (subgoal 3) 6. mark ?L1 as conveyed \u2022 features: (top-down compulsory implicit) Bottom-up Presentation The bottom-up presentation process simulates the unplanned part of proof presentation. Instead of splitting presentation goals into subgoals, it follows the local derivation relation to find a proof node to be presented next. The Local Focus The node to be presented next is suggested by the mechanism of local focus. Although logically any proof node having the local focus as a child could be chosen for the next step, usually the one with the greatest semantic overlap with the focal centers is preferred. As mentioned above, focal centers are semantic objects mentioned in the proof node which is the local focus. This is based on the observation that if one has proved a property about some semantic objects, one will tend to continue to talk about these particular objects, before turning to new objects. Let us examine the situation when the proof below is awaiting presentation. [1]: P(a,b) [1]: P(a,b), [3]: S(c) [2]: Q(a,b)' [4]: R(b,c) [5]: Q(a, b) A R(b, c) Assume that node [1] is the local focus, {a, b} is the set of focal centers, [3] is a previously presented node and node [5] is the current task. [2] is chosen as the next node to be presented, since it does not (re)introduce any new semantic element and its overlapping with the focal centers ({a, b}) is larger than the overlap of [4] with the focal centers ({b}). The Bottom-Up Presentation Operators Under different circumstances the derivation of the nextnode is also presented in different ways. The corresponding presentation knowledge is encoded as bottom-up presentation operators. In this paper, we only examine the most frequently used bottom-up operator below: Derive-Bottom-Up * Proof: ?Node1,.. \"7 ?Noden ?M ?Noden+l * Applicability Condition: ?Noden+l is suggested by the focus mechanism as the next node, and ?Node1, ..., ?Noden are conveyed. \u2022 Acts: a PCA that conveys the fact that the conclusion ?Noden+l is derived from the premises ?Node1, ..., ?Noden by applying the method ?M. \u2022 Features: (bottom-up general explicit detailed) If the conclusion, the premises and the method are instantiated to a E $1, (a E $2, $1 E $2), and def-subset respectively, the following verbalization can be produced: \"Since a is an element of $1, and S1 is a subset of $2, a is an element of $2 according to the definition of subset.\" Currently seven bottom-up operators are integrated in PROVERB. The Attentional Hierarchy The distinction between planned and unplanned presentation leads to a very natural segmentation of the discourse into an attentional hierarchy, since following the theory of Grosz and Sidner [GS86] , there is a one-to-one correspondence between the intentional hierarchy and the attentional hierarchy. In this section, we illustrate the attentional hierarchy with the help of an example, which will be used to discuss reference choices. The input proof in Figure 2 2 is an ND style proof at the assertion level, abstracted from a machine-generated ND proof [Hua94a] , for a theorem taken from a mathematical textbook. 2The first 6 lines axe definitions and theorems use, which are omitted  The proof-to be presented is represented in a linearized version of ND proofs. In this formalism, every proof is a sequence of proof lines, each of them is of the form: Label ~ ~-Conclusion (Justification reason-pointers) where Justification is either an ND inference rule, a definition or theorem, which justifies the derivation of the Conclusion using formulas in lines pointed to by reasonpointers as the premises. ~ can be ignored for our purpose. The corresponding discourse model after the completion of the presentation is a proof tree shown in Figure 3 . Children of nodes are given in the order as they have been presented. The circles denote nodes which are first derived at this place, and nodes in the form of small boxes are copies of some previously derived nodes, which are used as premises again. The big boxes represent attentional units called proof units, created during the presentation process. The naturalness of this segmentation is largely due to the naturalness of the top-down presentation operators. For example, unit U2 has two subordinate units U3 and U4. This reflects a natural shift of attention between a subproof that derives a formula of the pattern 3~P(x) (node 10, 3~x E U), and the subproof that proceeds after assuming a new constant u satisfying P(u) (node 11, ul E U). There are also elementary units composed of multiple PCAs, such as U5 and U6. They produce two important premises required by a theorem about the concept solution, which are applied at node 20. It is interesting to node that elementary attentional units that contain multiple PCAs would be impossible, if we did not distinguish between hierarchical planning and local organization. Adapting the theory of Reichman for our purpose [Rei85] , we assume that each proof unit may have one of the following status: \u2022 a unit is said to be. open, if its root is still awaiting to be conveyed. -The active proof unit is the innermost proof unit containing the local focus. There is exactly one active proof unit at any moment. -The controlling proof unit is the innermost proof unit containing the active unit. -precontrol proof units are proof units containing the controlling proof unit. \u2022 Closed units are proof units containing only conveyed proof nodes. A Classification of Reference Forms This section presents a classification of the possible forms with which mathematicians refer to intermediate conclusions previously proved (called reasons) or to methods of inference. The classification is based on our analysis of proofs presented in mathematical textbooks. Reference Forms for Reasons Three reference forms have been identified for reasons in naturally occurring proofs: 1. The omit form: where a reason is not mentioned at all. 2. The explicit form: where a reason is literally stated. For instance, if the omit form and the explicit form are adopted for the first respectively second reason in the PCA in Section 1, the sentence may be produced: \"Since u is an element in U, u * 1u = u.\" 3. The implicit form: By an implicit form we mean that although a reason is not verbalized directly, an implicit hint is nevertheless given in other components of the PCA. That is, either in the verbalization of the inference method, or in that of the conclusion. For example, in the verbalization below \"Since u is an element in U, u* 1v = u by the definition of unit.\" the first reason of the PCA in Section 1, \"since 1v is the unit element of U\" is hinted at by the inference method which reads \"by the definition of unit\". Note that although omit and implicit forms lead to an identical surface structure, the existence of an implicit hint in the other part of the verbalization affects a reader's understanding. 4.2 Reference Forms for Methods PROVERB must select referring expressions for methods of inference in PCAs as well. Below are the three reference forms identified, which are analogous to the corresponding cases for reasons: 1. the explicit form: this is the case where a writer may decide to indicate explicitly which inference rule he is using. For instance, explicit translations of domainspecific rules could look like: \"by the definition of unit element\", or \"by the uniqueness of solution.\" ND rules have usually standard verbalizations. 2. the omit form: in this case a word such as \"thus\" or \"therefore\" will be used. 3. The implicit form: Similar to the implicit form for reasons, an implicit hint to a domain-specific inference method can be given either in the verbalization of the reasons, or in that of the conclusion. Making Reference Choices for Reasons Because reasons are intermediate conclusions proved previously in context, their reference choices have much in common with the problem of choosing anaphoric referring expressions in natural language generation in general. A number of theories have been put forward to account for the pronominalization, which is usually ascribed to the focus mechanism. For this purpose, concepts like activatedness, foregroundness and consciousness have been introduced. More recently, the shift of focus has been further investigated in the light of a more structured flow of discourse [Rei85, GS86, Dal92] . The issue of salience is also studied in a broader framework in [PC93] . Apart from salience, it is also shown that referring expressions are strongly influenced by other aspects of human preference. For example, easily perceivable attributes and basic-level attributes values are preferred [DH91, Da192, RD92] . Common to all discourse based theories, the update of the focus status is tightly coupled to the factoring of the flux of text into segments. As illustrated in section 3, we basically follow the approach of Grosz and Sidner [GS86] in that a direct correspondence between the plan hierarchy and the attentional spaces is assumed. With the segmentation problem settled, the reference choices in our theory largely follow the approach of Reichman. Reichman handles the reference problem in a more general framework of her discourse grammar [Rei85] . Based on empirical data, Reichman argues that the choice of referring expressions is constrained both by the status of the discourse space and by the object's level of focus within this space. In her theory, there are seven status assignments a discourse space may have at any given time. Within a discourse space, four levels of focus can be as-signed to individual objects: high, medium, low, or zero, since there are four major ways of referring to an object using English, namely, by using a pronoun, by name, by a description, or implicitly. Thirteen rules are formulated to assign level of focus to objects when they are activated, either with the initialization of a discourse unit or when they are added to the active unit. Four rules further reassign the level of focus on reentrance to a suspended discourse space. Based on the status assignment of discourse spaces, as well as the level of focus of individual objects, four rules are formulated constraining adequate referring expressions. In short, Reichman takes into account both the foreground and background status of discourse spaces as well as the level of focus of individual objects. As a simplification for argumentative discourse, the notions of structural closeness and textual closeness are introduced. The structural closeness of a reason reflects the foreground and background character of the innermost proof unit containing it. Intuitively, reasons that may still remain in the focus of attention at the current point from the structural perspective are considered as structurally close. Otherwise they are considered as structurally distant. If a re,on, for instance, is last mentioned or proved in the active proof unit (the unit a writer is currently working on), it is more likely that this reason should still remain in his focus of attention. On the other hand, if the reason is in a closed unit, and is not the root, it is very likely that the reason has already been moved out of the writer's focus of attention. Although the notion of fore-and backgroundness might actually be a continuum, our theory only distinguishes between reasons residing in proof units which are structurally close or structurally disrant. Rules assigning this structural status are given as following. 1. Reasons in the active unit are structurally close. 2. Reasons in the controlling unit are structurally close. 3. Reasons in closed units: (a) reasons that are the root of a closed proof unit immediate subordinate to the active unit are structurally close. (b) Other reasons in a closed unit are structurally distant. 4. Reasons in precontrol proof units are structurally distant. Note that, the rules are specified with respect to the innermost proof unit containing a proof node. Rule 3 means that only the conclusions of closed subordinated subproofs still remain in the focus of attention. As a special treatment, premises of the entire theorem will be defined as both structurally distant and far in distance, if they are not repeated at the beginning of the proof. The textual closeness is used as an approximation to the level of focus of an individual reason. In general, the level of focus of an object is established when it is activated, and decreases with the flow of discourse. In Reichman's theory, although four levels of focus can be established upon activation, only one is used in the formulation of the four reference rules. In other words, it suffices to track the status high alone. Based on the discussion above, we use only two values to denote the level of focus of individual intermediate conclusions, depending solely on the textual distance between the last mentioning of a reason and the current sentence where the reason is referred to. In summary, we assume that each intermediate conclusion is put into high focus when it is presented as a newly derived result or cited as a reason supporting the derivation of a further intermediate result. This level of focus decreases, either when a proof unit is moved out of the foreground of discussion, or with the increase of textual distance. On account of the above, the four reference rules used in our computational model are given below. Choices for Referring Expressions for Reasons 1. If a reason is both structurally and textually close, it will be omitted. 2. If a reason is structurally close but textually far, first try to find an implicit form, if impossible, use an explicit form. 3. If a reason is structurally distant but textually close, first try to find an implicit form, if impossible, omit it. 4. An explicit form will be used for reasons that are both structurally and textually far. Notice that the result of applying rule 2 and rule 3 depends on the fact that an implicit form is available, which often interacts with the verbalization of the rest of the PCA. In particular, it interacts with the reference choices for inference methods. In PROVERB as it currently stands, the interaction is realized by associating a keyword with the verbalization of every predicate, function, and assertion. For instance, suppose the verbalization of unit(F, 1, *) as a reason is \"since 1 is an unit element of F\", and the verbalization of the definition of unit element as an inference method is \"by the definition of the unit element\". Both the predicate unit and the definition are associated with the same keyword \"unit\". Based on this information, PROVERB assumes that the verbalization of the reason unit(F, 1, *) and the verbalization of the definition of unit element hint at each other. Distance is currently calculated in an ad hoc way by counting the PCAs uttered after the corresponding reason was last explicitly mentioned. Making Reference Choices for Inference Methods Like the reference to a reason, the explicitness or implicitness of referring to an inference method at a particular point depends on whether the particular method can be easily called into the foreground of the focus of attention. In contrast to references to reasons, this is evidently irrelevant to the particular discourse structure concerned. Actually it is less concerned with the proof context than with the user's familiarity with the particular inference method. This referring behavior remains \u2022 the same throughout a whole discourse, similar to the referring behavior relating to the so-called canonical salience [PC93] . In the case of applications of definitions or theorems, it depends on the reader's familiarity with the corresponding definition or theorem. This is found to be sensitive to the global hierarchy of the mathematical theories. As it currently stands, PROVERB distinguishes only between assertions in the underlying theories and assertions belonging to the current theory. The reference choice rules for inference methods currently incorporated are listed as follows. \u00b0 Choices for Referring Expressions for Methods 1. Reference Choices for ND Inference Rules (a) All non-structural ND rules (such as eliminations of quantifiers) will be omitted (In the case of PCA DERIVE, a word like \"thus\", \"hence\", etc. will be used), because the readers are supposed to be familiar with the elementary logic. (b) All structural ND rules (such as the one dividing proofs into eases) will be explicitly given. Although they are also familiar to the readers, they provide landmarks for the overall proof structure. Reference Choices for Applications of Assertions Readers are assumed to be familiar with definitions and theorems of the \"underlying theories\" upon which the current theory is based. For example, when we are reasoning about properties of group theory we assume that the users are familiar with basic set theory: (a) Applications of definitions and theorems of underlying theories will be omitted. (b) For applications of definitions or theorems of the current theory, try first to find an implicit form. If impossible, an explicit indication will be given. An Integrated Algorithm for Reference Choices As illustrated above, reference choices for reasons and for methods interact with each other. This section describes an algorithm that combines the reference choice rules for reason and the reference choice rules for methods, to produce preverbal messages (PMs) from PCAs. As such, the main task is to utilize the interaction between the two sets of reference rules to eliminate the indeterminacy in both of the rule sets. The indeterminacy lies in reference rule 1 and 2 in Section 5 and in reference rule 2(b) in Section 6, which need information on decisions made by the other set of rules. In other words, decisions in one rule set may help to narrow the alternatives in the other set. PROVERB first makes the reference choice for the inference method. While doing so, it looks ahead and takes the possible reference choices for reasons into account. If still no unique choice can be made, the decision is made according to a predetermined order. Concretely the explicit form will be chosen for rule 2(b) in Section 6. After the reference form of the method has been determined, a unique reference form can always be determined for the reasons. The concrete algorithm is omitted due to space restrictions. Now we continue with the subgroup example introduced in section 2. The PCA below aims to convey the derivation of proof node 9 (1v E U) from a part of node 7 (unit(Iv, U, *)), justified by the application of the definition of the unit element in semigroups. (Derive Derived-Formula: iu EU Reasons : unit(iU, U, *) Method: Def-Semigroup*Unit) The current unit is U3. U2 and U1 are the controlling and precontrol unit, respectively, see Figure 3 . Since node 7 is in the controlling unit and is mentioned last only two steps earlier, it is therefore judged as both structurally close and near in distance. Rule 1 in Section 5 is applicable and the omit form is chosen. Since the definition of the unit element resides in the current theory, Rule 2(b) in Section 6 suggests that its application be referred to either implicitly or explicitly. Because the implicit option is ruled out by the omit form for reasons, the explicit form is chosen. The PM below is therefore generated: (Derive Derived-Formula: iu qU Method: Def-Semigroup*Unit) Next let us jump over some steps and consider the PCA below. (Derive Derived-Formula: u * IU = u Reasons: (unit(1u, U,*), u qU) Method: Def-unit) This PCA is generated to convey that node 12 (u * 1u) can be derived from part of node 7 (unit(1u, U,*)) and node 11 (u E U) by applying the definition of the unit element. U5 is now the current unit, with U4 and U2 as controlling and precontrol unit. U3 is the unique closed unit. Reason node 7 is now structurally distant but still near in distance, and node 11 is in the current unit, and is the node last conveyed, therefore, both 7 and 11 are omitted. The reference form for the definition of unit element is decided upon as above. The PM below is generated: (Derive Derived-Formula: u \u2022 iu ----u Method : Def-Semigroup*Unit) Fifteen preverbal messages are generated by PRO VERB for our example. One sentence in English is generated from each PM by the surface generator TAG-GEN [Kilger94] . The text as generated follows. Let F be a group, U be a subgroup of F, 1 be a unit element ofF and 1u be a unit element of U. According to the definition of unit element, 1u E U. Therefore there is an X, X E U. Now suppose that ul is such an X. According to the definition of unit element, ut * lu = ul. Since U is a subgroup of F, U C F. Therefore 1u E F. Similarly ul E F, since ul E U. Since F is a group, F is a semigroup. Because ul * 1u = ul, 1u is a solution of the equation ul * X = u~. Since 1 is a unit element ofF, ul * 1 = ul. Since 1 is a unit element ofF, 1 E F. Because ul E F, 1 is a solution of the equation ul * X = ul. Since F is a group, 1u = 1 by the uniqueness_of solution. This conclusion is independent of the choice of the element ul. Conclusion This paper describes the way in which PROVERB refers to previously conveyed intermediate results and inference methods while verbalizing natural deduction style proofs. By distinguishing between the planned and unplanned part of NL generation, PRO VERB achieves a natural segmentation of context into an attentional hierarchy. Based on this segmentation, PROVERB makes reference decisions basically according to a discourse theory adapted from Reichmann for this special application. The first experience shows that output texts are of good quality. Currently, it proves difficult to compare text generated by PROVERB with naturally occurring texts, since the latter are usually at a still higher level of abstraction then the assertion level proofs we can reconstruct [Hua94a] . Nonetheless it might still be useful to build up a small corpus of texts. On the other hand, the naturMness of references could also be improved by further experimenting with different settings of the ad hoc thresholds in the system. We are also exploring more flexible lexicon choices as well as refinement of text planning process. Acknowledgment This work was supported by the Deutsche Forschungsgemeinschaft, SFB 314 (D2). Thanks are due to Dan Nesmith, who carefully read this final version. I am also grateful to the two anonymous reviewers for their critical and constructive remarks.",
    "abstract": "This paper deals with the reference choices involved in the generation of argumentative text. A piece of argumentative text such as the proof of a mathematical theorem conveys a sequence of derivations. For each step of derivation, the premises (previously conveyed intermediate results) and the inference method (such as the application of a particular theorem or definition) must be made clear. The appropriateness of these references crucially affects the quality of the text produced. Although hot restricted to nominal phrases, our reference decisions are similar to those concerning nominal subsequent referring expressions: they depend on the availability of the object referred to within a context and are sensitive to its attentional hierarchy. In this paper, we show how the current context can be appropriately segmented into an attentional hierarchy by viewing text generation as a combination of planned and unplanned behavior, and how the discourse theory of Reichmann can be adapted to handle our special reference problem.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "8",
    "year": "1994",
    "month": "",
    "title": "Planning Reference {CH}oices for Argumentative Texts"
}