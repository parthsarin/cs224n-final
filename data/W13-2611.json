{
    "article": "We describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning. Working in an inherently incremental framework, Dynamic Syntax, we show how words can be associated with probabilistic procedures for the incremental projection of meaning, providing a grammar which can be used directly in incremental probabilistic parsing and generation. We test this on child-directed utterances from the CHILDES corpus, and show that it results in good coverage and semantic accuracy, without requiring annotation at the word level or any independent notion of syntax. Introduction Human language processing has long been thought to function incrementally, both in parsing and production (Crocker et al., 2000; Ferreira, 1996) . This incrementality gives rise to many characteristic phenomena in conversational dialogue, including unfinished utterances, interruptions and compound contributions constructed by more than one participant, which pose problems for standard grammar formalisms (Howes et al., 2012) . In particular, examples such as (1) suggest that a suitable formalism would be one which defines grammaticality not in terms of licensing strings, but in terms of constraints on the semantic construction process, and which ensures this process is common between parsing and generation. (1) A: I burnt the toast. * We are grateful to Ruth Kempson for her support and helpful discussions throughout this work. We also thank the CMCL'2013 anonymous reviewers for their constructive criticism. This work was supported by the EPSRC, RISER project (Ref: EP/J010383/1), and in part by the EU, FP7 project, SpaceBook (Grant agreement no: 270019). B: But did you burn . . . A: Myself? Fortunately not. [where \"did you burn myself?\" if uttered by the same speaker is ungrammatical] One such formalism is Dynamic Syntax (DS) (Kempson et al., 2001; Cann et al., 2005) ; it recognises no intermediate layer of syntax, but instead reflects grammatical constraints via constraints on the word-by-word incremental construction of meaning, underpinned by attendant concepts of underspecification and update. Eshghi et al. (2013) describe a method for inducing a probabilistic DS lexicon from sentences paired with DS semantic trees (see below) representing not only their meaning, but their functionargument structure with fine-grained typing information. They apply their method only to an artificial corpus generated using a known lexicon. Here, we build on that work to induce a lexicon from real child-directed utterances paired with less structured Logical Forms in the form of TTR Record Types (Cooper, 2005) , thus providing less supervision. By assuming only the availability of a small set of general compositional semantic operations, reflecting the properties of the lambda calculus and the logic of finite trees, we ensure that the lexical entries learnt include the grammatical constraints and corresponding compositional semantic structure of the language. Our method exhibits incrementality in two senses: incremental learning, with the grammar being extended and refined as each new sentence becomes available; resulting in an inherently incremental, probabilistic grammar for parsing and production, suitable for use in state-of-the-art incremental dialogue systems (Purver et al., 2011) and for modelling humanhuman dialogue. Grammar Induction and Semantics We can view existing grammar induction methods along a spectrum from supervised to unsupervised. Fully supervised methods take a parsed corpus as input, pairing sentences with syntactic trees and words with their syntactic categories, and generalise over the phrase structure rules to learn a grammar which can be applied to a new set of data. Probabilities for production rules sharing a LHS category can be estimated, producing a grammar suitable for probabilistic parsing and disambiguation e.g. a PCFG (Charniak, 1996) . While such methods have shown great success, they presuppose detailed prior linguistic information and are thus inadequate as human grammar learning models. Fully unsupervised methods, on the other hand, proceed from unannotated raw data; they are thus closer to the human language acquisition setting, but have seen less success. In its pure form -positive data only, without bias-unsupervised learning is computationally too complex ('unlearnable') in the worst case (Gold, 1967) . Successful approaches involve some prior learning or bias (see (Clark and Lappin, 2011 )) e.g. a set of known lexical categories, a probability distribution bias (Klein and Manning, 2005) or a semisupervised method with shallower (e.g. POS-tag) annotation (Pereira and Schabes, 1992) . Another point on the spectrum is lightly supervised learning: providing information which constrains learning but with little or no lexicosyntactic detail. One possibility is the use of semantic annotation, using sentence-level propositional Logical Forms (LF). It seems more cognitively plausible, as the learner can be said to be able to understand, at least in part, the meaning of what she hears from evidence gathered from (1) her perception of her local, immediate environment given appropriate biases on different patterns of individuation of entities and relationships between them, and (2) helpful interaction, and joint focus of attention with an adult (see e.g. (Saxton, 1997) ). Given this, the problem she is faced with is one of separating out the contribution of each individual linguistic token to the overall meaning of an uttered linguistic expression (i.e. decomposition), while maintaining and generalising over several such hypotheses acquired through time as she is exposed to more utterances involving each token. This has been successfully applied in Combinatorial Categorial Grammar (CCG) (Steedman, 2000) , as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2012) ; as CCG is a lexicalist framework, grammar learning involves inducing a lexicon assigning to each word its syntactic and semantic contribution. Moreover, the grammar is learnt incrementally, in the sense that the learner collects data over time and does the learning sentence by sentence. Following this approach, Eshghi et al. ( 2013 ) outline a method for inducing a DS grammar from semantic LFs. This brings an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is inherently word-by-word incremental (see section 2.2 below). However, their method requires a higher degree of supervision than (Kwiatkowski et al., 2012) : the LFs assumed are not simply flat semantic formulae, but full DS semantic trees (see e.g. Fig. 1 ) containing information about the function-argument structure re-quired for their composition, in addition to fine grained type and formula annotations. Further, they test their method only on artificial data created using a known, manually-specified DS grammar. In contrast, in this paper we provide an approach which can learn from LFs without any compositional structure information, and test it on real language data; thus providing the first practical learning system for an explicitly incremental grammar that we are aware of. Dynamic Syntax (DS) Dynamic Syntax (Kempson et al., 2001; Cann et al., 2005) is a parsing-directed grammar formalism, which models the word-by-word incremental processing of linguistic input. Unlike many other formalisms, DS models the incremental building up of interpretations without presupposing or indeed recognising an independent level of syntactic processing. Thus, the output for any given string of words is a purely semantic tree representing its predicate-argument structure; tree nodes correspond to terms in the lambda calculus, decorated with labels expressing their semantic type (e.g. T y(e)) and formula, with beta-reduction determining the type and formula at a mother node from those at its daughters (Figure 1 ). These trees can be partial, containing unsatisfied requirements for node labels (e.g. ?T y(e) is a requirement for future development to T y(e)), and contain a pointer \u2666 labelling the node currently under development. Grammaticality is defined as parsability: the successful incremental construction of a tree with no outstanding requirements (a complete tree) using all information given by the words in a sentence. The complete sentential LF is then the formula decorating the root node -see Figure 1 . Note that in these trees, leaf nodes do not necessarily correspond to words, and may not be in linear sentence order; syntactic structure is not explicitly represented, only the structure of semantic predicate-argument combination. Actions in DS The parsing process is defined in terms of conditional actions: procedural specifications for monotonic tree growth. These include general structurebuilding principles (computational actions), putatively independent of any particular natural language, and language-specific actions associated with particular lexical items (lexical actions). The latter are what we learn from data here. Computational actions These form a small, fixed set, which we assume as given here. Some merely encode the properties of the lambda calculus and the logical tree formalism itself, LoFT (Blackburn and Meyer-Viol, 1994 ) -these we term inferential actions. Examples include THIN-NING (removal of satisfied requirements) and ELIMINATION (beta-reduction of daughter nodes at the mother). These actions are languageindependent, cause no ambiguity, and add no new information to the tree; as such, they apply nonoptionally whenever their preconditions are met. Other computational actions reflect the fundamental predictivity and dynamics of the DS framework. For example, *-ADJUNCTION introduces a single unfixed node with underspecified tree position (replacing feature-passing or type-raising concepts for e.g. long-distance dependency); and LINK-ADJUNCTION builds a paired (\"linked\") tree corresponding to semantic conjunction (licensing relative clauses, apposition and more). These actions represent possible parsing strategies and can apply optionally whenever their preconditions are met. While largely languageindependent, some are specific to language type (e.g. INTRODUCTION-PREDICTION in the form used here applies only to SVO languages). Lexical actions The lexicon associates words with lexical actions; like computational actions, these are sequences of tree-update actions in an IF..THEN..ELSE format, and composed of explicitly procedural atomic tree-building actions such as make (creates a new daughter node), go (moves the pointer), and put (decorates the pointed node with a label). Figure 2 shows an example for a proper noun, John. The action checks whether the pointed node (marked as \u2666) has a requirement for type e; if so, it decorates it with type e (thus satisfying the requirement), formula John \u2032 and the bottom restriction \u2193 \u22a5 (meaning that the node cannot have any daughters). Otherwise the action aborts, i.e. the word 'John' cannot be parsed in the context of the current tree. Graph-based Parsing & Generation These actions define the parsing process. Given a sequence of words (w 1 , w 2 , ..., w n ), the parser starts from the axiom tree T 0 (a requirement to construct a complete propositional tree, ?T y(t)), and applies the corresponding lexical actions (a 1 , a 2 , . . . , a n ), optionally interspersing computational actions.  This parsing process can be modelled as a directed acyclic graph (DAG) rooted at T 0 , with partial trees as nodes, and computational and lexical actions as edges (i.e. transitions between trees) (Sato, 2011) . Figure 3 shows an example: here, intro, pred and *adj correspond to the computational actions INTRODUCTION, PREDICTION and *-ADJUNCTION respectively; and 'john' is a lexical action. Different DAG paths represent different parsing strategies, which may succeed or fail depending on how the utterance is continued. Here, the path T 0 \u2212 T 3 will succeed if 'John' is the subject of an upcoming verb (\"John upset Mary\"); T 0 \u2212 T 4 will succeed if 'John' turns out to be a left-dislocated object (\"John, Mary upset\"). This incrementally constructed DAG makes up the entire parse state at any point. The rightmost nodes (i.e. partial trees) make up the current maximal semantic information; these nodes with their paths back to the root (tree-transition actions) make up the linguistic context for ellipsis and pronominal construal (Purver et al., 2011) . Given a conditional probability distribution P (a|w, T ) over possible actions a given a word w and (some set of features of) the current partial tree T , we can parse probabilistically, constructing the DAG in a best-first, breadth-first or beam parsing manner. Generation uses exactly the same actions and structures, and can be modelled on the same DAG with the addition only of a goal tree; partial trees are checked for subsumption of the goal at each stage. The framework therefore inherently provides both parsing and generation that are word-by-word incremental and interchangeable, commensurate with psycholinguistic results (Lombardo and Sturt, 1997; Ferreira and Swets, 2002) and suitable for modelling dialogue (Howes et al., 2012) . While standard grammar formalisms can of course also be used with incremental parsing or generation algorithms (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007) , their string-based grammaticality and lack of inherent parsing-generation interoperability means examples such as (1) remain problematic. Method Our task here is to learn an incremental DS grammar; following Kwiatkowski et al. (2012) , we assume as input a set of sentences paired with their semantic LFs. Eshghi et al. (2013) outline a method for inducing DS grammars from semantic DS trees (e.g. Fig. 1 ), in which possible lexical entries are incrementally hypothesized, constrained by subsumption of the target tree for the sentence. Here, however, this structured tree information is not available to us; our method must therefore constrain hypotheses via compatibility with the sentential LF, represented as Record Types of Type Theory with Records (TTR). Type Theory with Records (TTR) Type Theory with Records (TTR) is an extension of standard type theory shown useful in semantics and dialogue modelling (Cooper, 2005; Ginzburg, 2012) . It is also used for representing non-linguistic context such as the visual perception of objects (Dobnik et al., 2012) , suggesting potential for embodied learning in future work. Some DS variants have incorporated TTR as the semantic LF representation (Purver et al., 2011; Hough and Purver, 2012; Eshghi et al., 2012) . Here, it can provide us with the mechanism we need to constrain hypotheses in induction by restricting them to those which lead to subtypes of the known sentential LF. In TTR, logical forms are specified as record types (RTs), sequences of fields of the form [ l : T ] containing a label l and a type T . RTs can be witnessed (i.e. judged true) by records of that type, where a record is a sequence of label-value pairs Fields can be manifest, i.e. given a singleton type e.g. [ l : T a ] where T a is the type of which only a is a member; here, we write this using the syntactic sugar [ l =a : T ]. Fields can also be dependent on fields preceding them (i.e. higher) in the record type -see R 1 in Figure 4 . Importantly for us here, the standard subtyping relation \u2291 can be defined for record types: [ l = v ], and [ l = v ] is of type [ l : T ] just in case v is of type T . R 1 : \uf8ee \uf8f0 l 1 : T 1 l 2=a : T 2 l 3=p(l 2 ) : T 3 \uf8f9 \uf8fb R 2 : l 1 : T 1 l 2 : T 2 \u2032 R 3 : [] R 1 \u2291 R 2 if for all fields [ l : T 2 ] in R 2 , R 1 contains [ l : T 1 ] where T 1 \u2291 T 2 . In Figure 4, R 1 \u2291 R 2 if T 2 \u2291 T 2 \u2032 , and both R 1 and R 2 are subtypes of R 3 . Following Purver et al. (2011) , we assume that DS tree nodes are decorated not with simple atomic formulae but with RTs, and corresponding lambda abstracts representing functions from RT to RT (e.g. \u03bbr : [ l 1 : T 1 ].[ l 2=r.l 1 : T 1 ] where r.l 1 is a path expression referring to the label l 1 in r) -see Figure 5 . The equivalent of conjunction for linked trees is now RT extension (concatenation modulo relabelling -see (Cooper, 2005; Fern\u00e1ndez, 2006) ). TTR's subtyping relation now allows a record type at the root node to be inferred for any partial tree, and incrementally further specified via subtyping as parsing proceeds (Hough and Purver, 2012) . We assume a field head in all record types, with this corresponding to the DS tree node type. We also assume a neo-Davidsonian representation of  predicates, with fields corresponding to the event and to each semantic role; this allows all available semantic information to be specified incrementally via strict subtyping (e.g. providing the subj() field when subject but not object has been parsed) -see Figure 5 for an example. Problem Statement Our induction procedure now assumes as input: \u2022 a known set of DS computational actions. \u2022 a set of training examples of the form S i , R T i , where S i = w 1 . . . w n is a sentence of the language and R T i -henceforth referred to as the target RT -is the record type representing the meaning of S i . The output is a grammar specifying the possible lexical actions for each word in the corpus. Given our data-driven approach, we take a probabilistic view: we take this grammar as associating each word w with a probability distribution \u03b8 w over lexical actions. In principle, for use in parsing, this distribution should specify the posterior probability p(a|w, T ) of using a particular action a to parse a word w in the context of a particular partial tree T . However, here we make the simplifying assumption that actions are conditioned solely on one feature of a tree, the semantic type T y of the currently pointed node; and that actions apply exclusively to one such type (i.e. ambiguity of type implies multiple actions). This simplifies our problem to specifying the probability p(a|w). In traditional DS terms, this is equivalent to assuming that all lexical actions have a simple IF clause of the form IF ?T y(X); this is true of most lexical actions in existing DS grammars (see Fig. 2 ), but not all. Our assumption may therefore lead to over-generation -inducing actions which can parse some ungrammatical strings -we must rely on the probabilities learned to make such parses unlikely, and evaluate this in Section 4. Given this, our focus here is on learning the THEN clauses of lexical actions: sequences of DS atomic actions such as go, make, and put (Fig. 2 ), but now with attendant posterior probabilities. We will henceforth refer to these sequences as lexical hypotheses. We first describe how we construct lexical hypotheses from individual training examples; we then show how to generalise over these, while incrementally estimating corresponding probability distributions. Hypothesis construction DS is strictly monotonic: actions can only extend the current (partial) tree T cur , deleting nothing except satisfied requirements. Thus, we can hypothesise lexical actions by incrementally exploring the space of all monotonic, well-formed extensions T of T cur , whose maximal semantics R is a supertype of (extendible to) the target R T (i.e. R \u2291 R T ). This gives a bounded space described by a DAG equivalent to that of section 2.2.1: nodes are trees; edges are possible extensions; paths start from T cur and end at any tree with LF R T . Edges may be either known computational actions or new lexical hypotheses. The space is further constrained by the properties of the lambda-calculus and the modal tree logic LoFT (not all possible trees and extensions are well-formed). 1 Hypothesising increments In purely semantic terms, the hypothesis space at any point is the possible set of TTR increments from the current LF R to the target R T . We can efficiently compute and represent these possible increments using a type lattice (see Figure 6 ), 2 which can be constructed for the whole sentence before processing each training example. Each edge is a RT R representing an increment from one RT, R j , to another, R j+1 , such that R j \u2227 R I = R j+1 (where \u2227 represents record type intersection (Cooper, 2005) ); possible parse DAG paths must correspond to some path through this lattice. Hypothesising tree structure These DAG paths can now be hypothesised with the lattice as a constraint: hypothesising possible sequences of ac-1 We also prevent arbitrary type-raising by restricting the types allowed, taking the standard DS assumption that noun phrases have semantic type e (rather than a higher type as in Generalized Quantifier theory) and common nouns their own type cn, see Cann et al. (2005) , chapter 3 for details. 2 Clark (2011) similarly use a concept lattice relating strings to their contexts in syntactic grammar induction. tions which extend the tree to produce the required semantic increment, while the increments themselves constitute a search space of their own which we explore by traversing the lattice. The lexical hypotheses comprising these DAG paths are divide into two general classes: (1) treebuilding hypotheses, which hypothesise appropriately typed daughters to compose a given node; and (2) content hypotheses, which decorate leaf nodes with appropriate formulae from R i (nonleaf nodes then receive their content via betareduction/extension of daughters). Tree-building can be divided into two general options: functional decomposition (corresponding to the addition of daughter nodes with appropriate types and formulae which will form a suitable mother node by beta-reduction); and type extension (corresponding to the adjunction of a linked tree whose LF will extend that of the current tree, see Sec. 3.1 above). The availability of the former is constrained by the presence of suitable dependent types in the LF (e.g. in Fig. 5 , p = subj(e, x) depends on the fields with labels x and e, and could therefore be hypothesised as the body of a function with x and/or e as argument). The latter is more generally available, but constrained by sharing of a label between the resulting linked trees. Figure 7 shows an example: a template for functional decomposition hypotheses, extending a node with some type requirement ?T y(X) with daughter nodes which can combine to satisfy that requirement -here, of types Y and Y \u2192 X. Specific instantiations are limited to a finite set of types: e.g. X = e \u2192 t and Y = e is allowed, but higher types for Y are not. We implement these constraints by packaging together permitted sequences of tree updates as macros, and using these macros to hypothesise DAG paths commensurate with the lattice. Finally, semantic content decorations (as se-IF ?T y(X) THEN make( \u21930 ); go( \u21930 ) put(?T y(Y )); go( \u2191 ) make( \u21931 ); go( \u21931 ) put(?T y(Y \u2192 X)); go(\u2191) ELSE ABORT Figure 7 : Tree-building hypothesis quences of put operations) are hypothesised for the leaf nodes of the tree thus constructed; these are now determined entirely by the tree structure so far hypothesised and the target LF R T . Probabilistic Grammar Estimation This procedure produces, for each training sentence w 1 . . . w n , all possible sequences of actions that lead from the axiom tree T 0 to a tree with the target RT as its semantics. These must now be split into n sub-sequences, hypothesising a set of word boundaries to form discrete word hypotheses; and a probability distribution estimated over this (large) word hypothesis space to provide a grammar that can be useful in parsing. For this, we apply the procedure of Eshghi et al. (2013) . For each training sentence S = w 1 . . . w n , we have a set HT of possible Hypothesis Tuples (sequences of word hypotheses), each of the form HT j = h j 1 . . . h j n , where h j i is the word hypothesis for w i in HT j . We must estimate a probability distribution \u03b8 w over hypotheses for each word w, where \u03b8 w (h) is the posterior probability p(h|w) of a given word hypothesis h being used to parse w. Eshghi et al. (2013) define an incremental version of Expectation-Maximisation (Dempster et al., 1977) for use in this setting. Re-estimation At any point, the Expectation step assigns each hypothesis tuple HT j a probability based on the current estimate \u03b8 \u2032 w : p(HTj|S) = n i=1 p(h j i |wi) = n i=1 \u03b8 \u2032 w i (h j i ) (2) The Maximisation step then re-estimates p(h|w) as the normalised sum of the probabilities of all observed tuples HT j which contain h, w: \u03b8 \u2032\u2032 w (h) = 1 Z {j|h,w\u2208HT j } n i=1 \u03b8 \u2032 w i (h j i ) (3) where Z is the appropriate normalising constant summed over all the HT j 's. Incremental update The estimate of \u03b8 w is now updated incrementally at each training example: the new estimate \u03b8 N w is a weighted average of the previous estimate \u03b8 N \u22121 w and the new value from the current example \u03b8 \u2032\u2032 w from equation (3): \u03b8 N w (h) = N \u2212 1 N \u03b8 N\u22121 w (h) + 1 N \u03b8 \u2032\u2032 w (h) (4) \u03bbe.not (aux|do(v|have(pro|he, det|a(x, n|hat(x) ), e), e), e) \u2193 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 Experimental Setup Corpus We tested our approach on a section of the Eve corpus within CHILDES (MacWhinney, 2000) , a series of English child-directed utterances, annotated with LFs by Kwiatkowski et al. (2012) following Sagae et al. (2004) 's syntactic annotation. We convert these LFs into semantically equivalent RTs; e.g. Fig 8 shows the conversion to a record type for \"He doesn't have a hat\". Importantly, our representations remove all part-of-speech or syntactic information; e.g. the subject, object and indirect object predicates function as purely semantic role information expressing an event's participants. This includes e.g. do-aux(e) in (8), which is taken merely to represent temporal/aspectual information about the event, and could be part of any word hypothesis. From this corpus we selected 500 short utterance-record type pairs. The minimum utterance length in this set is 1 word, maximum 7, mean 3.7; it contains 1481 word tokens of 246 types, giving a type:token ratio of 6.0). We use the first 400 for training and 100 for testing; the test set also has a mean utterance length of 3.7 words, and contains only words seen in training. Evaluation We evaluate our learner by comparing the record type semantic LFs produced using the induced lexicon against the gold standard LFs, calculating precision, recall and f-score using a method similar to Allen et al. (2008) . Each field has a potential score in the range [0,1]. A method maxM apping(R 1 , R 2 ) constructs a mapping from fields in R 1 to those in R 2 to maximise alignment, with fields that map completely scoring a full 1, and partially mapped fields receiving less, depending on the proportion of the R 1 field's representation that subsumes its mapped R 2 field;e.g. a unary predicate field in RT 2 such as p =there(e) : t could score a maximum of 3 -1 for correct type t, 1 for correct predicate there and 1 for the subsumption of its argument e; we use the total to normalise the final score. The potential maximum for any pair is therefore the number of fields in R 1 (including those in embedded record types). So, for hypothesis H and goal record type G, with N H and N G fields respectively: (5) precision = maxM apping(H, G)/NH recall = maxM apping(H, G)/NG Results Table 1 shows that the grammar learned achieves both good parsing coverage and semantic accuracy. Using the top 3 lexical hypotheses induced from training, 92% of test set utterances receive a parse, and average LF f-score reaches 0.851. We manually inspected the learned lexicon for instances of ambiguous words to assess the system's ability to disambiguate (e.g. the word ''s' (is) has three different senses in our corpus: (1) auxiliary, e.g. \"the coffee's coming\"; (2) verb predicating NP identity, e.g. \"that's a girl\"; and (3) verb predicating location, e.g. \"where's the pencil\"). From these the first two were in the top 3 hypotheses (probabilities p=0.227 and p=0.068). For example, the lexical entry learned for (2) is shown in Fig. 9 . However, less common words fared worse: e.g. the double object verb 'put', with only 3 tokens, had no correct hypothesis in the top 5. Given sufficient frequency and variation in the token distributions, our method appears successful in inducing the correct incremental grammar. However, the complexity of the search space also limits the possibility of learning from larger record types, as the space of possible subtypes used for hypothesising IF ?T y(e \u2192 t) THEN make( \u2193 0 ); go( \u2193 0 ) put(?T y(e)) go( \u2191 0 ) make( \u2193 1 ); )) put( \u2193 \u22a5) ELSE ABORT Figure 9 : Action learned for second sense of 'is' tree structure grows exponentially with the number of fields in the type. Therefore, when learning from longer, more complicated sentences, we may need to bring in further sources of bias to constrain our hypothesis process further (e.g. learning from shorter sentences first). Conclusions We have outlined a novel method for the induction of a probabilistic grammar in an inherently incremental and semantic formalism, Dynamic Syntax, compatible with dialogue phenomena such as compound contributions and with no independent level of syntactic phrase structure. Assuming only general compositional mechanisms, our method learns from utterances paired with their logical forms represented as TTR record types. Evaluation on a portion of the CHILDES corpus of child-directed dialogue utterances shows good coverage and semantic accuracy, which lends support to viewing it as a plausible, yet idealised, language acquisition model. Future work planned includes refining the method outlined above for learning from longer utterances, and then from larger corpora e.g. the Groningen Meaning Bank (Basile et al., 2012) , which includes more complex structures. This will in turn enable progress towards large-scale incremental semantic parsers and allow further investigation into semantically driven language learning.",
    "abstract": "We describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning. Working in an inherently incremental framework, Dynamic Syntax, we show how words can be associated with probabilistic procedures for the incremental projection of meaning, providing a grammar which can be used directly in incremental probabilistic parsing and generation. We test this on child-directed utterances from the CHILDES corpus, and show that it results in good coverage and semantic accuracy, without requiring annotation at the word level or any independent notion of syntax.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "22",
    "year": "2013",
    "month": "August",
    "title": "Incremental Grammar Induction from Child-Directed Dialogue Utterances"
}