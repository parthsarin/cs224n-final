{
    "article": "This paper presents our solution for SemEval-2022 Task 10: Structured Sentiment Analysis. The solution consisted of two modules: the first for sequence tagging and the second for relation classification. In both modules we used transformer-based language models. In addition to utilizing language models specific to each of the five competition languages, we also adopted multilingual models. This approach allowed us to apply the solution to both monolingual and cross-lingual sub-tasks, where we obtained average Sentiment Graph F1 of 54.5% and 53.1%, respectively. The source code of the prepared solution is available at https://github.com/rafalposwiata/structuredsentiment-analysis. Introduction Structured Sentiment Analysis (SSA) can be formulated as an information extraction task in which one attempts to find all of the opinion tuples O = O i , ..., O n in a text. Each opinion O i is a tuple (h, t, e, p) where h is a holder who expresses a polarity p towards a target t through a sentiment expression e, implicitly defining pairwise relationships between elements of the same tuple (Barnes et al., 2021) . An example of such tuples as a structure sentiment graph was shown in Figure 1 . This problem is relatively new and there has been little work published on the subject to date. To stimulate interest in this issue among the NLP community the SemEval-2022 Task 10: Structured Sentiment Analysis (Barnes et al., 2022) competition was organized. The contest consisted of two sub-tasks: monolingual and cross-lingual. In the monolingual sub-task, the systems were trained and then tested on the datasets in the same languages. In the cross-lingual sub-task, systems had to be prepared for Catalan, Basque and Spanish datasets, while data in these languages could not be used for training. This setup is often known as zero-shot cross-lingual transfer (Hu et al., 2020) . In this paper we present our system for this competition. We mainly focused on the solution for the monolingual track, however, it has also been successfully applied to the cross-lingual. The rest of the paper is organized as follows. Section 2 briefly describes related work. Section 3 shows an overview of used datasets. Section 4 elaborates on our solution. Experiments showing the effectiveness of the created system performed on development and test sets are presented in Section 5. The next section briefly describes the mistakes and limitations of our system. Finally, Section 7 concludes this paper. Related Work Structured Sentiment Analysis can be broken down into five sub-tasks: a) expression (opinion) extraction, b) target (aspect) extraction, c) holder extraction, d) defining the relationship between these elements, and e) assigning polarity (Barnes et al., 2021) . 2  A few years ago, the main focus was on Aspect-Based Sentiment Analysis (ABSA), which only concerned on targets extraction (task b) and classifying the polarity towards them (task e) (Pontiki et al., 2014 (Pontiki et al., , 2015 (Pontiki et al., , 2016)) . Sequence tagging solutions have proven to be effective in this issue (Li et al., 2019a) . An extension of this problem was End2End Aspect-Based Sentiment Analysis (E2E-ABSA), which adds the issue of expression extraction (task a). He et al. (2019) propose an interactive multi-task learning network (IMN) which is able to jointly learn multiple related tasks simultaneously, to resolve this problem. Chen and Qian (2020) Table 1 : Statistics of the datasets. Mixed tags means a situation where a given term in different opinions plays a different role, e.g. once it is a target and once it is a holder. Nested tags are when a term in one opinion is part of a term in another opinion. Opposite polarity expressions refers to the case where a sentence contains an expression that has a different sentiment depending on the opinion. (Li et al., 2019b; Hu et al., 2019) . The tasks listed above did not require resolving relationships between extracted tags. The recently proposed, Aspect Sentiment Triplet Extraction (ASTE) fill this gap (Peng et al., 2020) . The task is to extracting all aspects terms with their corresponding opinion terms and sentiment polarity (tasks a, b, d and e). Peng et al. (2020) propose two stage model. In the first stage, it extracts opinions and aspects along with sentiment using sequence tagging based on the unified BIO scheme. The second stage pairs up the predicted terms from the first stage to output triplets. ASTE is most similar to SSA, missing only the holder extraction. For SSA, the subject of the competition, there are few solutions. Barnes et al. (2021) cast the structured sentiment problem as dependency graph parsing. Peng et al. (2021) extend this work and propose a sparse and fuzzy attention scorer with pooling layers which improves parser performance. Datasets Seven structured sentiment datasets in five languages were selected for the competition. The MPQA dataset (Wiebe et al., 2005) contains news documents from the world press in English. DS Unis (Toprak et al., 2010) are English reviews of online universities and e-commerce. OpeNER en and OpeNER es (Agerri et al., 2013) consist of hotel reviews in English and Spanish, respectively. MultiB eu and MultiB ca (Barnes et al., 2018) are also hotel reviews, but in Basque and Catalan. The last dataset is NoReC Fine (\u00d8vrelid et al., 2020) , a multi-domain dataset of professional reviews in Norwegian. The statistics of each dataset are sum- marized in Table 1 . System Overview The architecture of our solution is shown in Having already extracted entities, the role of the second module is to classify whether there is a relationship between them. Specifically, it is about verifying that there is a holder and/or target associated with a particu-lar expression. We utilized the R-BERT (Wu and He, 2019) model to accomplish this task. Based on a sentence with two appropriately marked entities (expression and holder/target), it determines whether or not they are related. 3 Entities that are related are combined and form an output. Extraction and Relation Classification modules are trained independently. Experiments Experimental Setup To conduct the experiments, we first utilized the Simple Transformers library (Rajapakse, 2019) for the implementation of the Extraction Module. For the Relation Classification Module we modify publicly available source code of R-BERT. 4 The hyperparameters used in learning each of these modules are presented in Table 2 . All models were run five times on a single GPU Tesla V100. Pretrained Language Models We chose two types of language models based on transformer architecture for experiments: monolingual (at least one for each of the five competition languages) and multilingual. The use of multilingual models allowed us to obtain a more general solution and was necessary for the cross-lingual sub-task. Metrics Following the works on Named Entity Recognition problem (Akbik et al., 2018; Yamada et al., 2020; Zhou and Chen, 2021) , we used micro-average F1 score as our main measure for the Extraction Module. In addition for this module we added a detailed measure for each tag type i.e. F1 score for holders, targets and expressions with sentiment classes, separately. For the Relation Classification Module, we used Accuracy and macro-average F1 measures. Evaluation of the overall system was based on the official competition metric i.e. Sentiment Graph F1. 5 https://huggingface.co/models Development Results Table 4 shows the results on the development sets for each module. For the Extraction Module, the XLM-R model was the best on five of the seven datasets. In only two cases (MPQA and DS Unis ) language-specific models were found to be superior: XLNet and RoBERTa, respectively. For the Relation Classification Module, we only used models based on the BERT architecture, following the original R-BERT work (Wu and He, 2019) . The mBERT usually proved to be the best (5/7 cases), except for two cases (MultiB eu and NoReC Fine ) where BERTeus and NB-BERT were the best. The best models for each module were used to test the overall system. A summary of this experiment can be found in Table 5 . The average Sentiment Graph F1 was 55.0%. Test Results The best models verified on the development sets were used on the test sets which are the official competition sets. For the monolingual sub-task, we used exactly the same configuration of models as in Table 5 . For the cross-lingual sub-task, we used models trained on the OpeNER en set, namely XLM-R for extraction and mBERT for relation classification. There were two reasons for this choice. First is the use of multilingual models in both modules. Second, from the fact that the results on the development sets were high compared to the results for other models trained on English language sets. The results are summarized in Table 6 . We achieved average SF1 scores of 54.5% and 53.1% for the monolingual and cross-lingual subtasks, respectively. This allowed us to rank 11th and 9th out of the 32 teams in these sub-tasks. Errors Analysis As a result of the used architecture, most errors are due to incorrect tagging. In particular, this is relevant to expressions where a correct sentiment is additionally required. The results were significantly worse for expressions limited in a given set, e.g., neutrals in the MPQA or DS Unis sets. Furthermore, by using a single extraction model, the solution is not able to correctly handle more complicated cases such as mixed or nested tags or opposite polarity expressions. This is most noticeable in the NoReC Fine dataset. results. The use of multilingual language models enabled the solution to be used for monolingual and cross-lingual sub-tasks. At the same time it can be easily extended e.g. by using an additional CRF layer (Souza et al., 2019) in the Extraction Module or by using other multilingual language models e.g. InfoXLM (Chi et al., 2021) .",
    "abstract": "This paper presents our solution for SemEval-2022 Task 10: Structured Sentiment Analysis. The solution consisted of two modules: the first for sequence tagging and the second for relation classification. In both modules we used transformer-based language models. In addition to utilizing language models specific to each of the five competition languages, we also adopted multilingual models. This approach allowed us to apply the solution to both monolingual and cross-lingual sub-tasks, where we obtained average Sentiment Graph F1 of 54.5% and 53.1%, respectively. The source code of the prepared solution is available at https://github.com/rafalposwiata/structuredsentiment-analysis.",
    "countries": [
        "Poland"
    ],
    "languages": [
        "English",
        "Catalan",
        "Basque"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "{OPI} at {S}em{E}val-2022 Task 10: Transformer-based Sequence Tagging with Relation Classification for Structured Sentiment Analysis"
}