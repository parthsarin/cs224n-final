{
    "article": "Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense-for example, \"I slept like a log\" does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful. Introduction In argument similes are like songs in love; they describe much, but prove nothing. -Franz Kafka Comparisons are fundamental linguistic devices that express the likeness of two things-be it entities, concepts or ideas. Given that their working principle is to emphasize the relation between the shared properties of two arguments (Bredin, 1998) , comparisons can synthesize important semantic knowledge. Often, comparisons are not meant to be understood literally. Figurative comparisons are an important figure of speech called simile. Consider the following two examples paraphrased from Amazon product reviews: (1) Sterling is much cheaper than gold. (2) Her voice makes this song shine brighter than gold. In (1) the comparison draws on the relation between the price property shared by the two metals, sterling and gold. While (2) also draws on a common property (brightness), the polysemantic use (vocal timbre vs. light reflection) makes the comparison figurative. Importantly, there is no general rule separating literal from figurative comparisons. More generally, the distinction between figurative and literal language is blurred and subjective (Hanks, 2006) . Multiple criteria for delimiting the two have been proposed in the linguistic and philosophical literature-for a comprehensive review, see Shutova (2010) -but they are not without exceptions, and are often hard to operationalize in a computational framework. When considering the specific case of comparisons, such criteria cannot be directly applied. Recently, the simile has received increasing attention from linguists and lexicographers (Moon, 2008; Moon, 2011; Hanks, 2013) as it became clearer that similes need to be treated separately from metaphors since they operate on fundamentally different principles (Bethlehem, 1996) . Metaphors are linguistically simple structures hiding a complex mapping between two domains, through which many properties are transferred. For example the conceptual metaphor of life as a journey can be instantiated in many particular ways: being at a fork in the road, reaching the end of the line (Lakoff and Johnson, 1980) . In contrast, the semantic context of similes tends to be very shallow, transferring a single property (Hanks, 2013) . Their more explicit syntactic structure allows, in exchange, for more lexical creativity. As Hanks (2013) puts it, similes \"tend to license all sorts of logical mayhem.\" Moreover, the overlap between the expressive range of similes and metaphors is now known to be only partial: there are similes that cannot be rephrased as metaphors, and the other way around (Israel et al., 2004) . This suggests that figurativeness in similes should be modeled differently than in metaphors. To further underline the necessity of a computational model for similes, we give the first estimate of their frequency in the wild: over 30% of comparisons are figurative. 1 We also confirm that a state of the art metaphor detection system performs poorly when applied directly to the task of detecting similes. In this work we propose a computational study of figurative language in comparisons. To this end, we build the first large collection of naturally occurring comparisons with figurativeness annotation, which we make publicly available. Using this resource we explore the linguistic patterns that characterize similes, and group them in two conceptually distinctive classes. The first class contains cues that are agnostic of the context in which the comparison appears (domain-agnostic cues). For example, we find that the higher the semantic similarity between the two arguments, the less likely it is for the comparison to be figurative-in the examples above, sterling is semantically very similar to gold, both being metals, but song and gold are semantically dissimilar. The second type of cues are domain-specific, drawing on the intuition that the domain in which a comparison is used is a factor in determining its figurativeness. We find, for instance, that the less specific a comparison is to the domain in which it appears, the more likely it is to be used in a figurative sense (e.g., in example (2), gold is very unexpected in the musical domain). We successfully exploit these insights in a new prediction task relevant to text understanding: discriminating figurative comparisons from literal ones. Encouraged by the high accuracy of our system-which is within 10% of that obtained by human annotators-we automatically extend the figurativeness labels to 80,000 comparisons occurring in product reviews. This enables us to conduct a fine-grained analysis of how comparison usage interacts with their social context, opening up a research direction with applications in sentiment analysis and opinion mining. In particular we find that figurative comparisons are more likely to accompany reviews showing extreme sentiment, and that they are uncommon in opinions deemed as being helpful. To the best of our knowledge, this is the first time figurative language is tied to the social context in which it appears. To summarize, the main contributions of this work are as follows: \u2022 it introduces the first large dataset of comparisons with figurativeness annotations (Section 3); \u2022 it unveils new linguistic patterns characterizing figurative comparisons (Section 4); \u2022 it introduces the task of distinguishing figurative from literal comparisons (Section 5); \u2022 it establishes the relation between figurative language and the social context in which it appears (Section 6). Further Related Work Corpus studies on figurative language in comparisons are scarce, and none directly address the distinction between figurative and literal comparisons. Roncero et al. (2006) observed, by searching the web for several stereotypical comparisons (e.g., education is like a stairway), that similes are more likely to be accompanied by explanations than equivalent metaphors (e.g., education is a stairway). Related to figurativeness is irony, which Veale (2012a) finds to often be lexically marked. By using a similar insight to filter out ironic comparisons, and by assuming that the rest are literal, Veale and Hao (2008) learn stereotypical knowledge about the world from frequently compared terms. A similar process has been applied to both English and Chinese by Li et al. (2012) , thereby encouraging the idea that the trope behaves similarly in different languages. A related system is the Jigsaw Bard (Veale and Hao, 2011) , a thesaurus driven by figurative conventional similes extracted from the Google N-grams. This system aims to build and generate canned expressions by using items frequently associated with the simile pattern above. An extension of the principles of the Jigsaw Bard is found in Thesaurus Rex (Veale and Li, 2013) , a data-driven partition of words into ad-hoc categories. Thesaurus Rex is constructed using simple comparison and hypernym patterns and is able to provide weighted lists of categories for given words. In text understanding systems, literal comparisons are used to detect analogies between related geographical places (Lofi et al., 2014) . Tandon et al. (2014) use relative comparative patterns (e.g., X is heavier than Y) to enrich a common-sense knowledge base. Jindal and Liu (2006) extract graded comparisons from various sources, with the objective of mining consumer opinion about products. They note that identifying objective vs. subjective comparisons-related to literality-is an important future direction. Given that many comparisons are figurative, a system that discriminates literal from figurative comparisons is essential for such text understanding and information retrieval systems. The vast majority of previous work on figurative language focused on metaphor detection. Tsvetkov et al. (2014a) propose a cross-lingual system based on word-level conceptual features and they evaluate it on Subject-Verb-Object triples and Adjective-Noun pairs. Their features include and extend the idea of abstractness used by Turney et al. (2011) for Adjective-Noun metaphors. Hovy et al. (2013) contribute an unrestricted metaphor corpus and propose a method based on tree kernels. Bridging the gap between metaphor identification and interpretation, Shutova and Sun (2013) proposed an unsupervised system to learn sourcetarget domain mappings. The system fits conceptual metaphor theory (Lakoff and Johnson, 1980) well, at the cost of not being able to tackle figurative language in general, and similes in particular, as similes do not map entire domains to one another. Since similes operate on fundamentally different principles than metaphors, our work proposes a computational approach tailored specifically for comparisons. 3 Background and Data Structure of a comparison Unlike metaphors, which are generally unrestricted, comparisons are more structured but also more lexically and semantically varied. This enables a more structured computational representation of which we take advantage. The constituents of a comparison according to Hanks (2012) are: \u2022 the TOPIC, sometimes called tenor: it is usually a noun phrase and acts as logical subject; \u2022 the VEHICLE: it is the object of the comparison and is also usually a noun phrase; \u2022 the shared PROPERTY or ground: it expresses what the two entities have in common-it can be explicit but is often implicit, left for the reader to infer; \u2022 the EVENT (eventuality or state): usually a verb, it sets the frame for the observation of the common property; \u2022 the COMPARATOR: commonly a preposition (like) or part of an adjectival phrase (better than), it is the trigger word or phrase that marks the presence of a comparison. The literal example (1) would be segmented as: [Sterling /TOPIC] [is /EVENT] much [cheaper /PROPERTY] [than /COMPARATOR] [gold /VE- HICLE] Annotation People resort to comparisons often when making descriptions, as they are a powerful way of expressing properties by example. For this reason we collect a dataset of user-generated comparisons in Amazon product reviews (McAuley and Leskovec, 2013) , where users have to be descriptive and precise, but also to express personal opinion. We supplement the data with a smaller set of comparisons from WaCky and WaCkypedia (Baroni et al., 2009) to cover more genres. In preliminary work, we experimented with dependency parse tree patterns for extracting comparisons and labeling their parts (Niculae, 2013) . We use the same approach, but with an improved set of patterns, to extract comparisons with the COMPARA-TORS like, as and than. 2 We keep only the matches where the TOPIC and the VEHICLE are nouns, and the PROPERTY, if present, is an adjective, which is the typical case. Also, the head words of the constituents are constrained to occur in the distributional resources used (Baroni and Lenci, 2010; Faruqui and Dyer, 2014) . 3  We proceed to validate and annotate for figurativeness a random sample of the comparisons extracted using the automated process described above. The annotation is performed using crowdsourcing on the Amazon Mechanical Turk platform, in two steps. First, the annotators are asked to determine whether a displayed sentence is indeed a comparison between the highlighted words (TOPIC and VEHICLE). Sentences qualified by two out of three annotators as comparisons are used in the second round, where the task is to rate how metaphorical a comparison is. We use a scale of 1 to 4 following Turney et al. ( 2011 ), and then binarize to consider scores of 1-2 as literal and 3-4 as figurative. Finally, in this work we only consider comparisons where all three annotators agree on this binary notion of figurativeness. For both tasks, we provide guidelines mostly in the form of examples and intuition, motivated on one hand by the annotators not having specialized knowledge, and on the other hand by the observation that the literal-figurative distinction is subjective. All annotators have the master worker qualification, reside in the U.S. and completed a linguistic background questionnaire that verifies their experience with English. In both tasks, control sentences with confidently known labels are used to filter low quality answers; in addition, we test annotators with a simple paraphrasing task shown to be effective for eliciting and verifying linguistic attention (Munro et al., 2010) . Both tasks seem relatively difficult for humans, with interannotator agreement given by Fleiss' k of 0.48 for the comparison identification task and of 0.54 for the figurativeness annotation after binarization. This is comparable to 0.57 reported by Hovy et al. (2013) for general metaphor labeling. We show some statistics about the collected data in Table 1 . Overall, this is a costly process: out of 2400 automatically extracted comparison candidates, about 60% were deemed by the annotators to be actual comparisons and only 12% end up being selected confidently enough as figurative comparisons. Our dataset of human-filtered comparisons, with the scores given by the three annotators, is made publicly available to encourage further work. 4 This also includes about 400 comparisons where the annotators do not agree perfectly on binary figurativeness. Linguistic Insights We now proceed to exploring the linguistic patterns that discriminate figurative from literal comparisons. We consider two broad classes of cues, which we discuss next. Domain-specific cues Figurative language is often used for striking effects, and comparisons are used to describe new things in terms of something given (Hanks, 2013) . Since the norms that define what is surprising and what is well-known vary across domains, we expect that such contextual information should play an important role in figurative language detection. This is a previously unexplored dimension of figurative language, and Amazon product reviews offer a convenient testbed for this intuition since category information is provided. Specificity To estimate whether a comparison can be considered striking in a particular domain-whether it references images or ideas that are unexpected in its context-we employ a simple measure of word specificity with respect to a domain: the ratio of the word frequency within the domain and the word frequency in all domains being considered. 5 It should be noted that specificity is not purely a function of the word, but  But the same word can play a very different role in another context, for example, book reviews: Her books are like sweet melodies that flow through your head. Indeed, the word melody has a specificity of 96% in the music domain and only of 3% in the books domain. An analysis on the labeled data confirms that literal comparisons do indeed tend to have more domain-specific VEHICLES (Mann-Whitney U test, p < 0.01) than figurative ones. Furthermore, the distribution of specificity across both types of comparisons, as shown in Figure 1a , has the appearance of a mixture model of general and specific words. Figurative comparison VEHICLES largely exhibit only the general component of the mixture. 6   Domain label An analysis of the annotation results reveals that the percentage of comparisons that are figurative differs widely across domains, as indicated in the last column in Table 1 . This suggests that simply knowing the domain of a text can serve to adjust some prior expectation about figurative language presence and therefore improve detection. We test this hypothesis using a Z-test comparing all Amazon categories. With the exception of books and music reviews, that have similar ratios, all other pairs of categories show significantly different proportions of figurative comparisons (p < 0.01). Domain-agnostic cues Linguistic studies of figurative language suggest that there is a fundamental generic notion of figurativeness. We attempt to capture this notion in the context of comparisons using syntactic and semantic information. Topic-Vehicle similarity The default role of literal comparisons is to assert similarity of things. Therefore, we expect that a high semantic similarity between the TOPIC and the VEHICLE of a comparison is a sign of literal usage, as we previously hypothesized in preliminary work (Niculae, 2013) . To test this hypothesis, we compute TOPIC-VEHICLE similarity using Distributional Memory (Baroni and Lenci, 2010) , a freely available distributional semantics resource that captures word relationships through grammatical role co-occurrence. By applying this measure to our data, we find that there is indeed an important difference between the distributions of TOPIC-VEHICLE similarity in figurative and literal comparisons (shown in Figure 1b ); the means of the two distributions are significantly different (Mann-Whitney p < 0.01).  applied in the context of comparisons. To that end we consider features shown to provide state of the art performance in the task of metaphor detection (Tsvetkov et al., 2014a) : abstractness, imageability and supersenses. Metaphor-inspired features Abstractness and imageability features are derived from the MRC Psycholinguistic Database (Coltheart, 1981) , a dictionary based on manually annotated datasets of psycholinguistic norms. Imageability is the property of a word to arouse a mental image, be it in the form of a mental picture, sound or any other sense. Concreteness is defined as \"any word that refers to objects, materials or persons,\" while abstractness, at the other end of the spectrum, is represented by words that cannot be usually experienced by the senses (Paivio et al., 1968) . Table 2 shows a few examples of words with high and low concreteness and imageability scores. Supersenses are a very coarse form of meaning representation. Tsvetkov et al. (2014a) used WordNet (Miller, 1995) semantic classes for nouns and verbs, for example noun.body, noun.animal, verb.consumption, or verb.motion. For adjectives, Tsvetkov et (2014b) developed and made available a novel classification in the same spirit. 7 We compute abstractness, imageability and supersenses for the TOPIC, VEHICLE, EVENT, and PROPERTY. 8 We concatenate these features with the raw vector representations of the constituents, following Tsvetkov et al. (2014a) . We find that such features relate to figurative comparisons in a meaningful way. For example, out of all comparisons with explicit properties, figurative comparisons tend to have properties that 7 Following Tsvetkov et al. (2014a) we train a classifier to predict these features from a vector space representation of a word. We use the same cross-lingually optimized representation from Faruqui and Dyer (2014) and a simpler classifier, a logistic regression, which we find to perform as well as the random forests used in Tsvetkov et al. (2014a) . We treat supersense prediction as a multi-label problem and apply a oneversus-all transformation, effectively learning a linear classifier for each supersense. 8 If the PROPERTY is implicit, all corresponding features are set to zero. An extra binary feature indicates whether the PROPERTY is explicit or implicit. are more imageable (Mann-Whitney p < 0.01), as illustrated by Figure 1c . This is in agreement with Hanks (2005) , who observed that similes are characterized by their appeal to sensory imagination. Definiteness We introduce another simple but effective syntactic cue that relates to concreteness: the presence of a definite article versus an indefinite one (or none at all). We search for the indefinite articles a and an and the definite article the in each component of a comparison. We find that similes tend to have indefinite articles in the VEHICLE more often and definite articles less often (Mann-Whitney p < 0.01). In particular, 59% of comparisons where the VEHICLE has a indefinite article are figurative, as opposed to 13% of the comparisons where VEHICLE has a definite article. Prediction Task We now turn to the task of predicting whether a comparison is figurative or literal. Not only does this task allow us to assess and compare the efficiency of the linguistic cues we discussed, but it is also highly relevant in the context of natural language understanding systems. We conduct a logistic regression analysis, and compare the efficiency of the features derived from our analysis to a bag of words baseline. In addition to the features inspired by the previously described linguistic insights, we also try to computationally capture the lexical usage patterns of comparisons using a version of bag of words adapted to the comparison structure. In this slotted bag of words system, features correspond to occurrence of words within constituents (e.g., bright \u2208 PROPERTY). We perform a stratified split of our comparison dataset into equal train and test sets (each set containing 408 comparisons, out of which 134 are figurative), 9 and use a 5-fold stratified cross validation over the training set to choose the optimal value for the logistic regression regularization parameter and the type of regularization (\u2113 1 or \u2113 2 ) for each feature set. Classifier performance The performance on the classification task is summarized in Table 3 . We note that the bag of words baseline is remarkably strong, because of common idiomatic similes that can be captured through keywords. Our full system (which relies on our linguistically inspired cues discussed in Section 4 in addition to slotted bag of words) significantly outperforms the bag of words baseline and the slotted bag of words system in terms of accuracy, F 1 score and AUC (p < 0.05), 11 suggesting that linguistic insights complement idiomatic simile matching. Importantly, a system using only our linguistic insight cues also significantly improves over the baseline in terms of accuracy and AUC and it is not significantly different from the full system in terms of performance, in spite of having about an order of magnitude fewer features. It is also worth noting that the domain-specific cues play an important role in bringing the performance to this level by capturing a different aspect of what it means for a comparison to be figurative. The features used by the state of the art metaphor detection system of Tsvetkov et al. (2014a) , adapted to the comparison structure, perform poorly by themselves and do not improve significantly over the baseline. This is consistent with the theoretical motivation that figurativeness in comparisons requires special computational treatment, as discussed in Section 1. Furthermore, the linguistic insight features not only significantly outperform the metaphor inspired features (p < 0.05), but are also better at exploiting larger amounts of data, as shown in Figure 2 . 11 All statistical significance results in this paragraph are obtained from 5000 bootstrap samples. Comparison to human performance To gauge how well humans would perform at the classification task on the actual test data, we perform another Amazon Mechanical Turk evaluation on 140 examples from the test set. For the evaluation, we use majority voting between the three annotators, 12 and compare to the agreed labels in the dataset. Estimated human accuracy is 96%, placing our full system within 10% of human accuracy. Feature analysis The predictive analysis we perform allows us to investigate to what extent the features inspired by our linguistic insights have discriminative power, and whether they actually cover different aspects of figurativeness. Feature Coef. Example where the feature is positively activated Table 4 shows the best linguistic insight and slotted bag of words features selected by the full model. The strongest feature by far is the semantic similarity between the TOPIC and the VEHI-CLE. By itself, this feature gets 70% accuracy and 61% F 1 score. The rest of the top features involve mostly the VEHICLE. This suggests that the VEHICLE is the most informative element of a comparison when it comes to figurativeness. Features involving other constituents also get selected, but with slightly lower weights, not making it to the top. VEHICLE specificity is one of the strongest features, with positive values indicating literal comparisons. This confirms our intuition that domain information is important to discriminate figurative from literal language. Of the adapted metaphor features, the noun communication supersense and the imageability of the VEHICLE make it to the top. Nouns with low communication rating occurring in the training set include puddles, arrangements, carbohydrates while nouns with high communication rating include languages and subjects. Presence of an indefinite article in the VEHICLE is a strong indicator of figurativeness. By themselves, the definiteness and indefiniteness features perform quite well, attaining 78% accuracy and 67% F 1 score. The salient bag of words features correspond to specific types of comparisons. The words other and others in the VEHICLE indicate comparisons between the same kind of arguments, for example some songs are more memorable than others, and these are likely to be literal. The word pic-ture is specific to the review setting, as products are accompanied by photos, and for certain kinds of products, the resemblance of the product with the image is an important factor for potential buyers. 13 The bag of words systems are furthermore able to learn idiomatic comparisons by identifying common figurative VEHICLES such as life and crap, corresponding to fixed expressions such as larger than life. Error analysis Many of the errors made by our full system involve indirect semantic mechanisms such as metonymy. For example, the false positive the typeface was larger than most books really means larger than the typefaces found in most books, but without the implicit expansion the meaning can appear figurative. A similar kind of ellipsis makes the example a lot [of songs] are even better than sugar be wrongly classified as literal. Another source of error is polysemy. Examples like the rejuvelac formula is about 10 times better than yogurt are misclassified because of the multiple meanings of the word formula, one being closely related to yogurt and food, but the more common ones being general and abstract, suggesting figurativeness. Social Correlates The advantage of studying comparisons situated in a social context is that we can understand how their usage interacts with internal and external human factors. An internal factor is the sentiment of  In Figure 3b the average proportion is different because we only consider reviews rated by at least 10 readers. the user towards the reviewed product, indicated by the star rating of the review. An external factor present in the data is how helpful the review is perceived by other users. In this section we analyze how these factors interact with figurative language in comparisons. To gain insight about fine grained interactions with human factors at larger scale, we use our classifier to find over 80,000 figurative and literal comparisons from the same four categories. The trends we reveal also hold significantly on the manually annotated data. Sentiment While it was previously noted that similes often transmit strong affect (Hanks, 2005; Veale, 2012a; Veale, 2012b) , the connection between figurativeness and sentiment was never empirically validated. The setting of product reviews is convenient for investigating this issue, since the star ratings associated with the reviews can be used as sentiment labels. We find that comparisons are indeed significantly more likely to be figurative when the users express strong opinions, i.e., in one-star or five-star reviews (Mann-Whitney p < 0.02 on the manually annotated data). Figure 3a shows how the proportion of figurative comparisons varies with the polarity of the review. Helpfulness It is also interesting to understand to what extent figurative language relates to the external perception of the content in which it ap-pears. We find that comparisons in helpful reviews 14 are less likely to be figurative. Figure 3b shows a near-constant high ratio of figurative comparisons among unhelpful and average reviews; as helpfulness increases, figurative comparisons become less frequent. We further validate that this effect is not a confound of the distribution of helpfulness ratings across reviews of different polarity by controlling for the star rating: given a fixed star rating, the proportion of figurative comparisons is still lower in helpful (helpfulness over 50%) than in unhelpful (helpfulness under 50%) reviews; this difference is significant (Mann-Whitney p < 0.01) for all classes of ratings except one-star. The size of the manually annotated data does not allow for star rating stratification, but the overall difference is statistically significant (Mann-Whitney p < 0.01). This result encourages further experimentation to determine whether there is a causal link between the use of figurative language in user generated content and its external perception. Conclusions and Future Work This work proposes a computational study of figurative language in comparisons. Starting from a new dataset of naturally occurring comparisons with figurativeness annotation (which we make publicly available) we explore linguistic patterns that are indicative of similes. We show that these insights can be successfully operationalized in a new prediction task: distinguishing literal from figurative comparisons. Our system reaches accuracy that is within 10% of human performance, and is outperforming a state of the art metaphor detection system, thus confirming the need for a computational approach tailored specifically to comparisons. While we take a data-driven approach, our annotated dataset can be useful for more theoretical studies of the kinds of comparisons and similes people use. We discover that domain knowledge is an important factor in identifying similes. This suggests that future work on automatic detection of figurative language should consider contextual parameters such as the topic and community where the content appears. Furthermore, we are the first to tie figurative language to the social context in which it is produced and show its relation to internal and external human factors such as opinion sentiment and helpfulness. Future investigation into the causal effects of these interactions could lead to a better understanding of the role of figurative language in persuasion and rhetorics. In our work, we consider common noun TOP-ICS and VEHICLES and adjectival PROPERTIES. This is the most typical case, but supporting other parts of speech-such as proper nouns, pronouns, and adverbs-can make a difference in many applications. Capturing compositional interaction between the parts of the comparison could lead to more flexible models that give less weight to the VEHICLE. This study is also the first to estimate how prevalent similes are in the wild, and reports that about one third of the comparisons we consider are figurative. This is suggestive of the need to build systems that can properly process figurative comparisons in order to correctly harness the semantic information encapsulated in comparisons. Acknowledgements We would like to thank Yulia Tsvetkov for constructive discussion about figurative language and about her and her co-authors' work. We are grateful for the suggestions of Patrick Hanks, Constantin Or\u0203san, Sylviane Cardey, Izabella Thomas, Ekaterina Shutova, Tony Veale, and Niket Tandon. We extend our gratitude to Julian McAuley for preparing and sharing the Amazon review dataset. We are thankful to the anonymous ers, whose comments were like a breath of fresh air. We acknowledge the help of the Amazon Mechanical Turk annotators and of the MPI-SWS students involved in pilot experiments. Vlad",
    "abstract": "Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense-for example, \"I slept like a log\" does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "26",
    "year": "2014",
    "month": "October",
    "title": "Brighter than Gold: Figurative Language in User Generated Comparisons"
}