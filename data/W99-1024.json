{
    "article": "The Data-Oriented Parsing Model (DOP, [1]; [2]) has been presented as a promising paradigm for NLP. It has also been used as a basis for Machine Translation (MT) -Data-Oriented TVanslation (DOT, [9] ). Lexical Functional Grammar (LFG, [5] ) has also been used for MT ([6]). LFG has recently been allied to DOP to produce a new LFG-DOP model ([3]) which improves the robustness of LFG. We summarize the DOT model of translation as well as the DOP model on which it is based. We demonstrate that DOT is not guaranteed to produce the correct translation, despite provably deriving the most probable translation. Finally, we propose a novel hybrid model for MT based on LFG-DOP which promises to improve upon DOT, as well as the pure LFG-based translation model. In tr o d u c tio n Neither of the main paradigmatic approaches to MT, namely rule-based and statistical, currently suffice to the standard required. Nevertheless, each contains elements which if properly harnessed should lead to an overall improvement in translation performance. It is in this new hybrid spirit that our search for a better solution to the problems of MT can be seen. We propose that combining DOP ([1];[2]) with the conventional transfer rules of LFG ( [6 ] ) may derive a new model for MT, LFG-DOT, which promises to improve upon DOT, as well as the pure LFG-based translation model. T h e D O P A r c h ite c tu r e for N L P DOP language models ([1];[2]) assume that past experiences of language are significant in both perception and production. DOP prefers performance models over competence gram mars, in that abstract grammar rules are eschewed in favour of models based on large collections of previously occurring fragments of language. New language fragments are pro cessed with reference to already existing fragments from the corpus, which are combined using probabilistic techniques to determine the most likely analysis for the new fragment. DOP models typically use surface PS-trees as the chosen representatio n for strings (hence \"Tree-DOP\"), but nothing hangs on this choice. However, given that LFG c-structures are little more than annotated PS-trees allows us to proceed very much on the same lines as in Tree-DOP, which has two decom position operations to produce subtrees from sentence representations: (i) the Root operation, which takes any node in a tree as the root of a new subtree, deleting all other nodes except this new root and all nodes dominated by it; and (ii) the Frontier operation, which selects a (possibly empty) set of nodes in the newly created subtree, excluding the root, and deletes all subtrees dominated by these selected nodes. (1) NP VP NP John V 1 swims John NP VP VP V John V V swims swims NP VP V The full set of DOP trees derived from the sentence John swims are those in (1). Proceedings of NODALIDA 1999 Tree-DOP recom bines fragments starting from the leftmost non-terminal frontier node, and replaces this with a fragment having the same root symbol. For instance, assuming the treebank in (1), John swims has (2) as a possible derivation (among many others); VP 0 NP _ NP VP V John John V swims swims (2) Finally, the chosen probability m odel for Tree-DOP is based quite simply on the relative frequencies of fragments in the corpus. These elements enable representations of new strings to be constructed from previously occurring fragments in a number of ways. If each derivation t has a probability P(t) (i.e. its relative frequency), then the probability of deriving a Tree-DOP representation is the sum of the probabilities of the individual derivations, as in (3): (3) # {t I root(t) = root[tij)) The probability of each individual derivation t is calculated as the product of the probabilities of all the constituent elements (ti, t2 ---tn) involved in choosing tree t from the corpus, as in (4); P{{h,t2...tn)) -n P(t) >=1 '/^co rp u s Pit') Given these formulae, the probability of the derivation for John swims in (2) is This is calculated by multiplying together the probability of each of the two tree fragments involved in the derivation, namely those in (5): (5) P(t = [NP vp[v[swims]]] | root(t) = S).P(t = [np[John]]| root(t) = NP) = 11 -1 6\"1 6 The probability of the parse of John swims, however, is calculated by summing all derivations resulting in the parse-tree for the sentence (as (3) shows), which, given the trivial corpus in Proceedings of NODALIDA 1999 (1), is 1. However, adding the fragments from a new sentence Peter laughs to the treebank in (1) allows us now to derive the probability of two new strings -Peter swims and John laughs -with respect to this small corpus of tree fragments. In this way, it can be seen that DOP handles unseen data on the basis of previous experience -despite the fact that we have never seen either new sentence before, we are able to process them compositionally, on the basis of previously occurring fragments of each in our corpus. Each tree which can play a part in combining together with other trees to form a representation for a sentence is used to contribute to the overall probability of that representation given the corpus. Opportunities for Hybridity-LFG DOP DOP-based approaches are necessarily limited to those contextual dependencies actually occurring in the corpus, which is a reflection of surface phenomena only. Given its facility to capture and provide representations of linguistic phenomena other than those occurring at surface structure, the functional structures of LFG have been allied to the techniques of DOP to create a new model, LFG-DOP ([3]), which adds a measure of robustness not available to models based solely on LFG. We suggest that this framework has the potential to be utilised for MT. As with DOP, LFG-DOP needs to be defined using four parameters. Its representations are simply lifted en bloc from LFG theory, so that each string is annotated with a c-structure, an f-structure, and a mapping < j > between them, with well-formedness conditions operating solely on f-structure, as usual. Since we are now deeding with (c,f) pairs of structure, the Root and Frontier decom position operations of DOP need to be adapted to stipulate exactly which c-structure nodes are linked to which f-structure components, thereby maintaining the fundamentals of c-and f-structure correspondence. As in DOP, Root erases all nodes outside of the selected node, except this new root and all nodes dominated by it, and in addition deletes all 0 -links leaving the erased nodes, as well as all f-structure units that are not 0 -accessible from the remaining nodes, reflecting the intuitive notion that nodes in a tree carry information only about the f-structure elements to which the root node of the tree permits access. Frontier operates as in DOP, selecting a set of nodes in the newly created subtree, excluding the root, and deleting all subtrees dominated by these selected nodes. Furthermore, it deletes all 0 -links of these erased nodes together with any semantic form corresponding to the same D a ta -O r ie n te d T ra n sla tio n (D O T ) [9] has developed a DOP-based model of translation -Data-Oriented Translation -which relates POS-fragments between two languages (English and Dutch), with an accompanying probability. Once a derivation for the source language sentence has been arrived at, the target structure is cissembled, and a string produced. Since there are typically many different Proceedings of NODALIDA 1999 derivations for the source sentence, there may be as many different translations available. As is the case when DOP is used monolingually, the probability of a translation is calculated by summing the probabilities of all possible derivations forming the translation. Poutsma shows that the most probable translation can be computed using Monte-Carlo disambiguation, and exemplifies this using sentence idioms, where corresponding source-target translations are linked at all possible nodes. Som e Lim itations of DOT DOT is an interesting model, yet it fails to capture the correct translation when this is non-compositional and considerably less probable than the default, compositional alterna tive. When LFG-DOP MT (LFG-DOT) is used instead this problem may be overcome. Furthermore, OOP's statistical model also gives a \"level of correctness\" figure to alternative translations. This is useful in cases where the default translation in LFG-MT (and in many other systems) cannot be suppressed when the specific translation is required. For example, assuming the basic default rules in ( 7 ): (cf. John se suicide). We would like specific rules to override the default translation where applicable, but this is not possible in LFG-MT, so we would get both translations here, i.e. a correct one (via the specific r-equations in (1 0 )) and a wrong one (via the default T-equations, required to translate commettre as commit in other circumstances). Assuming a DOP treebank built from the French sentences in (8 ) as well as Marie se suicide, the ill-formed string Jean commet le suicide is preferred (in the French language model) about half as much again as the correct alternative Jean se suicide. There are several reasons for this; the preference for Jean as subject of commettre, the co-occurrence of le and suicide, plus the fact that commettre is followed by an NP consisting of a Det -t-N sequence. Note also that these results are obtained with the same number of instances of each verb -in a larger corpus commettre would surely greatly outnumber instances of se suicider. This is by no means an unexpected result. As an example, in the LOB Corpus, there are 6 6 instances of commit as a verb (including its morphological variants), only 4 of which have suicide as its object, out of the 15 occurrences of suicide as an NP. Consequently, even for this small sample, we can see that 94% of these examples need to be translated compositionally (by commettre + NP), while only the commit suicide examples require a specific rule to apply (i.e. se suicider). In the on-line Canadian Hansards covering 1986-1993, there are just 106 instances of se suicider (including its morphological variants). There will, of course, be many thousands of instances of commettre. Given occurrences of suicide as an NP in French corpora, it is not an unreasonable hypothesis to expect that the wrong, compositional translations will be much more probable than those derived via the specific rule. Given Poutsma's model, it would appear that the adherence to left-most substitution in the target given a priori left-most substitution in the source is too strictly linked to linear order of words, so that, as soon as this deviates to any significant extent even between similar languages, DOT has a huge bias in favour of the incorrect translation. Even if the correct, non-compositional translation is achievable in such circumstances via DOT, it is likely to be so outranked by other wrong alternatives that it will be dismissed, unless all possible translations are maintained for later scrutiny by the user. L F G -D O T : A N e w T h e o r y o f T r a n sla tio n The DOT model cannot explicitly relate parts of the source language structure to the cor responding, correct parts in the target structure. One line of investigation which we now develop that can overcome this linear restriction is to use LFG-DOP ([3]) as the basis for an innovative MT system, using LFG's r-equations to relate translation fragments between languages. M odel 1: {c,<p, f,r, f') Using separate language corpora, this simple, linear model builds a target f-structure / ' from a source c-structure c and f-structure /, the mapping between them 4> , and the tauequations r. Prom this target f-structure / ', a target string is generated via the standard LFG generation algorithms ( [7] ; [11] ). The probability of the target f-structure Rt being the translation of the source string W, is: I W,) = E P[Rs I W,).P{R^ I R\" W.) ' ' R t.. = Z p ( R , \\ w ,).p {r , \\ r ,) R t,. (9) incorporating a Markov assumption that the target f-structure's derivation from a source string (via 4> and r) is independent of the original words involved: it is dependent solely on the monolingual LFG-DOP representation assigned. This is an attempt to avoid as much as possible the sparse data problem, given that in all probability we will never have enough LFG-DOP fragments to model these numbers with any reasonable accuracy. The components needed given (9), therefore, are (i) a source language LFG-DOP model, P(R, \\ VFj); (ii) the T mapping (the translation model) plus the associated probabilities that a source f-structure produces a target equivalent, P{Rt \\ R,). The advantage of this model over DOT is the availability of the explicit r-equations to link source-target correspondences, as in (1 0 ): ( 10) commit: (rf PRED ) = se suicider, r ( t SUBJ) = (rf SUBJ), (f OBJ PRED) =c suicide Using LFG r-equations ensures the derivation of the correct target f-structure, along with some wrong alternatives (here) via the default rules. We cannot be sure that the generation of a target string via the correct target f-structure will be a more probable translation than any wrong alternative, but it will exist as one of a small number of high-ranking candidate solutions from which the final translation can be selected. Of course, we may instead choose to derive the target string using a target language LFG-DOP model (via 4>') rather than the standard LFG generation algorithms, in which case the probability model in (9) needs to be adapted to incorporate P{Wt \\ Rt), where again we presume that the target string generation is independent of all source language representations: it is dependent solely on the r-equations derived from the source f-structure. M odel 2: (c, / , 4>) -vy, r -{d, f , 4>') Here we have integrated language corpora, where for each node in a tree c, we relate it both to its corresponding f-structure fragment / and its corresponding target c-structure node d , and for each source f-structure fragment, we relate that to its target language fragment in f-structure /', via r. The probability model used this time is: ( 11) ^P{t I s) = M ax t D P{t, I s) = Rt., M ax t E P{Ri I s) Rt.> where now are the full (c, / ) representation pairings for the target and source strings, respectively. Our basic units are pairs of linked LFG-DOP fragments (cf. the linked DOP fragments in DOT, [9] ), and the basic stochastic event is the combination of two linked LFG-DOP fragment pairs. Thus, we compute the probability of P{t \\ s) by the sum of the probabilities of all Rt, R, pairs that generate t and s (and, ultimately, of course, choosing that t for which this probability sum is maximal), where the probability of an Rt, Rs pair is computed as the sum of the probabilities of its derivation-pairs; each derivation-pair is the product of its linked fragment-pairs; and each linked fragment-pair has a probability equal to its normalized relative frequency. Bod & Kaplan discuss four different ways of calculating the probability of an (unlinked) fragment, depending on which LFG grammaticality checks (if any) are integrated into the competition sets assumed (cf. section 2 .1 ). The principal reason for hypothesising the 7 function in this model is that it is reasonable to assume, as [9] has shown, that valuable information concerning the final formulation of the target string can be influenced by the source c-structure. In this way we have two pieces of information at hand with which to build the target string-the 7 and < p' functions, which if they can be properly harnessed, should bring about a better translation, given the extra evidence that is being brought to bear in its generation. Sem i-Autom atic Creation o f LFG & LFG-DOP Corpora A major problem for researchers interested in LFG and LFG-DOP is the absence of suitable, extensive corpora. Given this, in order to demonstrate practically the feasibility of LFG-DOT, we have begun to develop our own LFG and LFG-DOP corpora ( [10] ). Initially we took the publicly available set of 100 sentences of the AP Treebank ([8 ] ). Despite its small size, this was sufficiently lajge to demonstrate the plausibility of our approach. One lexical and structural rules, and reparse the treebank entries. Once these target f-structures exist, we can test out the translation models and report results. C o n clu sio n s The DOT translation system, despite provably deriving the most probable translation, is not guaranteed to produce the best, or even a correct translation, since it is unable to explicitly link exactly those fragments which are playing the decisive role in translation. [3] have shown how DOP and LFG can be integrated to provide a powerful mechanism for the treatment of parsing. We described how such a model may be extended to provide a robust solution for the problems of MT in the spirit of the current trend for hybrid approaches. LFG-DOT promises to improve on previous attempts at LFG-MT, particular where robustness is concerned, being able to handle both unseen and ill-formed input with relative ease. It also ensures that the correct target f-structure is input into the generation process. It is reasonable to expect LFG-DOT to outperform pure statistics-based systems, in having the additional facility of grammatical information at hand to use where necessary. Much of this work is ongoing, and a number of issues remain for the future, especially the automatic creation of large LFG-DOP corpora necessary as training and test data for the translation models. This will complete the development of the systems described, leading to greater experimentation on a larger scale. R e feren ces particular entry is: AOOl 39 V [N The_AT march.NNl N][V was.VBDZ [J peaceful.JJ J]V] ... We then automatically extract the rules from this corpus (following the method of [4] ), and create automatically LFG-macros for each lexical category: We then annotate the extracted rules with LFG functional schemata by hand: and 'reparse' the original treebank entries, not the strings, simply by recursively following the tree annotations provided by the original annotators. In so doing the interpreter solves the constraint equations associated with the grammar rules and lexical macros involved in the parse, returning single f-structures, as in: (15) subj : spec : the pred : march num : sg xcomp : pred : peaceful subj : spec : the pred : march num : sg tense : past pred : be In order to produce target f-structures, all that is necessary is to add r-equations to the Proceedings of NODALIDA 1999",
    "abstract": "The Data-Oriented Parsing Model (DOP, [1]; [2]) has been presented as a promising paradigm for NLP. It has also been used as a basis for Machine Translation (MT) -Data-Oriented TVanslation (DOT, [9] ). Lexical Functional Grammar (LFG, [5] ) has also been used for MT ([6]). LFG has recently been allied to DOP to produce a new LFG-DOP model ([3]) which improves the robustness of LFG. We summarize the DOT model of translation as well as the DOP model on which it is based. We demonstrate that DOT is not guaranteed to produce the correct translation, despite provably deriving the most probable translation. Finally, we propose a novel hybrid model for MT based on LFG-DOP which promises to improve upon DOT, as well as the pure LFG-based translation model.",
    "countries": [
        "Ireland"
    ],
    "languages": [
        "French",
        "Dutch",
        "English"
    ],
    "numcitedby": "0",
    "year": "2000",
    "month": "December",
    "title": "{LFG}-{DOT}: Combining Constraint-Based and Empirical Methodologies for Robust {MT}"
}