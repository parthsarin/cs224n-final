{
    "article": "We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google's open source OCR system. Introduction Standard techniques for transcribing modern documents do not work well on historical ones. For example, even state-of-the-art OCR systems produce word error rates of over 50% on the documents shown in Figure 1 . Unsurprisingly, such error rates are too high for many research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010) . We present a new, generative model specialized to transcribing printing-press era documents. Our model is inspired by the underlying printing processes and is designed to capture the primary sources of variation and noise. One key challenge is that the fonts used in historical documents are not standard (Shoemaker, 2005) . For example, consider Figure 1a . The fonts are not irregular like handwriting -each occurrence of a given character type, e.g. a, will use the same underlying glyph. However, the exact glyphs are unknown. Some differences between fonts are minor, reflecting small variations in font design. Others are more severe, like the presence of the archaic long s character before 1804. To address the general problem of unknown fonts, our model learns the font in an unsupervised fashion. Font shape and character segmentation are tightly coupled, and so they are modeled jointly. A second challenge with historical data is that the early typesetting process was noisy. Handcarved blocks were somewhat uneven and often failed to sit evenly on the mechanical baseline. Figure 1b shows an example of the text's baseline moving up and down, with varying gaps between characters. To deal with these phenomena, our model incorporates random variables that specifically describe variations in vertical offset and horizontal spacing. A third challenge is that the actual inking was also noisy. For example, in Figure 1c some characters are thick from over-inking while others are obscured by ink bleeds. To be robust to such rendering irregularities, our model captures both inking levels and pixel-level noise. Because the model is generative, we can also treat areas that are obscured by larger ink blotches as unobserved, and let the model predict the obscured text based on visual and linguistic context. Our system, which we call Ocular, operates by fitting the model to each document in an unsupervised fashion. The system outperforms state-ofthe-art baselines, giving a 47% relative error reduction over Google's open source Tesseract system, and giving a 31% relative error reduction over ABBYY's commercial FineReader system, which has been used in large-scale historical transcription projects (Holley, 2010) . Over-inked It appeared that the Prisoner was very E : X : Wandering baseline Historical font Figure 2 : An example image from a historical document (X) and its transcription (E). Related Work Relatively little prior work has built models specifically for transcribing historical documents. Some of the challenges involved have been addressed (Ho and Nagy, 2000; Huang et al., 2006; Kae and Learned-Miller, 2009) , but not in a way targeted to documents from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006) , which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011) . The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001) . They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003) , but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011) . The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. Model Most historical documents have unknown fonts, noisy typesetting layouts, and inconsistent ink levels, usually simultaneously. For example, the portion of the document shown in Figure 2 has all three of these problems. Our model must handle them jointly. We take a generative modeling approach inspired by the overall structure of the historical printing process. Our model generates images of documents line by line; we present the generative process for the image of a single line. Our primary random variables are E (the text) and X (the pixels in an image of the line). Additionally, we have a random variable T that specifies the layout of the bounding boxes of the glyphs in the image, and a random variable R that specifies aspects of the inking and rendering process. The joint distribution is: P (E, T, R, X) = P (E) [Language model] \u2022 P (T |E) [Typesetting model] \u2022 P (R) [Inking model] \u2022 P (X|E, T, R) [Noise model] We let capital letters denote vectors of concatenated random variables, and we denote the individual random variables with lower-case letters. For example, E represents the entire sequence of text, while e i represents ith character in the sequence. Language Model P (E) Our language model, P (E), is a Kneser-Ney smoothed character n-gram model (Kneser and Ney, 1995) . We generate printed lines of text (rather than sentences) independently, without generating an explicit stop character. This means that, formally, the model must separately generate the character length of each line. We choose not to bias the model towards longer or shorter character sequences and let the line length m be drawn uniformly at random from the positive integers less than some large constant M. 1 When i < 1, let e i denote a line-initial null character. We can now write: a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a P (E) = P (m) \u2022 m i=1 P (e i |e i\u22121 , . . . , e i\u2212n ) e i 1 e i+1 e i l i g i r i X RPAD i X LPAD i X GLYPH i P ( \u2022 | th) P ( \u2022 | th) a b c . . . P ( \u2022 | pe) Inking: \u2713 INK Inking params Typesetting Model P (T |E) Generally speaking, the process of typesetting produces a line of text by first tiling bounding boxes of various widths and then filling in the boxes with glyphs. Our generative model, which is depicted in Figure 3 , reflects this process. As a first step, our model generates the dimensions of character bounding boxes; for each character token index i we generate three bounding box widths: a glyph box width g i , a left padding box width l i , and a right padding box width r i , as shown in Figure 3 . We let the pixel height of all lines be fixed to h. Let T i = (l i , g i , r i ) so that T i specifies the dimensions of the character box for token index i; T is then the concatenation of all T i , denoting the full layout. Because the width of a glyph depends on its shape, and because of effects resulting from kerning and the use of ligatures, the components of each T i are drawn conditioned on the character token e i . This means that, as part of our parameterization of the font, for each character type c we have vectors of multinomial parameters \u03b8 LPAD c , \u03b8 GLYPH c , and \u03b8 RPAD c governing the distribution of the dimensions of character boxes of type c. These parameters are depicted on the right-hand side of Figure 3 . We can now express the typesetting layout portion of the model as: P (T |E) = m i=1 P (Ti|ei) = m i=1 P (li; \u03b8 LPAD e i ) \u2022 P (gi; \u03b8 GLYPH e i ) \u2022 P (ri; \u03b8 RPAD e i ) Each character type c in our font has another set of parameters, a matrix \u03c6 c . These are weights that specify the shape of the character type's glyph, and are depicted in Figure 3 as part of the font parameters. \u03c6 c will come into play when we begin generating pixels in Section 3.3. Inking Model P (R) Before we start filling the character boxes with pixels, we need to specify some properties of the inking and rendering process, including the amount of ink used and vertical variation along the text baseline. Our model does this by generating, for each character token index i, a discrete value d i that specifies the overall inking level in the character's bounding box, and a discrete value v i that specifies the glyph's vertical offset. These variations in the inking and typesetting process are mostly independent of character type. Thus, in our model, their distributions are not characterspecific. There is one global set of multinomial parameters governing inking level (\u03b8 INK ), and another governing offset (\u03b8 VERT ); both are depicted on the left-hand side of Figure 3 . Let R i = (d i , v i ) and let R be the concatenation of all R i so that we can express the inking model as: P (R) = m i=1 P (Ri) = m i=1 P (di; \u03b8 INK ) \u2022 P (vi; \u03b8 VERT ) The d i and v i variables are suppressed in Figure 3 to reduce clutter but are expressed in Figure 4 , which depicts the process of rendering a glyph box. Noise Model P (X|E, T, R) Now that we have generated a typesetting layout T and an inking context R, we have to actually generate each of the pixels in each of the character boxes, left padding boxes, and right padding boxes; the matrices that these groups of pixels comprise are denoted We assume that pixels are binary valued and sample their values independently from Bernoulli distributions. 2 The probability of black (the Bernoulli parameter) depends on the type of pixel generated. All the pixels in a padding box have the same probability of black that depends only on the inking level of the box, d i . Since we have already generated this value and the widths l i and r i of each padding box, we have enough information to generate left and right padding pixel matrices X GLYPH i , X LPAD X LPAD i and X RPAD i . The Bernoulli parameter of a pixel inside a glyph bounding box depends on the pixel's location inside the box (as well as on d i and v i , but for simplicity of exposition, we temporarily suppress this dependence) and on the model parameters governing glyph shape (for each character type c, the parameter matrix \u03c6 c specifies the shape of the character's glyph.) The process by which glyph pixels are generated is depicted in Figure 4 . The dependence of glyph pixels on location complicates generation of the glyph pixel matrix X GLYPH i since the corresponding parameter matrix 2 We could generate real-valued pixels with a different choice of noise distribution.  \u03c6 e i has some type-level width w which may differ from the current token-level width g i . Introducing distinct parameters for each possible width would yield a model that can learn completely different glyph shapes for slightly different widths of the same character. We, instead, need a parameterization that ties the shapes for different widths together, and at the same time allows mobility in the parameter space during learning. g i d i v i ei \u2713 PIXEL (j, k, g i , d i , v i ; ei ) \u21e5 X GLYPH i \u21e4 jk \u21e0 Bernoulli Our solution is to horizontally interpolate the weights of the shape parameter matrix \u03c6 e i down to a smaller set of columns matching the tokenlevel choice of glyph width g i . Thus, the typelevel matrix \u03c6 e i specifies the canonical shape of the glyph for character e i when it takes its maximum width w. After interpolating, we apply the logistic function to produce the individual Bernoulli parameters. If we let [X GLYPH i ] jk denote the value of the pixel at the jth row and kth column of the glyph pixel matrix X GLYPH i for token i, and let \u03b8 PIXEL (j, k, g i ; \u03c6 e i ) denote the token-level Bernoulli parameter for this pixel, we can write: [X GLYPH i ] jk \u223c Bernoulli \u03b8 PIXEL (j, k, gi; \u03c6e i ) The interpolation process for a single row is depicted in Figure 5 . We define a constant interpolation vector \u00b5(g i , k) that is specific to the glyph box width g i and glyph box column k. Each \u00b5(g i , k) is shaped according to a Gaussian centered at the relative column position in \u03c6 e i . The glyph pixel Bernoulli parameters are defined as follows: \u03b8 PIXEL (j, k,gi; \u03c6e i ) = logistic w k =1 \u00b5(gi, k) k \u2022 [\u03c6e i ] jk The fact that the parameterization is log-linear will ensure that, during the unsupervised learning process, updating the shape parameters \u03c6 c is simple and feasible. By varying the magnitude of \u00b5 we can change the level of smoothing in the logistic model and cause it to permit areas that are over-inked. This is the effect that d i controls. By offsetting the rows of \u03c6 c that we interpolate weights from, we change the vertical offset of the glyph, which is controlled by v i . The full pixel generation process is diagrammed in Figure 4 , where the dependence of \u03b8 PIXEL on d i and v i is also represented. Learning We use the EM algorithm (Dempster et al., 1977) to find the maximum-likelihood font parameters: \u03c6 c , \u03b8 LPAD c , \u03b8 GLYPH c , and \u03b8 RPAD c . The image X is the only observed random variable in our model. The identities of the characters E the typesetting layout T and the inking R will all be unobserved. We do not learn \u03b8 INK and \u03b8 VERT , which are set to the uniform distribution. Expectation Maximization During the E-step we compute expected counts for E and T , but maximize over R, for which we compute hard counts. Our model is an instance of a hidden semi-Markov model (HSMM), and therefore the computation of marginals is tractable with the semi-Markov forward-backward algorithm (Levinson, 1986) . During the M-step, we update the parameters \u03b8 LPAD Coarse-to-Fine Learning and Inference The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004) . As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008) . We use the marginals 4 from the coarse pass to prune states from the dynamic program of the fine pass. In the early iterations of EM, our font parameters are still inaccurate, and to prune heavily based on such parameters would rule out correct analyses. Therefore, we gradually increase the aggressiveness of pruning over the course of EM. To ensure that each iteration takes approximately the same amount of computation, we also gradually increase the order of the fine pass, only reaching the full order n on the last iteration. To produce a decoding of the image into text, on the final iteration we run a Viterbi pass using the pruned fine model. Figure 6 : Portions of several documents from our test set representing a range of difficulties are displayed. On document (a), which exhibits noisy typesetting, our system achieves a word error rate (WER) of 25.2. Document (b) is cleaner in comparison, and on it we achieve a WER of 15.4. On document (c), which is also relatively clean, we achieve a WER of 12.5. On document (d), which is severely degraded, we achieve a WER of 70.0. Data We perform experiments on two historical datasets consisting of images of documents printed between 1700 and 1900 in England and Australia. Examples from both datasets are displayed in Figure 6 . Old Bailey The first dataset comes from a large set of images of the proceedings of the Old Bailey, a criminal court in London, England (Shoemaker, 2005) . The Old Bailey curatorial effort, after deciding that current OCR systems do not adequately handle 18th century fonts, manually transcribed the documents into text. We will use these manual transcriptions to evaluate the output of our system. From the Old Bailey proceedings, we extracted a set of 20 images, each consisting of 30 lines of text to use as our first test set. We picked 20 documents, printed in consecutive decades. The first document is from 1715 and the last is from 1905. We choose the first document in each of the corresponding years, choose a random page in the document, and extracted an image of the first 30 consecutive lines of text consisting of full sentences. 5  The ten documents in the Old Bailey dataset that were printed before 1810 use the long s glyph, while the remaining ten do not. Trove Our second dataset is taken from a collection of digitized Australian newspapers that were printed between the years of 1803 and 1954. This collection is called Trove, and is maintained by the the National Library of Australia (Holley, 2010) . We extracted ten images from this collection in the same way that we extracted images from Old Bailey, but starting from the year 1803. We manually produced our own gold annotations for these ten images. Only the first document of Trove uses the long s glyph. Pre-processing Many of the images in historical collections are bitonal (binary) as a result of how they were captured on microfilm for storage in the 1980s (Arlitsch and Herbert, 2004) . This is part of the reason our model is designed to work directly with binarized images. For consistency, we binarized the images in our test sets that were not already binary by thresholding pixel values. Our model requires that the image be presegmented into lines of text. We automatically segment lines by training an HSMM over rows of pixels. After the lines are segmented, each line is resampled so that its vertical resolution is 30 pixels. The line extraction process also identifies pixels that are not located in central text regions, and are part of large connected components of ink, spanning multiple lines. The values of such pixels are treated as unobserved in the model since, more often than not, they are part of ink blotches. Experiments We evaluate our system by comparing our text recognition accuracy to that of two state-of-the-art systems. Baselines Our first baseline is Google's open source OCR system, Tesseract (Smith, 2007) . Tesseract takes a pipelined approach to recognition. Before recognizing the text, the document is broken into lines, and each line is segmented into words. Then, Tesseract uses a classifier, aided by a wordunigram language model, to recognize whole words. Our second baseline, ABBYY FineReader 11 Professional Edition, 6 is a state-of-the-art commercial OCR system. It is the OCR system that the National Library of Australia used to recognize the historical documents in Trove (Holley, 2010) . Evaluation We evaluate the output of our system and the baseline systems using two metrics: character error rate (CER) and word error rate (WER). Both these metrics are based on edit distance. CER is the edit distance between the predicted and gold transcriptions of the document, divided by the number of characters in the gold transcription. WER is the word-level edit distance (words, instead of characters, are treated as tokens) between predicted and gold transcriptions, divided by the number of words in the gold transcription. When computing WER, text is tokenized into words by splitting on whitespace. Language Model We ran experiments using two different language models. The first language model was trained on the initial one million sentences of the New York Times (NYT) portion of the Gigaword corpus (Graff et al., 2007) , which contains about 36 million words. This language model is out of domain for our experimental documents. To investigate the effects of using an in domain language model, we created a corpus composed of the manual annotations of all the documents in the Old Bailey proceedings, excluding those used in our test set. This corpus consists of approximately 32 million words. In all experiments we used a character n-gram order of six for the final Viterbi de- coding pass and an order of three for all coarse passes. Initialization and Tuning We used as a development set ten additional documents from the Old Bailey proceedings and five additional documents from Trove that were not part of our test set. On this data, we tuned the model's hyperparameters 7 and the parameters of the pruning schedule for our coarse-to-fine approach. In experiments we initialized \u03b8 RPAD c and \u03b8 LPAD c to be uniform, and initialized \u03b8 GLYPH c and \u03c6 c based on the standard modern fonts included with the Ubuntu Linux 12.04 distribution. 8 For documents that use the long s glyph, we introduce a special character type for the non-word-final s, and initialize its parameters from a mixture of the modern f and | glyphs. 9 Results and Analysis The results of our experiments are summarized in Table 1 . We refer to our system as Ocular w/ NYT or Ocular w/ OB, depending on whether the language model was trained using NYT or Old Bailey, respectively. We compute macro-averages across documents from all years. Our system, using the NYT language model, achieves an average WER of 28.1 on Old Bailey and an average WER of 33.0 on Trove. This represents a substantial error reduction compared to both baseline systems. If we average over the documents in both Old Bailey and Trove, we find that Tesseract achieved an average WER of 56.3, ABBYY FineReader achieved an average WER of 43.1, and our system, using the NYT language model, achieved an average WER of 29.7. This means that while Tesseract incorrectly predicts more than half of the words in these documents, our system gets more than threequarters of them right. Overall, we achieve a relative reduction in WER of 47% compared to Tesseract and 31% compared to ABBYY FineReader. The baseline systems do not have special provisions for the long s glyph. In order to make sure the comparison is fair, we separately computed average WER on only the documents from after 1810 (which do no use the long s glyph). We found that using this evaluation our system actually acheives a larger relative reduction in WER: 50% compared to Tesseract and 35% compared to ABBYY FineReader. Finally, if we train the language model using the Old Bailey corpus instead of the NYT corpus, we see an average improvement of 4 WER on the Old Bailey test set. This means that the domain of the language model is important, but, the results are not affected drastically even when using a language model based on modern corpora (NYT). Figure 7a demonstrates a case where our model has effectively explained both the uneven baseline and over-inked glyphs by using the vertical offsets v i and inking variables d i . In Figure 7b the model has used glyph widths g i and vertical offsets to explain the thinning of glyphs and falling baseline that occurred near the binding of the book. In separate experiments on the Old Bailey test set, using the NYT language model, we found that removing the vertical offset variables from the model increased WER by 22, and removing the inking variables increased WER by 16. This indicates that it is very important to model both these aspects of printing press rendering. Figure 7c shows the output of our system on a difficult document. Here, missing characters and ink blotches confuse the model, which picks something that is reasonable according to the language model, but incorrect. Learned Typesetting Layout Learned Fonts It is interesting to look at the fonts learned by our system, and track how historical fonts changed over time. Figure 8 shows several grayscale images representing the Bernoulli pixel probabilities for the most likely width of the glyph for g under various conditions. At the center is the representation of the initial parameter values, and surrounding this are the learned parameters for documents from various years. The learned shapes are visibly different from the initializer, which is essentially an average of modern fonts, and also vary across decades. We can ask to what extent learning the font structure actually improved our performance. If we turn off learning and just use the initial parameters to decode, WER increases by 8 on the Old Bailey test set when using the NYT language model. Unobserved Ink Blotches As noted earlier, one strength of our generative model is that we can make the values of certain pixels unobserved in the model, and let inference fill them in. We conducted an additional experiment on a document from the Old Bailey proceedings that was printed in 1719. This document, a fragment of which is shown in Figure 9 , has severe ink bleeding from the facing page. We manually annotated the ink blotches (shown in red), and made them unobserved in the model. The resulting typesetting layout learned by the model is also shown in Figure 9 . The model correctly predicted most of the obscured words. Running the model with the manually specified unobserved pixels re-duced the WER on this document from 58 to 19 when using the NYT language model. Remaining Errors We performed error analysis on our development set by randomly choosing 100 word errors from the WER alignment and manually annotating them with relevant features. Specifically, for each word error we recorded whether or not the error contained punctuation (either in the predicted word or the gold word), whether the text in the corresponding portion of the original image was italicized, and whether the corresponding portion of the image exhibited over-inking, missing ink, or significant ink blotches. These last three feature types are subjective in nature but may still be informative. We found that 56% of errors were accompanied by over-inking, 50% of errors were accompanied by ink blotches, 42% of errors contained punctuation, 21% of errors showed missing ink, and 12% of errors contained text that was italicized in the original image. Our own subjective assessment indicates that many of these error features are in fact causal. More often than not, italicized text is incorrectly transcribed. In cases of extreme ink blotching, or large areas of missing ink, the system usually makes an error. Conclusion We have demonstrated a model, based on the historical typesetting process, that effectively learns font structure in an unsupervised fashion to improve transcription of historical documents into text. The parameters of the learned fonts are interpretable, as are the predicted typesetting layouts. Our system achieves state-of-the-art results, significantly outperforming two state-of-the-art baseline systems.",
    "abstract": "We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google's open source OCR system.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "45",
    "year": "2013",
    "month": "August",
    "title": "Unsupervised Transcription of Historical Documents"
}