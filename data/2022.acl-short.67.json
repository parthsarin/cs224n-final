{
    "article": "Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities. We extend Mixup and propose DMIX, an adaptive distanceaware interpolative Mixup that selects samples based on their diversity in the embedding space. DMIX leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation. DMIX achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations. We probe the effectiveness of DMIX in conjunction with various similarity measures and qualitatively analyze the different components. DMIX being generalizable, can be applied to various tasks, models and modalities. Introduction Deep learning models, though effective for many applications are prone to overfitting in absence of sufficient training data. Data augmentation techniques can efficiently use this limited training data (Liu et al., 2021; Shi et al., 2020) . Interpolationbased augmentation techniques such as Mixup (Zhang et al., 2018) have shown improved performance across different modalities. Mixup over latent representations of inputs leads to further improvements (Chen et al., 2020a) . However, Mixup does not account for the spatial distribution of dataset samples, but choosing samples randomly for interpolation-based augmentation. While randomization in Mixup helps, augmenting Mixup's sample selection strategy with logic based on the similarity of the samples to be mixed can lead to improved generalization (Chen et 2020b). The relative spatial position of samples can be leveraged to produce more suitable synthetic inputs for training underlying models (Xu et al., 2021) . Further, natural language possesses hierarchical structures and complex geometries, which the standard Euclidean space cannot capture effectively (Ganea et al., 2018) . Hyperbolic geometry presents a solution in defining similarity between latent representations (Tifrea et al., 2019) . We propose DMIX, an adaptive distance-aware interpolative data augmentation method. Instead of choosing random inputs from the complete training distribution as in the case of Mixup, DMIX samples instances based on the (dis)similarity between latent representations of samples in the hyperbolic space. Furthermore, DMIX performs interpolations with trainable pair-wise parameters derived from the spatial distribution of the samples rather than sampling mixing ratios randomly from standard distributions, making it adaptive for pair-wise interpolation. Our contributions are: \u2022 We propose DMIX, a novel adaptive distanceaware interpolative regularization method developed over the spatial distribution of dataset sampled in the hyperbolic space. \u2022 DMIX outperforms existing interpolative data augmentation baselines for 8 benchmark sentence classification tasks across four languages. \u2022 DMIX achieves threshold F1 scores with 3 times less number of iterations than random Mixup while being generalizable across tasks, datasets, and modalities. Methodology We present an overview of DMIX in Figure 1 . We first introduce interpolative Mixup ( \u00a72.1), and then formulate DMIX by leveraging the relative sample distribution in the hyperbolic space ( \u00a72.2). Interpolative Mixup Given two data samples x i , x j \u2208 X with labels y i , y j \u2208 Y , and i, j \u2208 [1, N ], Mixup (Zhang et al., 2018) uses linear interpolation with mixing ratio r to generate the synthetic sample x and corresponding mixed label y , x = Mixup(xi, xj) = r\u2022xi + (1 \u2212 r)\u2022xj y = Mixup(yi, yj) = r\u2022yi + (1 \u2212 r)\u2022yj (1) Interpolative Mixup (Chen et al., 2020a) performs linear interpolation over the latent representations of models. Let f \u03b8 (\u2022) be a model with parameters \u03b8 having K layers, f \u03b8,n (\u2022) denotes the n-th layer of the model and h n is the hidden space vector at layer n for n \u2208 [1, K] and h 0 denotes the input vector. To perform interpolative Mixup at a layer k \u223c [1, K], we calculate the latent representations separately for the inputs for layers before the k-th layer. For input sample x i , we let h i n denote the hidden state representations at layer n, h i n = f \u03b8,n (h i n\u22121 ), n \u2208 [1, k] h j n = f \u03b8,n (h j n\u22121 ), n \u2208 [1, k] (2) We then perform Mixup over individual hidden state representations h i k , h j k from layer k as, h k = Mixup(h i k , h j k ) = r\u2022h i k + (1 \u2212 r)\u2022h j k (3) The mixed hidden representation h k is used as the input for the continuing forward pass, hn = f \u03b8,n (hn\u22121); n \u2208 [k + 1, K] (4) DMIX: Distance-aware Mixup Though Mixup helps generalize models better, it selects samples completely randomly for interpolation. Augmenting the sample selection strategy with intelligence derived from the spatial distribution of the samples to be mixed can lead to improved generalization. Hence, we formulate distance-aware Mixup, or DMIX. To perform DMIX, we first create a learnable matrix M N xN , which is used to perform Mixup between pair of samples. We use the hyperbolic distance as our similarity metric to initialize matrix M as it effectively captures the hierarchical structures and complex geometries that natural language text possesses. The hyperbolic distance D h between sentence embeddings e i = f \u03b8 (x i ) and e j = f \u03b8 (x j ) is, D h (ei, ej) = 2 tan \u22121 ( (\u2212ej) \u2295 ei ) (5) Here, \u2295 represents the M\u00f6bius addition \u2295 for a pair of points x, y \u2208 B, defined as, x \u2295 y := (1 + 2 x, y + ||y|| 2 )x + (1 \u2212 ||x|| 2 )y 1 + 2 x, y + ||x|| 2 ||y|| 2 (6) , ., . , || \u2022 || are Euclidean inner product and norm. We initialize M using hyperbolic distance D h and normalize it row wise to scale the values, Mij = D h (ei, ej); Mi = Mi max(Mi) (7) Using learnable matrix M, we change the Mixup formulation (Equation 1 ) for samples i and j and define DMixup as, DMixup(xi, xj) = (1 \u2212 Mij) * xi + Mij * xj (8) DMIX is defined for one sample as compared to Mixup which is defined for two samples. To perform DMIX over a sample x i , we create a set S i of the most diverse samples in the dataset based on a threshold. To create this set, we select samples having M ij above a threshold \u03c4 , Si = {x k |x k \u2208 X, M ik \u2265 \u03c4 } (9) We use \u03c4 to control the diversity of the selected samples. \u03c4 = T * max(M i ) at each step of the training, where T is a hyperparameter \u2208 (0, 1). To perform DMIX, we operate DMixup over samples x i and a random sample x j \u2208 S i , DMIX(xi) = DMixup(xi, xj), xj \u2208 Si (10) We replace the Mixup operation in Equation 3 with the DMIX operation in Equation 10 to evaluate DMIX. The final hidden state output h K is passed through a multi-layer perceptron (MLP) g \u03c6 for classification. We optimize the network using KL Divergence loss between the final output g \u03c6 (h K ) and mixed label y = DMixup(y i , y j ), which also trains matrix M end-to-end. Experimental Setup We evaluate DMIX on standard English, GLUE, and multi-lingual datasets in 4 languages (Table 1 ). Training Setup DMIX is performed over a layer randomly sampled from all the layers of the model. We use a learning rate of 2e-5, batch size of 8 and a weight decay of 0.01 for all the combinations, DMIX, DMix-NT, and Mixup. For the baselines, we sample r from a beta distribution following previous works. All hyperparameters were selected based on validation F1-score. We use BERT for English and mBERT for other languages as the base model f \u03b8 for our experiments, and their [CLS] token representation as the sentence embeddings to calculate the distances (Equation 5 ). Due to resource constraints, we only use 10, 000 samples of SST-2 for training, but do not change the validation and test split. Evaluation We compare DMIX with word-mixup (WMix) and sentence-mixup (SMix) (Guo et al., 2019) present in sentence representations, leading to better comparisons and sample selection. We also compare DMIX and its variants with their nontrainable versions (denoted by -NT in Table 3 ). These methods have matrix M fixed, and only select samples based on their relative positions in the embedding space. We observe that for all variants, the non-trainable counterparts perform poorer than the trainable counterparts, indicating that M is able to capture sample-specific information relative to other samples, generating more suitable sample selection and mixing ratio for performing interpolative data augmentation. Analyzing Convergence of DMIX We validate \"Does DMIX converge faster than TMix?\". We observe that across all datasets, DMIX achieves a benchmark F1 score in less number of training iterations compared to TMix (Figure 2 ). Since DMIX selects samples for Mixup in an adaptive distance-aware manner, it is able to generate more diverse and suitable interpolations leading to faster generalization of the underlying base model. DMIX requires 3 times less number of iterations on an average compared to TMix, or random Mixup, and hence is more generalizable and effective across languages. Impact of Sample Selection and Distance-Aware Mixing Ratio We probe the individual impact of using matrix M for distance-based sample selection and using it for performing mixup in Table 4 . We observe that both the applications of matrix M lead to improvements over TMix. Using matrix M for sample selection obtains larger improvements compared to using it as the ratio for performing mixup. This suggests that the selection of inputs for interpolation is more important than the mixing ratio when performing interpolative regularization. 82.91 94.63 77.12 79.44 70.11 73.45 {7, 9, 12} 85.30 95.63 77.44 80.19 70.19 74.32 {3, 4, 6, 7, 9, 12} 84.03 95.94 76.99 80.27 70.03 74.98 Table 5: Layer-wise ablation (F1 scores) when performing interpolative augmentations. Layer-wise Ablation We compare the performance of DMIX and TMix for different sets of mixup layers in Table 5 . TMix attains the best performance when the layer set {7, 9, 12} is used since layers 6, 7, 9 and 12 contain the most amount of syntactic and semantic information (Chen et al., 2020a) . Interestingly, DMIX achieves the best performance when the layer is sampled from the set {3, 4, 6, 7, 9, 12}. This suggests that the surface-level information contained in layers 3 and 4 (Jawahar et al., 2019) is effectively leveraged by the distance-aware matrix M, leading to further improvements over purely syntactic and semantic information in layers {6, 7, 9, 12}. We perform a study by varying the threshold \u03c4 for DMIX and present it in Figure 3 . A decreasing \u03c4 denotes a larger distribution space for sampling instances for Mixup, and a T of 0% decomposes it to TMix or random Mixup. We observe an initial increase in the performance as we constrain the embedding space, suggesting the sampling of more diverse samples for interpolation. We observe a drop in performance when the constrain becomes very high, indicating that further expanding the sampling space does not lead to more diverse synthetic samples. This shows the existence of an optimum set of input samples for performing Mixup, and we conjecture it can be related to the sparsity in the embedding distribution of different languages. Effect of Varying Thresholds Conclusion We propose DMIX, a novel data augmentation technique that interpolates samples intelligently chosen based on their hyperbolic distance in the embedding space. DMIX achieves state-of-the-art results over existing data augmentation approaches on 8 standard and multilingual datasets in English, Arabic, Turkish, and Hindi languages, requiring 3 times less number of iterations than random mixup. DMIX being independent of the underlying model and modality, holds potential to be applied on text, speech, and vision downstream tasks. Acknowledgements This work has been supported by the German Federal Ministry of Education and Research (BMBF) as a part of the Junior AI Scientists program under the reference 01-S20060. We thank the anonymous reviewers for their valuable inputs. We compare the performance of DMIX on standard English and GLUE datasets with additional baselines and interpolative augmentation methods like EMix (Jindal et al., 2020) and SSMix (Yoon et al., 2021) . B Dataset Details 1. TRAC. (Bhattacharya et al., 2020 ) is a collection of posts, comments, and other content from popular social media, streaming and sharing platforms. For the purpose of our experiments, we perform the aggression classification task, for which, the data is labelled into 3 classes based on the level of aggression. 2. TREC-Coarse. (Li and Roth, 2002) , The Text REtrieval Conference-Coarse is a question classification dataset consisting of 6 classes. The data is sourced from English questions by USC, TREC 8, TREC 9, TREC 10 and manually constructed questions. 3. TREC-Fine. (Li and Roth, 2002) C Experimental Setup We mention the optimal hyperparameter settings in Table 8 . D Comparison with Contrastive Learning E Qualitative Analysis To further analyze DMIX, we perform a qualitative study by choosing examples from the dataset and compare the predictions made by TMix and DMIX-NT with DMIX. We analyze token-level attention assigned to the individual terms by BERT, where color intensity corresponds to the attention score. We present these results in Table 7 .",
    "abstract": "Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities. We extend Mixup and propose DMIX, an adaptive distanceaware interpolative Mixup that selects samples based on their diversity in the embedding space. DMIX leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation. DMIX achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations. We probe the effectiveness of DMIX in conjunction with various similarity measures and qualitatively analyze the different components. DMIX being generalizable, can be applied to various tasks, models and modalities.",
    "countries": [
        "Germany",
        "United States"
    ],
    "languages": [
        "Turkish",
        "English",
        "Hindi",
        "Arabic"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "{DM}ix: Adaptive Distance-aware Interpolative Mixup"
}