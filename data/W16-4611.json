{
    "article": "This paper describes the NICT-2 translation system for the 3rd Workshop on Asian Translation. The proposed system employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality. Introduction In this paper, we describe the NICT-2 translation system for the 3rd Workshop on Asian Translation (WAT2016) (Nakazawa et al., 2016a ). The proposed system employs Imamura and Sumita (2016) 's domain adaptation technique, which improves translation quality using other domain data when the target domain data is insufficient. The method employed in this paper assumes multiple domains and improves the quality inside the domains (cf., Section 2). For WAT2016, the Japan Patent Office (JPO) Corpus can be regarded as multi-domain data because it includes chemistry, electricity, machine, and physics patents with their domain ID, and thus it is suitable for observing the effects of domain adaptation. WAT2016 provides the JPO corpora in Japanese and English (Ja-En), Japanese and Chinese (Ja-Zh), and Japanese and Korean (Ja-Ko) pairs. We used Ja-En and Ja-Zh pairs in order to add Asian Scientific Paper Experts Corpus (ASPEC) (Nakazawa et al., 2016b) as a fifth domain. 1 The relationship between the corpora and domains used in this paper is shown in Table 1 The remainder of this paper is organized as follows. Section 2 briefly reviews our domain adaptation. Section 3 describes the proposed translation system, including preprocessing, training, and translation. Section 4 explains experimental results focusing on the effects of domain adaptation. Domain D Data \u22ef ( \u2022 , \u2022 ) ( \u2022 , \u2022 ) ( \u2022 , \u2022 ) ( \u2022 , \u2022 ) ( \u2022 , \u2022 ) ( \u2022 , \u2022 ) \u2205 Feature Weights Domain Adaptation We used the domain adaptation method proposed by Imamura and Sumita (2016) . This method adapts a weight vector by feature augmentation (Daum\u00e9, 2007) and a feature vector using a corpus-concatenated model. Since this method only operates in feature space, it can be applied to various translation strategies, such as tree-to-tree translation. In this study, we applied it to phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003; Koehn et al., 2007) . Adaptation of Weight Vector by Feature Augmentation Most statistical machine translation employs log-linear models that interpolate feature function values obtained from various submodels, such as phrase tables and language models (LMs). The likelihood of a translation is computed as follows: log P (e|f ) \u221d w \u2022 h(e, f ), (1) where h(e, f ) denotes a feature vector and w denotes its weight vector. Figure 1 shows a feature space structure of feature augmentation. When we translate texts of D domains, the feature space is segmented into D + 1 subspaces: common, domain 1, \u2022 \u2022 \u2022 domain D. A feature vector (subvector) of each subspace is the same as that of a normal translator, i.e., feature function values obtained from phrase tables and language models. Features of each translation hypothesis are deployed to different spaces depending on the domain of the input data. For example, features obtained from domain 1 data are deployed to the common and domain 1 spaces. Features obtained from domain 2 data are deployed to the common and domain 2 spaces. In other words, features are always deployed to the common spaces. We obtain the weight vector w by optimizing a feature matrix of development data acquired by the above process. This weight vector is optimized to each domain. When we translate test data of domain i, only the subspaces of the common and domain i (i.e., subvectors w c and w i ) are used. Adaptation of Feature Vector using Corpus-Concatenated Model and Single-Domain Models Our domain adaptation method adapts the feature function h(e, f ) by changing submodels according to the feature spaces. \u2022 For the common space, where all domain features are located, we use a model trained from a concatenated corpus of all domains (i.e., the corpus-concatenated model) to obtain the features. \u2022 For the domain spaces, where only the domain specific features are located, we use models trained from specific domain data (i.e., single-domain models) to obtain the features. The procedure is summarized as follows. 1. The training corpora of all domains are concatenated. From this corpus, the corpus-concatenated model is trained. This includes all submodels, such as phrase tables, language models, and lexicalized reordering models. Similarly, the single-domain models are trained from the training corpus of each domain. 2. In feature augmentation, the scores obtained from the corpus-concatenated model are deployed to the common space as the feature function values, while those from the single-domain models are deployed to the domain spaces. We represent the augmented feature space as follows: h(f, e) = \u27e8h c , h 1 , . . . , h i , . . . , h D \u27e9, (2) where h c denotes a feature vector of the common space, and h i denotes a feature vector of the domain i space. The feature vector \u03a6 c (f, e) obtained from the corpus-concatenated model is always located in the common space. The feature vector \u03a6 i (f, e) is located in the domain-specific space i iff the domain of an input sentence is matched to i. h c = \u03a6 c (f, e), (3) h i = { \u03a6 i (f, e) if domain(f ) = i \u2205 otherwise. (4) 3. A feature matrix is obtained by translating a development set, and the weight vector w is acquired by optimizing the feature matrix. 4. For decoding, phrase pairs are first retrieved from both the corpus-concatenated and single-domain phrase tables. Use of the corpus-concatenated phrase table reduces the number of unknown words because phrase pairs appearing in other domains can be used to generate hypotheses. 5. During search of the best hypothesis, the likelihood of each translation hypothesis is computed using only the common space and domain-specific space of the input sentence. Implementation Notices There are some notices for applying the proposed method to phrase-based statistical machine translation. Empty Value In the proposed method, several phrases appear in only one of the phrase tables of the corpus-concatenated and single-domain models. The feature functions are expected to return appropriate values for these phrases. We refer to these as empty values. Even though an empty value is a type of unknown probability and should be computed from the probability distribution of the phrases, we treat it as a hyper-parameter. In other words, an empty value was set experimentally to maximize the BLEU score of a development corpus. Since the BLEU scores were almost stable between -5 and -10 in our preliminary experiments, we used -7 for all settings. If this value is regarded as a probability, it is exp(\u22127) \u2248 0.0009. Very Large Monolingual Corpora In machine translation, monolingual corpora are easier to obtain than bilingual corpora. Therefore, language models are sometimes constructed from very large monolingual corpora. They can be regarded as corpus-concatenated models that contain various domains. When we introduce models constructed from external knowledge, they are located in the common space while increasing the dimension. We introduce language models constructed from Google n-grams in Section 4. The same as the baseline system of WAT2016. Japanese Lex. Reordering Models The same as the baseline system of WAT2016. Language Models (1) 5-gram model built from the target side of the bilingual corpora. ( System Description In this section, we describe the preprocessing, training, and translation components of the proposed system (Table 2 ). Preprocessing Preprocessing is nearly the same as the baseline system provided by the WAT2016 committee. However, preorderers are added because our system is phrase-based with preordering. We used Nakagawa (2015)'s Top-Down Bracketing Transduction Grammar (TDBTG) trained by the JPO corpus as the preorderer without external knowledge. For the preorderer with external knowledge, we used the one developed in-house (Chapter 4.5 of Goto et al. (2015) ), 2 which was tuned for patent translation. Training and Optimization We used the Moses toolkit (Koehn et al., 2007) to train the phrase tables and lexicalized reordering models. We used multi-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of the bilingual corpora using KenLM (Heafield et al., 2013) . In addition, we included the Google n-gram language models for Japanese and English as the external knowledge. These are back-off models estimated using maximum likelihood. The Japanese model was constructed from Web Japanese N-gram Version 1, 3 and the English model was constructed from Web 1T 5-gram Version 1 (LDC2006T13). For optimization, we used k-best batch MIRA (Cherry and Foster, 2012) . Translation The decoder used here is a clone of the Moses PBSMT decoder. It accepts feature augmentation, i.e., it can use multiple submodels and set an empty value. Experimental Results For evaluation, we used two toolkits based on BLEU (Papineni et al., 2002) . One is the official BLEU scores provided by the WAT2016 committee. Because the official tool cannot measure a significance level of two systems, we also used the MultEval tool (Clark et al., 2011) , which can measure significance levels based on bootstrap resampling. Since we represent the mean scores of three optimizations, the MultEval scores differ from the official scores. JPO Corpus (without External Knowledge) For JPO corpus experiments, we did not use external knowledge and compared translations of the singledomain model, corpus concatenation, and domain adaptation. The JPO corpus was divided into four domains (chemistry, electricity, machine, and physics). Tables 3 and 4 show the results evaluated by the official scorer and MultEval tools, respectively. The symbol (-) indicates that the score was significantly degraded compared to that of the domain adaptation (p < 0.05). Note that test sentences of each domain were translated using the corresponding models, and the BLEU score was computed by concatenating all test sentences as a document. Results are presented in Table 4 . Corpus concatenation corresponds to typical translation quality where only the JPO corpus was used. The single-domain model scores were inferior to the corpus concatenation scores because the corpus sizes were reduced by one-quarter. In contrast, the domain adaptation scores for most language pairs improved significantly and the domain adaptation was successful. JPO and ASPEC Corpora (with External Knowledge) Next, we conducted experiments using five domains with the JPO and ASPEC corpora. In these experiments, we evaluated the effects of external knowledge using the Google n-gram language model. The results are shown in Tables 5 and 6 . We first describe the effects of external knowledge, as shown in Table 6 . In Table 6 , the upper and lower halves show the BLEU scores before and after adding the Google n-gram language model, respectively. By adding the Google n-gram LMs, 0.27, 0.82, and 0.12 BLEU scores were improved on average in the JPO domains of Ja-En, En-Ja and Zh-Ja pairs, respectively. In the ASPEC domain, \u22120.03, 0.56, and 0.67 BLEU scores were improved. Except for the Ja-En pair of the ASPEC domain, the Google n-gram language model contributed to translation quality. The Japanese model tends to be suitable for JPO and ASPEC domains compared to the English model. Next, we focused on the effect of domain adaptation with the Google n-gram LMs. In most cases, domain adaptation worked effectively except for the Ja-En pair of the ASPEC domain because the BLEU scores improved or were maintained the same level compared to those of the single-domain model and JPC ASPEC LM Method Ja-En En-Ja Ja-Zh Zh-Ja Ja-En En-Ja Ja-Zh Zh-Ja w/o   corpus concatenation. However, we confirmed that the effects of the ASPEC domain were less than those of the JPO domains because the score did not improve significantly. This is because the ASPEC domain uses one million bilingual sentences; thus, domain adaptation could not contribute to the high-resource domains. Conclusions We have described the NICT-2 translation system. The proposed system employs Imamura and Sumita (2016)'s domain adaptation. In this study, we regarded the JPO corpus as a mixture of four domains and improved the translation quality. Although we added the ASPEC corpus as a fifth domain, the effects were not significant. Our domain adaptation can incorporate external knowledge, such as Google n-gram language models. The proposed domain adaptation can be applied to existing translation systems with little modification. Acknowledgments",
    "abstract": "This paper describes the NICT-2 translation system for the 3rd Workshop on Asian Translation. The proposed system employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese",
        "English"
    ],
    "numcitedby": "1",
    "year": "2016",
    "month": "December",
    "title": "{NICT}-2 Translation System for {WAT}2016: Applying Domain Adaptation to Phrase-based Statistical Machine Translation"
}