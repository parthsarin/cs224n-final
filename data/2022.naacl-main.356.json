{
    "article": "Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from extra information and the well-designed network, QUEEN achieves state-of-the-art performance on several public datasets. Introduction Multi-turn dialogue modeling, a classic research topic in the field of human-machine interaction, serves as an important application area for pragmatics (Leech, 2003) and Turing Test. The major challenge in this task is that interlocutors tend to use incomplete utterances for brevity, such as referring back to (i.e., coreference) or omitting (i.e., ellipsis) entities or concepts that appear in dialogue history. As shown in Table 1 , the incomplete utterance u 3 refers to \"Smith\" (''\u53f2\u5bc6\u65af\") from u 1 and u 2 using a pronoun \"He\" (''\u4ed6\") and omits \"the type of cuisine\" (''\u83dc\u80b4\u7684\u7c7b\u578b\") from u 2 . This may cause referential ambiguity and semantic incompleteness problems if we only read this single utterance u 3 , which is a common case of downstream applications like retrieval-based dialogue systems (Boussaha et al., 2019) . Moreover, previous studies (Su et al., 2019; Pan et al., 2019) No, Smith does not care about the type of cuisine. \u4e0d\uff0c\u53f2\u5bc6\u65af\u4e0d\u5173\u5fc3\u83dc\u80b4\u7684\u7c7b\u578b\u3002 Table 1 : An example in multi-turn dialogue including dialogue utterance history u 1 and u 2 , incomplete utterance u 3 and rewritten utterance u \u2032 3 . also find that coreference and ellipsis exist in more than 70% of the utterances, especially in pro-drop languages like Chinese. These phenomena make it imperative to effectively model dialogue in incomplete utterance scenarios. To cope with this problem, previous works (Kumar and Joshi, 2016a; Elgohary et al., 2019; Su et al., 2019) propose the Incomplete Utterance Rewriting (IUR) task. It aims to rewrite an incomplete utterance into a semantically equivalent but self-contained utterance by mining semantic clues from the dialogue history. Then the generated utterance can be understood without reading dialogue history. For example, in Table 1 , after recovering the referred and omitted information from u 3 into u \u2032 3 , we could better understand this utterance comprehensively than before. Early works use coreference resolution methods (Clark and Manning, 2016) to identify the entity that a pronoun refers to. However, they ignore the more common cases of ellipsis. So the text generation-based methods (Su et al., 2019; Pan et al., 2019) are introduced to generate the rewritten sequence from the incomplete sequence by jointly considering coreference and ellipsis problems. Though effective, generation models neglect a key trait of the IUR task, where the main semantic structure of a rewritten utterance is usually similar to the original incomplete utterance. So the inherent structure-unawareness and uncontrollable feature of generation-based models impede their performances. For semantic structure-aware methods, Liu et al. (2020) utilize an edit operation matrix (e.g., substitution, insertion operations) to convert an incomplete utterance into a complete one. They formulate this task as a semantic segmentation problem with a CNN-based model (Ronneberger et al., 2015) on the matrix to capture the semantic structural relations between words implicitly. Xu et al. (2020) attempt to add additional semantic information to language models (Devlin et al., 2019 ) by annotating semantic role information but it is time-consuming and costly. Huang et al. (2021) propose a semi-autoregressive generator using a tagger to model the some considerable overlapping regions between the incomplete utterance and rewritten utterance, yet only implicitly learn the difference between them. Although these methods maintain some similarities between the incomplete utterance and the rewritten utterance (i.e., the overlap between them), it is difficult for these methods to explicitly model the semantic structure, especially the difference between the two utterances, ignoring the information in the incomplete utterance, such as which tokens are more likely to be replaced and which positions are more likely to require the insertion of new tokens. Therefore, there are still limitations of existing methods for IUR task, especially in jointly considering coreference and ellipsis cases and better utilizing semantic structural information. This paper proposes a simple yet effective QUEry-Enhanced Network (QUEEN) to solve the IUR task. QUEEN jointly considers coreference and ellipsis problems that frequently happen in multi-turn utterances. Specifically, we propose a straightforward query template featuring two linguistic properties and concatenate this query with utterances as input text. This query explicitly brings semantic structural guided information shared between the incomplete and the rewritten utterances, i.e., making model perceive where to refer back to or recover omitted tokens. We regard the rewritten utterance as the output from a series of edit operations on the incomplete utterance by constructing a token-pair edit operation matrix, which attempts to model the the overlap between the incomplete utterance between the rewritten utterance. Different from Liu et al. (2020) , we adopt a well-designed edit operation scoring network on the matrix to perform incomplete utterance rewriting, which is faster and more effective. QUEEN brings semantic structural information from linguistics into the model more explicitly and avoids unnecessary overheads of labeled data from other tasks. Experiments on several IUR benchmarks show that QUEEN outperforms previous state-ofthe-art methods. Extensive ablation studies also confirm that the proposed query template makes key contributions to the improvements of QUEEN. Methodology Overview Our QUEEN mainly consists of two modules: query template construction module (Sec. 2.1) and edit operation scoring network module (Sec. 2.2). From two linguistic perspectives, the former module aims to generate a query template for each incomplete utterance, i.e., coreference-ellipsis-oriented query template, to cope with coreference and ellipsis problems. This query template explicitly hints the model where to refer back and recover omitted tokens. The latter module tries to capture the semantic structural relations between tokens by constructing an edit operation matrix. As shown in Figure 1 , our goal is to learn a model to generate correct edit operations on this matrix and compute edit operation scores between token pairs so as to convert the incomplete utterance into the complete one. Query Template Construction Module By observing incomplete and rewritten utterance pairs in existing datasets, we find that pronouns and referential noun phrases in the incomplete utterance often need to be substituted by text spans in dialogue history. And ellipsis often occurs in some specific positions of incomplete utterance, conforming to a certain syntactic structure. In this module, we expect to encode these linguistic prior knowledge into the input of QUEEN. The query template is constructed as follows: Coreference-oriented Query Template In order to make QUEEN perceive the positions of coreference that need to be substituted by text spans from dialogue history, we use a special token [COREF] to replace pronouns and referential noun phrases in the incomplete utterance so as to get our coreference-oriented query template. For example, the coreference-oriented query template of the incomplete utterance \"No, he does not care\" (''\u4e0d, \u4ed6 \u4e0d \u5173 \u5fc3\") is \"No, [COREF] does not care\" (''\u4e0d,[COREF] \u4e0d\u5173\u5fc3\") . To get the target complete utterance, this query explicitly tells the model we should replace the \"He\" (''\u4ed6\") with text spans (such as 'Smith'(''\u53f2\u5bc6\u65af\") ) from dialogue history, rather than replacing other words. Here, we find all pronouns that required to be replaced using a predefined pronoun collection. Ellipsis-oriented Query Template To make QUEEN perceive the positions of ellipsis that need to be inserted by text spans from dialogue history, we define a special token [ELLIP] and put it in a linguistically right place of the incomplete utterance. Since a self-contained utterance usually contains a complete S-V-O (Subject-Verb-Object) structure, if an incomplete utterance lack any of these key elements, we could assume there is a case of ellipsis in its corresponding text position. So we perform dependency parsing on the incomplete utterance to get the structure of the incomplete utterance. For example, the parsing result of the incomplete utterance \"No, he does not care\" (''\u4e0d\uff0c\u4ed6\u4e0d\u5173\u5fc3\") is an S-V structure and lack object element, thus we put [ELLIP] at the end of the sentence to get the ellipsis-oriented query template as \"No, he does not care [ELLIP]\" (''\u4e0d, \u4ed6\u4e0d\u5173\u5fc3 [ELLIP]\"). Other circumstances are detailed in the Appendix. Then we fuse these two query templates into the final coreference-ellipsis-oriented query template. For incomplete utterance \"No, he does not care\" (''\u4e0d, \u4ed6\u4e0d\u5173\u5fc3\"), we get \"No, [COREF] does not care [ELLIP]' (''\u4e0d,[COREF] \u4e0d \u5173 \u5fc3 [ELLIP]\") as our final query template. Under supervised setting, the models will perceive the positions to refer back and recover omitted tokens for this utterance. For a multi-turn dialogue d = (u 1 , ..., u N \u22121 , u N ) containing N utterances where u 1 \u223c u N \u22121 are dialogue history and the last utterance u N needs to be rewritten, we could get the dialogue history text s = (w 1 1 , ..., w n i , ..., w N L N ) where w n i is the i-th token in the n-th utterance and L n is the length of n-th utterance. We then concatenate our coreference-ellipsis-oriented query template with the dialogue history text to get our final input text s \u2032 = (w q 1 , ..., w q k , ..., w q M , w 1 1 , ..., w n i , ..., w N L N ) where w q k is the k-th token of the query template and M is the length of query template. Edit Operation Scoring Network Module Since pre-trained language models have been proven to be strongly effective on several natural language processing tasks, we employ BERT (Devlin et al., 2019) to encode our input text to get the contextualized hidden representation H = (h q 1 , ..., h q k , ..., h q M , h 1 1 , ..., h n i , ..., h N L N ). Our model attempts to predict whether there is an edit operation between each token pair. To this end, we define an operation scoring function as follows. Since the order of utterance is also important for dialogue, we further use RoPE (Su et al., 2021) to provide relative position information : q \u03b1 i = W \u03b1 h i + b \u03b1 (1) k \u03b1 j = W \u03b1 h j + b \u03b1 (2) s \u03b1 ij = (R i q \u03b1 i ) T (R j k \u03b1 j ) (3) where \u03b1 is edit operation type including Substitution and Pre-Insertion. R is a transformation matrix from RoPE to inject position information and s \u03b1 ij is the score for \u03b1-th edit operation from i-th token in dialogue history to j-th token in incomplete utterance. During decoding for \u03b1-th operation, edit operation label Y \u03b1 ij satisfies: Y \u03b1 ij = { 1 s \u03b1 ij >= \u03b8 0 s \u03b1 ij < \u03b8 (4) where \u03b8 is a hyperparameter. Once Y \u03b1 ij equals to 1, the edit operation \u03b1 should be performed between token i and token j. Since the label distribution of edit operation is very unbalanced (most elements are zeros), we employ Circle Loss (Sun et al., 2020) Evaluation We use BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) and the exact match (EM score) as our evaluation metrics. Baseline Models We compare our model with a large number of baselines and state-of-the-art models. (i) Baselines and Generation models include sequence to sequence model (L-Gen) (Bahdanau et al., 2014) , the hybrid pointer generator network (L-Ptr-Gen) (See et al., 2017a) , the basic transformer model (T-Gen) (Vaswani et al., 2017) and the transformer-based pointer generator (T-Ptr-Gen) (See et al., 2017a) , Syntactic (Kumar and Joshi, 2016b) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) , GECOR (Quan et al., 2019) . Above methods need to generate rewritten utterances from scratch, neglecting the semantic structure between a rewritten utterance and the original incomplete utterance. (ii) Structureaware models include CSRL (Xu et al., 2020) , RUN (Liu et al., 2020) , SARG (Huang et al., 2021) . Results and Analysis Main Results We report the experiment results in Table 2, Table 3, Table4 and Table 5 . On all datasets with different languages and evaluation metrics, our approach outperforms all previous state-of-the-art methods. The improvement in EM shows that our model has a stronger ability to find the correct span, due to our model making full use of the prior information of semantic structure from our coreference-ellipsis-oriented query template. On the Chinese datasets Table 2 and Table  3 , QUEEN outperforms previous methods. Since Chinese is a pro-drop language where coreference and ellipsis often happen, the improvement confirms that QUEEN is superior in finding the correct ellipsis and referring back positions. The results on data sets of different domains and languages also show that our model is robust and effective. Ablation Study To verify the effectiveness of the query in our model, we present an ablation study in Table 8 in the Appendix. It is clear that query is important to improve performance on all evaluation metrics. Meanwhile, only using coreference-oriented or ellipsis-oriented template still improves the performance, as it can also bring semantic structure information. Inference Speed Meanwhile, we compare inference speed between our Edit Operation Scoring Network and the current fastest model RUN in A.5. As we remove the heavy model U-net of RUN and apply the simple network, our Edit Operation Scoring Network is faster than previous SOTA RUN. Case Study We also do a case study in the Appendix. Our model avoids the uncontrolled situations that the generation-based model is prone to, and our model can more easily capture the correct semantic span. Conclusion We propose a simple yet effective query-enhanced network for IUR task. Our well-designed query template explicitly brings semantic structural information to improve the ability to predict correct edit operations between incomplete utterance and complete one. Experiments have shown that our model with this well-designed query achieves promising results than previous methods. Acknowledgements This paper is supported by the National Science Foundation of China under Grant No.61876004, 61936012, the National Key R&D Program of China under Grand No.2020AAA0106700. Ethics Consideration We collect our data from public datasets that permit academic use. The open-source tools we use are freely accessible online without copyright conflicts. A Experimental Details A.1 Constructing Supervision The expected supervision for our model is the edit operation matrix, but existing datasets only contain rewritten utterances. So we adopt Longest Common Subsequence (LCS) and 'Distant Supervision' (Liu et al., 2020) to get correct supervision, which contains edit operations Substitute and Pre-Insert. A.2 Details in Query Construction Coreference-oriented Query During the training, we use the ground truth of pronouns and referential noun phrases to construct the coreferenceoriented query. During the inference, we use the constructed pronoun collection to construct the coreference-oriented query, which contains pronouns and referential noun phrases from training data and common pronouns (except ''\u6211\", \"me\" etc. ). Ellipsis-oriented Query Construction If the parsing result of the incomplete utterance is an S-V (Subject-Verb) structure and lacks subject element, we insert an [ELLIP] at the end of the incomplete utterance as the query. When there is not the S-V structure after parsing, we insert an [ELLIP] at the beginning of the incomplete utterance as the query. In other cases, we insert [ELLIP] at both the beginning and end of the incomplete utterance as the query. We use spaCy 1 for English and LTP (Che et al., 2020) for Chinese to get the result of parsing. Extra findings During the experiment, we find two interesting points: (i) As [COREF] and [ELLIP] are sparse respectively, we use a unified token [UNK] to replace [COREF] and [ELLIP] in the query to relieve the sparsity. (ii) In most cases, if there is the referring back in the utterance, there is generally no ellipsis in the utterance. Redundant [ELLIP] tokens can't bring correct guided information in this case. Therefore, once we construct Coreference-oriented Query Template successfully, we will not try to construct the Ellipsis-oriented Query Template. Our experimental results are improved by the above two tricks.  nese Open-Domain Dialogue. TASK is from English Task-oriented Dialogue. CANARD is constructed from English Context Question Answering. We fellow the same data split as their original paper. A.3 Datasets details Some statistics are shown in A.4 Details in Scoring Network Hyper-parameters We implement our model on top of a BERT-base model (Devlin et al., 2019) . We initialize QUEEN with bert-base-uncased for English and bert-base-chinese for Chinese. We use Adam (Kingma and Ba, 2015) with learning rate 1e-5. 2021 ) and Su 2 . We refer readers to their paper for more details. A.5 Inference Speed To compare the inference speed between the current fastest model RUN and our Edit Operation Scoring Network, we conduct experiments using the code released 3 . Both models are implemented in PyTorch on a single NVIDIA V100. The batch size is set to 16. Meanwhile, In order to fairly compare the speed of the two networks, we performed Distant Supervision and Query Construction before comparing. The results are shown in Table 7 . Variant EM B 1 B 2 B 3 B 4 R 1 R 2 R L QUEEN A.6 Case Study Table 9 gives 3 examples that indicate the representative situations as Hao et al. (2021) . The first example illustrates the cases when RUN inserts unexpected characters into the wrong places. T-Ptr-Gen just copies the incomplete utterance. Due to our generated query, the position that needs to be inserted has been explicitly promoted by the query. The second example shows a common situation for generation-based models. T-Ptr-Gen messes up by repeating stupidly. However, this situation doesn't happen to our model, as it is not a generation-based model. The last example refers to a long and complex entity. For these cases, it is easier for our model to get the correct span. This is because our model learns the span boundaries from the edit operation matrix. Compared to the generation-based model, we don't generate sentences from scratch and this reduces the difficulty. Meanwhile, our model is not based on CNN as RUN, which suffers from the limitation of receptive-field to find a longer span. A.7 Limitation One limitation of current edit-based IUR models, is that only tokens that have appeared in the history dialogue can be selected. Therefore, these models, including ours, cannot generate novel words, e.g., conjunctions and prepositions, to cater to other metrics, like fluency. However, this can be alleviated by incorporating an additional word dictionary as See et al. (2017b) and Liu et al. (2020) deals with the out-of-vocabulary (OOV) words to improve fluency. For fairness, we keep the same words during the experiment as RUN to mitigat it. We will consider this question as a promising direction for future works. Example",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article states it is supported by the National Science Foundation of China under Grant No.61876004, 61936012, and the National Key R&D Program of China under Grand No.2020AAA0106700. These are government-funded organizations, indicating research agency funding. There is no mention of defense, corporate, or foundation funding, and since there is funding mentioned, it is not correct to say there is none.",
    "abstract": "Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from extra information and the well-designed network, QUEEN achieves state-of-the-art performance on several public datasets.",
    "countries": [
        "China"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": 0,
    "year": 2022,
    "month": "July",
    "title": "Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting"
}