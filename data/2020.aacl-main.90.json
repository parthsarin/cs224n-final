{
    "article": "In this paper, we propose an effective deep learning framework for multilingual and codemixed visual question answering. The proposed model is capable of predicting answers from the questions in Hindi, English or Codemixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Visual Question Answering (VQA) focus on English questions only. However, many applications such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust technique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the different languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image features. We perform extensive evaluation and ablation studies for English, Hindi and Codemixed VQA. The evaluation shows that the proposed multilingual model achieves state-ofthe-art performance in all these settings. Introduction Visual Question Answering (VQA) is a challenging problem that requires complex reasoning over visual elements to provide an accurate answer to a natural language question. An efficient VQA system can be used to build an Artificial Intelligence (AI) agent which takes a natural language question * Work carried out during the internship at IIT Patna and predicts the decision by analyzing the complex scene(s). VQA requires language understanding, fine-grained visual processing and multiple steps of reasoning to produce the correct answer. As the existing research on VQA are mainly focused on natural language questions written in English (Antol et al., 2015; Hu et al., 2017; Fukui et al., 2016; Anderson et al., 2018; Li et al., 2018; Xu and Saenko, 2016; Shih et al., 2016) , their applications are often limited. Multilingual speakers often switch back and forth between their native and foreign (popular) languages to express themselves. This phenomenon of embedding the morphemes, words, phrases, etc., of one language into another is popularly known as code-mixing (Myers-Scotton, 1997 , 2002) . Codemixing phenomena is common in chats, conversations, and messages posted over social media, especially in bilingual / multilingual countries like India, China, Singapore, and most of the other European countries. Sectors like tourism, food, education, marketing, etc. have recently started using code-mixed languages in their advertisements to attract their consumer base. In order to build an AI agent which can serve multilingual end users, a VQA system should be put in place that would be language agnostic and tailored to deal with the code-mixed and multilingual environment. It is worth studying the VQA system in these settings which would be immensely useful to a very large number of population who speak/write in more than one language. A recent study (Parshad et al., 2016 ) also shows the popularity of code-mixed English-Hindi language and the dynamics of language shift in India. Our current work focuses on developing a language agnostic VQA system for Hindi, English and code-mixed English-Hindi languages. Let us consider the examples shown in Fig 1 . The majority of the VQA models (Anderson et al., 2018; Li et al., 2018; Yu et al., 2018) are capable enough to provide correct answers for English questions Q E , but our evaluation shows that the same model could not predict correct answers for Hindi Q H and Code-mixed question Q CM . The questions Q H and Q CM correspond to the same question Q E , but are formulated in two different languages. In this paper, we investigate the issue of multilingual and code-mixed VQA. We assume that there are several techniques available for monolingual (especially, English) VQA such that a strong VQA model can be built. However, we are interested in building a system that can answer the questions from different languages (multilingual) and the language formed by mixing up of multiple languages (code-mixed). We show that in a cross-lingual scenario due to language mismatch, applying directly a learned system from one language to another language results in poor performance. Thus, we propose a technique for multilingual and code-mixed VQA. Our proposed method mainly consists of three components. The first component is the multilingual question encoding which transforms a given question to its feature representation. This component handles the multilinguality and code-mixing in questions. We use multilingual embedding coupled with a hierarchy of shared layers to encode the questions. To do so, we employ an attention mechanism on the shared layers to learn language specific question representation. Furthermore, we utilize the self-attention to obtain an improved question representation by considering the other words in the question. The second component (image features) obtains the effective image representation from object level and pixel level features. The last component is multimodal fusion which is accountable to encode the question-image pair representation by ensuing that the learned representation is tightly coupled with both the question (language) and image (vision) feature. It is to be noted that designing a VQA system for each language separately is computationally very expensive (both time and cost), especially when multiple languages are involved. Hence, an endto-end model that integrates multilinguality and code-mixing in its components is extremely useful. We summarize our contribution as follows: 1. We create linguistically-driven Hindi and English-Hindi code-mixed VQA datasets. To the best of our knowledge, this is the very first attempt towards this direction. 2. We propose a unified neural model for multilingual and code-mixed VQA, which can predict answer of a multilingual or code-mixed question. 3. To effectively answer a question, we enhance the vision understanding by combining local image grid and object-level visual features. We propose a simple, yet powerful mechanism based on soft-sharing of shared layers to better encode the multilingual and code-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014) . Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and Computer Vision. In majority of these datasets, the images are taken from the large-scale image database MSCOCO (Lin et al., 2014) or artificially constructed (Antol et al., 2015; Andreas et al., 2016; Johnson et al., 2017) . There are a few datasets (Gao et al., 2015; Shimizu et al., 2018) for multilingual VQA, but these are limited only to some chosen languages, and unlike our dataset they do not offer any code-mixed challenges. VQA Models: The popular frameworks for VQA in the literature are built to learn the joint representation of image and question using the attention mechanism (Kim et al., 2018; Lu et al., 2016; Yu et al., 2017; Kafle and Kanan, 2017; Zhao et al., 2017) . Hu et al. (2018) proposed a technique to separately learn the answer embedding with best parameters such that the correct answer has higher likelihood among all possible answers. There are some works (Chao et al., 2018; Liu et al., 2018; Wu et al., 2018) which exploit the adversarial learning strategy in VQA. VQA has also been explored in medical domains (Zhou et al., 2018; Gupta et al., 2021; Abacha et al., 2018; Ben Abacha et al., 2019) . These learned representations are passed to a multilabel classifier whose labels are the most frequent answers in the dataset. Our analysis (c.f. Section 5.5) reveals that these models perform very poorly in a cross-lingual setting. MCVQA Dataset Dataset Creation: The popular VQA dataset released by Antol et al. (2015) contains images, with their corresponding questions (in English) and answers (in English). This is a challenging large scale dataset for the VQA task. To create a comparable version of this English VQA dataset in Hindi and code-mixed Hinglish, we introduce a new VQA dataset named \"Multilingual and Codemixed Visual Question Answering\" (MCVQA) which comprises of questions in Hindi and Hinglish. Our dataset 1 , in addition to the original English questions, also presents the questions in Hindi and Hinglish languages. This makes our MCVQA dataset suitable for multilingual and code-mixed VQA tasks. A sample of question-answer pairs and images from our dataset are shown in Fig 2 . We do not construct the answer in code-mixed language because a recent study (Gupta et al., 2018b) has shown that code-mixed sentences and their corresponding English sentences share the same nouns (common nouns, proper nouns, spatiotemporal nouns), adjectives, etc. For example, given an English and its corresponding code-mixed question: Q E : Where is the tree in this picture? Q CM : Is picture me tree kahan hai? It can be observed that both Q E and Q CM share the same noun { picture, tree}. The majority of answers in the VQA v1.0 dataset are of type 'yes/no', 'numbers', 'nouns', 'verbs' and 'adjectives'. Therefore, we keep the same answer in both English and Code-mixed VQA dataset. We follow the techniques similar to Gupta et al. (2018b) for our code-mixed question generation, which takes a Hindi sentence as input and generates the corresponding Hinglish sentence as the output. We translate original English questions and answers using the Google Translate 2 that has shown remarkable performance in translating short sentences (Wu et al., 2016) . We use this service as our original questions and answers in English are very short. For the code-mixed question generation, we first obtain the Part-of-Speech 3 (PoS) and Named Entity 4 (NE) tags of each question. Thereafter, we replace the Hindi words having the PoS tags (common noun, proper noun, spatio-temporal noun, adjective) with their best lexical translation. Same strategy is also followed for the words having the NE tags as LOCATION and ORGANIZA-TION. The remaining Hindi words are replaced with their Roman transliteration. In order to obtain the best lexical translation, we follow the iterative disambiguation algorithm (Monz and Dorr, 2005) . We generate the lexical translation by training the Statistical Machine Translation (SMT) model on the publicly available English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014) . Please refer to the Appendix for the comparison with other VQA datasets. Dataset Analysis: The MCVQA dataset consists of 248, 349 training questions and 121, 512 validation questions for real images in Hindi and Code-mixed. For each Hindi question, we also provide its 10 corresponding answers in Hindi. In order to analyze the complexity of the generated code-mixed questions, we compute the Code-mixing Index (CMI) (Gamb\u00e4ck and Das, 2014) and Complexity Factor FVQA (Wang et al., 2018) MSCOCO In-house participants Japanese VQA (Shimizu et al., 2018) MSCOCO Crowd workers (Yahoo) MCVQA (Ours) MSCOCO Automatically generated where S is the number of code-switches and W is the number of words in the sentences or block of text. M F = W \u2032 \u2212max{w} W \u2032 , if W' > 0 M F = 0 , if W' = 0 where W' is the number of words in distinct languages, i.e., the number of words except the undefined ones, max{w} is the maximum number of words belonging to the most frequent language in the sentence. LF = W N where W is the number of words and N is the number of distinct languages in the sentence. (CF) (Ghosh et al., 2017) . These metrics indicate the level of language mixing in the questions. A detailed distribution of the generated code-mixed questions w.r.t to various metrics are in the Appendix. We perform qualitative analysis by randomly selecting 5, 200 questions from our MCVQA dataset. A bilingual (En, Hi) expert was asked to manually create the code-mixed questions and translate the English questions into Hindi. We compute the BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) and Translation Error Rate (TER) (Snover et al., 2006) on the human translated questions and the translations obtained from the Google Translate. We achieve high BLEU and Rouge scores (BLEU 3: 80.22; ROUGE -L: 92.20) and lower TER (9.63). Methodology for MVQA Problem Statement: Given a natural language question Q in English, Hindi or code-mixed and a correlative image I, the task is to perform a complex reasoning over the visual element of the image to provide an accurate natural language answer \u00c2 from all the possible answers A. Mathematically: \u00c2 = arg max \u00c2\u2208A p( \u00c2|Q, I; \u03c6) (1) where \u03c6 is the network parameters. The architecture of our proposed methodology is depicted in Fig 3 . Our proposed model has the following components: Multilingual Question Encoding Given a question 5 Q = {q 1 , q 2 , . . . , q T } having T words, we obtain the multilingual embedding q e t \u2208 R d (c.f. Section 5.1) for each word q t \u2208 Q. The resulting representation is denoted by {q e t } T t=1 . We use multilingual word-embedding to obtain the lower-level representation of the words from English, Hindi and English-Hindi code-mixed questions. However, only word-embedding is not capable enough to offer multilingual and code-mixing capability. For a better multilingual and codemixing capability at a higher level, we introduce the shared encoding layers. In order to capture the notion of a phrase, first the embedded input {q e t } T t=1 is passed to a CNN layer. Mathematically, we compute inner product between the filter F l \u2208 R l\u00d7d and the windows of l word embedding. In order to maintain the length of the question after convolution, we perform appropriate zero-padding to the start and end of the embedded input {q e t } T t=1 . The convoluted feature q l,c t for l length filter is computed as follows: q l,c t = tanh(F l q e t:t+l\u22121 ) (2) A set of filters L of different window sizes is applied on the embedded input. The final output q c t at a time step t is computed by the max-pooling operation over different window size filters. Mathematically, q c t = max(q l 1 ,c t , q l 2 ,c t , . . . , q l L ,c t ). The final representation computed by CNN layer can be denoted as {q c t } T t=1 . Inspired from the success in other NLP tasks (Luong et al., 2015; Yue-Hei Ng et al., 2015) , we employ stacking of multiple Bi-LSTM (Hochreiter and Schmidhuber, 1997) layers to capture the semantic representation of an entire question. The input to the first layer of LSTM is the convoluted representation of the question {q c t } T t=1 . q r t = Bi-LSTM(q r t\u22121 , q c t ) (3) where, q r t and q r t\u22121 are the hidden representations computed by the Bi-LSTM network at time t and t \u2212 1, respectively. Specially, we compute the forward \u2212 \u2192 q r t and backward hidden representation \u2190 \u2212 q r t at each time step t and concatenate them to obtain the final representation q r t = \u2212 \u2192 q r t \u2295 \u2190 \u2212 q r t . The output from the previous layer of LSTM is passed as input to the next layer of LSTM. Layer Attention The encoding layers discussed in Section 4.1 are exploited by the questions from English, Hindi and English-Hindi code-mixed languages. It might not be the case that the representation of a question (in a given language) obtained from a particular encoding layer would also make a meaningful representation for the same question (in another language). In order to learn the language-specific control parameter for the encoding layer, we introduce an attention based mechanism over the encoding layer. Basically, our model learns an attentional vector over each encoder layer for each language. Our model learns a language importance weight matrix W \u2208 R m\u00d7n , where m and n correspond to the number of encoding layers and the number of different languages, respectively. The language importance weight matrix W is applied on a given language's (i) question representation in the j th encoding layer. Let us assume that the j th multilingual encoding layer generates the question representation: Q i,j = {q i,j 1 , q i,j 2 , . . . , q i,j T }. The language attentive representation for a language i and layer j is computed as follows: q i,j t = W i,j q i,j t , t = {1, 2, . . . , T } W i,j = e \u2212W i,j n k=1 e \u2212W k,j (4) The weighted question representation of i th language obtained from the j th layer can be denoted as Q i,j = {q i,j 1 , q i,j 2 , . . . , q i,j T }. In our work, we use one layer of CNN and two layers of Bi-LSTM to encode multilingual questions. At each layer of encoding, we apply language specific weight to obtain the language specific encoding layer representation. We denote the question representation obtained from the final encoding layer after applying the language specific attention as h = {h t } T t=1 . Self-Attention on Question Inspired from the success of self-attention on various NLP tasks (Vaswani et al., 2017; Kitaev and Klein, 2018) , we adopt self-attention to our model for better representation of a word by looking at the other words in the input question. The encoding obtained from multilingual encoding layer (c.f. Section 4.1.1) is passed to the self-attention layer. The multi-head self-attention mechanism (Vaswani et al., 2017 ) used in our model can be precisely described as follows: Attention(Q, K, V ) = softmax( QK T \u221a d h )V (5) where, Q, K, V and d h are the query, key, value matrices and dimension of the hidden representation obtained from the multilingual encoding layer, respectively. These matrices are obtained by multiplying different weight matrices to h. The value d h is the dimension of the hidden representation obtained from the multilingual encoding layer. Firstly, multi-head attention linearly projects queries, keys and values to the given head (p) using different linear projections. These projections then perform the scaled dot-product attention in parallel. Finally, these results of attention are concatenated and once again projected to obtain a new representation. Formally, attention head (z p ) at given head p can be expressed as follows: z p = Attention(hW Q p , hW K p , hW V p ) = sof tmax( (hW Q p )(hW K p ) T \u221a d h )(hW V p ) (6) where W Q p , W K p and W V p are the weight matrices. We exploit multiple heads to obtain the attentive representation. Finally, we concatenate all the attention heads to compute the final representation. The final question encoding obtained from the multilingual encoding layer can be represented by U = {q h 1 , q h 2 , . . . , q h T }. Image Features Unlike the previous works (Fukui et al., 2016; Yu et al., 2017; Ben-Younes et al., 2017) Multimodal Fusion We fuse the multilingual question encoding (c.f. Section 4.1.2) and image features by adopting the attention mechanism described in Kim et al. (2018) . Let us denote the question encoding feature by U \u2208 R n 1 \u00d7T and the image feature by V \u2208 R n 2 \u00d7R . The k th element representation using bi-linear attention network can be computed as follows: f k = (U T X) T k M(V T Y ) k (7) where X \u2208 R n 1 \u00d7K , Y \u2208 R n 2 \u00d7K , (U T X) k \u2208 R T , (V T Y ) k \u2208 R R are the weight matrices and M \u2208 R T \u00d7R is the bi-linear weight matrix. The E.q. 7 computes the 1-rank bi-linear representation of two feature vectors. We can compute the Krank bi-linear pooling for f \u2208 R K . With Krank bilinear pooling, the bi-linear feature representation can be computed by multiplying a pooling vector P \u2208 R K\u00d7C with f . f = P T f (8) where C is the dimension of the bi-linear feature vector. The f is a function of U , V with the parameter (attention map) M. Therefore, we can represent f = f un(U, V ; M). Similar to Kim et al. (2018) , we compute multiple bi-linear attention maps (called as visual heads) by introducing different pooling vectors. To integrate the representations learned from multiple bi-linear attention maps, we use the multi-modal residual network (MRN) (Kim et al., 2016) . Using MRN, we can compute the joint feature representation in a recursive manner: f j+1 = f un j (f j , V ; M j ).1 T + f j (9) The base case f 0 = U and 1 \u2208 R T is the vector of ones. We extract the joint feature representation for image level f i as well as object level feature f o . Answer Prediction Given the final joint representation of question with image level and object level features (c.f. Section 4.3), we augment both of these features to the counter feature (c f ) proposed in Zhang et al. (2018) . The counter feature helps the model to count the objects. Finally, we employ a two-layer perceptron to predict the answer from a fixed set of candidate answers. It is predetermined from all of the correct answers in the training set that appear more than 8 times. To this end, the logits can be computed by the following equation: A logits = Relu M LP (f i \u2295 f o \u2295 c f ) (10) The A logits is passed to a sof tmax function to predict the answer. Experimental Setup and Results Datasets and Network Training In our experiments, we use the VQA v1.0 dataset for English questions. There isn't a single setup for a multilingual VQA system which can handle both multilingual and code-mixed questions at the same time. Therefore, our primary motivation has been to set up a basic VQA system using the VQA v1.0 dataset. For Hindi and Code-mixed questions, we use our own multilingual VQA dataset (c.f. Section 3). Both the datasets have 248, 349 and 121, 512 questions in their training and test set, respectively. Each question has 10 answers. The test dataset of English VQA does not have publicly available ground truth answers. In order to make a fair comparison of the results in all the three setups, viz. English, Hindi and Code-mixed, we evaluate our proposed multilingual model on validation set of English and test set of Hindi and Code-Mixed dataset (MCVQA dataset). The training is performed jointly with English, Hindi and Code-Mixed QA pairs by interleaving batches. We update the gradient after computing the loss of each mini-batch from a given language of sample (question, image, answer). The other baselines are trained and evaluated for each language separately. For evaluation, we adopt the accuracy metric as defined in Antol et al. (2015) . Hyperparameters For English, we use the fastText (Bojanowski et al., 2016) word embedding of dimension 300. We use Hindi sentences from Bojar et al. (2014) , and then train the word embedding of dimension 300 using the word embedding algorithm (Bojanowski et al., 2016) . In order to obtain the embedding of Roman script, we transliterate 6 the Hindi sentence into the Roman script. These sentences are used to train the code-mixed embedding using the same embedding algorithm (Bojanowski et al., 2016) , and we generate the embedding of dimension 300. These three word embeddings have the same dimensions but they are different in vector spaces. Finally, we align monolingual vectors of Hindi and Roman words into the vector space of English word embedding using the approach as discussed in Chen and Cardie (2018) . While training, the model loss is computed using the categorical cross entropy function. Optimal hyper-parameters are set to: maximum no. of words in a question=15, CNN filter size={2, 3}, # of shared CNN layers=1, # of shared Bi-LSTM layers=2, hidden dimension =1000, # of attention heads=4, image level and object level feature dimension =2048, # of spatial location in image level feature =100, # of objects in object level feature=36, # of rank in bi-linear pooling=3, # of bilinear attention maps=8, # of epochs=100, initial learning rate=0.002. Results In order to compare the performance of our proposed model, we define the following baselines: MFB (Yu et al., 2017) , MFH (Yu et al., 2018) , Bottom-up-Attention (Anderson et al., 2018) and Bi-linear Attention Network (Kim et al., 2018) . These are the state-of-the-art models for VQA. We report the performance in Table 1 . The trained multilingual model is evaluated on the English VQA and MCVQA datasets as discussed in Section 5.1. Results of these experiments are reported in Table 1 . Our proposed model outperforms the state-of-the-art English (with 65.37% overall accuracy), and achieves overall accuracy of 64.51% and 64.69% on Hindi and Code-mixed VQA, respectively. Due to the shared hierarchical question encoder, our proposed model learns complementary features across questions of different languages. Gao et al. (2015) created a VQA dataset for Chinese question-answer pairs and translated them to English. Their model takes the Chinese equivalent English question as input and generates an answer. A direct comparison in terms of performance is not feasible as they treat the problem as seq2seq learning (Sutskever et al., 2014) and their model was also trained on a monolingual (English) setup.  We use their question encoding and language feature interaction component to train a model with English question and achieve overall accuracy of 57.89% on English validation dataset (our model achieves 65.37%). Recently, Shimizu et al. (2018) created a dataset for Japanese question-answer pairs and applied transfer learning to predict Japanese answers from the model trained on English questions. We adopt their approach, evaluate the model on English VQA and MCVQA dataset, and achieve 61.12%, 58.23%, 58.97% overall accuracy on English, Hindi and Code-mixed, respectively. In comparison to these, we rather solve a more challenging problem that involves both multilingualism and code-mixing. Comparison to the non-English VQA Analysis and Discussion We perform ablation study to analyze the contribution of various components of our proposed system.  features focus on encoding the objects of an image, which assist in answering the questions more accurately. Image grid level features help the model to encode those parts of the image which could not be encoded by the object level features. The proposed VQA model is built on two channels: vision (image) and language (question). We perform a study (Table 3 ) to know the impact of both the channels on the final prediction of the model. We turn off vision features and train the model with the textual features to assess the impact of vision (image) features. Similarly, we also measure the performance of the system with image features (object and image level) only. Our study provides answer to the following question: \"How much does a VQA model look at these channels to provide an answer?\". The study reveals that the proposed VQA model is strongly coupled with both the vision and language channels. This confirms that the outperformance of the model is not because of the textual similarity between questions or pixel-wise similarity between the images. We also perform experiments to evaluate the system in a cross-lingual setting. Towards this, we train the best baseline system (Kim et al., 2018) on the training dataset of one language and evaluate it on test datasets of the other two. The model performs pretty well when the languages for training and validation are the same. However, the performance of the model drops significantly when it is trained on one language and evaluated on a different language. We analyze the answers predicted by the model and make following observations: (1) Our model learns the question representation from different surface forms (English, Hindi and Hinglish) of the same word. It helps for much better representation of multilingual questions by encoding their linguistic properties. These rich information also interact with the image and extract language independent joint representation of question and image. However, the state-of-the-art models are language dependent. The question representation obtained from the state-of-the-art models could not learn language independent features. Therefore, they perform poorly in cross-lingual and multilingual setups (results are reported in Table 2 ). (2) We observe that the model performance on English VQA dataset is slightly better than Hindi and Code-mixed. One possible reason could be that the object-level features are extracted after training on the English Visual Genome dataset. Our VQA approach is language agnostic and can be extended to other languages as well. Error Analysis: We perform a thorough analysis of the errors encountered by our proposed model on English VQA and MCVQA datasets. We categorize the following major sources of errors: (i) Semantic similarity: This error occurs when an image can be interpreted in two ways based on its visual surroundings. In those scenarios, our model sometimes predicts the incorrect answer that is semantically closer to the ground truth answer. For example, in Figure 4 (b), the question is Where is the bear?. Our model predicts the forest as the answer. However, the ground truth answer is woods which is semantically similar to forest and is a reasonable answer. (ii) Ambiguity in object recognition: This error occurs when objects of an image have similar object and image-level features. For example, in Figure 4 (a) the question is Is this a ram or a ewe?. Our model predicts sheep as the answer in all the three setups, but the ground truth answer is ram. As a sheep, a ram and an ewe have similar object and image-level features and all of them resemble the same, our model could not predict the correct answer in such cases. (iii) Object detection at fine-grained level: This type of errors occur, when our model focuses on the fine-grained attributes of an image. In Figure 4 (c), the question is What is on the plate?. The ground truth answer for this question is food. However, our model predicts broccoli as the answer. The food that is present on the plate is broccoli. This shows that our model is competent enough to capture the fine-grained characteristics of the image and thus predicts an incorrect answer. (iv) Cross-lingual training of object-level features: Our proposed model has the capability to learn question features across multiple languages. However, the object-level features used in this work are trained on English language dataset (Visual Genome dataset). We observe (c.f. Figure 4(d) ) that the model sometimes fails when the question is in Hindi or Hinglish. Conclusion In this work, we propose a unified end-to-end framework for multilingual and Code-mixed question answering and create a dataset for Hindi and Codemixed VQA. We believe this dataset will enable the research in multilingual and code-mixed VQA. Our unified end-to-end model is capable of predicting answers for English, Hindi and Code-mixed questions. Experiments show that we achieve state-ofthe-art performance on multilingual VQA. We believe our work will pave the way towards creation of multilingual and Code-mixed AI assistants. In the future, we plan to explore transformer-based architectures for VQA in multilingual and code-mixed setups considering various diverse languages. Acknowledgment Asif Ekbal gratefully acknowledges the Young Faculty Research Fellowship (YFRF) Award supported by the Visvesvaraya PhD scheme for Electronics and IT, Ministry of Electronics and Information Technology (MeitY), Government of India, and implemented by Digital India Corporation (formerly Media Lab Asia). A Appendices The detailed comparison of automatically created and manually code-mixed questions w.r.t the Codemixing Index (CMI) score, Complexity Factor (CF2 and CF3) are shown in Table 4 . We also show the comparison of our MCVQA dataset with other VQA datasets in Table 5 . The analysis of MCVQA dataset are illustrated in Fig 5 and 6 . (Antol et al., 2015) MSCOCO Crowd workers (AMT) Visual7W (Zhu et al., 2016) MSCOCO Crowd workers (AMT) CLEVR (Johnson et al., 2017) Synthetic Shapes Automatically generated KB-VQA (Wang et al., 2017) MSCOCO In-house participants FVQA (Wang et al., 2018) MSCOCO In-house participants Japanese VQA (Shimizu et al., 2018) MSCOCO Crowd workers (Yahoo) MCVQA (Ours) 913 MSCOCO Automatically generated Table 5 : Comparison of VQA datasets with our MCVQA dataset. The images used are: MSCOCO (Lin et al., 2014) and NYU Depth v2 (Nathan Silberman and Fergus, 2012) .",
    "abstract": "In this paper, we propose an effective deep learning framework for multilingual and codemixed visual question answering. The proposed model is capable of predicting answers from the questions in Hindi, English or Codemixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Visual Question Answering (VQA) focus on English questions only. However, many applications such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust technique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the different languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image features. We perform extensive evaluation and ablation studies for English, Hindi and Codemixed VQA. The evaluation shows that the proposed multilingual model achieves state-ofthe-art performance in all these settings.",
    "countries": [
        "India"
    ],
    "languages": [
        "Hindi",
        "English"
    ],
    "numcitedby": "4",
    "year": "2020",
    "month": "December",
    "title": "A Unified Framework for Multilingual and Code-Mixed Visual Question Answering"
}