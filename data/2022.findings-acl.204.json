{
    "article": "Dialogue agents can leverage external textual knowledge to generate responses of a higher quality. To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable. Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge. Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner. We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an endto-end manner. Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments. Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context. In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts. Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner. 1 Introduction Building knowledge grounded dialogue agents has been an important research line (Bordes et al., 2016; Young et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2019; Moon et al., 2019; Dziri et al., 2021) . Such incorporation of real-world knowledge (Young et al., 2017; Zhou et al., 2018) gives rise to consistent, informative and engaging response generation. Unfortunately, even with a high-quality knowledge retriever, there is no guarantee that the desired knowledge can always be retrieved. There is indeed even no guarantee for the existence of the desired knowledge in the knowledge database. Hence, presenting fallback responses is an essential ability for grounded dialogue agents. We make use of the notion of answerability that represents whether a dialogue context is answerable or not conditioned on the knowledge retrieved. Figure 1 depicts an example to illustrate the importance of answerability in grounded dialogue response generation. As in the unanswerable dialogue context, a fallback response is desirable. Conversely, as in the answerable dialogue context, the response should be as informative as possible. Although the concept of answerability has been well explored in other NLP areas such as Question Answering (Rajpurkar et al., 2018) , it is underex-plored in the dialogue community. Most existing knowledge grounded dialogue agents (Young et al., 2017; Chaudhuri et al., 2019; Prabhumoye et al., 2021) and knowledge grounded dialogue datasets (Zhou et al., 2018) ignore the fallback issue. However, this is almost impractical, and it is unlikely to happen in the real world that all the contexts are answerable. One recent approach has been proposed to calibrate responses with the appropriate linguistic confidence (Mielke et al., 2020) ; however, it overlooks informativeness, or diversity, (Li et al., 2016a; Vijayakumar et al., 2016; Fan et al., 2018; Holtzman et al., 2020; Tang et al., 2021; Wang et al., 2021) , which is an important quality metric for a dialogue system. Though the previous work mentioned above (Mielke et al., 2020) employs an additional classifier for answerability, or in their case, linguistic confidence level, we demonstrate that our proposed method can achieve higher accuracy with the response generator. Our proposed model employs controlled text generation (CTG, Niu and Bansal 2018; Mielke et al. 2020; Gehman et al. 2020; Xu et al. 2020; Baheti et al. 2021) . Its central idea is to bias the generation towards a specific style by placing a control token in the input context. This control token has been investigated via two strategies: manually placed (Baheti et al., 2021) or model classified (Mielke et al., 2020) . One can manually place a control token with low offensiveness to prevent the dialogue response generator from generating an offensive context (Baheti et al., 2021) . One can also use a classifier to determine the linguistic confidence that the generator should present in its response generation (Mielke et al., 2020) . In contrast to these works, one of our characteristics is that while these works focus on the classification task only, our work turns the classification task into a generative manner and then exploits the classification result for the succeeding generation task within a single autoregressive generator. Since no existing dataset is suitable for our task, we derived a dataset by augmenting an existing dialogue dataset with unanswerable tuples of the dialogue context and the knowledge retrieved, and we conducted our experiments on the derived dataset. Our experimental results indicate that incorporating controlled text generation (Mielke et al., 2020) can capture answerability and rigorously replies with a fallback response to unanswerable contexts. However, it still undesirably hurts informativeness for answerable contexts by frequently responding with fallback responses to answerable contexts. Our method can achieve higher accuracy in classifying answerability than the traditional controlled text generation. This reduces the chance of responding with fallback to answerable contexts and thus improves the informativeness for responses to answerable contexts while still responding appropriately with fallback to unanswerable contexts. Related Work Grounded Dialogue Generation Augmenting the dialogue agents with either tableformatted knowledge base (Bordes et al., 2016) or graph-formatted knowledge base (Moon et al., 2019) enables the dialogue agents to leverage realworld facts. This is crucial in both task-oriented dialogue (Moon et al., 2019) and chitchat dialogue (Chaudhuri et al., 2019) . Dialogue agents grounded with common sense tends to be more engaging as well (Young et al., 2017) . Furthermore, it also has been pointed out that using a knowledge base could reduce the problem of hallucinations (Dziri et al., 2021) . Another research line tends to compress knowledge into model parameters, either by training set augmentation with template-based method (Madotto et al., 2020) or using neural architectures as domain-specific adapters (Xu et al., 2021) . Fallback Response in Dialogue Generation Fallback response, or even answerability, remains under-explored for grounded dialogue agents. One recent close work calibrates responses with appropriate linguistic confidence (Mielke et al., 2020) . Another close work parapharses fallback responses with contextualization (Shrivastava et al., 2021) . Informative Dialogue Generation Informativeness, or diversity, plays an important role in engaging response generation. Modified decoding strategy with a dedicated objective improves diversity (Vijayakumar et al., 2016) . Maximum mutual information (Li et al., 2016a) improves diversity with a diversity-promoting objective function for reranking. More recently, top-k sampling (Fan et al., 2018) and nucleus sampling (Holtzman et al., 2020) improve diversity by truncating the vocabularies or probability density to be sampled from and has shown their superiority over the traditional beam search for diverse dialogue generation. Methodology Background We focus on the task of dialogue generation that is capable of recognizing unanswerable dialogue contexts and generating fallback response generation in an end-to-end manner. We adopt an end-to-end autoregressive language model (Zhang et al., 2020) as our neural dialogue generator. We denote this model as M. By further denoting the knowledge retrieved as k, dialogue context as c and dialogue response as r, this generation task can be formulated as a mapping function that generates the dialogue response conditioned on the dialogue context and the knowledge retrieved: M : k, c \u2192 r Unfortunately, naively approximating this function with maximum likelihood estimation might confuse the generator as the responses for the unanswerable contexts typically confess ignorance. This type of fallback response then becomes universally likely. Without an appropriate control on generating fallback responses, our generator can even give an answerable context a response that confesses ignorance. For example, a response that confesses ignorance could be templated as 'I do not know, I have not ...' where the contextualization follows. However, simply training on this instance will make 'I' to be universally likely followed by 'do'. Therefore, even for answerable user intention, the generator could fail into producing a fallback response immediately after decoding an 'I'. Controlling Fallback Response To effectively bias generation towards confessing ignorance for unanswerable dialogue as well as bias generation towards expressing informativeness for answerable contexts, we leverage controlled text generation. The task can be expressed as: p(r | k, c) \u221d p(a | k, c) p(r | a, k, c), where the answerability a in the above formula is a binary control token that is either <|ANS|> or <|UNANS|>. The former biases the succeeding dialogue response generation towards informativeness, and the latter biases the succeeding generation towards fallback. In the previous work done by Mielke et al. (2020) , this control token is predicted by employing an extra classifier that outputs whether the dialogue context is answerable: p(r | k, c) \u221d p classifier (a | k, c) p(r | a, k, c) This introduces extra parameters from the classifier and extra overheads for the inference. Indeed, this work has primarily focused on rephrasing responses with appropriate linguistic confidence, and their methodology requires two generators and one classifier. Our method differs as we augment the dialogue agent with the unstructured textual knowledge while theirs tests the knowledge inherently encoded in the model. 2 Their proposed method operates in a pipeline fashion that first generates a response, then obtains the control token with the classifier, and finally rephrases the generation with the second generator. An important observation is that the question or the dialogue context already contains enough information to judge the appropriate linguistic confidence level (Mielke et al., 2020) . In addition, our primary goal is to directly control the fallback generation rather than maintain the semantics while calibrating the linguistic confidence. Therefore, we exclude the use of the first generator throughout our experiments. Control Token Generation Since a confidence level, or in our words, answerability, can be appropriately obtained even before generation, we could exploit this and remove the rephrasing generator. Furthermore, if we can further reduce the need for an answerability classifier, we can build an end-to-end system that replies with fallback answers to unanswerable contexts. To this end, we propose a framework that incorporates the classification of control tokens into the response generation by leveraging the power of pre-trained language models to formulate language understanding tasks into a generative manner (Raffel et al., 2019; Liu et al., 2021a,b; Zhang et al., 2021) . We illustrate the overall idea of our proposed framework in Figure 2 . Our framework incorporates a notion called control token generation, where the control token could be automatically generated by the dialogue generator in an end-to-end manner. Firstly, we place a token of <|GEN|> as a prompt to signal the model to generate a binary control token, either <|ANS|> or <|UNANS|>. The former indicates the dialogue context as answerable, and the latter indicates the dialogue context as unanswerable. This then continues in an autoregressive manner for the model to complete the remaining response generation. For the control token of <|ANS|>, it follows a search space that is diverse and informative. In contrast, the control token <|UNANS|> guides into a semantical search space for fallback responses, which typically confesses ignorance, or low linguistic confidence level (Mielke et al., 2020) . We thus formulate the problem as: p(r | k, c) \u221d p generator (a | k, c) p(r | a, k, c) Although previous works have formulated fallback response generation in a pipeline manner where the original response should attend (Mielke et al., 2020; Shrivastava et al., 2021) , our proposed framework leverages control token to directly guide the response into either informative response or fallback response that confesses ignorance. Furthermore, our framework leverages the understanding power of large-scaled pre-trained language model (Liu et al., 2021b) and reduces the need for an extra answerability classifier by incorporating control token generation. As a result, this turns the whole system from a pipeline manner into an end-to-end manner, which drastically reduces the model size and the inference overheads. Sequence-to-Sequence Learning We adopt a single autoregressive Seq2Seq generator (Zhang et al., 2020) as both our control token generator as well as our dialogue response generator. Precisely, our network accepts an input concatenation of text knowledge k and dialogue context c, and outputs an answerability control to-ken a first, and then outputs the remaining dialogue response r one by one and left to right. At the i-th timestep, the generator picks the next token r i to be presented in the output that maximises the conditional probability: r i = argmax r i \u2208V p(r i | r 1 , ..., r i\u22121 , a, k, c) Note that V in the equation above represents the vocabulary space to be decoded from. Training To train our language model, we preprocess the original training instances to incorporate control token generation. The original training instance is the concatenation of knowledge, dialogue context, and response: [k; c; r] We derive our new training instances as the concatenation of knowledge, dialogue context, control token, and response: [k; c; <|GEN|>; a; r] Note that <|GEN|> is a prompt token to signal the model to generate the succeeding answerability control token, and a is the binary control token that guides the subsequent dialogue generation. Inferencing While our dialogue generation follows the traditional scheme where we adopt the nucleus sampling, we found in our early experiments that greedy decoding can be effective for the task of control token generation, which improves classification accuracy. We thus propose two decoding strategies: \u2022 Unhindered Sampling uses nucleus sampling for both control token generation or answerability classification and dialogue response generation throughout the decoding stage. \u2022 Bottleneck Sampling 3 uses greedy decoding for control token generation and nucleus sampling for dialogue response generation. Although the former is straightforward and easy to implement, we demonstrate that the latter variant can remarkably improve the answerability classification accuracy and hence improve the succeeding response generation. Both of them can improve the response quality for the answerable contexts. Experimental Setup Dataset Preparation Since no existing dataset is suitable for our aim, we derive our dataset based on the CMU DOCUMENT GROUNDED CONVER-SATIONS DATASET (CMU DOG) dataset (Zhou et al., 2018) . CMU DOG is a multi-turn dyadic dialogue dataset in which two crowdsource workers converse and find out more about a specific movie based on that particular film profile. While most of the dialogue datasets focus only on either chitchat (Zhang et al., 2018) or task-oriented dialogue (Budzianowski et al., 2018) , CMU DOG interleaves chitchat and task-oriented dialogue (Zhou et al., 2018) . It thus requires the agent to be both informative and knowledge grounded. Such knowledge grounded dialogue agents should appropriately respond with fallbacks to the unanswerable contexts without hurting informativeness on the responses to the answerable contexts. Therefore, CMU DOG is a suitable dataset to validate the effectiveness of our proposed framework. We label all of the original instances as answerable conditioned on the ground truth knowledge. Indeed, the crowdsource workers converse based on the ground truth knowledge (Zhou et al., 2018) . We then augment with unanswerable dialogues by sampling two instances [k 1 ; c 1 ; r 1 ] and [k 2 ; c 2 ; r 2 ] from the original dataset where k 1 \u0338 = k 2 . This results into two unanswerable instances [k 1 ; c 2 ; f ] and [k 2 ; c 1 ; f ], where f represents the fallback responses that typically confess ignorance. This operation derives into a training / development / testing partition with 100,497 / 6,677 / 18,921 instances respectively for the CMU DOG dataset. 4  Unlike chitchat dialogue datasets (Zhang et al., 2018) which consist of several dialogue topics that can be irrelevant to each other, the movie profiles from CMU DOG guarantees to be within the same domain. This is important as real-world retrievers can be competitive, meaning that irrelevant retrieved knowledge can make the task oversimplified into relevance classification. Fortunately, our augmentation strategy can still derive an answerability task with moderate difficulty in which the competitive classifiers report only about 82% test accuracy on the derived CMU DOG dataset. Baseline and Comparison Model Our baseline adopts a vanilla Seq2Seq generator as a basic function mapper as described in Section 3.1 which maps the concatenation of knowledge retrieved k and dialogue context c to dialogue response r without any control over the fallback response as well as the notion of answerability. One comparison model is derived from the previous work done by Mielke et al. (2020) to employ an additional classifier to map the concatenation of knowledge retrieved k and dialogue context c to answerability control token a. It then follows the classical controlled text generation procedure to feed the concatenation of the knowledge, context and control token into the generator for response generation. Implementation Details For all the generators implemented for the baseline, comparison model and our method, we employ the state-of-the-art GPT2-based (Radford et al., 2019) dialogue response generator DIALOGPT-SMALL (Zhang et al., 2020) . We also attempted on DIALOGPT-MEDIUM and DIALOGPT-LARGE. We found all three of them tend to respond inappropriately with fallbacks to the answerable dialogue contexts, and they report similar diversity measurements. Therefore, we adopt DIALOGPT-SMALL for simplicity. We use a learning rate of 5e\u22124, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and \u03f5 = 1e\u22128. We adopt ROBERTA-BASE (Liu et al., 2019) as the answerability classifier to be used in our comparison model. We also experimented on BERT-BASE, BERT-LARGE (Devlin et al., 2019) and ROBERTA-LARGE, which led to a similar accuracy. Therefore, we adopt ROBERTA-BASE for simplicity. For the classifier, we use a learning rate of 5e\u22126, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and \u03f5 = 1e\u22128. Since we are interested in diversity, or informativeness, we use nucleus sampling, or top-p sampling (Holtzman et al., 2020) as our decoding mechanism throughout our experiments for all our baseline, comparison model, and our method, in which we set p = 0.95 as the hyper-parameter as in the work done by Holtzman et al. (2020) . We conduct our experiments with the TRANSFORMERS library (Wolf et al., 2020) . Evaluation Metrics In this work, we mainly focus on generation diversity for answerable contexts. We also report our investigation on fallback issues for unanswerable contexts as well as the classification accuracy. We followed previous works to adopt Distinct-n (Li et al., 2016b; Gao et al., 2019 Cai et al., 2019; Lippe et al., 2020) . It is the ratio of the number of unique n-grams against the total number of n-grams generated. We follow the work done by Gao et al. (2019) to calculate Distinct-n: ; Model B + B-2 + B-3 + B-4 + D-1 + D-2 + D-3 + D-4 + D-5 + D- Distinct-n = | N i=1 R i | N i=1 | R i | , where R i represents the set of n-grams in the sample i and | R i | represents the number of elements in the set. Gao et al. (2019) has employed n={1,2}, and they primarily focused on task-oriented dialogue. In contrast, we conducted our experiments on CMU DOG, which interleaves chit-chat and task-oriented dialogue. Since two tasks naturally differ, for our investigation on CMU DOG, we extend the unigrams and the bigrams to trigrams, four-grams, five-grams and six-grams, and we report Distinct-n where n={1,2,3,4,5,6} to measure phrase-level and sentence-level diversity. We also report BLEU score, which is a widely adopted sequence evaluation metrics (Papineni et al., 2002) . To investigate fallback response generation, we report the number of fallback responses replied to answerable (FR + ) and unanswerable contexts (FR \u2212 ). 5 The former attains a better quality with lower values and the latter attains a better quality with higher values. For the control token generation, or answerability classification, we report the 5 The scores are obtained by keyword detection. overall accuracy (Acc.), recall (Rec.), precision (Pre.), and F1-score (F1). 5 Results and Discussions Main Results Table 1 depicts the main results on dialogue generation. B represents BLEU scores, and D represents Distinct scores. We mainly report on the answerable dialogue contexts, i.e. the original dataset. As done in Mielke et al. (2020) , we build a comparison model by incorporating the idea of controlled text generation to generate fallback responses. Incorporating controlled text generation does improve response diversity; however, it degrades the BLEU scores, which could be a side effect of naively incorporating controlled text generation. We postulate that placing an unanswerable control token makes the model more confident in outputting a fallback response even to answerable contexts. In contrast, a basic E2E model without controlled text generation can still escape from the fallback situation during the decoding phase. This leads to the conclusion that naively incorporating controlled text generation still hurts the response quality. In contrast, our proposed methods are not influenced by the side effect discussed above and report better BLEU scores than our baselines. In addition to the remarkable improvement in BLEU scores, our proposed method can improve word-level diversity (Li et al., 2016b; Gao et al., 2019; Cai et al., 2019; Lippe et al., 2020) as well as phrase-level and sentence-level diversity, which surpasses all our baseline and comparison models. Decoding Methods Non-deterministic sampling can improve the diversity or surprisingness of the response generation (Fan et al., 2018; Holtzman et al., 2020) . One should be curious about whether such a case applies to the control token generation as well. Our results indicate that it is not the case. Our method with Bottleneck Sampling reports better diversity measurements and BLEU scores than Unhindered Sampling on CMU DOG. Indeed, we observe that decoding greedily on the answerability control token gives better accuracy than sampling, which could be the reason for the improved response generation. Still, Unhindered Sampling is straightforward to implement, and it reports a better quality in almost all of the metrics than our baselines, and the improvements with Bottleneck Sampling are less significant than the improvements in comparing Unhindered Sampling with our baselines. Fallback Response Generation Table 2 reports the number of fallbacks generated for answerable and unanswerable dialogue contexts on CMU DOG. As mentioned in Section 3.1, our observation is that the basic E2E model without controlled generation fails to capture the notion of answerability. Our model has a much better FR + score than our E2E baseline. For the baseline, such a failure in determining the answerability drastically affects the informativeness for answerable dialogue contexts by responding undesirably frequently with fallback. A similar phenomenon can be observed for our comparison model, though the problem is reduced, and the comparison model is better than the E2E baseline at responding with fallback to unanswerable contexts. However, our comparison model still suffers from responding with fallback to answerable contexts, which is undesirable for informative response generation for answerable contexts. In contrast, our method can reduce this problem more effectively and appropriately reply with fallback to unanswerable contexts. Note that the number reported here is not strictly the answerability classification accuracy, as we observed that a fallback response could be generated even with an answerable control token. This aligns with the fact reported in Baheti et al. (2021) that the model can generate an offensive response even with an offensiveness control token. Answerability Classification By prompting the dialogue response generator, our proposed methods can achieve better classification results than an external classifier that introduces extra model parameters as well as the extra classification overheads. As mentioned in Section 4, we are particularly interested in the dataset of CMU DOG, where all the knowledge for negative samples are in-domain movie profiles. This is important, as real-world retrievers are competitive, and we do not want the task to be oversimplified. Fortunately, our competitive classifiers achieve an accuracy of about 82% on the derived dataset. This fact validates that the derived task is with moderate difficulty as there was still space for improvements for the classical classification models. Table 3 reports the classification results on CMU DOG dataset. Our proposed method has a better score on Rec. + , Pre. \u2212 , F1 + , F1 \u2212 and Acc., which remarkably surpasses all the competitive classifiers. Our model also reports an on-par performance on Rec. \u2212 and Pre. + with ROBERTA-BASE. This aligns with the fact reported in Section 5.3 that our method can capture more answerable contexts and prevent the model from responding with fallback to them. Consequently, as we report in the main result in Section 5.1, it improves informativeness to the succeeding response generation to answerable contexts. Unhindered Sampling reports a bit lower accuracy than Bottleneck Sampling. This means that employing greedy decoding is desirable for classification and can improve the answerability classification accuracy. As in correlates well with the improvements in response generation. In addition, this also aligns with the FR scores reported in Table 2 , where Bottleneck Sampling has better FR scores than Unhindered Sampling. We conclude that our method is better at capturing answerable contexts than our baseline models while still achieving on-par performance on recalling unanswerable contexts and generating fallbacks to them. Human Evaluation We hired three experienced annotators who have degrees relevant to English Linguistics. We present 400 questions with 100 sampled answerable testing instances and ask them to conduct A/B testing. We conduct two sets of the experiment. The first set compares the baseline with our model, and the second set compares the comparison model we built as done in Mielke et al. (2020) and our model. By following previous work (Li et al., 2019; Zou et al., 2021) , we adopt the following criteria: \u2022 (Appropriateness): \"Which one is more appropriate given the dialogue context?\" \u2022 (Informativeness): \"Which one presents a more informative and diverse answer?\" \u2022 (Engagingness): \"Which one would you prefer to talk with for a long talk?\" \u2022 (Human-likeness): \"Which one do you think sounds like a real person?\" Table 4 and Table 5 report the human evaluation results. Our proposed method significantly surpasses our baseline and our comparison model in all of the four quality metrics. This phenomenon is expected and aligns with the fact presented in Section 5.1 which states that the automatic evaluation reports better diversity measurements on the response generation. This also aligns with the fact reported in Section 5.3 and Section 5.4 that the E2E baseline is unaware of the notion of answerability, and our competitive classifier employed for our comparison model has a low Rec. + on answerable contexts. In contrast, our method solidly improves the overall response quality by appropriately incorporating controlled fallback response generation in an endto-end manner. Note that we conduct both sets of human evaluation based on our proposed method with Bottleneck Sampling. Conclusion Building a grounded dialogue agent is an important research line. However, most previous works have overlooked the situation when the retrieved knowledge cannot help the agent answer the dialogue. Under such a situation, fallback answers should be appropriately presented, and such incorporation should not degrade the informativeness in responses to answerable contexts. We demonstrate that a standard language model fails to handle this situation well and degrades the informativeness of responses to answerable dialogue contexts. Controlled text generation can be a solution that rigorously replies with fallback to unanswerable contexts. However, naively incorporating controlled text generation still hurts informativeness for the answerable contexts. We propose a novel end-to-end framework that leverages the understanding power of language models for answerability classification that steps into controlled response generation naturally in an autoregressive manner. Our experimental results from both automatic and human evaluation demonstrate that our method achieves higher accuracy on dialogue answerability classification than the competitive models specially designed for language understanding. This improves the informativeness for answerable dialogue contexts while still maintaining the ability to reply with fallback to unanswerable dialogue contexts. Ethics Statement This work conducts experiments on the well-known dialogue datasets, and the dataset pre-processing does not make use of any external textual resource. Pre-trained end-to-end dialogue generators using large-scale text corpus are also employed, which might be subjected to offensive contexts and demographic or historical biases buried in the training data. Although the model releasers have attempted their efforts to reduce offensiveness contexts and biases in their training data, the model retains the potential to generate output that triggers offensive replies and might express agreement towards offensive or unethical contexts. The reverse situation also applies, and the model might express disagreement towards ethical contexts. However, due to the fact that current state-of-the-art end-to-end pre-trained dialogue generators or pre-trained language models are mostly trained on large corpus or conversations that naturally occur, the abovementioned issues are widely known to commonly exist for these models. Either heuristics or neuralbased methods are suggested to be employed to post-process the outputs to eliminate any potential ethical issues presented by the models. Finally, we declare that any biases or offensive contexts generated from the model do not reflect the views or values of the authors. Acknowledgments This research/paper was supported by the Center for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission's InnoHK scheme.",
    "abstract": "Dialogue agents can leverage external textual knowledge to generate responses of a higher quality. To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable. Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge. Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner. We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an endto-end manner. Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments. Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context. In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts. Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner. 1",
    "countries": [
        "Hong Kong"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "On Controlling Fallback Responses for Grounded Dialogue Generation"
}