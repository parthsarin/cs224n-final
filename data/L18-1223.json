{
    "article": "Commonsense knowledge plays an essential role in our language activities. Although many projects have aimed to develop language resources for commonsense knowledge, there is little work focusing on connotational meanings. This is because constructing commonsense knowledge including connotational meanings is challenging. In this paper, we present a Japanese knowledge base where arguments in event sentences are associated with various feature changes caused by the events. For example, \"my child\" in \"my wife hits my child\" is associated with some feature changes, such as increase in pain, increase in anger, increase in disgust, and decrease in joy. We constructed this knowledge base through crowdsourcing tasks by gathering feature changes of arguments in event sentences. After the construction of the knowledge base, we conducted an experiment in anaphora resolution using the knowledge base. We regarded anaphora resolution as an antecedent candidate ranking task and used Ranking SVM as the solver. Experimental results demonstrated the usefulness of our feature change knowledge base. Introduction Commonsense knowledge plays an essential role in our language activities. Such knowledge also plays an important role for computers to understand texts. In the area of language resource development, most studies on commonsense knowledge focus on denotational meanings such as predicate-argument structures (Baker et al., 1998; Palmer et al., 2005) . Meanwhile, there are few studies focusing on connotational meanings (Nakamura and Kawahara, 2016; Rashkin et al., 2016) . Even in such studies, abstract features such as polarities are used. However, concrete knowledge (that is, finegrained knowledge) is better for computers than abstract knowledge because events actually cause various concrete feature changes to participants in the events or those who know of the events. There are two approaches to acquiring commonsense knowledge. One is the automatic acquisition approach; the other is the manual acquisition approach. The automatic acquisition approach uses machine learning techniques and pattern matching methods. Although this approach is useful when the amount of data to be acquired is extremely large, the quality of acquired knowledge might be low. Moreover, the existence of reporting bias in texts (Gordon et al., 2010; Gordon and Durme, 2013) shows that infrequent events tend to appear more in texts than quotidian events. The manual approach is useful for gathering subjective information (such as emotional information) and quotidian commonsense knowledge, which are difficult to acquire automatically. This approach may use a fully manual technique (at a very early stage of artificial intelligence studies) or collective intelligence (e.g., crowdsourcing and games with a purpose). In this study, we aim to solve some problems in previous work on connotational meanings by constructing a large-scale knowledge base of concrete feature changes of arguments in event sentences with a controlled gran-ularity. For example, \"my child\" in \"my wife hits my child\" is associated with some feature changes such as increase in pain, increase in anger, increase in disgust, and decrease in joy. The proposed knowledge base is for Japanese. We combined automatic and manual approaches (collective intelligence) to exploit the merits of both techniques. After the construction of our feature change knowledge base, we conducted an experiment on anaphora resolution using the knowledge base. In this experiment, we used a Japanese translated version of the Winograd Schema Challenge (WSC) dataset (JWSC) (Levesque, 2011; Shibata et al., 2015) . Related Work There are many studies focusing on events themselves. FrameNet (Baker et al., 1998) is a corpus in which deep cases (semantic roles) of predicates are defined. In FrameNet, each argument in an example sentence is labeled with a deep case. ConceptNet (Speer and Havasi, 2012 ) is a large semantic network constructed in the Open Mind Common Sense project (Singh et al., 2012) . It is composed of relationships between concepts, where concepts are noun, verb or adjectival phrases. To date, some knowledge bases have been developed to capture connotational meanings of predicates. Lexical Conceptual Structure (LCS) is a useful way to describe behavior of arguments in event sentences (Jackendoff, 1983) . In LCS, not only behaviors of arguments but also relationships between arguments in event sentences are described, for each verb. However, LCS focuses only on direct and explicit information. It is necessary to develop a method to process indirect and implicit information because such knowledge is frequently used. Moreover, there is a possibility that many predicates are excessively generalized because the information used in LCS is abstract. WordNet-Affect (Strapparava and Valitutti, 2004 ) is an extended version of WordNet (Fellbaum, 1998) . Although synsets of WordNet are associated with some emotions in WordNet-Affect, it is hard to know who/what is associated with the emotions in events. Connotation Frames are a knowledge base of emotional implications of events (Rashkin et al., 2016) . In Connotation Frames, implications of an event are represented by a set of polarities categorized into five types (writers' perspective, entities' perspective, effect, value, and mental state). To construct Connotation Frames, crowdsourcing and a news corpus were used. Although this knowledge base is useful for understanding information conveyed by event descriptions, it is hard to know details of the emotions associated with events because emotions themselves and associated behaviors are abstracted by polarities in the knowledge base. As for connotational meanings, especially for mental states, there are many studies that exploit traditional emotion models proposed by psychologists (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Hasegawa et al., 2013; Vu et al., 2014) . In these studies, Ekman's Big Six Model (Ekman, 1992) or Plutchik's wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora or Web documents. However, as pointed out in Gui et al. (2017) , most natural language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014) . There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events (Ghazi et al., 2015; Gui et al., 2016; Gui et al., 2017) . These studies use explicit keywords in texts to recognize relationships between emotions and events. There are many studies on automatic acquisition of relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014) . Knowledge bases constructed in these studies are useful to determine what events happen after other events. However, these studies do not focus on motivations for events or effects caused by the events. As described in the previous section, this approach cannot guarantee quotidian connotational meanings from texts because of the reporting bias. Therefore, it is important to construct knowledge bases of such commonsense knowledge by manual intervention as well as through automatic acquisition. Design of JFCKB Our final goal is to construct a knowledge base of various feature changes of arguments in event sentences with the controlled granularity and to develop various methods for deep understanding of texts. To achieve the goal, we propose a knowledge base structure shown in Table 1 . In this structure, each argument in an event sentence is associated with various feature changes caused by the event. For example, in the case of \"my wife hits my child,\" \"my child\" is associated with changes of some features such as increase in pain, increase in anger, increase in disgust, decrease in joy, and decrease in trust. We gathered such information through collective intelligence (i.e., crowdsourcing). Event sentences to be stored in our knowledge base are created Sentence Case Probability (word) My wife hits ga joy my child (nominative) (+, \u2212, UNC) (wife) = (0, 1, 0) : wo anger (accusative) (+, \u2212, UNC) (child) = (1, 0, 0) : ni (dative) N/A (NULL) reader disgust (NULL) (+, \u2212, UNC) = (0.99, 0, 0.01) : Table 1 : Example of the proposed knowledge base structure (JFCKB). This knowledge base is for Japanese. In JFCKB, each sentence has various feature changes for three cases (ga case, wo case, and ni case) and those of readers. The three cases are Japanese language specific syntactic roles. The ga, wo, and ni cases roughly correspond to nominative, accusative, dative, respectively. Readers are not arguments in the sentence. In the \"Probability\" column, symbols +, \u2212, and UNC denote increased, decreased, and unchanged, respectively. based on some knowledge bases automatically constructed from large-scale Web documents. Hereafter, we call our knowledge base JFCKB (Japanese Feature Change Knowledge Base). JFCKB is a Japanese knowledge base. We pay attention to the controlled granularity for feature definition. Many studies suggest that it is important to focus on the types of changes caused by events to recognize the events. In infant cognitive development studies, there are several reports that even infants use information about the basic features of participants in events to understand the events (Massey and Gelman, 1988; Baillargeon et al., 1989; Spelke et al., 1995) . In cognitive linguistics, basic level category is an extremely important concept (Rosch et al., 1976; Taylor, 1995) . Basic level categories minimize attributes shared with other categories. Members belonging to a basic level category maximize attributes shared with other members of that same category. For example, \"dog\" is a basic level category while \"Great Pyrenees\" and \"German shepherd\" are not. Many cognitive linguists believe that our languages are built around basic level categories. Therefore, the granularity of knowledge is an important aspect of our language activity that needs to be captured. as closely as possible. This decision was based on a traditional emotion study (Plutchik, 1980) , Japanese thesauri (Ikehara, 1997; NINJAL, 2004) , sentiment analysis studies (Tokuhisa et al., 2008; Tokuhisa et al., 2009) , and features used in the VerbCorner project (Hartshorne et al., 2014) . Although our final version of the proposed knowledge base will have all the features in Table 3 , at present, emotional and sensory features in the table have been mainly investigated through crowdsourcing tasks described in the following section. Construction of JFCKB In our previous work (Nakamura and Kawahara, 2016) , we constructed a trial version of a knowledge base in which each argument of an event sentence was associated with various feature changes caused by the events. As a result of a subjective evaluation experiment, it was shown that such feature changes can be appropriately acquired using crowdsourcing. The construction of JFCKB was achieved using crowdsourcing. The details are described in the next subsections. Phase 1: Event Sentence Creation Event sentences in JFCKB are representative sentences of case frames of verbs in KUCF. The sentence creation procedure was as follows. Step 1: Selection of verbs. We used the 200 most frequent verbs in KWDLC, the 1,000 most frequent verbs in KUCF, and all verbs in the JWSC dataset. Step 2: Representative argument selection. For each case (syntactic role) of each case frame of each verb, the most frequent argument was used as the representative argument of the case. In the case of KWDLC, the two most frequent arguments were used. We used ga case, wo case, and ni case as arguments. These cases are Japanese grammatical syntactic roles and roughly correspond to nominative, accusative, and dative, respectively. Step 3: Representative sentence creation. Sentences were created by combining the verbs with the three types of representative arguments. Step 4: Incomprehensible sentence exclusion. Sentences difficult to understand were pruned based on a crowdsourcing task. The crowdsourcing workers were asked whether they could understand the sentences presented. The comprehensibility of each sentence was judged by ten workers. In total, 1,559 people participated in the task. After the crowdsourcing task, we estimated the probability that each sentence was judged comprehensible by the participants based on the aggregation method proposed by Whitehill et al. (2009) . Sentences with probabilities of comprehensible less than 0.9 were discarded. Unlike majority voting, this method calculates the probability based on worker agreements. As a result, 9,073 sentences remained out of 11,189 representative sentences. These 9,073 sentences included 19,052 arguments (tokens) (4,882 types). Phase 2: Animacy Investigation When a word does not denote a concrete object, it is not necessary to ask about feature changes of the word (e.g., \"discussion\"). Even if the word denotes a concrete object, it is not necessary to investigate feature changes of emotions and senses of the word when the word denotes an object without emotions and senses (e.g., \"stone\"). To avoid such redundant asking in the feature change investigation task, we conducted a crowdsourcing task to investigate the animacy of words (arguments) in event sentences in advance. In this task, crowdsourcing workers were asked whether presented words denote concrete objects or may have emotions or senses. That is, we divided this phase into two sub phases. The former (sub phase 1) is a task to investigate the concreteness of arguments. The other (sub phase 2) is a task to investigate whether arguments have emotions. In total, 1,011 participants completed these two sub tasks. The method for calculating the probability of answers was the same as in the event sentence creation phase. We discarded words whose probabilities of concrete object or object with emotion were less than 0.5. As a result, as far as we investigated, 1,575 types of words were judged as those representing concrete objects out of 3,457 types of words. 677 types of words were judged as those with emotions out of How does anger of \"my child\" change before and after the event described by the sentence below? (select increased, decreased, or unchanged) My wife hits my child (a) How did your anger change before and after you read the event described by the sentence below? (select increased, decreased, or unchanged) My wife hits my child (b) Figure 1 : Questions used for crowdsourcing tasks. These tasks are to acquire concrete feature changes caused by events described by the sentences. 3,457 types of words. Note that the number of words whose animacies were investigated (3,457 types) is different from the number of words in JFCKB (4,882 types) because this phase was applied from the middle of constructing JFCKB. Phase 3: Feature Change Investigation According to results of the previous two phases (i.e., event sentence creation and animacy investigation), we conducted a crowdsourcing task to gather feature changes of arguments in event sentences. We also attempted to gather those of sentence readers. That is, crowdsourcing workers were asked to answer one of feature changes of presented arguments in sentences (Figure 1 (a)) or those of the workers themselves (Figure 1 (b)). In total, 33,683 people participated in this task. Probability calculations were carried out in the same way as the previous phases. The resulting knowledge base includes 5,647 case frames (types). These case frames are for 975 verbs (types). Note that one verb has one or more case frames because of the verbal polysemy as shown in Table 2 . Validating Usefulness of JFCKB: Anaphora Resolution Evaluation Settings To validate the usefulness of JFCKB, we conducted an experiment of anaphora resolution using JFCKB. We adopted JWSC as an anaphora resolution problem. Each problem in JWSC is basically composed of two sentences including one anaphor, two antecedent candidates, and a correct antecedent. This anaphora resolution task can be regarded as an antecedent candidate ranking task because it is a task to select the correct antecedent from two candidates. We used Ranking SVMs 3 (Joachims, 2002) as the solver and used two-degree polynomial kernel as the kernel function. We set the cost parameter (trade-off between training error and margin) to 0.01. In this experiment, three conditions were compared: (1) the case using only feature change information (hereafter, FC); (2) the case using only word embeddings of word2vec (hereafter, W2V), and (3) the case using both feature change information and word embeddings (hereafter, BOTH). The vectors given to Ranking SVMs were composed of vectors expressing anaphors and antecedent candidates, vectors expressing the nearest (in parse trees) predicates of anaphoras and antecedent candidates, vectors expressing the first and second sentences, vectors expressing differences between anaphoras and antecedent candidates, and vectors expressing either of two antecedent candidates. As for the order in Ranking SVMs, the vector with the last component expressing the correct antecedent was regarded the first one. The feature change vector of each argument is a 33 dimensional vector because we used eight emotion features ( joy, trust, surprise, disgust, fear, sadness, anger, and anticipation) and three sensory featues (pain, sleepiness, and tiredness). Each feature change is composed of three probabilities (increased, decreased, and unchanged). The feature change vector of each predicate is a 99 dimensional vector because each predicate vector is composed of vectors for three cases (ga case (nominative), wo case (accusative), and ni case (dative)). Vectors in word2vec are 500 dimensional vectors learned from 100 million Web sentences analyzed by JUMAN++ 4 . Vectors composed of the following feature vectors were given to Ranking SVM in all three conditions. (1) vectors expressing anaphors and antecedent candidates, (2) vectors expressing the nearest (in parse trees) predicates of anaphoras and antecedent candidates, (3) vectors expressing the first and second sentences, (4) vectors expressing differences between anaphoras and antecedent candidates, and (5) vectors expressing either of two antecedent candidates. We used KNP 5 for dependency parsing. To solve JWSC problems, we created each vector of antecedent candidates based on five types of vectors described above and compared them. For each antecedent candidate, the five types of vectors were concatenated. Therefore, according to the dimension numbers of feature change vectors and word2vec vectors described above,vectors in FC, W2V, and BOTH are 693 dimensional, 5,500 dimensional, and 6,193 dimensional vectors respectively. As for the order in Ranking SVM, the vector with the last component expressing the correct antecedent was regarded the first one. For example, when the first sentence \"the bee landed on the flower,\" the second sentence \"Because it wanted pollen,\" two antecedent candidates \"bee, flower,\" the anaphor \"it,\" and the correct antecedent \"bee\" were given, the vector in which the last component represents \"bee\" is the first one (Figure 2 ). We conducted ten-fold cross validation to evaluate system performance. Although JWSC is composed of 1,321 problems, we used 441 problems in this evaluation. These prob-  lems satisfied the following conditions: (1) each was composed of two sentences; (2) all the predicates of anaphors and those of antecedent candidates have information about their feature changes and word embeddings. Results The result of the cross validation is shown in Table 4 . Table 4 shows that the accuracy of FC outperformed those of the others. An example that feature changes worked well was the case \"James always gives orders to Owen. Because he is very bossy.\" Figure 3 (a) shows feature changes of this example. In this case, FC and BOTH estimated the correct answer while W2V failed. An example that feature changes did not work was the case \"Bill punched Larry. And he was hurt.\" Figure 3 (b) shows feature changes of this example. In this case, FC and BOTH failed to estimate the correct answer while W2V succeeded. Note that Figure 3 shows representative feature change values. The representative value of each feature is the weighted average of the feature, where weights of increased, decreased, and unchanged are +1, \u22121, and 0, respectively. For example, when the feature change probabilities are 0.8, 0.15, and 0.05 for increased, decreased, and unchanged, respectively, the representative value of the feature is 0.65. In the case that feature changes worked well, as can be seen in Figure 3 (a), the correct antecedent has some feature changes in common with the anaphors but the incorrect antecedent has a conflicting feature change (e.g., anticipation). In the case that feature changes did not work, as can be seen in Figure 3 (b), there are no conflicting feature changes. Considering these results, we speculate that such common feature changes and conflicting feature changes influence the estimation of the correct antecedent candidates in anaphora resolution. Conclusion In this study, we constructed a knowledge base where arguments in event sentences are associated with various feature  changes caused by the events. Features used in the proposed knowledge base are determined by considering various studies such as traditional psychological studies, studies on cognitive development of infants, sentiment analysis studies. This knowledge base was constructed through crowdsourcing tasks. After the construction of our feature change knowledge base, we conducted an experiment of anaphora resolution using the knowledge base. In this experiment, we regarded anaphora resolution as an antecedent candidate ranking task and compare three conditions (the case using only the feature change information, the case using only the word2vec information, and the case using both of the feature change information and the word2vec information). As a result, it was shown that the condition where only the feature change information was used outperformed the other conditions. This result suggests the usefulness of our feature change knowledge base. Acknowledgements This work was supported by JST PRESTO Grant Number JPMJPR1402, Japan.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 1.9361263126072004e-07,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section of the article explicitly mentions that the work was supported by JST PRESTO Grant Number JPMJPR1402, Japan. JST (Japan Science and Technology Agency) is a government-funded organization that provides grants for scientific and technological research, which classifies it as a research agency. There is no mention of funding from defense, corporate entities, foundations, or an indication that no funding was received.",
    "abstract": "Commonsense knowledge plays an essential role in our language activities. Although many projects have aimed to develop language resources for commonsense knowledge, there is little work focusing on connotational meanings. This is because constructing commonsense knowledge including connotational meanings is challenging. In this paper, we present a Japanese knowledge base where arguments in event sentences are associated with various feature changes caused by the events. For example, \"my child\" in \"my wife hits my child\" is associated with some feature changes, such as increase in pain, increase in anger, increase in disgust, and decrease in joy. We constructed this knowledge base through crowdsourcing tasks by gathering feature changes of arguments in event sentences. After the construction of the knowledge base, we conducted an experiment in anaphora resolution using the knowledge base. We regarded anaphora resolution as an antecedent candidate ranking task and used Ranking SVM as the solver. Experimental results demonstrated the usefulness of our feature change knowledge base.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": 1,
    "year": 2018,
    "month": "May",
    "title": "{JFCKB}: {J}apanese Feature Change Knowledge Base"
}