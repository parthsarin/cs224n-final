{
    "article": "Automatic keyphrase extraction aims to pick out a set of terms as a representation of a document without manual assignment efforts. Supervised and unsupervised graph-based ranking methods have been studied for this task. However, previous methods usually computed importance scores of words under the assumption of single relation between words. In this work, we propose WordTopic-MultiRank as a new method for keyphrase extraction, based on the idea that words relate with each other via multiple relations. First we treat various latent topics in documents as heterogeneous relations between words and construct a multi-relational word network. Then, a novel ranking algorithm, named Biased-MultiRank, is applied to score the importance of words and topics simultaneously, as words and topics are considered to have mutual influence on each other. Experimental results on two different data sets show the outstanding performance and robustness of our proposed approach in automatic keyphrase extraction task. Introduction Keyphrases refer to the meaningful words and phrases that can precisely and compactly represent documents. Appropriate keyphrases help users a lot in better grasping and remembering key ideas of articles, as well as fast browsing and reading. Moreover, qualities of some information retrieval and natural language processing tasks have been improved with the help of document keyphrases, such as document indexing, categorizing, cluster- \u2021 Corresponding author. ing and summarizing (Gutwin et al., 1999; Krulwich and Burkey, 1996; Hammouda et al., 2005) . Usually, keyphrases are manually assigned by authors, which is time consuming. With the fast development of Internet, it becomes impractical to label them by human effort as articles on the Web increase exponentially. Therefore, automatic keyphrase extraction plays an important role in keyphrases assignment task. In most existing work, words are assumed under a single relation and then scored or judged within it. Considering the famous TextRank (Mihalcea and Tarau, 2004) , a term graph under a single relatedness was built first, then a graph-based ranking algorithm, such as PageRank (Page et al., 1999) , was used to determine the importance score for each term. Another compelling example is (Liu et al., 2010) , where words were scored under each topic separately. In this study, inspired by some multi-relational data mining techniques, such as (Ng et al., 2011) , we assume each topic as a single relation type and construct an intra-topic word network for each relation type. In other words, it is to map word relatedness within multiple topics to heterogeneous relations, meaning that words have interactions with others based on different topics. A multi-relational words example of our proposed WordTopic-MultiRank model is shown in Figure 1(a) . There are four words and three relations in this example, implying that there are three potential topics contained in the document. Further, we represent such multi-relational data in a tensor shape in Figure 1(b) , where each twodimensional plane represents an adjacency matrix for one type of topics. Then the heterogeneous network can be depicted as a tensor of size 4 \u00d7 4 \u00d7 3, where (i, j, k) entry is nonzero if the ith word is related to the jth word under kth topic. After that, we raise a novel measurement of word relatedness considering different topics, and then apply Biased-MultiRank algorithm to deal with multi-relational words for co-ranking purpose, based on the idea that words and topics have mutual influence on each other. More specifically, a word, connected with highly scored words via highly scored topics, should receive a high score itself, and similarly, a topic, connecting highly scored words, should get a high score as well. Experiments have been performed on two different data sets. One is a collection of scientific publication abstracts, while the other consists of news articles with human-annotated keyphrases. Experimental results demonstrate that our WordTopic-MultiRank method outperforms representative baseline approaches in specified evaluation metrics. And we have investigated how different parameter values influence the performance of our method. The rest of this paper is organized as follows. Section 2 introduces related work. In Section 3, details of constructing and applying WordTopic-MultiRank model are presented. Section 4 shows experiments and results on two different data sets. Finally, in Section 5, conclusion and future work are discussed . Related Work Existing methods for keyphrase extraction task can be divided into supervised and unsupervised approaches. The supervised methods mainly treat keyphrase extraction as a classification task, so a model needs to be trained before classifying whether a candidate phrase is a keyphrase or not. Turney (1999) firstly utilized a genetic algorithm with parameterized heuristic rules for keyphrase extraction, then Hulth (2003) added more linguistic knowledge as features to achieve better perfor-mance. Jiang et al. (2009) employed linear Ranking SVM, a learning to rank method, to extract keyphrase lately. However, supervised methods require a training set which would demand timeconsuming human-assigned work, making it impractical in the vast Internet space. In this work, we principally concentrate on unsupervised methods. Among those unsupervised approaches, clustering and graph-based ranking methods showed good performance in this task. Representative studies of clustering approaches are (Liu et al., 2009) and (Grineva et al., 2009) . Liu et al. (2009) made use of clustering methods to find exemplar terms and then selected terms from each cluster as keyphrases. Grineva et al. (2009) applied graph community detection techniques to partition the term graph into thematically cohesive groups and selected groups that contained key terms, discarding groups with unimportant terms. But as is widely known, one of the major difficulties in clustering is to predefine the cluster number which influences performance heavily. As for basic graph-based approaches, such as (Mihalcea and Tarau, 2004) and (Litvak and Last, 2008) , a graph based on word linkage or word similarity was first constructed, then a ranking algorithm was used to determine the importance score of each term. Wan et al. (2007) presented an idea of extracting summary and keywords simultaneously under the assumption that summary and keywords of the same document can be mutually boosted. Moreover, Wan and Xiao (2008a) used a small number of nearest neighbor documents for providing more knowledge to improve performance and similarly, Wan and Xiao (2008b) made use of multiple documents with a cluster context. Recently, topical information was under consideration to be combined with graphbased approaches. One of the outstanding studies was Topic-sensitive PageRank (Haveliwala, 2002) , which computed scores of web pages by incorporating topics of the context. As another representative, Topical PageRank (Liu et al., 2010) applied a Biased PageRank to assign an importance score to each term under every latent topic separately. To the best of our knowledge, previous graphbased researches are based on the assumption that all words exist under a unified relation, while in this work, we view latent topics within documents as word relations and words as multi-relational data, in order to make full use of word-word relatedness, word-topic interaction and inter-topic impacts. WordTopic-MultiRank Method In this section, we will introduce our proposed WordTopic-MultiRank method in details, including topic decomposition, word relatedness measurement, heterogeneous network construction and Biased-MultiRank algorithm. Topic Detection via Latent Dirichlet Allocation There are some existing methods to infer latent topics of words and documents. Latent Dirichlet Allocation (LDA) (Blei et al., 2003 ) is adopted in our work as it is more feasible for inference and it can reduce the risk of over-fitting. Firstly, we denote the learning corpus for LDA as C, and |C| represents the total number of documents in C. The i th document in the corpus is denoted as d i , in which i = 1, 2, \u2022 \u2022 \u2022 , |C|. Then, words are denoted as w ij where i indicates that word w ij appears in document d i and j refers to jth position in According to LDA, observed words in each document are supposed to be generated by a document-specific mixture of corpus-wide latent topics. More specifically, each word w ij in document d i is generated by first sampling a topic z k from d i 's document-topic multinomial distribution \u03b8 d i , and then sampling a word from z k 's topicword multinomial distribution \u03d5 z k . And each \u03b8 d i is generated by a conjugate Dirichlet prior with parameter \u03b1, while each \u03d5 z k is generated by a conjugate Dirichlet prior with parameter \u03b2. The full generative model for w ij is given by: d i (j = 1, 2, \u2022 \u2022 \u2022 , |d i |, |d i | is the to- tal word number in d i ). Further, topics inferred from |C| is z k , k = 1, 2, \u2022 \u2022 \u2022 , |T |, p(w ij |d i , \u03b1, \u03b2) = |T | \u2211 k=1 p(w ij |z k , \u03b2)p(z k |d i , \u03b1) (1) Using LDA, we finally obtain the documenttopic distribution, namely p(z k |d i ) for all the topics z k on each document d i , as well as the topicword distribution, namely p(w ij |z k ) for all the words w ij on each topic z k . In this work, we use GibbsLDA++ 1 , a C/C++ implementation of LDA using Gibbs Sampling, to detect latent topics. Measurement of Word Relatedness under Multi-relations Next, we apply Bayes' theorem to get word-topic distribution p(z k |w ij ) for every word in a given document d i : p(z k |w ij ) = p(w ij |z k , \u03b2)p(z k |d i , \u03b1) \u2211 |T | k=1 p(w ij |z k , \u03b2)p(z k |d i , \u03b1) (2) Therefore, we can obtain word relatedness as follows: p(w im |w in , z k ) = p(w im |z k )p(z k |w in ) (3) where m, n = 1, 2, Constructing a Heterogeneous Network on Words Like Figure 1 (a) shown in Introduction, now we construct a multi-relational network for words. In the same way mentioned by typical graph-based methods, for every document d i in corpus C, we treat every single word as a vertex and make use of word co-occurrences to construct a word graph as it indicates the cohesion relationship between words in the context of document d i . In this process, a sliding window with maximum W words is used upon the word sequences of documents. Those words appearing in the same window will have a link to each other under all the relations in the network. Further, we obtain the word relatedness under every topic from Formula (3), and use them as weights of edges for constructing the heterogeneous network. For instance, p(w im |w in , z k ) is regarded as the weight of the edge from w in to w im under kth relation if there is a co-occurrence relation between the two words in document d i . As (Hulth, 2003) pointed out, most manually assigned keyphrases were noun groups whose pattern was zero or more adjectives followed by one or more nouns. We only take adjectives and nouns into consideration while constructing networks in experiments. Ranking Algorithm In our proposed method, we employ Biased-MultiRank algorithm for co-ranking the importance of words and topics. It is obtained by adding prior knowledge of words and topics to Basic-MultiRank, a basic co-ranking scheme designed for objects and relations in multi-relational data. Therefore, we will demonstrate Basic-MultiRank first, then derive Biased-MultiRank algorithm from it. Basic-MultiRank Algorithm In this subsection, we take document d i into discussion for convenience. First, we call A = (a w im ,w in ,z k ) a real (2, 1)th order (|d i | \u00d7 |T |)- dimensional rectangular tensor, where a w im ,w in ,z k denotes p(w im |w in , z k ) obtained in last subsec- tion, in which m, n = 1, 2, \u2022 \u2022 \u2022 , |d i | and k = 1, 2, \u2022 \u2022 \u2022 , |T |. For example, Figure 1(b) is a (2, 1)th order (4\u00d73)-dimensional tensor representation of a document, in which there are 4 words and 3 topics. Then two transition probability tensors O = (o w im ,w in ,z k ) and R = (r w im ,w in ,z k ) are constructed with respect to words and topics by normalizing all the entries of A: o w im ,w in ,z k = a w im ,w in ,z k \u2211 |d i | m=1 a w im ,w in ,z k (4) r w im ,w in ,z k = a w im ,w in ,z k \u2211 |T | k=1 a w im ,w in ,z k (5) Here we deal with dangling node problem in the same way as PageRank (Page et al., 1999) . Namely, if a w im ,w in ,z k is equal to 0 for all words w im , which means that word w in had no link out to any other words via topic z k , we set o w im ,w in ,z k to be 1/|d i |. Likewise, if a w im ,w in ,z k is equal to 0 for all z k , which means that word w in had no link out to words w im via all topics, we set r w im ,w in ,z k to be 1/|T |. In this way, we ensure that 0 \u2264 o w im ,w in ,z k \u2264 1, |d i | \u2211 m=1 o w im ,w in ,z k = 1 0 \u2264 r w im ,w in ,z k \u2264 1, |T | \u2211 k=1 r w im ,w in ,z k = 1 Following the rule of Markov chain, we derive the probabilities like: P [Xt=wim]= |d i | \u2211 n=1 |T | \u2211 k=1 ow im ,w in ,z k \u00d7P [Xt\u22121=win,Yt=z k ] (6) P [Yt=z k ]= |d i | \u2211 m=1 |d i | \u2211 n=1 rw im ,w in ,z k \u00d7P [Xt=wim,Xt\u22121=win] (7) where subscript t denotes the iteration number. Notice that Formula ( 6 ) and ( 7 ) accord with our basic idea that, a word connected with high probability words via high probability relations, should have a high probability so that it will be visited more likely, and a topic connecting words with high probabilities, should also get a high one. After employing a product form of individual probability distributions, we decouple the two joint probability distributions in Formula ( 6 ) and ( 7 ) as follows: P [Xt\u22121=win,Yt=z k ]=P [Xt\u22121=win]P [Yt=z k ] (8) P [Xt=wim,Xt\u22121=win]=P [Xt=wim]P [Xt\u22121=win] (9) Considering stationary distributions of words and topics, while t goes infinity, the WordTopic-MultiRank values are given by: x=[x w i1 ,x w i2 ,\u2022\u2022\u2022,x w i|d i | ] T (10) y=[y z 1 ,y z 2 ,\u2022\u2022\u2022,y z | T | ] T (11) with x w im = lim t\u2192\u221e P [X t =w im ] (12) y z k = lim t\u2192\u221e P [Y t =z k ] (13) Under the assumptions from Formula (8) to (13), we can derive these from Formula (6) and (7): x w im = |d i | \u2211 n=1 |T | \u2211 k=1 o w im ,w in ,z k x w in y z k (14) y z k = |d i | \u2211 m=1 |d i | \u2211 n=1 r w im ,w in ,z k x w im x w in (15) which mean that the score of w im depends on its weighted-links with other words via all topics and the score of z k depends on scores of the words which it connects with. Now we are able to solve two tensor equations shown below to obtain the WordTopic-MultiRank values of words and relations according to tensor operations Formula ( 14 ) and (15): Oxy=x (16) Rx 2 =y (17) Ng et al. (2011) show the existence and uniqueness of stationary probability distributions x and y, then propose MultiRank, an iterative algorithm, to solve Formula ( 16 ) and ( 17 ) utilizing Formula ( 14 ) and ( 15 ). We refer it as Basic-MultiRank algorithm, shown as Algorithm 1, for the reason that it will be modified later in the following subsection. Algorithm 1 Basic-MultiRank algorithm Require: Tensor A , initial probability distributions x 0 and y 0 ( \u2211 |d i | m=1 [x 0 ] wm =1 and \u2211 |T | k=1 [y 0 ] z k =1) , Biased-MultiRank Algorithm Inspired by the idea of Biased PageRank (Liu et al., 2010) , we treat document-word distribution p(w ij |d i ), which can be computed from Formula (1), and document-topic distribution p(z k |d i ), acquired from topic decomposition, as prior knowledge for words and topics in each document d i . Therefore, we modify Formula ( 16 ) and ( 17 ) by adding prior knowledge to it as follows: (1\u2212\u03bb)Oxy+\u03bbx p =x (18) (1\u2212\u03b3)Rx 2 +\u03b3y p =y (19) where, xp=[p(wi1|di),p(wi2|di),\u2022\u2022\u2022,p(w i|d i | |di)] T and y p =[p(z1|di),p(z2|di),\u2022\u2022\u2022,p(z |T | |di)] T . Then we propose Biased-MultiRank, shown as Algorithm 2, as a new algorithm to solve the prior-tensors and Formula ( 18 ) and ( 19 ). Finally it is used in our WordTopic-MultiRank model. Algorithm 2 Biased-MultiRank algorithm Require: Tensor A, initial probability distributions x 0 and y 0 ( \u2211 |d i | m=1 [x 0 ] wm =1 and \u2211 |T | k=1 [y 0 ] z k =1), Experiment To evaluate the performance of WordTopic-MultiRank in automatic keyphrase extraction task, we utilize it on two different data sets and describe the experiments specifically in this section. Experiments on Scientific Abstracts Data Set We first employ WordTopic-MultiRank model to conduct experiments on a data set of scientific publication abstracts from the INSPEC database with corresponding manually assigned keyphrases 2 . The data set is also used by Hulth (2003) , Mihalcea and Tarau (2004) , Liu et al. (2009), and Liu et al. (2010) , meaning that it is classically used in the task of keyphrase extraction, and is convenient for comparison. Actually, this data set contains 2,000 abstracts of research articles and 19,254 manually annotated keyphrases, and is split into 1,000 for training, 500 for validation and 500 for testing. In this study, we use the 1,000 training documents as corpus C for topic detection and like other unsupervised ranking methods, 500 test documents are used for comparing the performance with baselines. Following previous work, only the manually assigned uncontrolled keyphrases that occur in the corresponding abstracts are viewed as standard answers. 2 It can be obtained from https://github.com/snkim/AutomaticKeyphraseExtraction Baselines and Evaluation Metrics We choose methods proposed by Hulth (2003) , Mihalcea and Tarau (2004) , Liu et al. (2009), and Liu et al. (2010) as baselines for the reason that they are either classical or outstanding in keyphrase extraction task. Evaluation metrics are precision, recall, F1measure shown as follows: P = T P T P +F P , R= T P T P +F N , F 1= 2P R P +R (20) where T P is the total number of correctly extracted keyphrases, F P is the number of incorrectly extracted keyphrases, and F N is the number of those keyphrases which are not extracted. Data Pre-processing and Configuration Documents are pre-processed by removing stop words and annotated with POS tags using Stanford Log-Linear Tagger 3 . Based on the research result of (Hulth, 2003) , only adjectives and nouns are used in constructing multi-relational words network for ranking, and keyphrases corresponding with following pattern are considered as candidates: (JJ) * (N N |N N S|N N P )+ in which, JJ indicates adjectives while NN, NNS and NNP represent various forms of nouns. At last, top-M keyphrases, which have highest sum scores of words contained in them, are extracted and compared with standard answers after stemming by Porter stemmer 4 . In experiments, we set \u03b1=1, \u03b2=0.01 for Formula (1) to (3) empirically, and \u03bb=0.5, \u03b3=0.9 for Formula (18), (19) indicated by (Li et al., 2012) . Influences of these parameters will not be discussed further in this work as they have been studied intensively in previous researches. Experimental Results In this subsection, we investigate how different parameter values influence performance of our proposed model first, then compare the best results obtained by baseline methods and our model. First of all, we inspect influences of topic number |T | on our model performance. At last, Table 4 shows the best results of baseline methods and our proposed model. In fac- Method Precision Recall F1 Hulth's (Hulth, 2003) 0.252 0.517 0.339 TextRank (Mihalcea and Tarau, 2004) 0.312 0.431 0.362 Topical PageRank (Liu et al., 2010) 0.354 0.183 0.242 Clustering (Liu et al., 2009) 0 (Wan and Xiao, 2008a) 0.288 0.354 0.317 CollaRank (Wan and Xiao, 2008b) 0.283 0.348 0.312 Topical PageRank (Liu et al., 2010) 0.282 0.348 0.312 WordTopic-MultiRank 0.296 0.399 0.340 Table 5 : Comparison on DUC2001 t, the best result of (Hulth, 2003) was obtained by adding POS tags as features for classification, while running PageRank on an undirected graph, which was built via using window W =2 on word sequence, resulted best of (Mihalcea and Tarau, 2004) . According to (Liu et al., 2009) , spectral clustering method got best performance in precision and F1-measure. On the other hand, Topical PageRank (Liu et al., 2010) performed best when setting window size W =10, topic number |T |=1,000. Since the influences of parameters have been discussed above, we set W =2, |T |=60 and M =10 as they result in best performance of our model on the same data set. Table 4 demonstrates that our proposed model outperforms all baselines in both precision and F1-measure. Noting that baseline methods are all under a single relation type assumption for word relatedness, estimations of their word ranking scores are limited, while WordTopic-MultiRank assumes words as multi-relational data and considers interactions between words and topics more comprehensively. Experiments on DUC2001 In order to show the generalization performance of our model, we also conduct experiments on another data set for automatic keyphrase extraction task and describe it in this subsection briefly. Following (Wan and Xiao, 2008a) , (Wan and Xiao, 2008b) and (Liu et al., 2010) , a data set annotated by Wan and Xiao 5 was used in this experiment for evaluation. This data set is the testing part of DUC2001 (Over and Yen, 2004) , con-taining 308 news articles with 2,488 keyphrases manually labeled. And at most 10 keyphrases were assigned to each document. Again, we choose precision, recall and F1-measure as evaluation metrics and use the train part of DUC2001 for topic detection. At last, keyphrases extracted by our WordTopic-MultiRank model will be compared with the ones occurring in corresponding articles after stemming. As indicated in (Wan and Xiao, 2008b) , performance on test set does not change much when cooccurrence window size W ranges from 5 to 20, and (Liu et al., 2010 ) also reports that it does not change much when topic number ranges from 50 to 1,500. Therefore, we pick co-occurrence window size W =10 and topic number |T |=60 to run WordTopic-MultiRank model. As for Keyphrase number M , we vary it from 1 to 20 to obtain different performances. Results are shown in Figure 2 . as M increases from 1 to 20, precision decreases from 0.528 to 0.201 in our experiment, while recall increases from 0.065 to 0.551. As for F1measure, it obtains maximum value 0.340 when M =10 and decreases gradually as M leaves 10 farther. Therefore, W =10, |T |=60 and M =10 are optimal for our proposed method on this test set. Table 5 lists the best performance comparison between our method and previous ones. All previous methods perform best on DUC2001 test set while setting co-occurrence window size W =10 and Keyphrase number M =10, which is consistent with our model. Experimental results on this data set demonstrate the effectiveness of our proposed model again as it outperforms baseline methods over all three metrics. Conclusion and Future Work In this study, we propose a new method named WordTopic-MultiRank for automatic keyphrase extraction task. It treats words in documents as objects and latent topics as relations, assuming words are under multiple relations. Based on the idea that words and topics have mutual influence on each other, our model ranks importance of words and topics simultaneously, then extracts highly scored phrases as keyphrases. In this way, it makes full use of word-word relatedness, word-topic interaction and inter-topic impacts. Experiments demonstrate that WordTopic-MultiRank achieves better performance than baseline methods on two different data sets. It also shows the good effectiveness and strong robustness of our method after we explored the influence of different parameter values. In future work, for one thing, we would like to investigate how different corpora influence our method and choose a large-scale and general corpus, such as Wikipedia, for experiments. For another, exploring more algorithms to deal with heterogeneous relation network may help to unearth more knowledge between words and topics, and improve our model performance. Acknowledgments This research is financially supported by NSFC Grant 61073082 and NSFC Grant 61272340.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article mentions financial support from NSFC Grant 61073082 and NSFC Grant 61272340. NSFC stands for the National Natural Science Foundation of China, which is a government-funded organization that provides grants for research. There is no mention of funding from defense, corporate, foundation sources, or an indication that there was no funding.",
    "abstract": "Automatic keyphrase extraction aims to pick out a set of terms as a representation of a document without manual assignment efforts. Supervised and unsupervised graph-based ranking methods have been studied for this task. However, previous methods usually computed importance scores of words under the assumption of single relation between words. In this work, we propose WordTopic-MultiRank as a new method for keyphrase extraction, based on the idea that words relate with each other via multiple relations. First we treat various latent topics in documents as heterogeneous relations between words and construct a multi-relational word network. Then, a novel ranking algorithm, named Biased-MultiRank, is applied to score the importance of words and topics simultaneously, as words and topics are considered to have mutual influence on each other. Experimental results on two different data sets show the outstanding performance and robustness of our proposed approach in automatic keyphrase extraction task.",
    "countries": [
        "China"
    ],
    "languages": [
        "Wan"
    ],
    "numcitedby": 8,
    "year": 2013,
    "month": "October",
    "title": "{W}ord{T}opic-{M}ulti{R}ank: A New Method for Automatic Keyphrase Extraction"
}