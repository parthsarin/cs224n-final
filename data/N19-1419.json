{
    "article": "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry. Introduction As pretrained deep models that build contextualized representations of language continue to provide gains on NLP benchmarks, understanding what they learn is increasingly important. To this end, probing methods are designed to evaluate the extent to which representations of language encode particular knowledge of interest, like part-ofspeech (Belinkov et al., 2017) , morphology (Peters et al., 2018a) , or sentence length (Adi et al., 2017) . Such methods work by specifying a probe (Conneau et al., 2018; Hupkes et al., 2018) , a supervised model for finding information in a representation. Of particular interest, both for linguistics and for building better models, is whether deep models' representations encode syntax (Linzen, 2018) . Despite recent work (Kuncoro et al., 2018; Peters et al., 2018b; Tenney et al., 2019) , open questions remain as to whether deep contextual models encode entire parse trees in their word representations. In this work, we propose a structural probe, a simple model which tests whether syntax trees are consistently embedded in a linear transformation of a neural network's word representation space. Tree structure is embedded if the transformed space has the property that squared L2 distance between two words' vectors corresponds to the number of edges between the words in the parse tree. To reconstruct edge directions, we hypothesize a linear transformation under which the squared L2 norm corresponds to the depth of the word in the parse tree. Our probe uses supervision to find the transformations under which these properties are best approximated for each model. If such transformations exist, they define inner products on the original space under which squared distances and norms encode syntax trees -even though the models being probed were never given trees as input or supervised to reconstruct them. This is a structural property of the word representation space, akin to vector offsets encoding word analogies (Mikolov et al., 2013) . Using our probe, we conduct a targeted case study, showing that ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) representations embed parse trees with high consistency in contrast to baselines, and in a low-rank space. 1  In summary, we contribute a simple structural probe for finding syntax in word representations ( \u00a72), and experiments providing insights into and examples of how a low-rank transformation recovers parse trees from ELMo and BERT representations ( \u00a73,4). Finally, we discuss our probe and limitations in the context of recent work ( \u00a75). Methods Our goal is to design a simple method for testing whether a neural network embeds each sentence's dependency parse tree in its contextual word representations -a structural hypothesis. Under a reasonable definition, to embed a graph is to learn a vector representation of each node such that geometry in the vector space-distances and normsapproximates geometry in the graph (Hamilton et al., 2017) . Intuitively, why do parse tree distances and depths matter to syntax? The distance metric-the path length between each pair of words-recovers the tree T simply by identifying that nodes u, v with distance d T (u, v) = 1 are neighbors. The node with greater norm-depth in the tree-is the child. Beyond this identity, the distance metric explains hierarchical behavior. For example, the ability to perform the classic hierarchy test of subject-verb number agreeement (Linzen et al., 2016) in the presence of \"attractors\" can be explained as the verb (V) being closer in the tree to its subject (S) than to any of the attactor nouns: S ... A1 ... A2 ... V ... . . . . Intuitively, if a neural network embeds parse trees, it likely will not use its entire representation space to do so, since it needs to encode many kinds of information. Our probe learns a linear transformation of a word representation space such that the transformed space embeds parse trees across all sentences. This can be interpreted as finding the part of the representation space that is used to encode syntax; equivalently, it is finding the distance on the original space that best fits the tree metrics. The structural probe In this section we provide a description of our proposed structural probe, first discussing the distance formulation. Let M be a model that takes in a sequence of n words w 1:n and produces a sequence of vector representations h 1:n , where identifies the sentence. Starting with the dot product, recall that we can define a family of inner products, h T Ah, parameterized by any positive semidefinite, symmetric matrix A \u2208 S m\u00d7m + . Equivalently, we can view this as specifying a linear transformation B \u2208 R k\u00d7m , such that A = B T B. The inner product is then (Bh) T (Bh), the norm of h once transformed by B. Every inner product corresponds to a distance metric. Thus, our family of squared distances is defined as: d B (h i , h j ) 2 = B(h i \u2212 h j ) T B(h i \u2212 h j ) (1) where i, j index the word in the sentence. 2 The parameters of our probe are exactly the matrix B, which we train to recreate the tree distance between all pairs of words (w i , w j ) in all sentences T in the training set of a parsed corpus. Specifically, we approximate through gradient descent: min B 1 |s | 2 i,j d T (w i , w j ) \u2212 d B (h i , h j ) 2 where |s | is the length of the sentence; we normalize by the square since each sentence has |s | 2 word pairs. Properties of the structural probe Because our structural probe defines a valid distance metric, we get a few nice properties for free. The simplest is that distances are guaranteed nonnegative and symmetric, which fits our probing task. Perhaps most importantly, the probe tests the concrete claim that there exists an inner product on the representation space whose squared distancea global property of the space-encodes syntax tree distance. This means that the model not only encodes which word is governed by which other word, but each word's proximity to every other word in the syntax tree. 3 This is a claim about the structure of the representation space, akin to the claim that analogies are encoded as vector-offsets in uncontextualized word embeddings (Mikolov et al., 2013) . One benefit of this is the ability to query the nature of this structure: for example, the dimensionality of the transformed space ( \u00a7 4.1). Tree depth structural probes The second tree property we consider is the parse depth w i of a word w i , defined as the number of edges in the parse tree between w i and the root of the tree. This property is naturally represented as a norm -it imposes a total order on the words in the sentence. We wish to probe to see if there exists a squared norm on the word representation   space that encodes this tree norm. We replace the vector distance function d B (h i , h j ) with the squared vector norm h i 2 B , replacing Equation 1 with h i A = (Bh i ) T (Bh i ) and training B to recreate w i . Like the distance probe, this norm formulation makes a concrete claim about the structure of the vector space. Experiments Using our probe, we evaluate whether representations from ELMo and BERT, two popular English models pre-trained on language modeling-like objectives, embed parse trees according to our structural hypothesis. Unless otherwise specified, we permit the linear transformation B to be potentially full-rank (i.e., B is square.) Later, we explore what rank of transformation is actually necessary for encoding syntax ( \u00a7 4.1). Representation models We use the 5.5B-word pre-trained ELMo weights for all ELMo representations, and both BERT-base (cased) and BERT-large (cased) . The representations we evaluate are denoted ELMOK, BERTBASEK, BERTLARGEK, where K indexes the hidden layer of the corresponding model. All ELMo and BERT-large layers are dimensionality 1024; BERT-base layers are dimensionality 768. Data We probe models for their ability to capture the Stanford Dependencies formalism (de Marneffe et al., 2006) , claiming that capturing most aspects of the formalism implies an understanding of English syntactic structure. To this end, we obtain fixed word representations for sentences of the parsing train/dev/test splits of the Penn Treebank (Marcus et al., 1993) , with no pre-processing. 4   Baselines Our baselines should encode features useful for training a parser, but not be capable of parsing themselves, to provide points of comparison against ELMo and BERT. They are as follows: LINEAR : The tree resulting from the assumption that English parse trees form a left-to-right chain. A model that encodes the positions of words should be able to meet this baseline. ELMO0 : Strong character-level word embeddings with no contextual information. As these representations lack even position information, we should be completely unable to find syntax trees embedded. DECAY0 : Assigns each word a weighted average of all ELMO0 embeddings in the sentence. The weight assigned to each word decays exponentially as 1 2 d , where d is the linear distance between the words. PROJ0 : Contextualizes the ELMO0 embeddings with a randomly initialized BiLSTM layer of dimensionality identical to ELMo (1024), a surprisingly strong baseline for contextualization (Conneau et al., 2018) . Tree distance evaluation metrics We evaluate models on how well the predicted distances between all pairs of words reconstruct gold parse trees and correlate with the parse trees' distance metrics. To evaluate tree reconstruction, we take each test sentence's predicted parse tree distances and compute the minimum spanning tree. We evaluate the predicted tree on undirected BERTlarge16 The complex financing plan in the S+L bailout law includes raising $ 30 billion from debt issued by the newly created RTC . ELMo1 The complex financing plan in the S+L bailout law includes raising $ 30 billion from debt issued by the newly created RTC . Proj0 The complex financing plan in the S+L bailout law includes raising $ 30 billion from debt issued by the newly created RTC .  attachment score (UUAS)-the percent of undirected edges placed correctly-against the gold tree. For distance correlation, we compute the Spearman correlation between true and predicted distances for each word in each sentence. We average these correlations between all sentences of a fixed length, and report the macro average across sentence lengths 5-50 as the \"distance Spearman (DSpr.)\" metric. 5 Tree depth evaluation metrics We evaluate models on their ability to recreate the order of words specified by their depth in the parse tree. We report the Spearman correlation betwen the true depth ordering and the predicted ordering, averaging first between sentences of the same length, and then across sentence lengths 5-50, as the \"norm Spearman (NSpr.)\". We also evaluate models' ability to identify the root of the sentence as the least deep, as the \"root%\". 6 Results We report the results of parse distance probes and parse depth probes in Table 1 . We first confirm that our probe can't simply \"learn to parse\" on top of any informative representation, unlike parserbased probes (Peters et al., 2018b) . In particular, ELMO0 and DECAY0 fail to substantially outperform a right-branching-tree oracle that encodes the linear sequence of words. PROJ0, which has all of the representational capacity of ELMO1 but none of the training, performs the best among the baselines. Upon inspection, we found that our probe on PROJ0 improves over the linear hypothesis with mostly simple deviations from linearity, as visualized in Figure 2 . We find surprisingly robust syntax embedded in each of ELMo and BERT according to our probes. Figure 2 shows the surprising extent to which a minimum spanning tree on predicted distances recovers the dependency parse structure in both ELMo and BERT. As we note however, the distance metric itself is a global notion; all pairs of words are trained to know their distance -not just which word is their head; Figure 4 demonstrates the rich structure of the true parse distance metric recovered by the predicted distances. Figure 3 demonstrates the surprising extent to which the depth in the tree is encoded by vector norm after the probe transformation. Between models, we find consistently that BERTLARGE performs better than BERTBASE, which performs better than ELMO. 7 We also find, as in Peters et al.   of probes trained on each layer of each system. Analysis of linear transformation rank With the result that there exists syntax-encoding vector structure in both ELMo and BERT, it is natural to ask how compactly syntactic information is encoded in the vector space. We find that in both models, the effective rank of linear transformation required is surprisingly low. We train structural probes of varying k, that is, specifying a matrix B \u2208 R k\u00d7m such that the transformed vector Bh is in R k . As shown in Figure 5 , increasing k beyond 64 or 128 leads to no further gains in parsing accuracy. Intuitively, larger k means a more expressive probing model, and a larger fraction of the representational capacity of the model being devoted to syntax. We also note with curiosity that the three models we consider all seem to require transformations of approximately the same rank; we leave exploration of this to exciting future work. Discussion & Conclusion Recent work has analyzed model behavior to determine if a model understands hierarchy and other linguistic phenomena (Linzen, 2018; Gulordava et al., 2018; Kuncoro et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018; Tang et al., 2018; Futrell et al., 2018) . Our work extends the literature on linguistic probes, found at least in (Peters et al., 2018b; Belinkov et al., 2017; Blevins et al., 2018; Hupkes et al., 2018) . Conneau et al. (2018) present a task similar to our parse depth prediction, where a sentence representation vector is asked to classify the maximum parse depth ever achieved in the sentence. Tenney et al. ( 2019 ) evaluates a complementary task to ours, training probes to learn the labels on structures when the gold structures themselves are given. Peters et al. (2018b) evaluates the extent to which constituency trees can be extracted from hidden states, but uses a probe of considerable complexity, making less concrete hypotheses about how the information is encoded. Probing tasks and limitations Our reviewers rightfully noted that one might just probe for headedness, as in a bilinear graph-based dependency parser. More broadly, a deep neural network probe of some kind is almost certain to achieve higher parsing accuracies than our method. Our task and probe construction are designed not to test for some notion of syntactic knowledge broadly construed, but instead for an extremely strict notion where all pairs of words know their syntactic distance, and this information is a global structural property of the vector space. However, this study is limited to testing that hypothesis, and we foresee future probing tasks which make other tradeoffs between probe complexity, probe task, and hypotheses tested. In summary, through our structural probes we demonstrate that the structure of syntax trees emerges through properly defined distances and norms on two deep models' word representation spaces. Beyond this actionable insight, we suggest our probe may be useful for testing the existence of different types of graph structures on any neural representation of language, an exciting avenue for future work. Acknowledgements We would like to acknowledge Urvashi Khandelwal and Tatsunori B. Hashimoto for formative advice in early stages, Abigail See, Kevin Clark, Siva Reddy, Drew A. Hudson, and Roma Patel for helpful comments on drafts, and Percy Liang, for guidance on rank experiments. We would also like to thank the reviewers, whose helpful comments led to increased clarity and extra experiments. This research was supported by a gift from Tencent. A Appendix: Implementation Details A.1 Squared L2 distance vs. L2 distance In Section 2.2, we note that while our distance probe specifies a distance metric, we recreate it with a squared vector distance; likewise, while our norm probe specifies a norm, we recreate it with a squared vector norm. We found this to be important for recreating the exact parse tree distances and norms. This does mean that in order to recreate the exact scalar values of the parse tree structures, we need to use the squared vector quantities. This may be problematic, since for example squared distance doesn't obey the triangle inequality, whereas a valid distance metric does. However, we note that in terms of the graph structures encoded, distance and squared distance are identical. After training with the squared vector distance, we can square-root the predicted quantities to achieve a distance metric. The relative ordering between all pairs of words will be unchanged; the same tree is encoded either way, and none of our quantitative metrics will change; however, the exact scalar distances will differ from the true tree distances. This raises a question for future work as to why squared distance works better than distance, and beyond that, what function of the L2 distance (or perhaps, what L p distance) would best encode tree distances. It is possibly related to the gradients of the loss with respect to the function of the distance, as well as how amenable the function is to matching the exact scalar values of the tree distances. A.2 Probe training details All probes are trained to minimize L1 loss of the predicted squared distance or squared norm w.r.t. the true distance or norm. Optimization is performed using the Adam optimizer (Kingma and Ba, 2014) initialized at learning rate 0.001, with \u03b2 1 = .9, \u03b2 2 = .999, = 10 \u22128 . Probes are trained to convergence, up to 40 epochs, with a batch size of 20. For depth probes, loss is summed over all predictions in a sentence, normalized by the length of the sentence, and then summed over all sentences in a batch before a gradient step is taken. For distance probes, normalization is performed by the square of the length of the sentence. At each epoch, dev loss is computed; if the dev loss does not achieve a new minimum, the optimizer is reset (no momentum terms are kept) with an initial learning rate multiplied by 0.1. All models were implemented in both DyNet (Neubig et al., 2017) , and in PyTorch (Paszke et al., 2017) . B Appendix: Extra examples In this section we provide additional examples of model behavior, including baseline model behavior, across parse distance prediction and parse depth prediction. In Figure 6 and Figure 7 , we present a single sentence with dependency trees as extracted from many of our models and baselines. In Figure 8 , we present tree depth predictions on a complex sentence from ELMO1, BERTLARGE16, and our baseline PROJ0. Finally, in Figure 9 , we present gold parse distances and predicted squared parse distances between all pairs of words in large, high-resolution format. ELMo0 But the RTC also requires \" working \" capital to maintain the bad assets of thrifts that are sold , until the assets can be sold separately . Decay0 But the RTC also requires \" working \" capital to maintain the bad assets of thrifts that are sold , until the assets can be sold separately . Proj0 But the RTC also requires \" working \" capital to maintain the bad assets of thrifts that are sold , until the assets can be sold separately . ELMo1 But the RTC also requires \" working \" capital to maintain the bad assets of thrifts that are sold , until the assets can be sold separately . BERTbase7 But the RTC also requires \" working \" capital to maintain the bad assets of thrifts that are sold , until the assets can be sold separately . BERTlarge16 But the RTC also requires \" working \" capital to maintain the bad assets of thrifts that are sold , until the assets can be sold separately .",
    "abstract": "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "636",
    "year": "2019",
    "month": "June",
    "title": "{A} Structural Probe for Finding Syntax in Word Representations"
}