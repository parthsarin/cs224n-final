{
    "article": "In this paper we outline the advantages of deploying a shallow parser based on Supertagging in an automatic dialogue system in a call center that basically leaves the initiative with the user as far as (s)he wants (in the literature called userinitiative or adaptive in contrast to system-initiative dialogue systems ). The Supertagger relies on a Hidden Markov model and is trained with Gennan input texts. The entire design of a Hidden Markov-based Supertagger with trigrams builds the central issue of this paper. The evaluation of our German Supertagger lags behind the English one. Some of the reasons will be addressed later on. Nevertheless shallow parsing with the Supertags increases the accuracy compared to a basic version ofKoHDaS that only relies on recurrent plausibility networks. 1. The acronym KoHDaS stands for Koblenzer Help Desk with automatic Speech recognition. In the following the basic version is called KoHDaS-NN (NN stands for Neural Networks). Later on we investigate KoHDaS-ST where ST stands for SuperTagging. Introduction Wizard--0f-Oz experiments show that users of automatic dialogue systems would preferentially take the initiative in many dialogues instead ofbeing asked a long list oftiny little questions by the system (cf. (Boje et al\" 1999) ). Empirical evaluations demonstrate that adaptation to the user's dialogue preference leads to significantly higher user satisfaction and task success (cf. (Strachan et al\" 1997) or (Litman, Pan and Walker, 1998) ). In contrast to these results, it can also be observed that in such user-initiated dialogue systems the user is sometimes left without a clear understanding ofhis/her options at a given point in the dialpgue. This can cause frustration or even breakdowns of the communication. Consequently, an adaptive system wlrich reacts to the user's preferred mode, i.e. is able to ask explicit questions when the user doesn't take the initiative and to react to user-provided complex tums adequately as weil at any particular state of the dialogue, serves as a user-friendly dialogue system. The criticised strict dialogue structure with an explicit and inevitable initiative by the system (henceforth called system-initiative in contrast to user-initiative) results from the crucial fact that with any of these questions by the system a particular sub-grammar and sub-lexicon of the speech analysis system (e.g. a simple number or yes/no grammar and Iexicon, respectively) can be associated to analyse the user's answer more reliably. Clarification dialogues caused by incorrectly analysed words can be circumvented by this method. Hence it is essential for a user-initiative or adaptive (or also called mixed-initiative) system to remedy the shortcomings resulting from the less reliable analysis of the user's spoken turn with a general grammar and lexicon, respectively. Furthermore, the task parameters, i.e. the infonnation provided in the user's turn to petform the user-intended task by the system, have to be extracted without knowing exactly where in the user's turn or whether at all they have been uttered yet. In the case that not all task parameters are provided even a user-initiative system has to ask questions -similar to a system-initiative system. KoHDaS-NN 1 is an automatic help desk system in a call center that basically leaves the dialogue initiative with the user as far as (s)he wants. The user's turn circumscribing the problem as a whole is handed to a hierarchy of recurrent plausibility networks which classify the according problem. In the next step the system extracts even only implicitly mentioned task parameters of this problem class from the turn by a graph-matclring technique. Remaining or unidentified task parameters required to solve the problem are asked by the system in an ordinary question-answering manner. The results of KoHDaS-NN where the classification and the extraction of the task parameters is performed only on the basis of simple words are promising. However the number ofwrong classifications and questions for yet uttered task parameters has to be further decreased in order to provide a user-friendly dialogue with the customers. Hence we investigate in the following whether deploying a shallow parser based on Supertagging increases the performance of the system -both with respect to the classification and the extraction of the task parameters. The Supertagger in KoHDaS-ST relies on a Hidden Markov model and is trained with German input texts. The main section ofthis paper is devoted to the description ofthis method (cf. Section 3; it also comprises some implementation details to gain efficiency). With respect to accuracy our Gennan Supertagger lags behind the English one. Some of the reasons will be addressed later on. Nevertheless shallow parsing with the Supertags increases the accuracy compared to a basic version ofKoHDaS that only relies on recurrent plausibility networks. Tue paper is organized as follows. In the next Section KoHDaS and its functionality is described. In Section 3 the Supertagger based on Hidden Markov models is outlined. Section 4 is devoted to the description ofKoHDaS-ST, i.e. how Supertagging used in shallow parsing is integrated into the application of KoHDaS. Furthennore, it depicts the results ofKoHDaS-ST compared to KoHDaS-NN. In Section 5 related work is portrayed. Here further methods that favorably compare to Supertagging are outlined on the one band. On the other, different Supertagging methods and their application domains are delineated. The paper ends addressing future work and open problems. 2. KoHDaS -an adaptive dialogue system for First-Level Support via telephon KoHDaS ( see, e.g., (Harbusch, Knapp and Laumann, 2001 ) ) explores methods which provide automatic userinitiative dialogues in call centers, i.e. the initiative should basically be left witb the user as far as (s)he wants. Compared to system-initiative dialogue systems, user-initiative systems cannot rely on restricted dictionaries and grammars during the speech recognition process to gain more reliable results. Hence, methods to remedy the reduction of understanding in the first phase have tobe impinged on the system. KoHDaS-NN deploys the following two techniques towards a natural dialogue behaviour. First, a hierarchy of neural networks classifies the user's whole turn ( on average 25 words) according to a list of problem classes. After a confinnation dialogue the task parameters mentioned in the user's turn are extracted to avoid redundant questions by the system for infonnation to perfonn the task the user wants the system to perform -in our case a dai:a base look-up for a solution of the user's hardware prob lern. As for classification, a hierarchy of recurrent plausibility networks (cf. (Wermter, 1995) ) accomplishes the consideration of arbitrary previous contexts in current decision making in KoHDaS. After a confi.rmation that the problem class is correctly recognized, the task parameters, i.e. the necessary information to enable the system to perfonn the task the user intended the system to do, are extracted from the user's initial turn by a graph matching technique imposed on the dialogue graphs, i.e. the specification of all possibly asked questions by the system and the according task parameters provided by the user (note that these graphs have to be specified anyhow to model a user who does not take the initiative at all). KoHDaS is currently customized to the domain offirst level support for computer hardware but it can easily be adapted to new domains. A drawback of KoIIDaS-NN is the absence of syntactic infonnation in the processing of the user's turn as KoHDaS-NN works simply word-based. With respect to the classification task, a word such as \"monitor\" remains active by the context-layer independent whether the word in mentioned in a subordinate clause or as a key concept to characterize the problem (cf. \"My new monitor fiickers since 1 have .\"\" vs. \"My SCSI hard drive which I bought together with my new monitor two weeks ago from your customer's service cannot be fonnatted ... \"). With respect to the extraction of task parameters negations are completely ignored in KoHDaS-NN. For instance, \"My monitor fiickers although the boxes are not activated\" takes for granted that the monitor only occasionally ftickers when the boxes are activated. Hence the wrong conclusion is drawn from the customer' s utterance. In order to remedy this shortcoming, KoIIDaS-ST deploys a robust syntactic analysis based on a Supertagger (Joshi and Srinivas, 1994) and a Lightweight Dependency Analyzer (LDA) (Srinivas, l 997a) in the analysis of the user's turns. Particularly for the correct interpretation of the scope of negations we have decided to use Supertagging instead of chunking (cf. Section 5). The structural information -which possibly remains partial -provided by this analysis is used in the classification step as weil as in the infonnation extraction step ofKoHDaS. The use of structural information in classification can prevent words occurring in deeply nested sub-sentences from having high impact on the determination of the problem class. In the infonnation extraction step, the use of structural information allows to detect dependencies between structures in the turn, that can't be detected in the word-based version ofKoHDaS-NN (e.g., negations; cf. Section 4). Hidden Markov model-based Supertagging Our German Supertagger uses a trigram model and is based on Hidden Markov models (HMMs) enabling the use of the weil known algorithms on HMMs (see, e.g\" (Rabiner, 1989) ) to guess the most likely syntactic structure of a sentence. We use a trigram model as it has shown to achieve good results in Supertagging (cf. (Srinivas, 1997a) ). In this section the basic concepts of Hidden-Markov models are briefly introduced. Thereafter a Hidden Markov model-based Supertagger is portrayed. Finally, aspects of the implementation -particularly with respect to time and space efficiency -are highlighted. Basic concepts ofHidden Markov models Let us assume the following notational conventions (adapted from (Rabiner and Juang, 1986) and (Charniak, 1993) or see, e.g., (Rabiner, 1989) for a good introduction): \u2022 T = length ofthe sequence of observations (training set), \u2022 N = number of states ( either known or guessed), \u2022 M = number ofpossible observations (from the training set), \u2022 nx = {q 1 , \".qN} (finite set ofpossible states), \u2022 flo = {vi, \"., VM} (finite Set ofpossible Observations), \u2022 Xt random variable denoting the state at timet (state variable), \u2022 Ot random variable denoting the observation at time t ( output variable), \u2022 r; = o 1 , .\", OT (sequence ofactual observations) and distributional parameters: \u2022 A = {%} with aij = Pr(Xt+l = qjJXt = qi) (transitionprobabilities), \u2022 B = {bi} with bi(k) = Pr(Ot = vklXt = qit) (observationprobabilities), \u2022 11\" = {7ri} with 7l\"i = Pr(Xo = qi) (initial state distribution). AHiddenMarkov model (HMM) is a five-tuple Olx, no, A,B, n). Let>.= {A, B, 7r} denote the parameters for a given HMM with fixed n X and n o . This means, a discrete-time, discrete-space dynamical System governed by a Markov chain emits a sequence of observable outputs: one output ( observation) for each state in a trajectory of such states. From the observable sequence of outputs, the most likely dynamical system can be inferred. The result is a model for the underlying process. Altematively, given a sequence of outputs, the most likely sequence of states can be inferred. The model can also be used to predict the next observation or more generally a continuation of the sequence of observations. Three basic problems can be fonnulated for HMMs: 1. Find Pr(O'\\A), i.e. the probability ofthe observations given the model. 2. Find the most Iikely state trajectory given the model and observations. Adjust A = {A, B, 7r} to maximize Pr(O'J>.). For any of these questions efficient algorithms are known (see, e.g., (Rabiner, 1989) ). The Forward-Backward algorithm (Baum and Eagon, 1967) solves the first problem, problem 2 is yielded by the Viterbi algorithm (Viterbi,  1967) und problem 3 can be dealt with by the Baum-Welch algorithm (Baum, 1972) . Hidden Markov models for Supertagging Many variants of Supertagging use models similar to POS-Tagging (cf. Section 5 for a brief description of the variants Trigram Supertagging, Head Supertagging and Transformation-based Supertagging). Here, we underlay the Supertagger with Hidden Markov models (HMMs) . In this framework, the Supertags are encoded as states and the words as symbols of the output alphabet of the HMM. Assuming a bigram model (i.e, n-Gram with n = 2), the realization is easy and straightforward. Any Supertag becomes an individual state and any terminal an individual output symbol. The according Tagger can be trained with the Baum-Welch algorithm. The observation sequence is provided by the sentences of the training set (unsupervised learning). However, this method Jacks behind supervised learning methods (see, e.g\" (Merialdo, 1994) ). Such a corpus can be gained with the Viterbi algorithm (cf. problem 2 mentioned above), as this algorithm denotes the optimal sequence of states for a given observation sequence -in our case the optimal sequence of Supertags. The Supertagger we report on in this paper uses a trigram model (cf. (B\u00e4cker, 200 l) for an evaluation of the HMM-based Supertagger using bigrarns ). According to the trigram model, two previous states (i.e. Supertags) are encoded in the HMM in a well-known manner (see, e.g., (El-Beze and Merialdo, 1999)): \u2022 The states of the HMM: correspond to pairs of Supertags ( ti-1, ti). \u2022 The transition probability Pr[(ti-1, ti)l(ti_z, ti-1)] is denoted by the trigram probability Pr(tilti-zti_i). \u2022 The output symbols are provided by the words which are tagged with ti and which are emitted in states (_, ti )-At the beginning of a sentence pseudo states (0, tj) with 0 a pseudo category are assumed. In general, the Baum-Welch algorithm (cf. problem 3) can be applied to optimize the model parameters in order to maximize Pr(training setl.\\). Our results are gained on the basis of a labeled corpus. Hence we don't impose the Baum-Welch algorithm on our Supertagger. On the basis ofthe Iabeled corpus we directly estimate the model pararneters according to the Maximum Likelihood Estimation (MLE) method. For trigrams this means: with: (tk,tj,ti)   PrMLEtitk,tj = c(tk,tj), l:Si:SN, l:::;j:::;N, 1::::; j:::; N, = nurnber of occurrences of tj in the training set, \u2022 nurnber of occurrences oftj followed by tk, = nurnber of occurrences of tk followed by tj, which itself is followed by t\" and number of occurences of wk labelled as t j. c(tj) c(tj,tk) c(tk, tj, ti) c(wk , tj) ( ! ) c (1) (2) In order to overcome problems with sparse data, i.e. not all trigrams occur in the training set, smoothing techniques (for a good introduction see, e.g., (Jurafsky and Martin, 2000) or (Manning and Sch\u00fctze, 2000) ; in (Chen and Goodman, 1996) the perforrnance of various smoothing techniques are evaluated) are applied in our system. Furtherrnore the treatment ofunknown words is described in the following. In our system we ernploy Good-Turing Discounting (Good, 1953) . This means, the equation (1) ofthe :MLE estimation which relies on the absolute frequency c(tk, t i, ti ) ofa trigram is replaced by the foUowing equation: (3) (5) Here the constant k denotes a threshold to prevent the system from re-estimating rather accurate results ( according to high frequency). The discounting method can be further improved by a method that differentiates between unseen trigrams. The Backojf method (Katz, 1987) uses the frequencies of (n -1)-grams in the following manner: if c(ti -2ti-1ti) > 0, if c(ti-2ti-1ti ) = 0 and c(ti-lti ) > 0, otherwise. (6) If the frequency of a trigram (bigram, resp.) is zero, the frequency of the bigram (unigram, resp.) is considered. However, a factor 0:1 (0:2, resp.) is supposed to normalize the resulting probability. This means, for any given tn the following holds: l:Pr(tnltitj) = 1. (7) i,j In formula (6) P denotes the probability which results forma discounting method (otherwise the values don'tfulfil equation ( 7 )). We use again the Good-Turing discounting method here. The according formula looks as follows: ifc(ti-2ti-lti) > 0, ifc(ti-2ti-1ti) = 0 and c(ti-lti) > 0, otherwise. (8) Unknown words are treated in our system in the following manner. The probability Pr( Wk Jtj) is computed by the Backoffmethod. In case Wk is an unknown word we adapt the method by (Weischedel et al., 1993) which deals withfeatures ofwords. The prefixes and suffixes ofwords are considered to estimate the probabilities according to the following fonnula: (9) The probability of the occurrence of an unknown word Pr( unknown it 3 ) for the currently considered Supertag is estimated according to: .c (10) where N1 ( tj) is the number of words which occur in the training set exactly once with the Supertag t 3 ; Pr(f eaturesltj) denotes the probability whether a word with the same prefix or suffix as wk, respectively, occurs together with the Supertag t 3 . Now we face the task of tagging itself. The tagging is performed by the Viterbi algorithm. For a given observation sequence 0 = { 01, 02, ... , Or} the most likely sequence of states Q :: { q 1 , q 2 , ... , qT} is computed in four steps (Initialization, Recursion, Tennination and Reconstruction (Path Backtracking); the time complexity of the Viterbi algorithm ist O(N 2 n) where n is the length of the input). Fora Gennan test set of 30 sentences, 78.3% of the words were assigned the correct Supertag. In the conclusions we compare this result with English Supertagging. In general the above described ffivIM-bases Supertagger was tested with a German corpus of 250 tagged sentences (cf. the evaluation in Section 4). The German training and test corpus has been constructed in the following manner. Basically we looked at written Gennan dialogues in news groups in the area of first level support for computer hardware. We developed an LTAG with 127 elementary trees covering the domain of the KoHDaS system (cf. (B\u00e4cker, 2002) ) and automatically parsed these dialogues. The reviewed results of all parses constitute the tagged corpus. We trained our Supertagger using 250 tagged sentences. For the estimation of the HMM's model parameters we used Good--Turing discounting combined with Katz's Backoffmodel to smooth the parameters resulting in better estimations ofunseen trigrams. We use word features similar to (Weischedel et al.,   1993) ( i.e. prefixes and suffixes of words) to estimate the observation probability of unknown words. Implementation of the HMM-based Supertagger The overall system is implemented in Java. In this paragraph we highlight some implementational details which reduce space and time complexity of our system (cf. (Cutting et al. , 1992) for a discussion of efficiency matters for POS-Taggers). Let us first beer in mind which complexity a HMM comes along with. The model parameters of a HMM consist of N states and M output symbols from a N x N matrix A of transition probabilities, a N x M matrix B of obseivation probabilities and a N-dimensional vector 7r (initial state distribution). All these parameters have to be yielded, i.e. the space complexity is O(N 2 +MN). The states of our HMM comprise pairs of Supertags. Hence the number of states equals the square of the number of Supertags T. Consequently the space complexity is O(T 4 + MT 2 ), und the run time of the Viterbi algoritlun is O{T 4 n). From this fact directly follows that the model parameters cannot be represented by a twodimensional array (for the 127 Supertags in our system, the two-dimensional array of 64-bit digits for the transition probabilities requires 2 GB space). As a consequence, all model parameters are stored in an associative manner in our system 2 \u2022 \u2022 A reasonable space reduction results from only storing probabilities greater than zero 3 \u2022 With respect to the transition probabilities the following holds. These probabilities are computed during the training phase where they don't become smoothed. Smoothing is performed during tagging. During that process the trigram, bigram and unigram models are detennined. Furthennore, the factors of the Backoff method are computed. A smoothed probability is only computed on demand (getA ( (ti-2 , ti-1)). Consequently the overall space complexity depends on the actually deployed training set (unknown trigrams are not stored). With respect to the run time the following improvements can be perfonned to gain more efficiency in Supertagging. The Viterbi algorithm iterates over all words and all states in the following manner: for each word Wi in sentence { for each s tate m { for each state n { } } } Shortcuts for states with an observation probability zero and unique POS can reduce the run time reasonably. The associative hash tables allow to access all statcs of the currently considered word occurring in the training phase. These sets computed for the current word and its predecessor build the basis to collect the set of relevant state.s of the current word (Backoff method for the observation probabilities). Only for these states the iteration needs tobe perfonned instead ofthe nested iteration over all states. More fonnally speaking: relevantStates(i,j) = {(tk, tz) 1Pr(wiltk)>0 /\\ Pr(w; itz) > O} is supposed to be regarded in the two nested loops mentioned above. For i < 0 and j < 0, respectively, Wi and Wj, respectively, denote the pseudo words at the beginning of a sentence. Assuming only relevant states decreases the average run time reasonably. Our Supertagger requires approximately 28 ms for the tagging of a sentence only conducting relevant states whereas it runs at least a second if all states are considered. Application of Supertagging in the user-initiative Help Desk system KoHDaS The results of the Supertagger described in the previous section allow a LDA (Srinivas, 1997a) to discover dependencies between the Supertags in the user's turn. The dependency structure accomplished by this method is used in classification and in \uff69 \uff6e \uff66 \uff6f \uff72 \uff6d \uff7e \uff74 \uff69 \uff6f \uff6e \uff20 extraction in the following manner. In KoHDaS-NN, the user's turn is classified according to significance vectors ofthe form: ( ) occurrences of a word from w within class Ci vw,<;=n E occurrence of words from w within class c; j=l 2. The associative storing in Java is realized by the class HashMap, which provides a Hash table (see, e.g\" (Flanagan, 2002) ). 3. lt is important to notice here that due to the fact that the real-<iigit arithmetics cannot differentiate between zero an very small values (as holds for the products ofprobabilities computed in the Viterbi algorithrn) we deal with the logarithrns ofthe probabilities in the hash table of the probabilities of Supertags. This states a suitable rnethod here because not probabilities themselves but the arguments of such probabilities are maximized, i.e. the Viterbi algorithm computes the maximum sum of the logarithms of the probabilities instead of the maximum product of the probabilities. where only 616 words are actually regarded and matched with a reduced vocabulary with 131 word groups Ci, i.e. general concepts in our domain (such as 'hard disk', 'monitor' and 'capacity') containing words, which can be considered tobe synonymous (e.g., words in class 'hard disk' are 'disk' or 'harddrive'). Generally, significance vectors account forthe importance ofthe word in a specific problem class. In KoHDaS-ST, these significance vectors are adjusted using the results of the structural information collected by the Supertagger and the LDA. The adjusted significance vectors v* are computed by: { adv (w,c;) v*(w,Ci)::::: (l+\u00dfd)*0.1 (1 + \u00dfd) v(w, Ci) if v( w, Ci) > \uff7e \uff20 2:7=1 Cj, ifv(w, c,;):::::: 0 and d > 0, otherwise. where d represents the syntactic depth of the sentence in that the current word occurs and a and \u00df are constant values. Tests have shown that suitable values for a and \u00df are o: ::::: 0.8 and \u00df ::::: 0.6. The results ofthis approach compared to the pure neural net-based version ofK.oHDaS areoutlined in Table 1 . The table shows that the Mean Squared Error (MSE) of three sub-networks of KoHDaS-NN is decreased in the top level net as weil as in the local net for disk problems but increased in the local net for monitor problems. The reasons why the monitor problems behave in this unexpected manner are topic of future investigations (cf. Section 6). 4.91 In the graph-based information extraction step, each node ofthe graph corresponds to the information already extracted from the turn. Nodes can be associated with questions tobe asked by the system. Edges are labeled with sets ofword groups enabling a transition if an appropriate word occurs in the user's input. See Figure 1 for a partial dialogue graph ofKoHDaS-NN. In KoHDaS-ST the results of the dependency analysis together with features in the lexicon are used to create a kind of semantic representation of the user's turn (cf. (B\u00e4cker, 2002) ). Edges in the new dialogue graphs are labeled with this representation (see Figure 2 ) resulting in improved processing ofthe turn. For example the turn \"Sometimes my computer drives me mad. My monitor started glimmering 3 days ago.\" would enable the transition from node 5 to 51, as 'sometimes' is found in the input. In KoHDaS-ST this will not happen, because 'sometimes' is not related to 'glimmering '. Related work A weil studied method to extract relevant infonnation from potentially ill-formed (as is the case for spoken utterances) or not completely mastered (as is the case for automatically analysed spoken utterances) input is chunk ls there any electronic e ui ment next to the montto!'? \u2022. parsing (also called chunking; see, e.g., (Abney, 1991) , (Appelt et al., 1993) , (Grishman, 1995) or (Hobbs et al., 1997) )). Cascaded finite state automata analyse various phrase types expressed in terms ofregular expressions. Tue main advantage ofthis approach is its robustness and the fast run time. The main obstacle of chunking issues from the restricted fonnal power offinite state automata. Cascades of several levels (cf. the system FASTUS (Appelt et al., 1993) , (Hobbs et al. , 1997) with five Jevels) allow for the analysis ofrecursive structures to some extend. The basic level accepts smaller linguistic constructions, whereas in the next levels these elernents become grouped into )arger linguistic units. Accordingly, FASTUS can basically recognize 'complex words' such as proper nouns consisting of several individual words. However the coverage remains restricted to the static number of cascaded phases. Another robust and fast parsing method offers statistical parsing. According to a Jabeled corpus generalization rules can be extracted (cf. Treebank, (Marcus, Santorini and Marcinkiewicz, 1993) ). These rules can be grammar rules (see, e.g., (Chamiak, 1997) , (Collins, 1996) ) or decision trees (see, e.g., (Magerman, 1995) ). Each rule becomes associated with a probability, which is determined in the \uff74 \uff72 \uff61 \uff69 \uff6e \uff69 \uff7e \uff67 \uff20 phase with respect to the corpus. Parsing means to find the most likely derivation according to these rules. The term statistical parsing subsumes a further variant where the rule set is also determined beforehand (see, e.g., (Black et al., 1993)). In the training phase probabilities for these rules are computed according to the corpus (cf. Probabilistic Context-Free Grammars (PCFG, (Booth,.1969 )). The goal ofthis method is primarily disambiguation whereas robustness is a not so relevant here. Let us finally mention recent work in the area of Supertagging. Supertagging (Joshi and Srinivas, 1994 ) is a disarnbiguation method which extends Part-Of-Speech Tagging (POS-Tagging). A Lexicalized Tree Adjoining Grammar, i.e. any rule has at least one lexical anchor (cf. (Schabes, 1990 )) underpins the system. As is the case for Part-Cf-Speech Tagging, the model is trained with a labeled corpus. In the training phase, each word of each input sentence becomes associated with a lexicalized rule according to the model (Supertag). On the basis of this relation, a Lightweight Dependency Analyzer (LDA, (Srinivas, 1997a) ) identifies relations between the Supertags of the individual lexical entries of a input sentence, i.e. the grammar rules the Supertagger gives the highest probabilities. As far as possible a complete parse is computed. Particularly the ability to produce complete parses (if possible) compared to individual phrases in chunking led to the choice of the latter method for our application domain. Here, the user's utterance should be best analysed particularly in the step of extracting task parameters. Our conjecture is that Supertagging allows for a more elaborate identification of complex constructions such as negations and their scope in the user's utterance. In the area of Supertagging various approaches for the model have been proposed in the literature ( e.g., Trigram Supertagging by (Srinivas, 1997a) with Good-Turing Discounting (Good, 1953) and the Backoff method by Katz (Katz, 1987) ). On the basis of a training corpus of 1000000 English words the Supertagger provides an accuracy of 92,2%. Head Trigram Supertagging (Srinivas, 1997a ) is a similar method based on trigrams. However not the two previous Supertags are used to compute the current Supertag but the two previous Head Supertags. A Head Supertag is a previously computed Supertag which influences the choice ofthe current Supertag. Tue method works in two phases. In the first one all Head Supertags are determined. In the second phase, the Head Supertags are used to compute the probabilities of all Supertags. This method assigns in 91,2% of the cases the correct Head Supertags for a training corpus of 1 000 000 words. Its accuracy is 87%. Transfonnation-based Supertagging (Srinivas, 1997a) , (Srinivas and Joshi, 1999) adapts the central idea of transformation-based POS-Tagging (Brill, 1993) , (Brill, 1995) . For this method any word in the corpus is labeled with its most frequent tag. During Tagging these tags can be changed by a list of transformations. Such a transformation consists of a pattem, which activates the rule and a rewriting rule. In order to train this Supertagger a set of transfonnation patterns and a labeled reference corpus has to be provided. The training algorithm detennines the best order of rule applications by minimizing the error rate of the Tagger compared to the reference corpus. The Supertagger based on this model has been trained with 200 000 words and reaches an accuracy of 90%. A Supertagger for Gennan (B\u00e4cker, 2001) based on Hidden Markov models and a bigram model was trained and tested with word classes instead of individual words. Notice that from this fact a loss of accuracy results. Furthennore only basic smoothing techniques were imposed. Accordingly, first results Jack far behind the previously mentioned ones. On the basis of 5 460 sentences of the NEGRA corpus (Brants et al. , 1997) the Supertagger bas an accuracy of 65,5%. This was basically the reason to impose trigrams to the Supertagger we describe here. Supertagging and Lightweight Dependency Analyzers exhibit high robustness and efficiency 4 and are deployed for shallow parsing in various contexts ( e.g., text chunking with Supertags (Srinivas, l 997b )). Text chunking (Abney, 1991) means that a sentence is divided into several non-overlapping segments. By this method individual types of phrases ( e.g., identification of noun phrases Noun Chunking) can be identified in a text. A respective LDA reaches high precision (91,8%) and recall (93%) (cf. (Srinivas, 1997b) ). Conclusions In order to conclude, the application of Hidden Markov model-based Supertagging in the user-initiative dialogue system KoHDaS-ST helps to remedy the lack of accuracy resulting from speaker-independent speech recognition on the basis of general dictionaries and grammars as required in a user-initiative dialogue system. Comparing the results of Gennan Supertagging (78.3%) to English (92.2%), two different reasons lead to less good results. First, our training set (1973 words) is small compared to the English one (1000000 words). Accordingly, many unseen trigrams are imposed on the system. Second, German is a language with free word order. This fact amplifies the effects of sparse data (cf. Spanish Supertagging has an accuracy of about 80% (Srinivas, 1998) ). In the future the training set of KoHDaS-ST will be extended. Furthermore, unsupervised learning methods integrated with supervised methods (cf. (Montoya, Su\u00e4rez and Palomar, 2002) ) will be deployed in our system. How far we can get with a free word order language like Gennan is currently an open problem. A further open problem is why the adjustment of significance vectors according to the identified sentence structure decreases the number of correctly classified problems with respect to the class of monitor problems. Perhaps our conjecture that concepts mentioned in subordinate clauses have a lower impact to the decision making than those in the main clause is too strict.",
    "abstract": "In this paper we outline the advantages of deploying a shallow parser based on Supertagging in an automatic dialogue system in a call center that basically leaves the initiative with the user as far as (s)he wants (in the literature called userinitiative or adaptive in contrast to system-initiative dialogue systems ). The Supertagger relies on a Hidden Markov model and is trained with Gennan input texts. The entire design of a Hidden Markov-based Supertagger with trigrams builds the central issue of this paper. The evaluation of our German Supertagger lags behind the English one. Some of the reasons will be addressed later on. Nevertheless shallow parsing with the Supertags increases the accuracy compared to a basic version ofKoHDaS that only relies on recurrent plausibility networks. 1. The acronym KoHDaS stands for Koblenzer Help Desk with automatic Speech recognition. In the following the basic version is called KoHDaS-NN (NN stands for Neural Networks). Later on we investigate KoHDaS-ST where ST stands for SuperTagging.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German",
        "English"
    ],
    "numcitedby": "7",
    "year": "2002",
    "month": "May",
    "title": "Hidden {M}arkov model-based supertagging in a user-initiative dialogue system"
}