{
    "article": "This paper presents a description for the ICT systems involved in the IWSLT 2008 evaluation campaign. This year, we participated in Chinese-English and English-Chinese translation directions. Four statistical machine translation systems were used: one linguistically syntax-based, two formally syntax-based, and one phrase-based. The outputs of the four SMT systems were fed to a sentence-level system combiner, which was expected to produce better translations than single systems. We will report the results of the four single systems and the combiner on both the development and test sets. Introduction The ICT system for IWSLT 2008 is a combination of four statistical machine translation systems: 1. Silenus, a linguistically syntax-based system that uses tree-to-string rules learned from packed forests; 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Mencius, a phrase-based system that enables lexicalized reordering and similarity-based partial matching of bilingual phrases; 4. Change, a formally syntax-based system that employs hierarchical phrases. They are combined at sentence level using a general linear model. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. SMT Systems Silenus Deriving from the tree-to-string system Lynx [1, 2] , Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5] . A forest can be formally defined as a tuple V, E, v, R , where V is a finite set of nodes, E is a finite set of hyperedges, v \u2208 V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w 1:l = w 1 . . . w l , each node v \u2208 V is in the form if X i,j , which denotes the recognition of non-terminal X spanning from position i through j (that is, w i+1 . . . w j ). Each hyperedge e \u2208 E is a triple e = T (e), h(e), f (e) , where h(e) \u2208 V is its head, T (e) \u2208 V * is a vector of tail nodes, and f (e) is a weight function from R |T (e)| to R. Figure 1 shows a pair of Chinese forest and English string. The solid lines denotes hyperedges and the dashed (1) IP(x 1 :NP-B,x 2 :VP)\u2192 x 1 x 2 (2) NP-B(x 1 :NR)\u2192 x 1 (3) NR(bushi)\u2192Bush (4) VP(x 1 :PP,x 2 :VP-B)\u2192 x 1 x 2 (5) PP(x 1 :P, x 2 :NP-B)\u2192 x 1 x 2 (6) P(yu))\u2192with (7) NP-B(x 1 :NR)\u2192 x 1 (8) NR(shalong)\u2192Sharon (9) VP-B(x 1 :VV, AS(le), x 2 :NP-B)\u2192 x 1 a x 2 (10) VV(juxing) \u2192held (11) NN(huitan)\u2192talk lines denote word alignments. Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity. In a forest, a node usually has multiple incoming hyperedges. For example, the source node IP 0,6 has two incoming hyperedges: e 1 = (NP-B 0,1 , VP 1,6 ), IP 0,6 , 0.6 e 2 = (NP 0,3 , VP-B 3,6 ), IP 0,6 , 0.4 Silenus searches for the best derivation (a sequence of translation rules) d that converts a source tree T in the packed forest into a target-language string s: d = argmax d\u2208D P r(d|T ) (1) Table 1 gives a derivation for the example forest-string pair. To learn tree-to-string rules from annotated training data, we follow GHKM [6] to first identify minimal rules and then obtain composed rules. Like in tree-based extraction, we extract rules from a packed forest F in two steps: frontier set computation (where to cut) and fragmentation (how to cut). It turns out that the exact formulation developed for frontier set in tree-based case can be applied to a forest without change. The fragmentation step, however, becomes much more complicated since we now face a choice of multiple hyperedges at each node. We develop a breadth-first search algorithm for extracting tree-to-string rules from packed forests. The basic idea is to visit each frontier node v, and keep a queue open of growing fragments rooted at v. We keep expanding incomplete fragments from open, and extract a rule if a complete fragment is found. Some minimal rules learned from the example forest-string pair are listed in Table 1 . In tree-based extraction, for each sentence pair, each rules extracted naturally has a count of 1, which will be used in maximum-likelihood estimation of rule probabilities. However, a forest is an implicit collection of trees. Each tree has its own probability (that is, product of hyperedge probabilities). As a result, a rule extracted from non 1-best parse should be penalized accordingly and should have fractional counts instead of unit count. We penalize a rule r by the posterior probability of the corresponding tree fragment t = lhs(r), which can be computed as the product of the outside probability of its root, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment: \u03b1\u03b2(t) = \u03b1(root(t)) \u00d7 e\u2208t P (e) \u00d7 v\u2208leaves(t) \u03b2(v) ( 2 ) where \u03b1(\u2022) and \u03b2(\u2022) are the outside and inside probabilities of nodes, root(\u2022) returns the root of a tree fragment and leaves(\u2022) returns the leaf nodes of a tree fragment. Now, the fractional count of a rule r is simply c(r) = \u03b1\u03b2(lhs(r)) \u03b1\u03b2(v) ( 3 ) where v denotes the root of the forest. We extend the simple model in Eq. 1 to a log-linear model [7] : d = argmax d\u2208D P r(d|T ) \u03bb1 \u00d7 p lm (s) \u03bb2 \u00d7 e \u03bb3|d| \u00d7 e \u03bb4|s| (4) where p lm (s) is the language model score, |d| is the number of rules in a derivation, and |s| is the number of target words produced. The derivation probability P r(d|T ) is the product of probabilities of translation rules involved in d: P r(d|T ) = r\u2208d P r(r) (5) where each P r(r) can be decomposed into the product of six probabilities: P r(r) = p(r|lhs(r)) \u03bb5 \u00d7 p(r|rhs(r)) \u03bb6 \u00d7p(r|root(lhs(r))) \u03bb7 \u00d7p lex (lhs(r)|rhs(r)) \u03bb8 \u00d7p lex (rhs(r)|lhs(r)) \u03bb9 \u00d7p(T ) \u03bb10 (6) where the first three terms are conditional probabilities based on fractional counts, p lex (\u2022) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9] . Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] rules. Bruin employs the following three BTG rules to direct translation: A [ ] \u2192 (A 1 , A 2 ) (7) A \u2192 (A 1 , A 2 ) (8) A \u2192 (x, y) (9) The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the following, we will define the model by separating different features (including the language model) from the rule probabilities and organizing them in a log-linear model. This straight way makes it clear how rules are used and what they depend on. For the two merging rules, applying them on two consecutive blocks A 1 and A 2 is assigned a probability P r m (A): P r m (A) = \u2126 \u03bb\u2126 \u2022 p LM (A 1 , A 2 ) \u03bb LM (10) where \u2126 is the reordering score of blocks A 1 and A 2 , \u03bb \u2126 is its weight, and p LM (A 1 , A 2 ) is the increment of the language model score of two blocks according to their final order, and \u03bb LM is its weight. The application of a lexical rule is assigned a probability P r l (A): P r l (A) = p(x|y) \u03bb1 \u2022 p(y|x) \u03bb2 \u2022 p lex (x|y) \u03bb3 \u2022p lex (y|x) \u03bb4 \u2022 exp(1) \u03bb5 \u2022 exp(|x|) \u03bb6 \u2022p LM (x) \u03bb LM ( 11 ) where p(\u2022) are the phrase translation probabilities in both directions, p lex (\u2022) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. We define the reordering model \u2126 on three factors: an order o, a block A 1 , and a block A 2 . Given two neighboring blocks A 1 and A 2 , the central problem is how to predict their order o \u2208 {monotonic, inverted}. This is a typical twoclass classification. To estimate the conditional probability p(o|A 1 , A 2 ), a reasonable way is to use features of blocks as reordering evidences under maximum entropy model: p \u03b8 (o|A 1 , A 2 ) = exp( i \u03b8 i h i (o, A 1 , A 2 )) o exp( i \u03b8 i h i (o , A 1 , A 2 )) (12) where  Two kinds of features are designed for our MaxEnt-based reordering model: lexical features and collocation features. Lexical features are defined on the first words of source and target phrases. Collocation features are defined on the combination of boundary words. Why are we particularly interested in boundary words? We believe that boundary words of blocks capture information of block reordering. To test this assumption, we calculate the information gain ratio(IGR) for boundary words as well as the whole blocks against the order on the extracted reordering orders. IGR measures how precisely a feature f predicts a class c: h i (o, A 1 , A 2 ) \u2208 {0, IGR(f, c) = E(c) \u2212 E(c|f ) Ef ( 13 ) where E(\u2022) is an entropy and E(\u2022|\u2022) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13] . The major difference is that we introduce similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. Given two source phrases f J 1 and f J 1 , their matching similarity is given by SIM( f J 1 , f J 1 ) = J j=1 \u03b4(f j , f j ) J ( 14 ) where \u03b4(f, f ) = 1 if f = f 0 otherwise (15) Note that we only consider two source phrases that have the same length. To make partially matching more reliable, we further restrict that they share with the same parts-ofspeech sequence. Our hope is that similar bilingual phrases can be used to create translation templates if one source phrase cannot find translations in the phrase table. For example, suppose that we cannot find translations for a source phrase \"yu zuotian dida taiguo\" in a phrase table, in which we find a similar source phrase \"yu zuowan dida bulage\" with its translation \"arrived in Prague last evening\". According to the alignment information, we obtain a translation template: yu X 1 dida X 2 , arrived in X 2 X 1 Then, the unmatched source substrings \"zuotian\" and \"taiguo\" can be translated into \"yesterday\" and \"Thailand\", respectively. As a result, the translation for \"yu zuotian dida taiguo\" is \"arrived in Thailand yesterday\". Given a source sentence, the decoder firstly search for all possible translation options from the phrase table by exact matching. For source phrases which have no translations, we construct translations by similarity-based partially matching, as shown in above example. Then, the decoder works the same as Moses does. Change Change is an implementation of the state-of-the-art hierarchical phrase-based model. Considered as an extension of standard phrase-based model, hierarchical phrase-based model allows non-contiguous parts of source sentence to be translated into possibly non-contiguous parts of target sentence. The model can be formalized as a synchronous context-free grammar, in which a rule is of the form: X \u2192 \u03b3, \u03b1, \u223c ( 16 ) where X is a non-terminal, \u03b3 and \u03b1 are strings of terminals and non-terminals, and \u223c is a one-to-one correspondence between the non-terminals of \u03b3 and \u03b1. Our implementation faithfully follows Chiang's work [8] . The only exception is the condition for terminating cube pruning. Chiang's implementation [8] quits upon considering the next item if its score falls outside the beam by more than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. System Combination We combine the outputs of single SMT systems at sentence level, similarly to the work by Macherey and Och [14] . Global linear models are used as a framework for reranking a merged n-best list: \u0177 = argmax y\u2208GEN(x) f (x, y) \u2022 W ( 17 ) where x is a source sentence, y is a translation, f (x, y) is a feature vector, W is a weight vector, and GEN(x) is the set of possible candidate translations. There types of features are used: (1) relative BLEU scores against 1-best translations from other candidates, (2) language model scores, and (3) length of the translation. The feature weights are tuned using minimum-error-rate training [15] . In this year's evaluation, each single SMT system generated 200-best list translations, which were merged and served as the input to the combiner. Experimental Results Data Besides the data provided by the organizer, we used the following additional data 1 : The training corpus contains about 8.1M Chinese words and 8.6M English words. The data were used by all the four single systems to train their model parameters respectively. The English sentences of training corpus were used to train a 5-gram language model using SRILM [16] . Similarly, the Chinese part was used to train a 5-gram language model for English-to-Chinese direction. Annotation We used the Chinese lexical analysis system ICTCLAS for splitting Chinese characters into words and the tokenizer provided by IWSLT for tokenizing English sentences. After that, we convert all alphanumeric characters to their 2-byte representation. Then, we ran GIZA++ and used the \"growdiagfinal\" heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, \" s \" and \" /s \", to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. \"provided+additional\" denotes all the training data we have, as listed at the beginning of Section 3.1. We observe that using more data results in substantial improvements of about 5 BLEU points. Results Table 3 gives the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the test sets. \"BTEC CE\" denotes Chinese-English direction of BTEC task, \"CT CE\" denotes Chinese-English direction of challenge task, and \"CT EC\" denotes English-Chinese direction of challenge task. \"CRR\" denotes correct recognition results and \"ASR.1\" denotes using 1-best ASR output. Our sentence-level system combiner outperformed single systems consistently on all tasks. While system combination benefited Chinese-English direction significantly, the improvements on English-Chinese direction were relatively small. One possible reason might be that fewer development sets are available for English-Chinese direction for system combiner to optimize the parameters automatically. For single SMT systems, Bruin got better results than the others on Chinese-English direction. Interestingly, Silenus surpassed other systems significantly on English-Chinese direction. There are two findings worth noting: 1. Silenus uses packed forests instead of 1-best parses, minimizing the negative effect of parsing errors. As the amount and domain of data used for training parsers are comparatively limited, parsers will inevitably output ill-formed trees when handle realworld text. Guided by such noisy syntactic information, syntax-based models that rely on only 1best parses are prone to produce degenerate translations. The results suggest that packed forests do help syntax-based systems to achieve comparable performance with phrase-based systems on tourism-related sentences. 2. Parsing accuracy has a substantial effect on syntaxbased models. Silenus obtained better results on English-Chinese direction than Chinese-English direction. We believe the major reason is that parsing on English is more accurate than Chinese. Conclusion In this paper, we give a brief introduction to our four single SMT systems and one system combiner. We report the resources used, annotation techniques, and results achieved on the test sets. We find that our implementation of sentencelevel system combination works for all tasks. Another interesting finding is that syntax-based models could produce translations as good as phrase-based systems on tourismrelated text if packed forests are used. Acknowledgements",
    "abstract": "This paper presents a description for the ICT systems involved in the IWSLT 2008 evaluation campaign. This year, we participated in Chinese-English and English-Chinese translation directions. Four statistical machine translation systems were used: one linguistically syntax-based, two formally syntax-based, and one phrase-based. The outputs of the four SMT systems were fed to a sentence-level system combiner, which was expected to produce better translations than single systems. We will report the results of the four single systems and the combiner on both the development and test sets.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": "1",
    "year": "2008",
    "month": "October 20-21",
    "title": "The {ICT} system description for {IWSLT} 2008."
}