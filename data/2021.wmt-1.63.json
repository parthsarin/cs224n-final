{
    "article": "Recently, the Machine Translation (MT) community has become more interested in document-level evaluation especially in light of reactions to claims of \"human parity\", since examining the quality at the level of the document rather than at the sentence level allows for the assessment of suprasentential context, providing a more reliable evaluation. This paper presents a document-level corpus annotated in English with context-aware issues that arise when translating from English into Brazilian Portuguese, namely ellipsis, gender, lexical ambiguity, number, reference, and terminology, with six different domains. The corpus can be used as a challenge test set for evaluation and as a training/testing corpus for MT as well as for deep linguistic analysis of context issues. To the best of our knowledge, this is the first corpus of its kind. Introduction Machine translation (MT) is now widely used in a variety of fields, mainly due to advancements in neural models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) . As a result of these recent advances, scientists have been increasingly attempting to include discourse into neural machine translation (NMT) systems (Wang, 2019; Lopes et al., 2020) . Thus, researchers started to consider a more suitable evaluation for these document-level systems as the standard MT automatic evaluation metrics have been shown to underestimate the quality of NMT systems (Shterionov et al., 2018) and the appropriateness of these metrics for document-level systems has been challenged (Smith, 2017) since they are not sensitive to their improvements (Voita et al., 2019) . Accordingly, document-level human evaluation of MT has attracted the community's attention since it allows for a more thorough examination of the output quality with context. While a few works have taken into account document-level human evaluation (L\u00e4ubli et al., 2018; Toral et al., 2018; Barrault et al., 2019; Castilho, 2020 Castilho, , 2021)) , one common practice for document-level evaluation is the usage of test suites with context-aware markers (Bawden et al., 2018; Guillou et al., 2018; M\u00fcller et al., 2018; Voita et al., 2019; Cai and Xiong, 2020) . However, the concept of documentlevel evaluation, in terms of how much text must be shown, remains uncertain (Castilho et al., 2020) . While most research on document-level MT evaluation works with contrastive pairs, very few works have tried to use full documents for human evaluation (L\u00e4ubli et al., 2018; Castilho, 2020 Castilho, , 2021) ) and challenge test sets (Rysov\u00e1 et al., 2019; Vojt\u011bchov\u00e1 et al., 2019) . Methodologies for assessing MT at the document-level have been looked into (Barrault et al., 2019 (Barrault et al., , 2020) ) as well as the types of issues that come with different methodologies (Castilho, 2020 (Castilho, , 2021)) . We present a document-level corpus annotated with context-aware issues when translating from English (EN) into Brazilian-Portuguese (PT-BR). In total, 60 documents from six different domains (literary, subtitles, news, reviews, medical, and europarl) were annotated with context-aware issues, namely gender, number, ellipsis, reference, lexical ambiguity, and terminology. The corpus can be used as a challenge test set for the evaluation and as a training/testing corpus for MT and quality estimation, as well as for deep linguistic analysis of context issues. Moreover, we believe that the annotation can be also used for close-related languages such as Spanish. Related Work Document-level MT evaluation has attracted interest in the field as it allows for the evaluation of suprasentential content, which in turn, provides more meaningful insights on the MT output. However, the definition of what constitutes a documentlevel MT evaluation is still unclear (Castilho et al., 2020) . Context plays an important role as it is widely used in translation and interpreting literature (Baker, 2006) , although it lacks a precise definition for practical purposes, including in everyday work of a professional translator (Melby and Foster, 2010) . For Melby and Foster (2010, p 3) , context in translation could be studied \"either for the purpose of analysing existing translations or for the purpose of improving the production of new translations\". For the authors, context can be categorised into nontext (non-linguistic variables) and text (linguistic aspects), where the latter is divided into four aspects of context: relating to the source text: co-text (the version of the document itself) and chron-text (past and future versions); and relating to other text: rel-text (monolingual related texts) and bi-tex (bilingual related texts). In this work, we adopt Melby and Foster's view of context that is important to the analysis of translations, and focus (i) on the co-text, i.e. the boundaries within the document translated, and (ii) in the non-text, where the name of the authors, speakers, and products have an effect on the translation. In a survey with native speakers, Castilho et al. (2020) tested the context span for the translation of 300 sentences in three different domains, namely reviews, subtitles, and literature. The results showed that over 33% of the sentences tested were found to require more context than the sentence itself to be translated or evaluated, and from those, 23% required more than two previous sentences to be properly evaluated. The authors found that ambiguity, terminology, and gender agreement were the most common issues to hinder translation. Moreover, differences in issues and context span were found between domains. Their recommendations include to show whole documents when possible, include information on text type, topic, product, hotel and movie names in case of reviews, and include visual context whenever possible (nontext) . This shows that document-level evaluation enables the assessment of textual cohesion and coherence types of errors which are impossible at times to recognise at sentence level. Regarding overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. L\u00e4ubli et al. (2018) compared sentence-level evaluation versus documentlevel evaluation with pairwise rankings of fluency and adequacy to evaluate the quality of MT against human translation (HT) with professional translators. Their results show that document-level raters clearly preferred HT over MT, especially in terms of fluency. The authors argue that documentlevel evaluation enables the identification of certain types of errors, such as ambiguous words, or errors related to textual cohesion and coherence. The Conference for Machine Translation (WMT), which has been running since 2006 and only evaluated sentences, attempted documentlevel human evaluation for the news domain for the first time in 2019 (Barrault et al., 2019) . Their direct assessment (DA) (Graham et al., 2016) required crowdworkers to assign a score (0-100) to each sentence. They asked raters to evaluate (i) whole texts, (ii) single consecutive segments in their original order, and (iii) single random phrases. In the following year, WMT20 changed the approach and expanded the context span to include full papers, requiring raters to evaluate specific segments while seeing the complete document, as well as to assess the content's translation (Barrault et al., 2020) . Castilho (2020 Castilho ( , 2021) ) tested for the differences in inter-annotator agreement (IAA) between single sentence and document-level set-ups. In Castilho (2020) , the author asked translators to evaluate the MT output of freely available online systems in terms of fluency, adequacy (Likert scale), ranking and error annotation in two different set-ups: (i) translators give one score per single isolated sentence, and (ii) translators give one score per document. The results showed that IAA scores for the document-level set-up reached negative levels, and the level of satisfaction of translators with that methodology was also very low. Nonetheless, it avoided cases of misevaluation that happen in isolated single sentences. Following on from that work, Castilho (2021) modifies the document-level set-up and re-runs the experiment with more translators, where she compares the IAA in evaluation of (i) random single sentences, (ii) evaluation of individual sentences where translators have access to the full source and MT output, and (iii) evaluation of full documents. Results showed that a methodology where translators assess individual sentences within the context of a document yields a good level of IAA compared to the random single-sentence methodology, while a methodology where translators give one score per document shows a very low level of IAA. The author demonstrates that the methodology of assigning one score per sentence in context avoids misevaluation cases which are extremely common in the random sentencesbased evaluation set-ups. Moreover, the author posits that the higher IAA agreement in the random single sentence set-up is because \"raters tend to accept the translation when adequacy is ambiguous but the translation is correct, especially if it is fluent\" (Castilho, 2021, p 42) , and asserts that the single random sentence evaluation method should be avoided as the misevaluation issue is especially problematic when assessing the quality of NMT systems as they have an improved fluency level. One current way of evaluating document-level issues is the use of test suites designed to better evaluate translation of the addressed discourse-level phenomena. Commonly, these test suites are contrastive, that is, each sample sentence in the test has both correct and wrong translations for a given phenomena (Bawden et al., 2018; Guillou et al., 2018; M\u00fcller et al., 2018; Voita et al., 2019; Cai and Xiong, 2020) . The higher the accuracy of the model in rating correct translations over incorrect ones, the better the quality is deemed to be. Test suites with document-level boundaries are still scarce, e.g. Vojt\u011bchov\u00e1 et al. (2019) present a test suite designed to evaluate coherence when testing MT models trained for the news domain on audit reports, and Rysov\u00e1 et al. (2019) designed a document-level test suite to assess three documentlevel discourse phenomena, namely information structure, discourse connectives, and alternative lexicalisation of connectives. Given the above, the need to move toward document-level methodologies in MT is indisputable. Moreover, with the lack of resources for the topic of document-level MT, the documentlevel corpus annotated with context-aware issues presented here can be used as a challenge test set for evaluation and as a training/testing corpus for MT as well as for deep linguistic analysis of context issues. Corpus Compilation The corpus was collected from a variety of freely available sources. Following a pre-determined list of context issues found in Castilho et al. (2020) that hindered the translation of single sentences and sentence pairs, the annotators searched for challenging English texts for the MT systems when translating into PT-BR. In total, 60 full documents (57217 tokens) were collected from six different domains: literary, subtitles, news, reviews, medical, and legislation (europarl). Table 1 shows the statistics of the corpus. Each domain has their plain text and .xls versions of the documents segmented into sentences with sentence id and document boundary tags, and all documents contain the source (url or corpus) where the documents were retrieved from. What follows is a detailed description of each domain is provided. 1 Domains Subtitles To compile the corpus for the subtitle domain, nine full TED Talks were selected from the Opus Corpus (Tiedemann, 2012) from a variety of different topics and speakers, where: doc1: education, doc2: climate change, doc3: astronomy, doc4: computers, doc5: creativity, doc6: science, doc7: technology, doc8: anthropology, and doc9: psychology. We chose these talks specifically in order to obtain a blend of different topics and speakers' genders. To compile the corpus for the literature domain, four documents 2 were selected: doc1: one chapter from a fan-fiction story. 3 doc2: one excerpt from \"The Road to Oz\" book. 4 doc3: a short story generated with the PlotGenerator website. 5 doc4: a short play generated with the PlotGenerator website. Note that a blend of contemporary and classic excerpts, combining descriptive and fast moving styles, were gathered. Note too that the synthetic stories (doc3 and doc4) were generated as they allowed the researchers to add a good number of possible issues, including lexical ambiguities cases that can only be solved with a larger context than two consecutive sentences which is rather difficult to find in \"natural\" texts. Nonetheless, English native speakers then revised both stories for fluency and readability. Table 3 shows the statistics for each document in the literary domain. 6 News The news domain was compiled with 15 documents gathered from different sources. Table 4 shows the statistics of the corpus. 7  Five documents were gathered from the WMT series (four documents from WMT19 8 and one from WMT20 9 ), and their size varied from 13 to 32 sentences. Ten documents were gathered from several news websites, 10 Reviews The reviews domain was compiled with 28 documents gathered from reviews available on Amazon 11 and TripAdvisor 12 websites. Table 5 shows the statistics of the corpus. 13  Reviews gathered from Amazon consist of users' reviews about a variety of products and movies, totalling 25 reviews, and vary from 6 to 84 sentences. The reviews were sought by searching products that could generate lexical ambiguities, such as \"plant\", \"ship\", etc. Reviews gathered from TripAdvisor consist of 3 reviews about places, and vary from 23-35 sentences. Medical The medical domain corpus was compiled with three full documents, where two of them were collected from Autopsy reports available on the Medical Transcriptions website, 14 and one document was collected from the leaflets available on the Methodology for Annotation Following literature on document-level test suites (see Section 2), together with issues found when trying to define how much context span is needed to translate and evaluate MT (Castilho et al., 2020) , we compiled a list of context-aware issues that are challenging for MT when translating from EN into PT-BR to be annotated: 1-Gender 2-Number 3-Ellipsis 4-Reference 5-Lexical Ambiguity 6-Terminology Three annotators tagged those issues that might occur in a translation from EN into PT-BR when no context information is given. For example, in the following single sentence given to a translator to translate: \"And thanks for the case.\" The translator will not be able to translate this sentence with absolute certainty because: i) it is not possible to know the gender of the person who is saying 'thanks' as Portuguese differentiates between masculine and feminine genders. ii) it is not possible to know what the word \"case\" is as this word has a few different meanings that would fit this sentence, i.e it could be some type of protective box (a case for my phone, a case for my glasses), a woman's bag, a pencil case, a folder, a suitcase, or a police case to be investigated, each one with a different translation in Portuguese. Consequently, the translation of \"for\" will have a different gender depending on the meaning of the word \"case\". When evaluating the translation of the source sentence given by 3 different MT systems (Google Translate 17 (GG), Microsoft Bing 18 (MS) and DeepL 19 (DPL) the translator has to evaluate all three systems' outputs as correct: GG: \"E obrigado pelo caso.\" (masculine, police case) MS: \"E obrigado pelo estojo.\" (masculine, pencil case) DPL: \"E obrigado pela caixa.\" (masculine, box) That is because without a wider context, it is impossible to know the correct translation or the sentence, which should be: HT: \"E obrigada pela capa.\" (feminine, phone case) Therefore, the issues tagged in the corpus are issues that might arise in the translation of sentences when the full context is not given. Annotators used different MT systems to help check for issues that would go unnoticed when only looking at the source text. Moreover, a few modifications to the source text were performed in order to add those issues and make the translation more challenging for MT, such as modifying the gender, substituting the name of a product for 'it', splitting a sentence into two, etc. These modifications are explained in the spreadsheet file for each line modified, so researchers can decide if they can use or not documents that had the source modified. Annotation of Context-Related Issues As previously mentioned, six context-related issues were tagged in the corpus when they could not be solved within the sentence they appeared. A detailed guideline was developed as the annotators gathered the corpus and discussed how the annotation would be better performed. Figure 1 shows the decision tree that guides the annotation of the context-related issues. Reference Reference is associated with the notion that \"anaphoric references and noun phrase organizers may serve as cohesive ties linking separate sentences into unified paragraphs [aiding] the reader's memory structure\" (Fishman, 1978, p 159) . Differently from ellipsis which is generally dependent on the previous clause, reference can reach back a long way in the text and extend over a long passage (Halliday and Matthiessen, 2013) , thus being of significance for the present work. In the annotation guide, we annotated the reference whenever we faced a disruption or ambiguity in the referential chain, e.g., we only annotated dependent referential units. Moreover, and similar to all annotated categories, the disagreement had to be expressed at the document level, e.g. the issue could not be solved only by looking at the sentence. In example A), we annotate the second individual it as being a referential issue because there is not enough lexical material in the sentence to properly establish the referent, thus affecting translation correctness and final text readability. A) It is understandable though since it was shipped from China. reference -> it = the ship it = o navio. In example B), we annotated they as being a referential unit issue, due to the fact that there is not enough lexical material in the sentence to determine its referent. Moreover, we also tagged this issue as a gender problem since there is no information in the source sentence that allows one to determine that the referential unit should be translated into PT-BR as a plural feminine pronoun. B) They actually hurt reference -> they = the choices gender -> they = feminine They actually hurt = Elas / As escolhas realmente machucam. Ellipsis Ellipsis is a form of anaphoric cohesion where there is an omission from a clause, and, so, the reader must \"presuppose something by means of what is left out\" (Halliday and Matthiessen, 2013, p 635) . Ellipsis differs from reference as the relationship it entails is lexicogrammatical rather than semantic (Halliday and Matthiessen, 2013) . In the annotation guide, we annotate ellipsis exclusively when the omission of information affects the translation of that specific single sentence which needs a broader context to be understood. For example, in C), ellipsis is tagged because the omission with the explicit indication of \"do\" causes lexical ambiguity that cannot be solved within the sentence. 20 Therefore, the tags and the solution for the issues are both ellipsis and lexical ambiguity, with the translation of the ellipsis also containing an explanation of the lexical ambiguity caused by it. C) In my laughter, I bellied out a \"YES, I do!!\" ellipsis -> do = think lexical ambiguity -> do = make (incorrect) vs think (correct) Sim, eu fa\u00e7o! (Yes, I make, incorrect) vs Sim, eu acho! (Yes, I \"think\", correct) In example D), ellipsis is tagged because the omission causes gender issues that cannot be solved within the sentence. Therefore, the tags and the solution for the issues are both ellipsis and gender, with the translation of the ellipsis also containing an explanation of the lexical ambiguity caused by the ellipsis: Sentence E) is an example of ellipsis with the auxiliary do that has not been tagged in the corpus because the omission is solved within the sentence: E) Also, not once did I feel a blast of hot air like I do when taking things out of the oven. do = feel a blast of hot air Gender As Portuguese is a language in which grammatical gender (feminine and masculine) plays a very significant role, the definition of gender used is from a grammatical standpoint, where word classes like adjectives, articles or pronouns are bound to respect and reflect a word's gender (Grosjean et al., 1994) . In the annotation guide, we annotated gender whenever facing a gender issue e.g., gender disagreement, unsolvable within the sentence itself and requiring broader context information. 21 For example, in example F), gender (feminine) is tagged because the issue is not possible to be solved within the sentence. Since the default of the PT-BR is to have everything in the masculine, translations (both HT and MT) follow the same pattern. Therefore, we tag the word that needs to be in a different gender and the solution for its gender marker, with the translation containing an explanation: F) I'm surprised to see you back so early. gender -> surprised = feminine surprised = surpresa In example G), we note that not only the pronoun \"they\" needs to be tagged with the feminine gender tag, but also the expression \"staying at\", as it is translated with an adjective in Portuguese: G) She waited for a few minutes longer, but nothing happened, no one followed her, so she made her way back to the motel they were staying at. gender -> they = feminine gender -> staying at = feminine they were staying at = elas estavam hospedadas Gender was also tagged even when the most used translation for the the given term was a neutral one, because the adjective could still be translated with one of its synonyms. For instance, in example H), the adjective \"glad\" has its most common translation as \"feliz\" which is used for both masculine and feminine gender. If a translator chooses to translate the text as \"I'm glad = Estou feliz\", no gender marking is needed. However, synonyms of that translation would need to be translated into feminine (satisfeita, grata, encantada, animada), and so, gender is tagged for that case: H) I'm so glad that it comes with the extender, so I have more levels to use to continue to get smaller. gender -> glad = feminine reference -> it = waist cincher Number Number agreement is one of the basic coherence devices within a text, and it is \"part of the code, used to signal that linguistic constituents carrying the same number are linked regardless of whether they appear together or apart in an utterance\" (Bock et al., 1999, p 330) , in the entirety of the text, and thus is significant for the present work. In the annotation guide, we annotated number whenever we faced a number disagreement within the referential chain, e.g. (i) noun or pronoun, (ii) verb and noun/pronoun, (iii) adjective, caused by lack of enough contextual information within the sentence. 22 In example I), the number category was applied to the word you because it is not possible to identify within this single sentence whether we are facing a pronoun in the plural or singular. I) I was praying for you. number -> you = plural you -> voc\u00eas Example J) depicts a mistranslated number agreement chain into PT-BR which originated from the absence of contextual evidence in the sentence that allowed us to determine whether you should be translated in the plural rather than in the singular. Furthermore, as a consequence of this initial mistranslation, the adjective agreeable was affected, being translated in the singular rather than the plural. J) You should be more agreeable. 22 Note that number was most exclusively tagged as plural for the pronoun \"you\" (and its referential chain (verb/adjectives)) when a problem with the agreement was obvious. As the MT systems typically tend to translate \"you\" in the singular (when no specific plural markers are given in the single sentence) for PT-BR, the pronoun was only tagged for singular when there was an ambiguity in the sentence. number -> you = plural number -> agreeable = plural number -> should be = plural gender -> agreeable = feminine You should be more agreeable. -> Voc\u00eas deveriam ser mais simp\u00e1ticas/ agrad\u00e1veis. Lexical Ambiguity Lexical ambiguity refers to the fact that \"a single word form can refer to more than one different concept\" (Rodd, 2018, p 2) . Lexical ambiguity can be divided into two categories: (i) one takes into account a word's morphological aspect (verbs, adjectives) referred to as syntactic ambiguity, e.g. the word \"play\" can be either the act of taking part in a sport or the conducting of a sporting match; and (ii) the second focuses on the fact that a word can assume different meanings according to context, e.g. the word \"ball\" as in They danced till dawn at the ball versus This dog can be entertained all day with a ball (Small et al., 2013, p 4) , which is referred as semantic ambiguity. In the annotation guide, we annotated lexical ambiguity, the more generic term, whenever we faced one of the two cases above ((i) and (ii)) and whenever they appeared to be detrimental to the translation and understandable only within the broader context, rather than at sentence level. In example K), lexical ambiguity is tagged because the clause I lost it, without context, can be interpreted either as someone losing something or someone losing control: K) He came back in the house and said \"So you think this is funny?!\" up the stairway at me and I LOST IT. lexical ambiguity -> lose something vs to lose control I lost it -> Eu o/a perdi vs Eu perdi o controle In example L), lexical ambiguity is tagged because the word Period is polysemic, meaning simultaneously menstruation, a portion of time, and a punctuation mark, and by the fact that there is not enough lexical information at a sentence level to disambiguate the complete meaning. L) Period. lexical ambiguity -> period = era/menstruation vs full stop Per\u00edodo vs Ponto final Terminology Terminology, according to Sager and Nkwenti-Azeh (1990) (as cited in (Kast-Aigner, 2018 )), can have three different definitions: (i) the theory behind the relationship between a concept and a term; (ii) terminology curatorship activities, e.g. collection, description, processing and presentation of terms; and (iii) the vocabulary of a specific field. In the present work, we perceived terminology as (iii) i.e. the lexical expression of a domain-specific area. In the annotation guide, we annotated terminology whenever we faced a wrongly domain-specific word translation caused by contextual poor sentences. In the following example M), the category terminology was applied to the word 'farm' because its meaning shifts from \"a piece of land used for crops and cattle raising\", its more generalised conceptualisation, into a more domain-specific concept, \"an area of land with a group of energy-producing windmills or wind turbines\". M) The center will also conduct testing (power curve, mechanical loads, noise, and power quality) at its own experimental wind farm terminology -> generalised lexic (farm) vs domainspecific lexic (park) wind farm -> parque e\u00f3lico Format The annotation was performed for each sentence, which are tagged one per line, in the order they appear in the sentence, followed by their explanation/solution, along with modifications performed in the source (if any) and translations of some cases and notes. Sentences with no context-related issues are followed by two Xs for the issue and the solution. For Reference and Ellipsis, the term that contains the issue is stated along with an equals sign (=) and the explanation of what it refers to. For Gender and Number, the issue is tagged along with an equals sign (=) and the solution (feminine/masculine or singular/plural) is given. For Lexical Ambiguity and Terminology, the term (or terms) is stated along with an equals sign (=) and a contrasting solution is given, the wrong meaning(s) compared to (vs) the correct one. Table 8 illustrates how the annotation format is performed for each issue. The corpus will be made freely available in two formats. One is a spreadsheet (.xls) containing the tagged corpus in all domains and full information. This .xls format will allow for filtering specific issues or sentences and enable users/researchers to see the rationale of the annotation. The corpus will be also available in plain text (.txt) format, containing the segment id, sentence, issue and explanation all in one line. 23 This format will allow for an automatic use of the corpus, for training or as a test suite. Figure 2 shows the .xls and .txt formats. Agreement As previously mentioned, three annotators compiled and annotated the corpus. Their backgrounds include linguistics, translation and computational linguistics. Throughout the process of compilation and annotation, the annotators worked closely together to discuss the corpus compilation and also what issues should be tagged. Disagreements were discussed and resolved, and then the annotation process would resume. This process helped to refine the list of issues as well as to develop and finalise the guidelines. The corpus annotation carried out by the three first annotators was corrected at the final stage in order to ensure that it follows the established version of the guidelines. In order to reveal some possible weaknesses of the annotation guidelines and the decision tree, another expert annotator was involved at the final stage. The fourth annotator worked with 9% of the documents from the original collection, where at least one document of each domain was selected randomly. The annotation was done according to the guidelines and the decision tree used by the first three annotators (see Figure 1 ). During the annotation process, the annotator was given the guidelines, decision tree and was explained what the goal of the annotation was, but was not allowed to communicate with the other annotators. We then calculated inter-annotator agreement using Cohen's Kappa (Cohen, 1960) treating the first annotation (performed by the three annotators) as the gold standard. Results show that the overall Kappa score was 0.61 meaning that, by using the guidelines and the decision tree on a first try, we could reach a substantial agreement (Landis and Koch, 1977) . We note that the majority of disagreement cases are related to agreeing whether or not a sentence contains an issue to be annotated, while our gold Conclusion We have presented a document-level corpus annotated with context-aware issues when translating from EN into PT-BR, namely gender, number, ellipsis, reference, lexical ambiguity, and terminology. This first version of the corpus contains 60 documents, with 3680 sentences, in six different domains: subtitles, literary, news, reviews, medical and legislation. To the best of our knowledge, this is the first corpus of its kind. 24  With the rise in NMT quality and the claims of human parity, the need to move towards a more fine-grained evaluation involving the whole document is beyond question. Moreover, with the lack of resources for the document-level MT area, this document-level corpus can be used as a challenge 24 The corpus and guidelines will be freely available at https://github.com/SheilaCastilho/ DELA-Project test set for evaluation and as a training/testing corpus for MT as well as for deep linguistic analysis of context issues. We believe that the annotation can be also used for closely-related languages such as Spanish. We intend to increase the corpus, adding more documents, domains and more context-aware issues. The full translation into PT-BR is ongoing, and we want to annotate it for other languages, starting with the Romance language family. Acknowledgements We would like to than Helena Moniz and Vera Cabarr\u00e3o for the fruitful discussions and invaluable help. This project was funded by the Irish Research Council (GOIPD/2020/69). ADAPT, the Science Foundation Ireland Research Centre for AI-Driven Digital Content Technology at Dublin City University, is funded by the Science Foundation Ireland through the SFI Research Centres Programme (Grant 13/RC/2106_P2).",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions that the project was funded by the Irish Research Council, which is a government-funded organization providing grants for research. Additionally, it mentions ADAPT, the Science Foundation Ireland Research Centre for AI-Driven Digital Content Technology at Dublin City University, indicating funding from a research agency. There is no mention of funding from defense, corporate entities, foundations, or an absence of funding."
}