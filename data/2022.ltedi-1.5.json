{
    "article": "Deaf and hard of hearing individuals regularly rely on captioning while watching live TV. Live TV captioning is evaluated by regulatory agencies using various caption evaluation metrics. However, caption evaluation metrics are often not informed by preferences of DHH users or how meaningful the captions are. There is a need to construct caption evaluation metrics that take the relative importance of words in a transcript into account. We conducted correlation analysis between two types of word embeddings and human-annotated labeled wordimportance scores in existing corpus. We found that normalized contextualized word embeddings generated using BERT correlated better with manually annotated importance scores than word2vec-based word embeddings. We make available a pairing of word embeddings and their human-annotated importance scores. We also provide proof-of-concept utility by training word importance models, achieving an F1-score of 0.57 in the 6-class word importance classification task. Introduction Over 360 million people worldwide are Deaf or Hard of Hearing (DHH) (Mitchell et al., 2006; Blanchfield et al., 2001) . In the U.S. alone, over 15% people are DHH, and regularly rely on captioning while watching videos to perceive salient auditory information (Berke et al., 2019) . To provide quality captioning services to this group, it is essential to monitor the quality of captioning regularly. Regulators, e.g., the Federal Communication Commission (FCC) in the U.S. (Commission, 2014) are entrusted with regularly checking the quality of caption transcription generated by different broadcasters. However, given the abundant production of captioned live TV broadcasts, caption evaluation is a tedious and costly task. DHH viewers are often dissatisfied with the quality of captioning provided in live contexts, which provide less time for caption production than prerecorded contexts (Amin et al., 2021b; Kushalnagar and Kushalnagar, 2018) . If regulatory organizations that measure the quality of captions used quality metrics that better reflect the DHH users' preferences, DHH viewers' experience may improve. Existing metrics used in transcription or captioning include Word Error Rate (WER) (Ali and Renals, 2018) or Number of Error in Recognition (NER) (Romero-Fresco and Mart\u00ednez P\u00e9rez, 2015) . As noted by Kafle et al. (2019b) , a major shortcoming of these metrics is that they do not consider the importance of individual words when measuring the accuracy of captioned transcripts (comparing to the reference transcript) and most metrics assign equal weights to each word. DHH viewers rely more heavily on important keywords while skimming through caption text (Kafle et al., 2019b) . Motivated by these shortcomings, prior work had proposed metrics which assign differential importance weights to individual words in captioned text when calculating an evaluation score (Kafle and Huenerfauth, 2019; Kafle et al., 2019a) . Specifically, this prior work leveraged word2vec-based word embeddings to generate and propagate features to another layer of the network (Kafle and Huenerfauth, 2018) . We build on this prior work and propose an updated approach. The feature space we are using contains both contextual and semantic information of the captioned text, which is crucial in conversational setting, often common in TV, and may better capture long-distance semantic and syntactic relationships. Thus, in this work, we contribute more current strategies for calculating importance of words in transcript text, toward a metric that takes word-importance into account when evaluating captions. Our contributions in this paper include: 1. We conducted a comparative correlation analysis between human-annotated impor-tance scores for words in conversational transcripts and aggregated lexical semantic score generated from: (a) word2vecbased word embeddings as in prior work contrasted with (b) BERT-based contextualized embeddings. Our findings revealed that scores generated from contextualized embeddings had higher correlation with the humanannotated word-importance scores. 2. We contribute data consisting of BERT contextualized word embeddings, paired with their word-importance scores, to augment a prior dataset of human-assigned importance scores for words in conversational transcripts (Kafle and Huenerfauth, 2018) . This enhanced data can be used by researchers for constructing improved caption-evaluation metrics or by researchers studying conversational discourse. 3. To illustrate the use of this dataset, we show how interpretable classical machinelearning models can be trained to determine the importance of words using these contextualized word embedding vectors from our data. In this proof-of-concept study, we show how these data can be used in training models. We leave detailed evaluation and comparison of models for future work. 2 Related Work Word Importance Prediction NLP researchers have explored approaches to determine word-importance for various downstream tasks, e.g. term weight determination when querying text (Dai and Callan, 2020) , for text summarization (Hong and Nenkova, 2014) or text classification (Sheikh et al., 2016) . Prior research on identifying and scoring important words in a text has largely focused on the task of keyword or important-term extraction (Dai and Callan, 2020; Sheikh et al., 2016) . This task involves identifying words in a document that densely summarize it. Several automatic keyword-extraction techniques have been investigated, including unsupervised methods such as interpolation of Term Frequency and Inverse Document Frequency (TF-IDF) weighting (Sammut and Webb, 2010), Positive Pointwise Mutual Information (PPMI) (Bouma, 2009) , word2vec embedding (Sheikh et al., 2016) , and supervised methods that leverage linguistic features from text for word importance estimation (Dai and Callan, 2020; Kafle and Huenerfauth, 2018) . While the conceptualization of word importance as a keyword-extraction problem has enabled retrieving relevant information from large textual or multimedia datasets (Dai and Callan, 2020; Shah and Bhattacharyya) , this approach may not generalize across domains and functional, situational contexts of language use. For instance, given the meandering nature of topic transitions in television news broadcasts or talk shows (Kafle and Huenerfauth, 2019) , when processing caption transcripts, a model of word importance that is more local may be more successful, rather than considering the entire transcript of the broadcast or show. Caption Evaluation Methods Several caption evaluation approaches have been proposed (Ali and Renals, 2018; Apone et al., 2011) , with some approaches specifically taking into account the perspective of DHH participants (Kafle and Huenerfauth, 2018; Amin et al., 2021b) . The most common caption evaluation used by different regulatory organizations is Word Error Rate (WER) (Ali and Renals, 2018) . While penalizing insertion, deletion, and substitution errors in transcripts, a limitation of WER is that it considers importance of each word token equally. To address this, Apone et al. (2011) proposed a metric that assign weights to words in a text, but this probabilistic approach has not been trained on weights set to address priorities assigned by actual caption users. In the most closely related work, Kafle and Huenerfauth (2018) investigated models for predicting word-importance during captioned one-onone conversations. Their Automatic Caption Evaluation (ACE) framework utilized a variety of linguistic features to predict which words in a caption text were most important to its meaning, and which would be most problematic if incorrectly transcribed in a caption. Prior research on determining the importance of a word in a document had shown that an embedding can characterize a word's syntactic (e.g., word dependencies) and semantic character (e.g., named entity labeling), which in turn can help estimate a word's importance (Sheikh et al., 2016) . Thus, Kafle and Huenerfauth (2018) used word2vec embeddings of words in the transcript. In this paper, we examine whether an alter-native embedding, based on BERT, would lead to superior models of word-importance. Annotation of Word Importance Scores In this work, we contribute a dataset that augments a previously-released dataset from Kafle and Huenerfauth (2018) , consisting of a 25,000-token subset of the Switchboard corpus of conversational transcripts (Godfrey et al., 1992) . Kafle and Huenerfauth (2018) asked a pair of human annotators to assign word-importance scores to each word within these transcripts, on a range from 0.0 to 1.0, where 1.0 was most important. After partitioning scores into 6 discrete categories: [0-0.1), [0.1-0.3), [0.3-0.5), [0.5-0.7), [0.7-0.9), and [0.9 -1], they trained a Neural Network-based classifier, using Long Short Term Memory (LSTM), to predict the importance category of each word in these transcripts. We augment this annotated corpus with recent contextualized word embeddings from BERT (Devlin et al., 2019) , pairing up the embeddings with the hand-annotated word importance data. Corpus Augmentation Extracting Word Embeddings Vectors We have augmented the dataset described above, and will be releasing the version that includes two embeddings per word token: BERT contextualized word embeddings and word2vec embeddings. With this paper, we will be releasing the BERT-generated contextualized word embeddings 1 of 25,000 tokens, each with a feature vector of length 768, augmented with the human-annotated word-importance scores 2 . To enable comparison with the work of Kafle and Huenerfauth (2018) , we extracted a word2vec (Rehurek and Sojka, 2011) embedding vector of length 100 for each word that occurred at least twice within each transcript. Next, we employed the pretrained BERT model entitled bert-base-uncased (Devlin et al., 2019) to generate a contextualized word-embedding vector for each word within transcripts. For each word within each sentence, using BERT, we generated a three-dimensional embedding of shape 32 \u00d7 12 \u00d7 768. These embeddings were created based upon the architecture of the pretrained BERT model that included 32 transformer blocks, 12 attention heads and 768 hidden layers. We follow prior work that has reshaped or composed the three dimensions into a one-dimensional vector while retaining similar semantic information (Turton et al., 2020) . After performing these operations, for each word we obtained a contextualized embedding vector of length 768. have been excerpted from one transcript. The humanassigned importance of these importance score are 0.60, 0.40, and 0.70. For noise and plan, aggregated scores generated from word2vec-based embedding are 0.17 and 0.18, which does not belong to the same importance categories annotated. On the contrary, Bert-based embedding generates a score that aligns with humanassigned importance for noise and plan. However, for sunday, the word2vec-based semantic score is relatively closer to the actual importance score than BERT-based embedding. In fact, sunday appears as an isolated response to someone's question in transcript. Correlation Analysis to Assess Fit with Word Importance Scores After calculating two types of embeddings for each word in this dataset, we asked which one would be more useful within a model to predict word importance. Prior work on the state-of-art word-importance learning algorithm Neural Bagof-Words (NBOW) has revealed that learning importance of words within a sentence is effective while using the mean of each word-embedding vector as a feature (Sheikh et al., 2016) . Following this common practice for determining word importance (Kalchbrenner et al., 2014; Dai and Callan, 2020) , we calculated the mean of each word-embedding vector, to represent its word semantic score (Sheikh et al., 2016) . For both the word2vec and BERTbased embeddings, for each sentence in the transcript, we normalized word-semantic scores within the sentence, to obtain a value in a [0,1] range for each word. BERT embeddings produce sub-word tokens for a complete word and to handle such a scenario we have computed the average of the subwords to calculate the final composite semantic score. After performing this operation across sentences in the transcripts, we conducted an analysis to determine which form of pre-trained embedding (word2vec or BERT) better correlated with humanproduced annotations of word importance in the original dataset. The values based on word2vec were correlated with human annotations with a Pearson correlation coefficient of r = 0.30, and for the BERT-based scores, the coefficient was r = 0.41. A Fisher z-transformation (Upton and Cook, 2014) revealed that word semantic scores generated using BERT contextualized word embeddings were significantly better correlated (z = \u22123.05, p < 0.001) with human-assigned scores than word2vec counterparts. Based on these findings, we decided to use BERT contextualized embeddings in continued analysis. We also tried another traditional approach called TF-IDF to calculate a semantic score for words. A correlation analysis between the score generated by TF-IDF and human annotations resulted in a Pearson correlation coefficient of r = 0.25, which was lower than the coefficient generated using word2vec word embedding. Predicting Word Importance To demonstrate how to use our dataset to predict the importance of each word, we have begun to investigate several supervised learning methods. The independent variable is the processed 768 \u00d7 1 BERT-embedding vector of each word, and the output variable is the human-labeled importance score, discretized into six classes, for each word in the dataset. This classification experiment partitioned the corpus into 80% training, 10% development, and 10% test set. This partition has been directly adapted from (Kafle and Huenerfauth, 2018) . We evaluated the model using two measures: (i) Root Mean Square Error (RMSE) -the deviation of the model predictions from the human-assigned categories, and (ii) the F1 measure for classification performance. For classification, we categorized annotation scores into the 6 levels, as described above: [0-0.1), [0.1-0.3), [0.3-0.5), [0.5-0.7), [0.7-0.9), and [0.9 -1]. Table 2 illustrates that the better performing supervised model (of four traditional approaches) in predicting the importance class is Logistic Regression with F1-score 0.57 and RMSE 0.92. Even if the classes are discretized, we are generating continuous value for each word. And since both the human and supervised model generated scores, we calculated this RMSE. Among other approaches, the Linear Support Vector Classifier achieves F1score 0.51, Random-Forest achieves 0.25, and Multi-layer Perceptron achieves 0.10. Limitations and Future Work There are several limitations of this ongoing research that we intend to address in future work. \u2022 In our current research, we have determined a semantic score for each word using three methods. Future research can use other methods to generate the semantic score and retrospectively compare the generated semantic score with the score assigned by the human annotators. \u2022 The findings from this analysis leaves the room for future improvements, since we did not modify the hyperparameters to observe how accurately the models would predict the importance of words. Therefore, future research can explore variations of these models. Conclusion The analysis presented above has revealed that BERT contextualized word-embedding can better represent the importance of words compared to word2vec embeddings, which had been used in prior work on word-importance prediction (Kafle and Huenerfauth, 2019) . Research indicates that DHH viewers often follow key terms while skimming through captions, and researchers have proposed approaches to guide DHH readers to quickly identify keywords in caption text through visual highlighting (Kafle et al., 2019b) . Our findings may allow broadcasters to use embeddings to determine the important words within a sentence and to highlight those words in captions, support DHH viewers' ability to read (Amin et al., 2021a) the captions effectively. In this study, a traditional Logistic Regression algorithm performed better at predicting importance classes. We are also broadly investigating how to accurately measure the quality of caption transcriptions that are broadcast during live TV programs from the perspective of DHH viewers. We plan to incorporate predictive models into new word-importance weighted metrics, to better capture the usability of live captioning from DHH users' perspective. Ethics Statement This work advocates for improved inclusion of DHH individuals. A risk of the study is that results may not generalize across conversational corpora. Acknowledgments This material is based on work supported by the Department of Health and Human Services under Award No. 90DPCP0002-0100, and by the National Science Foundation under Award No. DGE-2125362. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Department of Health and Human Services or National Science Foundation.",
    "abstract": "Deaf and hard of hearing individuals regularly rely on captioning while watching live TV. Live TV captioning is evaluated by regulatory agencies using various caption evaluation metrics. However, caption evaluation metrics are often not informed by preferences of DHH users or how meaningful the captions are. There is a need to construct caption evaluation metrics that take the relative importance of words in a transcript into account. We conducted correlation analysis between two types of word embeddings and human-annotated labeled wordimportance scores in existing corpus. We found that normalized contextualized word embeddings generated using BERT correlated better with manually annotated importance scores than word2vec-based word embeddings. We make available a pairing of word embeddings and their human-annotated importance scores. We also provide proof-of-concept utility by training word importance models, achieving an F1-score of 0.57 in the 6-class word importance classification task.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Using {BERT} Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users"
}