{
    "article": "PARSEVAL, the default paradigm for evaluating constituency parsers, calculates parsing success (Precision/Recall) as a function of the number of matching labeled brackets across the test set. Nodes in constituency trees, however, are connected together to reflect important linguistic relations such as predicate-argument and direct-dominance relations between categories. In this paper, we present FREVAL, a generalization of PARSEVAL, where the precision and recall are calculated not only for individual brackets, but also for co-occurring, connected brackets (i.e. fragments). FREVAL fragments precision (FLP) and recall (FLR) interpolate the match across the whole spectrum of fragment sizes ranging from those consisting of individual nodes (labeled brackets) to those consisting of full parse trees. We provide evidence that FREVAL is informative for inspecting relative parser performance by comparing a range of existing parsers. Current approaches to parsing usually employ a training treebank to learn a statistical parser. The goal of learning is to obtain a parser that can reproduce the test set treebank parses as accurately as possible. The rationale behind this is that the treebank parses themselves are products of trained human annotators and, hence, should serve as the gold standard. If indeed the ultimate goal of learning statistical parsers from treebanks is to obtain parsers that immitate human parsing capability as represented by a sample in a treebank, then parser evaluation should aim at measuring the amount match/mismatch between a parse produced by a parser and a parse produced by human annotation. It is crucial at this point to highlight the difference between this view of parser evaluation and a linguistically-oriented point of view: a linguistically-motivated parser evaluation focuses on linguistically-relevant aspects of parse trees that are crucial for subsequent linguistic processing, e.g., dependency relations might be very important for semantic or other linguistic processing (cf. alternative linguistically relevant proposals (Sampson and Babarczy, 2003; Carroll et al., 1998) ). The contrast between linguistic relevance and the statistical view of treebank parsing as a learning problem (with some cognitive relevance) is crucial, because parser output often has other practical uses besides serving as mere input for subsequent linguistic processing, e.g., parsers may serve as target language models in machine translation systems. Consequently, to evaluate a statistical parser learned from a treebank we need a measure of similarity between its output parse and the human annotated parse in the test set. Such a measure of similarity between two tre could measure different shared aspects between two trees. The PARSEVAL measures (Black et al., 1991) are currently the de facto standard for evaluating (English) parser output. To calculate the Precision and Recall, the output trees of a parser are compared to a gold standard, i.e. human-annotated trees in a treebank. A well known treebank in this respect is the Penn Wall Street Journal treebank (Marcus et al., 1993) . To facilitate comparison among different parsers, it is common practice to test a parser on section 23 of that corpus and report PARSEVAL F-scores. PARSEVAL counts how many individual brackets match between a test-tree and a gold-tree, and also whether the test-tree was a complete match or not. However, what PARSEVAL does not count, for example, is whether the matching brackets together constitute a connected unit (e.g. a subtree or paths of direct-dominance relations). Consider for example the following trees: The trees differ in a single node labeled VP vs XP. This label change ruins the relation of VP with its VBZ verb and object NP, with its parent S and finally, this ruins the subject-verb structure (S (NP VP)). Consequently, different parsers may report very close F-scores coming from completely different parse trees, some of which might be more useful than others. In this paper we exploit a more elaborate measure of similarity between two trees as the basis for a new parser evaluation measure called FREVAL. FREVAL is a generalization of PARSEVAL from individual nodes to arbitrary size fragments, i.e., subtrees defined as connected non-empty subgraphs of a tree. FREVAL computes its final precision (and recall) as a mixture of the individually computed precisions (and recalls) for each of the fragment granularity levels. By employing a mixture of evaluation measures of a range of fragment sizes, FREVAL allows discriminating between parsers performing closely under PARSEVAL but otherwise having completely different kinds of output. As well as subtrees, FREVAL considers paths and parent-child relations also as fragments, thereby accommodating certain as-pects of leaf-ancestor (Sampson and Babarczy, 2003) and dependency (Carroll et al., 1998) proposals. Interestingly, fragment mixtures has been exploited in statistical parsing, e.g., (Bod et al., 2003; Sima'an, 2000; Bansal and Klein, 2010) , but never before for parser evaluation as far as we know. Preliminaries We start with an overview of PARSEVAL. We assume a test set consisting of sentences {U 1 , U 2 , . . . , U n } and their corresponding gold-standard trees T C = {\u03c4 1 C , \u03c4 2 C , . . . , \u03c4 n C }. Now, let the parser output be a set of 'guessed' trees T g = {\u03c4 1 g , \u03c4 2 g , . . . , \u03c4 n g }. More accurately, \u03c4 i C and \u03c4 i g denote the correct and the 'guessed' tree for sentence U i , respectively. PARSEVAL can be seen to represent a tree \u03c4 as a set of labeled constituents: Tree(\u03c4 ) = { i, X, j | i, X, j \u2208 \u03c4 } where i, X, j stands for a constituent in \u03c4 that covers span i to j with label X. |Tree(\u03c4 )| is the cardinality of the set, in this case the number of brackets/constituents. The PARSEVAL (Labeled Recall, Precision, and Exact Match) are as follows: LR(T C , T g ) def = i |Tree(\u03c4 i C ) \u2229 Tree(\u03c4 i g )| i |Tree(\u03c4 i C )| LP (T C , T g ) def = i |Tree(\u03c4 i C ) \u2229 Tree(\u03c4 i g )| i |Tree(\u03c4 i g | EM (T C , T g ) def = i \u03b4(T i C , T i g ) n where \u03b4 is the Kronecker delta function, returning 1 if the specified trees are equal and 0 otherwise. FREVAL: Beyond Sets of Constituents We introduce a new representation of trees in terms of their fragments. Let max = |\u03c4 | denote the number of nodes in a tree \u03c4 . A tree \u03c4 is represented by a sequence of sets of situated fragments Tree 1 (\u03c4 ), Tree 2 (\u03c4 ), . . . , Tree max (\u03c4 ) where for every 1 \u2264 s \u2264 max, we define Tree s (\u03c4 ) as the set of all situated fragments \u03d5 in \u03c4 of size |\u03d5| = s. A situated fragment i, \u03d5, j is a fragment \u03d5 together with the span span(\u03d5) = i, j that \u03d5 covers. More formally, Tree s (\u03c4 ) def = { i, \u03d5, j | fragment(\u03d5, \u03c4 ) \u2227 |\u03d5| = s \u2227 span(\u03d5) = i, j } Where fragment(\u03d5, \u03c4 ) is True iff \u03d5 is a fragment of \u03c4 , i.e., a non-empty, connected subgraphs of \u03c4 . Note here that we maintain for every fragment size s a separate set Tree s (\u03c4 ) of situated fragments, i.e., we do not put together fragments of different sizes. This is crucial next because we will calculate over the whole test set a separate precision/recall for each fragments size separately. Had we not done so, the counts of larger fragments would dominate the final precision and recall figures because the number of fragements of a certain size in a tree could be exponential in the number of nodes in the tree. With this new representation of trees in place, now we define for every fragment size s a separate Labeled Precision (LP s ) and Labeled Recall (LR s ): LP s (T C , T g ) = i |Tree s (\u03c4 i C ) \u2229 Tree s (\u03c4 i g )| i |Tree s (\u03c4 i g )| LR s (T C , T g ) = i |Tree s (\u03c4 i C ) \u2229 Tree s (\u03c4 i g )| i |Tree s (\u03c4 i C )| The Fragment Labeled Recall (FLR) and Fragment Labeled Precision (FLP) are defined as a linear interpolation over the sequence of different fragment sizes: 1 FLR = s \u03b1 s \u00d7 LR s FLP = s \u03b1 s \u00d7 LP s where \u03b1 s fulfills s \u03b1 s = 1.0. If we set \u03b1 1 = 1 we would obtain standard PARSEVAL LP and LR. And when \u03b1 max = 1.0 this is the Exact Match for the largest trees in the treebank. Hence, FREVAL is the mean of all measures between these two extremes. In the lack of preference for certain fragments sizes over others, we choose to set \u03b1 s uniformly over all fragment sizes. Another reasonable setting for \u03b1 s could be one that takes the sparsity of the space of fragments of size s into account for smoothing the FREVAL outcomes for larger fragment sizes using results from smaller fragment sizes. The FREVAL F1 is defined F1 = 2\u00d7(FLR\u00d7FLP) FLR+FLP , but we also define F1 s values for every s using the corresponding LR s and LP s values. Empirical explorations Equipped with our new evaluation metric, we ran various popular and new parsers of English on section 23 of the Penn Wall Street Journal tree-bank (Marcus et al., 1993) . We cleaned up section 23 by (1) pruning traces subtrees (-NONE-), (2) removing numbers in labels (e.g., NP-2 or NP=2), (3) removing semantic tags (e.g., NP-SBJ), and finally by removing redundant rules (e.g., NP \u2192 NP). The tested parsers are Bansal and Klein (2010) with basic refinement (B&K (basic)), Bansal and Klein (2011) allfragments, shortest-derivation with richer annotations and state-splits (B&K (SDP)); the Berkeley Parser 2 (Petrov et al., 2006) ; the Charniak parser (Charniak and Johnson, 2005) 3 with and without Johnson reranking; the Collins parser (Collins, 1999) as implemented in (Bikel, 2004) 4 ; Double-DOP 5 (Sangati and Zuidema, 2011) ; and the Stanford Parser 6 (Klein and Manning, 2003) with PCFG and with Factored models. We ran the parsers using the models trained on WSJ sections 02-21. 7 Most parsers provided such a model out-of-the-box, except for the Bikel-Collins parser, which we trained ourselves on the same sections. We evaluated the output of the parsers with PARSEVAL (using the Evalb implementation of Sekine and Collins (1997) ) and FREVAL. Table 1 shows the FREVAL evaluation results for the tested parsers. We can choose to only evaluate up to a certain fragment size, which is reflected in the various columns. In the first column, the maximum fragment size is 1 -single nodes. Therefore, here FREVAL's results are identical to the results of PARSEVAL. 8 In the second column, FLR and FLP were calculated on fragments of size 1, 2, . . . , 15, and in the third column on fragments of size 1, 2, . . . , 25. Finally, in the fourth column all fragments are taken into account (in this case, 1, 2, . . . , 55). Interestingly, as we take bigger fragments into account the ranking of the parsers changes. For example, when evaluating with just single nodes the Berkeley parser outperforms B&K (basic), but when we also take larger fragments into account (all other columns) B&K (basic) has the upper hand. The same is true for Double-DOP, which is outperformed by Berkeley under PARSEVAL, but finally is on par with it when evaluating all fragments. q q q q q q q q q q q q q q q q q q q q q q q q q q q q F1 by Fragment Size q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q F1 by Fragment Size Fragment Size Figure 1 shows how these performance changes depend on the individual F 1 scores (of LR s and LP s ) for each fragment size. Intuitively, with uniform \u03b1, the parser with the largest \"area\" under the curve (sum) performs best. For fragments up to size 15, Charniak's reranking parser clearly scores highest. The Berkeley parser, though, scores high on fragment size 1, but starts losing to other parsers as we take larger fragments into account. Figure 2 magnifies the differences between the parsers: it plots for each parser the difference between its F 1 score and the Berkeley parser's corresponding score as a function of fragment size. Some parsers seem to have worse performance for smaller fragment sizes but improve considerably for larger fragment sizes (e.g., Bikel-Collins, Double-DOP, Stanford factored). Both versions of Charniak's parser as well as B&K (SDP) perform well across the whole range of fragment sizes, with the plot of the latter looking almost as a horizontal shift of that of the former. Discussion The FLR and FLP measures provide an interesting new perspective on parser performance. The Parser ranking, according to the highest FREVAL F1 score, may change along the Fragment size axes. It is easy to see that one node's mismatch can cause the mismatch of a whole lot of bigger fragments. For this very reason, there are hardly any matches for fragments of size 25 and bigger. Moreover, FREVAL, like PARSEVAL, can punish a parser severely for certain mistakes, e.g. attachment errors (see e.g. K\u00fcbler and Telljohann (2002) ). The tested parsers differ from one another in various ways. We concentrate on two particular axis of differences q q q q q q q q q q q q q q q q F1 by Fragment Size  treebank productions, leading to CFG and fragment models with horizontal Markovization. Comparing the two B&K versions (basic and SDP), the increase in FREVAL F1 scores as larger fragment sizes are included confirms the importance of category refinement; the same holds for the head-lexicalization of categories, where some of the best performing parsers are found (Charniak's, Bikel-Collins) ; and finally, we see that parsers using fragment models are performing increasingly well along the size line, most notably B&K (basic) already outperforms Berkeley parser for fragment size 1-15, 1-25 and for all sizes, whereas it is far less accurate than Berkeley according to PAR-SEVAL. And surprisingly, for all fragment sizes, we find that Double-DOP (a selected-fragments parser) performs as well as Berkeley. The mix of head-lexicalization/category refinement with all-fragment modeling B&K (SDP) provides for a parser that outperforms Charniak's (without reranking) for FREVAL values (1-15), (1-25) and all fragments, despite performing slightly less accurately according to PARSEVAL. Adding a fragment-based discriminative reranker on top of Charniak's arrives at the overall best results. Conclusion Where the original PARSEVAL measure only looks at individual nodes when matching two trees, we present FREVAL, which looks at all the situated fragments in those trees. This causes a radically more fine-grained analysis of the performance of existing parsers. By looking at increasingly larger situated fragments, FREVAL indeed shows what is inside the 'evaluation gap' between the original Precision and Recall scores on the one hand and the Complete Match score on the other. Furthermore, FREVAL helps explore the impact of the different kinds of techniques (CFG rules vs. all-fragments, and refined categories) at a variety of treebank linguistic units.",
    "abstract": "PARSEVAL, the default paradigm for evaluating constituency parsers, calculates parsing success (Precision/Recall) as a function of the number of matching labeled brackets across the test set. Nodes in constituency trees, however, are connected together to reflect important linguistic relations such as predicate-argument and direct-dominance relations between categories. In this paper, we present FREVAL, a generalization of PARSEVAL, where the precision and recall are calculated not only for individual brackets, but also for co-occurring, connected brackets (i.e. fragments). FREVAL fragments precision (FLP) and recall (FLR) interpolate the match across the whole spectrum of fragment sizes ranging from those consisting of individual nodes (labeled brackets) to those consisting of full parse trees. We provide evidence that FREVAL is informative for inspecting relative parser performance by comparing a range of existing parsers.",
    "countries": [
        "Netherlands"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "4",
    "year": "2014",
    "month": "May",
    "title": "All Fragments Count in Parser Evaluation"
}