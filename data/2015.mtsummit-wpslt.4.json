{
    "article": "In response-based structured prediction, instead of a gold-standard structure, the learner is given a response to a predicted structure from which a supervision signal for structured learning is extracted. Applied to statistical machine translation (SMT), different types of environments such as a downstream application, a professional translator, or an SMT user, may respond to predicted translations with a ranking, a correction, or an acceptance/rejection decision, respectively. We present algorithms and experiments that show that learning from responses alleviates the supervision problem and allows a direct optimization of SMT for tasks such as cross-lingual patent prior art retrieval, or translation of technical patent documents. Introduction Response-based learning describes a range of statistical learning methods that replace the fullinformation supervised learning scenario by extracting supervision signals from the response of an extrinsic environment to a predicted translation. Learning proceeds by \"trying out\" or \"grounding\" translations in a task that is external to translation itself, receiving a response from interacting in this task, and converting this response into a supervision signal for updating model parameters. We focus on response-based structured prediction that operates according to the following learning protocol: 1. Environment generates input structure x t 2. Learner predicts output structure y t 3. Environment generates response signal r t 4. Learner uses pair (x t , r t ) to update its prediction rule Clearly, the key advantage of response-based learning is to alleviate the supervision problem by a repeatable generate-and-test procedure where feedback is obtained from the environment. Such feedback is in general easier and less costly to obtain than full supervision information. Furthermore, learning from task-based responses has the effect of grounding the learning process in an extrinsic environment. In this paper, we will describe three different types of response signals that are elicitated by grounding SMT into three different environments, namely ranking responses elicited in crosslingual information retrieval (Section 2), correction responses elicited in form of post-edits by professional translators (Section 3), and acceptance/rejection responses elicited in personalized SMT (Section 4). Grounding SMT in Cross-Lingual Patent Retrieval The industry standard in cross-lingual information retrieval (CLIR) is to use established translation models for context-aware translation of query strings, effectively reducing the problem of CLIR to a pipeline of direct translation (DT) and monolingual retrieval (Chin et al., 2008) . Only recently, research approaches to probabilistic structured queries (PSQ) have been presented, that is, they include (weighted) translation alternatives into the query structure to allow a more generalized term matching (Ture et al., 2012a,b) . In both DT and PSQ, the integration of SMT remains agnostic about its use for CLIR, and is instead optimized to match fluent, human reference translations. In contrast, retrieval systems often use bag-of-word representations, stopword filtering, and stemming techniques during document scoring, and queries are rarely fluent, grammatical natural language queries (Downey et al., 2008) . Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or re-ranking (Nikoulina et al., 2012) or ranking (Sokolov et al., 2014) . The most direct integration of SMT and CLIR has been presented by Hieber and Riezler (2015) in an approach to Bag-of-Words Forced Decoding (BOW-FD) where IR features for words in the bag-of-words representation of documents force the SMT decoder to prefer relevant documents with high probability. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Full translation hypergraph. Response: BM25 scores of partial translation hypotheses. Learning: Jointly optimize SMT and IR feature weights by direct ranking optimization on relevant documents. The key advantage of this approach clearly is the exploitation of the full translation search space, which is made possible by decomposable IR features that relate partial translation hypotheses to documents in the retrieval collection. This in turn allows to use the SMT decoder directly in retrieval. The key modeling idea is to factorize the cross-lingual search problem of finding document d e given query q f as follows: P (d e |q f ) = h P (h|q f ) translation \u00d7 P (d e |h, q f ) retrieval Learning is done by a joint optimization of a score function for linear models F smt and F ir where score(q f , d e ) = max h e Fsmt(h,qe,q f )+Fir (h,de) . The parameters of this objective are optimized for ranking for given relevance ranked documents s.t.  English Patent CLIR data 1 and German-English Wikipedia CLIR data 2 . Both datasets were extracted automatically by using the citation graph in patents and Wikipedia to extract ranked relevance links. rank(d + , q) > rank(d \u2212 , q) \u21d0\u21d2 score(d + , q) > score(d \u2212 , q). For more information on the data, model, and learning of BOW-FD, see Hieber and Riezler (2015) . Learning SMT from Translator Post-Edits Recent research in computer-assisted translation (CAT) has shown that post-editing of machine translations by professional translators leads to improved productivity of translators, and to improved quality of final translations (Koehn, 2009; Garcia, 2011; Green et al., 2013) . This scenario can be turned on its head by focusing on the human post-editor supporting the SMT system, leading to a mutually beneficial cycle of human-assisted machine translation where the SMT system performs online learning from human post-edits. The goal is to improve translation consistency for a given document, and to offer the user the experience of a system that immediately learns from corrections (W\u00e4schle et al., 2013; Denkowski et al., 2014; Green et al., 2014) . Patent data are especially well-suited for an application of this scenario because of the high repetitivity of patent documents. W\u00e4schle et al. (2013) and Bertoldi et al. (2014) recently presented an application of human-assisted SMT to patent data and data from legal and IT domains. The crucial steps of the response-based online learning protocol are instantiated in online learning from post-edits as follows: Prediction: Most probable sentence translation. Response: User post-edit. Learning: Dynamically extend phrase table and language model; update feature weights by reranking. The key difference between online learning from post-edits instead of from reference translations is the dynamic extension of the phrase w = w + f (x t , y t ) \u2212 f (x t , \u0177). Table 2 shows experimental results for German-English patent data sampled from title, abstract and description sections from the PatTR corpus. 3 Significant gains in BLEU score can be obtained by reranking using a perceptron, or by updating phrase table (tm) and language model (lm). In combination, nearly 4 BLEU points improvement are achieved. For more information on data, model, and learning, see W\u00e4schle et al. (2013) and Bertoldi et al. (2014) . 4 Towards Personalized SMT: Learning from Partial User Feedback While the above described post-editing scenario is a big step towards high-quality humanassisted SMT, the high cost and effort of post-editing may dampen the excitement about this scenario. The question to ask is whether production-quality post-edits from professional translators are really necessary for human-assisted SMT, or whether weaker feedback from less specialized users might be sufficient for learning. Sokolov et al. (2014) recently addressed this question from a coactive learning perspective that provides a formal notion of feedback strength. They present a convergence analysis of online structured prediction algorithms that learn from feedback consisting of slight improvements over predicted translations, instead of optimal feedback consisting of full post-edits or gold-standard translations. A simulation experiment on news data confirmed this theoretical finding. This research implies that well-known online structured predictors can be used for learning from weak feedback, without changes to the algorithms. What is open for change is the strength of feedback, allowing \"light\" post-edits from non-professionals. An application of this scenario to online learning for patent translation is a desideratum for future work. We refer the reader to Sokolov et al. (2014) for more information on the theory and proofof-concept experiments in the coactive learning framework. Another attack at learning from weak feedback can be taken from the direction of bandit learning. 4 Learning from bandit feedback describes an online learning scenario, where on each of a sequence of rounds, a learning algorithm makes a prediction, and receives partial information in terms of feedback to a single predicted point. In difference to the full information Algorithm 1 Bandit Structured Prediction 1: Input: sequence of learning rates \u03b3 t 2: Initialize w 0 3: for t = 0, . . . , T do w t+1 = w t supervised scenario, the learner does not know what the correct prediction looks like, nor what would have happened if it had predicted differently. Applied to SMT, this means that the learning algorithm only has access to a 1\u2212BLEU loss evaluation of a predicted translation instead of obtaining a gold standard reference translation. Clearly, a one-shot user quality estimate of the predicted translation is easier and faster to obtain than light or full post-edits of predicted translations, or than reference translations generated from scratch. This framework can be seen as a first step towards personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structure. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Exploration/exploitation sampling of sentence translation. Response: User feedback on loss value of sampled translation. Learning: Stochastic update using unbiased estimate of gradient. Sokolov et al. (2015) presented two algorithms for online structured prediction in SMT from bandit feedback that implement these ideas as follows. Algorithm 1 optimizes an expected 1 \u2212 BLEU loss criterion (Och (2003) , Smith and Eisner (2006) , He and Deng (2012) , Auli et al. (2014) , Wuebker et al. (2015) , inter alia) by performing simultaneous exploration/exploitation by sampling translations from a Gibbs model (line 6) , and using the obtained user feedback (line 7) to perform an update in the negative direction of the instantaneous gradient (line 8). The second algorithm extends Yue and Joachims (2009)'s dueling bandits algorithm to a Structured Dueling Bandits algorithm. It compares a current weight vector w t with a neighboring point w t along a direction u t , performing exploration (controlled by \u03b4, line 5) by probing random directions, and exploitation (controlled by \u03b3, line 8) by taking a step into the winning direction. The comparison step in line 6 is adapted to structured prediction from the original algorithm by comparing the quality of w t and w t via an evaluation of the losses \u2206(\u0177 wt (x t )) and Sokolov et al. (2015) present an evaluation that follows the standard of simulating bandit feedback by evaluating task loss functions against gold standard structures without revealing them to the learner. Here the setup is a reranking approach to SMT domain adaptation where the k-best list of an out-of-domain model is re-ranked (without re-decoding) based on bandit feedback from in-domain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. The out-of-domain baseline SMT model is trained on 1.6 million parallel Europarl data and includes the English side of Europarl and in-domain NewsCommentary in the language model. The full-information in-domain SMT model gives an upper bound by MERT tuning the outof-domain model on in-domain development data. Learning under bandit feedback started at the learned weights of the out-of-domain median model. It uses the parallel NewsCommentary data to simulate bandit feedback, by evaluating the sampled translation against the gold standard reference using as loss function \u2206 a smoothed per-sentence 1 \u2212 BLEU (by flooring zero n-gram counts to 0.01). Table 3 shows the final results that were are obtained by online-to-batch conversion where the model trained for 100 epochs on in-domain training data is evaluated on a separate indomain test set. Results for Bandit Structured Prediction and Dueling Bandits are very close, however, both are significant improvements over the out-of-domain SMT model that even includes an in-domain language model. The range of possible improvements is given by the difference of the BLEU sore of the in-domain model and the BLEU score of the out-of-domain model -nearly 3 BLEU points. Bandit learning can improve the out-of-domain baseline by 1.26 BLEU points (Bandit Structured Prediction) and by 1.52 BLEU points (Dueling Bandits). Clearly, a comparison between Bandit Structured Prediction and Dueling Bandits is skewed towards the latter approach that has access to two-point feedback instead of one-point feedback as in the former case. It has been shown that querying the loss function at two points leads to convergence results that closely resemble bounds for the full information case (Agarwal et al., 2010) , however, such feedback is clearly twice as expensive and, depending on the application, might not be elicitable from users. We refer the reader to Sokolov et al. (2015) for more information on data, model, and experiments. Conclusion We presented a comprehensive perspective on machine learning approaches that attempt to replace the full information supervised scenario by a setup in which supervision signals are extracted from responses of an extrinsic environment to system predictions. The discussed types of responses ranged from rankings deduced from performance of translations in extrinsic tasks such as cross-lingual retrieval, to improvements of structures by post-edits of professional translators, to partial feedback consisting of mere assessments of the quality of the predicted translation. We showed the efficacy of response-based learning in several simulation experi-ments. Clearly, improvements over traditional full-information structured prediction cannot be expected from learning from such weaker types of feedback. Instead, the goal is to investigate learning situations in which full information is not available. Moreover, task-based feedback might be even preferable to independently created gold standard structures if the ultimate goal is improved performance of a translation-related extrinsic task. In future work, we would like to apply response-based learning from weak feedback to real-life interactive scenarios. The new challenges of future work will be an investigation of response signals that can be elicited from humans efficiently and reliably, and still are informative enough for learning in SMT. Acknowledgements This research was supported in part by DFG grant RI-2221/1-2 \"Weakly Supervised Learning of Cross-Lingual Systems\".",
    "abstract": "In response-based structured prediction, instead of a gold-standard structure, the learner is given a response to a predicted structure from which a supervision signal for structured learning is extracted. Applied to statistical machine translation (SMT), different types of environments such as a downstream application, a professional translator, or an SMT user, may respond to predicted translations with a ranking, a correction, or an acceptance/rejection decision, respectively. We present algorithms and experiments that show that learning from responses alleviates the supervision problem and allows a direct optimization of SMT for tasks such as cross-lingual patent prior art retrieval, or translation of technical patent documents.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English",
        "German"
    ],
    "numcitedby": "0",
    "year": "2015",
    "month": "October 30 {--} November 3",
    "title": "Response-based learning for patent translation"
}