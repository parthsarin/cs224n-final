{
    "article": "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entitypairs, which improves the performance on twohop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard. 1 * \u2020 Qingyu Tan is under the Joint PhD Program between Alibaba and National University of Singapore. Introduction The problem of document-level relation extraction 2 (DocRE) is highly important for information extraction and NLP research. The DocRE task aims to extract relations among multiple entities within a document. The DocRE task is more challenging than its sentence-level counterpart in the following aspects: (1) The complexity of DocRE increases quadratically with the number of entities. If a document contains n entities, classification decisions must be made on n(n \u2212 1) entity pairs and most of them do not contain any relation. (2) Aside from the imbalance of positive and negative examples, the distribution of relation types for the positive entity pairs is also highly imbalanced. Considering the DocRED (Yao et al., 2019) dataset as an example, there are 96 relation types in total, where the top 10 relations take up 59.4% of all the relation labels. This imbalance significantly increases the difficulty of the document-level RE task. Most existing approaches of DocRE leverage dependency information to construct a documentlevel graph (Zeng et al., 2021; Zeng et al., 2020) , and then use graph neural networks for reasoning. Another popular strand of this field uses transformer-only (Vaswani et al., 2017) architecture (Zhou et al., 2021; Xu et al., 2021; Zhang et al., 2021) . Such models are able to achieve state-of-theart performance without explicit graph reasoning, showing that pre-trained language models (PrLMs) are able to implicitly capture long-distance relationships. However, there are three limitations of the existing DocRE methods. Firstly, existing methods mainly focus on the syntactic features from PrLMs while neglecting the interactions between entity pairs. Zhang et al. (2021) and Li et al. (2021) have used CNN structure to encode the interaction between entity pairs, but CNN structure cannot capture all the elements within the two-hop reasoning paths. Secondly, there is no prior work that explicitly tackles the class-imbalance problem for DocRE. Existing works (Zhou et al., 2021; Zhang et al., 2021; Zeng et al., 2020) only focus on threshold learning for balancing the positive and negative examples, but the class-imbalance problem within positive examples is not addressed. Lastly, there are very few works discussing the method of adapting distantly supervised data for the DocRE task. Xu et al. (2021) has shown that distantly supervised data is able to improve the performance of document-level relation extraction. However, it only uses the distantly supervised data to pre-train the RE model in a naive manner. To overcome the limitations of existing works, we propose a semi-supervised learning framework for document-level relation extraction. Firstly, to improve the reasoning for two-hop relations, we propose to use an axial attention module as feature extractor. This module enables us to attend to elements that are within two-hop logical paths and capture the interdependency among the relation triplets. Secondly, we propose Adaptive Focal Loss to address the imbalanced label distribution problem. The proposed loss function encourages the long-tail classes to contribute more to the overall loss. Lastly, we use knowledge distillation to overcome the differences between the annotated data and the distantly supervised data. Specifically, we first train a teacher model with a small amount of human annotated data. The teacher model will then be used to generate predictions on a large amount of distantly supervised data. The generated predictions are used as soft labels for pre-training our student model. Finally, the pre-trained student model is further fine-tuned on the human annotated data. We conducted experiments on two datasets -the DocRED (Yao et al., 2019) dataset and the Ha-cRED (Cheng et al., 2021) dataset. Experimental results show that our model consistently outperforms competitive baselines. Moreover, our model significantly outperforms the existing state-of-theart SSAN-Adapt (Xu et al., 2021) on the DocRED leaderboard by 1.36 in F1 score and 1.46 in Ign_F1 score. 3 Besides, we provide a thorough ablation study and error analysis to identify the bottleneck of our method. Methodology Problem Formulation In this section, we describe the task formulation of document-level relation classification. Given a document D that contains a set of entities {e i } n i=1 , the document-level relation extraction task is to predict the relation types between entity pairs (e s , e o ) s,o\u2208{1...n},s\u0338 =o , where the subscripts of e s and e o refer to subject and object. The set of relations is defined as R \u222a {NR}, where NR stands for no relation. An entity may occur multiple times in a document, thus for each entity e i , there can be multiple mentions {m i j } Ne i j=1 . If no relation exists between the entities in the pair (e s , e o ), it will be labeled as NR. During test time, the relation labels for all entity pairs (e s , e o ) s,o\u2208{1...n},s\u0338 =o will be predicted. Essentially, this is a multi-label classification problem, as there can be multiple relations between e s and e o . Model Architecture As shown in Figure 1 , our semi-supervised learning framework mainly consists of three parts: (1) representation learning; (2) adaptive focal loss; and (3) knowledge distillation for distant supervision pretraining. For representation learning, we first extract the contextual representation for each entitypair by a pre-trained language model. The entity pair representations will be further enhanced by the axial attention module, which will encode the inter-dependent information between entity pairs. We then use a feedforward neural network (FFN) classifier to obtain the logits and compute their losses. We use our proposed adaptive focal loss to better learn from long-tail classes. Finally, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. Specifically, we train a teacher model with the annotated data and use its output as soft labels. We then pre-train a student model based on the soft labels and the distant labels. The pretrained student model will be fine-tuned again with the annotated data. We will describe the details for each part in the following sections. Representation Learning Entity Representation We use a pretrained language model as the encoder. For a document D of length l, we have D = [x t ] l t=1 , where x t is the word at location t. Following prior works for relation classification, we use special token markers to represent entities. The entity mentions will be marked by a special token \"*\" at the start and end position. We then use a pre-trained language model (PrLM) to obtain the contextualized embeddings H of this document. H = P rLM ([x 1 , ..., x l ]) = [h 1 , ..., h l ]) (1) where H \u2208 R l\u00d7d and d is the hidden dimension of the PrLM. If the document length exceeds the maximum position of the PrLM, the document will be encoded as multiple overlapping chunks, and the contextualized embeddings of the overlapping chunks will be averaged. We take the embedding of the special token \"*\" at the start of the mention as its embedding, which is denoted as h m j . Then, for each entity e i with mentions {m i j } Ne i j=1 , where N e i is the number of mentions for entity e i , its global representation is obtained by logsumexp pooling: h e i = log Ne i j=1 exp(h m j ) (2) where h e i \u2208 R d is the aggregated feature of e i . Context-enhanced Entity Representation As prior works (Xu et al., 2021; Peng et al., 2020) have shown that contextual information is crucial for the relation classification task, our model also adapts contextual pooling method from Zhou et al. (2021) . For each entity e i , we first aggregate the attention output for its mentions by mean pooling A e i = Ne i j=1 (a m j ) , where a m j \u2208 R H\u00d7l is the the self-attention weight at the position of mention m j , H is the number of attention heads, and l is the document length. Then the context query is calculated as: q (s,o) = H i=1 (A i es \u2022 A i eo ) (3) c (s,o) = H \u22ba q (s,o) (4) where A es \u2208 R H\u00d7l is the aggregated attention output for entity e s , likewise for e o . q (s,o) \u2208 R l is the mean-pooled attention weight for entity pair (e s , e o ) and H \u2208 R l\u00d7d is the contextual embedding of the whole document. Then the context vector c (s,o) \u2208 R d is fused with the entity representations. z s = tanh(W s h es + W c c (s,o) ) (5) where z s \u2208 R d is the context-enhanced representation of subject s for entity pair (e s , e o ). We obtain the object representation z o in the same manner. Entity Pair Representation Following Zhou et al. ( 2021 ), we use a grouped bilinear function for feature combination. The entity embedding z s will first will be split into k equal-sized groups, such that z s = [z 1 s , z 2 s , ..., z k s ]. We perform the same splitting for z o . The value g (s,o) i at each dimension of our entity pair representation is obtained by: g (s,o) i = k j=1 (z j\u22ba s W j g i z j o ) + b i g (s,o) = [g (s,o) 1 , g (s,o) 2 , ..., g (s,o) d ] (6) where W j g i \u2208 R d/k\u00d7d/k , for i = 1, ..., d, j = 1, ..., k, is the weight matrix for dimension i. b i is a scalar bias of dimension i. g (s,o) \u2208 R d is our final entity pair representation. For a given document D with n entities, we need to classify n(n \u2212 1) number of entity pair permutations. To help us encode all the entity pairs and their positions, we used an R n\u00d7n\u00d7d matrix G to represent all the entity pairs of document D, and the diagonal of the n \u00d7 n index is neglected during training and inference. Axial Attention-Enhanced Entity Pair Representation Instead of using only head and tail embedding for relation classification, we propose to use two-hop attention to encode the axial neighboring information of each entity pair (e s , e o ) representation. Although there are prior works that use Convolution Neural Networks (CNNs) to encode the neighbor information for relation classification (Zhang et al., 2021) , we believe that attending to the axial elements is more effective and intuitive. Given an n \u00d7 n entity table, for entity pair (e s , e o ), attending to its axial elements corresponds to attending to elements that are either (e s , e i ) or (e i , e o ). That is, if a two-hop relation (e s , e o ) can be dissected into a path (e s , e i ) and (e i , e o ), then the most informative neighbors for classifying (e s , e o ) are the one-hop candidates that share e s or e o with this entity pair. The axial attention is simply computed by self-attention along the height axis and the width axis, and each computation along the axes is followed by a residual connection. For the cell (e s , e o ), we have: r (s,o) w = r (s,o) h + p\u22081..n sof tmaxp(q T (s,o) k (s,p) )v (s,p) r (s,o) h = g (s,o) + p\u22081...n sof tmaxp(q T (s,o) k (p,o) )v (p,o) (7) where we denote query q (i,j) = W Q g (i,j) , key j) , and value v (i,j) = W V g (i,j) , which are all linear projections of the entity pair representation g at position (i, j). W Q \u2208 R d\u00d7d , W K \u2208 R d\u00d7d , and W V \u2208 R d\u00d7d are all learnable weight matrices. The output of the axial attention module is r k (i,j) = W K g (i, (s,o) w \u2208 R d . The sof tmax p function denotes a softmax function that applies to all possible p = (i, j) positions. The formulation of this mechanism resembles Wang et al. (2020) . However, our motivation is different, as Wang et al. (2020) aim to use this mechanism to reduce the computational complexity of semantic segmentation, whereas our motivation is to attend to the one-hop neighbors for the two-hop relation triplets. Adaptive Focal Loss Finally, we have a linear layer for predicting relations: l (s,o) = W l r (s,o) w + b l (8) where l (s,o) \u2208 R c denotes the output logits for all relations, W l \u2208 R d\u00d7c is the weight matrix that maps the relation embedding to the logit of each class and c is the number of classes. Our relation extraction problem is essentially a multi-label classification problem. Traditionally, binary cross-entropy (BCE) loss is used to tackle this problem. However, this method relies on a global probability threshold for inference. Recently Adaptive Thresholding Loss (ATL, Zhou et al., 2021) has been proposed for multi-label classification. Instead of using a global probability threshold for all examples, ATL introduced a special class T H as the adaptive threshold value for each example. For each entity pair (e s , e o ), the classes whose logits are larger than the T H class logit will be predicted as positive classes, and the rest will be predicted as negative classes. We propose Adaptive Focal Loss (AFL) as an enhancement to ATL for long-tail classes. Our loss consists of two parts, the first part is for positive classes and the second part is for negative classes. During training, the label space is divided into two subsets: positive class subset P T and negative class subset N T . The positive class subset P T contains the relations that exist in entity pair (e s , e o ), and if there is no relation between (e s , e o ), P T is empty (P T = \u2205). The negative subset N T , on the other hand, contains the relation classes that do not belong to the positive classes, N T = R \\ P T . The probability of each positive class is computed as: P (r i |e s , e o ) = exp(l (s,o) r i ) exp(l (s,o) r i ) + exp(l (s,o) T H ) (9) where the logit of r i is ranked with the logit of threshold class T H individually. This is different from the original ATL, where all positive logits are ranked together with a softmax function. For simplicity, P (r i |e s , e o ) is denoted as P (r i ) in this section, because we are only discussing (e s , e o ). For the negative classes, we use their logits to compute the probability of the T H class: P (rT H |es, eo) = exp(l (s,o) r T H ) r j \u2208N T \u222a{T H} exp(l (s,o) r j ) (10) Similarly, P (r T H |e s , e o ) is referred to as P (r T H ) in the remainder of this section. Since the distribution of the positive labels is highly imbalanced, we leverage the idea of focal loss (Lin et al., 2017) for balancing the logits of the positive classes. We have our loss function as: LRE = r i \u2208P T (1\u2212P (ri)) \u03b3 log(P (ri))+log(P (rT H )) (11) where \u03b3 is a hyper-parameter. Our loss is designed to focus more on the low-confidence classes. If P (r i ) is low, the loss contribution from the relevant class will be higher, which enables a better optimization for long-tail classes. Knowledge Distillation for Distant Supervison In this section, we describe how we utilize the distantly supervised data in a more effective manner. The distantly supervised data included in the Do-cRed dataset (Yao et al., 2019) was obtained by performing entity linking on the Wikidata Knowledge Base (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and the Wikipedia data dump. It is shown that pre-training from the distantly supervised data is beneficial for document-level relation extraction (Xu et al., 2021) . However, prior work only adapts the distantly supervised data in a naive manner. The key challenge for the distant supervision adaptation is to overcome the differences between probability distributions of the distantly supervised data and the human annotated data. We compare two strategies for adapting the distantly supervised data. Naive Adaptation Adopting from (Xu et al., 2021) , this method first pretrains the model with the distantly supervised data with the relation extraction loss L RE (Eqn. 11), and then the model is fine-tuned on the human-annotated data with the same objective. We denote this method as Naive Adaptation (NA). Knowledge Distillation To further utilize the annotated data, we use a relation classification model trained on the human-annotated data (#Train in Table 1) as the teacher model. The teacher model is used to generate soft labels on the distantly supervised data. Specifically, the distantly supervised data is fed into the teacher model and the predicted logits will be the soft labels used for training the student model. The student model has the same configuration as the teacher model, but is trained with two signals simultaneously. The first signal is the supervision from the hard labels of the distantly supervised data and the second is from the predicted soft labels. We denote the loss computed on the hard labels as L RE and the knowledge distillation loss computed on the soft labels as L KD . We use mean squared error (MSE) as the knowledge distillation loss function: L KD = M SE(l (s,o) S , l (s,o) T ) (12) where l (s,o) S denotes the predicted logits of the student model and l (s,o) T is the prediction of the teacher model. The student model is further fine-tuned with human-annotated data (#Train in Table 1 ) after it has been pre-trained on the distantly supervised data. The overall loss of pre-training with distantly supervised data is computed as: L = L KD + L RE (13) We denote this method as KD in our main experimental results section. Besides the MSE loss, we also compare different adaptation methods, such as KL-Divergence, in section 3.6. Experiments Statistics Implementation Details We implemented our model with the PyTorch version of the Huggingface Transformers (Wolf et al., 2020) . For experiments on DocRED, we experimented with Roberta-large (Liu et al., 2019) and Bert-base (Devlin et al., 2019) as our document encoder respectively. For experiments on HacRED, we use XLM-R base (Conneau et al., 2020) to 1e-6 and we train the model for 10 epochs. We performed grid search for \u03b3 \u2208 [0, 0.5, 1.0, 1.5, 2.0] and set it to 0.5. Our model is trained on a single NVIDIA V100 GPU with 32 GB memory. The main evaluation metrics are Ign_F1 and F1 score following Yao et al. (2019) , where Ign_F1 refers to the F1 score that ignores the triples that appear in the annotated training data. Compared Methods We The GAIN (Zeng et al., 2020) model adds a graph neural network on top of a pre-trained language model, constructs a document-level graph for each example, and uses the graphical structure to extract relations. SIRE (Zeng et al., 2021) uses two encoders for different types of relation -a sentencelevel encoder to extract intra-sentence relations and a document encoder to extract inter-sentence relations. ATLOP (Zhou et al., 2021) is purely based on the transformer architecture and a novel adaptive thresholding loss to deal with the multi-label problem for DocRE. Besides, it also fuses the contextual information with the aggregated attention weights for each entity. The DocuNet (Zhang et al., 2021) model treats the relation extraction task in a similar way as semantic segmentation in computer vision. We also conducted an experiment that pretrained the ATLOP-Rb-l model with distantly supervised data, as this model is the best model by our reproduction. Main Results Our main results for the DocRED dataset are shown in Table 2 . Knowledge distillation is able to significantly improve the performance of our model.  The experiment results for the HacRED dataset are shown in Table 3 . The main difference of our method with the ATLOP baseline is the Adaptive Focal Loss and the Axial Attention Module. Our proposed method is able to exceed the ATLOP baseline by 1.12 F1. Besides the performance of the models, it is worth noting that for each method, the absolute performance of HacRED is significantly higher than its performance on DocRED. This is counter-intuitive as HacRED focuses on the hard relations whereas DocRED is more general. This can be caused by the following: 1) The human annotated training instances of the HacRED dataset are significantly more than DocRED, leading to better generalization performance. 2) Even though HacRED claims it focuses on the hard cases for relation extraction, it only has 27 classes, and the relation type distribution within the HacRED dataset is more balanced. Ablation Study We first separate our label space into two subsets. The first subset consists of the 10 most frequent labels, accounting for 59.4% of the positive relations in the training data. The second subset is denoted as the long-tail labels, which includes the rest of the 86 relations (the total label space is 97 and there is one T H class). Since our Adaptive Focal loss function is mainly designed for improving the performance on the less frequent classes, we show the ablation study by frequent and long-tail classes in Table 4 . When we change the AFL loss to con- We also provide an ablation study on the multihop relations in Table 5 . We use the same evaluation method for multi-hop relations as Zeng et al. (2020) . This evaluation method ignores all the onehop relation triples. Our axial attention module effectively improves Infer-F1 by 1.67, while its improvement for overall performance is only 0.63. Comparison of Adaptation Methods In this section, we directly compare the knowledge adaptation methods on the development set of DocRED (Table 6 ). We mainly compare three methods for adaptation: 1) Naive Adaptation (NA), 2) KD KL knowledge distillation with the KL divergence loss and 3) KD M SE with mean squared error loss. The adaptation performance on the development set is positively correlated with the per- formance of downstream fine-tuning. In the distant adaptation setting, our best method KD M SE is able to outperform NA by 3.07 F1 and KD KL by 0.77 F1. Similar performance differences are observed in the continue-trained setting. Error Analysis Even though our final model significantly outperforms the previous state of the art on the Do-cRED leaderboard, the absolute performance of our model still does not match human performance. In this section, we provide a detailed error analysis of our model on the development set of DocRED.  We first construct the union of our model's predictions and the ground truth triples (without NR label). Then, we categorize the union into four categories: (1) Correct (C), where prediction triples are in the ground truth. ( 2 ) Wrong (W), where the predicted head entity and tail entity are in the ground truth but the predicted relation is wrong. (3) Missed (MS), where the model predicts no relation for a pair of head entity and tail entity with some relation in the ground truth. (4) More (MR), where the model predicts an extraneous relation for a pair of head entity and tail entity not related in the ground truth. From Table 7 , we observe that the error percentage of the W category is very small. This indicates that for a pair of head entity and tail entity with some relation in the ground truth, and when our model predicts that there is a relation between these two entities, it is able to predict the correct relation rather accurately. However, we observe that most of our errors are under the MR and MS categories, and their counts are about the same. To better understand the performance bottleneck of the document-level RE task, we evaluate our model on a simplified subtask (Table 8 ). This subtask is binary classification, i.e., to determine whether two entities are related or not, and it is denoted as Binary Labels. In this subtask, we only care about predicting correctly that there is some relation between a head entity and a tail entity, but not what the exact relation is among the 97 relation classes. The performance on this simplified task is 68.64 F1 score, which is only marginally higher than the original F1 score of 67.12. This may be due to incomplete annotation of the two document-level relation extraction datasets, and we will illustrate this hypothesis in Figure 2 In Figure 2 , we show an example document from the dev set of DocRED and its predictions. We observe that many triples in the MR category are factually correct. That is, some of the pairs of entities are truly related but are labeled as NR throughout the dataset. For instance, from the ground truth, we can see that Labour Party and Hol are all entities from country Norway. Similarly, Nordland and Vestv\u00e5g\u00f8y are all in Norway, but their relations with Norway are not present in the ground truth triples. Therefore, when our model predicts these triples, its performance would be unfairly penalized during evaluation. This observation indicates that there are some incomplete annotations in the DocRED dataset. However, this is not the focus of this paper and we would like to leave this as future work. Related Work Early works on relation extraction mainly focused on sentence-level RE (Zhang et al., 2017; Baldini Soares et al., 2019; Peng et al., 2020) . However, prior works have shown that a large number of relations can only be extracted from multiple sentences (Verga et al., 2018; Yao et al., 2019; Cheng et al., 2021) . Various methods have been proposed to tackle document-level relation extraction (DocRE). Graph neural networks (GNNs; Scarselli et al., 2008) have been widely used for the DocRE task. Quirk and Poon (2017) used words as nodes and dependency information as edges to construct document-level graphs. This graph will be used to extract features for each entity pair. Later works extended this idea by applying different GNN architectures (Peng et al., 2017; Verga et al., 2018; Christopoulou et al., 2019; Nan et al., 2020; Zhang et al., 2018; Zeng et al., 2020) . In particular, Nan et al. (2020) proposed the latent stucture refinement (LSR) model, which used structured attention to induce the document-level graph. Zeng et al. (2020) constructed the document-level graph by entity-mention nodes and sentence edges. Besides the graph-based methods, transformer-only architectures have also proven to be highly effective for the DocRE task (Tang et al., 2020; Zhou et al., 2021) . Specifically, Zhou et al. (2021) proposed adaptive thresholding loss to tackle the multi-label classification problem in DocRE. On the other hand, learning from distant supervision is another important problem for relation extraction. Qin et al. (2018) used generative adversarial training for selecting informative examples and Feng et al. (2018) used reinforcement learning to achieve the same goal. However, there are no existing works that jointly learn from annotated data and distant data. To this end, this paper is the first to overcome the differences between the human annotated and distantly supervised data. Moreover, this paper also tackles the under-explored class imbalance problem and the two-hop logical reasoning problem with novel solutions to the shortcomings of existing approaches. Conclusions In this paper, we have proposed a novel framework for document-level relation extraction, based on knowledge distillation, axial attention, and adaptive focal loss. Our proposed method is able to significantly outperform the previous state of the art on the DocRED leaderboard. Besides, we also conducted a thorough ablation study and error analysis to identify the bottleneck of the document-level relation extraction task. Acknowledgements We would like to thank the anonymous reviewers for their insightful feedback and comments.",
    "abstract": "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entitypairs, which improves the performance on twohop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard. 1 * \u2020 Qingyu Tan is under the Joint PhD Program between Alibaba and National University of Singapore.",
    "countries": [
        "Singapore"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "2",
    "year": "2022",
    "month": "May",
    "title": "Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation"
}