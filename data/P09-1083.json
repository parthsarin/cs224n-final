{
    "article": "Opinion Question Answering (Opinion QA), which aims to find the authors' sentimental opinions on a specific target, is more challenging than traditional factbased question answering problems. To extract the opinion oriented answers, we need to consider both topic relevance and opinion sentiment issues. Current solutions to this problem are mostly ad-hoc combinations of question topic information and opinion information. In this paper, we propose an Opinion PageRank model and an Opinion HITS model to fully explore the information from different relations among questions and answers, answers and answers, and topics and opinions. By fully exploiting these relations, the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set. A gain of over 10% in F scores is achieved as compared to many other systems. Introduction Question Answering (QA), which aims to provide answers to human-generated questions automatically, is an important research area in natural language processing (NLP) and much progress has been made on this topic in previous years. However, the objective of most state-of-the-art QA systems is to find answers to factual questions, such as \"What is the longest river in the United States?\" and \"Who is Andrew Carnegie?\" In fact, rather than factual information, people would also like to know about others' opinions, thoughts and feelings toward some specific objects, people and events. Some examples of these questions are: \"How is Bush's decision not to ratify the Kyoto Protocol looked upon by Japan and other US al-lies?\" (Stoyanov et al., 2005) and \"Why do people like Subway Sandwiches?\" from TAC 2008 (Dang, 2008) . Systems designed to deal with such questions are called opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005) , directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performances. Similar to other complex QA tasks (Chen et al., 2006; Cui et al., 2007) , the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination (Varma et al., 2008) or just filter out the candidate without matching the question sentiment (Stoyanov et al., 2005) . However, topic and opinion are not independent in reality. The opinion words are closely associated with their contexts. Another problem is that existing algorithms compute the score for each answer candidate individually, in other words, they do not consider the relations between answer candidates. The quality of a answer candidate is not only determined by the relevance to the question, but also by other candidates. For example, the good answer may be mentioned by many candidates. In this paper, we propose two models to address the above limitations of previous sentence ranking models. We incorporate both the topic relevance information and the opinion sentiment information into our sentence ranking procedure. Meanwhile, our sentence ranking models could naturally consider the relationships between different answer candidates. More specifically, our first model, called Opinion PageRank, incorporates opinion sentiment information into the graph model as a condition. The second model, called Opinion HITS model, considers the sentences as authorities and both question topic information and opinion sentiment information as hubs. The experiment results on the TAC QA data set demonstrate the effectiveness of the proposed Random Walk based methods. Our proposed method performs better than the best method in the TAC 2008 competition. The rest of this paper is organized as follows: Section 2 introduces some related works. We will discuss our proposed models in Section 3. In Section 4, we present an overview of our opinion QA system. The experiment results are shown in Section 5. Finally, Section 6 concludes this paper and provides possible directions for future work. Related Work Few previous studies have been done on opinion QA. To our best knowledge, (Stoyanov et al., 2005) first created an opinion QA corpus OpQA. They find that opinion QA is a more challenging task than factual question answering, and they point out that traditional fact-based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders. More recently, TAC 2008 QA track (evolved from TREC) focuses on finding answers to opinion questions (Dang, 2008) . Opinion questions retrieve sentences or passages as answers which are relevant for both question topic and question sentiment. Most TAC participants employ a strategy of calculating two types of scores for answer candidates, which are the topic score measure and the opinion score measure (the opinion information expressed in the answer candidate). However, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn't match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999) . These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008) . Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are looking for, thus they are limited to fact-based QA tasks. To solve the opinion QA problems, we need to consider both topic and sentiment in a non-trivial manner. Our Models for Opinion Sentence Ranking In this section, we formulate the opinion question answering problem as a topic and sentiment based sentence ranking task. In order to naturally integrate the topic and opinion information into the graph based sentence ranking framework, we propose two random walk based models for solving the problem, i.e. an Opinion PageRank model and an Opinion HITS model. Opinion PageRank Model In order to rank sentence for opinion question answering, two aspects should be taken into account. First, the answer candidate is relevant to the question topic; second, the answer candidate is suitable for question sentiment. Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005) . Given the set V s = {v i } containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence v i and sentence v j is induced from sentence similarity measure as follows: p(i \u2192 j) = f (i\u2192j) P |Vs| k=1 f (i\u2192k) , where f (i \u2192 j) represents the similarity between sentence v i and sentence v j , here is cosine similarity (Baeza-Yates and Ribeiro-Neto, 1999 ). We define f (i \u2192 i) = 0 to avoid self transition. Note that p(i \u2192 j) is usually not equal to p(j \u2192 i). We also compute the similarity rel(v i |q) of a sentence v i to the question topic q using the cosine measure. This relevance score is then normalized as follows to make the sum of all relevance values of the sentences equal to 1: rel \u2032 (v i |q) = rel(v i |q) P |Vs| k=1 rel(v k |q) . The saliency score Score(v i ) for sentence v i can be calculated by mixing topic relevance score and scores of all other sentences linked with it as follows: Score(v i ) = \u00b5 j =i Score(v j ) \u2022 p(j \u2192 i) + (1 \u2212 \u00b5)rel \u2032 (v i |q) , where \u00b5 is the damping factor as in the PageRank algorithm. The matrix form is: p = \u00b5 M T p + (1 \u2212 \u00b5) \u03b1, where p = [Score(v i )] |Vs|\u00d71 is the vector of saliency scores for the sentences; M = [p(i \u2192 j)] |Vs|\u00d7|Vs| is the graph with each entry corresponding to the transition probability; \u03b1 = [rel \u2032 (v i |q)] |Vs|\u00d71 is the vector containing the relevance scores of all the sentences to the question. The above process can be considered as a Markov chain by taking the sentences as the states and the corresponding transition matrix is given by A \u2032 = \u00b5 M T + (1 \u2212 \u00b5) e \u03b1 T . Considering Topics and Sentiments Together: In order to incorporate the opinion information and topic information for opinion sentence ranking in an unified framework, we propose an Opinion PageRank model (Figure 1 ) based on a two-layer link graph (Liu and Ma, 2005; Wan and Yang, 2008) . In our opinion PageRank model, the Formally, the new representation for the twolayer graph is denoted as G * = V s , V o , E ss , E so , where V s = {v i } is the set of sentences and V o = {o j } is the set of sentiment words representing the opinion information; E ss = {e ij |v i , v j \u2208 V s } corresponds to all links between sentences and E so = {e ij |v i \u2208 V s , o j \u2208 V o } corresponds to the opinion correlation between a sentence and the sentiment words. For further discussions, we let \u03c0(o j ) \u2208 [0, 1] denote the sentiment strength of word o j , and let \u03c9(v i , o j ) \u2208 [0, 1] denote the strength of the correlation between sentence v i and word o j . We incorporate the two factors into the transition probability from v i to v j and the new transition probability p(i \u2192 j|Op(v i ), Op(v j )) is defined as f (i\u2192j|Op(v i ),Op(v j )) P |Vs| k=1 f (i\u2192k|Op(v i ),Op(v k )) when f = 0, and defined as 0 otherwise, where Op(v i ) is denoted as the opinion information of sentence v i , and f (i \u2192 j|Op(v i ), Op(v j )) is the new similarity score between two sentences v i and v j , conditioned on the opinion information expressed by the sentiment words they contain. We propose to compute the conditional similarity score by linearly combining the scores conditioned on the source opinion (i.e. f (i \u2192 j|Op(v i ))) and the destination opinion (i.e. f (i \u2192 j|Op(v j ))) as follows: f (i \u2192 j|Op(vi), Op(vj )) = \u03bb \u2022 f (i \u2192 j|Op(vi)) + (1 \u2212 \u03bb) \u2022 f (i \u2192 j|Op(vj )) = \u03bb \u2022 X o k \u2208Op(v i ) f (i \u2192 j) \u2022 \u03c0(o k ) \u2022 \u03c9(o k , vi) + (1 \u2212 \u03bb) \u2022 X o k \u2032 \u2208Op(v j )) (i \u2192 j) \u2022 \u03c0(o k \u2032 ) \u2022 \u03c9(o k \u2032 , vj ) (1) where \u03bb \u2208 [0, 1] is the combination weight controlling the relative contributions from the source opinion and the destination opinion. In this study, for simplicity, we define \u03c0(o j ) as 1, if o j exists in the sentiment lexicon, otherwise 0. And \u03c9(v i , o j ) is described as an indicative function. In other words, if word o j appears in the sentence v i , \u03c9(v i , o j ) is equal to 1. Otherwise, its value is 0. Then the new row-normalized matrix M * is defined as follows: M * ij = p(i \u2192 j|Op(i), Opj). The final sentence score for Opinion PageRank model is then denoted by: Score (v i ) = \u00b5 \u2022 j =i Score(v j ) \u2022 M * ji + (1 \u2212 \u00b5) \u2022 rel \u2032 (s i |q). The matrix form is: p = \u00b5 M * T p + (1 \u2212 \u00b5) \u2022 \u03b1. The final transition matrix is then denoted as: A * = \u00b5 M * T +(1\u2212\u00b5) e \u03b1 Opinion HITS Model The word's sentiment score is fixed in Opinion PageRank. This may encounter problem when the sentiment score definition is not suitable for the specific question. We propose another opinion sentence ranking model based on the popular graph ranking algorithm HITS (Kleinberg, 1999) . This model can dynamically learn the word sentiment score towards a specific question. HITS algorithm distinguishes the hubs and authorities in the objects. A hub object has links to many authorities, and an authority object has high-quality content and there are many hubs linking to it. The hub scores and authority scores are computed in a recursive way. Our proposed opinion HITS algorithm contains three layers. The upper level contains all the sentiment words from a lexicon, which represent their opinion information. The lower level contains all the words, which represent their topic information. The middle level contains all the opinion sentences to be ranked. We consider both the opinion layer and topic layer as hubs and the sentences as authorities. Figure 2 gives the bipartite graph representation, where the upper opinion layer is merged with lower topic layer together as the hubs, and the middle sentence layer is considered as the authority. Formally, the representation for the bipartite graph is denoted as G # = V s , V o , V t , E so , E st , where V s = {v i } is the set of sentences. V o = {o j } is the set of all the sentiment words representing opinion information, V t = {t j } is the set of all the words representing topic information. Each edge e ij is associated with a weight ow ij denoting the strength of the relationship between the sentence v i and the opinion word o j . The weight ow ij is 1 if the sentence v i contains word o j , otherwise 0. E st denotes the relationship between sentence and topic word. Its weight tw ij is calculated by tf \u2022 idf (Otterbacher et al., 2005) . E so = {e ij |v i \u2208 V s , o j \u2208 V o } corresponds to the We define two matrixes O = (O ij ) |Vs|\u00d7|Vo| and T = (T ij ) |Vs|\u00d7|Vt| as follows, for O ij = ow ij , and if sentence i contains word j, therefore ow ij is assigned 1, otherwise ow ij is 0. (Otterbacher et al., 2005) . T ij = tw ij = tf j \u2022 idf j Our new opinion HITS model is different from the basic HITS algorithm in two aspects. First, we consider the topic relevance when computing the sentence authority score based on the topic hub level as follows: Auth sen (v i ) \u221d tw ij >0 tw ij \u2022 topic score(j)\u2022hub topic (j), where topic score(j) is empirically defined as 1, if the word j is in the topic set (we will discuss in next section), and 0.1 otherwise. Second, in our opinion HITS model, there are two aspects to boost the sentence authority score: we simultaneously consider both topic information and opinion information as hubs. The final scores for authority sentence, hub topic and hub opinion in our opinion HITS model are defined as: The matrix form is: Auth (n+1) sen (vi) = (2) \u03b3 \u2022 X tw ij >0 twij \u2022 topic score(j) \u2022 Hub (n) topic (tj) + (1 \u2212 \u03b3) \u2022 X ow ij >0 owij \u2022 Hub (n) opinion (oj ) Hub (n+1) topic (ti) = X tw ki >0 tw ki \u2022 Auth (n) sen (vi) (3) Hub (n+1) opinion (oi) = X ow ki >0 ow ki \u2022 Auth (n) sen (vi) (4) a (n+1) = \u03b3 \u2022 T \u2022 e \u2022 t T s \u2022 I \u2022 h (n) t + (1 \u2212 \u03b3) \u2022 O \u2022 h (n) o (5) h (n+1) t = T T \u2022 a (n) (6) h (n+1) o = O T \u2022 a (n) (7) where e is a |V t |\u00d71 vector with all elements equal to 1 and I is a |V t | \u00d7 |V t | identity matrix, t s = [topic score(j)] |Vt|\u00d71 is the score vector for topic words, a (n) = [Auth (n) sen (v i )] |Vs|\u00d71 is the vector authority scores for the sentence in the n th iteration, and the same as h (n) t = [Hub (n) topic (t j )] |Vt|\u00d71 , h (n) o = [Hub (n) opinion (t j )] |Vo|\u00d71 . In order to guarantee the convergence of the iterative form, authority score and hub score are normalized after each iteration. For computation of the final scores, the initial scores of all nodes, including sentences, topic words and opinion words, are set to 1 and the above iterative steps are used to compute the new scores until convergence. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any nodes falls below a given threshold (10e-6 in this study). We use the authority scores as the saliency scores in the Opinion HITS model. The sentences are then ranked by their saliency scores. System Description In this section, we introduce the opinion question answering system based on the proposed graph methods. Figure 3 shows five main modules: Question Analysis: It mainly includes two components. 1).Sentiment Classification: We classify all opinion questions into two categories: positive type or negative type. We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yu and Hatzivassiloglou, 2003) ). 2).Topic Set Expansion: The opinion question asks opinions about a particular target. Semantic role labeling based (Carreras and Marquez, 2005) and rule based techniques can be employed to extract this target as topic word. We also expand the topic word with several external knowledge bases: Since all the entity synonyms are redirected into the same page in Wikipedia (Rodrigo et al., 2007) , we collect these redirection synonym words to expand topic set. We also collect some related lists as topic words. For example, given question \"What reasons did people give for liking Ed Norton's movies?\", we collect all the Norton's movies from IMDB as this question's topic words. Document Retrieval: The PRISE search engine, supported by NIST (Dang, 2008) , is employed to retrieve the documents with topic word. Answer Candidate Extraction: We split retrieved documents into sentences, and extract sentences containing topic words. In order to improve recall, we carry out the following process to handle the problem of coreference resolution: We classify the topic word into four categories: male, female, group and other. Several pronouns are defined for each category, such as \"he\", \"him\", \"his\" for male category. If a sentence is determined to contain the topic word, and its next sentence contains the corresponding pronouns, then the next sentence is also extracted as an answer candidate, similar as (Chen et al., 2006) . Answer Ranking: The answer candidates are ranked by our proposed Opinion PageRank method or Opinion HITS method. Answer Selection by Removing Redundancy: We incrementally add the top ranked sentence into the answer set, if its cosine similarity with every extracted answer doesn't exceed a predefined threshold, until the number of selected sentence (here is 40) is reached. Experiments Experiment Step Dataset We employ the dataset from the TAC 2008 QA track. The task contains a total of 87 squishy opinion questions. 1 These questions have simple forms, and can be easily divided into positive type or negative type, for example \"Why do people like Mythbusters?\" and \"What were the specific actions or reasons given for a negative attitude towards Mahmoud Ahmadinejad?\". The initial topic word for each question (called target in TAC) is also provided. Since our work in this paper focuses on sentence ranking for opinion QA, these characteristics of TAC data make it easy to process question analysis. Answers for all questions must be retrieved from the TREC Blog06 collection (Craig Macdonald and Iadh Ounis, 2006) . The collection is a large sample of the blog sphere, crawled over an eleven-week period from December 6, 2005 until February 21, 2006. We retrieve the top 50 documents for each question. Evaluation Metrics We adopt the evaluation metrics used in the TAC squishy opinion QA task (Dang, 2008) . The TAC assessors create a list of acceptable information nuggets for each question. Each nugget will be assigned a normalized weight based on the number of assessors who judged it to be vital. We use these nuggets and corresponding weights to assess our approach. Three human assessors complete the evaluation process. Every question is scored using nugget recall (NR) and an approximation to nugget precision (NP) based on length. The final score will be calculated using F measure with TAC official value \u03b2 = 3 (Dang, 2008) . This means recall is 3 times as important as precision: F (\u03b2 = 3) = (3 2 + 1) \u2022 N P \u2022 N R 3 2 \u2022 N P + N R where N P is the sum of weights of nuggets returned in response over the total sum of weights of all nuggets in nugget list, and N P = 1 \u2212 (length \u2212 allowance)/(length) if length is no less than allowance and 0 otherwise. Here allowance = 100 \u00d7 (\u266fnuggets returned) and length equals to the number of non-white characters in strings. We will use average F Score to evaluate the performance for each system. Baseline The baseline combines the topic score and opinion score with a linear weight for each answer candidate, similar to the previous ad-hoc algorithms: final score = (1 \u2212 \u03b1) \u00d7 opinion score + \u03b1 \u00d7 topic score (8) 1 3 questions were dropped from the evaluation due to no correct answers found in the corpus The topic score is computed by the cosine similarity between question topic words and answer candidate. The opinion score is calculated using the number of opinion words normalized by the total number of words in candidate sentence. For lexicon-based opinion analysis, the selection of opinion thesaurus plays an important role in the final performance. HowNet 2 is a knowledge database of the Chinese language, and provides an online word list with tags of positive and negative polarity. We use the English translation of those sentiment words as the sentimental lexicon. Sen-tiWordNet (Esuli and Sebastiani, 2006) is another popular lexical resource for opinion mining. Table 1 shows the detail information of our used sentiment lexicons. In our models, the positive opinion words are used only for positive questions, and negative opinion words just for negative questions. We initially set parameter \u03bb in Opinion PageRank as 0 as (Liu and Ma, 2005) , and other parameters simply as 0.5, including \u00b5 in Opinion PageRank, \u03b3 in Opinion HITS, and \u03b1 in baseline. The experiment results are shown in Figure 4 . Performance Evaluation Performance on Sentimental Lexicons We can make three conclusions from Figure 4 : 1. Opinion PageRank and Opinion HITS are both effective. The best results of Opinion PageRank and Opinion HITS respectively achieve around 35.4% (0.199 vs 0.145), and 34.7% (0.195 vs 0.145) improvements in terms of F score over the best baseline result. We believe this is because our proposed models not only incorporate the topic information and opinion information, but also con- The experiment results demonstrate the effectiveness of these relations. 2. Opinion PageRank and Opinion HITS are comparable. Among five sentimental lexicons, Opinion PageRank achieves the best results when using HowNet and Union lexicons, and Opinion HITS achieves the best results using the other three lexicons. This may be because when the sentiment lexicon is defined appropriately for the specific question set, the opinion PageRank model performs better. While when the sentiment lexicon is not suitable for these questions, the opinion HITS model may dynamically learn a temporal sentiment lexicon and can yield a satisfied performance. 3. Hownet achieves the best overall performance among five sentiment lexicons. In HowNet, English translations of the Chinese sentiment words are annotated by nonnative speakers; hence most of them are common and popular terms, which maybe more suitable for the Blog environment (Zhang and Ye, 2008) . We will use HowNet as the sentiment thesaurus in the following experiments. In baseline, the parameter \u03b1 shows the relative contributions for topic score and opinion score. We vary \u03b1 from 0 to 1 with an interval of 0.1, and find that the best baseline result 0.170 is achieved when \u03b1=0.1. This is because the topic information has been considered during candidate extraction, the system considering more opinion information (lower \u03b1) achieves better. We will use this best result as baseline score in following experiments. Since F(3) score is more related with recall, F score and recall will be demonstrated. In the next two sections, we will present the performances of the parameters in each model. For simplicity, we denote Opinion PageRank as PR, Opinion HITS as HITS, baseline as Base, Recall as r, F score as F. Opinion PageRank Performance In Opinion PageRank model, the value \u03bb combines the source opinion and the destination opinion. Figure 5 shows the experiment results on parameter \u03bb. When we consider lower \u03bb, the system performs better. This demonstrates that the destination opinion score contributes more than source opinion score in this task. The value of \u00b5 is a trade-off between answer reinforcement relation and topic relation to calculate the scores of each node. For lower value of \u00b5, we give more importance to the relevance to the question than the similarity with other sentences. The experiment results are shown in Figure 6 . The best result is achieved when \u00b5 = 0.8. This figure also shows the importance of reinforcement between answer candidates. If we don't consider the sentence similarity(\u00b5 = 0), the performance drops significantly. Opinion HITS Performance The parameter \u03b3 combines the opinion hub score and topic hub score in the Opinion HITS model. The higher \u03b3 is, the more contribution is given Opinion HITS model ranks the sentences by authority scores. It can also rank the popular opinion words and popular topic words from the topic hub layer and opinion hub layer, towards a specific question. Take the question 1024.3 \"What reasons do people give for liking Zillow?\" as an example, its topic word is \"Zillow\", and its sentiment polarity is positive. Based on the final hub scores, the top 10 topic words and opinion words are shown as Table 2 .  Zillow is a real estate site for users to see the value of houses or homes. People like it because it is easily used, accurate and sometimes free. From the Table 2 , we can see that the top topic words are the most related with question topic, and the top opinion words are question-specific sentiment words, such as \"accurate\", \"easily\", \"free\", not just general opinion words, like \"great\", \"excellent\" and \"good\". Comparisons with TAC Systems We are also interested in the performance comparison with the systems in TAC QA 2008. From Conclusion and Future Works In this paper, we proposed two graph based sentence ranking methods for opinion question answering. Our models, called Opinion PageRank and Opinion HITS, could naturally incorporate topic relevance information and the opinion sentiment information. Furthermore, the relationships between different answer candidates can be considered. We demonstrate the usefulness of these relations through our experiments. The experiment results also show that our proposed methods outperform TAC 2008 QA Task top ranked systems by about 10% in terms of F score. Our random walk based graph methods integrate topic information and sentiment information in a unified framework. They are not limited to the sentence ranking for opinion question answering. They can be used in general opinion document search. Moreover, these models can be more generalized to the ranking task with two types of influencing factors. Acknowledgments: Special thanks to Derek Hao Hu and Qiang Yang for their valuable comments and great help on paper preparation. We also thank Hongning Wang, Min Zhang, Xiaojun Wan and the anonymous reviewers for their useful comments, and thank Hoa Trang Dang for providing the TAC evaluation results. The work was supported by 973 project in China(2007CB311003), NSFC project(60803075), Microsoft joint project \"Opinion Summarization toward Opinion Search\", and a grant from the International Development Research Center, Canada.",
    "abstract": "Opinion Question Answering (Opinion QA), which aims to find the authors' sentimental opinions on a specific target, is more challenging than traditional factbased question answering problems. To extract the opinion oriented answers, we need to consider both topic relevance and opinion sentiment issues. Current solutions to this problem are mostly ad-hoc combinations of question topic information and opinion information. In this paper, we propose an Opinion PageRank model and an Opinion HITS model to fully explore the information from different relations among questions and answers, answers and answers, and topics and opinions. By fully exploiting these relations, the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set. A gain of over 10% in F scores is achieved as compared to many other systems.",
    "countries": [
        "China"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "45",
    "year": "2009",
    "month": "August",
    "title": "Answering Opinion Questions with Random Walks on Graphs"
}