{
    "article": "We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions. Introduction One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring. Much previous work has been devoted to the automated scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013) , error detection and correction (Leacock et al., 2010) , and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012) . In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community. Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you separate the salt from the water? REF. ANS. The water was evaporated, leaving the salt. STUD. ANS. The water dried up and left the salt. Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder? REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the brown mineral will have a scratch. STUD. ANS. The harder will leave a scratch on the other. Figure 1 : Example questions and answers help a full dialog system to generate appropriate and effective feedback on errors. System designers typically create a repertoire of questions that the system can ask a student, together with reference answers (see Figure 1 for an example). For each student answer, the system needs to decide on the appropriate tutorial feedback, either confirming that the answer was correct, or providing additional help to indicate how the answer is flawed and help the student improve. This task requires semantic inference, for example, to detect when the student answers are explaining the same content but in different words, or when they are contradicting the reference answers. Recognizing Textual Entailment (RTE) is a series of highly successful challenges used to evaluate tasks related to semantic inference, held annually since 2005. Initial challenges used examples from information retrieval, question answering, machine translation and information extraction tasks (Dagan et al., 2006; Giampiccolo et al., 2008) . Later challenges started to explore the applicability and impact of RTE technology on specific application settings such as Summarization and Knowledge Base Population (Bentivogli et al., 2009; Bentivogli et al., 2010; Bentivogli et al., 2011) . The SRA Task offers a similar opportunity. We therefore organized a joint challenge at SemEval-2013, aiming to bring together the educational NLP and the semantic inference communities. The goal of the challenge is to compare approaches for student answer assessment and to evaluate the methods typically used in RTE on data from educational applications. We present the corpus used in the task (Section 2) and describe the Main task, including educational NLP and textual entailment perspectives and data set creation (Section 3). We discuss evaluation metrics and results in Section 4. Section 5 describes the Pilot task, including data set creation and evaluation results. Section 6 presents conclusions and future directions. Student Response Analysis Corpus We used the Student Response Analysis corpus (henceforth SRA corpus) (Dzikovska et al., 2012) as the basis for our data set creation. The corpus contains manually labeled student responses to explanation and definition questions typically seen in practice exercises, tests, or tutorial dialogue. Specifically, given a question, a known correct 'reference answer' and a 1-or 2-sentence 'student answer', each student answer in the corpus is labelled with one of the following judgments: \u2022 'Correct', if the student answer is a complete and correct paraphrase of the reference answer; \u2022 'Partially correct incomplete', if it is a partially correct answer containing some but not all information from the reference answer; \u2022 'Contradictory', if the student answer explicitly contradicts the reference answer; \u2022 'Irrelevant' if the student answer is talking about domain content but not providing the necessary information; \u2022 'Non domain' if the student utterance does not include domain content, e.g., \"I don't know\", \"what the book says\", \"you are stupid\". The SRA corpus consists of two distinct subsets: BEETLE data, based on transcripts of students interacting with BEETLE II tutorial dialogue system (Dzikovska et al., 2010) , and SCIENTSBANK data, based on the corpus of student answers to assessment questions collected by Nielsen et al. (2008b) . The BEETLE corpus consists of 56 questions in the basic electricity and electronics domain requiring 1-or 2-sentence answers, and approximately 3000 student answers to those questions. The SCI-ENTSBANK corpus contains approximately 10,000 answers to 197 assessment questions in 15 different science domains (after filtering, see Section 3.3) Student answers in the BEETLE corpus were manually labeled by trained human annotators using a scheme that straightforwardly mapped into SRA annotations. The annotations in the SCIENTSBANK corpus were converted into SRA labels from a substantially more fine-grained scheme by first automatically labeling them using a set of questionspecific heuristics and then manually revising them according to the class definitions (Dzikovska et al., 2012) . We further filtered and transformed the corpus to produce training and test data sets as discussed in the next section. Main Task Educational NLP perspective The 5-way SRA task focuses on associating student answers with categorical labels that can be used in providing tutoring feedback. Most NLP research on short answer scoring reports agreement with a numeric score (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler et al., 2011) , which is a potential contrast with our task. However, the majority of the NLP work makes use of underlying representations in terms of concepts, so the 5-way task is still likely to mesh well with the available technology. Research on tutorial dialog has emphasized generic methods that use latent semantic analysis or other machine learning methods to determine when text strings express similar concepts (Hu et al., 2003; Jordan et al., 2004; VanLehn et al., 2007; Mc-Carthy et al., 2008) . Most of these methods, like the NLP methods, (with the notable exception of (Nielsen et al., 2008a) ), are however strongly dependent on domain expertise for the definitions of the concepts. In educational applications, there would be great value in a system that could operate more or less unchanged across a range of domains and question-types, requiring only a question text and a reference answer supplied by the instructional designers. Thus, the 5-way classification task at Se-mEval was set up to evaluate the feasibility of such answer assessment, either by adapting the existing educational NLP methods to the categorical labeling task or by employing the RTE approaches. RTE perspective and 2-and 3-way Tasks According to the standard definition of Textual Entailment, given two text fragments called Text (T) and Hypothesis (H), it is said that T entails H if, typically, a human reading T would infer that H is most likely true (Dagan et al., 2006) . In a typical answer assessment scenario, we expect that a correct student answer would entail the reference answer, while an incorrect answer would not. However, students often skip details that are mentioned in the question or may be inferred from it, while reference answers often repeat or make explicit information that appears in or is implied from the question, as in Example 2 in Figure 1 . Hence, a more precise formulation of the task in this context considers the entailing text T as consisting of both the original question and the student answer, while H is the reference answer. We carried out a feasibility study to check how well the entailment judgments in this formulation align with the annotated response assessment, by annotating a sample of the data used in the SRA task with entailment judgments. We found that some answers labeled as \"correct\" implied inferred or assumed pieces of information not present in the text. These reflected the teachers' assessment of student understanding but would not be considered entailed from the traditional RTE perspective. However, we observed that in most such cases, a substantial part of the hypothesis was still implied by the text. Moreover, answers assigned labels other than \"correct\" were always judged as \"not entailed\". Overall, we concluded that the correlation between assessment judgments of the two types was sufficiently high to consider an RTE approach. The challenge for the textual entailment community was to address the answer assessment task at varying levels of granularity, using textual entailment techniques, and explore how well these techniques can help in this real-world educational setting. In order to make the setup more similar to pre-vious RTE tasks, we introduced 3-way and 2-way versions of the task. The data for those tasks were obtained by automatically collapsing the 5-way labels. In the 3-way task, the systems were required to classify the student answer as either (i) correct; (ii) contradictory; or (iii) incorrect (combining the categories partially correct but incomplete, irrelevant and not in the domain from the 5-way classification). In the two-way task, the systems were required to classify the student answer as either correct or incorrect (combining the categories contradictory and incorrect from the 3-way classification) Data Preparation and Training Data In preparation of the task four of the organizers examined all questions in the SRA corpus, and decided that to remove some of the questions to make the dataset more uniform. We observed two main issues. First, a number of questions relied on external material, e.g., charts and graphs. In some cases, the information in the reference answer was sufficient to make a reasonable assessment of student answer correctness, but in other cases the information contained in the questions was deemed insufficient and the questions were removed. Second, some questions in the SCIENTSBANK dataset could have multiple possible correct answers, e.g., a question asking for any example out of two or more unrelated possibilities. Such questions were also removed as they do not align well with the RTE perspective. Finally, parts of the data were re-checked for reliability. In BEETLE data, a second manual annotation pass was carried out on a subset of questions to check for consistency. In SCIENTSBANK, we manually re-checked the test data. The automatic conversion from the original SCIENTSBANK annotations into SRA labels was not perfectly accurate (Dzikovska et al., 2012) . We did not have the resources to check the entire data set. However, four of the organizers jointly hand-checked approximately 100 examples to establish consensus, and then one organizer hand-checked all of the test data set. Test Data We followed the evaluation methodology of Nielsen et al. (2008a) for creating the test data. Since our goal is to support systems that generalize across problems and domains (see Section 3.1), we created three distinct test sets: 1. Unseen answers (UA): a held-out set to assess system performance on the answers to questions contained in the training set (for which the system has seen example student answers). It was created by setting aside a subset if randomly selected learner answers to each question included in the training data set. 2. Unseen questions (UQ): a test set to assess system performance on responses to previously unseen questions but which still fall within the application domains represented in the training data. It was created by holding back all student answers to a subset of randomly selected questions in each dataset. 3. Unseen domains (UD): a domain-independent test set of responses to topics not seen in the training data, available only in the SCIENTS-BANK dataset. It was created by setting aside the complete set of questions and answers from three science modules from the fifteen modules in the SCIENTSBANK data. The final label distribution for train and test data is shown in Table 1 . Main Task Results Participants The participants were invited to submit up to three runs in any combination of the tasks. Nine teams participated in the main task, most choosing to attempt all subtasks (5-way, 3-way and 2-way), with 1 team entering only the 5-way and 1 team entering only the 2-way task. At least 6 (CNGL, CoMeT, CU, BIU, EHUALM, LIMSI) of the 9 systems used some form of syntactic processing, in most cases going beyond parts of speech to dependencies or constituency structure. CNGL emphasized this as an important aspect of the system. At least 5 (CoMeT, CU, EHUALM, ETS UKP) of the 9 systems used a system combination approach, with several components feeding into a final decision made by some form of stacked classifier. The majority of the systems used some kind of measure of text-to-text similarity, whether the inspiration was LSA, MT measures such as BLEU or in-house methods. These methods were emphasized as especially important by Celi, ETS and SOFTCARDINALITY. These impressions are based on short summaries sent to us by the participants prior to the availability of the full system descriptions. Check the individual system papers for detail. Evaluation Metrics For each evaluation data set (test set), we computed the per-class precision, recall and F 1 score. We also computed three main summary metrics: accuracy, macro-average F 1 and weighted average F 1 . Accuracy is the overall percentage of correctly classified examples. Macroaverage is the average value of each metric (precision, recall, F 1 ) across classes, without taking class size into account. It is defined as 1/N c c metric(c), where N c is the number of classes (2, 3, or 5 depending on the task). Note that in the 5-way SCIENTSBANK dataset the 'nondomain' class is severely underrepresented, with only 23 examples out of 4335 total (see Table 1 ). Therefore, we calculated macro-averaged P/R/F 1 over only 4 classes (i.e. excluding the 'non-domain' class) for SCIENTSBANK 5-way data. Weighted Average (or simply weighted) is the average value for each metric weighted by class size, defined as 1/N c |c| * metric(c) where N is the total number of test items and |c| is the number of items labeled as c in gold-standard data. 1 1 This metric is called microaverage in (Dzikovska et al., 2012) . However, microaverage is used to define a different metric in tasks where more than one label can be associated with each data item (Tsoumakas et al., 2010) . therefore, we use weighted average to match the terminology used by the Weka toolkit. The micro-average precision, recall and F1 computed In general, macro-averaging favors systems that perform well across all classes regardless of class size. Accuracy and weighted average prefer systems that perform best on the largest number of examples, favoring higher performance on the most frequent classes. In practice, only a small number of the systems were ranked differently by the different metrics. We discuss this further in Section 4.7. Results for all metrics are available online, and this paper focuses on two metrics for brevity: weighted and macro-average F 1 scores. Results The evaluation results for all metrics and all participant runs are provided online. 2 The tables in this paper present the F 1 scores for the best system runs. Results are shown separately for each test set (TS), with the simple mean over the five TSs reported in the final column. We used two baselines: the majority (most frequent) class baseline and a lexical overlap baseline described in detail in (Dzikovska et al., 2012) . The performance of the baselines is presented jointly with system scores in the results tables. For each participant, we report the single run with the best average TS performance, identified by the subscript in the run title, with the exception of ETS. With all other participants, there was almost always one run that performed best for a given metric on all the TSs. In the small number of cases where another run performed best on a given TS, we instead report that value and indicate its run with a subscript (these changes never resulted in meaningful changes in the performance rankings). ETS, on the other hand, subusing the multi-label metric are all equal and mathematically equivalent to accuracy. mitted results for systems that were substantially different from one another, with performance varying from being the top rank to nearly the lowest. Hence, it seemed more appropriate to report two separate runs. 3 In the rest of the discussion system is used to refer to a row in the tables as just described. Systems with performance that was not statistically different from the best results for a given TS are all shown in bold (significance was not calculated for the TS mean). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using approximate randomization test (Yeh, 2000) with 10,000 iterations; p \u2264 0.05 was considered statistically significant. Five-way Task The results for the five-way task are shown in Tables 2 and 3 . Comparison to baselines All of the systems performed substantially better than the majority class baseline (\"correct\" for both BEETLE and SCIENTS-BANK), on average exceeding it on the TS mean by 0.21 on the weighted F 1 and 0.24 on the macroaverage F 1 . Six systems outperformed the lexical baseline on the mean TS results for the weighted F 1 and five for the macro-average F 1 . Nearly all of the top results on a given TS (shown in bold in the tables) were statistically better than corresponding lexical baselines according to significance tests (indicated by italics in the tables). Comparing UA and UQ/UD performance The BEETLE UA (BUA) and SCIENTSBANK UA (SUA) test sets represent questions with example answers in training data, while the UQ and UD test sets represent transfer performance to new questions and new domains respectively. The top performers on UA test sets were CoMeT 1 and ETS 2 , with the addition of UKP-BIU 1 on SUA. However, there was not a single best performer on UQ and UD sets. ETS 2 performed statistically better than all other systems on BEETLE UQ (BUQ), but it performed statistically worse than the lexical baseline on SCIENTSBANK UQ (SUQ), resulting in no overlap in the top performing systems on the two UQ test sets. SoftCardinality 1 performed statistically better than all other systems on SUD and was among the three or four top performers on SUQ, but was not a top performer on the other three TSs, generally not performing statistically better than the lexical baseline on the BEETLE TSs. Group performance The two UA TSs had more systems that performed statistically better than the lexical baseline (generally six systems) than did the UQ TSs where on average only two systems performed statistically better than the lexical baseline. Over twice as many systems outperformed the lexical baseline on UD as on the UQ TSs. The top performing systems according to the macro-average F 1 were nearly identical to the top performing systems according to the weighted F 1 . Three-way Task The results for the three-way task are shown in Tables 4 and 5. Comparison to baselines All of the systems performed substantially better than the majority baseline (\"correct\" for BEETLE and \"incorrect\" for SCI-ENTSBANK), on average exceeding it on the TS mean by 0.28 on the weighted F 1 and 0.31 on the macro-average F 1 . Five of the eight systems outperformed the lexical baseline on the mean TS results for the weighted F 1 and five on the macroaverage F 1 , and all top systems outperformed the lexical baseline with statistical significance. Comparing UA and UQ/UD performance The top performers on both BUA and SUA were CoMeT 1 and ETS 2 . As for the 5-way task there was no single best performer for UQ and UD sets, and no overlap in top performing systems on BUQ and SUQ test sets, with ETS 2 being the top performer on BUQ, but statistically worse than the baseline on SUQ and SUD. On the weighted F 1 , SoftCardinality 1 performed statistically better than all other systems on SUD and was among the two statistically best systems on SUQ, but was not a top performer on BUQ or BUA/SUA TSs. On the macro-average F 1 , UKP-BIU 1 became one of the statistically best performers on all SCIENTSBANK TSs but, along with SoftCardinality 1 , never performed statistically better than the lexical baseline on the BEETLE TSs. Group performance With the exception of SUA, only around two systems performed statistically better than the lexical baseline on each TS. The top performing systems were nearly the same according to the weighted F 1 and the macro-average F 1 . Two-way Task The results for the two-way task are shown in Table 6. Because the labels are roughly balanced in the two-way task, the results on the weighted and macro-average F 1 are very similar and the top performing systems are identical. Hence this section will focus only on the macro-average F 1 . As in the previous tasks, all of the systems performed substantially better than the majority baseline (\"incorrect\" for all sets), on average exceeding it on the TS mean by 0.25 on the weighted F 1 and 0.30 on the macro-average F 1 . However, just four of .191 0.197 0.201 0.194 0.197 0.196 Table 5 : Three-way task macro-average F 1 the nine systems in the two-way task outperformed the lexical baseline on the mean TS results. In fact, the average performance fell below the lexical baseline. The differences in the macro-average F 1 between the top results on a SCIENTSBANK TS and the corresponding lexical baselines were all statistically significant. Two of the top results on BUA were not statistically better than the lexical baseline, and all systems performed below the baseline on BUQ. Discussion All of the systems consistently outperformed the most frequent class baseline. Beating the lexical overlap baseline proved to be more challenging, being achieved by just over half of the results with about half of those being statistically significant improvements. This underscores the fact that there is still a considerable opportunity to improve student The set of top performing systems on the weighted F 1 for a given TS were also always in the top on the macro-average F 1 , but a small number of additional systems joined the top performing set on the macro-average F 1 . Specifically, one, three, and two results joined the top set in the five-way, threeway, and two-way tasks, respectively. In principle, the metrics could differ substantially, because of the treatment of minority classes, but in practice they rarely did. Only one pair of participants swap adjacent TS mean rankings on the macro-average F 1 relative to the weighted F 1 on the two-way task. On the five-way task, two pairs swap rankings and another participant moved up two positions in the ranking, ending at the median value. Most (28/34) rank changes were only one position and most (21/34) were in positions at or below the median ranking. In the five-way task, a pair of systems, UKP-BIU 1 and ETS 1 , had a meaningful performance rank swap on the macro-average F 1 relative to the weighted F 1 on the UD test set. Specifically, UKP-BIU 1 moved up four positions from rank 6, where it was not statistically better than the lexical baseline, to the second best performance. Not surprisingly, performance on UA was substantially higher than on UQ and UD, since the UA is the only set which contains questions with example answers in training data. Performance on BUA was usually better than performance on SUA, most likely because BUA contains more similar questions and answers, focusing on a single science area, Elec-tricity and Magnetism, compared to 12 distinct science topics in SUA). In addition, the BEETLE study participants may have used simpler language, since they were aware that they were talking to a computer system instead of writing down answers for human teachers to assess as in SCIENTSBANK. Performance on BUQ versus SUQ was much more varied, presumably since there was no direct training data for either TS. For the five-way task, the best performance on the weighted F 1 measure for BUQ is 0.09 below the best result for BUA and the analogous decrease from SUA to SUQ is 0.13, with an additional 0.02 drop on SUD. On the two-way task, the best weighted F 1 for BUQ drops 0.11 from the best BUA value, but the decrease from SUA to SUQ is just 0.03, with another 0.03 drop to SUD. While the drop in performance is fairly similar from BUA to BUQ on all tasks and either metric, the decrease from SUA to SUQ seems to potentially be dependent on the task, ranging from 0.13 on the fiveway task to 0.08 on the three-way task and 0.03 on the two-way task. Pilot Task on Partial Entailment The SCIENTSBANK corpus was originally developed to assess student answers at a very fine-grained level and contains additional annotations that break down the answers into \"facets\", or low-level concepts and relationships connecting them (henceforth, SCIENTSBANK Extra). This annotation aims to support educational systems in recognizing when specific parts of a reference answer are expressed in the student answer, even if the reference answer is not entailed as a whole (Nielsen et al., 2008b) . The task of recognizing such partial entailment relationships may also have various uses in applications such as summarization or question answering, but it has not been explored in previous RTE challenges. Therefore, we proposed a pilot task on partial entailment, in which systems are required to recognize whether the semantic relation between specific parts of the Hypothesis is expressed by the Text, directly or by implication, even though entailment might not be recognized for the Hypothesis as a whole, based on the SCIENTSBANK facet annotation. Each reference answer in SCIENTSBANK data is broken down into facets, where a facet is a triplet consisting of two key terms (both single words and multi-words, e.g. carbon dioxide, each other, burns out) and a relation linking them, as shown in Figure 2 . The student answers were then annotated with regards to each reference answer facet in order to indicate whether the facet was (i) expressed, either explicitly or by assumption or easy inference; (ii) contradicted; or (iii) left unaddressed. Considering the SCIENTSBANK reference answers as Hypotheses, the facets capture their atomic components, and facet annotations may correspond to the judgments on the sub-parts of the H which are entailed by T. We carried out a feasibility study to explore this idea and to verify how well the facet annotations align with traditional entailment judgments. We focused on the reference answer facets labeled in the gold standard annotation as Expressed or Unaddressed. The working hypothesis was that Expressed labels assigned in SCIENTSBANK annotations corresponded to Entailed judgments in traditional textual entailment annotations, while Unaddressed labels corresponded to No-entailment judgments. Similarly to the feasibility study reported in Section 3.2, we concluded that the correspondence between educational labels and entailment judgments was not perfect due to the difference in educational and textual entailment perspectives. Nevertheless, the two classes of assessment appeared to be sufficiently well correlated so as to offer a good testbed for partial entailment in a natural setting. Task Definition Given (i) a text T, made up of a Question and a Student Answer; (ii) a hypothesis H, i.e. the Reference Answer for that question and (iii) a facet, i.e. a pair of key terms in H, the task consists of determining whether T expresses, either directly or by implication, the same relationship between the facet words as in H. In other words, for each of H's facets the system assign one of the following judgments: Expressed, if the Student Answer expresses the same relationship between the meaning of the facet terms as in H; Unaddressed, if it does not. Consider the example shown in Figure 2 . For facet 3, the system must decide whether the same relation between the two terms 'contains' and 'seeds' in H (the reference answer) is expressed, explicitly or implicitly, in T (the combination of question and student response). If the student answer is 'The part of a plant you are observing is a fruit if it has seeds.', the answer to the question is 'yes' and the correct judgment is 'Expressed'. But if the student says 'My rule is has to be sweet.', T does not express the same semantic relationship between 'contains' and 'seeds' exhibited in H, thus the correct judgment is 'Unaddressed'. Note that even though this is an exercise in textual entailment, student response assessment labels were used instead of traditional entailment judgments, due to the partial mismatch between the two assessment classes found in the feasibility study. Dataset We used a subset of the SCIENTSBANK Extra corpus (Nielsen et al., 2008b) with the same problematic questions filtered out as the main task (see Section 3.3). We further filtered out all the student answer facets which were labeled other than 'Expressed' or 'Unaddressed' in the gold standard annotation; the facets in which the relationship between the two key terms, as classified in the manual annotation, proved to be problematic to define and judge, namely Topic, Agent, Root, Cause, Quantifier, Neg; and inter-propositional facets, i.e. facets that expressed relations between higher-level propositions. Finally, the facet relations were removed from the dataset, leaving the relationship between the two facet terms unspecified so as to allow a more fuzzy approach to the inference problem posed by the exercise. We used the same training/test split as reported in Section 3.4. The training set created from the Training SCIENTSBANK Extra corpus contains 13,145 reference answer facets, 5,939 of which were labeled as 'Expressed' in the student answers and 7,206 as 'Unaddressed'. The Test set was created from the SCIENTSBANK Extra unseen data and is divided into the same subsets as the main task (Unseen Answers, Unseen Questions and Unseen Domains). It contains 16,263 facets total, with 5,945 instances labeled as 'Expressed', and 10,318 labeled as 'Unaddressed'. Evaluation Metrics and Baselines The metrics used in the Pilot task were the same as in the Main task, i.e. Overall Accuracy, Macroaverage . and Weighted Average Precision, Recall and F 1 , and computed as described in Section 4.2. We used only a majority class baseline, which labeled all facets as 'Unaddressed'. Its performance is presented in Section 5.4 jointly with the system results. Participants and results Only one participant, UKP-BIU, participated in the Partial Entailment Pilot task. The UKP-BIU system is a hybrid of two semantic relationship approaches, namely (i) computing semantic textual similarity by combining multiple content similarity measures (B\u00e4r et al., 2012) , and (ii) recognizing textual entailment with BIUTEE (Stern and Dagan, 2011) . The two approaches are combined by generating indicative features from each one and then applying standard supervised machine learning techniques to train a classifier. The system used several lexicalsemantic resources as part of the BIUTEE entailment system, together with SCIENTSBANK dependency parses and ESA semantic relatedness indexes from Wikipedia. The team submitted the maximum allowed of 3 runs. Table 7 shows Weighted Average and Macro Average F 1 scores respectively, also for the majority baseline. The system outperformed the majority baseline on both metrics. The best performance was observed on Run 2, with the highest results on the Unseen Domains test set. Conclusions and Future Work The Joint Student Response Analysis and 8th Recognizing Textual Entailment challenge has proven to be a useful, interdisciplinary task using a realistic dataset from the educational domain. In almost all cases the best systems significantly outperformed the lexical overlap baseline, sometimes by a large margin, showing that computational linguistics approaches can contribute to educational tasks. However, the lexical baseline was not trivial to beat, particularly in the 2-way task. These results are consistent with similar findings in previous RTE exercises. Moreover, there is still significant room for improvement in the absolute scores, reflecting the interesting challenges that both educational data and RTE tasks present to computational linguistics. The educational setting places new stresses on semantic inference technology because the educational notion of 'Expressed' and the RTE notion of 'Entailed' are slightly different. This raises the educational question of whether RTE can work in this setting, and the RTE question of whether this setting is meaningful for evaluating RTE system performance. The experimental results suggests that the answer to both questions is 'yes', a significant finding for both educators and RTE technologists going forward. The Pilot task, aimed at exploring notions of partial entailment, so far not explored in the series of RTE challenges, has proven to be an interesting, though challenging exercise. The novelty of the task, namely performing textual entailment not on a pair of full texts, but between a text and a hypothesis consisting of a pair of words, may have represented a more complex task than expected for some textual entailment engines. Despite this, the encouraging results obtained by the team which carried out the exercise has shown that this partial entailment task is worthy of further investigation. Acknowledgments The research reported here was supported by the US ONR award N000141010085 and by the Institute of Education Sciences, U.S. Department of Education, through Grant R305A120808 to the University of North Texas. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education. The RTErelated activities were partially supported by the Pascal-2 Network of Excellence, ICT-216886-NOE. We would also like to acknowledge the contribution of Alessandro Marchetti and Giovanni Moretti from CELCT to the organization of the challenge.",
    "abstract": "We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions.",
    "countries": [
        "United States",
        "Israel",
        "United Kingdom",
        "Italy"
    ],
    "languages": [],
    "numcitedby": "145",
    "year": "2013",
    "month": "June",
    "title": "{S}em{E}val-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge"
}