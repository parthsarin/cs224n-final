{
    "framing_question": "Who are the intended users of the work described in the paper, if any?",
    "response": "The intended users of the work described in the paper are non-ML specialists who work in technical and scientific domains, such as Life Sciences, and need to address the Entity Normalization task under the conditions of scarce training data. The presented strategies rely on publicly available tools and adequate data preparation, making them accessible to users without extensive expertise in machine learning.",
    "article": "Entity normalization (or entity linking) is an important subtask of information extraction that links entity mentions in text to categories or concepts in a reference vocabulary. Machine learning based normalization methods have good adaptability as long as they have enough training data per reference with a sufficient quality. Distributional representations are commonly used because of their capacity to handle different expressions with similar meanings. However, in specific technical and scientific domains, the small amount of training data and the relatively small size of specialized corpora remain major challenges. Recently, the machine learning-based CONTES method has addressed these challenges for reference vocabularies that are ontologies, as is often the case in life sciences and biomedical domains. Its performance is dependent on manually annotated corpus. Furthermore, like other machine learning based methods, parametrization remains tricky. We propose a new approach to address the scarcity of training data that extends the CONTES method by corpus selection, pre-processing and weak supervision strategies, which can yield high-performance results without any manually annotated examples. We also study which hyperparameters are most influential, with sometimes different patterns compared to previous work. The results show that our approach significantly improves accuracy and outperforms previous state-of-the-art algorithms. Introduction Entity Normalization, also known as Entity Linking or Entity Grounding, is an Information Extraction subtask that consists in the assignment of categories or concepts from a reference vocabulary to entity mentions in the text (i.e. words or sequences of words). Ontologies and taxonomies have been frequently used as reference vocabularies since the late 1990s (Faure and N\u00e9dellec, 1998; Hwang, 1999) . For instance, entity normalization with concepts of an ontology could consist in linking the textual entity mention \"T-cell\" to a concept referenced by a unique identifier and a label such as: <OBT:001342: lymphocyte>. Entity Normalization contributes to the reuse of the extracted information and to its integration with data in reference databases (N\u00e9dellec et al., 2009) . Entity Normalization has recently gained traction in technical and scientific domains, especially in Life Sciences and Health Sciences. Several benchmark datasets have been proposed in these domains (Wei et al., 2015; Roberts et al., 2017; Del\u00e9ger et al., 2016) . Entity Normalization methods handle the problem as a classification problem. They are based either on pattern-matching rules (Aronson, 2001) or on Machine Learning (ML) algorithms (Leaman et al., 2013) . Both are able to operate on the surface form of entities (i.e. their sequence of characters), on NLP analyses (lemmatization, POStagging, syntactic parsing) (Aronson, 2001) , or on distributional semantic representations such as word embeddings (Limsopatham and Collier, 2016) . The main limitation of rule-based and entity-form-based methods is that they require entity mentions to present some similarity with concept labels to be efficient. For instance, rule-based methods cannot assign the mention \"T-cell\" to a concept labelled \"lymphocyte\", unless a similar form of the term \"T-cell\" is added to the concept (Pratt and Yetisgen-Yildiz, 2003) . For a real task with thousands or millions of concepts, this represents a tremendous work, and can quickly introduce some ambiguities between concepts (\"plant\", vegetal or factory entity?), which require additional and special processing (Hanisch et al., 2005; Morgan et al., 2008; Aronson and Lang, 2010) . ML and embedding-based methods aim to address this limitation. They have also shown a better adaptability to various specific normalization tasks. These methods yield good results if provided with sufficient training data (Sil et al., 2018) . However training data in the form of manually annotated corpora are costly to produce and are generally limited in size (Uschold and King, 1995) , in particular in specific domains where the level of expertise required to annotate training data is high and the number of target concepts is large (Lipscomb, 2000; McCray, 1989; N\u00e9dellec et al., 2018) . Learning without examples is a problem called zero-data learning (or fewshot learning) (Larochelle et al., 2008; Ravi and Larochelle, 2016) , and is a well-known challenge for ML. CONTES (Ferr\u00e9 et al., 2017) and HONOR (Ferr\u00e9 et al., 2018) are two recent methods that address training data paucity by exploiting ontological subsumption information (is_a relation between concepts or categories). CONTES uses the subsumption graph of the ontology together with word embeddings. HONOR combines CONTES and the rule-based method ToMap (Golik et al., 2011) . HONOR achieves state of the art performance on the Bacteria Biotope normalization task of BioNLP Shared Task 2016 (BB3) (Del\u00e9ger et al., 2016) . Nevertheless, both of these methods still need an annotated corpus which provides ground truth training examples. In this paper we present novel approaches to entity normalization, using CONTES and HONOR with no manually annotated corpus, based on a weak supervision strategy. Moreover, embedding based methods vary depending on the representation of examples and algorithm hyperparameters (Chiu et al., 2016) . We propose an experimental setting with the aim of highlighting the influence of a wide range of different factors (corpus selection, pre-processing, word embedding hyperparameters) and finding the optimal configuration for adapting a generic normalization method to a specific task. Pre-processing still receives too little focus when evaluating embedding-based systems (Camacho-Collados and Pilehvar, 2018) , in favour of hyperparameter study or the use of general precomputed embeddings. We evaluated our approach on the Bacteria Biotope normalization task of BioNLP Shared Task 2016 (BB3). This task illustrates the challenge that we aim to address: it is a domain-specific normalization task, wellrecognized in the BioNLP community, and with a small amount of training data available compared to the number of concepts of the task. Related Work 2.1 A Brief History of Entity Normalization Methods in Scientific Domains Most normalization methods in technical and scientific domains rely on the similarity between entity forms and concept labels. Due to frequent linguistic variations (e.g. noun-phrase inversion, typographic variations, synonymy), these methods are dependent on comprehensive lexicons. Several strategies are used to ensure comprehensiveness: third-party resources (Gerner et al., 2010; Lee et al., 2015) inflection generation (Hanisch et al., 2005; Tsuruoka et al., 2007; Ghiasvand and Kate, 2014) , pre-processing (lemmatization, stemming or stopword filtering) (Schuemie et al., 2007) , giving more weight to syntactic heads of mentions and labels (Aronson, 2001; Golik et al., 2011) . To handle domain-specific ambiguities, which notably increase with strategies such as third-party resources and inflection generation, and simultaneously to specialize for the task domain, some methods use a hand-crafted blacklist: for instance, the ToMap method (Golik et al., 2011) has a version adapted to the BB3 task, and the Peregrine method (Schuemie et al., 2007) has a version adapted to BioCreative II. Another method by Claveau (2013) weights tokens with an Information Retrieval measure, which aims to automatically mitigate the weight of ambiguous words. Other methods use vector representations to compute a similarity measure between text mentions and concept labels. Notably, Tiftikci et al. (2016) and Mehryary et al. (2017) estimate the semantic similarity between two expressions by computing a cosine similarity between TF-IDF bag-of-words representations (Manning et al., 2009) . These representations are based on word forms, and fail to link mentions that do not share any common token with the correct concept label. To address this limitation, the ML-based DNorm method (Leaman et al., 2013) learns a function that estimates high similarities between distant TF-IDF bag-of-words representations of mentions and associated concept labels. Some advances in NLP have been made through word embeddings as built by Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) , FastText (Bojanowski et al., 2016) or more recently ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018) , as a way to compute and represent the meaning of words from the contexts in which they are observed. Word embeddings are vectors with the advantage of a smaller number of dimensions. However, their acquisition requires large amounts of untagged corpora. To favour their mapping, text mentions and labels can be represented by embeddings in the same space. So, a first approach to normalize a mention embedding is to find the nearest concept label embedding. For instance, the BOUNEL method (Karadeniz and \u00d6zg\u00fcr, 2019) computes a cosine similarity between these embeddings to find the best concept candidates for a mention. Limsopatham and Collier (Limsopatham and Collier, 2016 ) also use word embeddings for the representation of mentions, and a convolutional neural network architecture to learn to classify each mention representation by the correct concept. The results of word embedding-based methods significantly depend on the choice of a large unannotated corpus, on the chosen hyper-parameters (Chiu et al., 2016) , and on parameter initialization. Moreover, through specialized corpora, domain specialized embeddings can increase the performance of methods (Roberts, 2016) . This specialization can be emphasized by exploiting external knowledge (Faruqui et al., 2014; De Vine et al., 2014; Celikyilmaz, 2015) , such as that contained in ontologies (Ferr\u00e9 et al., 2017; Yen et al., 2018) . The CONTES and HONOR Methods The word embeddings-and ML-based CONTES method (Ferr\u00e9 et al., 2017) differs from these methods by its use of concept vectors instead of concept label embeddings or concept one-hot vectors. Each concept has a unique vector in a vector space that is different from the space of mention embeddings. CONTES then performs a multivariate linear regression to find a linear projection from the vector space of mentions to the vector space of concepts. The learning optimization goal is to minimize globally the Euclidean distance between each projected mention vector and its concept vector(s) (see Figure 1 ). CONTES then uses the learned parameters to project any mention vector onto the ontological space. It computes a cosine similarity between the projected vector and every concept vector. CONTES finally selects the concept with the most similar vector as the prediction for normalization (see Figure 2 ). The vector space of concepts includes vectors computed with ontological information rather than one-hot vectors (where all weights are set to zero except the weight associated with the current concept, which is set to one), as in Limsopatham and Collier (Limsopatham and Collier, 2016) . Each concept is associated with a vector whose size is the number of concepts in the ontology. Each dimension is associated with a fixed concept.   The vector is initialized as a one-hot, then each of the weights of the dimensions associated with all ancestors of the current concept are also set to one. This way, the representation encodes the hierarchical information of the ontology: when computing cosine similarity between ontology concepts, parent/child concepts are always nearest to each other. This hierarchical relation is the most frequent semantic relation (is_a relation) in existing ontologies. To compute mention embeddings, CONTES uses word embeddings from a large corpus. For each recognized mention of a corpus, a mention embedding is computed by calculating the barycenter of the vectors of the words that compose the entity. Roberts (2016) has shown that using word embeddings trained on a specific biomedical corpus for a biomedical task obtains better results than with a larger out-of-domain corpus. Thus, in order to improve the similarity between the two spaces, CONTES used specific word embeddings trained on a biomedical corpus rather than on large general corpora. CONTES authors hypothesize that it is possible to learn a structural similarity between a distributional space and an ontological space from the same domain. Rule-based methods predict correct concepts with a better precision whereas ML and embedding-based methods usually achieve better recall. To benefit from the best of both approaches, the HONOR method combines the ML and embedding-based CONTES with the rule-based method ToMap (Golik et al., 2011) in two steps. First, ToMap predicts concepts for all mentions it can, then CONTES predicts a concept for the mentions left out by ToMap (see Figure 3 ). Task Data Sets In this section, we present the BB3 normalization task dataset that we used to evaluate the methods. The task consists in linking mentions of bacteria species names to a taxonomy, and bacterial habitats to one or more of the 2,320 concepts of the dedicated ontology OntoBiotope 1 . Each concept can also contain some synonyms of the label (0.2 synonyms on average), which gives a total of 2,739 terms associated to concepts of the ontology. The normalization of taxonomic mentions of bacteria is not much of a challenge for the BioNLP community because the nomenclature is complete, variations are relatively standardized, and synonymy is rare (except in some special cases such as strain names). Thus string matching with basic variations yields decent results (Grouin, 2016) . Habitat mentions are subject to much more variations, and, due to the microscopic nature of bacteria, any object or place can be construed as a habitat. The normalization of habitat mentions is thus a challenging task that has generated many studies. The BB3 corpus is made of titles and abstracts from PubMed entries annotated with entities. The annotated corpus is split into training, development, and test sets. The BB3 online evaluation service measures the 1 http://2016.bionlp-st.org/tasks/bb2/OntoBiotope_BioNLP-ST-2016.obo performance of the predictions on the test set. The total volume of the annotated corpus is rather small: the whole training and development corpus contains around 25 thousand tokens, around 1,200 mentions of bacterial habitats and only 12% of the concepts of OntoBiotope occur in this corpus (266 distinct concepts). The evaluation metrics is based on a semantic similarity between the reference concept and the predicted concept. The similarity equals 1 if they are equal, and tends to zero if the concepts are farther in the hierarchy. The overall score is the mean of the similarity for each mention in the test set (Wang et al., 2007) . Methods Weakly Supervised Strategy CONTES achieves good results on BB3 with a small set of training data but it still needs manually annotated examples (Ferr\u00e9 et al., 2017) . To address this limitation, we developed a weak supervision strategy that exploits the lexicalization of the ontology: ontology concepts are usually labeled by terms and synonyms. We use these labels and synonyms as training examples, instead of, or combined with, annotated mentions in the training corpus. This gives about twice as many training examples as in the training corpus, and at least one training example per concept. Preprocessing We also extend CONTES by studying the impact of linguistic preprocessing as complementary to the study of hyperparameters for embeddings calculation that embedding-based methods usually focuse on. Preprocessing steps such as lemmatization or stemming, stop-words filtering and masking (i.e. replacing some expressions with a single mnemonic form) have the positive effect of decreasing the size of the vocabulary and increasing the distributional signal for each token. At the same time, lemmatization and stemming can have undesirable effects such as merging tokens that have no common meaning, and stopword filtering and masking can delete useful information. Thus, we study the impact of these preprocessing strategies on the performance of the CONTES and HONOR methods. For stemming, we tested an implementation of the Snowball algorithm (Porter, 1980) , and for lemmatization we used GeniaTagger 2 , a state-of-the-art lemmatizer and POS-tagger for the biomedical domain. For stopword filtering, we removed grammatical words (determiners, prepositions, conjunctions, \"to\"), and punctuations. For masking, we replaced each numerals and species names with a unique token. Word2Vec Parametrization and Corpus Selection We have extended the study of Word2Vec hyperparameters beyond those in CONTES and HONOR (Ferr\u00e9 et al., 2017; Ferr\u00e9 et al., 2018) . We used the Skip-Gram architecture of Word2Vec with negative sampling (Mikolov et al., 2013) in a similar way as CONTES. We have screened the hyperparameters, notably the size of the contextual window and the size of the word embedding vectors. We set the minimum occurrences of words to zero in order to consider rare domain words. We trained the embeddings on a corpus dedicated to the domain of BB3, i.e. microbiology. From the whole PubMed collection, we selected entries indexed by keywords of the MeSH controlled vocabulary 3 indicating that the entry's topic is microbiology (\"Bacteria\", \"Microbiology\", etc.). The resulting corpus contains 2,333,943 entries (412,240,083 tokens) . We refer to it as the Microbiology Corpus in the following. This corpus is smaller than the usual corpora used to calculate embeddings such as Wikipedia or Google News, which are still commonly used for biomedical tasks. However, it constitutes a relatively large biomedical corpus (Roberts, 2016) . It allows to generate embeddings in less than an hour on a standard server computer. It is manageable enough to enable the screening of the hyperparameters and of the pre-processing settings in a reasonable time. We also compared the results obtained with this corpus to publicly available domain-specific and general domain word embeddings 4 . Results All the presented scores are averaged over ten runs on the development set, except for the final evaluation (Table 4 ) which contains unique scores on the test set. In all our experiments, we observed a maximal standard deviation of the score of 0.011, due to the random initialization of Word2Vec. Impact Preprocessing and Word2Vec Parametrization We observed no impact of the window size (see Table 1 ). Thus, we set a short symmetrical window of two tokens in all subsequent experiments. We observe however a significant impact of the vector size on the performance (see Table 2 ). Previous work with the CONTES method had reported an optimal value of 200. We find the same optimal value for the supervised version but, surprisingly, we find a different optimal value of 1,000 for the weak supervision strategy. Pre-processing yields rather mixed results. Lemmatization does not significantly improve the results, and stemming even degrades them (see Table 3 ). We hypothesize that stemming shortens the token forms too aggressively and entails a loss of information in the word embeddings. Masking numerals and named entities also did not significantly affect the results. In contrast, we consistently observe a significant improvement of the score when stop words are removed (see Table 3 ). The embeddings calculated with the Microbiology Corpus and the pre-processed corpus give consistently better scores (0.59) than those obtained with off-the-shelf embeddings trained on Wikipedia (0.57) or the whole PubMed (0.56). Results for the Weak Supervision and Mixed Approaches To evaluate the weak supervision strategy, we used word embeddings obtained with the following settings: \u2022 the Microbiology Corpus with stopword filtering and no other pre-processing; \u2022 a window size of 2; \u2022 a vector size of 1,000. The results are shown in Table 4 . For comparison purposes we also report the score obtained by a baseline method. It consists in a strict string matching of concept labels and synonyms against lemmatized mentions. We also report published results obtained by four other systems on the same benchmark: BOUN (Tiftikci et al., 2016) , Turku (Mehryary et al., 2017) , BOUNEL (Karadeniz and \u00d6zg\u00fcr, 2019), and ToMap (Golik et al., 2011) . We trained, ran, and evaluated CONTES and HONOR using the generated embeddings. We trained them with three different training sets: \u2022 BB3 training and development sets (training); \u2022 concept labels and synonyms (labels) for the weak supervision approach; \u2022 the union of the above (training and labels). We observed that both CONTES and HONOR perform similarly when trained either on the training and development sets, or on concept labels and synonyms. Conclusion and Discussion We address the Entity Normalization task by concepts of an ontology under the conditions of scarce training data. This situation is common for technical and scientific domains such as Life Sciences. Using the CONTES and HONOR methods on the BB3 task, we experimented with several strategies and highlighted the relative merits of each. Both CONTES and HONOR use word embeddings. We have shown that training word embeddings on a reduced but targeted and adapted corpus yields better results than off-the-shelf word embeddings trained on a huge general corpus. Moreover, the reduced size of the training corpus allows us to screen and optimize hyperparameters. The word embeddings vector size has the largest influence on the prediction performance. We have also shown that token normalization has little, and sometimes a negative influence. Indeed lemmatization, stemming, numeral masking and entity masking did not improve results. On the other hand, stop word filtering produces better word embeddings for this task. The main result of our experiments is that CONTES achieves similar results when trained on annotated entity mentions or on concept labels and synonyms. Moreover when trained on the union of labels and the annotated training corpus, the results improve even more. This demonstrates that the use of the concept labels and synonyms can overcome the lack of annotated training data, or positively complement small training datasets. Our hypothesis is that both training sets are complementary because they present different characteristics. On one hand labels and synonyms cover the whole ontology because each concept has at least one label. On the other hand annotated corpora represent more faithfully mentions in texts. Further study should include the performance of the methods specifically on mentions of concepts that have not been mentioned in the training dataset. This could allow us to see the specific contribution of these methods to the problem of zero-data learning. By exploring parameters and training data, we were able to substantially outperform the state-of-the-art. In order to draw more general conclusions, we plan to apply the approach to other benchmarks, such as BioCreative V Chemical-Disease-Relations (Wei et al., 2015) and TAC Adverse Drug Reaction Extraction from Drug Labels (Roberts et al., 2017) , as future work. The presented strategies all rely on publicly available tools 5 and on adequate data preparation and thus they are within the reach of non-ML specialists. Acknowledgements",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 3.769715394619233e-06,
        "foundation": 0.0,
        "none": 0.9999732243284747
    }
}