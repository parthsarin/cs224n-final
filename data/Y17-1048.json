{
    "article": "Social media is an important source for news writers. However, extracting useful information for news writers from the vast amount of social media information is laborious. Therefore, services that enable news writers to extract important information are desired. In this paper, we describe a method to extract tweets that include useful information for news writers. Our method uses a Recurrent Neural Network (RNN) with an attention mechanism and multi-task learning that processes each character in the tweet to estimate whether the tweet includes important information. In our experiment, we compared two types of attention mechanism and compared their types with/without multi-task learning. By our proposed method, we obtained an F-measure of 0.627, which is 0.037 higher than that of baseline method. Introduction Social media information is now an important source for news writers. People who encounter an incident can post what is happening before his/her eyes using photos and videos. These posts are important primary information, so news writers want to gather them. However, extracting useful information from the vast amount of social media information is laborious. For this reason, services that enable news writers to extract information that can be used as a news source are desired. In fact, some services such as Spectee 1 and FASTALERT 2 have been launched in Japan. These services gather much information from social media and extract information that can be used as news sources. Information that news writers want to extract from social media includes many different topics such as fires, accidents, and other incidents. Therefore, extracting information from social media by filtering with keywords is difficult. Assuming the words \"delay\" and \"train\" are included in the keywords, the tweet \"xxx line is delayed by accident,\" which can be used as a news source, can be extracted. However, the tweet \"I hope the train is delayed because I haven't studied for today's exam,\" which cannot be used as a news source, is also extracted. To extract tweets that include important information, filtering by keyword is not enough because the output may include tweets that cannot be used as news sources. For this reason, we have been studying automatic extraction of useful information from social media. Our purposes are to reduce the amount of laborious work and extract information that cannot be extracted by using queries. In this paper, we describe a method to extract tweets that include useful information for news writers. Generally, social media posts are often written in colloquial style and often include abbreviations, slang and emojis. This makes word segmentation difficult. Therefore, our method is character-based approach, not a word-based one. Our method analyzes each character in a tweet by using a Recurrent Neural Network (RNN) and then decides whether the tweet includes important information. We adopted an attention mechanism and multi-task learning in our method and confirmed the effectiveness of our method. Our contribution is to reveal that the combination of attention mechanism and multi-task learning is effective for characterbased approaches. Methods for extracting important tweets In this section, we describe our method for extracting important tweets for news writers. We use a Recurrent Neural Network (RNN)-based model as the basic method. And, we add the attention mechanism and multi-task learning. Basic model (RNN-based model) As we mentioned, sentences in social media are often written in colloquial style and often include abbreviations, slang and emojis. This makes it difficult to use state-of-the-art word segmentation or other natural language processing (NLP) tools. Japanese, our target language, is written without word separation, so the accuracy of the word segmentation directly affects the performance of word-based NLP tasks. Actually, according to Ling et al. ( 2015) and Dhingra et al. ( 2016) , the character-based approach outperformed the word-based one in the social media analysis task. For these reasons, we chose characters, not words, as the input of our models. Our basic method uses bi-directional RNN (biRNN) for obtaining vector representations of the input tweet. Each character in a tweet is sequentially inputted for both the forward and backward directions. When all characters are inputted, the final hidden states of the biRNN are used as vector representations. Then, our method classifies the tweet according to whether it is important by using a twolayer Feed-Forward Neural Network (FFNN). Figure 1 shows the architecture of our basic model. Attention-based model The attention mechanism has been used in many NLP tasks, such as machine translation (Bahdanau et al., 2015; Luong et al., 2015a) and image captioning (Xu et al., 2015) , and can give weights to each input data taking into account the importance. In this paper, for comparison, we prepare two types of attention mechanism: \"FinalState\" and \"Mean-Vector.\" FinalState attention FinalState attention is the conventional method. In this method, we calculate the attention weight using the final hidden state of biRNN (Figure 2-(a) ). Here, we explain using the example of forward RNN: actually, we use both forward and backward RNNs. The score for the t-th character score t is calculated as follows: score t = h T f ht Here ht is the hidden state of RNN, in which the t-th character of the input tweet has been inputted, and h f is the hidden state of RNN, in which the final character of the tweet has been processed. By using the score, the weight for the t-th character W t is as follows: W t = exp(score t ) \u2211 t \u2032 exp(score t \u2032 ) Here, t \u2032 means the set of all characters in the tweet. By using the weight and hidden state of the character, the FinalState attention a f can be given: a f = \u2211 t \u2032 W t h t Our method uses the sum of a f and h f as a feature, and judges whether the tweet can be used as a news sources. The architecture of the FFNN consists of two layers as shown in Figure 1 . This method can give high weight to the characters that strongly affect the vector representation of the whole tweet because the higher the similarity between the t-th character's vector h t and the tweet's vector h f , the higher the weight W t is. MeanVector attention MeanVector attention is our proposed method. In this method, we calculate the attention weight using the mean vector of the hidden state of biRNN for every character in the tweet (Figure 2-(b) ). Similar to section 2.2.1, we explain using the example of the forward RNN. The score for the t-th character score t is calculated as follows: score t = h T m ht h m = \u2211 t \u2032 ht t \u2032 By using score t , we can calculate the weight W t and MeanVector attention a m in a similar way to that in section 2.2.1. W t = exp(score t ) \u2211 t \u2032 exp(score t \u2032 ) a m = \u2211 t \u2032 W t h t We use the sum of a m and h f as the input of the two-layer FFNN, and judge the tweet according to whether it can be used as a news source. MeanVector attention can also give high weight to the characters that strongly affect the meaning of the whole tweet. However, compared with FinalState attention, the effect of the position of the character appeared to be reduced. Multi-task learning In some studies using a neural network, models are trained with multiple tasks. This technique is called \"multi-task learning.\" It has been reported that by using multi-task learning, the model can be generic and accurate (Luong et al., 2015b; S\u00f8gaard et al., 2016) . Therefore we also use multi-task learning so that our model is more accurate. In addition to our target task, by judging whether the tweet can be used as a news sources, we prepare the task that involves estimating the next character of the input character as another task. This task is the same as \"neural language model learning.\" We do not need to prepare training data for this task; we can use the same dataset as that of the target task without new annotated data. We designed our architecture as sharing input and a biRNN layer with these two tasks and prepared two output layers for each task. To train this model, first we start to train with the neural language model learning. After finishing this task, we start to train for the target task using the results of the first training as the initial model of the target task. Figure 3 illustrates the overall structure of our method using FinalState attention and multi-task learning. Experiment Dataset For training data, we gathered tweets that can be used as actual news sources as positive samples and randomly sampled tweets as negative samples. NHK's social listening team gathered positive samples. About 20 people work for the team. The team members have been working every day for about three years as professionals, so they are well trained and highly reliable. Negative samples were randomly sampled, so some positive samples are included in the data. However, there are not that many positive samples 3 , so we regarded that their effect is limited. For evaluation data, we prepared two datasets. One was gathered in a similar way to that of gathering the training data. The ratio of positive to negative samples are adjusted to be almost the same as that of the actual tweets to reflect actual usage of our method in the news reporting section. This dataset is named \"all the data\" in this paper. The other was gathered from 2,000 tweets by using a combination of queries, which include about 180 words connected with \"and\" / \"or\" / \"not\" 4 . These tweets are annotated according to whether they can be used as news sources by one evaluator. This dataset is designed to consider actual use by a news reporting section of a broadcasting company. Compared to the task of the other dataset, this is a more difficult task because all the data in this dataset includes some news-related words. This data is named \"filtered data\" in this paper. The size of each dataset is given in Table 1 . The hashtags, user names, HTML tags, and URLs are removed from all tweets in the datasets. The ratio of positive / negative samples in the training data does not reflect real-world distribution because of the training cost. Implementation We use Chainer (Tokui et al., 2015) to implement our models.  For the middle layer, we use Long Short-Term Memory (LSTM) for each model of biRNN with 200 hidden states, and FFNN with a unit size of 200 and 100 from the near side of the input layer. The number of epochs is set to 10 for the target task and 3 for the neural language model learning (for multi-task learning). The mini-batch size is set to 200. We use Adam (Kingma and Ba, 2014) to optimize the parameters and Exponential Linear Units (ELUs) (Clevert et al., 2015) to activate function. Evaluation results Evaluation using all the data The results of the evaluation experiment using all the data are given in Table 2 . \"Query filtering\" uses the combination of queries mentioned in section 3.1 and is shown for reference. This table indicates that all methods outperformed query filtering, but the differences between each method are small. Therefore, we try another evaluation to find out the differences. Evaluation using filtered data The results of the evaluation experiment using filtered data are given in Table 3 . The accuracy of each method is lower than that shown in Table 2 because this task is far more difficult than the task in the other In total, the method using MeanVector attention with multi-task learning, which is our proposed method, obtained the highest F-measure, which is 0.037 higher than that of the basic method without attention and multi-task learning. The result of the MeanVector attention method is rather good compared to those of the two attention mechanisms. Both attention methods cannot increase the accuracy from basic method without multi-task learning, but using multi-task learning increased the F-measures of both attention methods. We can say that MeanVector attention is better, and multi-task learning is necessary for this task. Discussion MeanVector attention outperforms FinalState attention. Figure 4 shows the comparison of the attention weight for each character between the two attention methods. The input tweet is \"For some reason, policemen are gathering in front of my house, haha.\" This tweet can be a news source because \"policemen are gathering\" may mean an incident has occurred near the place where the post was written. In the example in Figure 4 , the FinalState attention method gives high weight to the character \"\u7b11 \" (haha). In this way, the FinalState attention method tends to give high weight to the characters that appear at the end of the tweet. On the other hand, MeanVector attention gives weight without being affected much by the position of the character in the tweet. Therefore, MeanVector attention gives high weight to \"\u8b66\u5bdf\" (policemen) and \"\u96c6\u307e\u308b\" (gathering). This is the reason MeanVector attention outperforms the other attention method in this task. In our experiment, which is described in section 3.3.2, the methods with attention mechanism with- out multi-task learning did not increase the accuracy; however, using multi-task learning increased the accuracy. Calculating the attention weight uses the hidden states of biRNN for each input character. We assume that the methods using attention without multi-task learning cannot learn the hidden state precisely, and multi-task learning allows the models to learn the hidden state for each input character more precisely. For more details, losses for the model update are calculated only once per mini-batch in our \"target task\" training. This means that only one loss covers the RNN of each input character. On the other hand, in our \"neural language model\" training, losses are calculated for each input character, and these losses are much more than those of \"target task\" training. Therefore \"neural language model\" training can help the model to learn more precisely for each input character. By using the \"neural language model\" trained model as the initial model of the \"target task,\" we can obtain the hidden states of biRNN that are used for calculating attention more precisely. As a result, the attention weight for each character can be calculated more precisely by using multi-task learning, so the attention mechanism works well. Figure 5 shows the effect of using multitask learning from the viewpoint of the attention weight for each character. The input tweet is \"Awful, Sakai-gawa river may overflow.\" This can be used as a news source because we can find out from the tweet that a river is now dangerous. The method without multi-task learning gives almost constant at- tention weight. On the other hand, the method with multi-task learning gives high attention weight to \" \u6ea2\u308c\" (overflow). For this reason, using multi-task learning made the accuracy high. The ratio of positive / negative samples in our training data is quite imbalanced, and our methods did not includes any special features to manage the imbalanced. However our methods can perform with rather high accuracy. Therefore, we can say that our neural network can overcome the imbalanced training data. Related work Large-scale social media analysis systems named \"DISAANA,\" and \"D-SUMM\" are now in operation (Mizuno et al., 2016) . These systems analyze tweets as information sources and extract useful information to assess the damage caused by large-scale disasters. There are many studies on extracting information that can be used as a news source (Vosecky et al., 2013; Hayashi et al., 2015) . They used the bagof-words approach and obtained good results. However, as we mentioned, tweets are often written in colloquial style, so word segmentation is difficult. Moreover, generally, tweets include too many words to handle, so we avoid using bag-of-words approach. To do this, there are some studies that use a character-based approach to handle tweets (Vosoughi et al., 2016; Dhingra et al., 2016; Vakulenko et al., 2017) . By using these approaches, word segmentation is not necessary, and the vocabulary to be handled is reduced; however, there are few studies that use the attention mechanism in character-based approaches. We used a similar architecture to that of the Tweet2Vec model (Dhingra et al., 2016) . We expanded this model by using the attention mechanism and multi-task learning. We referred to some studies for our future work. Dredze et al. ( 2016) estimated the geolocation of the tweet. They used the time the tweet was written and obtained good results. Chi et al. ( 2016) used textual features selected based on a frequency-based feature selection strategy. Kanouchi st al. ( 2015) classified each tweet according to the people who was mentioned in the tweet, such as the person who posted the tweet of himself/herself, his/her family or people around him/her. These studies are useful to detect where and who is the subject of the tweet, which is important to news writers, who are our target users. Conclusion We presented a method to extract tweets that can be used as news sources using a recurrent neural network with attention and multi-task learning. In this paper, we confirmed the effect of the attention mechanism and multi-task learning in our task. Comparing the two methods of attention mechanism, FinalState and MeanVector, we showed that the MeanVector method is better in our task. Overall, our method (MeanVector attention with multitask learning) achieved an F-measure of 0.627 in F-measure, which is 0.037 higher than baseline method. In our experiment, the attention mechanism is effective only when used with multi-task learning. Our future work is adding new features according to the task, multi-class classifying to detect which kind of incident is mentioned in the tweet, and extracting more information such as geolocation and the subject person of the tweet.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, or foundations. Without explicit mention of funding, it is not possible to determine the involvement of any of these entities based on the provided text alone.",
    "abstract": "Social media is an important source for news writers. However, extracting useful information for news writers from the vast amount of social media information is laborious. Therefore, services that enable news writers to extract important information are desired. In this paper, we describe a method to extract tweets that include useful information for news writers. Our method uses a Recurrent Neural Network (RNN) with an attention mechanism and multi-task learning that processes each character in the tweet to estimate whether the tweet includes important information. In our experiment, we compared two types of attention mechanism and compared their types with/without multi-task learning. By our proposed method, we obtained an F-measure of 0.627, which is 0.037 higher than that of baseline method.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "3",
    "year": 2017,
    "month": "November",
    "title": "Extracting Important Tweets for News Writers using Recurrent Neural Network with Attention Mechanism and Multi-task Learning"
}