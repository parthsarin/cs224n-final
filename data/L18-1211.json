{
    "article": "In recent years word embedding/distributional semantic models evolved to become a fundamental component in many natural language processing (NLP) architectures due to their ability of capturing and quantifying semantic associations at scale. Word embedding models can be used to satisfy recurrent tasks in NLP such as lexical and semantic generalisation in machine learning tasks, finding similar or related words and computing semantic relatedness of terms. However, building and consuming specific word embedding models require the setting of a large set of configurations, such as corpus-dependant parameters, distance measures as well as compositional models. Despite their increasing relevance as a component in NLP architectures, existing frameworks provide limited options in their ability to systematically build, parametrise, compare and evaluate different models. To answer this demand, this paper describes INDRA, a multi-lingual word embedding/distributional semantics framework which supports the creation, use and evaluation of word embedding models. In addition to the tool, INDRA also shares more than 65 pre-computed models in 14 languages. Introduction Word embedding is a popular semantic model which represents words and sentences in computational linguistics systems and machine learning models. In recent years a large set of algorithms for both generating and consuming word embedding models (WEMs) have been proposed, which includes corpus pre-processing strategies, WEM algorithms or weighting schemes, vector compositions and distance measures (Turney and Pantel, 2010; Lapesa and Evert, 2014; Mitchell and Lapata, 2010) . Determining the optimal set of strategies for a given problem demands the support of a tool that facilitates the exploration of the configuration space of parameters. Furthermore, given the applicability and maturity achieved by these systems and models, they have been promoted from academic prototypes to industry-level applications (Loebbecke and Picot, 2015; Hengstler et al., 2016; Moro et al., 2015) . In this new production scenario, a candidate tool should be able to scale to large number of requests and to the construction of models from large corpora, making use of parallel execution and traceability. From the functional point of view, integrated corpus pre-processing, generation of predictive-based and count-based models and unified access as a service are key features. To support this demand, this paper describes INDRA, a word embedding/distributional semantics framework which supports the creation, use and evaluation of word embedding models. INDRA provides a software infrastructure to facilitate the experimentation and customisation of multilingual WEMs, allowing end-users and applications to consume and operate over multiple word embedding spaces as a service or library. INDRA is available from two repositories (github.com/ Lambda-3/Indra and github.com/Lambda-3/ IndraIndexer) both licensed as open-source software. Additionally, INDRA also provides a Python client (pyindra) available via pip and from github.com/ Lambda-3/pyindra. Related Work S-SPACE is a library to support the construction of countbased distributional methods unifying different approaches in a common JAVA API (Jurgens and Stevens, 2010) . DEEPLEARNING4J 1 , on the other hand, is a library which concentrates predictive-based models. DEEPLEARNING4J is also written in JAVA and its API contains methods to access word vectors and to find nearest neighbours (kNN). GENSIM is one of the most popular word-embedding toolkits, mainly credited to its efficient implementation of nearest neighbours function ( \u0158eh\u016f\u0159ek and Sojka, 2010) . GEN-SIM is written in PYTHON and apart from its kNN function, it supports the generation of predictive-based models and methods to access word vectors. Following a different motivation, DISSECT (DIStributional SEmantics Composition Tookit) focuses on vector compositions (Dinu et al., 2013) . DISSECT is a PYTHON library containing methods to generate vector representation of sentences from the vector of its constituting words. DISSECT partially supports the generation of count-based models and brings an integrated baseline framework for evaluation purposes. JOBIMTEXT is a semantic similarity tool that implements its own algorithm named JoBim (Biemann et al., 2013) EasyESA (Carvalho et al., 2014) and DInfra (Barzegar et al., 2015) are also two initiatives to deliver distributional semantics capabilities under a more specific set of distributional semantic models. From the evaluation point of view, Barzegar et al. (2018c) defined a multi-lingual dataset to measure semantic relatedness in eleven languages. Table 1 summarises a comparative analysis of the main frameworks and their features. Apart from INDRA, none of the listed frameworks gives support to corpora preprocessing (which will be detailed in Section 3.1.2.). Other limitations addressed by Indra are (i) the generation of both count-based and predictive-based models, (ii) the support for vector composition and (iii) the support for translationbased models. Finally those libraries offer a limited set of pre-computed models, which makes the process of exploration timeconsuming and computationally costly. INDRA aims at covering these gaps by providing an end-to-end infrastructure to build, consume and evaluate multi-lingual word embedding models. Implementation Design The INDRA PROJECT is divided into two major modules: INDRAINDEXER and INDRA. INDRAINDEXER is responsible for the generation of the models, whereas INDRA implements the consumption methods. INDRA is designed to be a stand-alone library and also a web service. Figure 1 depicts the main components of its architecture. INDRAINDEXER supports the generation of WEMs directly from text files (Wikipedia-dump or plaintext formats), passing through the corpus pre-processing and multiword expression identification, to the model generation itself. INDRA dynamically builds the pipeline based on the metadata information produced during the model generation. This strategy guarantees that the same set of pre-processing operations are consistently applied to the input query. Additionally, the translation-based word embedding (Freitas et al., 2016; Barzegar et al., 2018b ) can be conveniently activated in the pipeline as described in Section 4.. Different languages, domains and application scenarios require different parametrisations of the underlying embedding models. Together with the availability of pregenerated models, INDRA's system architecture favours the exploration of a large grid of parameters. INDRA currently shares more than 65 pre-computed models which varies in languages, model algorithms and corpora (general-purpose and domain-specific). The list of available models are in the Github project's Wiki. Functionalities Table 1 shows the functionalities implemented in INDRA, among which the following set deserves our attention: text pre-processing, model generation, semantic relatedness, nearest neighbours, vector server, semantic relatedness, vector compositions and the support to translation-based models. Text Pre-processing One important step in the construction of word embeddings models is pre-processing the texts. Defining the tokenisation strategy, which depends on the language, whether or  not words must be lower-cased or stemmed is part of the pre-processing step. Furthermore, the pre-process strategy should be stated consistently during both construction and consumption phases as exemplified further. The corpus pre-processor is responsible for defining the tokenisation strategy and the tokens' subsequent transformations. It defines, for example, if United States of America corresponds to a unique or to multiple tokens. Stem and lowercase are two other popular transformations also supported by the pre-processor. INDRA uses the Lucene's StandardTokenizer, which implements the Unicode Text Segmentation algorithm based on the Word Break rules defined in the Unicode Standard Annex #29 (Davis and Iancu, 2017) . Additionally, IN-DRA allows users to specify a customised list of multi-word expressions which will be considered a unique token, independently of the tokeniser rules. This mechanism allows, for example, modelling a unique vector for named entities such as Nelson Mandela and Republic of Austria. As in the context of WEM numbers are usually disregarded tokens, the pre-processing step allows replacing them by a default placeholder (\u00a1NUMBER\u00bf). INDRA pre-processor also allows specifying stopwords, whose occurrences are removed from the text. Table 2 shows the full list of operations supported by the pre-processor. The pre-processor is defined as a package that is attached to both INDRAINDEXER and INDRA in order to guarantee that the consuming functions apply the same set of operations in retrieval time. Model Generation INDRAINDEXER is the module responsible for the generation of word embedding models. It defines a unified interface to generate predictive-based models (e.g. Skip-gram (Mikolov et al., 2013) and Global Vectors (Pennington et al., 2014) ) and count-based models (e.g. Latent Semantic Analysis (Dumais et al., 1988) and Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007)) whose imple-mentation comes from the libraries DeepLearning4J 2 and S-Space (Jurgens and Stevens, 2010) respectively. In addition to creating a unified interface for WEM algorithms, INDRAINDEXER integrates the corpus pre-processor package. INDRAINDEXER receives as input the pre-processed corpus and outputs the vectors in binary files in a format compatible with GenSim ( \u0158eh\u016f\u0159ek and Sojka, 2010) . In addition to the vector file, INDRAINDEXER also generates a metadata file containing all the parameters from both the preprocessing and generation steps. Figure 3 shows an example of a metadata file. { { \"windowSize\" : 5, \"minWordFrequency\" : 5, \"corpusMetadata\" : { \"corpusName\" : \"wiki-2014\", \"stopWords\" : [\"been\", \"don't\", ...], \"replaceNumbers\" : false, \"applyStemmer\" : 3, \"removeAccents\" : true, \"maxTokenLength\" : 100, \"minTokenLength\" : 3, \"description\" : null, \"language\" : \"en\", \"encoding\" : \"UTF-8\", \"applyLowercase\" : true }, \"vocabSize\" : 1181258, \"sparse\" :false, \"model\" : \"w2v\", \"dimensions\" : 300 } } During the consumption phase, INDRA applies the same set of options to guarantee consistence. For instance, let's assume a given model was generated by applying the stemmer and lowercase to the tokens. It means that the term University is represented in the model as univers. When it is required to retrieve the vector representation of University, INDRA guarantees this consistence by executing the pre-processing steps in the query at runtime. This method simplifies the execution of experiments that consumes models using different set of pre-processing transformations. Nearest Neighbours Given a term and an integer k, the Nearest Neighbours function lists the set of its k closest terms. This method is applied, for instance, in topic modelling ( \u0158eh\u016f\u0159ek and Sojka, 2010) library 3 , since a preliminary study suggests ANNOY's performance is an order of magnitude better performing than GENSIM's ( \u0158eh\u016f\u0159ek, 2014) . In addition to the identification of term's neighbours, the function also accepts a vector as input. Another related function present in INDRA is the selection based on thresholds, in which INDRA gets a query term and a set of target terms as inputs, and returns those target terms whose relatedness score is greater (or lower) than a given threshold. The threshold can be determined both statically or dynamically (Freitas et al., 2012) . Vector Server As a primary use, INDRA acts as a central repository of WEMs, serving vectors for terms in different languages and models. The set of pre-processed models allows the user to experiment different WEMs configurations as a one-stopshop fashion. INDRA can act as a central server in an enterprise context, or as a local library in more constrained environments. Semantic Relatedness Natural language understanding systems use semantic relatedness in fine-grained tasks such as word disambiguation (Freitas et al., 2013) or more coarse-grained such as paraphrase detection (Sales et al., 2016; Silva et al., 2018) , semantic parsing (Sales et al., 2018) and question answering (Freitas, 2015) . INDRA implements two semantic relatedness methods. The first is the pair-wise semantic relatedness in which the user provides pairs of terms to calculate their semantic relatedness. The other option is integrated to the nearest neighbours function which returns the relatedness of the k closest terms. Additionally Indra can support the application of various distance or correlation measures (Lapesa and Evert, 2014) . Currently INDRA supports more than ten different distance and correlation functions, including Cosine, Jaccard, Euclidean and Spearman Correlation. Vector Compositions In simple terms, vector composition aims at generating a vector representation of phrases and sentences from the combination of individual vectors of its compound terms (Kartsaklis, 2014) . For example, the vector representation of modern Democratic Party is generated by 3 https://github.com/spotify/annoy the composition of the corresponding vectors of the three compounding terms modern, Democratic and Party. Currently, INDRA implements three composition methods (Sum, Normalised Sum and Average) and supports the extension of user-defined functions as described in Section 3.2.. Vector composition is automatically associated to the semantic relatedness function or the retrieval of vectors. Whenever a expression comprehending more than one token is submitted, INDRA composes their corresponding vectors before executing the required function. 3.1.7. Support to Translation-based Models Some languages do not have large text corpora publicly available. As word embedding models are sensitive to the corpus size, (Freitas et al., 2016; Barzegar et al., 2018a) propose the use of translation-based models. In simple words, the translation-based strategy translates the original query terms to a second language for which a high quality WEM is available. INDRA gives native support to this operation as described in Section 4.. Extensibility INDRA implements a plugin-based extensible mechanism built on the top of the JAVA SERVICE API which allows including new compositional methods, score functions and threshold functions without recompiling Indra's code. To do so, it is required to pack the new functions' implementations in a JAR file and place it in the INDRA's classpath 4 . Use Examples INDRA's service exposes the functions as POST methods, whose data are passed as a JSON payload. For simplicity, we suppress the request headers to concentrate our attention in the payload itself. Every request has at least three mandatory fields: language, model and corpus. The first naturally specifies the request's language. The second and the third name respectively the algorithm and the corpus from which the word embedding model were generated. This trio is the model's unique identifier. Word Embedding: Figure 3 shows a payload to the endpoint /vectors which returns the respective word embedding vectors of the terms. In the case a term is composed of more than one token, termComposition is applied. { \"corpus\": \"googlenews300neg\", \"model\": \"W2V\", \"language\": \"EN\", \"terms\": [\"love\", \"best of you\"], \"termComposition\" : \"AVERAGE\" } Pair Relatedness: The endpoint /relatedness returns the semantic relatedness of the pairs. The relatedness operation is defined by the field scoreFunction as shown in Figure 4 . In the case that termComposition is not defined, the default function is used. { \"corpus\": \"wiki-2014\", \"model\": \"ESA\", \"language\": \"PT\", \"scoreFunction\": \"COSINE\", \"pairs\": [{ \"t1\": \"economia\", \"t2\": \"Rio de Janeiro\" }, { \"t1\": \"economia\", \"t2\": \"soja\" }] } One-to-many Relatedness: The endpoint /relatedness/otm returns the semantic relatedness of one term against a set of many terms. Similarly to the previous operation, the relatedness operation is defined by the field scoreFunction as shown in Figure 5 . Nearest Neighbours: The nearest neighbours function is exposed in two methods. The endpoint /neighbors/ relatedness returns the relatedness score between the target terms and theirs top-k neighbours, according to the payload depicted in Figure 6 . When submitting the same payload to the endpoint /neighbors/vectors, the service returns the list of the neighbours and their respective vectors. Translated-based Word Embbedings: The requests support the translated-based function, in which the vectors is extracted from the corresponding English corpus after translating the terms from the original query. The translated-based function is activated by appending { \"corpus\": \"wiki-2014\", \"model\": \"ESA\", \"language\": \"EN\", \"scoreFunction\": \"JACCARD\", \"one\": \"Germany\", \"many\" : [\"France\", \"China\", \"Brazil\"] } { \"corpus\": \"wiki-2014\", \"model\": \"GLOVE\", \"language\": \"SV\", \"topk\": 10, \"terms\": [\"ekonomi\", \"flicka\", \"frihet\"] } \"mt\"=true in the payload. INDRA offers seven precomputed light-weight translation models. For a complete description of the methods and parameters, please refer to the project documentation. Python Client Our project also offers a client to access the service from Python application. The pyindra package is available in the pip repository. The client source code is at https://github.com/ Lambda-3/pyindra. Summary Many applications of word embedding models require the customisation of the models in the direction of domainspecific vocabularies, specific languages or specific semantic approximation behaviour (e.g. paradigmatic vs syntagmatic behaviour), distance measures as well as compositional models. This work introduces the INDRA framework which manages the complexity of experimenting and using word embedding models in exploratory scenarios and production environments. INDRA also shares more than 65 pre-computed models and is available as an open-source software. Acknowledgment This publication has emanated from research partially supported by the National Council for Scientific and Technological Development, Brazil (CNPq). References Atzori, M., Balloccu, S., and Bellanti, A. (2018) . Unsupervised singleton expansion from free tex. In 2018 IEEE",
    "abstract": "In recent years word embedding/distributional semantic models evolved to become a fundamental component in many natural language processing (NLP) architectures due to their ability of capturing and quantifying semantic associations at scale. Word embedding models can be used to satisfy recurrent tasks in NLP such as lexical and semantic generalisation in machine learning tasks, finding similar or related words and computing semantic relatedness of terms. However, building and consuming specific word embedding models require the setting of a large set of configurations, such as corpus-dependant parameters, distance measures as well as compositional models. Despite their increasing relevance as a component in NLP architectures, existing frameworks provide limited options in their ability to systematically build, parametrise, compare and evaluate different models. To answer this demand, this paper describes INDRA, a multi-lingual word embedding/distributional semantics framework which supports the creation, use and evaluation of word embedding models. In addition to the tool, INDRA also shares more than 65 pre-computed models in 14 languages.",
    "countries": [
        "Ireland",
        "United Kingdom",
        "Germany"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "14",
    "year": "2018",
    "month": "May",
    "title": "{I}ndra: A Word Embedding and Semantic Relatedness Server"
}