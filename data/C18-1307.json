{
    "article": "Chinese word segmentation (CWS) trained from open source corpus faces dramatic performance drop when dealing with domain text, especially for a domain with lots of special terms and diverse writing styles, such as the biomedical domain. However, building domain-specific CWS requires extremely high annotation cost. In this paper, we propose an approach by exploiting domain-invariant knowledge from high resource to low resource domains. Extensive experiments show that our model achieves consistently higher accuracy than the single-task CWS and other transfer learning baselines, especially when there is a large disparity between source and target domains. Introduction Chinese word segmentation (CWS) is a fundamental task for Chinese natural language processing (NLP). Most state-of-art methods are based on statistical supervised learning and neural networks. They all rely heavily on human-annotated data, which is a time-consuming and expensive work. Specially, for domain CWS, e.g., medical field , the annotation expense is even higher because only domain experts are qualified for the work. Moreover, CWS tools trained from open source datasets, e.g., SIGHAN2005 1 , face a significance performance drop when dealing with domain text. The ambiguity caused by domain terms and writing style makes it extremely difficult to train a universal CWS tool. As shown in Table 1 , given a medical term \"\u9ad8\u94c1\u8840\u7ea2\u86cb\u767d\u8840\u75c7\" (methemoglobinemia), Chinese medical experts would annotate it as \"\u9ad8/\u94c1/\u8840\u7ea2 \u86cb\u767d/\u8840\u75c7\", which means anemia caused by hemoglobin with \"high iron\" (in Chinese, means iron with valence of 3), corresponding to the morphology of \"Methemoglobinemia\". \"PKU\" stands for a model trained on PKU's People's Daily corpus, we can see that after segmentation, the word \"\u94c1\u8840\" (jagged) is treated as one word, which is wrong semantically. Also, another popular Chinese CWS tool Jieba 2 mistakenly puts the characters \"\u9ad8\" and \"\u94c1\" together, which stands for the high-speed bullet train in China. 2. Annotated domain data is scarce due to high cost. CWS tool 3. How to leverage open source annotated data despite their generality is an open question. Recently, efforts have been made to exploit open source (high resource) data to improve the performance of domain specific (low resource) tasks and decrease the amount of domain annotated data (Yang et al., 2017; Peng and Dredze, 2016; Mou et al., 2016) . In this paper, we further this line of work by developing a multi-task learning (Caruana, 1997; Peng and Dredze, 2016) framework, named Adaptive Multi-Task Transfer Learning. Inspired by the success of Domain Adaptation (Saenko et al., 2010; Tzeng et al., 2014; Long and Wang, 2015b) , we propose to minimize distribution distance of hidden representation between the source and target domain, thus make the hidden representations adapt to each other and obtain domain-invariant features. Finally, we annotated 3 medical datasets from different medical departments and medical forum, together with 3 open source datasets ?? . The contribution of this paper can be summarized as follows: \u2022 We propose a novel framework for Chinese word segmentation in the medical domain. \u2022 To the best of our knowledge, we are the first to analyze the performance of transfer learning methods against the amount of disparity between target/source domains. \u2022 Our framework outperforms strong baselines especially when there is substantial disparity. \u2022 We open source 3 medical CWS datasets from different sources, which can be used for further study. 2 Related Work Chinese word segmentation Statistical Chinese word segmentation has been studied for decades. Xue and others (2003) was the first to treat it as a sequence tagging problem, using a maximum entropy model. Peng et al. (2004) achieved better results by using a conditional random field model (Lafferty et al., 2001) . This method has been followed by many other works (Zhao et al., 2006; Sun et al., 2012) . Recently, neural network models have been applied on CWS. These methods use automatically derived features from neural network instead of hand-crafted discrete features. Zheng et al. (2013) first adopted neural network architecture to CWS. Chen et al. (2015b) used Long short-term memory(LSTM) to capture long term dependency. Chen et al. (2015a) proposed a gated recursive neural network (GRNN) to incorporate context information. In this paper, we adopt Bidirectional LSTM-CRF Models (Huang et al., 2015) as our base model. Transfer Learning Transfer learning distills knowledge from source domain and helps target domain to achieve a higher performance (Pan and Yang, 2010) . In feature-based models, many transfer approached have been studied, including instance transfer (Jiang and Zhai, 2007; Liao et al., 2005) , feature representation transfer (Argyriou et al., 2006; Argyriou et al., 2007), parameter transfer(Lawrence and Platt, 2004; Bonilla et al., 2007) and relation knowledge transfer (Mihalkova et al., 2007; Mihalkova and et al., 2009) . Recently, the transferability of neural networks is also studied. For example, (Mou et al., 2016 ) studied two methods (INIT, MULT) on NLP applications. Peng and Dredze (2016) proposed to use domain mask and linear projection upon multi-task learning (MTL) (Long and Wang, 2015a) . In this paper, we follow MTL and extend the framework with a novel loss function. Single-Task Chinese word segmentation In this section, we briefly formulate the Chinese word segmentation task and introduce our base model, Bi-LSTM-CRF (Huang et al., 2015) . Problem Formulation Chinese word segmentation is often treated as a sequence tagging problem on character level. BIES tagging scheme is broadly accepted by annotators, each character in sentence is labeled as one of L = {B, I, E, S}, indicating begin, inside, end of a word, and a word consisting of a single character. Given a sequence with n characters X = {x 1 , . . . , x n }, the aim of the CWS task is to find a mapping from X to Y * = {y * 1 , . . . , y * n }: Y * = arg max Y \u2208L n p(Y |X) (1) where L = {B, I, E, S} The general architecture of neural CWS contains: (1) a character embedding layer; (2) an encoder automatically extracts feature and (3) a decoder inferences tag from the feature. In this paper, we utilize a widely-used model as the base of our framework, which consists of a bidirectional long short-term memory neural network (BiLSTM) as encoder and conditional random fields (CRF) (Lafferty et al., 2001) as decoder. Encoder In neural network models, an encoder is usually adopted to automatically extract feature instead of human-crafted feature engineering. Bi-LSTM LSTM is a popular variant of RNN in order to alleviate the vanishing gradient problem (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997) . In addition to considering past information from left, Bidirectional LSTM also captures future information from the right of the token. Decoder We deploy a conditional random fields layer as decoder. Specifically, p(Y |X) in Eq. ( 1 ) could be formulated as p(Y |X) = exp(\u03a6(X, Y )) Y \u2208L n exp(\u03a6(X, Y )) (2) Here, \u03a6(\u2022) is a potential function, consider the situation that we only take the influence between two consecutive variables into account: \u03a6(X, Y ) = n j=1 \u03c6(X, i, yi, yi\u22121) (3) \u03c6(X, i, yi, yi\u22121) = s(X, i)y i + ty i y i\u22121 (4) where s(X, i) \u2208 R |L| is a function that measure the score of the i th character for each label in L = {B, I, E, S}, and t \u2208 R |L|\u00d7|L| denotes the transition score between labels. More formally: s(X, i) = W hi + b (5) where h i is the hidden state of the i th character after BiLSTM; W \u2208 R d h \u00d7|L| and b \u2208 R |L| are all parameters in the model. Adaptive Multi-Task Transfer Learning With the motivation to leverage domain-invariant knowledge from high resource domain, we utilize the framework of multi-task learning (Caruana, 1997) , which is one of the methods in transfer learning, and further introduce three models under the proposed Adaptive Multi-Task Transfer Learning framework (AMTTL). We exploit three statistical distance measures as the Adaptive part to test the generality of our framework. Notations and Definitions In this paper, multi-task learning is defined as a dual-task learning, which contains two domains D S and D T . Our purpose is to improve the performance of target domain by exploiting knowledge from source domain. Each domain D contains two components: a feature space X and a marginal probability distribution P (X), where X is a sample sentence, and X = {x 1 , . . . , x n } \u2208 X . Given a single domain, D = {X , P (X)}, a task contains two components: a label space Y and a predictive function f (\u2022), which can be learned during the training phase. Formally, T = {Y, f (\u2022)}. Formal Definition We now give the definition of Adaptive Multi-Task Transfer Learning. Objective Function The objective function of our proposed Adaptive Multi-Task Transfer Learning can be formulated as follows: J (\u03b8 (a) , \u03b8 (b) ) = Jseg + \u03b1J Adap. + \u03b2JL 2 (6) where \u03b8 (a) and \u03b8 (b) are model parameters for task a and b, \u03b1 and \u03b2 are hyper-parameters to be chosen. J seg stands for the negative log likelihood for source domain and target domain. At each training step, we minimize the mean negative log likelihood: Jseg = \u2212 1 n n i=1 log p(Y (a) i |X (a) i ) \u2212 1 m m i=1 log p(Y (b) i |X (b) i ) (7) J Adap. is the Adaptive loss used to capture domain-invariant knowledge between different domains, which forces the hidden representations between two domains to adapt to each other. Given two sets of hidden representation, denoted as h (a) and h (b) , and a statistic distance function g(\u2022), J Adap. can be calculated as: JAdap. = g(h (a) , h (b) ) (8) where g(\u2022) can be, but is not limited to, KL divergence, maximum mean discrepancy (MMD) (Gretton et al., 2012) or central moment discrepancy (CMD) (Zellinger et al., 2017) ; h (a) and h (b) are different for different model setting, which will be defined in Sec 4.4. J L 2 is the L 2 regularization which is used to control overfitting problem: JL 2 = \u03b8 (a) 2 2 + \u03b8 (b) 2 2 (9) Models In this section, we present the design of three variants of our framework in detail. The architectures are presented in Figure 1 . Model-I Specific LSTM This model can be interpreted as two parallel tasks connected with J Adap. after specific Bi-LSTM layers of two tasks. We design the model in order to see whether knowledge can actually be transfered through the Adaptive loss alone. The hidden representation and CRF score of task t at position i can be computed as: Xa X b CRF CRF Ya Y b J Adap. (a) Model-I CRF CRF Ya Y b J Adap. SHARED Xa X b (b) Model-II Xa X b CRF CRF Ya Y b J Adap. (c) Model-III Figure 1 : Three models with different settings. The white block represents Embedding lookup layer, while the gray and black block represents Bi-LSTM layer. The \"SHARED\" in Figure 1b stands for shared Bi-LSTM for both tasks. The \"J Adap. \" represents Adaptive loss for the hidden representation after corresponding layer, which is formally discussed in Sec 4.3. The solid arrow and dotted arrow show the flow of task a and task b respectively. h (t) i = Bi-LSTM(X (t) , \u03b8 (t) ) (10) s(X, i) (t) = W (t) h (t) i + b (t) (11) where h t) denotes parameters of domain specific Bi-LSTM. The J Adap. between two tasks, denoted by a and b, is formulated as: (t) i \u2208 R 2d h , W (t) \u2208 R 2d h \u00d7|L| , b (t) \u2208 R |L| , \u03b8 ( JAdap. = g(h (a) , h (b) ) (12) where t) is a batch of input sequences. h (t) = {h (t) i |X (t) \u2208 X (t) }, X ( Model-II Shared LSTM Model-II is designed to adopt domain specific embedding layers, shared Bi-LSTM layer and domain specific CRF layers. Note that traditional multi-task learning uses shared embedding (Ruder, 2017) . Shared embedding means that source and target domain share the same set of embedding parameters while domain-specific embedding means that the two domains maintain their own sets. The hidden representation of task t at position i can be computed as: h (t) i = Bi-LSTM(X (t) , \u03b8) (13) where two tasks share Bi-LSTM parameter \u03b8, which is the only difference with Model-I. CRF score and J Adap. is the same as (11)(12). Model-III Shared & Specific LSTM Model-III is a combination of Model-I and Model-II, with both domain specific and shared Bi-LSTM layers. The hidden representation and CRF score of task t at position i can be computed as: h (t) i = Bi-LSTM(X, \u03b8 (t) ) \u2295 Bi-LSTM(X, \u03b8) = h (t) i(specif ic) \u2295 h (t) i(shared) (14) s(X, i) (t) = W (t) h (t) i + b (t) (15) where h t) and \u03b8 denote the parameter of domain specific and shared Bi-LSTM. J Adap. can be calculated as : (t) i \u2208 R 4d h , W (t) \u2208 R 4d h \u00d7|L| , and b (t) \u2208 R |L| . \u03b8 ( J Adap. = g(h (a) , h (b) ) (16) where h (t) = {h (t) i(specif ic) |X (t) \u2208 X (t) }, X (t) is a batch of input sequences. Experiment In this section, we evaluate our proposed models on real-world medical Chinese word segmentation tasks 3 , where annotated data is scarce and domain-drift is significant with open source annotated data. We conduct extensive experiments and discuss the result in detail. We also conduct an ablation test. Datasets We use three open source CWS datasets, namely PKU and MSR from SIGHAN2005 Bakeoff 4 and WEIBO from (Qiu et al., 2016) . The information of the datasets is shown in Table 2a . We annotated three medical datasets for our experiment and future research. The first two datasets are electronic medical records (EMR) from different departments. The third dataset is medical forum data from Good Doctor Online 5 , which is a Chinese forum for medical consult. The information of the datasets is shown in Table 2b . The electronic medical records are collected from our partner hospital, the data only permits noncommercial/academical use. The annotation was done by several doctors. It was carried out following a Chinese word segmentation criteria 6 created by Institute of Computational Linguistics at Peking University. For quality control, annotators were trained until they achieve about 80% inter-annotator agreement on previously annotated materials. Then we conducted double-blind annotation, with resolution of disagreements by a senior annotator. The entire annotation process follows Cohen et al. (2017) . On medicolegal issues, the EMR data we received had already been anonymized and de-identified. Given the fact that China doesn't offer an act like HIPPA(Health Insurance Portability and Accountability Act), we only released the medical forum dataset. Disparity Study Transfer Learning aim to improve the performance of low-resource domain task by exploiting the annotated data form high-resource domain, thus the Disparity between different tasks is a leading factor to influence the transferability between different domains with different methods. In this paper, we used X 2 test (Kilgarriff and Rose, 1998) to quantify the Disparity between three medical corpus. If the size of corpus 1 and corpus 2 are N 1 , N 2 and word w has observed frequencies o w,1 , o w,2 , then expected value e w,1 = N 1 \u00d7(o w,1 +o w,2 ) N 1 +N 2 , and likewise for e w,2 , then X 2 = (o \u2212 e) 2 e (17) X 2 test shows that Disparity between forum dataset and two EMR datasets are similar, but both are much larger than the Disparity between the two EMR datasets, as shown in Table 3 . Due to the fact that X 2 test doesn't permit comparison between corpus of different sizes (Kilgarriff and Rose, 1998) , we propose a simple agreement test, using the size of the intersection between the most common n tokens (bi-gram) to quantify the disparity between medical corpus and open source corpus. We set n to 500.  Agreement test shows that the Disparity between PKU/MSR and two EMR datasets are close, both far larger than the Disparity between PKU/MSR and forum dataset. WEIBO dataset is more similar with medical datasets than PKU and MSR. Single-task Performance Before introducing our experiments on proposed framework, we first evaluate the effectiveness of the single-task model (Bi-LSTM-CRF), which is our base model. We compare the model with the two state-of-art on Chinese word segmentation, proposed by Cai and Zhao (2016) and Zhang et al. (2016) respectively. We run experiments on our datasets with their code released on github 7,8 . The results show that the performance of single-task model and state-of-art are close, as shown in Table 5 , which indicates the single-task model is a strong baseline for our advanced models. Training The training phrase aims to optimize the model parameters \u03b8 (a) and \u03b8 (b) by minimizing the objective function defined in Eq. ( 6 ). We use Adam (Kingma and Ba, 2014) with mini-batch. Each batch contains sentences from both domains. The hyper-parameter setting is discussed later. Experiment Settings The dimension of character embedding and the LSTM hidden state dimension are 50. The batch size is 30. We evaluate our framework for a total of 15 transfer learning tasks. For each task, we take all of source training data and 10% of target training data. Hyper-parameters are determined by tuning against the development set. Baselines Several baseline methods are compared. Single-task uses target domain data only, as discussed in Section 3. INIT fine-tunes the model trained on source domain using target domain data. Multi-Task shares parameter for both source and target domain, the model is trained simultaneously. Linear Projection shares encoder and projects hidden representations into specific feature space. Domain Mask shares encoder and select different part of hidden representation for source and target domains. (Peng and Dredze, 2016) . Hyper-parameter In our framework, we have two hyper-parameters \u03b1 and \u03b2, which controls the weight of J Adap. and J L 2 . Our experiments show that \u03b1 \u2208 [0.3, 0.7] and \u03b2 \u2208 [0.2, 0.3] works best. Next, we analyze the result from a special aspect, the Disparity between source and target datasets: Result and Discussion 1. In Table 6 , INIT outperforms all other baselines and our approaches in task R \u2192 C and C \u2192 R, but downperforms our approaches in the others. We argue that the effectiveness of INIT on task between domain R and C result from the low Disparity between the two domains, as shown in Table 3 . We speculate that the INIT approach works so well between domains with low disparity because: (a) well trained model in the source domain provides a good start point for training in the target domain, which is very similar to the source; (b) the final model is fine-tuned against the target domain only. Our method is disadvantaged in this scenario because: (a) our model parameters are randomly initialized and are independent between two domains (except for the shared parameters), thus it cannot inherit so much information from the source domain as INIT does; (b) the final model is fine-tuned against both the source and the target domain at the same time; thus noise from the source domain may be introduced into the target domain. This is a research problem we want to tackle in the future. 2. We first refer to Then we can find that, in 4 tasks of high disparity, our approach outperforms all baselines. When disparity goes down to the second level, our approach underperforms INIT but only with gap of 0.4%. However, when disparity continuously goes down to the third and forth level, INIT outperforms our approach by 3-4%. At last, we'd like to discuss the effect of transferring from a general-domain dataset (which has the advantage of larger quantity) against that of transferring from a medical dataset (which is better at quality). After comparing the tasks with the same target domain, we conclude that quality weighs more than quantity. Taking Cardiology as an example, the size of source training set used in cross-medical (high quality) tasks is only 1 percent of that used in general-to-medical (high quantity) tasks, but the cross-medical results still outperform the latter. Ablation Test To investigate the effectiveness of different components in our framework, we do ablation test based on Model-II on task (P \u2192 R) with J Adap. calculated by MMD. Results are reported in Table 8 . Model-II w/o shared Bi-LSTM uses domain-specific Bi-LSTM, while Model-II w/o specific embedding uses shared embedding for both domains. Results show that the choice of statistic distance measure weights least, since the performance of different measures are close. The test verifies our choice of shared Bi-LSTM and specific embedding. (2) outperforms all baselines when the disparity between target and source dataset is high. For future work, we plan to study the transferability between different tasks for Chinese NLP and cross-lingual NLP tasks. Acknowledgments We would like to thank Kevin Bretonnel Cohen for his suggestions through the COLING Writing Mentoring Program.",
    "abstract": "Chinese word segmentation (CWS) trained from open source corpus faces dramatic performance drop when dealing with domain text, especially for a domain with lots of special terms and diverse writing styles, such as the biomedical domain. However, building domain-specific CWS requires extremely high annotation cost. In this paper, we propose an approach by exploiting domain-invariant knowledge from high resource to low resource domains. Extensive experiments show that our model achieves consistently higher accuracy than the single-task CWS and other transfer learning baselines, especially when there is a large disparity between source and target domains.",
    "countries": [
        "Slovakia",
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "15",
    "year": "2018",
    "month": "August",
    "title": "Adaptive Multi-Task Transfer Learning for {C}hinese Word Segmentation in Medical Text"
}