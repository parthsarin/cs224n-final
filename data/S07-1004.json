{
    "article": "The Multilingual Chinese-English lexical sample task at SemEval-2007 provides a framework to evaluate Chinese word sense disambiguation and to promote research. This paper reports on the task preparation and the results of six participants. Introduction The Multilingual Chinese-English lexical sample task is designed following the leading ideas of the Senseval-3 Multilingual English-Hindi lexical sample task (Chklovski et al., 2004) . The \"sense tags\" for the ambiguous Chinese target words are given in the form of their English translations. The data preparation is introduced in the second section. And then the participating systems are briefly described and their scores are listed. In the conclusions we bring forward some suggestion for the next campaign. Chinese Word Sense Annotated Corpus All the training and test data come from the People's Daily in January, February and March of 2000. The People's Daily is the most popular newspaper in China and is open domain. Before manually sense annotating, the texts have been word-segmented and part of speech (PoS) tagged according to the PoS tagging scheme of Institute of Computational Linguistics in Peking University (ICL/PKU). The corpus had been used as one of the gold-standard data set for the second international Chinese word segmentation bakeoff in 2005. 1 Manual Annotation The sense annotated corpus is manually constructed with the help of a word sense annotating interface developed in Java. Three native annotators, two major in Chinese linguistics and one major in computer science took part in the construction of the sense-annotated corpus. A text generally is first annotated by one annotator and then verified by two checkers. Checking is of course a necessary procedure to keep the consistency. Inspired by the observation that checking all the instances of a word in a specific time frame will greatly improve the precision and accelerate the speed, a software tool is designed in Java to gather all the occurrences of a word in the corpus into a checking file with the sense KWIC (Key Word in Context) format in sense tags order. The interannotator agreement gets to 84.8% according to Wu. et al. (2006) . The sense entries are specified in the Chinese Semantic Dictionary (CSD) developed by ICL/PKU. The sense distinctions are made mainly according to the Contemporary Chinese Dictionary, the most widely used dictionary in mandarin Chinese, with necessary adjustment and improvement is implemented according to words usage in real texts. Word senses are described using the featurebased formalism. The features, which appear in the form \"Attribute =Value\", can incorporate extensive distributional information about a word sense. The feature set constitutes the representation of a sense, while the verbal definitions of meaning serve only as references for human use. The English translation is assigned to each sense in the attribute \"English translation\" in CSD. Based on the sense-annotated corpus, a sense is replaced by its English translation, which might group different senses together under the same English word. Instances selection In this task together 40 Chinese ambiguous words: 19 nouns and 21 verbs are selected for the evaluation. Each sense of one word is provided at least 15 instances and at most 40 instances, in which around 2/3 is used as the training data and 1/3 as the test data. In order to escape from the sense-skewed distribution that really exists in the corpus of People's Daily, many instances of some senses have been removed from the sense annotated corpus. So the sense distribution of the ambiguous words in this task does not reflect the usages in real texts. Participating Systems In order to facilitate participators to select the features, we gave a specification for the PoS-tag set. Both word-segmented and un-segmented context are provided. Two kinds of precisions are evaluated. One is micro-average: \u2211 \u2211 = = = N i i N i i mir n m P 1 1 / N is the number of all target word-types. is the number of labeled correctly to one specific tar-get word-type and is the number of all test instances for this word-type. i m i n The other is macro-average: \u2211 = = N i i mar N p P 1 / , i i i n m p / = All teams attempted all test instances. So the recall is the same with the precision. The precision baseline is obtained by the most frequent sense. Because the corpus is not reflected the real usage, the precision is very low. Six teams participated in this word sense disambiguation task. Four of them used supervised learning algorithms and two used un-supervised method. For each team two kinds of precision are given as in table 2. As follow the participating systems are briefly introduced. SRCB-WSD system exploited maximum entropy model as the classifier from OpenNLP 2 The following features are used in this WSD system: \u2022 All the verbs and nouns in the context, that is, the words with tags \"n, nr, ns, nt, nz, v, vd, vn\" \u2022 PoS of the left word and the right word \u2022 noun phrase, verb phrase, adjective phrase, time phrase, place phrase and quantity phrase. These phrases are considered as constituents of context, as well as words and punctuations which do not belong to any phrase. \u2022the type of these phrases which are around the target phrases \u2022 word category information comes from Chinese thesaurus I2R system used a semi-supervised classification algorithm (label propagation algorithm) (Niu, et al., 2005) . They used three types of features: PoS of neighboring words with position information, unordered single words in topical context, and local collocations. In the label propagation algorithm (LP) (Zhu and Ghahramani, 2002) , label information of any vertex in a graph is propagated to nearby vertices through weighted edges until a global stable stage is achieved. Larger edge weights allow labels to travel through easier. Thus the closer the examples, the more likely they have similar labels (the global consistency assumption). In label propagation process, the soft label of each initial labeled example is clamped in each iteration to replenish label sources from these labeled data. Thus the labeled data act like sources to push out labels through unlabeled data. With this push from labeled examples, the class boundaries will be pushed through edges with large weights and settle in gaps along edges with small weights. If the data structure fits the classification goal, then LP algorithm can use these unlabeled data to help learning classification plane. CITYU-HIF system was a fully supervised one based on a Na\u00efve Bayes classifier with simple feature selection for each target word. The features used are as follows: \u2022 Local features at specified positions: PoS of word at w -2 , w -1 , w 1 , w 2 Word at w -2 , w -1 , w 1 , w 2 \u2022 Topical features within a given window: Content words appearing within w -10 to w 10 \u2022 Syntactic features: PoS bi-gram at w -2 w 0 , w -1 w 0 , w 0 w 1 , w 0 w 2 PoS tri-gram at w -2 w -1 w 0 and w 0 w 1 w 2 One characteristic of this system is the incorporation of the intrinsic nature of each target word in disambiguation. It is assumed that WSD is highly lexically sensitive and each word is best characterized by different lexical information. Human judged to consider for each target word the type of disambiguation information if they found useful. During disambiguation, they run two Na\u00efve Bayes classifiers, one on all features above, and the other only on the type of information deemed useful by the human judges. When the probability of the best guess from the former is under a certain threshold, the best guess from the latter was used instead. SWAT system uses a weighted vote from three different classifiers to make the prediction. The three systems are: a Na\u00efve Bayes classifier that compares similarities based on Bayes' Rule, a classifier that creates a decision list of context features, and a classifier that compares the angles between vectors of the features found most commonly with each sense. The features include bigrams, and trigrams, and unigrams are weighted by distance from the ambiguous word. TorMd used an unsupervised naive Bayes classifier. They combine Chinese text and an English thesaurus to create a `Chinese word'--`English category' co-occurrence matrix. This system generated the prior-probabilities and likelihoods of a Na\u00efve Bayes word sense classifier not from senseannotated (in this case English translation annotated) data, but from this word--category cooccurrence matrix. They used the Macquarie Thesaurus as very coarse sense inventory. They asked a native speaker of Chinese to map the English translations of the target words to appropriate thesaurus categories. Once the Na\u00efve Bayes classifier identifies a particular category as the intended sense, the mapping file is used to label the target word with the corresponding English translation. They rely simply on the bag of words that co-occur with the target word (window size of 5 words on either side). HIT is a fully unsupervised WSD system, which puts bag of words of Chinese sentences and the English translations of target ambiguous word to search engine (Google and Baidu). Then they could get all kinds of statistic data. The correct translation was found through comparing their cross entropy. Conclusion The goal of this task is to create a framework to evaluate Chinese word sense disambiguation and to promote research. Although the SRCB-WSD system got the highest scores among the six participants, it does not perform always better than other system from table 2 and table 3. But to each word, the four supervised systems always predict correctly more instances than the two un-supervised systems. Scores Besides the corpus, we provide a specification of the PoS tag set. Only SRCB-WSD system utilized this knowledge in feature selection. We will provide more instances in the next campaign. Acknowledgements We would like to thank Tao Guo and Yulai Pei for their hard work to guarantee the quality of the corpus. Huiming Duan provides us the corpus which has been word-segmented and PoS-tagged and gives some suggestions during the manual annotation.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 1.9361263126072004e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the research or the preparation of the manuscript."
}