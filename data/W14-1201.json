{
    "article": "This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures. The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu\u00edsio et al., 2008b) , and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999) , autism spectrum disorder ( \u0160tajner et al., 2012; Martos et al., 2012) , dyslexia (Rello, 2012) , congenital deafness (Inui et al., 2003) , and intellectual disability (Feng, 2009) . At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996) . This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood-send and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012) , Spanish (Saggion et al., 2011) , and Portuguese (Alu\u00edsio et al., 2008a) , with recent attempts at Basque (Aranzabe et al., 2012) , Swedish (Rybing et al., 2010) , Dutch (Ruiter et al., 2010) , and Italian (Barlacchi and Tonelli, 2013) . Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems is commonly evaluated by using a combination of readability metrics (measuring the degree of simplification) and human assessment (measuring the grammaticality and meaning preservation). Despite the noticeable similarity between evaluation of the fluency and adequacy of a machine translation (MT) output, and evaluation of grammaticality and meaning preservation of a TS system output, there have been no works exploring whether any of the MT evaluation metrics are well correlated with the latter, and could thus replace the time-consuming human assessment. The contributions of the present work are the following: \u2022 It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. \u2022 It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. \u2022 It proposes a decision-making procedure for the classification of simplified sentences into: (1) those which are acceptable; (2) those which need further post-editing; and (3) those which should be discarded. 1 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. ( 2013 ) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b) , and Glava\u0161 and \u0160tajner (2013) used human annotators' ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from \"original\" to \"simplified\" language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010) . In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002) , NIST (Doddington, 2002) , and TERp (Snover et al., 2009) . Methodology All experiments were conducted on a freely available sentence-level dataset 1 , fully described in (Glava\u0161 and \u0160tajner, 2013) , and the two datasets we derived from it. The original dataset and the instructions for the human assessment are given in the next two subsections. Section 3.3 explains how we derived two additional datasets from the original one, and to what end. Section 3.4 describes the automatic MT evaluation metrics used as features in correlation and classification experiments; Section 3.5 presents the main goals of the study; and Section 3.6 describes the conducted experiments. Original dataset The dataset contains 280 pairs of original sentences and their corresponding simplified versions annotated by humans for grammaticality, meaning preservation, and simplicity of the simplified version. We used all sentence pairs, focusing only on four out of eight available features: (1) the original text, (2) the simplified text, (3) the grammaticality score, and (4) the score for meaning preservation. (Glava\u0161 and \u0160tajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003) . Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each factual event mention into a separate sentence of the output. The last simplification scheme (pronominal anaphora) additionally employs pronominal anaphora resolution on top of the event-wise simplification scheme. 3 Human Assessment Human assessors were asked to score the given sentence pairs (or text snippets in the case of split sentences) on a 1-3 scale based on three criteria: Grammaticality (1 -ungrammatical, 2 -minor problems with grammaticality, 3 -grammatical), Meaning (1 -meaning is seriously changed or most of the relevant information lost, 2 -some of the relevant information is lost but the meaning of the remaining information is unchanged, 3 -all relevant information is kept without any change in meaning), and Simplicity (1 -a lot of irrelevant information is retained, 2 -some of irrelevant information is retained, 3 -all irrelevant information is eliminated). The inter-annotator agreement (IAA) was calculated using weighted Kappa (weighted \u03ba), Pearson's correlation (Pearson), and mean average error (MAE), and the obtained results are presented in Table 1 . A few examples of assigned scores are given in Table 2 , where G, M, and S denote human scores for grammaticality, meaning preservation and simplicity respectively. score for simplicity, which are not relevant here. 3 For more detailed explanation of simplification schemes and the dataset see (Glava\u0161 and \u0160tajner, 2013) . Ex. Original Simplified G M S SM (a) \"It is understood the dead girl had been living at her family home, in a neighbouring housing estate, and was visiting her older sister at the time of the shooting.\" \"The dead girl had been living at her family home, in a neighbouring housing estate and was visiting her older sister.\" 3 3 3 S (b) \"On Facebook, more than 10,000 people signed up to a page announcing an opposition rally for Saturday.\" \"On Facebook, more than 10,000 people signed to a page announcing an opposition rally for Saturday.\" 2 3 3 S (c) \"Joel Elliott, Derived Datasets The original dataset (Original) contains separate scores for grammaticality (G), meaning preservation (M), and simplicity (S), each of them on a 1-3 scale. From this dataset we derived two additional ones: Total3 and Total2. The Total3 dataset contains three marks (OKuse as it is, PE -post-editing required, and Dis -discard) derived from G and M in the Original dataset. Those simplified sentences which scored '3' for both meaning preservation (M) and grammaticality (G) are placed in the OK class as they do not need any kind of post-editing. A closer look at the remaining sentences suggests that any simplified sentence which got a score '2' or '3' for meaning preservation (M) could be easily postedited, i.e. it requires minimal changes which are obvious from its comparison to the corresponding original. For instance, in the sentence (b) in Table 2 the only change that needs to be made is adding the word \"up\" after \"signed\". Those sentences which scored '2' for meaning need slightly more, albeit simple modification. The simplified text snippet (c) in Table 2 would need \"but did not enter a plea\" added at the end of the last sentence. The next sentence (d) in the same table needs a few more changes, but still very minor ones: adding the word \"capture\" after \"had evaded\", adding the preposition \"on\" before \"the run\", and adding \"when\" after \"last year\". Therefore, we grouped all those sentences into one class -PE (sentences which require a minimal postediting effort). Those sentences which scored '1' for meaning need to either be left in their original form or simplified from scratch. We thus classify them as Dis. This newly created dataset (Total3) allows us to investigate whether we could automatically classify simplified sentences into those three categories, taking into account both grammaticality and meaning preservation at the same time. The Total2 dataset contains only two marks ('0' and '1') which correspond to the sentences which should be discarded ('0') and those which should be retained ('1'), where '0' corresponds to Dis in Total3, and '1' corresponds to the union of OK and PE in Total3. The derivation procedure for both datasets is presented in Table 3 . We wanted to investigate whether the classification task would be simpler (better performed) if there were only two classes instead of three. In the case that such clas-sification could be performed with satisfactory accuracy, all sentences classified as '0' would be left in their original form or simplified with some different simplification strategy, while those classified as '1' would be sent for a quick human postediting procedure. Original Total3 Total2 G M 3 3 OK 1 2 3 PE 1 1 3 PE 1 3 2 PE 1 2 2 PE 1 1 2 PE 1 3 1 Dis 0 2 1 Dis 0 1 1 Dis 0 Table 3: Datasets Here it is important to mention that we decided not to use human scores for simplicity (S) for several reasons. First, simplicity was defined as the amount of irrelevant information which was eliminated. Therefore, we cannot expect that any of the six MT evaluation metrics would have a significant correlation with this score (except maybe TERp and, in particular, one of its parts -'number of deletions'. However, none of the two demonstrated any significant correlation with the simplicity score, and those results are thus not reported in this paper). Second, the output sentences with a low simplicity score are not as detrimental for the TS system as those with a low grammaticality or meaning preservation score. The sentences with a low simplicity score would simply not help the target user read faster or understand better, but would not do any harm either. Alternatively, if the target \"user\" is an MT or information extraction (IE) system, or a parser for example, such sentences would not lower the performance of the system; they would just not improve it. Low scores for G and M, however, would lead to a worse performance for such NLP systems, longer reading time, and a worse or erroneous understanding of the text. Third, the simplicity of the output (or complexity reduction performed by a TS system) could be evaluated separately, in a fully automatic manner -using some readability measures or average sentence length as features (as in (Drndarevi\u0107 et al., 2013; Glava\u0161 and \u0160tajner, 2013) for example). Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011) , TERp (Snover et al., 2009) , TINE (Rios et al., 2011) , and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the \"original\" BLEU would give score '0') and SRL (which is the component of TINE based on semantic role labeling using SENNA 4 ). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contribute more to the correlation of TINE with human judgements of grammaticality, and SRL to contribute more to the correlation of TINE with human judgements of meaning preservation. As we do not have the reference for the simplified sentence, all metrics are applied in a slightly different way than in MT. Instead of evaluating the translation hypothesis (output of the automatic TS system in our case) with the corresponding reference translation (which would be a 'gold standard' simplified sentence), we apply the metrics to the output of the automatic TS system comparing it with the corresponding original sentence. Given that the simplified sentences in the used dataset are usually shorter than the original ones (due to the elimination of irrelevant content which was the main focus of the TS system proposed by Glava\u0161 and \u0160tajner ( 2013 )), we expect low scores of T-BLEU and METEOR which apply a brevity penalty. However, our dataset does not contain any kind of lexical simplification, but rather copies all relevant information from the original sentence 5 . Therefore, we expect the exact matches of word forms and semantic role labels (which are components of the MT evaluation metrics) to have a good correlation to human judgements of grammaticality and meaning preservation. 5 The exceptions being changes of gerundive forms into past tense, and anaphoric pronoun resolution in some simplification schemes. See Section 3.1 and (Glava\u0161 and \u0160tajner, 2013) for more details. Goal After we obtained the six automatic metrics (cosine, METEOR, TERp, TINE, T-BLEU, and SRL), we performed two sets of experiments, trying to answer two main questions: 1. Are the chosen MT evaluation metrics correlated with the human judgements of grammaticality and meaning preservation of the TS system output? 2. Could we automatically classify the simplified sentences into those which are: (1) correct, (2) require a minimal post-editing, (3) incorrect and need to be discarded? A positive answer to the first question would mean that there is a possibility of finding an automatic metric (or a combination of several automatic metrics) which could successfully replace the time consuming human evaluation. The search for that \"ideal\" combination of automatic metrics could be performed by using various classification algorithms and carefully designed features. If we manage to classify simplified sentences into the three aforementioned categories with a satisfying accuracy, the benefits would be two-fold. Firstly, such a classification system could be used for an automatic evaluation of TS systems and an easy comparison of their performances. Secondly, it could be used inside a TS system to mark those sentences of low quality which need to be checked further, or those sentences whose original meaning changed significantly. The latter could then be left in their original form or simplified using some different technique. Experiments The six experiments conducted in this study are presented in Table 4 . The first two experiments had the aim of answering the first question (Section 3.5) as to whether the chosen MT metrics correlate with the human judgements of grammaticality (G) and meaning preservation (M) of the TS system output. The results were obtained in terms of Pearson's, Kendall's and Spearman's correlation coefficients. The third and the fourth experiments (Table 4 ) could be seen as the intermediate experiments exploring the possibility of automatic classification of simplified sentences according to their grammaticality, and meaning preservation. The main experiment was the fifth experiment, trying to answer the second question (Section 3.5) Exp. Description 1. Correlation of the six automatic MT metrics with the human scores for Grammaticality 2. Correlation of the six automatic MT metrics with the human scores for Meaning preservation 3. Classification of the simplified sentences into 3 classes ('1' -Bad, '2' -Medium, and '3' -Good) according to their Grammaticality 4. Classification of the simplified sentences into 3 classes ('1' -Bad, '2' -Medium, and '3' -Good) according to their Meaning preservation 5. Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score 6. Classification of the simplified sentences into 2 classes ('1' -Retain, '0' -Discard) according to their Total2 score Table 4 : Experiments as to whether we could automatically classify the simplified sentences into those which are: (1) correct (OK), ( 2 ) require minimal post-editing (PE), and (3) incorrect and need to be discarded (Dis). The last experiment (Table 4 ) was conducted with the aim of exploring whether the classification of simplified sentences into only two classes -Retain (for further post-editing) and Discard -would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009) , using seven classification algorithms in a 10-fold cross-validation setup: \u2022 NB -NaiveBayes (John and Langley, 1995) , \u2022 SMO -Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), \u2022 Logistic (le Cessie and van Houwelingen, 1992), \u2022 Lazy.IBk -K-nearest neighbours (Aha and Kibler, 1991) , \u2022 JRip -a propositional rule learner (Cohen, 1995) , \u2022 J48 -Weka implementation of C4.5 (Quinlan, 1993) . As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. Results and Discussion The results of the first two experiments (correlation experiments in Table 4 ) are presented in Section 4.1, while the results of the other four experiments (classification experiments in Table 4 ) can be found in Section 4.2. When interpreting the results of all experiments, it is important to keep in mind that human agreements for meaning preservation (M) and grammaticality (G) were acceptable but far from perfect (Section 3.2), and thus it would be unrealistic to expect the correlation between the MT evaluation metrics and human judgements or the agreement of the classification system with human assessments to be higher than the reported IAA agreement. Correlation of Automatic Metrics with Human Judgements The correlations of automatic metrics with human judgements of grammaticality and meaning preservation are given in Tables 5 and 6 matic measures -METEOR, T-BLEU, and TINE, while it is negatively correlated with TERp (TERp measures the number of edits necessary to perform on the simplified sentence to transform it into its original one, i.e. the higher the value of TERp, the less similar the original and its corresponding simplified sentence are. The other five MT metrics measure the similarity between the original and its corresponding simplified version, i.e. the higher their value is, the more similar are the sentences are). All the MT metrics appear to be even better correlated with the human scores for meaning preservation (Table 6 ), demonstrating six positive and one (TERp) negative statistically significant correlation with M. The correlation is the highest for T-BLEU, TINE, and TERp, though closely followed by all others. Sentence Classification The results of the four classification experiments (Section 3.6) are given in Table 7 . At first glance, the performance of the classification algorithms seems similar for the first two tasks (classification of the simplified sentences according to their Grammaticality and Meaning preservation). However, one needs to take into account that the baseline for the first task was much much higher than for the second task (Table 7 ). Furthermore, it can be noted that for the first task, recall was significantly higher than precision for most classification algorithms (all except NB and Logistic), while for the second task they were very similar in all cases. More importantly, a closer look at the confusion matrices reveals that most of the incorrectly classified sentences were assigned to the nearest class (Medium into Bad or Good; Bad into Medium; and Good into Medium 6 ) in the second task, while it was not the case in the first task (Table 8 ). Classification performed on the Total3 dataset outperformed both previous classifications -that based on Grammaticality and that based on Meaning -on four different algorithms (NB, Logistic, JRip, and J48). Classification conducted on Total3 using Logistic outperformed all results of classifications on either Grammaticality or Meaning separately (Table 7 ). It reached a 0.61, 0.60, and 0.59 score for the weighted precision (P), recall (R), and F-measure (F), respectively, thus outperforming the baseline significantly. More importantly, classification on the Total3 dataset led to significantly fewer mis-classifications between Good and Bad ( the classification based only on Meaning (Table 8 ). Therefore, it seems that simplified sentences are better classified into three classes giving a unique score for both grammaticality and preservation of meaning together. The binary classification experiments based on the Total2 led to results which significantly outperformed the baseline in terms of precision and F-measure (Table 7 ). However, they resulted in a great number of sentences which should be retained (Retain) being classified into those which should be discarded (Discard) and vice versa (Table 10 ). Therefore, it seems that it would be better to opt for classification into three classes (Total3) than for classification into two classes (Total2). Additionally, we used CfsSubsetEval attribute selection algorithm (Hall and Smith, 1998) in order to identify the 'best' subset of features. The 'best' subsets of features for each of the four classification tasks returned by the algorithm are listed in Table 11 . However, the classification performances achieved (P, R, and F) when using only the 'best' features did not differ significantly from those when using all initially selected features, and thus are not presented in this paper. Limitations The used dataset does not contain any kind of lexical simplification (Glava\u0161 and \u0160tajner, 2013) .  Therefore, one should consider the limitation of this TS system which performs only syntactic simplification and content reduction. On the other hand, the dataset used contains a significant content reduction in most of the sentences. If the same experiments were conducted on a dataset which performs only syntactic simplification, we would expect much higher correlation of MT evaluation metrics to human judgements, due to the lesser impact of the brevity penalty in that case. If we were to apply the same MT evaluation metrics to a TS system which additionally performs some kind of lexical simplification (either a simple lexical substitution or paraphrasing), the correlation results for T-BLEU and cosine similarity would be lower (due to the lower number of exact matches), but not for METEOR, TERp and SRL (and thus TINE as well). As a similar problem is also present in the evaluation of MT systems where the obtained output could differ from the reference translation (while still being equally good), METEOR, TERp, and SRL in TINE additionally use inexact matching. The first two use the stem, synonym, and paraphrase matches, while SRL uses ontologies and thesaurus. Conclusions and Future Work While the results reported are preliminary and their universality needs to be validated on different TS datasets, the experiments and results presented can be regarded as a promising step towards an automatic assessment of grammaticality and meaning preservation for the output of TS systems. In addition and to the best of our knowledge, there are no such datasets publicly available other than the one used. Nevertheless, we hope that these results would initiate an interesting discussion in the TS community and start a new direction of studies towards automatic evaluation of text simplification systems. Acknowledgements The research described in this paper was partially funded by the European Commission under the Seventh (FP7-2007-2013) Framework Programme for Research and Technological Development (FP7- ICT-2011.5.5 FIRST 287607).",
    "abstract": "This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures. The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising.",
    "countries": [
        "United Kingdom",
        "Spain"
    ],
    "languages": [
        "Spanish",
        "English"
    ],
    "numcitedby": "30",
    "year": "2014",
    "month": "April",
    "title": "One Step Closer to Automatic Evaluation of Text Simplification Systems"
}