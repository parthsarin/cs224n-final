{
    "article": "We developed a web application for crowdsourcing transcriptions of Dutch words spoken by Spanish L2 learners. In this paper we discuss the design of the application and the influence of metadata and various forms of feedback. Useful data were obtained from 159 participants, with an average of over 20 transcriptions per item, which seems a satisfactory result for this type of research. Informing participants about how many items they still had to complete, and not how many they had already completed, turned to be an incentive to do more items. Assigning participants a score for their performance made it more attractive for them to carry out the transcription task, but this seemed to influence their performance. We discuss possible advantages and disadvantages in connection with the aim of the research and consider possible lessons for designing future experiments. Introduction In their research, (Burgos et al., 2013; Burgos et al., 2014; Burgos et al., 2015) studied the pronunciation of Dutch by Spanish L2 learners. Judgements of pronunciation quality were obtained from experts (Burgos et al., 2013; Burgos et al., 2014) . However, judgements by nonexpert Dutch native listeners are also relevant and informative, as they can reveal which features of the learners' vowel realizations may lead to confusions in perception. To get large numbers of transcriptions, it was decided to use crowdsourcing for data collection (Burgos et al., 2015) . The use of crowdsourcing to obtain annotations or scorings of intelligibility or accentedness of non-native speech is not new (Evanini et al., 2010; Cooke et al., 2013; Wang et al., 2013) . For our purpose, we built a web application that allows participants to listen to utterances and transcribe what they hear. In their crowdsourcing experiment (Cooke et al., 2013) had observed that limited feedback could lead to low task engagement. For this reason, we decided to build in a few feedback parameters, to see whether the presence or type of feedback would impact the number and nature of the transcriptions. We also asked for metadata such as gender, age and completed education to be able to study how these variables affect crowdsourcing behaviour/participation. The results of the analyses of the data that were collected with the application are reported in (Burgos et al., 2015) . The current paper describes the application, the feedback parameters, metadata and their influence on the results. Application Before designing the application, we defined a number of criteria aimed at maximizing response: \u2022 The application had to be easy to use. \u2022 The task had to be \"fun\" to do. \u2022 It had to be shared on Facebook to attract new participants. \u2022 Transcribers had to participate voluntarily. \u2022 Participants should be able to return and continue from where they left. This led to the web application that we called Palabras, the Spanish word for 'words'. Our conditions were met in the following way: \u2022 The task is very easy: the participant listens to a word that is played (with option to repeat), enters what (s)he hears and the next sound is played. The login procedure is also very easy. \u2022 We added a score, so participants could compare how well they did and share the score by posting it on Facebook. \u2022 After completing 50 items, participants could share their score on Facebook. By clicking on the picture (figure 1 ) their contacts were directed to the application. \u2022 No (monetary) compensation was given to the participants. \u2022 By using a login procedure, the application remembers which items a participant had transcribed and can continue from there. Figure 1 : Image on Facebook to share score and link. The application consists of two screens: \u2022 A screen with basic explanation of the task and the two login options (see figure 2 ). Participants can either use their Facebook account to login or can register and login at the same time by choosing a username/password combination. \u2022 A screen with the main application including feedback, the main parts of the explanation and a metadata fillin form (see figure 3 ). As soon as the metadata are filled in, the form disappears. The metadata that are asked from the participants are age group (10-20, 21-30, 31-40, 41-50, 51-60, >60) , gender and level of completed education (4 levels of Dutch education: lbo, havo/vwo, hbo, wo) and their mother tongue, if different from Dutch. The application gives three types of feedback: \u2022 A score with percentage correct. Each transcription is compared to the majority transcription of all transcribers (the transcription that is transcribed most often). If it is the same the transcription is counted as \"correct\". Mind that there is not a correct or incorrect transcription; one hears what one hears. But this was the closest we could get to a simple score. This type of feedback was given to all participants. \u2022 Transcription of previous utterance by the participant and by the majority. This gives the participant the possibility to compare his/her transcription with the majority and check if it is \"correct\". This type of feedback was given to half of the participants (either always or never). \u2022 Information about the number of items. This contains the total number of items and the number of items that are already transcribed or still have to be done. Half of the participants did not get this type of feedback. Of the other half, half (a quarter of the total) got the number of items already transcribed and half got the number of items that still had to be done. The speech material to be transcribed consisted of 29 monosyllabic Dutch words pronounced by 28 Spanish learners of Dutch. Six items were unusable and were removed. The 29 words contain the 15 vowels of Dutch followed by /s/ or /t/, as these consonants are known to alter the preceding vowel least (van der Harst, 2011; van der Harst et al., 2014) . Only the sequence /y/ + /s/ is missing, since there are no Dutch monosyllabic nouns with this combination (except proper names). See table 1 for all words presented as stimuli. The utterances were randomly chosen and presented to the participants in such a way that they did not get the same utterance (same word spoken by same speaker) twice. However every 30th utterance was a randomly chosen utterance that had already been transcribed and which was presented a second time to be able to calculate intratranscriber agreement. We also took care that the utterance that had to be transcribed was of a different word type than that of the last 20 transcribed utterances to prevent a carryover or learning effect. Results Almost 200 people participated and produced an average of 100 transcriptions. See figure 4 for a distribution of the number of items transcribed. About 70% of the participants transcribed more than 50 items and 3 participants transcribed more than all 806 words. Over 90% did only 1 session (a new session is started when there is more than 1 hour between two items). Three participants did five sessions. About 90% of the participants filled in their metadata. Quality control We checked the quality of the data in several ways. We applied filters to remove the following transcribers and transcriptions from our data set: \u2022 Testers of the application and the authors of the paper. \u2022 Transcribers that had indicated to have another native language than Dutch. \u2022 Transcribers with less than 10 transcriptions, these are not regarded as serious participants. \u2022 Transcribers that did not fulfill our quality criteria (inter and intratranscriber agreement below threshold). \u2022 Transcriptions that were entered more than once (when the server was slow in response). \u2022 Transcriptions that were produced after the whole set of stimuli had been completed. Transcriptions The main goal of the data collection was to find out how the pronunciation of the Dutch words by Spanish learners was perceived by nonexpert listeners. In 62% of all 17534 transcriptions the canonical transcription of the target word was used. In 19% of the cases the most often used alternative was selected and in another 18% another variant was chosen. See (Burgos et al., 2015) for concrete results on the transcription variants. The main conclusion is that the nonexperts found in general the same effects as experts. The score of a transcriber is defined as the percentage of \"correct\" transcriptions. A transcription is \"correct\" if it is the same as the most frequently used transcription. In 81% of the cases the canonical transcription was the correct transcription. Figure 5 shows the percentages correct transcriptions that the participants scored. The mean score is 67% and the medium score is 69%. Metadata In this subsection the relation between the metadata and the number of transcribers, transcriptions and scores are presented. This is only done for those transcribers that filled in the metadata, thus the numbers do not add up to the total numbers of participants and transcriptions. What stands out from this table is the high percentage of female participants (table 2). Almost three times as many women participated and they transcribed 1.5 times more items on average than men, which is a significant difference (t' (130.328) = 2.203, p =.029). The percentage of \"correctly\" transcribed words was not significantly different. The lowest education level has only few participants, who transcribed few words and scored low on percentage correct. The other three groups behave similarly to each other. The group of participants with a university degree is relatively large. This is not surprising since recruitment started from people in this category. Feedback In this subsection the different feedback parameters in relation with the number of participants, transcriptions and scores are presented. Table 5 shows the results for the two groups of participants, one of which got feedback information over the previous word and the other which did not. Getting feedback on the previous word did not result in transcribing more items, but it did lead to significant higher scores (t' (156.848) = 2.58, p = .012). This is to be expected: participants who get this feedback, learn what they have to do to get a good score and can adapt their strategy in this direction. In practice,  Table 6 : Results for different feedback on number of words. Table 6 shows the results with respect to the feedback about the number of words in the transcription set. Half of the participants got the number that had to be transcribed in total. Half of them got the number that still had to be done (todo) and the other half got the number that had already been done (done). The other half of the participants got none of this information (none). The participants that received to do information transcribed on average far more items than the other two groups. Because of the high standard deviation in the number of transcribed items this is not significant (F(2,156) = 2.289, p =.105), but the tendency is clear. This feedback might be an incentive to continue. The group that received done information scored lower than the other two groups, but the differences between the groups were not significant (F(2,156) = 2.614, p =.077). Discussion and Conclusions When we started the crowdsourcing experiment we recruited participants in our direct social network, but eventually many of the participants are unknown to the authors, which means that the Facebookshare method worked. With 159 useful participants and over 20 transcriptions per item on average, this crowdsourcing method is definitely a satisfactory result for L2 speech research. An unexpected result of the crowdsourcing was the large proportion of women that participated. We do not have a direct explanation for this. The recruitment started in environments with equal numbers of both genders and we do not know whether the transcription task was more appealing to women than to men. We have no indication that this difference might have influenced our results, since men and women scored almost equally. Giving the user information about how many items were to be transcribed in total seemed to be an incentive to do more items, but strangely only in the case when presented with the number of items still to be done and not with the number of items that had been done. It turned out that in an application in which participants do not get any monetary remuneration (Cooke et al., 2013) , adding a score to the application made it more attractive to do the transcription task. We got feedback from participants indicating that the score indeed did stimulate them to go further with the task, for example to beat their friends score. It has the disadvantage though that the participants main goal might not be to transcribe precisely what they hear, but what they think will give them a higher score. Users appeared to be confused when they transcribed what they heard, but the correct transcription appeared to be something else. Sometimes they adapted their strategy by transcribing what they thought was meant to be said to get a higher score. This gives a bias towards the canonical transcription. However, clear pronunciation errors still got the noncanonical transcription in the majority vote, which indicates that serious pronunciation errors were penalized anyway, while less serious errors were not noted down because they are probably considered not to hamper communication. Looking back at the goals of the present study, 1) to evaluate the transcription system designed and its parameters, and 2) to determine how feedback and reward affect transcribing behavior in the context of crowdsourcing, we can conclude that 1) the overall system worked satisfactorily and produced a considerable amount of interesting data, and that 2) feedback and reward had a positive effect because they motivated the participants to continue as in (Kaufmann et al., 2011) , but they did not always have a desirable effect on transcription behavior, which can be considered an important lesson for designing future experiments. The software that was developed for Palabras is reused for another project in which tweets are annotated. The software is open source and can be obtained by contacting the first author of this paper.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 4.320199066265573e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the research or development of the web application.",
    "abstract": "We developed a web application for crowdsourcing transcriptions of Dutch words spoken by Spanish L2 learners. In this paper we discuss the design of the application and the influence of metadata and various forms of feedback. Useful data were obtained from 159 participants, with an average of over 20 transcriptions per item, which seems a satisfactory result for this type of research. Informing participants about how many items they still had to complete, and not how many they had already completed, turned to be an incentive to do more items. Assigning participants a score for their performance made it more attractive for them to carry out the transcription task, but this seemed to influence their performance. We discuss possible advantages and disadvantages in connection with the aim of the research and consider possible lessons for designing future experiments.",
    "countries": [
        "Netherlands"
    ],
    "languages": [
        "Dutch",
        "Spanish"
    ],
    "numcitedby": 4,
    "year": 2016,
    "month": "May",
    "title": "{P}alabras: Crowdsourcing Transcriptions of {L}2 Speech",
    "values": {
        "ease of implementation": "The application had to be easy to use. The task had to be \"fun\" to do. This led to the web application that we called Palabras, the Spanish word for 'words'. Our conditions were met in the following way: The task is very easy: the participant listens to a word that is played (with option to repeat), enters what (s)he hears and the next sound is played. The login procedure is also very easy.",
        "performance": "The use of crowdsourcing to obtain annotations or scorings of intelligibility or accentedness of non-native speech is not new (Evanini et al., 2010; Cooke et al., 2013; Wang et al., 2013) . In their crowdsourcing experiment (Cooke et al., 2013) had observed that limited feedback could lead to low task engagement. Sometimes they adapted their strategy by transcribing what they thought was meant to be said to get a higher score. This gives a bias towards the canonical transcription."
    }
}