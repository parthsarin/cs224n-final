{
    "article": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags. The model learns language-specific features while capturing cross-lingual patterns in tag distribution for aligned words. Once the parameters of our model have been learned on bilingual parallel data, we evaluate its performance on a held-out monolingual test set. Our evaluation on six pairs of languages shows consistent and significant performance gains over a state-of-the-art monolingual baseline. For one language pair, we observe a relative reduction in error of 53%. Introduction In this paper, we explore the application of multilingual learning to part-of-speech tagging when no annotation is available. This core task has been studied in an unsupervised monolingual framework for over a decade and is still an active area of research. In this paper, we demonstrate the effectiveness of multilingual learning when applied to both closely related and distantly related language pairs. We further analyze the language features which lead to robust bilingual performance. The fundamental idea upon which our work is based is that the patterns of ambiguity inherent in part-of-speech tag assignments differ across languages. At the lexical level, a word with part-ofspeech tag ambiguity in one language may correspond to an unambiguous word in the other language. For example, the word \"can\" in English may function as an auxiliary verb, a noun, or a regular verb. However, each of the corresponding functions in Serbian is expressed with a distinct lexical item. Languages also differ in their patterns of structural ambiguity. For example, the presence of an article in English greatly reduces the ambiguity of the succeeding tag. In Serbian, a language without articles, this constraint is obviously absent. The key idea of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. While multilingual learning can address ambiguities in each language, it must be flexible enough to accommodate cross-lingual variations such as tag inventory and syntactic structure. As a result of such variations, two languages often select and order their tags differently even when expressing the same meaning. A key challenge of multilingual learning is to model language-specific structure while allowing information to flow between languages. We jointly model bilingual part-of-speech tag sequences in a hierarchical Bayesian framework. For each word, we posit a hidden tag state which generates the word as well as the succeeding tag. In addition, the tags of words with common semantic or syntactic function in parallel sentences are combined into bilingual nodes representing the tag pair. These joined nodes serve as anchors that create probabilistic dependencies between the tag se-quences in each language. We use standard tools from machine translation to discover aligned wordpairs, and thereafter our model treats the alignments as observed data. Our model structure allows language-specific tag inventories. Additionally, it assumes only that the tags at joined nodes are correlated; they need not be identical. We factor the conditional probabilities of joined nodes into two individual transition probabilities as well as a coupling probability. We define priors over the transition, emission, and coupling parameters and perform Bayesian inference using Gibbs sampling and the Metropolis-Hastings algorithm. We evaluate our model on a parallel corpus of four languages: English, Bulgarian, Serbian, and Slovene. For each of the six language pairs, we train a bilingual model on this corpus, and evaluate it on held-out monolingual test sets. Our results show consistent improvement over a monolingual baseline for all languages and all pairings. In fact, for one language pair -Serbian and Slovene -the error is reduced by over 53%. Moreover, the multilingual model significantly reduces the gap between unsupervised and supervised performance. For instance, in the case of Slovene this gap is reduced by 71%. We also observe significant variation in the level of improvement across language pairs. We show that a cross-lingual entropy measure corresponds with the observed differentials in performance. Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005) . Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003) . When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad\u00f3 and Lapata, 2006; Xi and Hwa, 2005) . In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006) . In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008) . In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994) , the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004) . Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007) . In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006) . Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speech level will differ in systematic ways; by considering multiple language simultaneously, the total inherent ambiguity can be reduced in each language. The model is designed to permit information to flow across the language barrier, while respecting language-specific idiosyncrasies such as tag inventory, selection, and order. We assume that for pairs of words that share similar semantic or syntactic function, the associated tags will be statistically correlated, though not necessarily identical. We use such word pairs as the bilingual anchors of our model, allowing crosslingual information to be shared via joint tagging decisions. We use standard tools from machine translation to identify these aligned words, and thereafter our model treats them as fixed and observed data. To avoid cycles, we remove crossing edges from the alignments. For unaligned parts of the sentence, the tag and word selections are identical to standard monolingual HMM's. Figure 1 shows an example of the bilingual graphical structure we use, in comparison to two independent monolingual HMM's. We formulate a hierarchical Bayesian model that exploits both language-specific and cross-lingual patterns to explain the observed bilingual sentences. We present a generative story in which the observed words are produced by the hidden tags and model parameters. In Section 4, we describe how to infer the posterior distribution over these hidden variables, given the observations. Generative Model Our generative model assumes the existence of two tagsets, T and T \u2032 , and two vocabularies W and W \u2032 , one of each for each language. For ease of exposition, we formulate our model with bigram tag de-pendencies. However, in our experiments we used a trigram model, which is a trivial extension of the model discussed here and in the next section. 1. For each tag t \u2208 T , draw a transition distribution \u03c6 t over tags T , and an emission distribution \u03b8 t over words W , both from symmetric Dirichlet priors. 1 2. For each tag t \u2208 T \u2032 , draw a transition distribution \u03c6 \u2032 t over tags T \u2032 , and an emission distribution \u03b8 \u2032 t over words W \u2032 , both from symmetric Dirichlet priors. 3. Draw a bilingual coupling distribution \u03c9 over tag pairs T \u00d7 T \u2032 from a symmetric Dirichlet prior. 4. For each bilingual parallel sentence: (a) Draw an alignment a from an alignment distribution A (see the following paragraph for formal definitions of a and A), (b) Draw a bilingual sequence of part-ofspeech tags (x 1 , ..., x m ), (y 1 , ..., y n ) according to: P (x 1 , ..., x m , y 1 , ..., y n |a, \u03c6, \u03c6 \u2032 , \u03c9). 2 This joint distribution is given in equation 1. (c) For each part-of-speech tag x i in the first language, emit a word from W : e i \u223c \u03b8 x i , (d) For each part-of-speech tag y j in the second language, emit a word from W \u2032 : f j \u223c \u03b8 \u2032 y j . We define an alignment a to be a set of one-toone integer pairs with no crossing edges. Intuitively, each pair (i, j) \u2208 a indicates that the words e i and f j share some common role in the bilingual parallel sentences. In our experiments, we assume that alignments are directly observed and we hold them fixed. From the perspective of our generative model, we treat alignments as drawn from a distribution A, about which we remain largely agnostic. We only require that A assign zero probability to alignments which either: (i) align a single index in one language to multiple indices in the other language or (ii) contain crossing edges. The resulting alignments are thus one-to-one, contain no crossing edges, and may be sparse or even possibly empty. Our technique for obtaining alignments that display these properties is described in Section 5. Given an alignment a and sets of transition parameters \u03c6 and \u03c6 \u2032 , we factor the conditional probability of a bilingual tag sequence (x 1 , ...x m ), (y 1 , ..., y n ) into transition probabilities for unaligned tags, and joint probabilities over aligned tag pairs: P (x 1 , ..., x m , y 1 , ..., y n |a, \u03c6, \u03c6 \u2032 , \u03c9) = unaligned i \u03c6 x i\u22121 (x i ) \u2022 unaligned j \u03c6 \u2032 y j\u22121 (y j ) \u2022 (i,j)\u2208a P (x i , y j |x i\u22121 , y j\u22121 , \u03c6, \u03c6 \u2032 , \u03c9) (1) Because the alignment contains no crossing edges, we can model the tags as generated sequentially by a stochastic process. We define the distribution over aligned tag pairs to be a product of each language's transition probability and the coupling probability: P (x i , y j |x i\u22121 , y j\u22121 , \u03c6, \u03c6 \u2032 , \u03c9) = \u03c6 x i\u22121 (x i ) \u03c6 \u2032 y j\u22121 (y j ) \u03c9(x i , y j ) Z (2) The normalization constant here is defined as: Z = x,y \u03c6 x i\u22121 (x) \u03c6 \u2032 y j\u22121 (y) \u03c9(x, y) This factorization allows the language-specific transition probabilities to be shared across aligned and unaligned tags. In the latter case, the addition of the coupling parameter \u03c9 gives the tag pair an additional role: that of multilingual anchor. In essence, the probability of the aligned tag pair is a product of three experts: the two transition parameters and the coupling parameter. Thus, the combination of a high probability transition in one language and a high probability coupling can resolve cases of inherent transition uncertainty in the other language. In addition, any one of the three parameters can \"veto\" a tag pair to which it assigns low probability. To perform inference in this model, we predict the bilingual tag sequences with maximal probability given the observed words and alignments, while integrating over the transition, emission, and coupling parameters. To do so, we use a combination of sampling-based techniques. Inference The core element of our inference procedure is Gibbs sampling (Geman and Geman, 1984) . Gibbs sampling begins by randomly initializing all unobserved random variables; at each iteration, each random variable z i is sampled from the conditional distribution P (z i |z \u2212i ), where z \u2212i refers to all variables other than z i . Eventually, the distribution over samples drawn from this process will converge to the unconditional joint distribution P (z) of the unobserved variables. When possible, we avoid explicitly sampling variables which are not of direct interest, but rather integrate over them-this technique is known as \"collapsed sampling,\" and can reduce variance (Liu, 1994) . We sample: (i) the bilingual tag sequences (x, y), (ii) the two sets of transition parameters \u03c6 and \u03c6 \u2032 , and (iii) the coupling parameter \u03c9. We integrate over the emission parameters \u03b8 and \u03b8 \u2032 , whose priors are Dirichlet distributions with hyperparameters \u03b8 0 and \u03b8 \u2032 0 . The resulting emission distribution over words e i , given the other words e \u2212i , the tag sequences x and the emission prior \u03b8 0 , can easily be derived as: P (e i |x, e \u2212i , \u03b8 0 ) = \u03b8x i \u03b8 x i (e i ) P (\u03b8 x i |\u03b8 0 ) d\u03b8 x i = n(x i , e i ) + \u03b8 0 n(x i ) + W x i \u03b8 0 (3) Here, n(x i ) is the number of occurrences of the tag x i in x \u2212i , n(x i , e i ) is the number of occurrences of the tag-word pair (x i , e i ) in (x \u2212i , e \u2212i ), and W x i is the number of word types in the vocabulary W that can take tag x i . The integral is tractable due to Dirichlet-multinomial conjugacy (Gelman et al., 2004) . We will now discuss, in turn, each of the variables that we sample. Note that in all cases we condition on the other sampled variables as well as the observed words and alignments, e, f and a, which are kept fixed throughout. Sampling Part-of-speech Tags This section presents the conditional distributions that we sample from to obtain the part-of-speech tags. Depending on the alignment, there are several scenarios. In the simplest case, both the tag to be sampled and its succeeding tag are not aligned to any tag in the other language. If so, the sampling distribution is identical to the monolingual case, including only terms for the emission (defined in equation 3), and the preceding and succeeding transitions: P (x i |x \u2212i , y, e, f, a, \u03c6, \u03c6 \u2032 , \u03c9, \u03b8 0 , \u03b8 \u2032 0 ) \u221d P (e i |x, e \u2212i , \u03b8 0 ) \u03c6 x i\u22121 (x i ) \u03c6 x i (x i+1 ). For an aligned tag pair (x i , y j ), we sample the identity of the tags jointly. By applying the chain rule we obtain terms for the emissions in both languages and a joint term for the transition probabilities: P (x i , y j |x \u2212i , y \u2212j , e, f, a, \u03c6, \u03c6 \u2032 , \u03c9, \u03b8 0 , \u03b8 \u2032 0 ) \u221d P (e i |x, e \u2212i , \u03b8 0 )P (f j |y, f \u2212j , \u03b8 \u2032 0 ) P (x i , y j |x \u2212i , y \u2212j , a, \u03c6, \u03c6 \u2032 , \u03c9) The expansion of the joint term depends on the alignment of the succeeding tags. In the case that the successors are not aligned, we have a product of the bilingual coupling probability and four transition probabilities (preceding and succeeding transitions in each language): P (x i , y j |x \u2212i , y \u2212j , a, \u03c6, \u03c6 \u2032 , \u03c9) \u221d \u03c9(x i , y j )\u03c6 xi\u22121 (x i ) \u03c6 \u2032 yj\u22121 (y j ) \u03c6 xi (x i+1 ) \u03c6 \u2032 yj (y j+1 ) Whenever one or more of the succeeding tags is aligned, the sampling formulas must account for the effect of the sampled tag on the joint probability of the succeeding tags, which is no longer a simple multinomial transition probability. We give the formula for one such case-when we are sampling an aligned tag pair (x i , y j ), whose succeeding tags (x i+1 , y j+1 ) are also aligned to one another: P (x i , y j |x \u2212i , y \u2212j , a, \u03c6, \u03c6 \u2032 , \u03c9) \u221d \u03c9(x i , y j ) \u2022 \u03c6 xi\u22121 (x i ) \u03c6 \u2032 yj\u22121 (y j ) \u03c6 xi (x i+1 ) \u03c6 \u2032 yj (y j+1 ) x,y \u03c6 xi (x) \u03c6 \u2032 yj (y) \u03c9(x, y) Similar equations can be derived for cases where the succeeding tags are not aligned to each other, but to other tags. Sampling Transition Parameters and the Coupling Parameter When computing the joint probability of an aligned tag pair (Equation 2 ), we employ the transition parameters \u03c6, \u03c6 \u2032 and the coupling parameter \u03c9 in a normalized product. Because of this, we can no longer regard these parameters as simple multinomials, and thus can no longer sample them using the standard closed formulas. Instead, to resample these parameters, we resort to the Metropolis-Hastings algorithm as a subroutine within Gibbs sampling (Hastings, 1970) . Metropolis-Hastings is a Markov chain sampling technique that can be used when it is impossible to directly sample from the posterior. Instead, samples are drawn from a proposal distribution and then stochastically accepted or rejected on the basis of: their likelihood, their probability under the proposal distribution, and the likelihood and proposal probability of the previous sample. We use a form of Metropolis-Hastings known as an independent sampler. In this setup, the proposal distribution does not depend on the value of the previous sample, although the accept/reject decision does depend on the previous model likelihood. More formally, if we denote the proposal distribution as Q(z), the target distribution as P (z), and the previous sample as z, then the probability of accepting a new sample z * \u223c Q is set at: min 1, P (z * ) Q(z) P (z) Q(z * ) Theoretically any non-degenerate proposal distribution may be used. However, a higher acceptance rate and faster convergence is achieved when the proposal Q is a close approximation of P . For a particular transition parameter \u03c6 x , we define our proposal distribution Q to be Dirichlet with parameters set to the bigram counts of the tags following x in the sampled tag data. Thus, the proposal distribution for \u03c6 x has a mean proportional to these counts, and is thus likely to be a good approximation to the target distribution. Likewise for the coupling parameter \u03c9, we define a Dirichlet proposal distribution. This Dirichlet is parameterized by the counts of aligned tag pairs (x, y) in the current set of tag samples. Since this sets the mean of the proposal to be proportional to these counts, this too is likely to be a good approximation to the target distribution. Hyperparameter Re-estimation After every iteration of Gibbs sampling the hyperparameters \u03b8 0 and \u03b8 \u2032 0 are re-estimated using a single Metropolis-Hastings move. The proposal distribution is set to a Gaussian with mean at the current value and variance equal to one tenth of the mean. Experimental Set-Up Our evaluation framework follows the standard procedures established for unsupervised part-of-speech tagging. Given a tag dictionary (i.e., a set of possible tags for each word type), the model has to select the appropriate tag for each token occurring in a text. We also evaluate tagger performance when only incomplete dictionaries are available (Smith and Eisner, 2005; Goldwater and Griffiths, 2007) . In both scenarios, the model is trained only using untagged text. In this section, we first describe the parallel data and part-of-speech annotations used for system evaluation. Next we describe a monolingual baseline and our procedures for initialization and hyperparameter setting. Data As a source of parallel data, we use Orwell's novel \"Nineteen Eighty Four\" in the original English as well as translations to three Slavic languages -Bulgarian, Serbian and Slovene. This data is distributed as part of the Multext-East corpus which is publicly available. The corpus provides detailed morphological annotation at the world level, including part-of-speech tags. In addition a lexicon for each language is provided. We obtain six parallel corpora by considering all pairings of the four languages. We compute word level alignments for each language pair using Giza++. To generate one-to-one alignments at the word level, we intersect the one-to-many alignments going in each direction and automatically remove crossing edges in the order in which they appear left to right. This process results in alignment of about half the tokens in each bilingual parallel corpus. We treat the alignments as fixed and observed variables throughout the training procedure. The corpus consists of 94,725 English words (see Table 2 ). For every language, a random three quarters of the data are used for learning the model while the remaining quarter is used for testing. In the test set, only monolingual information is made available to the model, in order to simulate future performance on non-parallel data. (Goldwater and Griffiths, 2007) ) as well as a supervised HMM. In addition, the trigram part-of-speech tag entropy is given for each language. in our experiments. 3  In the Multext-East corpus, punctuation marks are not annotated. We expand the tag repository by defining a separate tag for all punctuation marks. This allows the model to make use of any transition or coupling patterns involving punctuation marks. We do not consider punctuation tokens when computing model accuracy. Table 2 shows the tag/token ratio for these languages. For Slavic languages, we use the tag dictionaries provided with the corpus. For English, we use a different process for dictionary construction. Using the original dictionary would result in the tag/token ratio of 1.5, in comparison to the ratio of 2.3 observed in the Wall Street Journal (WSJ) corpus. To make our results on English tagging more comparable to previous benchmarks, we expand the original dictionary of English tags by merging it with the tags from the WSJ dictionary. This process results in a tag/token ratio of 2.58, yielding a slightly more ambiguous dictionary than the one used in previous tagging work. 4  Monolingual Baseline As our monolingual baseline we use the unsupervised Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) . This model modifies the standard HMM by adding priors and by performing Bayesian inference. Its is in line with state-of-the-art unsupervised models. This model is a particulary informative baseline, since our model reduces to this baseline model when there are no alignments in the data. This implies that any performance gain over the baseline can only be at-tributed to the multilingual aspect of our model. We used our own implementation after verifying that its performance on WSJ was identical to that reported in (Goldwater and Griffiths, 2007) . Supervised Performance In order to provide a point of comparison, we also provide supervised results when an annotated corpus is provided. We use the standard supervised HMM with Viterbi decoding. Training and Testing Framework Initially, all words are assigned tags randomly from their tag dictionaries. During each iteration of the sampler, aligned tag pairs and unaligned tags are sampled from their respective distributions given in Section 4.1 above. The hyperparameters \u03b8 0 and \u03b8 \u2032 0 are initialized with the values learned during monolingual training. They are re-estimated after every iteration of the sampler using the Metropolis Hastings algorithm. The parameters \u03c6 and \u03c6 \u2032 are initially set to trigram counts and the \u03c9 parameter is set to tag pair counts of aligned pairs. After every 40 iterations of the sampler, a Metropolis Hastings subroutine is invoked that re-estimates these parameters based on the current counts. Overall, the algorithm is run for 1000 iterations of tag sampling, by which time the resulting log-likelihood converges to stable values. Each Metropolis Hastings subroutine samples 20 values, with an acceptance ratio of around 1/6, in line with the standard recommended values. After training, trigram and word emission probabilities are computed based on the counts of tags assigned in the final iteration. For smoothing, the final sampled values of the hyperparameters are used. The highest probability tag sequences for each monolingual test set are then predicted using trigram Viterbi decoding. We report results averaged over five complete runs of all experiments. Results Complete Tag Dictionary In our first experiment, we assume that a complete dictionary listing the possible tags for every word is provided in each language. Table 1 shows the monolingual results of a random baseline, an unsupervised Bayesian HMM and a supervised HMM. Table 3 show the results of our bilingual models for different language pairings while repeating the monolingual unsupervised results from Table 1 for easy comparison. The final column indicates the absolute gain in performance over this monolingual baseline. Across all language pairs, the bilingual model consistently outperforms the monolingual baseline. All the improvements are statistically significant by a Fisher sign test at p < 0.05. For some language pairs, the gains are quite high. For instance, the pairing of Serbian and Slovene (two closely related languages) yields absolute improvements of 6.7 and 7.7 percentage points, corresponding to relative reductions in error of 51.4% and 53.2%. Pairing Bulgarian and English (two distantly related languages) also yields large gains: 5.6 and 1.3 percentage points, corresponding to relative reductions in error of 50% and 14%, respectively. 5  When we compare the best bilingual result for each language (Table 3 , in bold) to the monolingual supervised results (Table 1 ), we find that for all languages the gap between supervised and unsupervised learning is reduced significantly. For English, this gap is reduced by 21%. For the Slavic languages, the supervised-unsupervised gap is reduced by even larger amounts: 57%, 69%, and 78% for Serbian, Bulgarian, and Slovene respectively. While all the languages benefit from the bilingual learning framework, some language combinations are more effective than others. Slovene, for instance, achieves a large improvement when paired with Serbian (+7.7), a closely related Slavic language, but only a minor improvement when coupled 5 The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by (Goldwater and Griffiths, 2007) on the WSJ corpus. We attribute this discrepancy to the slight differences in tag inventory used in our dataset. For example, when Particles and Prepositions are merged in the WSJ corpus (as they happen to be in our tag inventory and corpus), the performance of Goldwater's model on WSJ is similar to what we report here.  with English (+1.3). On the other hand, for Bulgarian, the best performance is achieved when coupling with English (+5.6) rather than with closely related Slavic languages (+3.1 and +2.4). As these results show, an optimal pairing cannot be predicted based solely on the family connection of paired languages. To gain a better understanding of this variation in performance, we measured the internal tag entropy of each language as well as the cross-lingual tag entropy of language pairs. For the first measure, we computed the conditional entropy of a tag decision given the previous two tags. Intuitively, this should correspond to the inherent structural uncertainty of part-of-speech decisions in a language. In fact, as Table 1 shows, the trigram entropy is a good indicator of the relative performance of the monolingual baseline. To measure the cross-lingual tag entropies of language pairs, we considered all bilingual aligned tag pairs, and computed the conditional entropy of the tags in one language given the tags in the other language. This measure should indicate the amount of information that one language in a pair can provide the other. ysis are given in the first column of Table 3 . We observe that the cross-lingual entropy is lowest for the Serbian and Slovene pair, corresponding with their large gain in performance. Bulgarian, on the other hand, has lowest cross-lingual entropy when paired with English. This corresponds with the fact that English provides Bulgarian with its largest performance gain. In general, we find that the largest performance gain for any language is achieved when minimizing its cross-lingual entropy. Reduced Tag Dictionary We also conducted experiments to investigate the impact of the dictionary size on the performance of the bilingual model. Here, we provide results for the realistic scenario where only a very small dictionary is present. Table 4 shows the performance when a tag dictionary for the 100 most frequent words is present in each language. The bilingual model's results are consistently and significantly better than the monolingual baseline for all language pairs. Conclusion We have demonstrated the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learn-ing is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulated a hierarchical Bayesian model for jointly predicting bilingual streams of tags. The model learns language-specific features while capturing cross-lingual patterns in tag distribution. Our evaluation shows significant performance gains over a state-of-the-art monolingual baseline. Acknowledgments The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168 and grant IIS-0835445) and the Microsoft Research Faculty Fellowship. Thanks to Michael Collins, Amir Globerson, Lillian Lee, Yoong Keok Lee, Maria Polinsky and the anonymous reviewers for helpful comments and suggestions. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.",
    "abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags. The model learns language-specific features while capturing cross-lingual patterns in tag distribution for aligned words. Once the parameters of our model have been learned on bilingual parallel data, we evaluate its performance on a held-out monolingual test set. Our evaluation on six pairs of languages shows consistent and significant performance gains over a state-of-the-art monolingual baseline. For one language pair, we observe a relative reduction in error of 53%.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Slovene",
        "English",
        "Bulgarian",
        "Serbian"
    ],
    "numcitedby": "67",
    "year": "2008",
    "month": "October",
    "title": "Unsupervised Multilingual Learning for {POS} Tagging"
}