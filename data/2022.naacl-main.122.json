{
    "article": "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify 'bias' and 'fairness' in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks. Introduction With the popularization of word embeddings by works such as Word2vec (Mikolov et al., 2013) , GLoVe (Pennington et al., 2014) and, more recently, contextualized variants such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) , Natural Language Processing (NLP) has seen significant growth and advancement. Word embeddings and later language models have been adopted by many applications. Many of these embeddings have been probed by researchers for biases such as gender stereotypes. Word embeddings are generally trained on realworld data such that they model statistical properties from the training data. Hence, they pick up biases and stereotypes that are typically present in the data (Garrido-Mu\u00f1oz et al., 2021) . Although Kurita et al. (2019) and Webster et al. (2020) opine that this can pose significant challenges in downstream applications, this view has been questioned, especially for non-contextualized word embeddings (Goldfarb-Tarrant et al., 2021) . Early works such as Bolukbasi et al. (2016) ; Caliskan et al. (2017) ; Gonen and Goldberg (2019) widely explored fairness in non-contextualized embedding methods. In non-contextualized embeddings such as Word2vec and GLoVe embeddings, models are trained to generate vectors that map directly to dictionary words and hence are independent of the context in which the word is used. In contrast, contextualized word embeddings take polysemy (words could have multiple meanings, e.g. 'a stick' vs 'let's stick to') into consideration. Thus different embeddings are generated for a given word depending on the context in which it appears. Because of such differences between the two approaches, popular techniques for detecting and measuring bias in non-contextualized word embeddings, such as WEAT (Caliskan et al., 2017) , do not apply naturally to contextualized variants. Many techniques have been proposed to measure bias in contextualized word embeddings, either as a standalone method (May et al., 2019; Bartl et al., 2020) or as an additional contribution to evaluate fairness interventions (Webster et al., 2020; Lauscher et al., 2021; Kurita et al., 2019) . This broad selection of methods makes it difficult for NLP practitioners to select an appropriate and reliable set of metrics to quantify bias and to compare results. This is further exacerbated as these quantifying techniques also involve different choices for attribute and target words, commonly jointly referred to as seed words, templates for context, and different methods for measuring similarity. In this paper, we combine literature survey and experimental comparisons to compare fairness met-rics for contextualized language models. We are guided by the following research questions: \u2022 Which fairness measures exist for contextualized language models such as BERT? (Section 3) \u2022 What challenges do languages other than English pose? ( \u00a7 3.3) \u2022 What are the relationships between fairness measures, the templates these measures use, embedding methods, and intrinsic vs extrinsic measures? (Section 4) \u2022 Which set of measures do we recommended to evaluate language resources? (Section 7) Background Static word embeddings have typically been used with recurrent neural networks (RNNs), optionally with an attention mechanism (Bahdanau et al., 2014) . The transformer architecture (Vaswani et al., 2017) introduced a new paradigm relying only on attention, which proved faster and more accurate than RNNs and did not rely on static word embeddings. The transformer consists of two stacks of attention layers, the encoder and the decoder, with each layer consisting of multiple parallel attention heads. BERT (Devlin et al., 2019) is based on the encoder from this transformer and obtained state-of-the-art results for multiple NLP tasks using transfer learning with a pre-training step and a second finetuning step. The pre-training task is to reconstruct missing words in a sentence, called masked language modeling (MLM), which helps capture interesting semantics. The training objective for a model with parameters \u03b8 is to predict the the original token on the position of a randomly masked token x m based on the positional-dependent context x /m = x 0 , . . . , x m\u22121 , x m+1 , . . . , x N , following max \u03b8 N i=1 1 x i =x /m log P x i | x /m ; \u03b8 with 1 x i =x /m as indicator function. After training, the language model can infer the probability that a token occurs on the masked position. As an illustration with the original BERT model, the sentence '[MASK]is a doctor.' is filled in with the token 'He' (62%), followed by 'She' (32%). Because the MLM task relies on co-occurrences, this example illustrates how this task captures stereotypes that are present in pre-training datasets, which is referred to as intrinsic bias. Pre-trained model e.g. BERT Pretraining corpora e.g. OSCAR, Wiki, ... Intrinsic biases Finetuned model e.g. BERT Extrinsic biases Transfer learning [CLS] Downsteam tasks e.g. NER, coref., POS As a second step, this pre-trained model can be finetuned on a new task, most commonly either sentence classification, which uses the contextualized embeddings of the first token x 0 = [CLS], or token classification, for which the embeddings of each respective token position are used. These embeddings are obtained from output states of the penultimate layer, after which a single linear layer is added and trained. This finetuning is typically done with different datasets that are labeled for the task at hand and here we can observe extrinsic bias with allocational harms (Goldfarb-Tarrant et al., 2021; Blodgett et al., 2020) , e.g. gender imbalances in co-reference resolution (see \u00a7 3.2). Many models improved on the original BERT architecture and training setup, e.g. RoBERTa (Liu et al., 2019) was trained on significantly more data for a longer period and without a second pretraining objective, next sentence prediction. AL-BERT (Lan et al., 2019) used parameter sharing between attention layers to obtain a smaller model without significant performance degradation. Sanh et al. (2019) also created a smaller BERT variation, DistilBERT, by using knowledge distillation. All these models are MLMs, so this gives us the opportunity to compare bias metrics across models. Fairness in word embeddings Fairness in machine learning has a long standing history and a general introduction is out of scope for this paper, so we refer the reader to Barocas et al. (2019) .Typical metrics, e.g. demographic parity, are not directly applicable to tasks dealing with natural language. Furthermore, many NLP applications finetune existing language models, which intertwines extrinsic and intrinsic biases as discussed earlier in Section 2. Early methods for evaluating bias in noncontextualized embeddings like Word2vec, are WEAT (Caliskan et al., 2017) and a direct bias metric (Bolukbasi et al., 2016) . The latter demonstrated that word embeddings contain a (linear) biased subspace, where for example 'man' and 'woman' can be projected on the same gender axis as 'computer programmer' and 'homemaker' (Bolukbasi et al., 2016) . These analogies are calculated using cosine distance between vectors to define similarity and also to evaluate the authors' proposed debiasing strategies. In addition, pairs of gendered words were also evaluated using Principal Component Analysis (PCA). This showed that most of the variance stemming from gender could be attributed to a single principal component (Bolukbasi et al., 2016) . In parallel, the Word Embeddings Association Test (WEAT; Caliskan et al., 2017) was developed based on the Implicit Association Tests (IAT; Greenwald et al., 1998) from social sciences. WEAT measures associations between two sets of target words X , Y, e.g. male and female names, and another two sets of attribute words A, B, e.g. career and family-related words, following s(X , Y, A, B) = x\u2208X u(x, A, B) \u2212 y\u2208Y u(y, A, B) with a similarity measure u(x, A, B) 1 that measures the association between one word embedding x and the word vectors of attributes a \u2208 A, b \u2208 B, defined as (x, A, B) = mean a\u2208A cos (x, a) \u2212 mean b\u2208B cos (x, b). This method relies on a vector representation for each word and by providing a representation from a contextualized model, WEAT can also be adapted for contextualized language models, which we discuss in Section 3 and \u00a7 4.3. 3 Measuring fairness in language models 3.1 Intrinsic measures Discovery of Correlations (DisCo). Webster et al. (2020) presented an intrinsic measure Discovery of Correlations (DisCo) that uses templates with two slots such as ' likes to [MASK] .', we provide a complete list in \u00a7 A.1. The first slot ( ) is filled with words based on a set of e.g. first names or nouns related to professions. The second masked slot is filled in by the language model and the three top predictions are kept. If these predictions differ between sets, this is considered an indication of bias. Lauscher et al. (2021) slightly modified this method by filtering predictions with P (x m | T ) > 0.1 instead of the top-three items. Log Probability Bias Score (LPBS). This bias score presented by Kurita et al. ( 2019 ) is a templatebased method that is similar to DisCo,but also corrects for the prior probability of the target attribute, as for example the token 'He' commonly has a higher prior than 'She'.The reasoning is that correction ensures that any measured difference between attributes can be attributed to the attribute and not to the prior of this token. Bartl et al. (2020) introduced an alternative dataset specifically for this evaluation method, called bias evaluation corpus with professions (BEC-Pro), with templates and seeds in both English and German. We will revisit the German results in \u00a7 3.3. Sentence Embedding Association Test (SEAT). A limitation of WEAT (Caliskan et al., 2017) is that the method does not work directly on contextualized word embeddings, which SEAT solves by using context templates (May et al., 2019) . These templates are semantically bleached, so there are no words in there that affect bias measurements, for instance ' is a [MASK].' We will investigate this concept further in \u00a7 4.2. These templates are used to extract an embedding to measure the mean cosine distance between two sets of attributes, after which WEAT is applied as discussed in \u00a7 2.1. This embedding is obtained from the [CLS] token in BERT. May et al. (2019) implemented three tests from WEAT. In addition, the authors also made new tests for double binds (Stone and Lovejoy, 2004) and angry Black woman stereotypes. An approach inspired by SEAT was taken by Lauscher et al. (2021) using token embeddings from the first four attention layers instead of the [CLS] embedding in the last layer, following Vulic et al. (2020) . Tan and Celis (2019) also adapted SEAT by relying on the embedding of the token of interest in the last layer, instead of the [CLS] token. We will discuss these different embedding methods in \u00a7 4.3. (Caliskan et al., 2017) was presented by Guo and Caliskan (2021) . CEAT uses Reddit data (up to 9 tokens) as context templates, which provide more realistic Table 1 : Overview of intrinsic measures of bias for language models. For brevity, we include most templates in Appendix A and address differences between templates in \u00a7 4.2. We also discuss the evaluation types ( \u00a7 3.1) and embedding types ( \u00a7 4.3) . We also indicate if data and source code are both available ( v ), or if only a dataset is available ( f s ), or if neither is publicly available ( f ). The repositories are linked in Appendix D. CrowS-Pairs. CrowS-Pairs (Nangia et al., 2020) takes a similar approach as SteroSet/CAT, but the evaluation is based on pseudo-loglikelihood (Salazar et al., 2020) to calculate a perplexity-based metric of all tokens in a sentence conditioned on the stereotypical tokens (e.g. 'He'). Contextualized Embedding Association Test (CEAT). Another extension of WEAT All samples consist of pairs of sentences where one has been modified to contain either a stereotype or an anti-stereotype. ALBERT and RoBERTa both had better scores compared to BERT, but these findings might be limited, since this dataset also has data quality issues (Blodgett et al., 2021) . All Unmaksed Likelihood (AUL). Kaneko and Bollegala (2021) modify the above CrowS-Pairs measure to consider multiple correct predictions, instead of only testing if the target tokens are predicted. In addition, the authors also argue against evaluations biases using [MASK] tokens, since these tokens are not used in downstream tasks. PCA-based methods. Both Basta et al. ( 2019 ); Zhao et al. (2019) analyzed gender subspaces in ELMo using a method that is very similar to Bolukbasi et al. (2016) . This approach was then applied to BERT-based models (Sedoc and Ungar, 2019) . We do not further compare to these methods, since they are less suited to obtain numerical bias scores as they rely on identifying a unique gender axis. Extrinsic measures Extrinsic measures are used to measure how bias propagates in downstream tasks such as occupation prediction and coreference resolution. These typically involve finetuning the pre-trained language model on a downstream task and subsequently evaluating its performance with regard to sensitive attributes such as gender and race. As elsewhere in the bias literature, most evaluations focus on gender bias due to the relative availability of gender-related datasets and the relatively widespread concern for gender-related biases. BiasInBios. De-Arteaga et al. ( 2019 ) developed an English dataset as a classification benchmark for measuring bias in language models, which has been adopted as an extrinsic measure (Webster et al., 2020; Zhao et al., 2020) . The task is to predict professions based on biographies of people. Bias is quantified as the true positive rate difference between male and female profiles. We will investigate BiasInBios as a fairness metric in ( \u00a7 4.4). Winograd schemas. The Winograd schema (Levesque et al., 2012) , originally designed to test machine intelligence based on anaphora resolution, has been adapted in various works into benchmark datasets for bias evaluation. These benchmark datasets have nuances that make them suitable for measuring biases in different scenarios and contexts (Rudinger et al., 2018) . Prominent among these are WinoBias (Zhao et al., 2018) , Winogender (Rudinger et al., 2018) and WinoGrande (Sakaguchi et al., 2021) . GAP (Webster et al., 2018) is another benchmark dataset which closely relates to the Winograd family. It has also been used to measure bias in pronoun resolution methods. The WinoBias dataset covers 40 occupations and is used to measure the ability of a language model to resolve coreferencing of gender pronouns (female and male) in the context of pro-stereotype and anti-stereotype jobs. A pro-stereotype setting is when, for instance, a male pronoun is linked to a male-dominated job, whereas a female pronoun being linked to that same job will be an antistereotype. E.g. Pro-stereotype: [The janitor] reprimanded the accountant because [he] got less allowance. Anti-stereotype: [The janitor] reprimanded the accountant because [she] got less allowance. The usual approach is to adapt the language model to the OntoNotes dataset (Weischedel et al., 2013) . A model is said to pass the WinoBias test if resolution is done with the same level of performance for pro-stereotype and anti-stereotype instances. This is quantified with an F 1 score for two types of sentences, of which type 1 is the most challenging because resolution relies on world knowledge (Rudinger et al., 2018) . Using this approach, de Vassimon Manela et al. ( 2021 ) extended Wino-Bias to include skew towards one gender, following 1 2 (|F fpro 1 \u2212 F mpro 1 | + |F f anti 1 \u2212 F m anti 1 |) . In ( \u00a7 4.4), we will also investigate WinoBias (type 1) and the skew variant as implemented by de Vassimon Manela et al. (2021). Measuring biases in other languages Many languages have some sort of grammatical gender, which can interfere with fairness evaluation metrics presented in \u00a7 3.1 that focus mostly on gender stereotyping by measuring associations. The assumption is that there should be no association between e.g. professions and gender. However, these associations can be expected in gendered languages. We provide a brief overview of some methods that address languages beyond English. Delobelle et al. (2020) and Ch\u00e1vez Mulsa and Spanakis (2020) evaluated RobBERT, a Dutch language model. Delobelle et al. (2020) did this visually with three templates ( \u00a7 A.5). Associations between gendered pronouns and professions were not considered an indicator of bias, since this is expected in Dutch. Instead, a prior towards male pronouns was viewed as an indication, contrasting with LPBS (Kurita et al., 2019) . For German, Bartl et al. (2020) evaluated BEC-Pro. The authors found that the scores for male and female professions were very similar, likely because of the gender system. Finally, Nozza et al. ( 2021 ) presented a multilingual approach using HurtLex (Bassignana et al., 2018) , focusing on six European languages (English, Italian, French, Portuguese, Romanian, and Spanish) with BERT and GPT-2. Both models replicated multiple stereotypes and reproduced derogatory words across languages, leading the authors to question the suitability for public deployment. On the compatibility of measures In this section, our goal is to objectively investigate the consistency in indicating bias between various techniques used by previous works. As mentioned earlier, besides the metric choice, three primary factors are important when measuring intrinsic bias in an embedding model: (i) choice of seed words, (ii) choice of templates and (iii) how representations for seed words are generated. Recent works investigating bias in language models have found issues with inconsistencies between seed words (Antoniak and Mimno, 2021) , unvoiced assumptions and data quality issues in StereoSet and CrowS-Pairs templates (Blodgett et al., 2021) , and issues with semantically bleached templates (Tan and Celis, 2019). These issues raise some questions for the remaining two factors, for example whether or not the choice of template and technique for selecting embeddings to represent seed words matters in measuring bias? And are \"semantically bleached\" templates really semantically bleached? Meaning, do they not affect bias measurements? Or in the extreme, can bias in embedding model stay hidden by picking the \"wrong\" templates or representations? These are questions we seek to answer with a series of experimental analysis where we measure correlations between various approaches to test if these templates and representations measure the same bias. 2 when using two different embedding approaches, namely the [CLS] (Figure 2a ) and the pooled target token embeddings (Figure 2b ). Different embeddings result in different results, which we discuss further in \u00a7 4.3. The Pearson correlation coefficients in bold are significant at the \u03b1 = 0.05 level. Methodology We conduct correlation analyses between different templates ( \u00a7 4.2) and between representation methods ( \u00a7 4.3), as well as between measures themselves ( \u00a7 4.4). To create a context and to help draw concise conclusions, we focus all our experiments on binary gender bias with respect to professions. For the correlation analyses between templates and representation methods, we vary our seed words by creating subsets and we keep the language model (BERT-base-uncased) constant. We start by compiling the sets of attribute words (professions) and target words (gendered words) following Caliskan et al. (2017) and Zhao et al. (2018) , which are split in two sets of male and female \"stereotyped\" professions ( \u00a7 B.1) and we create female and male sets of target words ( \u00a7 B.2). We generate 20 subsets {a 1 , ..., a 20 } by randomly sampling 10 professions for each set of attributes, thus for male and female professions (see \u00a7 B.1 for the full list). We expect that some subsets will show higher levels of bias than others and that given two \"accurate\" fairness metrics M 1 and M 2 , if M 1 indicates that a 1 contains less bias than a 2 which in turn contains less bias than a 3 , M 2 should likewise indicate bias for the three subsets. Caliskan et al. (2017) ; May et al. (2019) ; Lauscher et al. (2021) ; Tan and Celis (2019) used a similar approach to calculate distributional properties and quantify the variance.In our experiments, we use Pearson correlation coefficients. For the third correlation experiment between fair-ness metrics ( \u00a7 4.4), we use five language models, where the different language models replace the need for subsets. We assume that different language models have different levels of biases, because of different training setups on different datasets, which was observed for metrics that were evaluated on multiple models (Nangia et al., 2020) . We also use the templates and seed words for each metric as described in the original papers, since we compare the metrics as they are used. Compatibility between templates The choice of template for creating contexts for seed words plays a very important role in measuring bias in contextual word embeddings. Many papers propose the use of \"semantically bleached\" sentence templates for context which should contain no semantic meaning so that the embedding generated by inserting a seed word into such a template should only represent the seed word. May et al. (2019) ; Tan and Celis (2019) indicated that semantically bleached templates might still contain some semantics, at least related to the bias. If these templates are semantically bleached with regard to a gender bias, all these templates should have a high correlation with other bleached templates. We test the bleached SEAT templates (May et al., 2019) , listed in Table 2 (T 1 \u2212 T 8 ). We also compare with the masked template of used by Kurita et al. (2019) for their SEAT implementation (T 9 ), and add 2 semantically unbleached templates from Tan and Celis (2019) (T 10 \u2212 T 11 ) as control templates. We test both the [CLS] embedding as Table 2 : Templates used in our evaluation of the compatibility between templates. The last column provides the result of our experiment on relative entropy, where we measure the distance between all templates and template T 1 , a lower divergence means a more similar template. The source of the templates is indicated in Table 4 We test our hypothesis with a correlation analysis as described in \u00a7 4.1 and we additionally test how the distribution differs between templates. We operationalize semantically bleached templates as two templates T 1, T 2 having the same contextualized probability for a set of tokens on position x m , following P (x m | T 1 ) = P (x m | T 2 ) . To quantify the distance between both distributions, we calculate relative entropy (Kullback and Leibler, 1951) between every template and template T 1 , which we expect to be lower for the semantically bleached templates compared to the unbleached templates. We perform this relative entropy experiment twice: (i) once with all tokens in the model's vocabulary and (ii) once with a set of gendered tokens (see \u00a7 B.2). Both sets aim to evaluate how the contextualized distributions of the masked token t i = P (x m | T i ) differ, but we expect a lower divergence in particular for the gendered subset. Figure 2a and Table 2 present our results for the correlation analysis and difference in distributions, where we make three observations. Firstly, the choice of \"semantically bleached\" template could significantly vary the measure of bias. Although templates T 1 \u2212 T 9 are all bleached, there are weak and sometimes even negative correlations (e.g. T 7 ). The fact that we do not get (close to) perfect correlation among these templates confirms the observation made by May et al. (2019) on the possible impact that \"semantically bleached\" templates could have on fairness evaluations. Secondly, semantically and syntactically similar templates do not necessarily correlate strongly. E.g. \"There is the .\" (T 3 ) and \"The is there.\" (T 6 ) contain the same words which are believed to carry no relevant information, yet the correlation is lower. Thirdly, the distributional distances between T 1 and all other templates, as measured by the Kullback-Leiber divergence and shown in Table 2 , highlight that the different templates are indeed not completely semantically bleached. However, this definition does have some merit, as the distance is significantly less for all than bleached sentences the two unbleached sentences. Based on the above observations, we conclude that semantically bleached templates need to be used cautiously, and any results stemming from the use of such templates cannot be objectively maintained so long as there does not exist a standardized and validated scheme of selecting such templates. Compatibility between representations Word representations or embeddings could also be a source of inconsistency in evaluating contextualized language models. Since many techniques use templates, it is natural to use the entire sentence representation as the representation of the word in question, e.g. by mean-pooling over all target tokens or using the [CLS] embedding. We test these methods and some additional combinations that have been used in the literature, yet not necessarily for bias evaluations. A complete list with explanations can be found in Appendix C. Firstly, we investigate whether there are inconsistencies between methods by conducting corre- Using the [CLS] embedding as the representation of seed words may not be an accurate representation since it captures information from the context, meaning the templates are evidently not as semantically bleached as one would imagine. Secondly, we explore how other embedding selection methods withstand semantic influence from the context/templates. Tan and Celis (2019) propose using the contextual word representation of the token of interest instead of [CLS]. We investigate the effectiveness of this approach by replicating the experiment in Figure 2a . The results on the correlations between template types show that using only the embeddings of the target word (Figure 2b ) produces more consistent results than using the [CLS] embedding as the representation (Figure 2a ). Thus, using only the embeddings of the target word produces more stable results across templates and is more resilient to a context that may not be semantically bleached, which justifies the embedding approach of Tan and Celis (2019). Compatibility between metrics In this section, our goal is to (i) see if there is a general relationship between intrinsic and extrinsic bias measures and (ii) how individual bias metrics correlate with extrinsic bias. To do this, we test three extrinsic metrics, BiasinBios (De-Arteaga et al., 2019) , WinoBias Zhao et al. (2018), and skew (de Vassimon Manela et al., 2021) . and we evaluate five popular language models 2 . For Wino-Bias, we adapt the models to the OntoNotes 5.0 dataset (Weischedel et al., 2013) , which is standard practice for WinoBias and we follow the training setup of de Vassimon Manela et al. (2021) . We performed a correlation analysis between the results of the three extrinsic measures and a set of intrinsic fairness measures from Section 3; the results are presented in Figure 4 . We observe that most correlations with the extrinsic BiasInBios measure are negative-which is expected since this measure gives a higher score if more bias is present-but still strongly correlated with some intrinsic measures, like a WEAT variant by Tan and Celis (2019). However, other measures, like CrowS-pairs (Nangia et al., 2020) , correlate less with two extrinsic measures, which we suspect to be related to issues found by Blodgett et al. (2021) , although more experiments are needed to confirm this. Part of these poor correlations are caused by the differences in templates ( \u00a7 4.2) and representations ( \u00a7 4.3) that we observed, but such differences remain worrisome. Code We make the source code available and also publish a package to bundle fairness metrics at https:// github.com/iPieter/biased-rulers. Discussion and ethical considerations We mostly compare one of the most frequently studied settings, namely binary gender biases with a focus on professions. Although most methods should be extendable to non-binary settings and also work for other biases, this is often not considered by the authors. Furthermore, different works also consider different notions of gender and conflate multiple notions (Cao and Daum\u00e9 III, 2020) . Both issues should be addressed in future works. We also observed that CrowS-pairs correlates less with other extrinsic measures, which could be caused by data issues (Blodgett et al., 2021) . Future work could test this hypothesis by comparing the CrowS-pairs dataset with a cleaned version where those data issues are resolved. However, such a version does currently not exist. Related to this, is the design of the templates. We observed excessive variation between templates, similar to the differences between few-shot prompts that are used with autoregressive models like GPT-2 (Lu et al., 2021) . Future work could also focus on template designing and refine the concept of semantically bleached templates. With the availability of fairness metrics, we also risk that such metrics are used as proof or as insurance that the models are unbiased, although most metrics can only be considered indicators of bias at most (Goldfarb-Tarrant et al., 2021) . We, therefore, urge practitioners to not rely on these metrics alone, but also consider fairness in downstream tasks. We also did not draw much attention to many other negative impacts of language models that practitioners should consider, e.g. high energy usage or not including all stakeholders when training a language model (Bender et al., 2021) . Conclusion In this paper, we presented an overview of fairness metrics for contextualized language models and we focused on which templates, embeddings and measures these metrics used. We evaluated how these metrics correlate with each other, as well as how parts of these metrics correlate. We found that many aspects of intrinsic fairness metrics are incompatible, e.g. choosing different templates, embeddings, or even metrics. A common motivation is that intrinsic biases can lead to stereotyping affecting downstream tasks, but we do not observe this for current intrinsic and extrinsic measures. Our advice is to use a mix of some intrinsic measures of fairness that don't use embeddings directly and eliminate one source of variance, for example DisCo or LPBS, in addition to a measure like Tan and Celis (2019) that seems to correlate well with at least some notion of extrinsic bias. However, we also recommend to perform extrinsic fairness evaluations on downstream tasks, since this is where actual resource allocations happen and where intrinsic and extrinsic biases collude. Acknowledgements We thank Luc De Raedt for his continued support, Jessa Bekker for her practical advice on writing a survey, and Eva Vanmassenhove for sharing her knowledge on gender bias. Pieter Delobelle was supported by the Research Foundation -Flanders (FWO) under EOS No. 30992574 (VeriLearn). Both Pieter Delobelle and Ewoenam Kwaku Tokpo also received funding from the Flemish Government under the \"Onderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen\" programme. Bettina Berendt received funding from the German Federal Ministry of Education and Research (BMBF) -Nr. 16DII113. 'painter ', 'firefighter', 'machinist', 'conductor', 'cabinetmaker', 'pilot', 'laborer', 'engineer', 'cleaner', 'programmer', 'courier', 'porter', 'announcer', 'estimator', 'architect', 'chef', 'clergy', 'drafter', 'dishwasher' B. 2 List of target words \u2022 female list: 'female', 'woman', 'girl', 'sister', 'daughter', 'mother', 'aunt', 'grandmother' \u2022 male list: 'male', 'man', 'boy', 'brother', 'son', 'father', 'uncle', 'grandfather' C Embedding methods [CLS]-templates: Seed words with semantically bleached templates where the [CLS] token embedding is used as the representation -SEAT (May et al., 2019) . [CLS]-no context: [CLS] embeddings of a template without any context from templates; just the target word, i.e. '[CLS] X [SEP]' (May et al., 2019) . Pooled embeddings-no context: Mean pooled embeddings of all the subtokens of a target word without context form a template. Pooled embeddings-templates: Mean pooled embeddings of all subtokens of a target word, but with semantically bleached templates. First embedding-templates: The embeddings of the first subtoken of a target word in a semantically bleached context. (Tan and Celis, 2019; Kurita et al., 2019) . Vulic et al. (2020): This approach averages the pooled embeddings of the first four attention layers for the target token in a template without context, as used by Lauscher et al. (2021) . D Source code and datasets Table 3 : Publicly accessible source code and/or data repositories for different metrics. Metric Source code and datasets DisCo (Webster et al., 2020) https://github.com/google-research-datasets/zari LPBS (Kurita et al., 2019) https://github.com/keitakurita/contextual_embedding_bias_measure BEC-Pro (Bartl et al., 2020) https://github.com/marionbartl/gender-bias-BERT SEAT (May et al., 2019) https://github.com/W4ngatang/sent-bias Tan and Celis (2019) https://github.com/tanyichern/social-biases-contextualized Liang et al. (2021) https://github.com/pliang279/LM_bias Dinan et al. (2020) https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/md_gender Sedoc and Ungar (2019) https://github.com/jsedoc/ConceptorDebias Dev et al. (2020) https://github.com/sunipa/On-Measuring-and-Mitigating-Biased-Inferences-of-Word-Embeddings StereoSet (Nadeem et al., 2021) https://github.com/moinnadeem/stereoset CrowS-Pairs (Nangia et al., 2020) https://github.com/nyu-mll/crows-pairs Winogender (Rudinger et al., 2018) https://github.com/rudinger/Winogender-schemas WinoBias (Zhao et al., 2018) https://github.com/uclanlp/corefBias Vig et al. (2020) https://github.com/sebastianGehrmann/CausalMediationAnalysis CEAT (Guo and Caliskan, 2021) https://github.com/weiguowilliam/CEAT HONEST (Nozza et al., 2021) https://github.com/MilaNLProc/honest",
    "abstract": "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify 'bias' and 'fairness' in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
    "countries": [
        "Anguilla"
    ],
    "languages": [
        "Dutch",
        "English",
        "German"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models"
}