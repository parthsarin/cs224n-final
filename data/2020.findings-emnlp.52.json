{
    "article": "Syntactic information is essential for both sentiment analysis(SA) and aspect-based sentiment analysis(ABSA). Previous work has already achieved great progress utilizing Graph Convolutional Network(GCN) over dependency tree of a sentence. However, these models do not fully exploit the syntactic information obtained from dependency parsing such as the diversified types of dependency relations. The message passing process of GCN should be distinguished based on these syntactic information. To tackle this problem, we design a novel weighted graph convolutional network(WGCN) which can exploit rich syntactic information based on the feature combination. Furthermore, we utilize BERT instead of Bi-LSTM to generate contextualized representations as inputs for GCN and present an alignment method to keep word-level dependencies consistent with wordpiece unit of BERT. With our proposal, we are able to improve the stateof-the-art on four ABSA tasks out of six and two SA tasks out of three. Introduction Sentiment analysis(SA), also known as opinion mining, is the task of determining the polarity of a piece of text. Commonly the classification is whether the text is expressing a negative or positive attitude towards a topic or a product. Fine-grained sentiment analysis involves more than two sentiment classes (very negative, negative, neutral, positive and very positive). Aspect-based sentiment analysis(ABSA) is one step further by assigning sentiment polarities to specific aspects of an involved entity or a topic. For example, comment on a restaurant saying \"The restaurant was expensive, but the menu was great\" has positive and negative attitudes for two aspects food and price. Much progress has been made recently to advance the state-of-the-art on shared SA and ABSA tasks. Contributions mainly come from two research directions. One is to take advantage of the pre-trained language models such as ELMo (Peters et al., 2018) , BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019a) , which are typically employed to extract contextual features of a piece of text for the final classifer. These models effectively alleviate the heavy effort of feature engineering of earlier work on SA and ABSA. Further inventions have been proposed to better fine-tune these models. For instance, a recent work (Sun et al., 2019a) converts ABSA to a sentence pair classification task, where an auxiliary sentence is generated. It then finetunes the pre-trained model from BERT for this new task. Promising experimental results are observed. Second line of research is to exploit the syntactic structures of subjective sentences with a belief that interactions between words need to be considered in sentiment analysis, which however is not sufficiently captured by even the latest attention-based models. (Zhang et al., 2019) quotes a concrete example \"Its size is ideal and the weight is acceptable\", where acceptable is often incorrectly identified by attention models as the most attentive word to size. Previous works in (Socher et al., 2011; Dong et al., 2015; Qian et al., 2015; Socher et al., 2013) propose a recursive tree-structured model to compose sentence representation from its constituent phrases. (Kim et al., 2018) presents a novel RvNN architecture to dynamically integrate comprehensive syntactic information derived from the sentence parsing structure and linguistic tags on word level. Models using a Graph Convolutional Network(GCN) over the dependency tree of a sentence have shown evident effectiveness in ABSA tasks. The argument is that GCN captures long- range syntactic relations that are obscure from the surface (Sun et al., 2019b; Zhang et al., 2019; Zhao et al., 2019) . Though these efforts have substantially pushed up the state-of-the-art accuracy of SA and ABSA, some challenges remain for sentiment classification. For example, the aforementioned GCN-based models are designed to encode the dependency tree of a sentence, where the adjacency matrix is binary with 1 representing if there is a dependency relationship between two corresponding words and 0 for others. However, types of dependency relations are diversified and the corresponding words of each relation may have different part-of-speech(POS) tags. These syntactic information should also influence the message passing process of GCN. As it is shown in Figure 1 , the relationship(\"det(vehicle-3, a-2)\") has less influence on polarity than the relationship(\"nsubj(worthwhile-14, film-11)\") in the sentence \"As a vehicle to savour Binoche 's skill , the film is well worthwhile\". Besides, as (Sethi and Bhattacharyya, 2017) points, pitfalls of SA and ABSA like Sentiment Shifters (such as Negations, Double Negations and But clauses) have not been well handled by current models. In this paper, we are motivated to encode more syntactic features and leverage both the pre-trained models and the syntactic parsing in a compositional way. We believe these are complementary to tackle the long-standing challenges for SA and ABSA. More specifically, we propose a Weighted Graph Convolutional Network(WGCN) to work with BERT. WGCN improves on top of GCN to model rich syntactic information. The adjacency matrix in WGCN represents not only the binary representations of dependency relations, but also the types of dependency relations as well as the part-ofspeech(POS) categories of the involved words. We argue that the POS tag of each word is the category assigned in accordance with its syntactic function , hence has influence on the overall sentiment of the sentence as well as sentiments of aspects. All weights and embeddings in WGCN are trainable. Details of this model will be provided later in this paper. WGCN reply on BERT to extract contextualized representations as inputs for the WGCN layers. One challenge is the inconsistency between the WordPiece unit of BERT, and the word-pairs considered in the dependency tree. We propose an alignment method to bridge this chasm. Our contributions are summarized as follows: \u2022 We propose a novel weighted GCN(WGCN) architecture over dependency tree which can exploit rich syntactic features by assigning trainable weights for adjacent matrix. \u2022 We propose a framework to compositionally exploit the pre-trained language models(BERT) and WGCN for SA and ABSA. We refer to the whole architecture as BERT-WGCN. \u2022 With our proposal, we are able to improve the state-of-the-art on four ABSA tasks out of six and two SA tasks out of three. The rest of the paper is organized as follows. Section 2 gives a brief review of BERT and GCN. Section 3 elaborates on our proposed overall model architecture that integrates WGCN and BERT, as well as how the model is trained respectively for SA and ABSA tasks. Section 4 reports our experiments and analysis. Review of GCN and BERT Graph convolutional network(Kipf and Welling, 2016) is an adaptation of the convolutional neural network (LeCun et al., 1998) for encoding unstructured data. Given a graph with k nodes, we can obtain an adjacency matrix A where A ij is obtained based on the connection between node i and node j. In an L-layer GCN where H l\u22121 represents the output feature matrix at (l \u2212 1)-th layer and H l represents the output feature matrix at the l-th layer, a graph convolutional operation can be written as: H l = \u03c3( D\u2212 1 2 \u00c3 D\u2212 1 2 H l\u22121 W l ) (1) \u00c3 = A + I k is the adjacency matrix with selfloops, where I k is the identity matrix. Dii = j \u00c3ij . D\u2212 1 2 \u00c3 D\u2212 1 2 is the normalized adjacency matrix. W l is a linear transformation weight,and \u03c3 is a nonlinear function(e.g., ReLU). In each graph convolution, each node collects and processes information from its neighboring nodes. BERT (Devlin et al., 2018) is one of the key innovations in the recent progress of contextualized representation learning inspired by Tranformer (Vaswani et al., 2017) . Given a sentence s = {w 1 , ..., w n }, its tokenized sequential representation is {t 1 , t 2 , ..., t k }. Transformer creates three vectors (query, key and value) for each sequence position, and then applies the attention mechanism for each position x i , using the query vector for x i as well as key and value vectors for all other positions. This computation can be presented as: Attention(Q, K, V ) = sof tmax( QK T \u221a d k )V (2) Instead of performing a single attention function, (Vaswani et al., 2017) found it is beneficial to have multiple attention heads. Bert built on Transformers contains a number of layers(Transformer blocks) L . Each layer is identical with a fixed number of hidden units H and a fixed number of multi-threading self-attention heads A. Particularly we use the BERT LARGE model with L = 24, H = 1024 and A = 16 as hyper-parameters. Approach Figure 2 gives an overview of the whole architecture. Our model consists of 3 main components. First, the input sequence of text is parsed into wordbased syntactic features as inputs for WGCN. At the same time, the text is also directly fed into BERT for wordpiece contextualized representations. One challenge here is the inconsistency between the wordpiece unit of BERT and word-based syntactic features for WGCN. The second part is the reform of GCN to exploit rich syntactic features. The third component is the sentiment classifer for SA and ABSA. The components will be introduced separately in the rest of the section. Token Alignment towards BERT Traditional GCN-based approaches over dependency tree use Bi-LSTM to get contextualized representations as initialized inputs for GCN (Zhang To resolve this issue, we propose an alignment procedure to map the word-level sequence from the parser to the wordpiece sequence in BERT. Dependency relations and POS tags are then accordingly aligned. The procedure is as follows: Given a piece of text s, the parser tokenizes it into a n word-level sequence: s = {w 1 , ..., w i , ..., w n } and BERT processes it into a k wordpiece sequence: s t = {t 1 , ..., t m , ..., t n , ..., t k }. For any given w i in s, there is a corresponding subsequence of wordpiece tokens seg i = {t m , ..., t n }, where 1 \u2264 m \u2264 n \u2264 k. We apply two alignment rules to map parsing results into a new form: \u2022 Rule 1: If w i is labeled by a POS Tagger as p i , then all tokens in seg i are assigned the same tag p i . \u2022 Rule 2: If there is a dependency relation r ij between w i and w j , then we assign the same dependency relation r ij between any token in seg i and any token in seg j . With this alignment, given an adjacency matrix A where A ij = 1 if node i is connected with node j, we can obtain a new adjacency matrix A align where A align xy = 1 for any token x in seg i and any token y in seg j . We plot one example in Figure 3 . For a better illustration, we show what the adjacency matrix looks like before and after the alignment. The left side shows the dependency matrix between the 14 words for the sentence \"As a vehicle to savour Binoche 's skill , the film is well worthwhile\". Each color represents a particular relation type. The right side shows the dependency matrix on wordpiece sentence \"as a vehicle to sa ##vo ##ur bin ##oche ' s skill , the film is well worth ##while\" after we run alignment with the above procedure. It's worth noting that we present directed graphs in Figure 3 for clarity. As GCNs generally do not consider directions, we use un-directional graph in our model. We aim to extend GCN to model rich syntactic information. To this end, we propose WGCN, which is depicted in Figure 4 . Following the same strategy in (Sun et al., 2019b; Zhang et al., 2018b,a) , WGCN also considers the adjacency matrix obtained from dependency tree as input. Different from their approaches, WGCN assigns trainable weights to the adjacency matrix. Each weight is compositionally determined by syntactic information including the type of dependency relation and the corresponding POS tags of the word-pairs. Weighted Graph Convolutional Networks over Syntactic Information Our hypothesis is that the type of dependency relation and POS tags of the word-pairs should have combined impacts on the process of aggregating information from neighbours in GCN. We follow the procedure proposed by (Guo et al., 2017) for Factorization Machines(FM) to cast pairwise feature interactions as inner product of the latent vectors, which has shown very promising results on many tasks. Let W type be a matrix of R d\u00d7Ntype , where d is the dimension of the embedding space which is fixed hyper-parameter, and N type is the number of types of dependency relations. Let W pos be a matrix of R d\u00d7Npos and N pos is the number of combinations of POS tags of all word-pairs appeared in dependency relations. The feature combination weight over the dependency relation from node x to node y in adjacency matrix can be presented as:  then each value of A can be computed as: \u03b1 xy = f (r xy )g((p x , p y )) (3) A xy = \u03b1 xy A align xy (4) where \u03b1 xy is computed from Equation (3) and A align xy is obtained by alignment rules. The process of obtaining A is shown in Figure 5 . To adapt with trainable adjacency matrix, we reform the custom GCN. Inspired by (Zhang et al., 2018c) , we use K-th power of adjacency matrix to aggregate information from K-hop neighbours. Since nodes never connect to themselves in a dependency relation, following the idea of selflooping(Kipf and Welling, 2016), we add a matrix I align which is transformed by an identity matrix with proposed alignment method to carry over information. Let H b be the final output of BERT layer, WGCN can be presented as : H GCN = \u03c3(C i (( A) K + I align )H b W ) (5) C i (\u2022) is a clip function for the matrix. W is the parameter matrix for WGCN and \u03c3 is the nonlinear ReLU function. Model Training for SA and ABSA Sentiment analysis considers the polarity of the whole sequence. In our framework, we use an average pooling to aggregate the whole sequence. Let H GCN = {h GCN 1 , ..., h GCN k } be the final output of WGCN, Avg(\u2022) be the average pooling function. The pooling process can be presented as: h SA = Avg {h GCN 1 , ..., h GCN k } (6) Aspect-Based Sentiment Analysis considers the polarity of several aspect words given in a current sentence. The BERT model and WGCN allow embeddings for aspect tokens to respectively aggregate contextual tokens and neighbouring tokens in a dependency tree, providing supervisory signals for the aspect-based classification task. Different from sentiment analysis, we use an average pooling to aggregate only the aspect words. Given a sentence pair (a, s), where a is a sub-sequence of s as aspect tokens. The final outputs of WGCN are {h GCN 1 , ..., h GCN as , ..., h GCN ae , ..., h GCN k } where a s and a e are indexes an aspect starts from and ends at. The pooling process can be presented as: h ABSA = Avg {h GCN as , ..., h GCN ae } (7) h SA or h ABSA is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over polarities. For training we use Adam algrithm (Kingma and Ba) with the crossentropy loss and L2-regularization. Experiments Datasets and Experimental Settings We conduct our experiments on five aspect-based sentiment analysis datasets and three sentiment analysis datasets: \u2022 TWITTER dataset for ABSA, was originally built by (Li et al., 2014) containing thousands of twitter posts. Annotations are sentiment labels(negative, neutral and positive) for given keywords or topics such as \"taylor swift\",\"xbox\". \u2022 LAP14, REST14, REST15, REST16 datasets for ABSA are respectively from SemEval 2014 task 4 (pontiki et al., 2014) , SemEval 2015 task 12 (Pontiki et al., 2015) and Se-mEval 2016 task 5 (Pontiki et al., 2016) , consisting of data from two categories, i.e. laptop and restaurant. \u2022 SST(SST2, SST5) is a dataset for sentiment analysis on movie reviews, which are anno- tated with two or five labels (Socher et al., 2013) . \u2022 SemEval13 is a dataset of Semeval-2013 task 2 (Nakov et al., 2013) for sentiment analysis, consisting of tweets with three sentiment labels(positive, negative and neutral). The statistics of datasets are reported in Table 1 . The datasets are parsed by Stanford parser(v3.6.0) for dependency relation and spacy(2.2.3) for POS tag. We use a learning rate of 0.0001 and a batch size of 32. We set the number of WGCN layers to 3 and the dimension of syntactic feature to 20, which are the best-performing settings in pilot studies. Experiments and benchmarks are run with a single GPU server with 4 V100 GPU cards and 8Gb of RAM. All models are implemented with Tensorflow 1.13 using Cuda 10.1. The experimental results are obtained by averaging 5 runs with random initialization, where Accuracy and Macro-Averaged F1 are adopted as the evaluation metrics. Model for Comparison To evaluate the effectiveness of our model(BERT-WGCN), we compare our performance with a range of baselines and state-of-the-art models, as listed below: \u2022 CDT (Sun et al., 2019b ) is a dependency graph convolutional network integrated with a Bi-LSTM model. \u2022 ASGCN-DG (Zhang et al., 2019) utilizes aspect-aware attention on a dependency graph convolutional network. \u2022 BERT-PT (Xu et al., 2019) transforms ABSA tasks to machine reading comprehension (MRC) and uses a post-training approach on BERT for ABSA tasks.. \u2022 SDGCN (Zhao et al., 2019) employs GCN to model the sentiment dependencies between different aspects in one sentence. \u2022 TNET (Li et al., 2018) employs CNN as the feature extractor and uses target specific transformation component to better integrate target information into the word representation. \u2022 BERT-ADA (Rietzler et al., 2019) uses selfsupervised domain-specific BERT language model for tuning, followed by supervised taskspecific fine-tuning. \u2022 BCN+CoVe (Brahma, 2018) utilizes prefix and suffix of each token in a sentence, which is encoded in both forward and reverse directions to capture long range dependencies for classification tasks. \u2022 SSAN (Ambartsoumian and Popowich, 2018) is a simple multiple self-attention network with positional-encoding for sentiment analysis. \u2022 XLNet (Yang et al., 2019b ) is an unsupervised language representation learning method based on a novel generalized permutation language modeling objective and employs Transformer-XL as the backbone model. \u2022 BERT-GCN(comp) (Rietzler et al., 2019) alignment and the size of parameters is in the same order of magnitude with our BERT-WGCN. \u2022 BERT(comp) (Rietzler et al., 2019 ) is a model for comparison which is based on BERT-LARGE and the size of parameters is in the same order of magnitude with our BERT-WGCN. Experimental Results Table 2 shows the performance of our model on accuracy and macro-F1 on ABSA tasks. Our BERT-WGCN outperforms most of the compared models on REST15, REST16 and TWITTER datasets, and achieves competitive results on SEM14(LAP) and SEM14(REST) datasets compared with SDGCN and BERT-ADA. Notably, our model achieves highest average accuary and F1 on SEM14(LAP) and SEM14(REST) dataset combined. The results demonstrate the effectiveness of BERT-WGCN. For ablation study, we compare our GCN-based models with BERT(comp) with same number of parameters. BERT-GCN(comp) and BERT-WGCN can consistently show improvements. It implies that the syntactic structure is helpful for ABSA tasks. Compared to BERT-GCN(comp), BERT-WGCN is able to gain better performance for almost all ABSA datasets. It proves that WGCN factorizing dependency relations and POS tags is better at utlizing syntactic information than the traditional GCN architecture. For the slight F1 degradation on the REST16 dataset, the reason might be that the size of REST16 datasets is relatively small. Another important observation is that all architectures that achieve the state-of-the-art results utilize pre-trained model. SDGCN-BERT initializes the word embeddings with pre-trained BERT token embeddings and uses self-attention network for training. BERT-ADA uses domain-specific dataset for model pre-training. Thus we believe that the contextualized information is essential for ABSA tasks. For SA task, as it is shown in Table 4 , the message is complex. For SST-2 dataset, our proposed model has no improvement. For SST-5 and Se-mEval2013, as far as we know, we achieve the new state-of-the-art performance. For ablation study, BERT-GCN(comp) and BERT(comp) get almost the same performance. We believe the main reason is that the importance of sentence structure in SA tasks is not as important as that in ABSA tasks. BERT-WGCN gets better performance mainly based on the additional feature combinations. Model Case Analysis In this section we compare BERT-WGCN with two baseline models on case examples. To this end we present visualizations showing the weights extracting from the whole sentence by aspect tokens on ABSA tasks. To show the effectiveness of our model, we expect the aspect tokens can attend to tokens which can influence the sentiment correctly. As it is shown in Table 3 , the first example \"great food but the service was dreadful!\" has two aspects within one sentence. The BERT model is able to detect the polarity for the first aspect \"food\" but fails to infer sentiment polarities for aspect \"service\". Our hypothesis is that the distance between aspect token and adjunct token is important for attentionbased model. The GCN-based model can address this connection correctly because they are directly related on the dependency tree. The second example \"Our waiter was friendly and it is a shame that he didn't have a supportive staff to work with .\" shows the importance of feature combination of dependency relation and POS tags on Negatives. These results suggest the advantage of our model against attention-based model and traditional GCNbased models. Investigation on the Combination of Syntactic Features Table 5 : Importance of different Feature Combination on SST-5 task. To evaluate the influence of feature combination of dependency relation and POS tags of word-pairs, we present several combinations of different importance in WGCN based on the weight score in adjacency matrix. As we use clip function in training, the combinations in column is not ordered. As it is shown in Table 5 , relations of adjectival modifier(\"amod\") or nominal subject(\"nsubj\") from \"ADJ\" to from \"NOUN\" outweighs relation of determiner(\"det\") in SA tasks. Another observation is that dependency type and POS tags jointly determine the importance. Same dependency relation may have different importance according to the corresponding POS tags. The number of GCN layers K indicates that we can obtain K-hop neighborhood matrix. We vary the number of layers in {1, 2, 3, 4, 5, 6, 7} and check the corresponding accuracy of BERT-GCN(comp) and BERT-WGCN on the REST14 dataset. The results are shown in Figure 6 . In particular, the performances of both models increase in first 3 layers. The performance becomes unstable after that. With the increase of number of layers, the model becomes more difficult to train and the performance begins to fall. Impact of GCN Layers Conclusion In this paper we propose a novel weighted graph convolutional networks(WGCN) to work with BERT on sentiment analysis and aspect-based sentiment analysis tasks. WGCN improves on top of GCN to model rich syntactic information including dependency relations as well as POS tags. BERT is used as a powerful tool to extract contextual representations, which are then used as inputs to WGCN to derive the final vectors for classification. We propose an alignment approach to solve the token inconsistency issue between WGCN and BERT. Our experimental results with visualizations show the success of our proposal comparing to the baseline and previous approaches in the literature. ",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The provided text does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the research conducted or for the writing of the article. Without explicit mention of funding, it is not possible to determine the presence of any specific funding source.",
    "abstract": "Syntactic information is essential for both sentiment analysis(SA) and aspect-based sentiment analysis(ABSA). Previous work has already achieved great progress utilizing Graph Convolutional Network(GCN) over dependency tree of a sentence. However, these models do not fully exploit the syntactic information obtained from dependency parsing such as the diversified types of dependency relations. The message passing process of GCN should be distinguished based on these syntactic information. To tackle this problem, we design a novel weighted graph convolutional network(WGCN) which can exploit rich syntactic information based on the feature combination. Furthermore, we utilize BERT instead of Bi-LSTM to generate contextualized representations as inputs for GCN and present an alignment method to keep word-level dependencies consistent with wordpiece unit of BERT. With our proposal, we are able to improve the stateof-the-art on four ABSA tasks out of six and two SA tasks out of three.",
    "countries": [
        "China"
    ],
    "languages": [
        "Dong"
    ],
    "numcitedby": 1,
    "year": 2020,
    "month": "November",
    "title": "A structure-enhanced graph convolutional network for sentiment analysis",
    "values": {
        "building on past work": "Previous works in (Socher et al., 2011; Dong et al., 2015; Qian et al., 2015; Socher et al., 2013) propose a recursive tree-structured model to compose sentence representation from its constituent phrases. (Kim et al., 2018) presents a novel RvNN architecture to dynamically integrate comprehensive syntactic information derived from the sentence parsing structure and linguistic tags on word level. Models using a Graph Convolutional Network(GCN) over the dependency tree of a sentence have shown evident effectiveness in ABSA tasks. The argument is that GCN captures long- range syntactic relations that are obscure from the surface (Sun et al., 2019b; Zhang et al., 2019; Zhao et al., 2019) . Though these efforts have substantially pushed up the state-of-the-art accuracy of SA and ABSA, some challenges remain for sentiment classification. For example, the aforementioned GCN-based models are designed to encode the dependency tree of a sentence, where the adjacency matrix is binary with 1 representing if there is a dependency relationship between two corresponding words and 0 for others. However, types of dependency relations are diversified and the corresponding words of each relation may have different part-of-speech(POS) tags. These syntactic information should also influence the message passing process of GCN.",
        "novelty": "We propose a framework to compositionally exploit the pre-trained language models(BERT) and WGCN for SA and ABSA.",
        "performance": "Our BERT-WGCN outperforms most of the compared models on REST15, REST16 and TWITTER datasets, and achieves competitive results on SEM14(LAP) and SEM14(REST) datasets compared with SDGCN and BERT-ADA. Notably, our model achieves highest average accuary and F1 on SEM14(LAP) and SEM14(REST) dataset combined. The results demonstrate the effectiveness of BERT-WGCN. For the slight F1 degradation on the REST16 dataset, the reason might be that the size of REST16 datasets is relatively small. Another important observation is that all architectures that achieve the state-of-the-art results utilize pre-trained model. SDGCN-BERT initializes the word embeddings with pre-trained BERT token embeddings and uses self-attention network for training. BERT-ADA uses domain-specific dataset for model pre-training. Thus we believe that the contextualized information is essential for ABSA tasks. For ablation study, BERT-GCN(comp) and BERT(comp) get almost the same performance. We believe the main reason is that the importance of sentence structure in SA tasks is not as important as that in ABSA tasks. BERT-WGCN gets better performance mainly based on the additional feature combinations."
    }
}