{
    "article": "In this paper we explore the effect of selftraining on Hindi dependency parsing. We consider a state-of-the-art Hindi dependency parser and apply self-training by using a large raw corpus. We consider two types of raw corpus, one from same domain as of training and testing data and the other from different domain. We also do an experiment, where we add small gold-standard data to the training set. Comparing these experiments, we show the impact of adding small, but gold-standard data to training data versus large, but automatically parsed data on Hindi parser. Introduction Parsing morphologically rich free-word-order languages like Czech, Hindi, Turkish, etc., is a challenging task. Unlike English, most of the parsers for such languages have adopted the dependency grammatical framework. It is well known that for these languages, dependency framework is better suited (Shieber, 1985; Mel'\u010duk, 1988 , Bharati et al., 1995) . Due to the availability of annotated corpora in recent years, data driven dependency parsing has achieved considerable success. In spite of availability of annotated treebanks, state-of-the-art parsers for these languages have not reached the performance obtained for English (Nivre et al., 2007a) . Frequently stated reasons for low performance are small treebank size, complex linguistic phenomenon, long distance dependencies, and nonprojective structures (Nivre et al., 2007a; Nivre et al., 2007b; Bharati et al., 2008) . In this paper, we try to address the problem of small treebank size. We have lots of unannotated data. One way to increase treebanks' size is to manually annotate this data. But it is very time consuming task. Other way is to automatically parse this data and consider highly reliable parses. But, what criteria should be used for extracting reliable parses is a really challenging task. In this paper, we explore a bootstrapping technique called self training and see its impact on dependency parsing accuracy. We consider a state-of-the-art Hindi dependency parser and analyze its performance using selftraining. We consider two types of raw corpus, one from the same domain as of training and testing data and the other from a different domain. We also show the impact of adding small, but gold-standard data to training data versus large, but automatically parsed data on Hindi dependency parsing. The paper is arranged as follows. In section 2, we describe the related work in this field. In section 3, we present the current state-of-the-art of Hindi dependency parser. Section 3, talks about different experiments conducted and presents the results. We conclude with possible future work in section 4. Related Work In this section, we briefly describe major works on bootstrapping using statistical parsers. Steedman et al. (2003) did experiments to show that raw data can be used to improve the performance of statistical parsers by bootstrapping. Although their main focus was on cotraining between two statistical parsers, they have also performed self-training for each parser but the results are not that promising with selftraining. They have also done cross-genre experiments to show that co-training is beneficial even when the seed set was from a different domain compared to the raw data. Reichart and Rappoport (2007) also perform similar cross-genre experiments to improve the quality of their parser and to adapt the parser to a different domain. They have also reported significant reduction in annotation cost and amount of work because only small amount of manually annotated seed data was used. McClosky et al. (2006) used a two phase parser-reranker system for self-training using readily available raw data. In their approach, instead of adding the raw data in steps, they have added the entire data in one go. They have reported significant improvement in accuracy over the previous state-of-the-art accuracy for Wall Street Journal parsing. They have also performed sentence length analysis to show that there is a general improvement in intermediate-length sentences, but no improvement at the extremes. All the above mentioned works are on phrase structure parsing of English. There is an attempt at exploring usefulness of large raw corpus for dependency parsing by Chen et al. (2008) . They could achieve considerable improvement over baseline for Chinese using only high confident edges instead of entire sentences. In our work the focus is dependency parsing of Hindi. We also explore how domain and quality of data affects the parser performance. Hindi Dependency Parsing In ICON 2009 and 2010, two tools contests were held that focused on Indian Language dependency parsing (Husain, 2009; Husain et al., 2010) . In these contests, rule-based, constraint based, statistical and hybrid approaches were explored towards building dependency parsers for Hindi. In 2009 contest, given the gold standard chunk heads, the task was to find dependencies between them. But in 2010 contest, given words with gold features like part-of-speech (POS) and morph information, the task was to find word level dependency parse. Baseline (State-of-the-art) System We consider the best system (Kosaraju et al., 2010) Experiments and Analysis We have modified the Malt parser used in baseline system so that it gives a confidence value for each arc-decision taken. We have taken the average confidence value for all the arcs in the sentence to be the confidence value of a sentence. This system was first trained on ICON 2010 tools contest training data for Hindi. The model generated was then used to parse the large raw corpus. The output sentences were then sorted in descending order based on their scores. In the self-training experiments, in each iteration, we have added 1000 sentences from the sorted output generated above, to the training data and re-trained the parser. The resulting model was then used to parse the test data. Hindi data released in ICON-2010 tools contest is a portion of large treebank (Bhatt et al., 2009) , which is under development. This is a news corpus taken from well-known Hindi news daily. Self-training experiments were performed using two types of data: one from the same news domain (in-domain) and another from a different domain (out-of-domain). Self training: In-domain We have taken raw news corpus of about 108,000 sentences. As a first step, we have cleaned the data. In this process, we removed the repeated sentences, and very large sentences (>100 words per sentence). Using the above mentioned approach of self-training, we added top 1000 sentences one by one to the training data. Performance of the resulting system on test data for the first 50 iterations is shown in Figures 1a and 1b . After 50 iterations, there was steady drop in the accuracy of the system. This could be because of less confidence values of the sentences after 50th iteration. As the confidence values are low, major arcs in these sentences might be wrong. As a result, these sentences were giving negative impact on the parser performance. There were slight fluctuations in the initial iteration and peeked at 23rd iteration. At this iteration, accuracy of 78.6% LAS and 86.9% UAS was achieved. With this data, we could achieve 0.7% and 0.4% improvement in LAS and UAS respectively over the baseline. Self training: Out-of-domain In this experiment, raw data of a domain different from the actual training, and testing data was used for self-training. For this purpose, we have taken raw non-news corpus of about 700,000 sentences. Major part of this data is from tourism domain. Similar to in-domain data, we first cleaned the data. Apart from repeated, and very large, there were a few non-Hindi sentences. We also removed them during the process of cleaning. Using the above mentioned approach of selftraining (see section 4), we added top 1000 sentences one by one to the training data. Performance of the resulting system on test data for the first 50 iterations is shown in Figures 2a and 2b . After 50 iterations, there was sharp drop in the accuracy of the system. There isn't any improvement in LAS over the baseline. But in case of UAS after initial fluctuations, accuracy peeked at 17th iteration. At this iteration, accuracy of 77.8% LAS and 86.8% UAS was observed. We could achieve an improvement of 0.3% in UAS, but a decrement 0.1% in LAS. Figure 3a . Gold-standard data (LAS) Figure 3b . Gold-standard data (UAS) Gold-Standard Data In the previous two experiments (sections 4.1 and 4.2), we have taken large amount of raw data from same and different domains and applied self-training technique. In both these experiments, impact of large automatically annotated data was observed. In this experiment, we shall observe the impact of small but, gold-standard data. We have taken the development data of ICON 2010 tools contest. We have divided the data into sets of 50 sentences and added one by one similar to above experiments. We could achieve the accuracy of 79.2% LAS and 87.2% UAS at the final iteration. With this gold standard data, we could achieve an improvement of 0.7% in UAS and 1.3% in LAS. We could achieve significant improvement in the accuracy when the raw data is from the same domain. But, when the data is from different domain, we haven't seen any significant increase in the performance. This clearly shows the importance of domain of the training data. One can get better accuracies when training data is similar to testing data. As expected, adding gold-standard data outperformed both the self-training experiments. Our experiments show that gold-standard data is the best solution for improving the parser performance. When this is not possible, raw data from same domain seems to be a better option. Analysis Interesting observation is that even when gold data is being added, there isn't steady increase. Slight drop was observed when some sentences are added. This clearly shows that nature of the sentences being added to training data is very important. Currently, criterion being used to extract reliable sentences from automatically parsed ones is average confidence score given by the parser. We are considering all the nodes in the sentences for calculating confidence score of sentence. It was shown by Ambati et al. (2010) that accuracy for intra-chunk dependencies is pretty high and that of inter-chunk dependencies is low. We can explore considering sentences with high confidence scores for inter-chunk nodes, rather than average score considering all the nodes. We can also explore considering only high confidence nodes rather than entire sentence, similar to works of Chen et al. (2008) and Mannem and Dara (2011) . Conclusions and Future Work We explored the effect of self-training on Hindi dependency parsing. We showed the impact of adding small, but gold-standard data to training data versus large, but automatically parsed data on Hindi dependency parsing. We are planning to explore the importance of co-training technique also using another parser like MSTParser, as the parser can learn new information in case of co-training. We did experiments on Hindi. There are several other languages like Telugu, Bangla etc. for which annotated data is less but large amount of raw corpus is available. We are also planning to explore the importance of self-training and co-training techniques for parsing these languages.",
    "abstract": "In this paper we explore the effect of selftraining on Hindi dependency parsing. We consider a state-of-the-art Hindi dependency parser and apply self-training by using a large raw corpus. We consider two types of raw corpus, one from same domain as of training and testing data and the other from different domain. We also do an experiment, where we add small gold-standard data to the training set. Comparing these experiments, we show the impact of adding small, but gold-standard data to training data versus large, but automatically parsed data on Hindi parser.",
    "countries": [
        "India"
    ],
    "languages": [
        "Hindi"
    ],
    "numcitedby": "8",
    "year": "2011",
    "month": "November",
    "title": "Exploring self training for {H}indi dependency parsing"
}