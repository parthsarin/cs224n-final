{
    "article": "Finite-state approaches have been highly successful at describing the morphological processes of many languages. Such approaches have largely focused on modeling the phone-or character-level processes that generate candidate lexical types, rather than tokens in context. For the full analysis of words in context, disambiguation is also required (Hakkani-T\u00fcr et al., 2000; Haji\u010d et al., 2001) . In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages. The channel model exploits an existing morphological dictionary, constraining each word's analysis to be linguistically valid. The source model is a factored, conditionally-estimated random field (Lafferty et al., 2001 ) that learns to disambiguate the full sentence by modeling local contexts. Compared with baseline state-of-the-art methods, our method achieves statistically significant error rate reductions on Korean, Arabic, and Czech, for various training set sizes and accuracy measures. * This work was supported by a Fannie and John Hertz Foundation Fellowship, a NSF Fellowship, and a NDSEG Fellowship (sponsored by ARO and DOD). The views expressed are not necessarily endorsed by sponsors. We thank Eric Goldlust and Markus Dreyer for Dyna language support and Jason Eisner, David Yarowsky, and three anonymous reviewers for comments that improved the paper. We also thank Jan Haji\u010d and Pavel Krbec for sharing their Czech tagger. Introduction One of the great successes in computational linguistics has been the construction of morphological analyzers for diverse languages. Such tools take in words and enumerate the possible morphological analyses-typically a sequence of morphemes, perhaps part-of-speech tagged. They are often encoded as finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) . What such tools do not provide is a means to disambiguate a word in context. For languages with complex morphological systems (inflective, agglutinative, and polysynthetic languages, for example), a word form may have many analyses. To pick the right one, we must consider the word's context. This problem has been tackled using statistical sequence models for Turkish (Hakkani-T\u00fcr et al., 2000) and Czech (Haji\u010d et al., 2001) ; their approaches (and ours) are not unlike POS tagging, albeit with complex tags. In this paper, we describe context-based models for morphological disambiguation that take full account of existing morphological dictionaries by estimating conditionally against only dictionary-accepted analyses of a sentence ( \u00a72). These models are an instance of conditional random fields (CRFs; Lafferty et al., 2001) and include overlapping features. Our applications include diverse disambiguation frameworks and we make use of linguistically-inspired features, such as local lemma dependencies and inflectional agreement. We apply our model to Korean and Arabic, demonstrating state-of-theart results in both cases ( \u00a73). We then describe how our model can be expanded to complex, structured morphological tagging, including an efficient estimation method, demonstrating performance on Czech ( \u00a74). Modeling Framework Our framework is a source-channel model (Jelinek, 1976) . The source (modeled probabilistically by p s ) generates a sequence of unambiguous tagged morphemes y = y 1 , y 2 , ... \u2208 Y + (Y is the set of unambiguous tagged morphemes in the language). 1 The precise contents of the tag will vary by language and corpus but will minimally include POS. y passes through a channel (modeled by p c ), which outputs x = x 1 , x 2 , ... \u2208 (X \u222a {OOV}) + , a sequence of surface-level words in the language and out-of-vocabulary words (OOV; X is the language's vocabulary). Note that |x| may be smaller than |y|, since some morphemes may combine to make a word. We will denote by y i the contiguous subsequence of y that generates x i ; y will refer to a dictionaryrecognized type in Y + . At test time, we decode the observed x into the most probable sequence of tag/morpheme pairs: \u0177 = argmax y p(y | x) = argmax y p s (y) \u2022 p c (x | y) (1) Training involves constructing p s and p c . We assume that there exists a training corpus of text (each word x i annotated with its correct analysis y * i ) and a morphological dictionary. We next describe the channel model and the source model. Morphological dictionaries and the channel A great deal of research has gone into developing morphological analysis tools that enumerate valid analyses y \u2208 Y + for a particular word x \u2208 X. Typically these tools are unweighted and therefore do not enable token disambiguation. 2 They are available for many languages. We will refer to this source of categorial lexical information as a morphological dictionary d that maps X \u2192 2 Y + . The set d(x) is the set of analyses for word x; the set d(x) is the set of whole-sentence analyses for sentence x = x 1 , x 2 , ... . d(x) can be represented as an acyclic lattice with a \"sausage\" shape familiar from work in speech recognition (Mangu et al., 1999) . Note that for languages with bound morphemes, d(x) will consist of a set of sequences of tokens, so a given \"link\" in the sausage lattice may contain paths of different lengths. Fig. 1 shows sausage lattices for sentences in three languages. In this paper, the dictionary defines the support set of the channel model. That is, p c (x | y) > 0 if and only if y \u2208 d(x). This is a clean way to incorporate domain knowledge into the probabilistic model; this kind of constraint has been applied in previous work at decoding time (Hakkani-T\u00fcr et al., 2000; Haji\u010d et al., 2001) . In such a model, each word is independent of its neighbors (because the dictionary ignores context). Estimation. A unigram channel model defines 2 Probabilistic modeling of what we call the morphological channel was first carried out by Levinger et al. (1995) , who used unlabeled data to estimate p( y | x) for Hebrew, with the support defined by a dictionary. p c (x | y) def = |x| i=1 p(x i | y i ) (2) The simplest estimate of this model is to make p(\u2022, \u2022) uniform over (x, y) such that y \u2208 d(x). Doing so and marginalizing to get p(x | y) makes the channel model encode categorial information only, leaving all learning to the source model. 3  Another way to estimate this model is, of course, from data. This is troublesome, because-modulo optionality-x is expected to be known given y, resulting in a huge model with mostly 1-valued probabilities. Our solution is to take a projection \u03c0 of y and let p(\u2022 | y) \u2248 p(\u2022 | \u03c0( y)). In this paper, \u03c0 maps the analysis to its morphological tag (or tag sequence). We will refer to this as the \"tag channel.\" OOV. Morphological dictionaries typically do not have complete coverage of a language. We can augment them in two ways using the training data. If a known word x (one for which d(x) is non-empty) appears in the training dataset with an analysis not in d(x), we add the entry to the dictionary. Unknown words (those not recognized by the dictionary) are replaced by an OOV symbol. d(OOV) is taken to be the set of all analyses for any OOV word seen in training. Rather than attempt to recover the morpheme sequence for an OOV word, in this paper we try only for the tag sequence, replacing all of an OOV's morphemes with the OOV symbol. Since OOV symbols account for less than 2% of words in our corpora, we leave more sophisticated channel models to future work. The source model The source model p s defines a probability distribution over Y + , sequences of (tag, morpheme) pairs. Our source models can be viewed as weighted multi-tape finite-state automata, where the weights are associated with local, often overlapping features of the path through the automaton. Estimation. We estimate the source conditionally from annotated data. That is, we maximize (x,y)\u2208X + \u00d7Y + p(x, y) log p s y | d(x), \u03b8 (3) where p(\u2022, \u2022) is the empirical distribution defined by the training data and \u03b8 are the model parameters. In terms of Fig. 1 , our learner maximizes the weight of the correct (solid) path through each lattice, at the expense of the other incorrect (dashed) paths. Note that log p s y | d(x), \u03b8 = log p s y | \u03b8 y \u2208d(x) p s y | \u03b8 (4) The sum in the denominator is computed using a dynamic programming algorithm (akin to the forward algorithm); it involves computing the sum of all paths through the \"sausage\" lattice of possible analyses for x. By doing this, we allow knowledge of the support of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001) . When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003) , or word segmentation (Peng et al., 2004) , one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram \"O I\" is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries. 4  Our approach is the same, with two modifications. First, we model the relationship between labels y i and words x i in a separately-estimated channel model ( \u00a72.1). Second, our labels are complex. Each word x i is tagged with a sequence of one or more tagged morphemes; the tags may include multiple fields. This leads to models with more parameters. It also makes the dictionary especially important for limiting the size of the sum in the denominator, since a complex label set Y could in principle lead to a huge hypothesis space for a given sentence x. Importantly, it makes training conditions more closely match testing conditions, ruling out hypotheses a dictionary-aware decoder would never consider. Optimization. The objective function (Eq. 3) is concave and known to have a unique global maximum. Because log-linear models and CRFs have been widely described elsewhere (e.g., Lafferty, 2001) , we note only that we apply a standard first-order numerical optimization method (L-BFGS; Liu and Nocedal, 1989) . The structure, features, and regularization of our models will be described in \u00a73 and \u00a74. Prior work (morphological source models). Hakkani-T\u00fcr et al. (2000) described a system for Turkish that was essentially a source model; Haji\u010d et al. (2001) described an HMM-based system for Czech that could be viewed as a combined source and channel. Both used dictionaries and estimated their (generative) models using maximum likelihood (with smoothing). 5 Given enough data, a ML-estimated model will learn to recognize a good path y, but it may not learn to discriminate a good y from wrong alternatives per se. The generative framework is limiting as well, disallowing the straightforward inclusion of arbitrary overlapping features. We present a competitive Czech model in \u00a74. Concatenative Models The beauty of log-linear models is that estimation is straightforward given any features, even ones that are not orthogonal (i.e., \"overlap\"). This permits focusing on feature (or feature template) selection without worries about the mathematics of training. We consider two languages modeled by concatenative processes with surface changes at morpheme boundaries: Korean and Arabic. Our model includes features for tag n-grams, morpheme n-grams, and pairs of the two (possibly of different lengths and offsets). Fig. 2 illustrates TM3, our base model. TM3 includes feature templates for some tuples of three or fewer elements, plus begin and end templates. A variant, TM3H, includes all of the same templates, plus a similar set of templates that look only at head morphemes. For instance, a feature fires for each trigram of heads, even though there are (bound) morphemes between them. This increases the domain of locality for semantic content-bearing morphemes. This model requires slight changes to the dynamic programming algorithms for inference and training (the previous two heads must be remembered at each state). i\u22121 i\u22121 T M T n M n \u22122 i T \u22122 i M M 1 1 T T i M i Every instantiation of the templates seen in any lattice d(x) built from training data is included in the model, not just those seen in correct analyses y * . 6 Experimental design In all of our experiments, we vary the training set size and the amount of smoothing, which is enforced by a diagonal Gaussian prior (L 2 regularizer) with variance \u03c3 2 . The \u03c3 2 = \u221e case is equivalent to not smoothing. We compare performance to the expected performance of a randomized baseline that picks for each word token x an analysis from d(x); this gives a measure of the amount of ambiguity and is denoted \"channel only.\" Performance of unigram, bigram, and trigram HMMs estimated using maximum likelihood (barely smoothed, using add-10 \u221214 ) is also reported. (The unigram HMM simply picks the most likely y for each x, based on training data and is so marked.) In the experiments in this section, we report three performance measures. Tagging accuracy is the fraction of words whose tag sequence was correctly identified in entirety; morpheme accuracy is defined analogously. Lemma accuracy is the fraction of words whose lemma was correctly identified. Korean experiments We applied TM3 and TM3H to Korean. The dataset is the Korean Treebank (Han et al., 2002) , with up to 90% used for training and 10% (5K words) for test. The morphological dictionary is klex (Han, 2004) . There are 27 POS tags in the tag set; the corpus contains 10K word types and 3,272 morpheme types. There are 1.7 morphemes per word token on average (\u03c3 = 0.75). A Korean word generally consists of a head morpheme with a series of enclitic suffixes. In training the head-augmented model TM3H, we assume the first morpheme of every word is the head and lemma. Results are shown in Tab. 1. TM3H achieved very slight gains over TM3, and the tag channel model was helpful only with the smaller training set. The oracle (last line of Tab. 1) demonstrates that the coverage of the dictionary remains an obstacle, particularly for recovering morphemes. Another limitation is the small amount of training data, which may be masking differences among estimation conditions. We report the performance of TM3H with \"factored\" estimation. This will be discussed in detail in \u00a74; it means that a model containing only the head features was trained on its own, then combined with the independently trained TM3 model at test time. Factored training was slightly faster and did not affect performance at all; accuracy scores were identical with unfactored training. Prior work (Korean). Similar results were presented by Smith and Smith (2004) , using a similar estimation strategy with a model that included far more feature templates. TM3 has about a third as many parameters and TM3H about half; performance is roughly the same (numbers omitted for space). Korean disambiguation results were also reported by Cha et al. (1998) , who applied a deterministic morpheme pattern dictionary to segment words, then used a bigram HMM tagger. They also applied transformation-based learning to fix common errors. Due to differences in tag set and data, we cannot compare to that model; a bigram baseline is included. Arabic experiments We applied TM3 and TM3H to Arabic. The dataset is the Arabic Treebank (Maamouri et al., 2003) , with up to 90% used for training and 10% (13K words) for test. The morphological dictionary is Buckwalter's analyzer (version 2), made available by the LDC (Buckwalter, 2004 OOV words. There are 139 distinct POS tags; these contain some inflectional information which we treat atomically. For speed, TM3H was trained in two separate pieces: TM3 and the lemma features added by TM3H. Arabic has a templatic morphology in which consonantal roots are transformed into surface words by the insertion of vowels and ancillary consonants. Our system does not model this process except through the use of Buckwalter's dictionary to define the set of analyses for each word (cf., Daya et al., 2004, who modeled interdigitation in Hebrew) . We treat the analysis of an Arabic word as a sequence y of pairs of morphemes and POS tags, plus a lemma. The lemma, given in the dictionary, provides further disambiguation beyond the head morpheme. The lemma is a standalone dictionary headword and not merely the consonantal root, as in some other work. The \"heads\" modeled by TM3H correspond to these lemmas. There are 20K word types, and 34K morpheme types. There are 1.7 morphemes per word token on average (\u03c3 = 0.77). Results are shown in Tab. 1. Across tasks and training set sizes, our models reduce error rates by more than 36% compared to the trigram HMM source with tag channel. The TM3H model and the tag channel offer slight gains over the base TM3 model (especially on lemmatization), though the tag channel offers no help in POS tagging. Prior work (Arabic). Both Diab et al. (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. Diab et al. report results for a coarsened 24-tag set, while we use the full 139 tags from the Arabic Treebank, so the systems are not directly comparable. Habash and Rambow present even better results on the same POS tag set. Our full disambiguation results appear to be competitive with theirs. Khoja (2001) and Freeman (2001) describe Arabic POS taggers and many of the issues involved in developing them, but because tagged corpora did not yet exist, there are no comparable quantitative results. Czech: Model and Experiments Inflective languages like Czech present a new set of challenges. Our treatment of Czech is not concatenative; following prior work, the analysis for each word x is a single tag/lemma pair y. Inflectional affixes in the surface form are represented as features in the tag. While lemmatization of Czech is not hard (there is little ambiguity), tagging is quite difficult, because morphological tags are highly complex. Our tag set is the Prague Dependency Treebank (PDT; Haji\u010d, 1998) set, which consists of fifteen-field tags that indicate POS as well as inflectional information (case, number, gender, etc.) . There are over 1,400 distinct tag types in the PDT. Czech has been treated probabilistically before, perhaps most successfully by Haji\u010d et al. (2001) . 8 In contrast, we estimate conditionally (rather than by maximum likelihood for a generative HMM) and separate the training of the source and the channel. We also introduce a novel factored treatment of the morphological tags. Factored tags and estimation Because Czech morphological tags are not monolithic, the choice among them can be treated as several more or less orthogonal decisions. The case feature of one word, for example, is expected to be conditionally independent of the next word's gender, given the next word's case. Constraints in the language are expected to cause features like case, number, and gender to agree locally (on words that have such features) and somewhat independently of each other. Coarser POS tagging may be treated as another, roughly independent stream. Log-linear models and the use of a morphological dictionary make this kind of tag factoring possible. Our approach is to separately train five log-linear models. Each model is itself an instance of some of the templates from TM3, modeling a projection of the full analysis. The model and its factored components are illustrated in Fig. 3 . POS model. The full tag is replaced by the POS tag (the first two fields); there are 60 POS tags. The TM3 8 Czech morphological processing was studied by Petkevi\u010d (2001) , Hlav\u00e1cov\u00e1 (2001) (who focuses on handling OOV words), and Mr\u00e1kov\u00e1 and Sedlacek (2003) (who use partial parsing to reduce the set of possible analyses), inter alia. feature templates are included twice: once for the full tag and once for a coarser tag (the first PDT field, for which there are 12 possible values). 9 Gender, number, and case models. The full tag is replaced by the gender (or case or number) field. This model includes bigrams and trigrams as well as fieldmorpheme unigram features. These models are intended to learn to predict local agreement. Tag-lemma model. This model contains unigram features of full PDT tags, both alone and with lemmas. It is intended to learn to penalize morphological tags that are rare, or that are rare with a particular lemma. In our formulation, this is not a channel model, because it ignores the surface word forms. Each model is estimated independently of the others. The lattice d(x) against which the conditional probabilities are estimated contains the relevant projection of the full morphological tags (with lemmas). To decode, we run a Viterbi-like algorithm that uses the union of all models' features to pick the best analysis (full morphological tags and lemmas) allowed by the dictionary. There are two important advantages of factored training. First, each model is faster to train alone than a model with all features merged; in fact, training the fully merged model takes far too long to be practical. Second, factored models can be held out at test time to measure their effect on the system, without retraining. Prior work (factored training). Separately training different models that predict the same variables (e.g., x and y) then combining them for consensus-based inference (either through a mixture or a product of probabilities) is an old idea (Genest and Zidek, 1986) . Recent work in learning weights for the component \"expert\" models has turned to cooperative techniques (Hinton, 1999) . Decoding that finds y (given x) to maximize some weighted average of log-probabilities is known as a logarithmic opinion pool (LOP). LOPs were applied to CRFs (for named entity recognition and tagging) by Smith et al. (2005) , with an eye toward regularization. Their experts (each a CRF) contained overlapping feature sets, and the combined model achieved much the same effect as training a single model with smoothing. Note that our models, unlike theirs, partition the feature space; there is only one CRF, but some parameters are ignored when estimating other parameters. We have not estimated log-domain mixing coefficients-we weight all models' contributions equally. Sutton and McCallum (2005) maximizes a lower bound on the unfactored objective. Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. Experiments Our corpus is the PDT (Haji\u010d, 1998) , with up to 60% used for training and 10% (109K words) used for test. 10 The morphological dictionary is the one packaged with the PDT; it covers about 98% of the tokens in the corpus. The remaining 2% have (unsurprisingly) a diverse set of 300-400 distinct tags, depending on the training set size. 11  Results are shown in Tab. 2. We compare to the HMM of (Haji\u010d et al., 2001) without its OOV component. 12 We report morphological tagging accuracy on words; we also report lemma accuracy (on non-OOV words), POS accu-10 We used less than the full corpus to keep training time down; note that the training sets are nonetheless substantially larger than in the Korean and Arabic experiments. 11 During training, these project down to manageable numbers of hypotheses in the factored models. At test-time, however, Viterbi search is quite difficult when OOV symbols occur consecutively. To handle this, we prune OOV arcs from the lattices using the factored POS and inflectional models. For each OOV, every model prunes a projection of the analysis (e.g., the POS model prunes POS tags) until 90% of the posterior mass or 3 arcs remain (whichever is more conservative). Viterbi decoding is run on a lattice containing OOV arcs consistent with the pruned projected lattices. 12 Results with the OOV component are also reported in Tab. 2, but we cannot guarantee their experimental validity, since the OOV component is pre-trained and may have been trained on data in our test set. racy on all words, and POS accuracy on OOV words. The channel model (not shown) tended to have a small, harmful effect on performance. Without any explicit OOV treatment, our POS-only component model significantly reduces lemma and POS errors compared to Haji\u010d et al.'s model. On recovering full morphological tags, our full model is close in performance to Haji\u010d et al., but still significantly worse. It is likely that for many tasks, these performance gains are more helpful than the loss on full tagging is harmful. Why doesn't our full model perform as well as Haji\u010d et al.'s model? An error analysis reveals that our full model (768K, \u03c3 2 = 1), compared to the HMM (768K) had 91% as many number errors but 0.1% more gender and 31% more case errors. Taking out those three models (\"POS & tag-lemma\" in Fig. 2 ) is helpful on all measures except full tagging accuracy, due in part to substantially increased errors on gender (87% increase), case (54%), and number (35%). The net effect of these components, then, is helpful, but not quite helpful enough to match a well-smoothed HMM on complex tagging. We compared the models on the training set and found the same pattern, demonstrating that this is not merely a matter of over-fitting. Future Work Two clear ways to improve our models present themselves. The first is better OOV handling, perhaps through an improved channel model. Possibilities include learning weights to go inside the FST-encoded dictionaries and directly modeling spelling changes. The second is to turn our factored model into a LOP. Training the mixture coefficients should be straightforward (if time-consuming) with a development dataset. A drawback of our system (especially for Czech) is that some components (most notably, the Czech POS model) take a great deal of time to train (up to two weeks on 2GHz Pentium systems). Speed improvements are expected to come from eliminating some of the overlapping feature templates, generalized speedups for loglinear training, and perhaps further factoring. Conclusion We have explored morphological disambiguation of diverse languages using log-linear sequence models. Our approach reduces error rates significantly on POS tagging (Arabic and Czech), morpheme sequence recovery (Korean and Arabic), and lemmatization (all three languages), compared to baseline state-of-the-art methods For complex analysis tasks (e.g., Czech tagging), we have demonstrated that factoring a large model into smaller components can simplify training and achieve excellent results. We conclude that a conditionally-estimated source model informed by an existing morphological dictionary (serving as an unweighted channel) is an effective approach to morphological disambiguation.",
    "abstract": "Finite-state approaches have been highly successful at describing the morphological processes of many languages. Such approaches have largely focused on modeling the phone-or character-level processes that generate candidate lexical types, rather than tokens in context. For the full analysis of words in context, disambiguation is also required (Hakkani-T\u00fcr et al., 2000; Haji\u010d et al., 2001) . In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages. The channel model exploits an existing morphological dictionary, constraining each word's analysis to be linguistically valid. The source model is a factored, conditionally-estimated random field (Lafferty et al., 2001 ) that learns to disambiguate the full sentence by modeling local contexts. Compared with baseline state-of-the-art methods, our method achieves statistically significant error rate reductions on Korean, Arabic, and Czech, for various training set sizes and accuracy measures. * This work was supported by a Fannie and John Hertz Foundation Fellowship, a NSF Fellowship, and a NDSEG Fellowship (sponsored by ARO and DOD). The views expressed are not necessarily endorsed by sponsors. We thank Eric Goldlust and Markus Dreyer for Dyna language support and Jason Eisner, David Yarowsky, and three anonymous reviewers for comments that improved the paper. We also thank Jan Haji\u010d and Pavel Krbec for sharing their Czech tagger.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Korean",
        "Arabic"
    ],
    "numcitedby": "70",
    "year": "2005",
    "month": "October",
    "title": "Context-Based Morphological Disambiguation with Random Fields"
}