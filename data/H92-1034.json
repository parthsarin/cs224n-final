{
    "article": "How to capture important acoustic clues and estimate essential parameters reliably is one of the central issues in speech recognition, since we will never have sufficient training data to model various acoustic-phonetic phenomena. Successful examples include subword models with many smoothing techniques. In comparison with subword models, subphonetic modeling may provide a finer level of details. We propose to model subphonetic events with Markov states and treat the state in phonetic hidden Markov models as our basic subphonetic unit -senone. A word model is a concatenation of state-dependent senones and senones can be shared across different word models. Senones not only allow parameter sharing, but also enable pronunciation optimization and new word learning, where the phonetic baseform is replaced by the senonic baseform. In this paper, we report preliminary subphonetic modeling results, which not only significantly reduced the word error rate for speaker-independent continuous speech recognition but also demonstrated a novel application for new word learning. INTRODUCTION For large-vocabulary speech recognition, we will never have sufficient training data to model all the various acousticphonetic phenomena. How to capture important acoustic clues and estimate essential parameters reliably is one of the central issues in speech recognition. To share parameters among different word modes, context-dependent subword models have been used successfully in many state-of-the-art speech recognition systems [1, 2, 3, 4] . The principle of parameter sharing can also be extended to subphonetic models. For subphonetic modeling, fenones [5, 6] have been used as the front end output of the IBM acoustic processor. To generate a fenonic pronunciation, multiple examples of each word are obtained. The fenonic baseform is built by searching for a sequence of fenones which has the maximum probability of generating all the given multiple utterances. The codeworddependent fenonic models are then trained just like phonetic models. We believe that the 200 codeword-dependent fenones may be insufficient for large-vocabulary continuous speech recognition. In this paper, we propose to model subphonetic events with Markov states. We will treat the state in hidden Markov models (HMMs) as a basic subphonetic unit -senone. The total number of HMM states in a system is often too large to be well trained. To reduce the number of free parameters, we can cluster the state-dependent output distributions. Each clustered output distribution is denoted as a senone. In this way, senones can be shared across different models as illustrated in Figure 1 . The advantages of senones include better parameter sharing and improved pronunciation optimization. After clustering, different states in different models may share the same senone if they exhibit acoustic similarity. Clustefingat the granularity of the state rather than the entire model (like generalized tripbones) can keep the dissimilar states of two similar models apart while the other corresponding states are merged, and thus lead to better parameter sharing. For instance, the first, or the second states of the/ey/phones in PLACE and RELATION may be tied together. However, to magnify the acoustic effects of the fight contexts, their last states may be kept separately. In addition to finer parameter sharing, senones also give us the freedom to use a larger number of states for each phonetic model. Although an increase in the number of states will increase the total number of free parameters, with senone sharing we can essentially eliminate those redundant states and have the luxury of maintaining the necessary ones. Since senones depend on Markov states, the senonic baseform of a word can be constructed naturally with the forwardbackward algorithm [7] . Regarding pronunciation optimization as well as new word learning, we can use the forwardbackward algorithm to iteratively optimize a senone sequence appropriate for modeling multiple utterances of a word. That is, given the multiple examples, we can train a word HMM with the forward-backward algorithm. When the reestimation reaches its optimality, the estimated states can be quantized with the codebook of senones. The closest one can be used to label the state of the word HMM. This sequence of senones becomes the senonic baseform of the word. Here arbitrary sequences of senones are allowed to provide the freedom for the automatically learned pronunciation. After the senonic baseform of every word is determined, the senonic word models may be trained, resulting in a new set of senones. Although each senonic word model generally has more states than the traditional phoneme-concatenated word model, the number of parameters remains the same since the size of the senone codebook is intact. In dictation applications, new words will often appear during user's usage. A natural extension for pronunciation optimization is to generate senonic baseforms for new words. Automatic determination of phonetic baseforms has been considered by [8] , where four utterartces and spelling-to-soundrules are need. For the senonic baseform, we can derive senonic baseform only using acoustic data without any spelling information. This is useful for acronym words like IEEE (pronounced as 1-triple-E), CAT-2 (pronounced as cat-two) and foreign person names, where speUing-to-soundrules are hard to generalize. The acoustic-driven senonic baseform can also capture pronunciation of each individual speaker, since in dictation applications, multiple new-word samples are often from the same speaker. By constructing senone codebook and using senones in the triphone system, we were able to reduce the word error rate of the speaker-independent Resource Management task by 20% in comparison with the generalized triphone [2] . When senones were used for pronunciation optimization, our preliminary results gave us another 15% error reduction in a speaker-independent continuous spelling task. The word error rate was reduced from 11.3% to 9.6%. For new word learning, we used 4 utterances for each new word. Our preliminary results indicate that the error rate of automatically generated senonic baseform is comparable to that of handwritten phonetic baseform. SHARED DISTRIBUTION MODELS In phone-based HMM systems, each phonetic model is formed by a sequence of states. Phonetic models are shared across different word models. In fact, the state can also be shared across different phonetic models. This section will describe the usage of senones for parameter sharing. Senone Construction by State Clustering The number of triphones in a large vocabulary system is generally very large. With limited training data, there is no hope to obtain well-trained models. Therefore, different technologies have been studied to reduce the number of parameters [1, 9, 2, 10, 11] . In generalized triphones, every state of a triphone is merged with the corresponding state of another triphone in the same cluster. It may be true that some states are merged not because they are similar, but because the other states of the involved models resemble each other. To fulfill more accurate modeling, states with differently-shaped output distributions should be kept apart, even though the other states of the models are tied. Therefore, clustering should be carried out at the output-distribution level rather than the model level. The distribution clustering thus creates a senone codebook as Figure 2 shows [12] . The clustered distributions or senones are fed back to instantiatephonetic models. Thus, states of different phonetic models may share the same senone. This is the same as theshared-distributionmodel (SDM) [13] . Moreover, different states within the same model may also be tied together if too many states are used to model this phone's acoustic variations or ifa certain acoustic event appears repetitively within the phone.  Initially, every output distribution of all HMMs is created as a cluster. Find the most similar pair of clusters and merge them together. For each element in each cluster of the current configuration, move it to another cluster if that results in improvement. Repeat this shifting until no improvement can be made. Go to step 3 unless some convergence criterion is met. Figure 2 : The construction of senones (clustered output distributions). Sentries also give us the freedom to use a larger number of states for each phonetic model. Although an increase in the number of states will increase the total number of free parameters, yet by clustering similar states we can essentially eliminate those redundant states and have the luxury to maintain the necessary ones [13] . Performance Evaluation We incorporated the above distribution clustering technique in the SPHINX-II system [14] and experimented on the speakerindependent DARPA Resource Management (RM) task with a word-pair grammar of perplexity 60. The test set consisted of the February 89 and October 89 test sets, totaling 600 sentences. Table 1 shows the word error rates of several systems. System Word In the SPHINX system, there were 1100 generalized triphones, each with 3 distinct output distributions. In the SPHINX-II system, we used 5-state Bakis triphone models and clustered all the output distributions in the 7500 or so triphones down to 3500-5500 senones. The system with 4500 senones had the best performance with the given 3990 training sentences. The similarity between two distributions was measured by their entropies. After two distributions are merged, the entropy-increase, weighted by counts, is computed: (ca + cb)no+b --Coaa -CbHb where Ca is the summation of the entries of distribution a in terms of counts, and Ha is the entropy. The less the entropyincrease is, the closer the two distributions are. Weighting entropies by counts enables those distributions with less occurring frequency be merged before frequent ones. This makes each senone (shared distribution) more trainable. Behavior of State Clustering To understand the quality of the senone codebook, we examined several examples in comparison with 1100 generalized triphone models. As shown in Figure 3 , the two/ey/triphones in -PLACE and --LaTION were mapped to the same generalized triphone. Similarly, phone/d/in START and ASTORIA were mapped to another generalized triphone. Both has the same left context, but different right contexts. States with the same color were tied to the same senone in the 4500-SDM system, x, y, z, and w represent different sentries.  It is also interesting to note that when 3 states, 5 states, and 7 states per triphone model are used with a senone codebook size of 4500, the average number of distinct senones a triphone used is 2.929, 4.655, and 5.574 respectively. This might imply that 5 states per phonetic model are optimal to model the acoustic variations within a triphone unit for the given DARPA RM training database. In fact, 5-state models indeed gave us the best performance. PRONUNCIATION OPTIMIZATION As shown in Figure 1 , senones can be shared not only by different phonetic models, but also by different word models. This section will describe one of the most important applications of senones: word pronunciation optimization. Senonic Baseform by State Quantization Phonetic pronunciation optimization has been considered by [15, 8] . Subphonetic modeling also has a potential application to pronunciation learning. Most speech recognition systems use a fixed phonetic transcription for each word in the vocabulary. If a word is transcribed improperly, it will be difficult for the system to recognize it. There may be quite a few improper transcriptions in a large vocabulary system for the given task. Most importantly, some words may be pronounced in several different ways such as THE (/dh ax/or/dh ih/), TOMATO (/tax m ey dx owl or/t ax m aa dx ow/), and so on. We can use multiple phonetic transcriptions for every word, or to learn the pronunciation automatically from the data. Figure 4 shows the algorithm which looks for the most appropriate senonic baseform for a given word when training examples are available. 1. Compute the average duration (number of timeframes), given multiple tokens of the word. 2. Build a Bakis word HMM with the number of states equal to a portion of the average duration (usually 0.8). 3. Run several iterations (usually 2 -3) of the forward-backward algorithm on the word model starting from uniform output distributions, using the given utterance tokens. 4. Quantize each state of the estimated word model with the senone codebook. Here arbitrary sequences of senones are allowed to provide the freedom for the automatically learned pronunciation. This senonic baseform tightly combines the model and acoustic data. After the senonic baseform of every word is determined, the senonic word models may be trained, resulting in a new set of senones. Similar to fenones, sentries take full advantage of the multiple utterances in baseform construction. In addition, both phonetic baseform and senonic baseform can be used together, without doubling the number of parameters in contrast to fenones. So we can keep using phonetic baseforrn when training examples are unavailable. The senone codebook also has a better acoustic resolution in comparison with the 200 VQ-dependent fenones. Although each senonic word model generally has more states than the traditional phonemeconcatenated word model, the number of parameters are not increased since the size of the senone codebook is fixed. Performance Evaluation As a pivotal experiment for pronunciation learning, we used the speaker-independent continuous spelling task (26 English alphabet). No grammar is used. There are 1132 training sentences from 100 speakers and 162 testing sentences from 12 new speakers. The training data were segmented into words by a set of existing HMMs and the Viterbi alignment [16, 1] . For each word, we split its training data into several groups by a DTW clustering procedure according to their acoustic resemblance. Different groups represent different acoustic realizations of the same word. For each word group, we estimated the word model and computed a senonic baseform as Figure 4 describes. The number of states of a word model was equal to 75% of the average duration. The Euclidean distance was used as the distortion measure during state quantization. We calculated the predicting ability of the senonic word model M,o,a obtained from the g-th group of word w as: log P(X,olM~,,g) / ~ IX,,I X,. egroup g X,. ~group a where X~o is an utterance of word w. For each word, we picked two models that had the best predicting abilities. The pronunciation of each word utterance in the training set was labeled by: modei(Xto) = argmax { p(X~lM~o,a)} M~o,a E top2 After the training data were labeled in this way, we retrained the system parameters by using the senonic baseform. Table 2 shows the word error rate. Both systems used the sex-dependent semi-continuous HMMs. The baseline used word-dependent phonetic models. Therefore, it was essentially a word-based system. Fifty-six word-dependent phonetic models were used. Note both systems used exactly the same number of parameters. This preliminary results indicated that the senonic baseform can capture detailed pronunciation variations for speakerindependent speech recognition. NEW WORD LEARNING In dictation applications, we can start from speakerindependent system. However, new words will often appear when users are dictating. In real applications, these new System Word Error % Error Reduction phonetic baseform 11.3% senonic baseform 9.6% 15% Table 2 : Results of the phonetic baseform vs. the senonic baseform on the spelling task. word samples are often speaker-dependent albeit speakerindependent systems may be used initially. A natural extension for pronunciation optimization is to generate speakerdependent senonic baseforms for these new words. In this study, we assume possible new words are already detected, and we want to derive the senonic baseforms of new words automatically. We are interested in using acoustic data only. This is useful for acronym words like IEEE (pronounced as ltriple-E), CAT-2 (pronounced as cat-two) and foreign person names, where spelling-to-sound rules are hard to generalize. The senonic baseform can also capture pronunciation characteristics of each individual speaker that cannot be represented in the phonetic baseform. Experimental Database and System Configuration With word-based senonic models, it is hard to incorporate between-word co-articulation modeling. Therefore, our baseline system used within-word triphone models only. Again we chose RM as the experimental task. Speaker-independent (SI) sex-dependent SDMs were used as our baseline system for this study. New word training and testing data are speakerdependent (SD). We used the four speakers (2 females, 2 males) from the June-1990 test set; each supplied 2520 SD sentences. The SD sentences were segmented into words using the Viterbi alignment. Then we chose randomly 42 words that occurred frequently in the SD database (so that we have enough testing data) as shown in Table 3 , where their frequencies in the speakerindependent training database are also included. For each speaker and each of these words, 4 utterances were used as samples to learn the senonic baseform, and at most 10 other utterances as testing. Therefore, the senonic baseform of a word is speaker-dependent. There were together 1460 testing word utterances for the four speakers. During recognition, the segmented data were tested in an isolated-speech mode without any grammar. State Quantization of the Senonic Baseform For each of the 42 words, we used 4 utterances to construct the senonic baseform. The number of states was set to be 0.8 of the average duration. To quantize states at step 4 of Figure 4 , we aligned the sample utterances against the estimated word model by the Viterbi algorithm. Thus, each state had 5 to 7 frames on average. Each state of the word model is quantized to the senone that has the maximum probability of generating all the aligned frames. Given a certain senone, senone, the probability of generating the aligned frames of state s is computed in the same manner as the semi-continuous output probability: Experimental Performance For the hand-written phonetic baseform, the word error rate was 2.67% for the 1460 word utterances. As a pilot study, a separate senonic baseform was constructed for CASREP and its derivatives (CASREPED, and CASREPS). Similarly, for the singular and plural forms of the selected nouns. The selected 42 words were modeled by automatically constructed senonic baseforrns. They are used together with the rest 955 words (phonetic baseforms) in the RM task. The word error rate was 6.23%. Most of the errors came from the derivative confusion. To reduce the derivative confusion, we concatenated the original senonic baseform with the possible suffix phonemes as the baseform for the derived words. For example, the baseform of FLEETS became/fleet <ts s ix-z>~, where the context-independent phone model/ts/,/s/, and the concatenated/ix z/were appended parallelly after the senonic base-form of FLEE T. In this way, no training data were used to learn the pronunciations of the derivatives. This suffix senonic approach significantly reduced the word error to 3.63%. Still there were a lot of misrecognitions of CASREPED tO be CASREP and MAX tO be NEXT. These were due to the high confusion between/td/and/pd/,/m/and/n/. The above results are summarized in Table 4 . system error rate hand-written phonetic baseform 2.67 % pilot senonic baseform 6.23 % suffix senonic baseform 3.63% Table 4 : Results of the senonic baseforms on the 1460 word utterances for the selected 42 words. The study reported here is preliminary. Refinement on the algorithm of senonic-baseform construction (especially incorporation of the spelling information) is still under investigation. Our goal is to approach the phonetic system. CONCLUSION In this paper, we developed the framework of senones -state-dependent subphonetic unit. Senones are created by clustering states of triphone models. Thus, we reduced the the number of system parameters with the senone codebook, which renders finer acoustic modeling and provides a way to learn the model topology. In the mean time, we can construct senonic baseforms to improve phonetic baseforms and learn new words without enlarging system parameters. Senonic baseforms are constructed by quantizing the states of estimated word models with the senone codebook. We demonstrated that senones can not only significantly improve speaker-independent continuous speech recognition but also have a novel application for new word learning. Acknowledgements This research was sponsored by the Defense Advanced Research Projects Agency (DOD), Arpa Order No. 5167, under contract number N00039-85-C-0163. The authors would like to express their gratitude to Professor Raj Reddy for his encouragement and support, and other members of CMU speech group for their help.",
    "abstract": "How to capture important acoustic clues and estimate essential parameters reliably is one of the central issues in speech recognition, since we will never have sufficient training data to model various acoustic-phonetic phenomena. Successful examples include subword models with many smoothing techniques. In comparison with subword models, subphonetic modeling may provide a finer level of details. We propose to model subphonetic events with Markov states and treat the state in phonetic hidden Markov models as our basic subphonetic unit -senone. A word model is a concatenation of state-dependent senones and senones can be shared across different word models. Senones not only allow parameter sharing, but also enable pronunciation optimization and new word learning, where the phonetic baseform is replaced by the senonic baseform. In this paper, we report preliminary subphonetic modeling results, which not only significantly reduced the word error rate for speaker-independent continuous speech recognition but also demonstrated a novel application for new word learning.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "16",
    "year": "1992",
    "month": "",
    "title": "Subphonetic Modeling for Speech Recognition"
}