{
    "article": "In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes. We show accurate results on a large collection of free-form questions used in TREC 10. Introduction Open-domain question answering (Lehnert, 1986; Harabagiu et al., 2001; Light et al., 2001) and story comprehension (Hirschman et al., 1999) have become important directions in natural language processing. Question answering is a retrieval task more challenging than common search engine tasks because its purpose is to find an accurate and concise answer to a question rather than a relevant document. The difficulty is more acute in tasks such as story comprehension in which the target text is less likely to overlap with the text in the questions. For this reason, advanced natural language techniques rather than simple key term extraction are needed. One of the important stages in this process is analyzing the question to a degree that allows determining the \"type\" of the sought after answer. In the TREC competition (Voorhees, 2000) , participants are requested to build a system which, given a set of English questions, can automatically extract answers (a short phrase) of no more than 50 bytes from a 5-gigabyte document library. Participants have re-\u00a3 Research supported by NSF grants IIS-9801638 and ITR IIS-0085836 and an ONR MURI Award. alized that locating an answer accurately hinges on first filtering out a wide range of candidates (Hovy et al., 2001; Ittycheriah et al., 2001) based on some categorization of answer types. This work develops a machine learning approach to question classification (QC) (Harabagiu et al., 2001; Hermjakob, 2001) . Our goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process. For example, when considering the question Q: What Canadian city has the largest population?, the hope is to classify this question as having answer type city, implying that only candidate answers that are cities need consideration. Based on the SNoW learning architecture, we develop a hierarchical classifier that is guided by a layered semantic hierarchy of answer types and is able to classify questions into fine-grained classes. We suggest that it is useful to consider this classification task as a multi-label classification and find that it is possible to achieve good classification results (over 90%) despite the fact that the number of different labels used is fairly large, \u00bc. We observe that local features are not sufficient to support this accuracy, and that inducing semantic features is crucial for good performance. The paper is organized as follows: Sec. 2 presents the question classification problem; Sec. 3 discusses the learning issues involved in QC and presents our learning approach; Sec. 4 describes our experimental study. Question Classification We define Question Classification(QC) here to be the task that, given a question, maps it to one of classes, which provide a semantic constraint on the sought-after answer 1 . The intension is that this classification, potentially with other constraints on the answer, will be used by a downstream process which selects a correct answer from among several candidates. A question classification module in a question answering system has two main requirements. First, it provides constraints on the answer types that allow further processing to precisely locate and verify the answer. Second, it provides information that downstream processes may use in determining answer selection strategies that may be answer type specific, rather than uniform. For example, given the question \"Who was the first woman killed in the Vietnam War?\" we do not want to test every noun phrase in a document to see whether it provides an answer. At the very least, we would like to know that the target of this question is a person, thereby reducing the space of possible answers significantly. The following examples, taken from the TREC 10 question collection, exhibit several aspects of this point. Q: What is a prism? Identifying that the target of this question is a definition, strategies that are specific for definitions (e.g., using predefined templates) may be useful. Similarly, in: Q: Why is the sun yellow? Identifying that this question asks for a reason, may lead to using a specific strategy for reasons. The above examples indicate that, given that different answer types may be searched using different strategies, a good classification module may help the question answering task. Moreover, determining the specific semantic type of the answer could also be beneficial in locating the answer and verifying it. For example, in the next two questions, knowing that the targets are a city or country will be more useful than just knowing that they are locations. Q: What Canadian city has the largest population? Q: Which country gave New York the Statue of Liberty? However, confined by the huge amount of manual work needed for constructing a classifier for a complicated taxonomy of questions, most question answering systems can only perform a coarse classification for no more than 20 classes. As a result, existing approaches, as in (Singhal et al., 2000) , have adopted a small set of simple answer entity types, which consisted of the classes: Person, Location, Organization, Date, Quantity, Duration, Linear Measure. The rules used in the classification were of the following forms: -If a query starts with Who or Whom: type Person. -If a query starts with Where: type Location. -If a query contains Which or What, the head noun phrase determines the class, as for What X questions. While the rules used have large coverage and reasonable accuracy, they are not sufficient to support fine-grained classification. One difficulty in supporting fine-grained classification is the need to extract from the questions finer features that require syntactic and semantic analysis of questions, and possibly, many of them. The approach we adopted is a multi-level learning approach: some of our features rely on finer analysis of the questions that are outcomes of learned classifiers; the QC module then applies learning with these as input features. Classification Standard Earlier works have suggested various standards of classifying questions. Wendy Lehnert's conceptual taxonomy (Lehnert, 1986) , for example, proposes about 13 conceptual classes including causal antecedent, goal orientation, enablement, causal consequent, verification, disjunctive, and so on. However, in the context of factual questions that are of interest to us here, conceptual categories do not seem to be helpful; instead, our goal is to semantically classify questions, as in earlier work on TREC (Singhal et al., 2000; Hovy et al., 2001; Harabagiu et al., 2001; Ittycheriah et al., 2001) . The key difference, though, is that we attempt to do that with a significantly finer taxonomy of answer types; the hope is that with the semantic answer types as input, one can easily locate answer candidates, given a reasonably accurate named entity recognizer for documents. Question Hierarchy We define a two-layered taxonomy, which represents a natural semantic classification for typical answers in the TREC task. The hierarchy contains 6 coarse classes (ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NU-MERIC VALUE) and 50 fine classes, Table 1 shows the distribution of these classes in the 500 questions of TREC 10. Each coarse class contains a non-overlapping set of fine classes. The motivation behind adding a level of coarse classes is that of compatibility with previous work's definitions, and comprehensibility. We also hoped that a hierarchical classifier would have a performance advantage over a multi-class classifier; this point, however is not fully supported by our experiments. Table 1 : The distribution of 500 TREC 10 questions over the question hierarchy. Coarse classes (in bold) are followed by their fine class refinements. The Ambiguity Problem One difficulty in the question classification task is that there is no completely clear boundary between classes. Therefore, the classification of a specific question can be quite ambiguous. Consider 1. What is bipolar disorder? What do bats eat? 3. What is the PH scale? Question 1 could belong to definition or disease medicine; Question 2 could belong to food, plant or animal; And Question 3 could be a numeric value or a definition. It is hard to categorize those questions into one single class and it is likely that mistakes will be introduced in the downstream process if we do so. To avoid this problem, we allow our classifiers to assign multiple class labels for a single question. This method is better than only allowing one label because we can apply all the classes in the later precessing steps without any loss. Learning a Question Classifier Using machine learning methods for question classification is advantageous over manual methods for several reasons. The construction of a manual classifier for questions is a tedious task that requires the analysis of a large number of questions. Moreover, mapping questions into fine classes requires the use of lexical items (specific words) and therefore an explicit representation of the mapping may be very large. On the other hand, in our learning approach one can define only a small number of \"types\" of features, which are then expanded in a data-driven way to a potentially large number of features (Cumby and Roth, 2000) , relying on the ability of the learning process to handle it. It is hard to imagine writing explicitly a classifier that depends on thousands or more features. Finally, a learned classifier is more flexible to reconstruct than a manual one because it can be trained on a new taxonomy in a very short time. One way to exhibit the difficulty in manually constructing a classifier is to consider reformulations of a question: All these reformulations target the same answer type Location. However, different words and syntactic structures make it difficult for a manual classifier based on a small set of rules to generalize well and map all these to the same answer type. Good learning methods with appropriate features, on the other hand, may not suffer from the fact that the number of potential features (derived from words and syntactic structures) is so large and would generalize and classify these cases correctly. A Hierarchical Classifier Question classification is a multi-class classification. A question can be mapped to one of 50 possible classes (We call the set of all possible class labels for a given question a confusion set (Golding and Roth, 1999) ). Our learned classifier is based on the SNoW learning architecture (Carlson et al., 1999; Roth, 1998) 2 where, in order to allow the classifier to output more than one class label, we map the classifier's output activation into a conditional probability of the class labels and threshold it. The question classifier makes use of a sequence of two simple classifiers (Even-Zohar and Roth, 2001) \u00d6\u00b4 \u00be \u00b5 so that \u00bf \u00be and \u00bf . \u00bd and \u00bf are the ultimate outputs from the whole classifier which are used in our evaluation. Feature Space Each question is analyzed and represented as a list of features to be treated as a training or test example for learning. We use several types of features and investigate below their contribution to the QC accuracy. The primitive feature types extracted for each question include words, pos tags, chunks (nonoverlapping phrases) (Abney, 1991) , named entities, head chunks (e.g., the first noun chunk in a sentence) and semantically related words (words that often occur with a specific question class). Over these primitive features (which we call \"sensors\") we use a set of operators to compose more complex features, such as conjunctive (ngrams) and relational features, as in (Cumby and Roth, 2000; Roth and Yih, 2001) . A simple script that describes the \"types\" of features used, (e.g., conjunction of two consecutive words and their pos tags) is written and the features themselves are extracted in a data driven way. Only \"active\" features are listed in our representation so that despite the large number of potential features, the size of each example is small. Among the 6 primitive feature types, pos tags, chunks and head chunks are syntactic features while named entities and semantically related words are semantic features. Pos tags are extracted using a SNoW-based pos tagger (Even-Zohar and Roth, 2001) . Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001) . The named entity classifier is also learned and makes use of the same technology developed for the chunker (Roth et al., 2002) . The 'related word' sensors were constructed semiautomatically. Most question classes have a semantically related word list. Features will be extracted for this class if a word in a question belongs to the list. For example, when \"away\", which belongs to a list of words semantically related to the class distance, occurs in the sentence, the sensor Rel(distance) will be active. We note that the features from these sensors are different from those achieved using named entity since they support more general \"semantic categorization\" and include nouns, verbs, adjectives rather than just named entities. For the sake of the experimental comparison, we define six feature sets, each of which is an incremental combination of the primitive feature types. That is, Feature set 1 (denoted by Word) contains word features; Feature set 2 (Pos) contains features composed of words and pos tags and so on; The final feature set, Feature set 6 (RelWord) contains all the feature types and is the only one that contains the related words lists. The classifiers will be experimented with different feature sets to test the influence of different features. Overall, there are about \u00be\u00bc\u00bc \u00bc\u00bc\u00bc features in the feature space of RelWord due to the generation of complex features over simple feature types. For each question, up to a couple of hundreds of them are active. Decision Model For both the coarse and fine classifiers, the same decision model is used to choose class labels for a question. Given a confusion set and a question, SNoW outputs a density over the classes derived from the activation of each class. After ranking the classes in the decreasing order of density values, we have the possible class labels \u00bd \u00be \u00d2 , with their densities \u00c8 \u00d4 \u00bd \u00d4 \u00be \u00d4 \u00d2 (where, \u00c8 \u00d2 \u00bd \u00d4 \u00bd, \u00bc \u00d4 \u00bd, \u00bd \u00d2). As discussed earlier, for each question we output the first classes (\u00bd ), \u00bd \u00be where satisfies, \u00d1 \u00d2\u00b4 \u00d6 \u00d1 \u00d2 \u00d8 \u00b4\u00d8 \u00bd \u00d4 \u00cc \u00b5 \u00b5 (1) T is a threshold value in [0,1]. If we treat \u00d4 as the probability that a question belongs to Class i, the decision model yields a reasonable probabilistic interpretation. We use \u00cc \u00bc in the experiments. Experimental Study We designed two experiments to test the accuracy of our classifier on TREC questions. The first experiment evaluates the contribution of different feature types to the quality of the classification. Our hierarchical classifier is trained and tested using one of the six feature sets defined in Sect. 3.2 (we repeated the experiments on several different training and test sets). In the second experiment, we evaluate the advantage we get from the hierarchical classifier. We construct a multi-class classifier only for fine classes. This flat classifier takes all fine classes as its initial confusion set and classifies a question into fine classes directly. Its parameters and decision model are the same as those of the hierarchical one. By comparing this flat classifier with our hierarchical classifier in classifying fine classes, we hope to know whether the hierarchical classifier has any advantage in performance, in addition to the advantages it might have in downstream processing and comprehensibility. Data Data are collected from four sources: 4,500 English questions published by USC (Hovy et al., 2001) , about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set 3 . These questions were manually labeled according to our question hierarchy. Although we allow multiple labels for one question in our classifiers, in our labeling, for simplicity, we assigned exactly one label to each question. Our annotators were requested to choose the most suitable class according to their own understanding. This methodology might cause slight problems in training, when the labels are ambiguous, since some questions not treated as positive examples for possible classes as they should be. In training, we divide the 5,500 questions from the first three sources randomly into 5 training sets of 1,000, 2,000, 3,000, 4,000 and 5,500 questions. All 500 TREC 10 questions are used as the test set. Evaluation In this paper, we count the number of correctly classified questions by two different precision standards P \u00bd and P . Suppose labels are output for theth question ( ) and are ranked in a decreasing order according to their density values. We define \u00c1 \u00bd \u00d8 \u00d3\u00d6\u00d6 \u00d8 \u00d0 \u00d0 \u00d3 \u00d8 \u00d8 \u00d5\u00d9 \u00d7\u00d8 \u00d3\u00d2 \u00d7 \u00d3\u00d9\u00d8\u00d4\u00d9\u00d8 \u00d2 \u00d6 \u00d2 \u00bc \u00d3\u00d8 \u00d6\u00db \u00d7 (2) Then, \u00c8 \u00bd \u00c8 \u00d1 \u00bd \u00c1 \u00bd \u00d1 and \u00c8 \u00c8 \u00d1 \u00bd \u00c8 \u00bd \u00c1 \u00d1 where m is the total number of test examples. \u00c8 \u00bd corresponds to the usual definition of precision which allows only one label for each question, while \u00c8 allows multiple labels. \u00c8 reflects the accuracy of our classifier with respect to later stages in a question answering system. As the results below show, although question classes are still ambiguous, few mistakes are introduced by our classifier in this step. Experimental Results Performance of the hierarchical classifier Table 2 shows the \u00c8 precision of the hierarchical classifier when trained on 5,500 examples and tested on the 500 TREC 10 questions. The results are quite encouraging; question classification is shown to be solved effectively using machine learning techniques. It also shows the contribution of the feature sets we defined. Overall, we get a 98.80% precision for coarse classes with all the features and 95% for the fine classes.  Inspecting the data carefully, we can observe the significant contribution of the features constructed based on semantically related words sensors. It is interesting to observe that this improvement is even more significant for fine classes.   Tables 3 and 4 show the \u00c8 \u00bd and \u00c8 accuracy of the hierarchical classifier on training sets of different sizes and exhibit the learning curve for this problem. We note that the average numbers of labels output by the coarse and fine classifiers are 1.54 and 2.05 resp., (using the feature set RelWord and 5,500 training examples), which shows the decision model is accurate as well as efficient. Comparison of the hierarchical and the flat classifier The flat classifier consists of one classifier which is almost the same as the fine classifier in the hierarchical case, except that its initial confusion set is the whole set of fine classes. Our original hope was that the hierarchical classifier would have a better performance, given that its fine classifier only needs to deal with a smaller confusion set. However, it turns out that there is a tradeoff between this factor and the inaccuracy, albeit small, of the coarse level prediction. As the results show, there is no performance advantage for using a level of coarse classes, and the semantically appealing coarse classes do not contribute to better performance. Figure 2 give some more intuition on the flat vs. hierarchical issue. We define the tendency of Class to be confused with Class as follows: \u00d6 \u00d6 \u00a3 \u00be \u00b4AE \u2022 AE \u00b5 (3) where (when using \u00c8 \u00bd ), \u00d6 \u00d6 is the number of questions in Class i that are misclassified as belong-   ing to Class j, and AE AE are the numbers of questions in Class i and j resp. Figure 2 is a gray-scale map of the matrix D[n,n]. D[n,n] is so sparse that most parts of the graph are blank. We can see that there is no good clustering of fine classes mistakes within a coarse class, which explains intuitively why the hierarchical classifier with an additional level coarse classes does not work much better. Discussion and Examples We have shown that the overall accuracy of our classifier is satisfactory. Indeed, all the reformulation questions that we exemplified in Sec. 3 have been correctly classified. Nevertheless, it is constructive to consider some cases in which the classifier fails. Below are some examples misclassified by the hierarchical classifier. What French ruler was defeated at the battle of Waterloo? The correct label is individual, but the classifier, failing to relate the word \"ruler\" to a person, since it was not in any semantic list, outputs event. What is the speed hummingbirds fly ? The correct label is speed, but the classifier outputs animal. Our feature sensors fail to determine that the focus of the question is 'speed'. This example illustrates the necessity of identifying the question focus by analyzing syntactic structures. What do you call a professional map drawer ? The classifier returns other entities instead of equivalent term. In this case, both classes are acceptable. The ambiguity causes the classifier not to output equivalent term as the first choice. Conclusion This paper presents a machine learning approach to question classification. We developed a hierarchical classifier that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into fine-grained classes. Our experimental results prove that the question classification problem can be solved quite accurately using a learning approach, and exhibit the benefits of features based on semantic analysis. In future work we plan to investigate further the application of deeper semantic analysis (including better named entity and semantic categorization) to feature extraction, automate the generation of the semantic features and develop a better understanding to some of the learning issues involved in the difference between a flat and a hierarchical classifier.",
    "abstract": "In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes. We show accurate results on a large collection of free-form questions used in TREC 10.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "1195",
    "year": "2002",
    "month": "",
    "title": "Learning Question Classifiers"
}