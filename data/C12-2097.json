{
    "article": "Inferring the information structure of scientific documents has proved useful for supporting information access across scientific disciplines. Current approaches are largely supervised and expensive to port to new disciplines. We investigate primarily unsupervised discovery of information structure. We introduce a novel graphical model that can consider different types of prior knowledge about the task: within-document discourse patterns, cross-document sentence similarity information based on linguistic features, and prior knowledge about the correct classification of some of the input sentences when this information is available. We apply the model to Argumentative Zoning (AZ) scheme and evaluate it on a fully unsupervised learning scenario and two transduction scenarios where the categories of some test sentences are known. The model substantially outperforms similarity and topic model based clustering approaches as well as traditional transduction algorithms. Introduction Information structure of scientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010) . Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008) , rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009) , qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010) . Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007) . This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performance. Even the weakly supervised approach by (Guo and Korhonen, 2011) requires hundreds of annotated sentences for optimal performance. In this paper we focus on a primarily unsupervised approach to inferring information structure which avoids the high annotation cost of the supervised approaches. The only previous work on this topic that we are aware of is that of (Varge et al., 2012) who proposed a simple word-level Latent Dirichlet Allocation (LDA) model to the task, assuming that the phenomenon is mostly lexical. As we show in this paper, the information structure of scientific documents is governed by a number of additional factors, which calls for a more expressive model. We propose a more sophisticated and flexible model capable of integrating different types of task knowledge, depending on the knowledge available in a real-life situation. We investigate two scenarios: (1) the fully unsupervised scenario where no manually annotated sentences are available; and (2) the transductive scenario (Gammerman et al., 1998) where the classes of some of the test set sentences are given. The transductive scenario is of particular interest when some test time knowledge about the document collection is available that could benefit learning. Examples of such knowledge are lexical cues (e.g. key words associated with a database index) for test sentences from a particular target category or sentence annotations that can be obtained fast for a small fraction of test data (e.g. using mechanical turk annotators). Our model can take into account three types of knowledge about the task: (1) within-document discourse patterns; (2) linguistic feature representation used to model cross-document sentence similarity; and (3) in the transductive scenario, prior knowledge about the correct classification of some of the input sentences. Importantly, none of these knowledge types are actually required by the model, but the flexibility of the model enables us to consider all of them or only a subset. We formulate our approach as a graphical model that encodes sentence-level knowledge via single-vertex potentials and knowledge about sets of sentences, both within and between documents, via global potentials. While these potentials encode important linguistic properties, they complicate the inference process. We therefore apply a linear-programming (LP) relaxation method (Sontag et al., 2008) which approximates the maximum aposteriori (MAP) assignment of our model. In our experiments the algorithm provably finds the exact MAP assignment. We compare the predictions of our model to those of argumentative zoning (AZ) -a widely used information structure scheme (Teufel and Moens, 2002) where the core categories are argued to be domain-independent and which has been used to analyse texts in various disciplines such as computational linguistics (Teufel and Moens, 2002) , law (Hachey and Grover, 2006) , biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009) . We experiment with the only publicly available AZ corpus: the corpus of 792 biomedical abstracts by (Guo et al., 2010) which provides AZ annotations for 7886 sentences. Our experimental evaluation shows that the model outperforms traditional algorithms for both the unsupervised and the transductive setups. by a large margin Our results show that it is possible to infer high quality knowledge about the information structure of scientific documents even when only little or no human annotation effort is involved. Previous Work Machine Learning for Information Structure Nearly all previous work on automatic detection of information structure has relied on supervised algorithms and, consequently, on corpora consisting of thousands of manually annotated sentences (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Teufel et al., 2009; Guo and Korhonen, 2011) . Recently, (Varge et al., 2012) were the first to apply unsupervised learning to the information structure of scientific documents. They applied standard word-level LDA models to the IMRAD scheme for the biomedical domain (along with their own information structure scheme for the aerospace domain). This purely lexical approach ignores other important linguistic phenomena, such as discourse patterns and syntactic properties, which play a role in information structure. The 35 F-score performance of their model indeed show that there is much scope for improvement. Our model integrates a much wider range of linguistic knowledge about the task at both within-document (e.g. discourse patterns) and cross-document (e.g. sentence similarity) levels, and can be flexibly applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005) . We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data is biomedical, we use the version of the AZ scheme adapted for biology by (Mizuta et al., 2006) . This version has ten zone categories. We focus on the five that appear in abstracts (as opposed to full papers): BACKGROUND, OBJECTIVE, METHODS, RESULTS, and CONCLUSIONS. 1 For a detailed definition of the zone categories and an example annotated abstract see (Guo et al., 2010) . Model Structure We denote the number of sentences in the document collection with n and the number of target categories with K. We define an undirected graphical model (Markov Random Field, MRF) with the vertex set V = X \u222a A, where X = {x 1 , . . . , x n } consists of one vertex for every sentence in the document collection, and A = {a 1 . . . a K } is a set of agreement vertices. We integrate knowledge in the model through singleton potentials (defined over individual vertices) as well as pairwise potentials (defined between pairs of vertices). We consider the following types of knowledge: (1) Within-Document Discourse Patterns which encode the information conveyed by discourse patterns about the progress of information categories along a document. The discourse knowledge is encoded through within-document pairwise potentials as well as singleton potentials, both defined over vertices in X . (2) Cross-Document Sentence Similarity which encourages similar sentences in different documents to be assigned in the same category. This knowledge is encoded through cross-document pairwise potentials, defined over vertices in X , and through potentials between sentence vertices (X ) and the agreement vertices (A). (3) Class-Specific Lexical Cues Encode lexical cues for the cluster of a given sentence through within-document pairwise potentials. (4) Prior Knowledge on Sentence Categorization which encodes prior knowledge about the categories of a predefined set of sentences through local potentials. We use five types of potentials: (1) The singleton potentials encode context-free knowledge that does not depend on neighbouring vertices. (2) The pairwise potentials between sentence vertices in the same document encode discourse patterns that govern the information flow in the document. (3) The pairwise potentials between sentence vertices (X ) in different documents encode the similarity between the sentences. The more similar a pair of sentences, the stronger the tendency of its members to be assigned to the same category. (4) The pairwise potentials between sentence vertices (X ) and agreement vertices (A) encoded a tendency of sentences in different documents to be assigned to the same category based on sentence similarity patterns in their documents. In the last two potential types, the similarity between sentences is based on linguistic features. Finally, (5) the pairwise potentials between agreement vertices ensure that cat e gor y(a i ) = cat e gor y(a i\u22121 ) + 1 by giving an infinite bonus to those assignments. The resulting maximum aposteriori problem (MAP) takes the form of: M AP(V ) = n i=1 \u03b8 i (x i ) + n i=1 n j=1 \u03b8 i, j (x i , x j ) + n i=1 K j=1 \u03c6 i, j (x i , a j ) + K i=1 K j=1 \u03be i, j (a i , a j ) We define the singleton and the pairwise potentials to take the following forms 2 : \u03b8 i (x i ) = \uf8f1 \uf8f2 \uf8f3 \u03b1 if discourse pattern holds \u221e if prior sentence-classification condition holds 0 otherwise \uf8fc \uf8fd \uf8fe \u03b8 i, j (x i , x j ) = \uf8f1 \uf8f2 \uf8f3 \u03b2 if discourse pattern holds SimScor e i, j if similarity condition holds 0 otherwise \uf8fc \uf8fd \uf8fe \u03c6 i, j (x i , a j ) = \u03b3 if similarity pattern holds 0 otherwise Where SimScor e i, j are the feature-based similarity scores computed between sentences in different documents and \u03b1, \u03b2 and \u03b3 are the model parameters representing the relative strength of the different types of knowledge. In section 3.2 we give a detailed description of the information encoded into these potentials. Potentials and Encoded Knowledge Within Document Discourse Patterns \u03b8 i, j (x i , x j ) and \u03b8 i (x i ). We encode different types of knowledge through the pairwise and the singleton potentials. The pairwise potentials encode a number of discourse cues for the progress of information categories in the document: Therefore, if a sentence contains a passive verb or appears in the opening part 3 of a document, the pairwise potentials of that sentence and of its predecessor give a bonus to assignments in which a category change occur. Likewise, when moving from the opening part to the closing part of the document, the corresponding pairwise potentials encourage transition to a predefined set of clusters. 4 . The singleton potentials encode the tendency of scientific documents to start with a background knowledge related to the article, and to end with conclusions. They do so by encouraging the first sentence in each document to be in the first output category and, likewise, the last sentence in each document to be in the last output category. Cross-Document Sentence Similarity. We build a feature representation for each vertex in X . We consider three of the feature sets described in (Guo and Korhonen, 2011) : POS -the part-of-speech tags of the verbs in the sentence; Location -each document is divided into 10 parts, the location feature takes two values: the part where the sentence starts and the part where it ends; and Object -the words that appear as verb objects in the sentence . We use this representation to encourage identical category assignment for similar sentences. We do this by two types of pairwise potentials: (1) Pairwise potentials between sentence vertices (\u03b8 i, j (x i , x j )). We define the similarity between the i \u2212 th and the j \u2212 th sentences, SimScor e i, j , as the number of features that have the same value in their representation. The similarity condition in the potential definition holds if the similarity score between the sentences is among the top M scores for the i \u2212 th sentence; (2) Pairwise potentials between sentence and agreement vertices (\u03c6 i, j (x i , a j )). The similarity scores between sentences that belong to the same category tend to concentrate around the same value. Consequently a significant change in similarity between consecutive sentences is an indication of a category change. To encode this potential we scan the document from the beginning and compute the similarity between consecutive sentences. A similarity score that exceeds the maximum or deceeds the minimum of the previously observed similarity scores by a pre-defined threshold, is considered to be a class change indication 5 . For a sentence i that appears before the j \u2212 th change we write \u03c6(x i , a j ) = \u03b3. The other values of this potential are set to zero. An example of two documents where a similarity pattern exists and of one document in which it does not exist (and therefore the \u03c6 values for its sentences with all agreement vertices are set to zero) is given in Figure 1 . Prior knowledge about Sentence Classification (Transduction) \u03b8 i (x i ). We experiment with the conditions where we have oracle knowledge (i.e. knowledge that is taken from the gold standard) of the categories of some of the test set sentences and the model should predict the categories of the other sentences. The prior sentence-classification condition in the definition of \u03b8 i (x i ) is simply that the category of the i-th sentence is known to be x i . Figure 1 : Three examples of similarity patterns in the beginning of documents. Lines represent documents, vertices represent sentences and the label inside a vertex corresponds to the agreement vertex to which this vertex is connected. Edges are labeled with the similarity score between the vertices they connect. The similarity difference threshold in this example is 10. Inference Our model is a pairwise MRF. When cross-document sentence similarity knowledge is encoded, the model is very likely to have cycles which make exact inference NP-hard (see Section 3.1). When this knowledge is not encoded, the model becomes a simple linear chain model with edges between each pair of consecutive sentences in the same document. In such a model, exact inference can be done efficiently using dynamic programming. We addressed this problem by using the message passing algorithm for linear-programming (LP) relaxation of the MAP assignment (MPLP) described in (Sontag et al., 2008) . LP relaxation algorithms for the MAP problem define an upper bound on the original objective which takes the form of a linear program. Consequently, a minimum of this upper bound can be found using standard LP solvers or, more efficiently, using specialised message passing algorithms (Yanover et al., 2006) . The algorithm comes with an optimality guarantee: when the solution to the linear program is integral it is guaranteed to give the global optimum of the MAP problem. The MPLP algorithm described in (Sontag et al., 2008) is attractive in that it iteratively computes tighter upper bounds on the MAP problem. Experiments Data and Scenarios We experimented with the biomedical abstracts from the data set of (Guo et al., 2010) consisting of 1000 AZ-annotated abstracts (7985 sentences, 225785 words). We used the 792 abstract (7886 sentences) test set of (Guo and Korhonen, 2011) . We consider two scenarios: a fully unsupervised scenario, and a transduction. For the latter we consider two conditions: (1) the identity of all the sentences that belong to one of the clusters is known; and (2) the oracle cluster assignment of randomly selected 5% or 10% of the sentences is known. In all cases our model as well as the baselines induce K = 5 categories. Baselines Our first baseline is the K-means algorithm (Bishop, 2006) where sentences are represented by the same features that are used for constructing our similarity scores. In the fully unsupervised scenario we use the standard K-means. For transduction, in the condition where all the sentences of one of the classes are known, we run K-means only for the rest of the sentences and induce K-1 clusters; In the condition where the labels of a randomly selected sentence subset are known, we fix the classes of these sentences during the run of the algorithm (so that they affect their class centroid over the iterations) and use the mean of the vectors that are known to belong to each class as its initial centroid. For the fully unsupervised scenario we also compare to the Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) , a fully unsupervised, topic-model based algorithm. Like our within-document pairwise potentials, this algorithm models the sequential progress of topics in an abstract. However, in contrast to our model, it aims to maximize the lexical coherence of the induced categories. For the transduction scenario our second baseline is the transductive SVM algorithm (T-SVM from (Sinz, 2011) ) , where the feature-based representation is similar to the one we use in our model and in K-means. Being a classifier, this baseline is only useful in the second transduction condition where the categories of 5% or 10% of the sentences are given. T-SVM is a transductive classifier. To better understand the effect of each of these properties we also compare our model to the performance of a standard SVM . In addition to these baselines, we compare our full model to models created by omitting all the potentials related to a specific type of encoded knowledge: feature-based similarity or discourse patterns. Parameter Tuning Our model governs the relative weight of its components with three potential parameters \u03b1, \u03b2 and \u03b3, and with M , the connectivity degree of the graph (Section 3). We manually set these parameters on 10 abstracts to the values that give the best performance in the unsupervised scenario and used them across all experiments. The potential parameters used are: \u03b1 = 10 5 , \u03b2 = 10 2 , \u03b3 = 10 7 and M was set to 50. We run K-means 100 times, randomly selecting the cluster centers from the set of clustered vectors, and selected the output clustering with the highest objective values. For HTMM, we assumed symmetric prior and ran the algorithm 10 times for each hyperparameter value in {0.01, 0.05, 0.1, 0.15 . . . 1}. For each parameter assignment we selected the solution with the highest likelihood and the results we report are of oracle selection of the best of these solutions. The SVM algorithms were trained with the default setting of UniversSVM (Sinz, 2011) . Evaluation We uses greedy 1-1 mapping for evaluation. We mapped each induced category in the test set to one of the gold classes in a greedy 1-1 manner using the Kuhn-munkres algorithm for maximum matching in a bi-partite graph (Munkres, 1957) . We then report the sentence level accuracy across the entire test set. In addition, we report per-class F-score (adjusted to a 0-100 scale) after the greedy 1-1 mapping is performed. Results The Fully Unsupervised Scenario Table 1 (left) presents results for the unsupervised setup. The top line shows results for the full test set: our model outperforms the K-means and HTMM baselines by 7% and 17.3%, respectively. The bottom lines present a similar pattern for the per-class F-score: our model is better for the BACKGROUND, RESULTS and CONCLUSIONS zones by up to 52%. This result demonstrates the importance of modelling linguistic similarity between sentences jointly with the sequential progression of the abstract discourse. While K-means clusters sentences together according to their linguistic similarity and HTMM models the progression of lexical topics in the abstract, our approach is to model linguistic similarity and sequential category progress jointly. Furthermore, unlike HTMM, we capitalize on discourse elements rather than on lexical cohesion when modelling the sequential progress. The Transduction Scenario Results for this setup are in Table 2 : Results for the transduction scenario. In each of the first five lines the identity of all the sentences that belong to one of the classes is given to the models. In the last two lines the classes of a random sample of 5% or 10% of the sentences are known. that do not appear in the training data. Our model is better than K-means in propagating this knowledge achieving 5.1% -7.6% performance gain. The next two lines of the table compare the performance of the models when the categories of randomly selected 5% or 10% of the test-set sentences are known. Our model is superior again beating the baslines by 3.1% or more. Model Components The right sections of the tables present an ablation analysis where we compare the performance of our full model to that of its components. When excluding the potentials that model between-document similarity from our model (Model -Similarity), the performance drops by 3% for the full test set (Table 1 top) and by up to 9% for four of the zone classes (Table 1 bottom) in the unsupervised scenario. Our full model further outperforms its discourse component in the seven transduction scenarios by up to 6.9%. When excluding these potentials from the model (Model -Discourse), the performance in the unsupervised scenario drops by 8.5% for the full test set and by up to 26.6% for the per-class F-score. Similarly, the performance drops in six of the seven transductive scenarios, by up to 13.8%. Convergence The MPLP algorithm minimizes an upper bound on the MAP objective. Since this bound is convex, the MPLP algorithm is promised to converge to its global minimum, but the bound is promised to be tight only if the solution is integral -i.e. if every vertex is assigned to the same category by all the potentials that take it as an argument. In practice, in all the experimental conditions for all test subsets our model converges to an integral exact solution. Conclusion and perspectives We presented a novel unsupervised model for inferring information structure of scientific documents. The model integrates within-document discourse patterns and cross-document, feature-based linguistic information in a flexible way that enables to control the relative importance of different knowledge types by parameter setting. In the future we intend to extend our model to address more information sources and to use it for data-driven analysis of the various existing AZ schemes. Acknowledgments The work in this paper was funded by the Royal Society, (UK), EPSRC (UK) grant EP/G051070/1 and EU grant 7FP-ITC-248064.",
    "abstract": "Inferring the information structure of scientific documents has proved useful for supporting information access across scientific disciplines. Current approaches are largely supervised and expensive to port to new disciplines. We investigate primarily unsupervised discovery of information structure. We introduce a novel graphical model that can consider different types of prior knowledge about the task: within-document discourse patterns, cross-document sentence similarity information based on linguistic features, and prior knowledge about the correct classification of some of the input sentences when this information is available. We apply the model to Argumentative Zoning (AZ) scheme and evaluate it on a fully unsupervised learning scenario and two transduction scenarios where the categories of some test sentences are known. The model substantially outperforms similarity and topic model based clustering approaches as well as traditional transduction algorithms.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "4",
    "year": "2012",
    "month": "December",
    "title": "Document and Corpus Level Inference For Unsupervised and Transductive Learning of Information Structure of Scientific Documents"
}