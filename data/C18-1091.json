{
    "article": "Sentence compression condenses a sentence while preserving its most important contents. Delete-based models have the strong ability to delete undesired words, while generate-based models are able to reorder or rephrase the words, which are more coherent to human sentence compression. In this paper, we propose Operation Network, a neural network approach for abstractive sentence compression, which combines the advantages of both delete-based and generate-based sentence compression models. The central idea of Operation Network is to model the sentence compression process as an editing procedure. First, unnecessary words are deleted from the source sentence, then new words are either generated from a large vocabulary or copied directly from the source sentence. A compressed sentence can be obtained by a series of such edit operations (delete, copy and generate). Experiments show that Operation Network outperforms state-of-the-art baselines. Introduction Sentence compression is the natural language generation (NLG) task of condensing a sentence while preserving its most important contents. It can also be viewed as a sentence-level summarization task. With the rapid growth of the web contents in recent years, summarization techniques such as sentence compression are becoming more and more important since these techniques can greatly reduce the information overload on the web. Sentence compression can benefit a wide range of applications, especially those on mobile devices which have restricted screen spaces. Sentence compression models can be broadly classified into two categories: delete-based models and abstractive models. Delete-based approaches remove unimportant words from the source sentence and generate a shorter sentence by stitching the remaining fragments together. On the contrary, abstractive models consider operations beyond word deletion, such as reordering, substitution and insertion. Abstractive sentence compression models produce a reform of the source sentence from scratch, thus the results produced by abstractive sentence compression models are more expressive. Obviously, abstractive sentence compression is much harder than delete-based sentence compression, since it needs deeper understanding of the source sentence. Delete-based sentence compression treats the task as a word deletion problem: given an input source sentence x = x 1 , x 2 , ..., x n (where x i stands for the ith word in the sentence x), the goal is to produce a target sentence by removing any subset of words in the source sentence x (Knight and Marcu, 2002) . Delete-based sentence compression has been widely explored across different modeling paradigms, such as noisy-channel model (Knight and Marcu, 2002; Turner and Charniak, 2005) , large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) , integer linear programming (Clarke and Lapata, 2008) and variational auto-encoder (Miao and Blunsom, 2016) . In delete-based sentence compression models, only delete operations are allowed, thus the order of the remaining words can not be changed. These constraints make delete-based sentence compression a relatively easier task. However, in spite of the strong ability of deleting undesired words, delete-based models are not able to rephrase the words, which is far from human sentence compression. For example, in human sentence compression, the word \"remove\" can replace the phrase \"get rid of\" under some particular circumstance, but this substitution can not be accomplished by delete-only models. Abstractive sentence compression has the ability to reorder, substitute words or rephrase, thus it is more coherent to human sentence compression. Due to the difficulty of abstractive sentence compression, there was only a limited number of work on the task (Cohn and Lapata, 2008; Cohn and Lapata, 2013; Galanis and Androutsopoulos, 2011; Coster and Kauchak, 2011a) . However, with the recent success of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014) , which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon the Seq2Seq architecture with attention mechanism, such as (Chopra et al., 2016; Wubben et al., 2016; Nallapati et al., 2016; See et al., 2017) . These abstractive models (which will be termed generate-based models hereafter) have the ability to reorder words or rephrase. However, none of these models consider explicit word deletion. As Coster and Kauchak (2011b) pointed out, deletion is a frequently occurring phenomena in sentence compression dataset. Coster and Kauchak (2011a) imposed delete operation on their sentence compression model and improved the performance significantly. Thus, deletion is also very important for abstractive sentence compression task. Inspired by previous work, we propose an Operation Network for abstractive sentence compression, which combines the advantages of both delete-based and generated-based sentence compression. The central idea is to model the sentence compression process as an editing procedure. We not only enable the model with a strong ability of word deletion inspired from delete-based models, but also endow the model with the ability of word reordering and word rephrasing inspired from generate-based models. With a series of editing operations, the source text can be transformed into a condensed version of itself. There are three kinds of editing operations in our model: delete, copy and generate. Given a source text as input, first the delete operations remove unnecessary words from the source text yet retain the important content. Then, the summary is constructed by the copy and generate operations. Copy operations duplicate words directly from the selected source text, while generate operations produce words which are not in the source texts. Our model is built upon the Seq2Seq framework, and can be trained in an end-to-end fashion. To summarize, the contributions of this paper are as follows: \u2022 We propose Operation Network, a neural framework for abstractive sentence compression, which models the sentence compression task as a series of editing operations. \u2022 Our Operation Network combines the advantages of both delete-based and generate-based sentence compression. The model is equipped with a strong ability of not only word deletion, but also word reordering and word rephrasing. Experiments show that our model outperforms the baselines. Task Definition We formulate the problem of abstractive sentence compression discussed in this paper as follows: Given a source sentence X = (x 1 , x 2 , ..., x n ) as input, the goal is to generate a target sentence Y = (y 1 , y 2 , ..., y m ), m < n, which is a condensed version of X with good readability and retains the most important information in X. Essentially, the model estimates the conditional probability as follows: P (Y |X) = m t=1 P (y t |y <t , X). (1) 3 Background: Seq2Seq Model Our model is built upon the general Seq2Seq model (Sutskever et al., 2014) , which is also called the encoder-decoder model. The model consists of an encoder and a decoder. The encoder takes as input a source sequence X = (x 1 , x 2 , ..., x n ), and outputs a sequence of hidden states h = (h 1 , h 2 , ..., h n ). The decoder takes h as input and outputs the target sequence Y = (y 1 , y 2 , ..., y m ). In both encoder and decoder, we use gated recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) as the basic unit. We use a bi-directional RNN (Schuster and Paliwal, 1997) as encoder. A bidirectional RNN contains two distinct RNNs, a forward RNN and a backward RNN. Given the input X = (x 1 , x 2 , ..., x n ) and the embedding lookup table e, the forward RNN reads the input in the leftto-right direction, resulting a sequence of forward hidden states \u2212 \u2192 h = ( \u2212 \u2192 h 1 , \u2212 \u2192 h 2 , ..., \u2212 \u2192 h n ), where \u2212 \u2192 h t = GRU( \u2212 \u2192 h t\u22121 , e(x t )). Similarly, the backward RNN reads the input in the reversed direction and outputs \u2190 \u2212 h = ( \u2190 \u2212 h 1 , \u2190 \u2212 h 2 , ..., \u2190 \u2212 h n ). At each time step, we concatenate the hidden states of the corresponding forward and backward RNNs and obtain the encoder hidden states h = (h 1 , h 2 , ..., h n ), where h t = [ \u2212 \u2192 h t ; \u2190 \u2212 h t ], t = 1, ..., n and [A; B] denotes vector concatenation. In the decoder for general Seq2Seq model, another GRU is used for updating the decoder hidden states. Given a context vector c t and the previously decoded word y t\u22121 , the decoder hidden states are updated as follows: s t = GRU(s t\u22121 , [c t ; e(y t\u22121 )]) (2) The context vector c t is designed to dynamically attend on key information of the input sentence during the decoding procedure (Bahdanau et al., 2014) . c t is calculated as a weighted sum of the encoder hidden states: c t = n k=1 \u03b1 tk h k (3) \u03b1 t = (\u03b1 t1 , \u03b1 t2 , ..., \u03b1 tn ) is called the attention distribution. It can be viewed as a probability distribution over the source words, which tells the decoder where to attend to produce the next word. The attention distribution is calculated as follows: \u03b1 tk = softmax(e tk ) = exp(e tk ) n j=1 exp(e tj ) (4) e tk = v a tanh(W a s t\u22121 + U a h k + b a ) (5) where v a , W a , U a and b a are trainable parameters. The vocabulary distribution P voc t at time step t is calculated as follows: P voc t = softmax(W o (W o [s t ; c t ] + b o ) + b o ) (6) where W o , W o , b o and b o are trainable parameters. P voc t is a probability distribution over all words in the vocabulary V , and the output word y t at time step t is calculated as follows: y t = arg max w P voc t (w), w \u2208 V (7) During training, the loss for time step t is the negative log likelihood of the target word y * t : loss t = \u2212 log P voc t (y * t ). The overall loss function for the whole word sequence is: loss = 1 T Y T Y t=1 loss t , where T Y is the length of the target word sequence Y . Operation Network In this section, we introduce our model, called as the Operation Network, to address the task of abstractive sentence compression. In Operation Network, we consider the transformation from the source sentence to the target sentence as a series of operations. We use three kinds of different operations: delete, copy and generate. To enable the model to perform these three types of edit operations, we use two distinct decoders, one for delete and the other for copy and generate. Specifically, we use the same encoder as in the general Seq2Seq model, but employ a delete decoder and a copy-generate decoder. After the input sentence is encoded by the encoder, the delete decoder will first delete unnecessary words from the input sentence, then the copy-generate decoder will produce the output sentence either by copying from the choices of the delete decoder, or generating from a fixed vocabulary. The overall architecture of our model is shown in Figure 1 . \u00d7 \u00d7 \u2713 \u00d7 \u00d7 \u2713 \u00d7 \u00d7 \u00d7 \u00d7 \u2713 \u2713 \u2713 Delete Copy-Generate Decoder Encoder Delete Decoder Figure 1: Operation Network, which consists of an encoder, a delete decoder and a copy-generate decoder. Delete Decoder The delete decoder \"deletes\" all unimportant words from the sentence. It takes as input the sequence of the encoder hidden states h = (h 1 , h 2 , ..., h n ), the text sequence X = (x 1 , x 2 , ..., x n ), and outputs a sequence d = (d 1 , d 2 , ..., d n ), which has the same length n. d i \u2208 [0, 1], i \u2208 1, 2, ..., n. If d i is close to 0, then the corresponding word x i tends to be deleted. Otherwise if d i is close to 1, then the corresponding word x i tends to be kept. The output sequence d will be used together with the copy-generate decoder which we will discuss in Section 4.2. The architecture of the delete decoder is shown in Figure 2 . For decoding step t in the delete decoder, we feed the decoder with the embedding of word x t , the previous decoder output d t\u22121 and the context vector c t . The delete decoder states are updated as follows: s t = GRU(s t\u22121 , [c t ; e(x t ); d t\u22121 ]) (8) d t = \u03c3(W d s d t + b d ) (9) where \u03c3 is the sigmoid function, c t is the context vector calculated in the same way as Equation 3, e denotes the word embedding Copy-Generate Decoder The Copy-Generate decoder produces output the compressed sentence word by word, and the output words are either copied from the input words which are filtered by the delete decoder, or generated with a fixed vocabulary. In other words, our model integrates copy, generate, delete operations together to produce the output sequence. We implement the Copy-Generate decoder as a hybrid network between the basic Seq2Seq network and a pointer network (Vinyals et al., 2015) . The structure of the Copy-Generate decoder is close to CopyNet (Gu et al., 2016) , Pointer Softmax (Gulcehre et al., 2016) and Pointer-Generator (See et al., 2017) . However, the Copy-Generate decoder model has some unique characteristic designed for abstractive sentence compression task. The major difference is that our Copy-Generate decoder incorporates the result of the delete decoder, to make sure that unnecessary words will not be copied back by accident. Another difference from the other models is that our model generates explicit operations. During the training procedure, we also add supervision on these operations (see Section 4.3). We find that explicit operation supervision can lead to better results. Last, unlike Gu et al. (2016) and Gulcehre et al. (2016) , where copy operations are only used for handling UNK tokens (special tokens which stand for out-ofvocabulary words) or named entities, in our model, copy operations are triggered much frequently as long as the output word can be copied from the input sentence. This is useful especially when the training dataset is relatively small. The Copy-Generate decoder takes as input the encoder hidden states h = (h 1 , h 2 , ..., h n ) and the attention distributions \u03b1 = (\u03b1 1 , \u03b1 2 , ..., \u03b1 n ), and outputs the target sentence as a sequence of words Y = (y 1 , y 2 , ..., y m ). At each step of decoding, a switch network is used to decide whether to use copy mode or generate mode. If copy mode is chosen, the word is copied directly from the source words (filtered by the delete decoder). We use the attention distributions to sample the source words. The output of the delete decoder is used to modify the attention distributions to filter out unwanted words. Otherwise, if generate mode is chosen, the word is generated from the fixed vocabulary. Specifically, for words that are neither in the source sentence nor in the vocabulary, the switch network will choose generate mode and UNK tokens will be produced from the decoder. The Copy-Generate decoder use another distinct GRU to update its decoder hidden states as follows: s t = GRU(s t\u22121 , [c t ; e(y t\u22121 )]) (10) where s t is the decoder hidden state at time step t, c t is the context vector, e denotes the word embedding table, and y t\u22121 is the previous decoded output. At each time step t, the switch network calculates generate probability as follows: g gen t = \u03c3(W z [s t ; c t ] + b z ) (11) g gen t acts as a soft switch to choose between generate mode and copy mode. In generate mode, the target word is sampled from the vocabulary distribution P voc t (see Equation 6 ). In copy mode, first we use the outputs of the delete decoder d (see Section 4.1) to mask and renormalize the attention distribution \u03b1 t : \u03b1 t = 1 n i=1 d i \u03b1 ti (d 1 \u03b1 t1 , d 2 \u03b1 t2 , ..., d n \u03b1 tn ) = (\u03b1 t1 , \u03b1 t2 , ..., \u03b1 tn ). ( 12 ) Then the target word is sampled from the modified attention distribution \u03b1 t . For each sentence, let the extended vocabulary V denotes the union of the vocabulary V and all words appearing in the source sentence, then the final probability distribution over the extended vocabulary V is: P t (w) = g gen t P voc t (w) + (1 \u2212 g gen t ) k:w=x k \u03b1 tk , w \u2208 V (13) where x k denotes the kth words in the source sequence X. The output word y t at time step t is sampled from the final distribution P t : y t = arg max w P t (w), w \u2208 V (14) Loss Function The total loss consists of two parts: the loss from delete decoder and the loss from copy-generate decoder. Delete Decoder Loss The delete decoder loss measures the difference between the target deletion sequence and the actual deletion sequence predicted by the delete decoder. The delete decoder loss is calculated as follows: delete loss = 1 T X T X t=1 \u2212 dt log d t \u2212 (1 \u2212 dt ) log(1 \u2212 d t ) (15) where T X is the length of the source word sequence X, and d is the target deletion sequence, dt = 0 if word x t should be deleted, otherwise, if word x t should be kept, dt = 1. Copy-Generate Decoder Loss At each time step t, the copy-generate decoder loss can be divided into three parts: the switch loss, the copy loss and the generate loss. The switch loss measures the difference between the target switch and the predicted switch by the model. The copy loss measures the difference between the target attention distribution and the actual attention distribution predicted by the model. And the generate loss measures the loss between the target vocabulary distribution and the generated vocabulary distribution at that time step. The losses at time step t is calculated as follows: switch loss t = \u2212 \u011dgen t log g gen t \u2212 (1 \u2212 \u011dgen t ) log(1 \u2212 g gen t ) ( where T Y is the length of the target word sequence Y . Experiments In this section, we introduce the experiments for abstractive sentence compression with our proposed Operation Network. First, we present a brief description of the dataset, and the pre-processing procedure in our experiments. Then, we introduce baselines that are compared with our model. Next, we introduce parameters of our models in the experiments. At last we present and analyze the experiment results. Dataset We adopt the dataset provided by Toutanova et al. (2016) for our experiments. It's a manually-created, multi-reference dataset for sentence and short paragraph compression. It contains 6,169 source texts with multiple compressions (26,423 pairs of source and compressed texts), consisting of business letters, news journals, and technical documents sampled from the Open American National Corpus (OANC 1 ). Of all the source texts, 3,769 are single sentences and the rest are 2-sentence short paragraphs. Each pair of the source and compressed text is aligned by the state-of-the-art monolingual aligner Jacana (Yao et al., 2013) . The dataset is split into a training set (21,145 pairs), a validation set (1,908 pairs) and a test set (3,370 pairs). The dataset is available online 2 . The alignment information together with the pair of source text and compressed text are used for generating operation sequences for our experiments. Specifically, for each pair of the source and compressed text, a delete operation sequence and a copy/generate sequence are generated. The delete operation sequence is a sequence of delete/retain tokens which has the same length with the source text. For each word in the source text, if the word exists in the compressed text or is aligned by the aligner, the corresponding token in the delete operation sequence is retain. On the other hand, if the word neither exists in the compressed text nor aligned by the aligner, the corresponding token in the delete operation sequence should be delete. The copy/generate operation sequence is a sequence of copy/generate tokens which has the same length with the compressed text. For each word in the compressed text, if the word is aligned by the aligner and is the same as its counterpart, then the corresponding token in the sequence is copy, which means this word is copied from the source text. If the word is not aligned or not the same as its counterpart, then the corresponding token in the sequence is set to generate, which means this word is generated from the vocabulary. The reason we didn't put delete, copy and generate operations in a single sequence is that delete operations are source-text-based operations while copy and generate operations are target-text-based operations. It is consistent with that the delete operation sequence has the same length as the source text and the copy/generate operation sequence has the same length as the compressed text. Baselines We compared the abstractive sentence compression results generated by our model (Operation Network) with those by two baselines. These baselines consist of a generate-only model (Seq2Seq) and a generate + copy model (Pointer-Generator). The details of the baselines are described as follows: \u2022 Seq2Seq: Seq2Seq is an generate-only model similar to the model described by Nallapati et al. (2016) . It uses the same bi-directional RNN encoder and attention mechanisms as our model. The decoder of Seq2Seq model can only generate words from the fixed vocabulary. \u2022 Pointer-Generator: Pointer-Generator is a model proposed by See et al. (2017) . It uses a hybrid pointer-generator network that can copy words from source text or generate words directly from a large vocabulary. This model also use coverage to keep track of what has been summarized, which discourage repetition. Refer to See et al. (2017) for more details. Experiment Parameters For all experiments, we use 300-dimensional word embeddings. We also use the pre-trained GoogleNews vectors to initialize the embeddings. For words do not exist in GoogleNews vectors, we initialize them randomly. The vocabulary size is set to 20,000. We also use the large vocabulary tricks described by Jean et al. (2014) . The sampled vocabulary size is set to 4,096. The hidden state of a single GRU is 256dimensional. To overcome overfitting, we also imposed dropout on the input, output and state vector of the GRUs. The dropout probability is set to 0.5. We trained the models on a single Nvidia Titan X GPU with a batch size of 32. During training, we used Stochastic Gradient Descent with an initial learning rate of 0.15, and applied the exponential decay to the learning rate as the training process proceeds. At test time, the output sentences are decoded using beam search with beam size 4. Experiment Metric In the experiments, we compare our model and the baselines with the following metrics. Compression Ratio: The common assumption in compression research is that the system can make the determination of the optimal compression length. Thus, compression ratios can vary drastically across systems. Different systems can be compared only when they are compressing at similar ratios (Napoles et al., 2011) . Compression ratio is defined as: CompRatio = # of tokens in compressed text # of tokens in source text (20) ROUGE: We evaluated our models with the standard ROUGE metric proposed by Lin (2004) . ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is commonly used for measuring the quality of the summary by comparing computer-generated summaries to reference summaries generated by humans. The basic idea of ROUGE is to count the number of overlapping units such as n-grams, word sequences, and word pairs between computer-generated summaries and the reference summaries. In our experiments, we considered ROUGE-1, and ROUGE-L (which respectively measures the word-overlap, bigram-overlap, and longest common sequence between the reference summary and the summary to be evaluated). BLEU: We also report BLEU scores of the baseline models and Operation Network on the test dataset. BLEU is proposed by Papineni et al. (2002) , and is usually used for automatic evaluation of statistical machine translation systems. However, it can also be used for evaluating sentence compression task (Napoles et al., 2011) . We use the multi-bleu script 3 for BLEU score calculation. Result Analysis We present the average ratios of the operations in Operation Network's outputs and the human-written references on the test dataset in Figure 3 . Note that the delete ratio is calculated as the number of delete operations divide by the number of tokens in source text, and the copy and generate ratio is calculated as the number of copy and generate operations divided by the number of tokens in compressed text, respectively. From the figure we can see that Operation Network can utilize the operation information to help accomplish the sentence compression task. The average delete ratio in Operation Network is lower than in human reference, since there are multiple references for one source text in the dataset, and the Operation Network learns to delete a token only it is deleted in most of the references. The average copy ratio is higher in Operation Network than in human reference, this may suggest that Operation Network tends to copy words directly from the source text whenever it is possible. Table 1 shows the average compression ratios (%), ROUGE and BLEU scores of the baseline models and Operation Network on the test dataset. The second column shows the average compression ratios. The average compression ratio is calculated as the average of the compression ratios of all the compressed text outputs in the test dataset. We can see that the average compression ratios of both our model and the other two baselines are similar. Thus, the comparison between our model and the baseline models is fair. Since we have employed the delete decoder in Operation Network, the average compression ratio of our model is lower than the other baselines. From Table 1 we can see that the generate-only model (Seq2Seq) performs poorly compared to its counterparts. This is due to the fact that the Seq2Seq model can only generate words from a fixed vocabulary. When the training dataset is relatively small (about 21k pairs of source and compressed text), the model may not be trained adequately. We tried the same models with random-initialized word embeddings and with GoogleNews-vector-initialized word embeddings and it turned out that random-initialized word embeddings lead to worse performance. The copy-and-generate model (Pointer-Generator) performs much better than the generate-only model. This is because the copy operations allow to copy words directly from the source texts and the attention distribution is much smaller than the vocabulary distribution and easier to train. With the addition of delete operation, our model (Operation Network) outperforms the other baselines in terms of all three ROUGE metrics and the BLEU metric. Results show that the delete operations can effectively filter out unnecessary words from the source texts and lead to better performance. This can also show that the Operation Network is suitable for the abstractive sentence compression task. In order to evaluate the quality of the summaries produced by our model and the other baselines, we ask annotators to do a score evaluation on some aspects of the summaries. Specifically, we ask them to score the grammaticality and non-redundancy of the summaries. Annotators are instructed to read the summary carefully and rate each aspect with scores matching the quality of the corresponding aspect. Each aspect is rated with a score from 0 (bad) to 5 (excellent). We randomly sample 100 samples from the test set for evaluation. Each summary generated by our model or baselines is rated by at least 5 annotators. The summaries are randomly reordered and model information is anonymous to the annotators. The evaluation result is shown in Table 2 The results in Table 2 show that combined with the delete decoder, our model can effectively delete unimportant contents without losing much grammar quality of the text. This exactly matches what we expect from our model. Pointer-Generator model performs better on grammaticality aspect than our model, however, our model outperforms the other baselines on non-redundancy aspect. The basic Seq2Seq model performs poorly on both grammaticality aspect and non-redundancy aspect. Related Work Delete-based sentence compression. A large number of work is devoted to delete-based sentence compression. Jing (2000) presented a system that used multiple sources of knowledge to decide which phrases in a sentence can be removed. Knight and Marcu (2000) proposed statistical approaches to mimic the sentence compression process, they used both noisy-channel and decision-tree to solve the problem. McDonald (2006) presented a discriminative large-margin learning framework coupled with a feature set and syntactic representations for sentence compression. Clarke and Lapata (2006) compared different models for sentence compression across domains and assessed a number of automatic evaluation measures. Clarke and Lapata (2008) used integer linear programming to infer globally optimal compression with linguistically motivated constraints. Berg-Kirkpatrick et al. (2011) proposed a joint model of sentence extraction and compression for multi-document summarization. Filippova and Altun (2013) presented a method for automatically building delete-based sentence compression corpus and proposed an compression method which used structured prediction. Abstractive sentence compression. Abstractive sentence compression extends delete-based compression methods with additional operations, such as substitution, reordering and insertion. Cohn and Lapata (2008) proposed a discriminative tree-to-tree transduction model which incorporated a grammar extraction method and used a language model for coherent output. Galanis and Androutsopoulos (2011) presented a dataset for extractive and abstractive sentence compression and proposed a SVR based abstractive sentence compressor which utilized additional PMI-based and LDA-based features. Shafieibavani et al. (2016) proposed a word graph-based model which can improve both informativeness and grammaticality of the sentence at the same time. Neural sentence compression. Filippova et al. (2015) proposed a delete-based sentence compression system which took as input a sentence and output a binary sequence corresponding to word deletion decisions in the sentence. The model was trained on a set of 2 millions sentence pairs which was constructed by the same approach used in Filippova and Altun (2013) . There are also some neural approaches for abstractive sentence compression. Rush et al. (2015) proposed a fully data-driven approach which utilized neural language models for abstractive sentence compression. They tried different kinds of encoders to encode the input sentence into vector representation of fixed dimensions. Chopra et al. (2016) further improved the model with Recurrent Neural Networks. However, both works used vocabularies of fixed size for target sentence generation. Wubben et al. ( 2016 ) used a Seq2Seq model with bi-directional LSTMs for abstractive compression of captions. Toutanova et al. (2016) manually created a multi-reference dataset for sentence and short paragraph compression and studied the correlations between several automatic evaluation metrics and human judgment. Conclusion In this paper, we propose Operation Network, a neural approach for abstractive sentence compression, which combines the advantages of both delete-based and generate-based abstractive sentence compression. The central idea of Operation Network is to model the sentence compression process as an editing procedure. With 3 kinds of operations (delete, copy and generate), Operation Network can transform the source sentence into the condensed target. Operation Network is implemented based on the neural Seq2Seq network and pointer network, which consists of a delete decoder and a copy-generate decoder. Given a source sentence as input, first the delete decoder deletes unnecessary words from the source sentence, then new words are either generated from a large vocabulary or copied from the source sentence with the copy-generate decoder. The model is equipped with a strong ability of not only word deletion, but also word reordering and word rephrasing. Experiments show that the model outperforms the baselines. Acknowledgements",
    "abstract": "Sentence compression condenses a sentence while preserving its most important contents. Delete-based models have the strong ability to delete undesired words, while generate-based models are able to reorder or rephrase the words, which are more coherent to human sentence compression. In this paper, we propose Operation Network, a neural network approach for abstractive sentence compression, which combines the advantages of both delete-based and generate-based sentence compression models. The central idea of Operation Network is to model the sentence compression process as an editing procedure. First, unnecessary words are deleted from the source sentence, then new words are either generated from a large vocabulary or copied directly from the source sentence. A compressed sentence can be obtained by a series of such edit operations (delete, copy and generate). Experiments show that Operation Network outperforms state-of-the-art baselines.",
    "countries": [
        "China"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "8",
    "year": "2018",
    "month": "August",
    "title": "An Operation Network for Abstractive Sentence Compression"
}