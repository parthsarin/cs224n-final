{
    "article": "Directly adding the knowledge triples obtained from open information extraction systems into a knowledge base is often impractical due to a vocabulary gap between natural language (NL) expressions and knowledge base (KB) representation. This paper aims at learning to map relational phrases in triples from natural-language-like statement to knowledge base predicate format. We train a word representation model on a vector space and link each NL relational pattern to the semantically equivalent KB predicate. Our mapping result shows not only high quality, but also promising coverage on relational phrases compared to previous research. Introduction Knowledge bases (KBs) such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al., 2007) are fundamental resources for many intelligent applications. Currently, the construction and updating of KBs, directly or indirectly, rely on human labor. Keeping KBs up-to-date by humans is cost intensive and impractical. To reduce the cost and to minimize the updating latency, automatically updating a KB with knowledge extracted from natural language (NL) content is a feasible strategy. In KBs, a fact is represented by a triple (subject, predicate, object) , where subject and object are two entities in the KB, and predicate describes their relation. With an information extraction system (Carlson et al., 2010; Fader et al., 2011) , we can also extract facts from NL text in the format (np1, pattern, np2) , where np1 and np2 are two entities and pattern is the relational phrase between them. However, the extracted triples from NL text do not always follow the paradigm of KB, and that becomes a challenging issue for KB construction. For example, the NL triple (Garnett, was born in, Mauldin) extracted by ReVerb (Fader et al., 2011) shows a fact identical to the KB triple (Kevin Garnett, birthPlace, Mauldin (South Carolina)). Although these two triples state the same fact, there is a vocabulary gap between them. In the former triple, \"was born in\" is an NLlike expression and \"birthPlace\" in the latter triple is a formatted predicate used in KB. These two relational phrases are different in surface forms. They cannot be mapped by string matching directly. In addition, a KB predicate may be described in multiple NL statements. A number of ReVerb patterns such as \"is the hometown of\", \"was raised in\", and \"grew up in\" are related to the predicate \"hometown\" in DBpedia. That makes the mapping between KB and NL even more challenging. Recently, more and more works show their interests in the issue of KB construction. Knowledge graph embedding models (Bordes et al., 2013; Yang et al., 2015; Xie et al., 2016a; Xie et al., 2016b) focus on learning the vector representation on the KB side only. Previous works (Nakashole et al., 2012; Riedel et al., 2013; Dutta et al., 2015) aim to solve similar problems as ours. Nakashole et al. (2012) in their PATTY approach try to learn paraphrases to define the predicates of DBpedia, and Dutta et al. (2015) propose clustering-based approaches to transform the knowledge extracted by an open information extraction system into DBpedia paradigm. Riedel et al. (2013) propose universal schemas, which are the mapping between NL surface forms to the KB predicates, by using matrix factorization. However, all of them suffer from low coverage on relational phrases. In this work, we aim to propose a more general framework that maps relational phrases extracted from an NL resource to DBpedia predicates. Our method is capable of covering most NL patterns and KB predicates. The relational mappings can be used for a range of applications. For KB construction, the mappings can be consulted for mining the new facts from textual data written in NL. For question answering over the KB, the mappings can be used for looking up the facts in KB that are candidates for the answer. The rest of this paper is organized as follows. Section 2 describes the corpora used as NL data for learning the relational mapping. Section 3 presents our learning to map approach. In Section 4, we conduct experiments for evaluating the results. The challenging issues of this work are discussed in Section 5. Finally, Section 6 concludes this work. Linguistic Resource English Wikipedia is regarded as the NL text resource in this study. We obtain the NL relational triples from ReVerb (Fader et al., 2011) , a dataset of relational triples extracted from Wikipedia. Let an NL dataset \ud835\udc37 \ud835\udc41\ud835\udc3f be a 3-tuple (\ud835\udc43, \ud835\udc41, \ud835\udc3c \ud835\udc41\ud835\udc3f ), where \ud835\udc43 is a set of NL patterns, \ud835\udc41 is a set of entities, and \ud835\udc3c \ud835\udc41\ud835\udc3f is a set of NL triples. For example, (Garnett, was born in, Mauldin) \uf0ce \ud835\udc3c \ud835\udc41\ud835\udc3f = {(\ud835\udc5b \ud835\udc56 , \ud835\udc5d \ud835\udc58 , \ud835\udc5b \ud835\udc57 )|\ud835\udc5b \ud835\udc56 , \ud835\udc5b \ud835\udc57 \u2208 \ud835\udc41, \ud835\udc5d \ud835\udc58 \u2208 \ud835\udc43}. On the other hand, DBpedia (Auer et al., 2007) serves as our target KB. We define a KB dataset \ud835\udc37 \ud835\udc3e\ud835\udc35 as a 5-tuple (\ud835\udc45, \ud835\udc38, \ud835\udc47, \ud835\udc38 \ud835\udc47 , \ud835\udc3c \ud835\udc3e\ud835\udc35 ), where \ud835\udc45 is a set of KB predicates, \ud835\udc38 is a set of entities in KB triples, \ud835\udc47 is a set of KB entity types, \ud835\udc38 \ud835\udc47 is a set of entity-type pairs in KB, and \ud835\udc3c \ud835\udc3e\ud835\udc35 is a set of KB triples. For example, (Kevin Garnett, birthPlace, Mauldin (South Carolina)) \uf0ce \ud835\udc3c \ud835\udc3e\ud835\udc35 = {(\ud835\udc52 \ud835\udc5a , \ud835\udc5f \ud835\udc5c , \ud835\udc52 \ud835\udc5b )|\ud835\udc52 \ud835\udc5a , \ud835\udc52 \ud835\udc5b \u2208 \ud835\udc38, \ud835\udc61 \ud835\udc60 , \ud835\udc61 \ud835\udc62 \u2208 \ud835\udc47, (\ud835\udc52 \ud835\udc5a , \ud835\udc61 \ud835\udc60 ), (\ud835\udc52 \ud835\udc5b , \ud835\udc61 \ud835\udc62 ) \u2208 \ud835\udc38 \ud835\udc47 , \ud835\udc5f \ud835\udc5c \u2208 \ud835\udc45}. To resolve the entity disambiguation problem, a lexicalization dataset 1 released on the DBpedia Spotlight (Mendes et al., 2011) official website is consulted. We extract an entity-alias list from the dataset and let the alias list be \ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc4e\ud835\udc60 = {(\ud835\udc4e \ud835\udc5a1 , \ud835\udc4e \ud835\udc5a2 , \u2026 , \ud835\udc4e \ud835\udc5a|\ud835\udc34(\ud835\udc52 \ud835\udc5a )| )|\ud835\udc4e \ud835\udc5a1 , \ud835\udc4e \ud835\udc5a2 , \u2026 , \ud835\udc4e \ud835\udc5a|\ud835\udc34(\ud835\udc52 \ud835\udc5a )| \u2208 \ud835\udc34(\ud835\udc52 \ud835\udc5a )} where \ud835\udc52 \ud835\udc5a \u2208 \ud835\udc38 and \ud835\udc34(\ud835\udc52 \ud835\udc5a ) is a set of entity aliases corresponding to the KB entity \ud835\udc52 \ud835\udc5a . We further randomly sampled 10 million sentences \ud835\udc46\ud835\udc52\ud835\udc5b \ud835\udc36\ud835\udc59\ud835\udc62\ud835\udc52 from ClueWeb09 dataset 2 as an NL resource, which is considered as an auxiliary dataset for training word embedding models. Relational Mapping We propose an approach inspired by word2vec model (Mikolov et al., 2013) , i.e., Skip-gram and CBOW, to build a relational mapping. Our method projects all relational phrases, i.e., NL patterns and KB predicates, to a vector space and measures cosine similarity between relational phrases on this space. As illustrated in Figure 1 Figure 1 : Overview of our approach to relational mapping. EB : Entity Bridging with Alias Resolution Unlike KB predicates that are formatted, patterns in EB aims to capture the structural information between entities and relations, and then links relational phrases through entity bridging. Figure 2 shows how EB works. For instance, (Kobe Bryant, was born in, Philadelphia) and (Kobe Bryant, birthPlace, Philadelphia) are triples from NL and KB datasets, respectively. Through the co-occurrence of entity pairs and relational phrases, the model gradually learns the connection between \"was born in\" and \"birthPlace\". (2) where \ud835\udc63 \ud835\udc64 and \ud835\udc63 \ud835\udc64 \u2032 are the \"input\" and \"output\" embedding of word \ud835\udc64, and |\ud835\udc49| is the vocabulary size of the model. In addition, the \ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc4e\ud835\udc60 is consulted for alias resolution. For an entity em in KB training triple, its alias amx\u2208A(em) is updated by maximizing average log likelihood \u03b8alias in Equation (3). \ud835\udf03 \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc4e\ud835\udc60 = 1 |\ud835\udc34(\ud835\udc52 \ud835\udc5a )| \u2211 log \ud835\udc5d(\ud835\udc4e \ud835\udc5a\ud835\udc65 |\ud835\udc52 \ud835\udc5a ) \ud835\udc4e \ud835\udc5a\ud835\udc65 \u2208\ud835\udc34(\ud835\udc52 \ud835\udc5a ) (3) where \ud835\udc5d(\ud835\udc4e \ud835\udc5a\ud835\udc65 |\ud835\udc52 \ud835\udc5a ) is computed by using the softmax function ( 2 ). The trained model results in an embedding space, where NL patterns and KB predicates are represented as vectors in this space. Thus, the similarity between an NL pattern and a KB predicate can be measured by their cosine similarity. On the one hand, most similar KB predicates of an NL pattern can be considered as its mapping targets. On the other hand, most similar NL patterns of a KB predicate can be regarded as its mapping patterns. In Section 4, triple linking task and human verification task will evaluate the results from these two aspects, respectively. DR: Decomposing Relational Phrases and Introducing Additional NL Text EB may suffer from data sparseness because each relational phrase is treated as a distinct symbol, and information from the words that compose a relational phrase is completely ignored. For example, the meaning of predicate \"birthPlace\" can be captured from words \"birth\" and \"place\". Thus, DR is proposed and integrated with EB for leveraging the words decomposed from relational phrases. More clearly, given a KB predicate \ud835\udc5f \ud835\udc5c , the word semantics of \ud835\udc5f \ud835\udc5c will be jointly learned by maximizing the average log probability \u03b8compose as Equation (4). \ud835\udf03 \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc52 = log \ud835\udc5d(\ud835\udc5f \ud835\udc5c |\ud835\udc50 \ud835\udc5c1 , \ud835\udc50 \ud835\udc5c2 , \u2026 , \ud835\udc50 \ud835\udc5c\ud835\udc5b ) (4) where \ud835\udc50 \ud835\udc5c1 , \ud835\udc50 \ud835\udc5c2 , \u2026 , \ud835\udc50 \ud835\udc5c\ud835\udc5b are words decomposed from \ud835\udc5f \ud835\udc5c and \ud835\udc5b is the number of the words. The computation of the probability is done by the softmax function (2) and a composition function that simply averages the vectors of words \ud835\udc50 \ud835\udc5c1 , \ud835\udc50 \ud835\udc5c2 , \u2026 , \ud835\udc50 \ud835\udc5c\ud835\udc5b . Figure 3 illustrates how DR encodes words decomposed from relational phrases and jointly learns the meaning of relational phrases. The advantage of DR is that the semantics of a predicate is expanded by its compositional words, which are written in NL. For example, the word meaning of KB predicate \"birthPlace\" will also be considered from words \"birth\" and \"place\" and the word meaning of NL pattern \"was born in\" will similarly be viewed from \"was\", \"born\" and \"in\". Relational phrases that are semantically related might consist of words with similar meaning such as \"birth\" and \"born\" in this example. In other words, we reduce the data sparseness of relational phrases by connecting patterns and predicates through the decomposed words. In addition to the information from the KB, we add additional NL statements \ud835\udc46\ud835\udc52\ud835\udc5b \ud835\udc36\ud835\udc59\ud835\udc62\ud835\udc52 to the training set. These statements serve as an auxiliary resource that aids to model the meaning of the decomposed words. In other words, we co-train a distributed word model in the same vector space. In the training process, the decomposed words play a role of bridges that connect similar relational phrases. The natural language statements provide semantic information for those words. TF: Filter Relational Mapping by Argument Types of Relational Phrases We further filter the mapping results with argument type constraints. That is, the argument type of an NL pattern should be consistent with those of its corresponding KB predicates. The mappings with inconsistent argument types are removed from the relational mapping list. We obtain the argument type constraint for each relational phrase by voting with training triples. More formally, for each KB triple (\ud835\udc52 \ud835\udc5a , \ud835\udc5f \ud835\udc5c , \ud835\udc52 \ud835\udc5b ) \u2208 \ud835\udc3c \ud835\udc3e\ud835\udc35 , \ud835\udc52 \ud835\udc5a \u2208 \ud835\udc61 \ud835\udc60 , \ud835\udc52 \ud835\udc5b \u2208 \ud835\udc61 \ud835\udc62 the argument type for \ud835\udc5f \ud835\udc5c is voted as (\ud835\udc61 \ud835\udc60 , \ud835\udc61 \ud835\udc62 ) . We count the majority argument type for each predicate and define the argument type constraint of \ud835\udc5f \ud835\udc5c as \ud835\udc34\ud835\udc5f\ud835\udc54 \ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52(\ud835\udc5f \ud835\udc5c ) = (\ud835\udc61 \ud835\udc60 , \ud835\udc61 \ud835\udc62 ). In NL side, unlike the entity types defined in KB datasets, we have to find the types of entities extracted from natural language sentences. We obtain the type of each NL entity by matching it with KB entity aliases. Then, we vote argument types of NL patterns with NL triples in the similar way as above. If an entity \ud835\udc5b \ud835\udc56 has \ud835\udc50 possible types, each type of the entity \ud835\udc5b \ud835\udc56 will be weighted equally by 1 \ud835\udc50 . If \ud835\udc50 = 0, the triple will not contribute to argument type determination. In this way, we generate argument type constraints for NL patterns. Let \ud835\udc34\ud835\udc5f\ud835\udc54 \ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52(\ud835\udc5d \ud835\udc58 ) = (\ud835\udc61 \ud835\udc51 , \ud835\udc61 \ud835\udc53 ) be the type constraint of the corresponding pattern \ud835\udc5d \ud835\udc58 for some \ud835\udc61 \ud835\udc51 , \ud835\udc61 \ud835\udc53 \u2208 \ud835\udc47. Experiments Dataset and Experiment Setting: The statistics of NL and KB datasets show that |\ud835\udc3c \ud835\udc41\ud835\udc3f | = 407,239, |\ud835\udc43| = 100,264, |\ud835\udc3c \ud835\udc3e\ud835\udc35 | = 14,408,940, and |\ud835\udc45| = 662. Comparatively, even though the size of KB triple set is much larger than NL triple set, KB predicates are formatted and there are only 662 predicates in KB dataset. We randomly split our dataset into five folds and conduct five-fold cross validation for our experiment. We train 300-dimensional vector models with the proposed methods and analyze the mapping results. The training parameters of our model, i.e., (window size, negative sample, min count), are set to (5, 10, 0), respectively. The relational phrases with less than 5 occurrences in ReVerb and DBpedia are excluded from the mapping. After filtering, our approach builds a relational mapping that covers 7,361 of 9,171 frequent ReVerb patterns with 629 of 634 frequent DBpedia predicates. Compared to previous works, Dutta et al. (2015) cover 212 ReVerb patterns and 41 DBpedia predicates, Riedel et al. (2013) cover 100 Freebase facts in training and test data, and Nakashole et al. (2012) cover 225 predicates, we generate a higher coverage of relational mapping. Evaluation: Because of lack of ground truth, most previous work evaluates their relational mapping result by human annotation only. In this work, we try to evaluate our approach from two perspectives. Firstly, we conduct a triple linking task that aims to link NL triples to the KB triples sharing the same knowledge. It reflects the ability of our mapping approach to translate relational phrases from NL side to KB paradigm. Secondly, we further evaluate the performance of our mapping by human verification. It demonstrates the result and the accuracy of NL patterns that link to each KB predicate. Triple linking task: This task simulates knowledge base construction. The KB test set is considered as new facts. The word embedding model learned from the KB training set builds a relational mapping between NL patterns and KB predicates. Through the mapping, we can add new knowledge into the KB by translating NL triples to KB triples. We judge the correctness of translation by checking if the translated triple is actually in the test data. Formally, given an NL triple (\ud835\udc5b \ud835\udc56 , \ud835\udc5d \ud835\udc58 , \ud835\udc5b \ud835\udc57 ) , \ud835\udc5b \ud835\udc56 \u2208 \ud835\udc34(\ud835\udc52 \ud835\udc5a ), \ud835\udc5b \ud835\udc57 \u2208 \ud835\udc34(\ud835\udc52 \ud835\udc5b ) , we have to predict a KB predicate\ud835\udc5f \ud835\udc5c such that (\ud835\udc52 \ud835\udc5a , \ud835\udc5f \ud835\udc5c , \ud835\udc52 \ud835\udc5b ) is the corresponding fact in KB test set. Thus, we generate NL test triples \u039b \ud835\udc41\ud835\udc3f and ground truth \u039b \ud835\udc3e\ud835\udc35 as follows. \u039b NL is a set of triples (\ud835\udc5b \ud835\udc56 , \ud835\udc5d \ud835\udc58 , \ud835\udc5b \ud835\udc57 ) \u2208 \ud835\udc3c \ud835\udc41\ud835\udc3f and \u039b KB is a set of triples (\ud835\udc52 \ud835\udc5a , \ud835\udc5f \ud835\udc5c , \ud835\udc52 \ud835\udc5b ) \u2208 \ud835\udc3c \ud835\udc3e\ud835\udc35 , where \ud835\udc5b \ud835\udc56 \u2208 \ud835\udc34(\ud835\udc52 \ud835\udc5a ), \ud835\udc5b \ud835\udc57 \u2208 \ud835\udc34(\ud835\udc52 \ud835\udc5b ) . In this way, we derive |\u039b \ud835\udc41\ud835\udc3f | = 54,752 and |\u039b \ud835\udc3e\ud835\udc35 | = 58,504. Due to the low coverage on relational phrases, the results of Riedel et al. (2013) and Dutta et al. (2015) cannot be directly compared with ours under this task. Thus, we built a baseline model through a counting-based approach that counts the co-occurrence between each pattern and each predicate found in the same or alias entity pairs. The baseline model always selects the majority. TransE (Bordes et al., 2013) focuses on learning the vector representation in KB side only. Although it solves a problem different from ours, we also adapt it to this task. We train a model with NL and KB triples. Given an NL triple (\ud835\udc5b \ud835\udc56 , \ud835\udc5d \ud835\udc58 , \ud835\udc5b \ud835\udc57 ),the TransE model is trained to optimize the equation\ud835\udc5b \ud835\udc56 + \ud835\udc5d \ud835\udc58 \u2248 \ud835\udc5b \ud835\udc57 , so we can predict a KB predicate \ud835\udc5f \ud835\udc5c through the entity operation \ud835\udc5b \ud835\udc57 \u2212 \ud835\udc5b \ud835\udc56 . We denote this operation as TransE(entity). We can also rank mapping candidates of \ud835\udc5d \ud835\udc58 by cosine similarity of NL patterns and KB predicates. We denote it as TransE(rel). We calculate hit@k and MRR to measure the performance. Hit@k indicates the percentage of NL triples where correct relations can be found in the top k positions. In the special case where k=1, hit@1 is equivalent to P@1 (Precision at 1). MRR is the mean reciprocal rank of correct mapping and is calculated to the 100 position. As shown in Table 2 , counting-based baseline suffers from the low translation rate on relational phrases, i.e., no suitable mapping can be applied, and its hit@k shows no difference when k is larger than 5. By contrast, though TransE(entity) and TransE(rel) do not have such a problem, all our methods outperform them. Besides, DR expands the meaning of relational phrases by decomposed words. Although hit@k drops at k=1, the performance shows large improvement when k is larger than 5. That indicates the effectiveness of semantic information provided by decomposed words and the additional natural language statements. TF shows the strength of type filter. EB+DR+TF even achieves 0.800 and 0.327 of hit@20 and MRR, respectively, in this task. hit@1 hit@5 hit@10 hit@20 MRR baseline 0. The results of human verification are shown in Table 3 . The hit@1, hit@3, and hit@5 of the most frequent 50 and 100 predicates, respectively, are reported. This results further confirm the quality of our mapping from another aspect. Some NL patterns are mapped to KB predicates that have exactly the same meaning, e.g., \"birthplace\" and \"was born in\" is counted a correct mapping. Some NL patterns do not have exactly the same meaning with the KB predicates, but they can be inferred. For example, the pattern \"is a fantasy novel by\" infers the predicate \"author\". Thus, we regard them as correct mapping. Besides, there are some incorrect mapping examples, such as pattern \"was born in\" and predicate \"residence\". The predicate \"residence\" indicates a place a person live in. hit@1 hit@3 hit@5 Top 50 Predicates 0.352 0.480 0.528 Top 100 Predicates 0.326 0.456 0.510 Table 3 : Evaluation results of human verification task. Discussion We find four major types of errors in our mapping: Complex concept: Some KB predicates containing complex concept are difficult to map accurately. For instance, the KB predicate \"leftTributary\" contains not only the relational expression \"tributary\", but also the concept \"left\". In this case, NL patterns such as \"is a tributary of\", \"is a tributary to\", and \"is a river in\" can only capture partial phrase meaning, and they are regarded as inaccurate mapping. The complex concept would be better modeled by decomposing it to multiple simple concepts. Uncommon in NL sentences: Some KB predicates such as \"youthWing\" and \"varietals\" are uncommon in NL sentences, so that there are insufficient instances available for training. As a result, DR may not perform well since it aims to capture semantic information from NL sentences. Fortunately, most uncommon predicates are less important in general domain. For a specific-purpose application, the in-domain corpus can be used to train a dedicated relational mapping. Similar context: Word embedding models learn word meaning through NL context. However, some antonyms share similar context in NL, and they may have small cosine distance in the vector space. For instance, the NL patterns \"is a large town in\" and \"is a small town in\" are hard to distinguish. This issue has been addressed in other applications that use word embedding models. Multiple meanings: Some relational phrases have multiple meanings and only parts of the meanings are used in KB. This leads to wrong relational mapping. For example, the word \"billed\" has several meanings, but the predicate \"billed\" is only applied to the state that a wrestler comes from, such as (A-1(wrestler), billed, Niagara Falls (Ontari)). One of possible solutions to this issue is performing word sense disambiguation (WSD) on the corpus, and learning the relational mapping at the sense level, instead of at the word level. Conclusion Relational mapping is a challenging problem. This paper proposes an approach that provides a quality mapping in terms of coverage and correctness. This method is also unsupervised and is not restricted to a specific KB. It is easy to apply to different data resources for various applications such as KB construction and question-answering. Because the ground truth is not available, we also propose a triple linking task. The trask provides an automatic and scalable evaluation for relational mapping. In the end, we suggest some research directions for improving the proposed approach in the future. DR encodes words decomposed from relational phrases by a compositional function that simply averages the embeddings of these words. The joint learning model may handle the compositionality better in phrase embedding learning. TF considers entity type information by filtering the relational mapping with argument type constraints. Embedding model learns type information while training knowledge embedding may be explored further. Acknowledgements This research was partially supported by Ministry of Science and Technology, Taiwan, under grants MOST-105-2221-E-002 -154 -MY3, MOST-106-2923-E-002 -012 -MY3 and MOST-107-2634-F-002-011-, and Academia Sinica under grant AS-107-TP-A05.",
    "abstract": "Directly adding the knowledge triples obtained from open information extraction systems into a knowledge base is often impractical due to a vocabulary gap between natural language (NL) expressions and knowledge base (KB) representation. This paper aims at learning to map relational phrases in triples from natural-language-like statement to knowledge base predicate format. We train a word representation model on a vector space and link each NL relational pattern to the semantically equivalent KB predicate. Our mapping result shows not only high quality, but also promising coverage on relational phrases compared to previous research.",
    "countries": [
        "Taiwan"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "2",
    "year": "2018",
    "month": "May",
    "title": "Learning to Map Natural Language Statements into Knowledge Base Representations for Knowledge Base Construction"
}