{
    "article": "Although FrameNet is recognized as one of the most fine-grained lexical databases, its coverage of lexical units is still limited. To tackle this issue, we propose a two-step frame induction process: for a set of lexical units not yet present in Berkeley FrameNet data release 1.7, first remove those that cannot fit into any existing semantic frame in FrameNet; then, assign the remaining lexical units to their correct frames. We also present the Semi-supervised Deep Embedded Clustering with Anomaly Detection (SDEC-AD) model-an algorithm that maps high-dimensional contextualized vector representations of lexical units to a low-dimensional latent space for better frame prediction and uses reconstruction error to identify lexical units that cannot evoke frames in FrameNet. SDEC-AD outperforms the state-of-the-art methods in both steps of the frame induction process. Empirical results also show that definitions provide contextual information for representing and characterizing the frame membership of lexical units. Introduction A semantic frame is a conceptual structure that models a type of situation, entity, or event (Ruppenhofer et al., 2006) . Frame semantics is useful for inference-based natural language processing tasks such as question answering (Shen and Lapata, 2007 ), text summarization (Han et al., 2016) , and information extraction (Moschitti et al., 2003; Barzdins, 2014) . A widely-used frame semantic resource is Berkeley FrameNet (Baker et al., 1998) , whose current release 1.7 (BFN 1.7) contains 1224 hierarchically-related semantic frames and 13,669 lexical units (LUs). An LU is a pair of word lemma and the semantic frame it evokes. For example, the LU abandon.v falls under the Abandonment frame, which describes an agent leaving behind an object and rendering such object no longer within one's control. The lexical coverage of BFN 1.7 is low compared to other semantic lexical resources such as WordNet (Miller, 1995) , which contains 210,000 entries, as FrameNet is built entirely manually by linguistic experts. Expanding FrameNet automatically is challenging because of the high number and uneven granularity of semantic frames (Rastogi and Van Durme, 2014) , and polysemous lemmas such as shoot.v, which can be assigned to multiple frames such as Hit target and Ingest substance. The NOTR-LU (lexical unit with no training data) coverage gap (Palmer and Sporleder, 2010) , where 24% of LUs in BFN 1.7 lack example sentences, further complicates the challenge as the state-of-the-art frame induction methods (Anwar et al., 2019; Arefyev et al., 2019; Ribeiro et al., 2019) require example sentences featuring the LUs to create vector representations in the semantic space and induce frames. At the same time, current research (Pennacchiotti et al., 2008; Johansson, 2014; Pavlick et al., 2015; Materna, 2012; Rastogi and Van Durme, 2014; Ustalov et al., 2018) assumes that an unknown LU-a generic lexical unit not existing in the FrameNet database-can be characterized by one or more frames existing in BFN 1.7. This as- ). An anomalous LU is an LU that cannot be assigned to any semantic frame in BFN 1.7, whereas a normal LU is the exact opposite: it can induce a frame in BFN 1.7. sumption is unrealistic given the limited coverage of semantic frames (Palmer and Sporleder, 2010) . Rastogi and Van Durme (2014) mention that one of the missing frames is P rogramming, which would contain LUs such as code.v and program.v, and which would feature frame elements for the programmer, the programming language, and the purpose of the program. Current models would have assigned code.v to other semantically related frames in BFN 1.7 such as Creating, which is problematic because LUs in those frames exhibit different lexicographic behaviors. For instance, in the Creating frame, the programming language is the frame element INSTRUMENT. Its valence pattern will be \"LU -[CREATED ENTITY] -[in INSTRUMENT]\", which is exemplified by the sentence \"I coded [Facebook] CREATED ENTITY [in Python] INSTRUMENT \". However, none of the lexical units under the Creating frame share the same syntactic realization. The closest is \"LU -[CREATED ENTITY] -[with INSTRUMENT]\". In a departure from previous work on frame induction (see Section 2.1 for the review of previous work), we propose a two-step process to assign an unknown LU to its correct frame (see Figure 1 ). First, we decide whether any existing frame in BFN 1.7 can characterize the sense of an unknown LU. We cast this step as anomaly detection, where we refer to an LU that does not belong to any semantic frame in BFN 1.7 as an anomalous LU. The converse is a normal LU that can be assigned to a frame in BFN 1.7. The subsequent step is to use the sense information to assign the normal LU to its frame. The experimental results demonstrate that by mapping the high-dimensional contextualized representations of normal LUs to a low-dimensional latent space and learning to reconstruct the representations, our Semi-supervised Deep Embedded Clustering with Anomaly Detection (SDEC-AD) model outperforms all baseline models in filtering out anomalous LUs and inducing frames for normal LUs. We also show that we can generate embeddings to represent LUs that lack annotated sentences, which addresses the NOTR-LU coverage gap that hinders frame induction. Our contributions are three-fold: 1. we propose representing LUs that lack example sentences using their definitions so they can participate in the two-step frame induction workflow, 2. our autoencoder-based model (SDEC-AD) is the first algorithm that detects LUs that cannot be classified into any frame in the FrameNet database, preserving the consistency of the inventory of frames and LUs, 3. we are the first to apply the deep-embedded clustering algorithm to induce semantic frames, achieving stateof-the-art performance. Related Work Semantic Frame Induction Semantic frame induction is the task of labeling an unknown LU with a correct frame. Some (Pennacchiotti et al. (2008) ; Tonelli and Pianta (2009) ; Green et al. (2004) ) rely on additional semantic information from other complementary lexical resources such as WordNet (Miller, 1995) to induce frames. Pavlick et al. (2015) and Rastogi and Van Durme (2014) use The Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to paraphrase frame-annotated sentences and assign the paraphrased LUs to their frames. On the other hand, instead of augmenting FrameNet with other lexical resources, Materna (2012) , Modi et al. (2012) , and Ustalov et al. (2018) use semantic role information to induce frames for verb lemmas. Recognizing that unsupervised methods can induce frames better for unseen data than supervised methods, Qasem-iZadeh et al. (2019) aimed to benchmark unsupervised systems that assign verb lemmas to their semantic frames without using any explicit semantic annotation. They presented the task of Unsupervised Lexical Frame Induction in the International Workshop on Semantic Evaluation in 2019, and all the top three performing systems (Arefyev et al., 2019; Anwar et al., 2019; Ribeiro et al., 2019) employed a distributional approach: the systems cluster verb lemmas, which are represented as contextualized vectors computed over their example sentences by pretrained language models, in a semantic space. The best system (Arefyev et al., 2019) We are uncertain whether the three systems can reproduce frame induction success with the LUs in BFN 1.7, as the LUs in BFN 1.7 have parts of speech other than verb, and example sentences for many LUs are unavailable. Moreover, all three models directly cluster on the high-dimensional vector representations of LUs, and all report sensitivity to the choice of hyperparameters, particularly the number of clusters. In this paper, we fix the number of clusters as the number of existing frames in BFN 1.7, and we compare the state-of-the-art frame induction models with a deep clustering algorithm that uses a lowdimensional latent space to produce better clusters of highdimensional data points. Deep Clustering Deep clustering is a type of clustering that uses a deep neural network to learn dense feature representations that favor a clustering task. When the dimensionality of the input feature space is very high, similarity metrics used by traditional clustering algorithms such as k-means and hierarchical methods become unreliable, which renders the direct clustering of the input embeddings ineffective (Guo et al., 2017) . In contrast, deep clustering algorithms learn the representations in a low dimensional, clustering-friendly feature space. Xie et al. (2016) put forth the deep embedded clustering (DEC) algorithm-one of the most representative methods for deep clustering-that jointly learns the feature representations and the clustering assignments. To improve DEC's clustering performance, Ren et al. (2019) propose a semi-supervised version of DEC (SDEC), which incorporates pairwise constraints in the feature learning process such that data points in the same cluster become closer, and the incorrect cluster assignments can be adjusted by the existing information about the data. We propose applying SDEC to the frame induction task to overcome the \"curse of dimensionality\" (Bellman, 1966) of the high-dimensional contextualized representations of LUs. We also augment SDEC to detect LUs that cannot fit into any frame in BFN 1.7 as SDEC uses an autoencoder structure that excels at anomaly detection (see Section 2.3) Anomaly Detection using Autoencoder An autoencoder is an unsupervised learning algorithm that learns to reconstruct its input using a deep neural network. Its network structure consists of an encoder and a decoder. The encoder maps the original input vector to a hidden representation lower in dimensionality, and the decoder maps the hidden representation back to the original input space. The difference between the original input vector and the reconstructed vector is known as the reconstruction error. An autoencoder learns to minimize this reconstruction error such that the autoencoder approximates an identity function (Ng, 2010) . After the autoencoder is trained to reconstruct data without anomalies, the reconstruction error for anomalies is high (An and Cho, 2015) , which enables anomaly detection. Autoencoders have been applied across the natural language processing domain to detect anomalies, such as web attacks (Vartouni et al., 2018) , SMS spams (Al Moubayed et al., 2016) and even novel sport ideas (Mei et al., 2018) . In our task, the anomalies are the LUs that cannot evoke any semantic frame in BFN 1.7. encoder decoder KL Loss Figure 2 : Framework of our proposed method (SDEC-AD). Proposed Method Consider the problem of assigning a set of LUs that do not exist in BFN 1.7 to k frames. Here, k is the number of frames in BFN 1.7, and the LUs are embedded by language models in the vector space X. First, we remove the subset of anomalous LUs that cannot be described by any existing frame in BFN 1.7. Next, we group the n remaining normal LUs {x i \u2208 X} n i=1 into k clusters where each cluster is represented by a centroid {\u00b5 j } k j=1 and corresponds to a semantic frame. This is a hard clustering task-each LU with a unique identifier is only assigned to one frame. Hereafter, normal LUs refer to both unknown LUs that can induce semantic frames from BFN 1.7 or LUs that already exist in BFN 1.7, unless otherwise specified. We propose the Semi-supervised Deep Embedded Clustering with Anomaly Detection (SDEC-AD) model that jointly identifies anomalous LUs and assigns normal LUs to their frames 1 . For n normal LUs embedded in the vector space X, SDEC-AD uses an autoencoder to represent the semantic features of normal LUs in a low-dimensional latent space Z using a non-linear transformation f \u03b8 : X \u2192 Z (where \u03b8 are the learnable parameters of the encoder). After embedding the normal LUs in the latent space Z, SDEC-AD uses frame information about the LUs existing in BFN 1.7 to cluster the normal LUs (including the unknown LUs), which is the semi-supervised frame induction process. SDEC-AD also learns to reconstruct normal LUs from the latent space X to its original vector space X by minimizing their reconstruction error. Since SDEC-AD is not exposed to anomalous LUs during the training phase, SDEC-AD can identify anomalous LUs that have high reconstruction error. Figure 2 illustrates that SDEC-AD has a deep encoderdecoder architecture. First, the encoder layers learn to encode the input embedding x into a low-dimensional latent representation z, and the decoder layers learn to reconstruct z back to the original embedding (top part of Figure 2 ). The reconstructed embedding is x. The encoder layers are further trained to cluster the latent representations under the supervision of the pairwise constraints A (center part of Figure 2 ). The number of clusters is the number of lexical frames in BFN 1.7. By minimizing the Kullback-Leibler divergence clustering loss (KL loss) and the constraint loss, SDEC-AD jointly learns to represent and cluster x in the latent space. Finally, the decoder layers are retrained to reconstruct x from z by minimizing the reconstruction error (bottom part of Figure 2 ). The encoder layers (grey area), including the latent hidden representations, are frozen to prevent updating their parameters. Notice that we retrain the decoder layers of SDEC-AD (lower part of Figure 2 ) after SDEC-AD learns to embed and cluster normal LUs in the latent space Z (center part of Figure 2 ). The reason is that after the encoder layers learn to embed semantic features of normal LUs in the latent space Z (Xie et al., 2016) , the reconstruction error of anomalous LUs becomes more distinguishable than that of normal LUs. Parameter Initialization We initialize SDEC-AD with fully-connected stacked autoencoders-each layer is a denoising auto-encoder trained to reconstruct the previous layer's output after random corruption. The structure of the stacked autoencoders is d-7500-1000-7500-d, where d is the dimension of the vector representations of LUs. We use the same parameter settings and nonlinear activation functions as SDEC (Ren et al., 2019) . The encoder in SDEC-AD receives the vector representations of LUs {x i \u2208 X} n i=1 as input and returns their feature representations {z i \u2208 Z} n i=1 in the latent space Z as outputs. We then employ supervised initialization (See section 3.2) to obtain k initial centroids {\u00b5 j } k j=1 in space Z, where k is the number of frames in BFN 1.7. Supervised Initialization We apply the \"exploit and explore\" mechanism (Lemaire et al., 2015) by first exploiting the information about the frame labels of normal LUs to initialize the centroids before we start clustering the normal LUs. We define each centroid {\u00b5 j } k j=1 as the average of the vectors (in the latent space Z) of normal LUs that share the same semantic frame. Note that we only use the normal LUs already present in BFN 1.7 to initialize the centroids. Clustering with KL Divergence and Pairwise Constraints We use the objective function of SDEC (Ren et al., 2019) to train SDEC-AD on clustering the latent feature representations of normal LUs {z i \u2208 Z} n i=1 . The objective function is L = L u + \u03bbL s (1) where L u is the unsupervised Kullback-Leibler (KL) divergence clustering loss, L s is the semi-supervised constraint loss, and \u03bb is the parameter that controls the degree of supervision. SDEC-AD learns the latent representations that favor the clustering of normal LUs by minimizing L u . It treats the centroids {\u00b5 j } k j=1 as trainable weights and assigns each embedded latent point z i to a soft label q i by Student's tdistribution: q ij = (1 + z i \u2212 \u00b5 j 2 ) \u22121 j (1 + z i \u2212 \u00b5 j 2 ) \u22121 (2) where q ij represents the probability of z i belonging to cluster j. L u is then defined as L u = KL(P Q) = i j p ij log( p ij q ij ) ( 3 ) where Q is the soft assignment and P is the target distribution, defined as p ij = q 2 ij / i q ij j (q 2 ij / i q ij ) (4) At the same time, SDEC-AD minimizes the semisupervised constraint loss L s to move normal LUs with the same frames closer and normal LUs with different frames more apart. To calculate L s , SDEC-AD requires a matrix A that describes must-link and cannot-link pairwise constraints of LUs. The matrix A is defined as: (5) A = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 For must-link constraints, when two LUs x i and x k share the same frame, a ik = 1. On the other hand, when the two LUs are in two different frames, they satisfy the cannotlink constraint, so a ik = \u22121. The constraint loss L s is then defined as: L s = 1 n n i=1 n k=1 a ik z i \u2212 z k 2 (6) We only consider must-link and cannot-link constraints for normal LUs that exist in BFN 1.7; in other words, SDEC-AD is trained in a semi-supervised manner where it uses the frame label information for normal LUs in BFN 1.7 to cluster unknown normal LUs. Retraining of Decoders for Anomaly Detection After training the encoder of SDEC-AD to embed and cluster normal LUs in the latent space Z, we freeze the encoder and train the decoder to map the latent representations of LUs {z i \u2208 Z} n i=1 back to their original representations {x i \u2208 X} n i=1 . The objective function here is the squared reconstruction error, defined as L \u03b8 (x i ; xi ) = 1 n n i=1 (x i \u2212 x i ) 2 (7) where x i is the original representation and xi is the reconstructed representation of a normal LU. The error represents the semantic-wise difference between the original and reconstructed LU. Minimizing the reconstruction error of normal LUs makes SDEC-AD learn to capture and reconstruct semantic features of normal LUs. Therefore, the reconstruction error of normal LUs is lower than that of anomalous LUs because SDEC-AD \"recognizes\" normal LUs. A threshold \u03c4 can be chosen such that when the reconstruction error of an LU is above \u03c4 , the LU is considered anomalous. Optimization We use the stochastic gradient descent (SGD) and backpropagation to optimize both the centroids {\u00b5 j } k j=1 and the parameters of the encoder and decoder layers of SDEC-AD. During backpropagation, SDEC-AD passes the gradient \u2202L \u2202zi down the encoder layers to update their parameters \u03b8, which are used to perform the nonlinear transformation f \u03b8 . At the same time, the gradient \u2202L \u2202\u00b5i updates the centroids {\u00b5 j } k j=1 . Similarly, the gradient Normal lexical units refer to lexical units that can be assigned to a semantic frame in FrameNet 1.7, whereas anomalous lexical units cannot. Experimental Setup Datasets Similarly to Pennacchiotti et al. (2008) , our gold standard for the induction task is the BFN 1.7 database. 2 We retrieved 19770 unknown normal LUs from FrameNet+ (FN+) database (Pavlick et al., 2015) after we preprocessed it by removing frames such as M ake possible to do and Containment relation that do not exist in BFN 1.7. Every unknown normal LU has an example sentence and can be assigned to an existing frame in BFN 1.7. To assess the ability of the models in discriminating anomalous LUs, we hand-picked 300 WordNet synsets according to the following criteria. First, we removed the lexical entries in WordNet (Miller, 1995) that already exist in Berkeley FrameNet 1.7. Subsequently, we selected 300 lexical units which cannot be represented by any semantic frame in Berkeley FrameNet 1.7 (Baker et al., 1998) based on the frame definitions and the lexicographic properties of the existing lexical units in the frames. 218 of the anomalous lexical units are adjective satellites and the rest are adjectives. 90 of the chosen lexical units have example phrases and/or sentences. 3 . Implementations Vector Representations of Lexical Units We use the BERT (bert-base-uncased) (Devlin et al., 2019) and ELMo (Peters et al., 2018) language models to generate the contextualized word embeddings to represent LUs. Both BERT and ELMo embeddings can capture the semantic context and achieve state-of-the-art result in inducing frames (Arefyev et al., 2019; Anwar et al., 2019; Ribeiro et al., 2019) . We experimented with representing LUs from BFN 1.7 with and without information from their definitions. With-out including the definition, the vector representation of an LU is its contextualized word embedding based on its example sentences. To infuse information from the definition into the LU representation, we first remove the stopwords from the definition, and create the definition embedding by averaging embeddings of all word tokens in the definition. Finally, we represent the LU by adding the definition embedding to the contextualized representation generated from its example sentences. If an LU lacks example sentences, its representation is the definition embedding. We evaluate whether definition-infused contextualized representations improve frame induction using LUs from BFN 1.7 that have at least one example sentence. Since results (see Section 5.1) confirm that definitions improve semantic frame induction, we represent all LUs (including WordNet synsets) with their definitions and example sentences (if exist) for the frame induction (Section 5.2) and anomaly detection (Section 5.3) experiments. The exceptions are the LUs from FN+ as they lack definitions, so only their example sentences are used to generate the contextualized representations. Evaluation of Proposed Model We evaluate the frame-induction performance of SDEC-AD on two datasets: LUs that only come from BFN 1.7 and LUs that come from both BFN 1.7 and FN+. As mentioned in Section 3.3, the pairwise constraint matrix A is created using the existing LUs in BFN 1.7. We independently run SDEC-AD 20 times and report the average results. For anomaly detection, we train SDEC-AD only with LUs existing in BFN 1.7, and we measure its ability to discriminate anomalous LUs from normal LUs on the combined dataset of normal LUs from BFN 1.7, unknown normal LUs from FN+, and anomalous WordNet synsets. Baselines To evaluate the effectiveness of our SDEC-AD model, we apply the winning models benchmarked in SemEval-2019 Task 2: Unsupervised Lexical Frame Induction (Qasem-iZadeh et al., 2019) \u2022 Anwar et al. ( 2019 ): The authors run the agglomerative clustering algorithm with Manhattan affinity and single linkage on the LU representations. Differing from the original implementation, we fix the number of clusters, which is a hyperparameter, as the number of unique frames in the dataset. \u2022 Arefyev et al. ( 2019 ): First, the authors run an agglomerative clustering algorithm with cosine affinity and average linkage on the LU embeddings. They perform a grid search to find the optimal number of clusters. Then, they use language models such as BERT and ELMo to generate substitutes for the LUs using the example sentences, and they build TFIDF BoW vectors for the substitutes of each cluster. Finally, they use the agglomerative clustering algorithm to split each cluster of LUs into two clusters of their substitutes. In addition to using the three models as baselines for frame induction for normal LUs, we adapt the three models to the anomaly detection task using the distancebased approach: an LU is considered anomalous when its distance-measured by different distance metrics used by different models-to the closest clusters (in the case of agglomerative clustering) or the closest LU (in the case of graph clustering) is above a certain threshold value \u03c4 (Satari et al., 2019; Akoglu et al., 2015) . We use a random classifier as a baseline model for the anomaly detection task to simulate the process of randomly classifying an LU as anomalous or normal. The probability of random classification is 1%, which is the ratio of our hand-picked anomalous LUs to the unknown LUs. We assess the baseline models' performance in frame induction and anomaly detection similarly for the evaluation of SDEC-AD (see Section 4.2.2). Evaluation Metrics Similarly to QasemiZadeh et al. ( 2019 ), we report the models' performance in identifying LUs that evoke the same frame using two measures for evaluating text clustering techniques: the harmonic mean of Purity and inverse-Purity (PIF) and the harmonic mean of BCubed precision and recall (BCF). We frame the task of distinguishing anomalous LUs as an anomaly detection task, and we use the area under the receiver operating characteristic curve (AUC ROC) and the area under the precision-recall curve (AUC PRC) as the performance metrics. Since SDEC-AD and the baseline models produce anomaly scores-the reconstruction error and distances between LUs or clusters-instead of binary labels, and we do not know the best anomaly threshold \u03c4 , AUC ROC and AUC PRC are desirable metrics as they are threshold-invariant: they measure the quality of a model's predictions of anomalous LUs irrespective of the anomaly threshold \u03c4 (Chen et al., 2016) . Results and Discussion Our experiments differ from the SemEval-2019 Task 2 (QasemiZadeh et al., 2019)-where the baseline models (Ribeiro et al., 2019; Anwar et al., 2019; Arefyev et al., 2019) are initially designated for-by two aspects: (1) the LUs in our dataset (see Section 4.1) are not only restricted to verb lemmas, and (2) SemEval-2019 Task 2 did not require the models to identify anomalous LUs. Note that the performance results of baseline models shown in Table 2 , Table 3 , Table 4 and Figure 3 originate from our empirical study and not from their reported figures in SemEval-2019 Task 2. Effects of Definitions on Representations of Lexical Units When example sentences and definitions form the hybrid contextualized representations of lexical units (LUs), more LUs with the same semantic frame are clustered together (see Table 2 and Table 3 ). The reason is that the definition contains information enough to aid in the identication of a concept or a semantic frame (Orfan and Allen, 2013; Spiliopoulou and Hovy, 2019) , so the additional semantic context helps disambiguate polysemous lemmas better. On the other hand, we obtain an opposite effect from Arefyel et al.'s (2019) model, which involves clustering the embeddings of LUs and subsequently the TFIDF BoW of the substitutes of LUs (see Section 4.2.3). A possible explanation for the contradictory result is that the first clustering step has returned clusters of LUs which are refined enough and where many clusters already correspond to frames in a one-to-one manner, so further splitting each cluster into two worsens the clustering result. An advantage of using definitions to represent LUs is that we can now create contextualized representations for LUs that lack example sentences for frame induction, which is a significant breakthrough given that many unknown LUs lack annotations. Even in BFN 1.7, which is the biggest frame semantics database in terms of annotation, 39% LUs lack lexicographic annotations, and 24% LUs lack example sentences. The baseline models perform well in assigning LUs to their correct frames in the SemEval-2019 Task 2 (see Table 5 ) but poorly in our experiment (see Table 4 ). The large margin in performance suggests that the baseline models are not capable of inducing frames when the LUs are not only restricted to verb lemmas and when the number of semantic frames is large-there are only 149 frames in the dataset of SemEval-2019 Task 2 (QasemiZadeh et al., 2019) but there are 1073 lexical frames in BFN 1.7 4 . This can be correlated to the fact that verbal valence patterns-that is, the patterns extracted from the metadata associated to example sentences through annotation-in FrameNet are more informative than those presented by LUs with a different POS (nouns, adjectives, and adverbs) (Peron-Corr\u00eaa et al., 2016) . In our experiment, we also observe that three baseline models are biased to assign the LUs to frames with five or more existing LUs. One potential reason for the poor performance of baseline models is the \"curse of dimensionality\" (Bellman, 1966) . At the high-dimensional vector space, the vectors of LUs within the same frame are much more spread out. Therefore, the more LUs a frame contains, the more likely that a new LU is assigned to the denser frame. We want to point out that, since the SemEval-2019 Task 2 dataset (QasemiZadeh et al., 2019) is no longer freely available, we could not conduct further performance comparison of SDEC-AD with baselines on the SemEval-2019 frame induction task. This is indicated by the missing result for SDEC-AD in Table 5 . Frame Induction Anomaly Detection AUC ROC and AUC PRC measure models' performanceprecision, recall, specificity, and specificity-aggregated across all possible anomaly score thresholds. The higher the AUC ROC and AUC PRC scores, the better the model in identifying the anomalies. Figure 3 The probability distribution of 1% assumed by the random classifier (based on the proportion of anomalous LUs in our dataset) is unrealistically low since many domain-specific lexical units cannot be represented by semantic frames in BFN 1.7 (Venturi et al., 2009; da Costa et al., 2018; Dolbey et al., 2006) . Besides, the dataset of anomalous LUs only consists of adjectives and adjective satellites. Further research on the prior distribution of anomalous LUs with different parts-of-speech is therefore warranted. Even though SDEC-AD performs better than the rest in detecting anomalous lexical units, its ability to identify nouns and verbs that cannot fit into the existing frames of BFN 1.7 is unknown, especially when reconstruction-errorbased autoencoders lack the ability to address variability (An and Cho, 2015) . Besides, we did not define the exact anomaly threshold \u03c4 of the reconstruction error, which controls the ratio of false positives to false negatives, for SDEC-AD in our experiments. In practice, \u03c4 should be defined such that SDEC-AD identifies anomalous LUs with high recall. High recall is more important than high precision in expanding FrameNet because we want to minimize false negatives and avoid the contamination of FrameNeta situation where semantic frames contain anomalous LUs. Conclusion This paper presents using Semi-supervised Deep Embedded Clustering with Anomaly Detection, or SDEC-AD, to learn clustering-friendly representations of lexical units (LUs) for frame assignment. For a set of LUs not yet present in Berkeley FrameNet data release 1.7 (BFN 1.7), SDEC-AD removes the LUs that cannot be characterized by any semantic frame in BFN 1.7 and subsequently labels the remaining LUs with their correct frames. This two-step frame induction process automatically expands the lexical coverage in BFN 1.7 without compromising the withinframe consistency. Empirical studies show that SDEC-AD outperforms state-of-the-art unsupervised frame induction models. Furthermore, we demonstrate that using the definitions of LUs, which are already present in the lexical resource, enable us to better assign LUs, including those that lack example sentences, to their frames. In the future, we will explore representing frames and their spatial relations in the low-dimensional latent space using SDEC-AD and predict frame-to-frame relations. Acknowledgements This research used the archive and facilities of the Distributed Little Red Hen Lab, co-directed by Francis Steen and Mark Turner, and the FrameNet Brasil Computational Linguistics Lab. The material is based upon work supported by the National Science Foundation under grant 1028381 (2010)(2011)(2012)(2013)(2014)(2015). The FrameNet Brasil Computational Linguistics Lab is funded by CAPES PROBRAL (grant # 88887.144043/2017-00).",
    "abstract": "Although FrameNet is recognized as one of the most fine-grained lexical databases, its coverage of lexical units is still limited. To tackle this issue, we propose a two-step frame induction process: for a set of lexical units not yet present in Berkeley FrameNet data release 1.7, first remove those that cannot fit into any existing semantic frame in FrameNet; then, assign the remaining lexical units to their correct frames. We also present the Semi-supervised Deep Embedded Clustering with Anomaly Detection (SDEC-AD) model-an algorithm that maps high-dimensional contextualized vector representations of lexical units to a low-dimensional latent space for better frame prediction and uses reconstruction error to identify lexical units that cannot evoke frames in FrameNet. SDEC-AD outperforms the state-of-the-art methods in both steps of the frame induction process. Empirical results also show that definitions provide contextual information for representing and characterizing the frame membership of lexical units.",
    "countries": [
        "United States",
        "Brazil"
    ],
    "languages": [],
    "numcitedby": "5",
    "year": "2020",
    "month": "May",
    "title": "Semi-supervised Deep Embedded Clustering with Anomaly Detection for Semantic Frame Induction"
}