{
    "framing_question": "If this paper introduces a dataset, what information is included about people, if any?",
    "response": "The provided paper does not introduce any datasets or provide any information about people. Therefore, I cannot extract the requested data from the provided context.",
    "article": "We combine relational and attributional similarity for the task of identifying instances of semantic relations, such as PRODUCT-PRODUCER and ORIGIN-ENTITY, between nominals in text. We use no pre-existing lexical resources, thus simulating a realistic real-world situation, where the coverage of any such resource is limited. Instead, we mine the Web to automatically extract patterns (verbs, prepositions and coordinating conjunctions) expressing the relationship between the relation arguments, as well as hypernyms and co-hyponyms of the arguments, which we use in instance-based classifiers. The evaluation on the dataset of SemEval-1 Task 4 shows an improvement over the state-ofthe-art for the case where using manually annotated WordNet senses is not allowed. Introduction Recently, the natural language processing (NLP) community has shown renewed interest in the problem of deep language understanding, which was inspired by the notable progress in this important research direction in the last few years. Today, lexical semantics tasks such as word sense disambiguation, semantic role labeling, and textual entailment are already well-established and are gradually finding their way in real NLP applications, while a number of new semantic tasks are emerging. One such example is the task of extracting semantic relations between nominals from text, which has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010) . The ability to recognize semantic relations in text could potentially help many NLP applications. For example, a question answering system facing the question What causes tumors to shrink? would need to identify the CAUSE-EFFECT relation between shrinkage and radiation in order to be able to extract the answer from the following sentence: The period of tumor shrinkage after radiation therapy is often long and varied. One can also imagine a relational search engine that can serve queries such as \"find all X such that X causes wrinkles\", asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006) . Finally, modeling semantic relations has been shown to help statistical machine translation (Nakov, 2008a) . The task of identifying semantic relations in text is complicated by their heterogeneous nature. Thus, it is often addressed using non-parametric instance-based classifiers like the k nearest neighbors (kNN), which effectively reduce it to measuring the relational similarity between a testing and each of the training examples. The latter is studied in detail by Turney (2006) , who distinguishes between attributional similarity or correspondence between attributes, and relational similarity or correspondence between relations. Attributional similarity is interested in the similarity between two words (or nominals, noun phrases), A and B. In contrast, relational similarity focuses on the relationship between two pairs of words (or nominals, noun phrases), i.e., it asks how similar the relations A:B and C:D are. Measuring relational similarity directly is hard, and thus it is rarely done directly. Instead, relational similarity is typically modeled as a function of two instances of attributional similarity: (1) between A and C, and (2) between B and D. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010) . This is useful for context-dependent relations like CAUSE-EFFECT, which are dynamic and often episodic in nature, e.g., My Friday's exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, 2007; \u00d3 S\u00e9aghdha, 2009) . This works well for relations like PART-WHOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit from many preexisting lexical resources. For example, systems using WordNet (Fellbaum, 1998) had sizable performance gains for SemEval-1 Task 4. However, this advantage was mainly due to manually annotated WordNet senses for the relation arguments being provided for this task. There was a restricted track where using them was not allowed: this track was dominated by relation modeling approaches. Relation and argument modeling have their strengths and weaknesses, but there have been little attempts to combine them, which is our main objective. We use no lexical resources, thus simulating a realistic real-world situation, where the coverage of any such resource is limited. Instead, we mine the Web to extract linguistic patterns expressing the relation (verbs, prepositions, and coordinating conjunctions), as well as hypernyms and co-hyponyms of its arguments. We combine (a) relational and (b) attributional similarity between (i) the first and (ii) the second argument, 1 using weights that are tuned separately for each individual relation. While semantic relations can hold between different parts of speech, e.g., between a verb and a noun, we focus on relations between nominals. 2  The most relevant related publication is that of \u00d3 S\u00e9aghdha and Copestake (2009), who combine attributional and relational features using kernels. However, they are interested in a special kind of relations: between the nouns in a noun-noun compounds like steel knife. Moreover, they use the British National Corpus instead of the Web, which is known to cause data sparseness issues (Lapata and Keller, 2004) , they do not focus on linguistically motivated relational features such as verbs and prepositions explicitly, they use co-hyponyms but not hypernyms to generalize the relation arguments, and they give equal weights to the similarities between heads and between modifiers. The remainder of the paper is organized as follows: Section 2 introduces our Web mining methods for argument and relation modeling, Section 3 presents our experimental setup, Section 4 discusses the results, and Section 5 concludes and points to some directions for future work. Method Overview As we said above, we combine argument modeling and relation modeling for the task of extracting semantic relations between nominals from text. Given the heterogeneous nature of semantic relations, we use a non-parametric instance-based classifier: kNN. This effectively reduces the task to measuring the relational similarity between a given testing example and each of the training examples: we first need to find the training example that is most similar to the target testing example; then we assume they should have the same label. For argument modeling, we generalize the arguments of each training/testing example using a set of possible hypernyms and co-hyponyms. For example, given the guy who makes coffee, which is an instance of the PRODUCT-PRODUCER relation, we generate a list of potential hypernyms such as drink and beverage for coffee, and person and human for guy. We further generate cohyponyms for the arguments, e.g., tea and milk for coffee, and girl and boy for guy. These hypernyms and co-hyponyms are extracted from the Web and there is a frequency of extraction associated with each of them, which we use to build a hypernym/co-hyponym frequency vector for each argument and for each example. We then use these argument vectors to measure attributional similarity between training and testing examples. For relation modeling, we mine the Web to find verbs, prepositions and coordinating conjunctions that can express the typical relationship between the arguments of the target example, e.g., we generate verbs like make and brew, prepositions like with, and coordinating conjunctions like and for the arguments guy and coffee of the guy who makes coffee. Again, the paraphrasing verbs and prepositions and the coordinating conjunctions are extracted from the Web, and there is a frequency of extraction associated with each of them, which we incorporate into a relational vector and use to measure relational similarity between training and testing examples. Argument Modeling We model the arguments using a distribution over Web-derived hypernyms and co-hyponyms. Multiple knowledge harvesting procedures have been proposed in the literature for the automatic acquisition of hyponyms (Hearst, 1992; Pas \u00b8ca, 2007; Kozareva et al., 2008) and hypernyms (Ritter et al., 2009; Hovy et al., 2009) . While we could have used any of them for our experiments, we chose the method of Kozareva et al. (2008) , which (i) can extract hypernyms and hyponyms simultaneously, (ii) has been shown to achieve higher accuracy than the methods described in (Pas \u00b8ca, 2007; Ritter et al., 2009) , and also (iii) is easy to implement. It uses a doublyanchored pattern (DAP) of the following general form: \"sem-class such as term 1 and term 2 \" where sem-class stands for a semantic class, and term 1 and term 2 are members of this class. In our experiments, we use the following twoplaceholder form of DAP, which takes only one noun as a parameter and simultaneously extracts pairs of its hypernyms and co-hyponyms: \"* such as noun and *\" We execute the pattern against Google, trying both a plural and a singular form of noun, and we collect the returned snippets. Then, we extract the terms from the * positions, and we build a frequency vector of hypernyms and co-hyponyms. Table 1 shows an example for coffee guy. Relation Modeling We model the relation itself as a distribution over Web-derived verbs, prepositions, and coordinating conjunctions that can connect the target nouns. Following Nakov and Hearst (2008) , we use generalized patterns of the form: \"noun1 THAT? * noun2\" \"noun2 THAT? * noun1\" where noun1 and noun2 are inflected variants of the head nouns in the relation arguments, THAT? stands for that, which, who or the empty string, and * stands for up to eight instances 3 of the search engine's star operator. We instantiate these generalized patterns and we submit them to Google as exact phrase queries. We then collect the snippets for all returned results (up to 1,000). We split the extracted snippets into sentences, and we filter out all incomplete ones and those that do not contain the target nouns. We POS tag the sentences using the Stanford POS tagger (Toutanova et al., 2003) and we make sure that the word sequence following the second mentioned target noun is non-empty and contains at least one non-noun, i.e., that the snippet includes the entire noun phrase of the second noun in the pattern instantiation. This is because we want the second noun in the pattern instantiation to be the head of an NP: if the NP in incomplete, the second noun could be a modifier in that partial NP. We then run the OpenNLP tools 4 to shallow parse the sentences and to extract the verbs, prepositions and coordinating conjunctions connecting the two nouns. Finally, we lemmatize all extracted verbs. As a result, we end up with quadruples, each of which includes the following: (i) a pattern, i.e., a lemmatized verb, a preposition, or a coordinating conjunction, (ii) a pattern type, i.e., V for verb, P for preposition, or C for coordinating conjunction, (iii) direction, i.e., relative order of the arguments in the pattern (R marks reverse), and (iv) frequency of extraction. We concatenate the first three components of these quadruples to form typed directed patterns. We then build frequency vectors for them using the frequency of extraction to represent the semantics of the relation itself. Table 2 shows the resulting relational vector for coffee guy. Experiments and Evaluation In this section, we describe the dataset, the classifier, the similarity measures, and the way we combine relational and attributional similarity. Dataset We use with the dataset from SemEval-1 Task 4 on Classification of Semantic Relations between Nominals (Girju et al., 2009) , which is the most popular dataset for our problem; using it allows for a direct comparison to state-of-the-art systems that were evaluated on it. Each example in the dataset consists of a sentence annotated with two target nominals, e 1 and e 2 , which are to be judged on whether they are in a given target relation or not. In addition, manually annotated WordNet 3.0 senses for these nominals are provided. The Web query the task organizers used to mine the sentence from the Web is also made available. Here is a fully annotated training example (note that, for the test examples, the \"true\"/\"false\" labels are hidden from the system): \"The production assistant is basically the <e1>guy</e1> who makes <e2>coffee</e2> and goes to the post office.\" WordNet(e1) = \"guy%1:18:00::\", WordNet(e2) = \"coffee%1:13:00::\", Origin-Entity(e2, e1) = \"true\", Query = \"the * makes * coffee\" In our experiments, we ignored the WordNet senses and the Web query since having them is unrealistic for a real-world application. Table 3 shows the seven semantic relations defined by the task along with the positive/negative instance distribution and one example instance for each relation. In SemEval-1 Task 4, each relation is considered in isolation, i.e., there are seven separate classification tasks, and there are separate training and testing datasets for each of them. For each relation, the examples are annotated with true/false labels, depending on whether they are instances of the relation. Each of the seven datasets consists of 140 training and 71-93 testing examples per relation, approximately 50% of which are positive. Classifier and Similarity Measures Due to the small size of the individual training datasets and because of the heterogeneity of the examples, we found it hard to train a good model such as SVM or logistic regression. Therefore, we opted for a non-parametric classifier: kNN, and more precisely, 1-nearest-neighbor. Because of its sensitivity to the similarity function, we experimented with three weighting schemes: (1) frequency, (2) TF.IDF, and (3) TF.IDF with addone smoothing for the IDF part. Each of these schemes was combined with the following cosine and Dice similarity functions: The seven semantic relations defined by the task along with the distribution of positive/negative instances and one example for each relation. cosine(A, B) = \u2211 n i=1 a i b i \u221a \u2211 n i=1 a 2 i \u221a \u2211 n i=1 b 2 i (1) Dice(A, B) = 2 \u00d7 \u2211 n i=1 min(a i , b i ) \u2211 n i=1 a i + \u2211 n i=1 b i (2) We further experimented with the informationtheoretic similarity measure of Lin (1998). Experimental Setup For each example in the SemEval-1 Task 4 dataset, we removed all modifiers from the target entities e 1 and e 2 , retaining their head nouns only; below we will still refer to them as e 1 and e 2 though. We then mined the Web to extract features, as described in Section 2 above: (1) relational features: verbs, prepositions, and coordinating conjunctions connecting e 1 and e 2 (see Table 2 ); (2) attributional features: hypernyms and cohyponyms of e 1 and e 2 (see Table 1 ). We used the type (1) features as a baseline, and we studied the impact of combining them with type (2) features using the following five linear weights: w mod for the modifier, w head for the head, w rel for the relation, w hyp for the hypernyms, and w coh for the co-hyponyms. We tuned the values of these parameters using leave-one-out cross-validation on the development set, trying all values in [0.0; 1.0] with a step of 0.1, subject to the following two constraints: w mod + w head + w rel = 1 w hyp + w coh = 1 These tuned weights were then used to calculate the final similarity score s as follows: s = w mod s m + w head s h + w rel s r s m = w hyp s hyp (m 1 , m 2 ) + w coh s coh (m 1 , m 2 ) s h = w hyp s hyp (h 1 , h 2 ) + w coh s coh (h 1 , h 2 ) where s hyp (m 1 , m 2 ) is the similarity between the hypernyms of the modifiers, s coh (m 1 , m 2 ) is the similarity between the co-hyponyms of the modifiers, s hyp (h 1 , h 2 ) is the similarity between the hypernyms of the heads, s coh (h 1 , h 2 ) is the similarity between the co-hyponyms of the heads, and s r is the relational similarity. We also did two restricted experiments: (a) with hypernyms only, i.e., setting w hyp = 1, and (b) with co-hyponyms only, i.e., setting w coh = 1. Results and Discussion Following the experimental setup for SemEval-1 Task 4, we trained and evaluated a separate system for each of the seven relations. The macro-averaged accuracy over all relations is shown in Table 4 . Several interesting observations can be made about it. First, we can see consistent improvements over the corresponding baseline for all three combined systems, for all similarity measures and for all weighting schemes, ranging from 0.5% to 19.5% absolute. Second, in 15 of the 21 experimental conditions involving attributional patterns, the improvements over the corresponding baselines are statistically significant as measured by the \u03c7 2 test. Third, we improve by 1.4% absolute even over our strong baseline, Dice w/ TF.IDF, smoothed, which achieves 68.1% accuracy. Note that this baseline is better than the best accuracy of 66.0% achieved at SemEval-1 Task 4 for systems of type A, which do not use the Web query or the WordNet senses (Girju et al., 2007) . Table 4 : Overall macro-averaged results for all seven relations. The baseline system uses relational patterns only, while the following systems combine relational and attributional features using linear interpolation. Shown are the accuracy and the absolute difference (in %) compared to the baseline. The highest results in each row appear in bold. Statistically significant improvements over the baseline are marked with a star. Fourth, our best overall accuracy of 71.3% represents a statistically significant improvement not only over our corresponding baseline of 51.8% but also over the best result of 66.0% achieved at SemEval-1 Task 4 for systems of type A. It is also higher (but no statistically significant difference) than the state-of-the-art result of Davidov and Rappoport (2008) , who achieved 70.1%. The evaluation results for each of the seven individual relations are shown in Table 5 . We can see that not all relations benefit equally well from using attributional patterns in addition to relational ones. The most sizable improvements are for THEME-TOOL, which shows statistically significant improvements for all evaluation measures, ranging from +7.1% to +23.9% absolute. Very large consistent improvements can be also observed for PRODUCT-PRODUCER and ORIGIN-ENTITY. The results are somewhat mixed for relations like CAUSE-EFFECT, CONTENT-CONTAINER, INSTRUMENT-AGENCY and PART-WHOLE; still, the improvements are more sizable than the decreases. We can further see that relations like THEME-TOOL and ORIGIN-ENTITY are best characterized by the properties of their arguments, which makes them a good fit for attributional methods. In contrast, relations like INSTRUMENT-AGENCY and PRODUCT-PRODUCER, are better expressed by patterns: verbs, prepositions and coordinations. The weights in Table 5 suggest that, overall, the co-hyponyms are more important than the hypernyms, and the relations are typically determined primarily by the modifier and the relational similarity. There is also a lot of variety for the individual relations. For example, for THEME-TOOL, it is the head that matters most. Note that for two of the relations, we achieve results that are better than the best results achieved at SemEval-1 Task 4, even by systems that used WordNet and the original search engine query. In particular, for ORIGIN-ENTITY, we achieve up to 77.8% accuracy, which is statistically significantly better than the 72.8% at SemEval-1 Task 4. We also improve for THEME-TOOL, but our 74.7% is only marginally better than 74.6%. Conclusion and Future Work We have studied the combination of relational and attributional similarity for the task of semantic relation classification in text. Using the dataset for SemEval-1 Task 4, we have shown statistically significant improvements over a strong baseline that uses relational similarity only, and even a small improvement over the state-of-the-art. We have further studied the extent of the improvement across seven individual relations. In future work, we plan to do a similar study for the dataset for SemEval-2 Task 8, where, given its size and the specifics of the relation definitions, which are much more context-dependent, we will need to model the local context, in addition to relational and attributional similarity measures. Acknowledgments This research is partially supported (for the first author) by the SmartBook project, funded by the Bulgarian National Science Fund under Grant D002-111/15.12.2008. We would like to thank the anonymous reviewers for their detailed and constructive comments, which have helped us improve the paper.",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.003075528343269851,
        "foundation": 8.756606586823867e-05,
        "none": 0.9999989719621284
    }
}