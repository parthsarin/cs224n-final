{
    "article": "Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text. Introduction Standard automatic speech recognizers output unstructured streams of words. They neither perform a proper segmentation of the output into sentences, nor predict punctuation symbols. The unavailable punctuations and sentence boundaries in transcribed speech texts create barriers to many subsequent processing tasks, such as summarization, information extraction, question answering and machine translation. Thus, the segmentation of long texts is necessary in many real applications. For example, in speech-to-speech translation, continuously transcribed speech texts need to be segmented before being fed into subsequent machine translation systems (Takezawa et al., 1998; Nakamura, 2009) . This is because current machine translation (MT) systems perform the translation at the sentence level, where various models used in MT are trained over segmented sentences and many algorithms inside MT have an exponential complexity with regard to the length of inputs. The punctuation prediction problem has attracted research interest in both the speech pro-cessing community and the natural language processing community. Most previous work primarily exploits local features in their statistical models such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996) . The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008) . Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010) , such as \"could you\" in Figure 1 . There has been some work trying to incorporate syntactic features to broaden the view of hypotheses in the punctuation prediction models (Roark et al., 2006; Favre et al., 2008) . In their methods, the punctuation prediction is treated as a separated post-procedure of parsing, which may suffer from the problem of error propagation. In addition, these approaches are not able to incrementally process inputs and are not efficient for very long inputs, especially in the cases of long transcribed speech texts from presentations where the number of streaming words could be larger than hundreds or thousands. In this paper, we propose jointly performing punctuation prediction and transition-based dependency parsing over transcribed speech text. When the transition-based parsing consumes the stream of words left to right with the shift-reduce decoding algorithm, punctuation symbols are predicted for each word based on the contexts of the parsing tree. Two models are proposed to cause the punctuation prediction to interact with the transition actions in parsing. One is to conduct transition actions of parsing followed by punctuation predictions in a cascaded way. The other is to associate the conventional transition actions of parsing with punctuation perditions, so that predicted punctuations are directly inferred from the parsing tree. Our models have linear complexity and are capable of handling streams of words with any length. In addition, the computation of models use a rich set of syntactic features, which can improve the complicated punctuation predictions from a global view, especially for the long range dependencies. Figure 1 shows an example of how parsing helps punctuation prediction over the transcribed speech text. As illustrated in Figure 1 (b), two commas are predicted when their preceding words act as the adverbial modifiers (advmod) during parsing. The period after the word \"menu\" is predicted when the parsing of an adverbial clause modifier (advcl) is completed. The question mark at the end of the input is determined when a direct object modifier (dobj) is identified, together with the long range clue that the auxiliary word occurs before the nominal subject (nsubj). Eventually, two segmentations are formed according to the punctuation prediction results, shown in Figure 1(c) . The training data used for our models is adapted from Treebank data by excluding all punctuations but keeping the punctuation contexts, so that it can simulate the unavailable annotated transcribed speech texts. In decoding, beam search is used to get optimal punctuation prediction results. We conduct experiments on both IWSLT data and TDT4 test data sets. The experimental results show that our method can achieve higher performance than the CRF-based baseline method. The paper is structured as follows: Section 2 conducts a survey of related work. The transitionbased dependency parsing is introduced in Section 3. We explain our approach to predicting punctuations for transcribed speech texts in Section 4. Section 5 gives the results of our experiment. The conclusion and future work are given in Section 6. Related Work Sentence boundary detection and punctuation prediction have been extensively studied in the speech processing field and have attracted research interest in the natural language processing field as well. Most previous work exploits local features for the task. Kim and Woodland (2001) , Huang and Zweig (2002) , Christensen et al. (2001), and Liu et al. (2005) integrate both prosodic features (pitch, pause duration, etc.) and lexical features (words, n-grams, etc.) to predict punctuation symbols during speech recognition, where Huang and Zweig (2002) uses a maximum entropy model, Christensen et al. (2001) focus on finite state and multi-layer perceptron methods, and Liu et al. (2005) uses conditional random fields. However, in some scenarios the prosodic cues are not available due to inaccessible original raw speech waveforms. Matusov et al. (2006) integrate segmentation features into the log-linear model in the statistical machine translation (SMT) framework to improve the translation performance when translating transcribed speech texts. Lu and Ng (2010) It is natural to incorporate global knowledge, such as syntactic information, to improve punctuation prediction performance. Roark et al. (2006) use a rich set of non-local features including parser scores to re-rank full segmentations. Favre et al. (2008) integrate syntactic information from a PCFG parser into a log-linear and combine it with local features for sentence segmentation. The punctuation prediction in these works is performed as a post-procedure step of parsing, where a parse tree needs to be built in advance. As their parsing over the stream of words in transcribed speech text is exponentially complex, their approaches are only feasible for short input processing. Unlike these works, we incorporate punctuation prediction into the parsing which process left to right input without length limitations. Numerous dependency parsing algorithms have been proposed in the natural language processing community, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010) . Starting with the work from (Zhang and Nivre, 2011) , in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012) . The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these two tasks of POS tagging and punctuation prediction are different in two ways: 1). The former usually works on a well-formed single sentence while the latter needs to process multiple sentences that are very lengthy. 2). POS tags are must-have features to parsing while punctuations are not. The parsing quality in the former is more sensitive to the performance of the entire task than in the latter. Transition-based dependency parsing In a typical transition-based dependency parsing process, the shift-reduce decoding algorithm is applied and a queue and stack are maintained (Zhang and Nivre, 2011) . The queue stores the stream of transcribed speech words, the front of which is indexed as the current word. The stack stores the unfinished words which may be linked with the current word or a future word in the queue. When words in the queue are consumed from left to right, a set of transition actions is applied to build a parse tree. There are four kinds of transition actions conducted in the parsing process (Zhang and Nivre, 2011) , as described in Table 1 . The choice of each transition action during the parsing is scored by a linear model that can be trained over a rich set of non-local features extracted from the contexts of the stack, the queue and the set of dependency labels. As described in (Zhang and Nivre, 2011) , the feature templates could be defined over the lexicons, POS-tags and the combinations with syntactic information. Action Description In parsing, beam search is performed to search the optimal sequence of transition actions, from which a parse tree is formed (Zhang and Clark, 2008) . As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2n, where n is the length of the sentence. Thus, transitionbased parsing has a linear complexity with the length of input and naturally it can be extended to process the stream of words. Our method Model In the task of punctuation prediction, we are given a stream of words from an automatic transcription of speech text, denoted by \ud835\udc64 1 \ud835\udc5b : = \ud835\udc64 1 , \ud835\udc64 2 , \u2026 , \ud835\udc64 \ud835\udc5b . We are asked to output a sequence of punctuation symbols \ud835\udc46 1 \ud835\udc5b : = \ud835\udc60 1 , \ud835\udc60 2 , \u2026 , \ud835\udc60 \ud835\udc5b where \ud835\udc60 \ud835\udc56 is attached to \ud835\udc64 \ud835\udc56 to form a sentence like Figure 1(c ). If there are no ambiguities, \ud835\udc46 1 \ud835\udc5b is also abbreviated as \ud835\udc46, similarly for \ud835\udc64 1 \ud835\udc5b as \ud835\udc64. We model the search of the best sequence of predicted punctuation symbols \ud835\udc46 * as: \ud835\udc46 * = argmax S \ud835\udc43(\ud835\udc46 1 \ud835\udc5b |\ud835\udc64 1 \ud835\udc5b ) (1) We introduce the transition-based parsing tree \ud835\udc47 to guide the punctuation prediction in Model (2), where parsing trees are constructed over the transcribed text while containing no punctuations. \ud835\udc46 * = argmax \ud835\udc46 \u2211 \ud835\udc43(\ud835\udc47|\ud835\udc64 1 \ud835\udc5b ) \u00d7 \ud835\udc43(\ud835\udc46 1 \ud835\udc5b |\ud835\udc47, \ud835\udc64 1 \ud835\udc5b ) \ud835\udc47 (2) Rather than enumerate all possible parsing trees, we jointly optimize the punctuation prediction model and the transition-based parsing model with the form: (\ud835\udc46 * , \ud835\udc47 * ) = argmax (\ud835\udc46,\ud835\udc47) \ud835\udc43(\ud835\udc47|\ud835\udc64 1 \ud835\udc5b ) \u00d7 \ud835\udc43(\ud835\udc46 1 \ud835\udc5b |\ud835\udc47, \ud835\udc64 1 \ud835\udc5b ) (3) Let \ud835\udc47 1 \ud835\udc56 be the constructed partial tree when \ud835\udc64 1 \ud835\udc56 is consumed from the queue. We decompose the Model (3) into: (\ud835\udc46 * , \ud835\udc47 * ) = argmax (\ud835\udc46,\ud835\udc47) \u220f \ud835\udc43(\ud835\udc47 1 \ud835\udc56 |\ud835\udc47 1 \ud835\udc56\u22121 , \ud835\udc64 1 \ud835\udc56 ) \u00d7 \ud835\udc43(\ud835\udc60 \ud835\udc56 |\ud835\udc47 1 \ud835\udc56 , \ud835\udc64 1 \ud835\udc56 ) \ud835\udc5b \ud835\udc56=1 (4) It is noted that a partial parsing tree uniquely corresponds to a sequence of transition actions, and vice versa. Suppose \ud835\udc47 1 \ud835\udc56 corresponds to the action sequence \ud835\udc34 1 \ud835\udc56 and let \ud835\udc4e \ud835\udc56 denote the last action in \ud835\udc34 1 \ud835\udc56 . As the current word \ud835\udc64 \ud835\udc56 can only be consumed from the queue by either Shift or RightArc according to Table 1 , we have \ud835\udc4e \ud835\udc56 \u2208 {\ud835\udc46\u210e\ud835\udc56\ud835\udc53\ud835\udc61, \ud835\udc45\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc34\ud835\udc5f\ud835\udc50} . Thus, we synchronize the punctuation prediction with the application of Shift and RightArc during the parsing, which is explained by Model (5). (\ud835\udc46 * , \ud835\udc47 * ) = argmax (\ud835\udc46,\ud835\udc47) \u220f \ud835\udc43(\ud835\udc47 1 \ud835\udc56 , \ud835\udc34 1 \ud835\udc56 |\ud835\udc47 1 \ud835\udc56\u22121 , \ud835\udc64 1 \ud835\udc56 ) \ud835\udc5b \ud835\udc56=1 \u00d7 \ud835\udc43(\ud835\udc60 \ud835\udc56 |\ud835\udc4e \ud835\udc56 , \ud835\udc47 1 \ud835\udc56 , \ud835\udc64 1 \ud835\udc56 ) (5) The model is further refined by reducing the computation scope. When a full-stop punctuation is determined (i.e., a segmentation is formed), we discard the previous contexts and restart a new procedure for both parsing and punctuation prediction over the rest of words in the stream. In this way we are theoretically able to handle the unlimited stream of words without needing to always keep the entire context history of streaming words. Let \ud835\udc4f \ud835\udc56 be the position index of last full-stop punctuation 1 before \ud835\udc56, \ud835\udc47 \ud835\udc4f \ud835\udc56 \ud835\udc56 and \ud835\udc34 \ud835\udc4f \ud835\udc56 \ud835\udc56 the partial tree and corresponding action sequence over the words \ud835\udc64 \ud835\udc4f \ud835\udc56 \ud835\udc56 , Model (5) can be rewritten by: With different computation of Model ( 6 ), we induce two joint models for punctuation prediction: the cascaded punctuation prediction model and the unified punctuation prediction model. Cascaded punctuation prediction model (CPP) In Model ( 6 ), the computation of two sub-models is independent. The first sub-model is computed based on the context of words and partial trees without any punctuation knowledge, while the computation of the second sub-model is conditional on the context from the partially built parsing tree \ud835\udc47 \ud835\udc4f \ud835\udc56 \ud835\udc56 and the transition action. As the words in the stream are consumed, each computation of transition actions is followed by a computation of punctuation prediction. Thus, the two sub-models are computed in a cascaded way, until the optimal parsing tree and optimal punctuation symbols are generated. We call this model the cascaded punctuation prediction model (CPP). Unified punctuation prediction model (UPP) In Model ( 6 ), if the punctuation symbols can be deterministically inferred from the partial tree, \ud835\udc43(\ud835\udc60 \ud835\udc56 |\ud835\udc4e \ud835\udc56 , \ud835\udc47 \ud835\udc4f \ud835\udc56 \ud835\udc56 , \ud835\udc64 \ud835\udc4f \ud835\udc56 \ud835\udc56 ) can be omitted because it is always 1. Similar to the idea of joint POS tagging and parsing (Hatori et al., 2011; Bohnet and Nivre, 2012) , we propose attaching the punctuation prediction onto the parsing tree by embedding \ud835\udc60 \ud835\udc56 into \ud835\udc4e \ud835\udc56 . Thus, we extend the conventional transition actions illustrated in Table 1 to a new set of transition actions for the parsing, denoted by \ud835\udc34 \u0302: \ud835\udc34 \u0302= {\ud835\udc3f\ud835\udc52\ud835\udc53\ud835\udc61\ud835\udc34\ud835\udc5f\ud835\udc50, \ud835\udc45\ud835\udc52\ud835\udc51\ud835\udc62\ud835\udc50\ud835\udc52} \u222a {\ud835\udc46\u210e\ud835\udc56\ud835\udc53\ud835\udc61(\ud835\udc60)|\ud835\udc60 \u2208 \ud835\udc44} \u222a {\ud835\udc45\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc34\ud835\udc5f\ud835\udc50(\ud835\udc60)|\ud835\udc60 \u2208 \ud835\udc44} where Q is the set of punctuation symbols to be predicted, \ud835\udc60 is a punctuation symbol belonging to Q, Shift(s) is an action that attaches s to the current word on the basis of original Shift action in parsing, RightArc(s) attaches \ud835\udc60 to the current word on the basis of original RightArc action. With the redefined transition action set \ud835\udc34 \u0302, the computation of Model ( 6 ) is reformulated as: (\ud835\udc46 * , \ud835\udc47 * ) = argmax (\ud835\udc46,\ud835\udc47) \u220f \ud835\udc43 (\ud835\udc47 \ud835\udc4f \ud835\udc56 \ud835\udc56 , \ud835\udc34 \u0302\ud835\udc4f\ud835\udc56 \ud835\udc56 |\ud835\udc47 \ud835\udc4f \ud835\udc56 \ud835\udc56\u22121 , \ud835\udc34 \u0302\ud835\udc4f\ud835\udc56 \ud835\udc56\u22121 , \ud835\udc64 \ud835\udc4f \ud835\udc56 \ud835\udc56 ) \ud835\udc5b \ud835\udc56=1 (7) Here, the computation of parsing tree and punctuation prediction is unified into one model where the sequence of transition action outputs uniquely determines the punctuations attached to the words. We refer to it as the unified punctuation prediction model (UPP). Figure 2 illustrates an example how the UPP model works. Given an input \"so could you tell me\", the optimal sequence of transition actions in Figure 2 (b) is calculated based on the UPP model to produce the parsing tree in Figure 2(a) . According to the sequence of actions, we can determine the sequence of predicted punctuation symbols like \",NNN?\" that have been attached to the words shown in Figure 2 (a). The final segmentation with the predicted punctuation insertion could be \"so, could you tell me?\". Model training and decoding In practice, the sub-models in Model ( 6 ) and ( 7 ) with the form of \ud835\udc43(\ud835\udc4c|\ud835\udc4b) is computed with a linear model \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc4c, \ud835\udc4b) as \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc4c, \ud835\udc4b) = \ud835\udef7(\ud835\udc4c, \ud835\udc4b) \u2022 \ud835\udf06 \u20d7 where \ud835\udef7(\ud835\udc4c, \ud835\udc4b) is the feature vector extracted from the output \ud835\udc4c and the context \ud835\udc4b, and \ud835\udf06 \u20d7 is the weight vector. For the features of the models, we incorporate the bag of words and POS tags as well as tree-based features shown in Table 2 , which are the same as those defined in (Zhang and Nivre, 2011) . (a) ws; w0; w1; w2; ps; p0; p1; p2; wsps; w0p0; w1p1; w2p2; wspsw0p0; wspsw0; wspsp0; wsw0p0; psw0p0; wsw0; psp0; p0p1; psp0p1; p0p1p2; (b) pshpsp0; pspslp0; pspsrp0; psp0p0l; wsd; psd; w0d; p0d; wsw0d; psp0d; wsvl; psvl; wsvr; psvr; w0vl; p0vl; wsh; psh; ts; w0l; p0l; t0l; w0r; p0r; t0r; w1l; p1l; t1l; wsh2; psh2; tsh; wsl2; psl2; tsl2; wsr2; psr2; tsr2; w0l2; p0l2; t0l2; pspslpsl2; pspsrpsr2; pspshpsh2; p0p0lp0l2; wsTl; psTl; wsTr; psTr; w0Tl; p0Tl; Table 2 . The training data for both the CPP and UPP models need to contain parsing trees and punctuation information. Due to the absence of annotation over transcribed speech data, we adapt the Treebank data for the purpose of model training. To do this, we remove all types of syntactic information related to punctuation symbols from the raw Treebank data, but record what punctuation symbols are attached to the words. We normalize various punctuation symbols into two types: Middle-paused punctuation (M) and Full-stop punctuation (F). Plus null type (N), there are three kinds of punctuation symbols attached to the words. Table 3 illustrates the normalizations of punctuation symbols. In the experiments, we did not further distinguish the type among full-stop punctuation because the question mark and the exclamation mark have very low frequency in Treebank data. (Zhang and Clark, 2008; Zhang and Nivre, 2011) , we train CPP and UPP by generalized perceptron (Collins, 2002) . In decoding, beam search is performed to get the optimal sequence of transition actions in CPP and UPP, and the optimal punctuation symbols in CPP. To ensure each segment decided by a fullstop punctuation corresponds to a single parsing tree, two constraints are applied in decoding for the pruning of deficient search paths. (1) Proceeding-constraint: If the partial parsing result is not a single tree, the full-stop punctuation prediction in CPP cannot be performed. In UPP, if Shift(F) or RightArc(F) fail to result in a single parsing tree, they cannot be performed as well. (2) Succeeding-constraint: If the full-stop punctuation is predicted in CPP, or Shift(F) and RightArc(F) are performed in UPP, the following transition actions must be a sequence of Reduce actions until the stack becomes empty. Experiments Experimental setup Our training data of transition-based dependency trees are converted from phrasal structure trees in English Web Treebank (LDC2012T13) and the English portion of OntoNotes 4.0 (LDC2011T03) by the Stanford Conversion toolkit (Marneffe et al., 2006) . It contains around 1.5M words in total and consist of various genres including weblogs, web texts, newsgroups, email, reviews, question-answer sessions, newswires, broadcast news and broadcast conversations. To simulate the transcribed speech text, all words in dependency trees are lowercased and punctuations are excluded before model training. In addition, every ten dependency trees are concatenated sequentially to simulate a parsing result of a stream of words in the model training. There are two test data sets used in our experiments. One is the English corpus of the IWSLT09 evaluation campaign (Paul, 2009 ) that is the conversional speech text. The other is a subset of the TDT4 English data (LDC2005T16) which consists of 200 hours of closed-captioned broadcast news. In the decoding, the beam size of both the transition-based parsing and punctuation prediction is set to 5. The part-of-speech tagger is our re-implementation of the work in (Collins, 2002) . The evaluation metrics of our experiments are precision (prec.), recall (rec.) and F1-measure (F1). For the comparison, we also implement a baseline method based on the CRF model. It incorporates the features of bag of words and POS tags shown in Table 2(a), which are commonly used in previous related work. Experimental results We test the performance of our method on both the correctly recognized texts and automatically recognized texts. The former data is used to evaluate the capability of punctuation prediction of our algorithm regardless of the noises from speech data, as our model training data come from formal text instead of transcribed speech data. The usage of the latter test data set aims to evaluate the effectiveness of our method in real applications where lots of substantial recognition errors could be contained. In addition, we also evaluate the quality of our transition-based parsing, as its performance could have a big influence on the quality of punctuation prediction. Performance on correctly recognized text The evaluation of our method on correctly recognized text uses 10% We achieved good performance on full-stop punctuation compared to the baseline, which shows our method can efficiently process sentence segmentation because each segment is decided by the structure of a single parsing tree. In addition, the global syntactic knowledge used in our work help capture long range dependencies of punctuations. The performance of middle-paused punctuation prediction is fairly low between all methods, which shows predicting middle-paused punctuations is a difficult task. This is because the usage of middle-paused punctuations is very flexible, especially in conversional data. The last column in Table 4 presents the performance of the pure segmentation task where the middle-paused and full-stop punctuations are mixed and not distinguished. The performance of our method is much higher than that of the baseline, which shows our method is good at segmentation. We also note that UPP yields slightly better performance than CPP on full-stop and mixed punctuation prediction, and much better performance on middle-paused punctuation prediction. This could be because the interaction of parsing and punctuation prediction is closer together in UPP than in CPP. Performance on automatically recognized text Table 5 shows the experimental results of punctuation prediction on automatically recognized text from TDT4 data that is recognized using SRI's English broadcast news ASR system where the word error rate is estimated to be 18%. As the annotation of middle-paused punctuations in TDT4 is not available, we can only evaluate the performance of full-stop punctuation prediction (i.e., detecting sentence boundaries). Thus, we merge every three sentences into one single input before performing full-stop prediction. Generally, the performance shown in Table 5 is not as high as that in Table 4 . This is because the speech recognition error from ASR systems degrades the capability of model prediction. Another reason might be that the domain and style of our training data mismatch those of TDT4 data. The baseline gets a little higher recall than our method, which shows the baseline method tends to make aggressive segmentation decisions. However, both precision and F1 score of our method are much higher than the baseline. CPP has higher recall than UPP, but with lower precision and F1 score. This is in line with Table 4 , which consistently illustrates CPP can get higher recall on fullstop punctuation prediction for both correctly recognized and automatically recognized texts. Performance of transition-based parsing Performance of parsing affects the quality of punctuation prediction in our work. In this section, we separately evaluate the performance of our transition-based parser over various domains including the Wall Street Journal (WSJ), weblogs, newsgroups, answers, email messages and reviews. We divided annotated Treebank data into three data sets: 90% for model training, 5% for the development set and 5% for the test set. The accuracy of our POS-tagger achieves 96.71%. The beam size in the decoding of both our POS-tagging and parsing is set to 5. Table 6 presents the results of our experiments on the measures of UAS and LAS, where the overall accuracy is obtained from a general model which is trained over the combination of the training data from all domains. We first evaluate the performance of our transition-based parsing over texts containing punctuations (TCP). The evaluation results show that our transition-based parser achieves state-of-the-art performance levels, referring to the best dependency parsing results reported in the shared task of SANCL 2012 workshop 2 , although they cannot be compared directly due to the different training data and test data sets used in the experiments. Secondly, we evaluate our parsing model in CPP over the texts without punctuations (TOP). Surprisingly, the performance over TOP is better than that over TCP. The reason could be that we cleaned out data noises caused by punctuations when preparing TOP data. These results illustrate that the performance of transition-based parsing in our method does not degrade after being integrated with punctuation prediction. As a by-product of the punctuation prediction task, the outputs of parsing trees can benefit the subsequent text processing tasks. Conclusion and Future Work In this paper, we proposed a novel method for punctuation prediction of transcribed speech texts. Our approach jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features. It can not only yield parse trees, but also determine sentence boundaries and predict punctuation symbols from a global view of the in-puts. The proposed algorithm has linear complexity in the size of input, which can efficiently process the stream of words from a purely text processing perspective without the dependences on either the ASR systems or subsequent tasks. The experimental results show that our approach outperforms the CRF-based method on both the correctly recognized and automatically recognized texts. In addition, the performance of the parsing over the stream of transcribed words is state-ofthe-art, which can benefit many subsequent text processing tasks. In future work, we will try our method on other languages such as Chinese and Japanese, where Treebank data is available. We would also like to test the MT performance over transcribed speech texts with punctuation symbols inserted based on our method proposed in this paper.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources from defense, corporate entities, research agencies, or foundations. Without explicit mention of funding, it is not possible to determine the involvement of any of these sources based on the provided text.",
    "abstract": "Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text.",
    "countries": [
        "China"
    ],
    "languages": [
        "Japanese",
        "Chinese",
        "English"
    ],
    "numcitedby": "28",
    "year": 2013,
    "month": "August",
    "title": "Punctuation Prediction with Transition-based Parsing"
}