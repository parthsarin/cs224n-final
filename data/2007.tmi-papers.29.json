{
    "article": "A novel approach is presented for extracting syntactically motivated phrase alignments. In this method we can incorporate conventional resources such as dictionaries and grammar rules into a statistical optimization framework for phrase alignment. The method extracts bilingual phrases by incrementally merging adjacent words or phrases on both source and target language side in accordance with a global statistical metric. The extracted phrases achieve a maximum F-measure of over 80 with respect to human judged phrase alignments. The extracted phrases used as training corpus for a phrase-based SMT shows better cross-domain portability over conventional SMT framework. Introduction In the phrase-based SMT framework (Marcu & Wong, 2002; Och & Ney, 2004; Chiang, 2005) , extraction of phrase pairs is a key issue. Currently the standard method of extracting bilingual phrases is to use a heuristics such as diag-and (Koehn et. al., 2003) . In this method starting with the intersection of word alignments of both translation directions additional alignment points are added according to a number of heuristics and all the phrase pairs which are consistent with the word alignments are collected. Although this method is effective by itself it is very difficult to incorporate syntactic information in a straight manner because phrases extracted by this method have basically little syntactic significance. Especially if we intend to combine strength of conventional rule-based approach with that of SMT, it is essential that phrases, or translation units, carry syntactic significance such as being a constituent (Yamada & Knight, 2001) . Another drawback of the conventional method is that the phrase extraction process is deterministic and no quantitative evaluation is applied. Furthermore if the initial word alignments have errors, these errors propagate to the phrase alignment process. In doing so the burden of statistical optimization is imposed on the final decoding process. We propose in this paper a novel phrase alignment method in which we can incorporate conventional resources such as dictionaries and grammar rules into a statistical optimization framework for phrase alignment. The outline of the proposed method, applied to Japanese-English bilingual corpus, is as follows. 1) The training bilingual corpus is first word-aligned by GIZA++ (Och & Ney, 2000) . 2) A word translation model is learnt by relative frequency from the word-alignment and smoothed by a bilingual dictionary. 3) Chunking is performed on both sides. 4) The probability that an English word belongs to a Japanese chunk is evaluated from which an entropy score is computed. 5) The entropy score is used to guide the process of merging adjacent phrases of both languages. 6) The merging process terminates when the score takes a minimum value. Although the above steps are purely guided by a statistical metric, some syntactic preferences or constraints can guide the search. The objective of this work is to extract alignments of phrases which are linguistically motivated. However, there is no guarantee that even manually extracting, out of aligned sentences, bilingual phrases which correspond to each other in meaning results in a collection of pairs of source and target phrases which are both constituents. There might be cases in which a phrase in one language constitutes a constituent while the corresponding phrase in the other language does not. Therefore the basic strategy we adopt here is to try to extract bilingual phrases whose source language side at least constitutes a constituent. As for the target language side, a preference is given to constituent constructs. Phrase Alignment Method The phrase alignment method we propose here extracts bilingual phrases by incrementally merging adjacent words or phrases on both source and target languages in accordance with a global statistical metric along with syntactic constraints and preferences. The merging process is guided by an entropy score which is calculated from the alignment matrix. Figure 1 shows an example of the alignment matrix for the following sentence pair: (1a) \u6f14\u7b97\u56de\u8def\u306e\u8a18\u61b6\u5024\u306e\u4e57\u7b97\u3068\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u306e\u52a0\u7b97 \u306e\u30eb\u30fc\u30d7\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u306b\u3088\u308a\uff0c\u7c21\u5358\u306a\u6f14\u7b97\u56de\u8def \u3067\u73fe\u5728\u306e\u30c7\u30fc\u30bf\u306b\u91cd\u307f\u3092\u7f6e\u3044\u305f\u5e73\u5747\u5024\u3092\u7b97\u51fa\u53ef\u80fd \u3068\u3059\u308b\u3002 (1b) To calculate an average value weighed in the present data with a simple arithmetic circuit by repeating the loop of multiplication of the stored value in the arithmetic circuit and the addition of a new data. In the alignment matrix, English words are arranged in each row and Japanese chunks are arranged in each column. The value of the (i, j) element divided by the margin of the i-th row represents the probability that the translation of the i-th English word (w i ) appears in the j-th Japanese chunk (j \uff4a ). For example, the translation of w 1 (calculate) can be \"\u6f14\u7b97\", which appears in j 0 (\"\u6f14\u7b97\u56de\u8def\u306e\u8a18\u61b6\u5024 \u306e\" ) and j 8 (\"\u7c21\u5358\u306a\u6f14\u7b97\u56de\u8def\u3067\"), or \"\u7b97\u51fa\", which appears in j 13 (\"\u7b97\u51fa\u53ef\u80fd\u3068\u3059\u308b\"), or \"\u7b97\" , which appears in j 1 and j 3 in addition to j 0 , j 8 and j 13 . Since \"calculate\" is more likely to be translated as \"\u7b97\u51fa\" than others, the (1, 13) element has larger value than other elements in the same row. Determiners, prepositions, conjunctions, and other function words are treated as stopwords and their elements are all assigned a value of zero. When there is more than one element with a positive value in the same row, these elements are shown in Figure 1 with a shaded square, and this means that the corresponding English word is ambiguous on the identity of the corresponding Japanese chunk. On the other hand, if there is only one element, say (p,q), with positive value in the same row, it is certain that the English word w p belongs to the Japanese chunk j q . If there is one and only one nonzero element in each row and in each column, then we have a complete one-to-one matching between Japanese elements (phrases) and English elements (words or phrases). The intuition behind the proposed method is that by merging adjacent elements which constitute a phrase and tend to stay together in both languages, the alignment matrix approaches a one-to-one matching. Therefore if there is a global measure that shows how close the current alignment matrix is to a one-to-one matching, we can use it to guide the merging process. We use the entropy score which is described in the next section. Without Syntactic Information We begin by describing the proposed phrase alignment method in the case of incorporating no syntactic information. Figure 2 shows the framework of the phrase aligner. In the case of incorporating no syntactic information, Syntactic Component in the figure plays no role. We take here an example of translating from Japanese to English, but the framework presented here basically works for any language pair as long as a conventional rule-based approach is applicable. As a preparation step, word alignments are obtained from a bilingual corpus by GIZA++ for both directions (source to target and target to source), and the intersection A = A1\u2229A2 of the two sets of alignments are taken. Then for each English word e and Japanese word j, the frequency N(e) of e in A and the co-occurrence frequency N(e , j) of e and j in A are calculated. Furthermore, using a discrimination function \u03b4 (e, j) which determines whether e and j are a translation of each other with respect to a predefined bilingual dictionary, word based empirical translation probability is obtained as follows. (2) Pc(j|e) = (N(e,j) +\u03b4(e,j))/(N(e)+ \u03a3 t \u03b4(e,t)) \u03b4(e, j) takes a value of 1 when (e, j) appears in the bilingual dictionary, and 0 otherwise. An input to the phrase aligner is a pair (J, E) of Japanese and English sentences. The pair (J, E) is first chunkparsed to extract base phrases, such as minimum noun phrases and phrasal verbs on both sides. Let J = j 1 , j 2 , \u2026, j M be a series of Japanese chunks. These chunks are the minimum units for composing a final phrase alignment on Japanese side. Let E = w 1 , w 2 , \u2026, w N be a series of English words. Then the probability that the translation of word w i appears in chunk j \uff4a in the given sentence pair is given by (3) 1 . (3) P(j \uff4a |w i )= C ij / \u03a3 \uff4a C ij , where (4) C ij = \u03a3 t Pc(t| w i ) P(t appears in j \uff4a ) is what we will call an alignment matrix which represents the relative likelihood that the translation of word w i appears in chunk j \uff4a in comparison with other Japanese chunks, t is a translation candidate of w i , and P(t appears in j \uff4a ) is zero if j \uff4a doesn't contain t as a substring and one if it does. Note that the values of C ij can be calculated form the parallel sentence pair and the empirical translation probability (2). Similarly for Japanese phrases, we can calculate the probability P(w i |j \uff4a ) that the translation of j \uff4a is represented as w i as follows. (5) P ( w i | j \uff4a ) = C ij / \u03a3 i C ij Given the translation probability (3), we can define the entropy H(i) of the probability distribution P(\u30fb| w i ) as follows. ( 6 ) H(i) = \u2212\u03a3 j P(j \uff4a | w i )log 2 P(j \uff4a | w i ) Since lim X\u21920 X log 2 X = 0, we define H(i) = 0 when P(j \uff4a | w i ) =0 for all j. In the proposed method, a statistical metric based on the entropy ( 6 ) is used for judging which adjacent phrases are to be merged. We calculate the change in the evaluation metric resulting from the merge just in the same way as we calculate the information gain (the reduction of entropy) of a decision tree when the dataset is divided according to some attribute, with the only difference that in a decision tree a dataset is incrementally divided, whereas in our method rows and columns are merged. We treat each row and each column of the alignment matrix as a dataset. The entire entropy, or uncertainty, of mapping English phrases to Japanese phrases is then given by: (7) H = \u03a3 i [\u03a3 j C ij ]H(i)/ \u03a3 i \u03a3 j C ij The entropy of mapping Japanese phrases to English phrases is obtained in the same way. (8 ) H t = \u03a3 j [\u03a3 i C ij ]H(j)/ \u03a3 i \u03a3 j C ij Finally we define the total statistical metric, or an evaluation score, as the mean value of the two. H tot = (H + H t )/2 Phrase Extraction The merging process is terminated when the evaluation score H tot takes a minimum value. When the final value of the alignment matrix is obtained, then for each non-zero element C ij the corresponding English phrase in the i-th row and the Japanese phrase in the j-th column are extracted and paired as an aligned phrase pair. Even after H tot reaches zero we can continue merging as long as H tot stays zero and a different set of phrase pairs can be extracted at each merging step while H tot stays zero. Whether rows are merged or columns are merged at each merging step is determined by the evaluation score. Since the merging process is easily trapped by the local minimum with a greedy search, a beam search is employed while keeping multiple candidates (instances of alignment matrices). The typical beam size employed is between 300 and 1000. One of the advantages of the proposed method is that we can directly incorporate dictionary information into the scheme, which is quite effective for alleviating data sparseness problem especially in the case of small training corpus. Another distinctive feature of the method is that once word alignments are obtained and the empirical translation probability Pc(j|e) is calculated together with the dictionary information, the word alignments are discarded. This is how this method avoids deterministic phrase alignment, and keeps a possibility of recovering from word alignment errors. Multiple Correspondences As we saw in the example of Figure 1 there is very often more than one element with a positive value in the same row of the alignment matrix. Usually only one nonzero element is correct and others are erroneously assigned nonzero values due to an accidental string match between the Japanese chunks and the translation of the English word. However there is no simple way of preliminarily disambiguating the identity of the corresponding Japanese chunk. To cope with this initial ambiguity, a separate initial alignment matrix is constructed for each combination of a nonzero element of a row so that each row has at most one nonzero element. If there are n words w 1 , w 2 , \u2026w n in the English sentence, and each word w i has k i possible corresponding Japanese chunks, then the number of combinations is k 1 k 2 \u2026k n , which sometimes becomes huge. However, in the process of merging, most of the erroneous word alignments disappear in confrontation with correct word alignments. Figure 3 shows two examples of an initial alignment matrix candidate for the sentence pair (1) and phrase alignments obtained after the merging process. Since the evaluation score of (c) is zero, (a) is considered to be the correct initial alignment matrix. As a result, the initial ambiguity on the identity of the corresponding Japanese chunk for each English word is resolved. In some cases, however, multiple correspondences between English words and Japanese chunks are intrinsic. Consider the following sentence pair. (11a) \u771f\u7a7a\u8ce6\u52e2\u3057\u305f\u7ba1\u53ca\u3073\u8840\u6db2\u306e\u53d6\u308a\u51fa\u3057\u4e2d\u306b\u6dfb\u52a0 \u5264\u3092\u5206\u914d\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3092\u63d0\u4f9b\u3059\u308b\u3002 (11b) To provide a tube energized in vacuum and establish a method for distributing additives during the process of taking out the blood. Figure 4 shows the phrase alignment result for this pair and Figure 5 shows the initial and final alignment matrices. As Figure 4 shows the Japanese verb \"\u63d0\u4f9b\u3059\u308b\" (f) is aligned with both \"To provide\" (t) and \"and establish\" (v). This is because in the clausal conjunction different verbs are used for different objects (a tube and a method) in English whereas the same verb (f) is used in Japanese. In those cases one-to-one correspondence can never be achieved through merging, but still the evaluation score is expected to lead the merging process to a correct alignment result. With Syntactic Information The proposed framework also has a capability of incorporating syntactic constraints and preferences in the process of merging. For example, suppose that there are two competing merging candidates; one is to merge (i-th row, i+1-th row) and the other is to merge (k-th column, k+1-th column), and that their evaluation scores are H1 and H2 respectively. Then if there are no syntactic constraints or preferences, the merging candidate which has the lower evaluation score is elected. But if there are syntactic constraints, the only merging candidate which satisfies the constraints is executed. When a syntactic preference is introduced, then the evaluation score is multiplied by some value which represents the degree of the strength of the preference. If we intend to extract only pairs of phrases which constitute a constituent, then we introduce a constraint which eliminates merging candidates that produce a phrase which crosses a constituent boundary. Although our goal is to fully integrate complete set of CFG rules into the merging scheme, we are still in the process of constructing the syntactic rules, and in the present work we employed only a small set of preferences and constraints. Table 1 illustrates some of the syntactic constraints and preferences employed in the present work. Merging lines or columns in the alignment matrix can be viewed as a form of bottom-up parsing. When we trace the process of the merging, its history can be converted to [6] [7] 0 0 0 0 0 0 0 : To calculate 0 0 0 0 0 0 182 : an average value 0 0 0 0 0 133 0 : weighed in the present data 0 0 0 0 252 0 0 : with a simple arithmetic circuit 0 0 0 151 0 0 0 : by repeating the loop 0 72 0 0 0 0 0 : of multiplication 321 0 0 0 0 0 0 : of the stored value in the arithmetic circuit 0 0 131 0 0 0 0 : and the addition of a new data [0] [1] [2] [3] [4] [5] [0]: \u6f14\u7b97\u56de\u8def\u306e\u8a18\u61b6\u5024\u306e [1]: \u4e57\u7b97\u3068 [2]: \u65b0\u3057\u3044\u30c7\u30fc\u30bf\u306e\u52a0\u7b97\u306e [3]: \u30eb\u30fc\u30d7\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u306b\u3088\u308a\uff0c [4]: \u7c21\u5358\u306a\u6f14\u7b97\u56de\u8def\u3067 [5]: \u73fe\u5728\u306e\u30c7\u30fc\u30bf\u306b\u91cd\u307f\u3092\u7f6e\u3044\u305f [6]: \u5e73\u5747\u5024\u3092 [7]: \u7b97\u51fa\u53ef\u80fd\u3068\u3059\u308b [0] [1] [2] [3] [4] [5] [6] [7] [8] 0 0 0 0 0 9 0 0 0 : To calculate 0 0 0 0 0 0 0 182 0 : an average value 0 0 0 0 0 0 133 0 0 : weighed in the present data 0 0 0 0 0 252 0 0 0 : with a simple arithmetic circuit 0 0 0 0 53 0 0 0 0 : by repeating 0 0 0 97 0 0 0 0 0 : the loop 0 72 0 0 0 0 0 0 0 : of multiplication 124 0 0 0 0 0 0 0 0 : of the stored value 0 0 0 0 0 196 0 0 0 : in the arithmetic circuit 0 0 0 12 0 0 0 0 0 : and the addition 0 0 119 0 0 0 0 0 0 : of a new data To provide and establish a tube energized in vacuum a method for distributing additives during the process of taking out the blood To provide To provide and establish and establish a tube energized in vacuum a tube energized in vacuum a method a method for distributing additives for distributing additives during the process of taking out during the process of taking out the blood the blood To provide and establish a tube energized in vacuum a method for distributing additives during the process of taking out the blood To provide To provide and establish and establish a tube energized in vacuum a tube energized in vacuum a method a method for distributing additives for distributing additives during the process of taking out during the process of taking out the blood the blood   a binary parse tree on both language sides. Since we are not yet incorporating grammar rules in our phrase alignment system, the merge history-induced inner-structures of the obtained bilingual phrases are not quite linguistically intuitive, although the obtained phrases themselves are intended to be linguistically motivated. However, even within the current setting, the obtained alignment matrix can be useful for guiding parsing process or correcting parse results via interplay between parsers of both sides through the alignment matrix. Figure 6 illustrates an example. If we suppose that the Japanese parse tree is more reliable than the English parse tree, then the alignment matrix can be used to convert Japanese tree structure into English one and to correct the PP-attachment error of the original English parse tree in which \"by forming\" is attached to \"to perform\" instead of the correct attachment site which is the conjunction of the preceding two clauses. (a) (b) (c) (d) (e) (f) (t) (u) (v) (w) (x) (y) (z) (a) (b) (b) (c) (c) (d) (d) (e) (e) (f) (f) (t) (t) (u) (u) (v) (v) (w) (w) (x) (x) (y) (y) (z) (z) (a) (b) (c) (d) Experimets This section describes experiments with the proposed phrase alignment method. For the evaluation of the obtained phrase alignments, two types of experiments are conducted. One is to evaluate the F-measure of the obtained phrase alignments with respect to a hand crafted golden standard. The second type is to measure the quality of phrase-based SMT which uses the obtained phrase pairs as a bilingual corpus. Each experiment is described in the following subsections. We used the test collection of a parallel patent corpus from the Patent Retrieval Task of the 3rd NTCIR Workshop (2002) for training word alignments. The corpus comprises of patent abstracts of Japan (1995) (1996) (1997) (1998) (1999) and their English translation produced at Japan Patent Information Organization. We extracted 150 thousand sentence pairs from the PURPOSE part of the test collection of the year 1995. Each patent has its IPC category, from A through H. In-house English and Japanese parsers are used to chunk sentences and to make a constituent judgment. We also used in-house bilingual dictionary with 860 thousand word entries. For phrase alignment, we extracted 13,000 sentence pairs with English sentences of length smaller than 75 words, out of the sentence pairs in G-category (Physics) of the above word alignment training set. The sentence length is constrained to reduce the computational load. Table 2 summarizes the training corpora used. Out of 13,000 sentence pairs 208 thousand unique phrase pairs are extracted. More than one set of phrase alignments can often be extracted from one pair of aligned sentences when the evaluation score reaches zero. Figure 7 shows examples of obtained phrase alignments. Japanese phrases acquired are mostly constituents, whereas many of English phrases are not, such as \" by arranging\", or \"of infrared absorption ink\". This is partly due to the fact that Japanese phrases are constructed out of base phrases, or chunks, whereas English phrases are constructed starting from individual words. Another reason is the fact that Japanese precedence rule takes precedence over English one as stated in Table 1 . Evaluation of Phrases with Human Judgment Out of the 13,000 sentence pairs used for phrase alignments, 160 sentence pairs are randomly extracted for manual annotation. Although there have been a number of attempts to manually annotate word alignments, much less attempts have been made to construct a golden standard for phrase alignments. The major difficulty of aligning phrases is that there are many possible ways of aligning phrases, whereas word alignments have not much ambiguity. (c) Alignment matrix [0] [1] [2] [3] [4] [5] [6] [7] [8] [9][10] 0 0 0 0 0 0 0 0 31 0 0 : To be used 0 0 0 0 0 0 0 137 0 0 0 : as a packaging ma terial 0 0 0 0 0 0 350 0 0 0 0 : for preventing mi ldew of food or the other 0 0 0 0 0 0 0 0 0 0 1 : and to perform 0 0 0 0 0 0 0 0 0 80 0 : a mildewproofing effect 0 0 0 84 0 0 0 0 0 0 0 : by forming 0 0 428 0 0 0 0 0 0 0 0 : a resin layer con taining specific substance 0 62 0 0 0 0 0 0 0 0 0 : on one surface 215 0 0 0 0 0 0 0 0 0 0 : of a gas imperm eable film 0 0 0 0 0 88 0 0 0 0 0 : , and laminating 0 0 0 0 307 0 0 0 0 0 0 : a gas impermeable film thereon [6] [7] [8] [9] [10] 0 0 0 0 0 0 0 0 31 0 0 : To be used 0 0 0 0 0 0 0 137 0 0 0 : as a packaging ma terial 0 0 0 0 0 0 350 0 0 0 0 : for preventing mi ldew of food or the other 0 0 0 0 0 0 0 0 0 0 1 : and to perform 0 0 0 0 0 0 0 0 0 80 0 : a mildewproofing effect 0 0 0 84 0 0 0 0 0 0 0 : by forming 0 0 428 0 0 0 0 0 0 0 0 : a resin layer con taining specific substance 0 62 0 0 0 0 0 0 0 0 0 : on one surface 215 0 0 0 0 0 0 0 0 0 0 : of a gas imperm eable film 0 0 0 0 0 88 0 0 0 0 0 : , and laminating 0 0 0 0 307 0 0 0 0 0 0 : a gas impermeable film thereon [0] [1] [2] [3] [4] [5] (d) [0]:\u30ac\u30b9 \u4e0d\u900f\u904e\u6027\u30d5\u30a3\u30eb\u30e0\u306e [1]:\u4e00\u9762\u306b \uff0c [2]:\u7279\u5b9a \u7269\u8cea \u3092\u542b\u3080\u6a39\u8102 \u5c64 \u3092 [3]:\u5f62\u6210 \u3057 \uff0c [4]: \u305d\u306e\u4e0a\u306b \u30ac\u30b9 \u4e0d\u900f\u904e\u6027\u30d5\u30a3\u30eb\u30e0\u3092 [5]:\u7a4d\u5c64 \u3059\u308b \u3053\u3068\u306b\u3088\u308a\uff0c [6]: \u98df\u54c1 \u305d\u306e\u4ed6\u306e\u304b\u3073 \u767a\u751f \u3092 \u9632\u6b62 \u3059\u308b [7]:\u5305\u88c5\u6750\u6599\u3068\u3057\u3066 [8]:\u7528\u3044 \uff0c [9]:\u9632 \u304b\u3073\u52b9\u679c\u3092 [10]:\u767a\u63ee \u3059\u308b \uff0e To be used as a packaging material for preventing mildew of food or the other and to perform mildewproofing a resin layer containing specific substance To be used and to perform a effect by forming a resin layer containing specific substance on one surface of a gas impermeable film , and laminating a gas impermeable film thereon (b) Initial English parse tree (by a monolingual parser) (a) Japanese parse tree (by a monolingual parser) (c) Alignment matrix [0] [1] [2] [3] [4] [5] [6] [7] [8] [9][10] 0 0 0 0 0 0 0 0 31 0 0 : To be used 0 0 0 0 0 0 0 137 0 0 0 : as a packaging ma terial 0 0 0 0 0 0 350 0 0 0 0 : for preventing mi ldew of food or the other 0 0 0 0 0 0 0 0 0 0 1 : and to perform 0 0 0 0 0 0 0 0 0 80 0 : a mildewproofing effect 0 0 0 84 0 0 0 0 0 0 0 : by forming 0 0 428 0 0 0 0 0 0 0 0 : a resin layer con taining specific substance 0 62 0 0 0 0 0 0 0 0 0 : on one surface 215 0 0 0 0 0 0 0 0 0 0 : of a gas imperm eable film 0 0 0 0 0 88 0 0 0 0 0 : , and laminating 0 0 0 0 307 0 0 0 0 0 0 : a gas impermeable film thereon [6] [7] [8] [9] [10] 0 0 0 0 0 0 0 0 31 0 0 : To be used 0 0 0 0 0 0 0 137 0 0 0 : as a packaging ma terial 0 0 0 0 0 0 350 0 0 0 0 : for preventing mi ldew of food or the other 0 0 0 0 0 0 0 0 0 0 1 : and to perform 0 0 0 0 0 0 0 0 0 80 0 : a mildewproofing effect 0 0 0 84 0 0 0 0 0 0 0 : by forming 0 0 428 0 0 0 0 0 0 0 0 : a resin layer con taining specific substance 0 62 0 0 0 0 0 0 0 0 0 : on one surface 215 0 0 0 0 0 0 0 0 0 0 : of a gas imperm eable film 0 0 0 0 0 88 0 0 0 0 0 : , and laminating 0 0 0 0 307 0 0 0 0 0 0 : a gas impermeable film thereon [0] [1] [2] [3] [4] [5] (d) To be used as a packaging material for preventing mildew of food or the other and to perform mildewproofing a resin layer containing specific substance To be used and to perform a effect by forming a resin layer containing specific substance on one surface of a gas impermeable film , and laminating a gas impermeable film thereon To be used as a packaging material for preventing mildew of food or the other and to perform mildewproofing a resin layer containing specific substance To be used and to perform a effect by forming a resin layer containing specific substance on one surface of a gas impermeable film , and laminating a gas impermeable film thereon Figure 6 : An example of correcting an English parse result by the combination of Japanese parse tree and the alignment matrix. In the initial English parse tree (b), the phrase \"by forming\" can be interpreted to be attached to \"and to perform\". Through the alignment matrix (c), the Japanese parse tree (a) can be automatically mapped to the English parse tree (d) which can for instance derive the correct interpretation of the attachment site of the phrase \"by forming\". Since there is no obvious criterion to decide which phrase pairs are superior and which are not, we choose to extract all the possible ways of dividing a sentence pair into a set of bilingual phrases. Of course it is too much work for a human to exhaust all the possible combinations. However, there is a way of automatically generating all the possible phrase alignments from a result of manual work which is just repeating a simple task of dividing a phrase pair into pairs of sub-phrases. For example, consider a phrase pair in Figure 8 . The phrase pair (\"j1j2j3j4j5\", \"e1e2e3e4e5\") is first divided into two phrase pairs, (\"j1j2\", \"e4e5\") and (\"j3j4j5\", \"e1e2e3\"). There are in total four possible division steps like this: (12a) (\"j1j2j3j4j5\", \"e1e2e3e4e5\") \u21d2 (\"j1j2\", \"e4e5\") , (\"j3j4j5\", \"e1e2e3\") (12b) (\"j1j2\", \"e4e5\") \u21d2(\"j1\", \"e5\"), (\"j2\", \"e4\") (12c) (\"j3j4j5\",\"e1e2e3\") \u21d2(\"j3\",\"e3\"), (\"j4j5\", \"e1e2\") (12d) (\"j4j5\", \"e1e2\") \u21d2 (\"j4\", \"e2\"), (\"j5\", \"e1\") Given these four possible divisions, all the possible phrase alignments can be automatically calculated and the results are as follows. (\"j1j2\", \"e4e5\") ,(\"j3j4j5\", \"e1e2e3\") (\"j1j2\", \"e4e5\"), (\"j3\",\"e3\"), (\"j4j5\", \"e1e2\") (\"j1j2\",\"e4e5\"),(\"j3\",\"e3\"),(\"j4\",\"e2\"), (\"j5\",\"e1\") (\"j1\", \"e5\"), (\"j2\", \"e4\"), (\"j3j4j5\", \"e1e2e3\") (\"j1\",\"e5\"),(\"j2\",\"e4\"),(\"j3\",\"e3\"),(\"j4j5\", \"e1e2\") (\"j1\",\"e5\"),(\"j2\",\"e4\"),(\"j3\",\"e3\"),(\"j4\",\"e2\"), (\"j5\",\"e1\") Therefore the task of human annotator is to keep dividing a phrase pair into pairs of sub-phrases. The procedure of the manual annotation is as follows. 1) Let the aligned sentence pair be a pair of aligned phrases. 2) Pick a pair of aligned phrases and try to divide it into two constituents so that each of the Japanese sub-phrases can be regarded as a translation of either of the English subphrases. An Example is given in Figure 9 (a) and 9(b). 3) If 2) succeeds, repeat steps 2) through 4). If 2) fails, then try to divide the picked aligned pair of phrases into three, four, or more constituents in turn so that each of Japanese sub-phrases can be regarded as a translation of either of the English sub-phrases. 4) If 3) succeeds, repeat steps 2) through 4). Otherwise stop dividing the current pair of phrases and go through steps 2) through 4) with the next pair of phrases. If no more pair of phrases is available for dividing, terminate and output the set of division steps. Figure 9 shows an example of dividing a pair of sentences into aligned phrases. The set {(a), (b)} constitutes one division step like (12a), as is also the case with sets {(c), (d)} and {(e), (f)}. From manually created division steps for the 160 sentence pairs, all the possible phrase alignments are generated and stored as a set of golden standard. Outputs of phrase aligner for these 160 sentences are then compared with the golden standard. For each phrase alignment in the golden standard, F-measure is calculated with the system output, and the maximum value among all the phrase alignments of the golden standard is recorded as the F-measure of the system output. The mean value of the F-measures of all the 160 sentences was 80.4. The average number of phrases in a sentence for the golden standard phrase alignments which give the maximum F-measure was 6.0. Therefore it is not the case that the most simple phrase alignment, which is a partition of a sentence into two parts, is earning high F-measures. In order to examine the contribution of simple phrase alignments, F-measures are calculated by gradually eliminating from golden standard phrase alignments with small number of phrases. Table 3 shows the result. There are no big drops until MinNum = 4 , and after that F-measure declines rather rapidly. This also suggests that golden standard phrase alignments with 2 or three phrases are not playing a major role in the evaluation of the system outputs. Evaluation of Phrases with SMT The extracted phrase alignments were also evaluated with an SMT engine. We used Pharaoh (Koehn, 2004) as the baseline. Although our goal is to use obtained phrase alignments as translation units of Rule-based/SMT hybrid systems, we haven't yet processed large amount of parallel corpora, and the decoding scheme which takes advantage of the constituent oriented phrase alignments is still under development. Therefore, instead of testing the phrase alignments as translation units, we tested the cross-domain portability of the obtained phrase alignments. One of the major merits of a syntactic constituent is its generalization capability. N-gram statistics extracted from a large collection of data in a specific domain is a powerful resource within the same domain, but quite often fails to adopt to quite different domains. Constituents, or grammatical categories, on the other hand, cannot easily be tuned to a specific domain, but possess a generalization capability. In this experiment we trained Pharaoh using parallel sentences in one domain, namely IPC-G category (Physics), and tested the decoder in different domains. The training corpus we used for a baseline setting is the 13,000 sentence pairs in IPC-G category listed in Table 2 . We then used a set of aligned phrases extracted from the 13,000 sentence pairs for training Pharaoh (PhrAlign). The phrases are used alone and not mixed with the original parallel sentences. For testing, a set of 500 sentence pairs are randomly extracted from each IPC category. For development, another set of 500 sentence pairs are extracted from IPC-G category. Table 4 shows the result. PhrAlign outperforms Baseline in all the categoris. Especially in category E, PhrAlign scores 1.49 points higher than Baseline, which is relative percentage of 16% increase from Baseline. Since the training corpus is fairly small it is possible that the difference of the two cases decreases as the training data is increased, but this result suggests a generalizing capability of the syntactically oriented phrase alignments. Related work The inversion transduction grammar formalism (Wu, 1997) is one of the pioneering approaches for stochastically extracting bilingual phrases with constituent structure. A concept of bilingual parsing, where the input is a sentence pair rather than a sentence, is introduced in this framework. By allowing the inverse order of the righthand-side of productions, the expressiveness of the grammar is shown to be considerably enhanced. In order to control the computational complexity, however, several severe constraints are applied, which makes it difficult to apply ITG to free-word-order languages like Japanese. This formalism is also not intended to be robust against the translation lexicon inadequacies: sentences containing more than one word absent form the translation lexicon are rejected in the reported experiment. The proposed method, on the other hand, is quite robust to a sparse alignment matrix because of the utilization of statistical word-alignment and the robustness of the chunkers. Integrated Segmentation and Alignment (Zhang and Vogel, 2005) , or ISA, is probably most similar in concept to the proposed approach. ISA employs a greedy algorithm, called CGA, to extract phrase pairs out of a bilingual corpus. CGA extends the competitive linking algorithm (Melamed, 1997) , a greedy word alignment algorithm with one word-to-one word assumption, to allow for combining [6] [7] [8] [9] [10] 0 0 0 0 0 0 0 0 31 0 To be used 0 0 0 0 0 0 0 137 0 0 as a packaging material 0 0 0 0 0 0 350 0 0 0 for preventing mildew of food or the other 0 0 0 0 0 0 0 0 0 0 and to perform 0 0 0 0 0 0 0 0 0 80 a mildewproofing effect 0 0 0 84 0 0 0 0 0 0 by forming 0 0 428 0 0 0 0 0 0 0 a resin layer containing specific substance 0 62 0 0 0 0 0 0 0 0 on one surface 215 0 0 0 0 0 0 0 0 0 of a gas impermeable film 0 0 0 0 0 88 0 0 0 0 , and laminating 0 0 0 0 307 0 0 0 0 0 a gas impermeable film thereon Japanese Preference Constraint [0] [1] [2] [3] [4] [5] [0] [1] [2] [3] [4] [5] [6] [7] [8] [9][10][11][12][13][14] 0 0 0 0 0 0 0 0 0 0 0 0 0 47 To obtain 0 0 0 0 0 0 0 0 0 0 0 0 196 0 an information carrying sheet 0 0 0 0 0 0 0 0 0 0 0 0 0 0 in which an information pattern 0 0 0 0 0 0 0 0 0 0 0 95 0 0 is scarcely visually observed by bare eyes 0 0 0 0 0 0 0 0 0 0 23 0 0 0 by arranging 0 0 0 0 0 0 0 0 0 175 0 0 0 0 an information pattern 0 0 0 0 0 0 0 0 79 0 0 0 0 0 formed 0 0 0 0 0 0 0 208 0 0 0 0 0 0 of infrared absorption ink 0 0 0 0 0 0 58 0 0 0 0 0 0 0 containing 0 0 0 0 0 280 0 0 0 0 0 0 0 0 infrared absorption substance 0 0 0 0 16 0 0 0 0 0 0 0 0 0 represented 0 0 0 252 0 0 0 0 0 0 0 0 0 0 by the specific structural formula 0 0 89 0 0 0 0 0 0 0 0 0 0 0 on an upper surface 0 7 0 0 0 0 0 0 0 0 0 0 0 0 of a substrate 92 0 0 0 0 0 0 0 0 0 0 0 0 0 having infrared reflectivity 3 : F-measure with minimum number of phrases in the golden standard varied the detected \"sure\" word pair (a seed) with its neighbors to form a group. ISA uses \u03c7 2 statistics to measure the mutual translation likelihood between words, and the word pair with the highest \u03c7 2 value is selected as a seed. Neighboring words to be joined with the seed are also greedily searched on the basis of \u03c7 2 values. Although both approaches use a statistical measure for the decision of agglomeration, CGS uses a word-to-word association for the judgment of local grouping, whereas the proposed approach uses a sentence level, or global, association metric for the judgment of merging, which makes the merging judgment justifiable not only for the merged phrase pairs, but also for the other words and phrases in the sentence pair. The n-best search in the proposed method also avoids the greediness of the merging process. Another difference is that in order to make the computation tractable, ISA employs a \"locality assumption\" which requires that a source phrase of adjacent words only be aligned to a target phrase composed of adjacent words. This assumption is again not suitable for language pairs of a quite different word order like the pair of Japanese and English. Conclusion A novel approach is presented for extracting syntactically motivated phrase alignments. In this method we can incorporate conventional resources such as dictionaries and grammar rules into a statistical optimization framework for phrase alignment. The method extracts bilingual phrases by incrementally merging adjacent words or phrases on both source and target language sides in accordance with a global statistical metric along with constraints and preferences composed by combining statistical information, dictionary information, and also grammatical rules. The extracted phrases achieved a maximum F-measure of over 80 with respect to human judged phrase alignments. The extracted phrases used as a training corpus for a phrase-based SMT showed better cross-domain portability over conventional SMT framework.",
    "abstract": "A novel approach is presented for extracting syntactically motivated phrase alignments. In this method we can incorporate conventional resources such as dictionaries and grammar rules into a statistical optimization framework for phrase alignment. The method extracts bilingual phrases by incrementally merging adjacent words or phrases on both source and target language side in accordance with a global statistical metric. The extracted phrases achieve a maximum F-measure of over 80 with respect to human judged phrase alignments. The extracted phrases used as training corpus for a phrase-based SMT shows better cross-domain portability over conventional SMT framework.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese",
        "English"
    ],
    "numcitedby": "0",
    "year": "2007",
    "month": "September 7-9",
    "title": "Phrase alignment based on bilingual parsing"
}