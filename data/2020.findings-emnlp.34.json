{
    "article": "We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain, with the goal to train a good target model. A straightforward solution is to fine-tune a pre-trained source model by using those limited labeled target data, but it usually cannot work well due to the considerable difference between the data distributions of the source and target domains. Moreover, the availability of multiple modalities (i.e., images, questions and answers) in VQA poses further challenges in modeling the transferability between various modalities. In this paper, we address the above issues by proposing a novel supervised multi-modal domain adaptation method for VQA to learn joint feature embeddings across different domains and modalities. Specifically, we align the data distributions of the source and target domains by considering those modalities both jointly and separately. Extensive experiments on VQA 2.0 and VizWiz datasets demonstrate that our proposed method outperforms the existing stateof-the-art baselines for open-ended VQA in this challenging domain adaptation setting. Introduction The task of visual question answering (VQA) is to build a model for answering questions given an image-question pair. Recently, it has received great attention from computer vision community (Zhou et al., 2015; Kazemi and Elqursh, 2017; Tan and Bansal, 2019; Anderson et al., 2017; Kim et al., 2018; Zhang et al., 2018; Singh et al., 2019) . VQA requires techniques from both image recognition and natural language processing, and most existing works use Convolutional Neural Networks (CNNs) to extract visual features from images and Recurrent Neural Networks (RNNs) to generate textual features from questions, and then combine them to generate the final answers. However, most existing VQA datasets are created in a way that is not suitable as training data for real-world applications. For example, VQA 2.0 (Goyal et al., 2019) and Visual7W (Zhu et al., 2016) , arguably two of the most popular datasets for VQA, were created by using images from MSCOCO (Lin et al., 2014) with questions asked by crowd workers. Therefore, the images are typically of high quality and the questions are less conversational. On the contrary, the recently proposed VizWiz dataset (Gurari et al., 2018) was collected from blind people taking photos and asking questions about those photos. Therefore, the images in VizWiz are often of poor quality, and questions are more conversational with some questions might even be unanswerable due to the poor quality of the images. While VizWiz dataset reflects a more realistic setting for VQA, its size is much smaller due to the difficulty of collecting such data. A straightforward solution to this problem is to first train a model on the VQA 2.0 dataset and then fine-tune it using the VizWiz data. However, this solution can only provide limited improvement with two major issues. First, the VQA datasets are constructed in a different way, making them differ significantly in visual content, textual questions and answers. (Sha et al., 2018) conducted an experiment to classify different VQA datasets with a simple multi-layer perceptron (MLP) of one hidden layer and it achieved over 98% accuracy. This is a strong indication of the significant bias across different datasets. Our experiments also validate that directly fine-tuning the model trained on VQA 2.0 results in minor improvement on VizWiz. Sec-ond, the two modalities (visual and textual) also pose a big challenge in the generalizability across datasets. It is a nontrivial task to consistently bridge the domain gap in a coordinated fashion, when multiple modalities are involved, due to the nature of the multi-modal heterogeneity with no common feature representations. Domain adaptation methods, which handle the difference between two domains, have been developed to address the first issue (Hoffman et al., 2015; Koniusz et al., 2017; Tzeng et al., 2017; Ganin and Lempitsky, 2015; Shen et al., 2017; Gong et al., 2012; Guo and Xiao, 2012; Yao et al., 2015) . However, most existing domain adaptation methods focus on single-modal tasks such as image classification or sentiment classification, and thus may not be directly applicable to multi-modal settings. On the other hand, these methods are usually subject to a strong assumption on the label distribution that the source domain and the target domain share the same (usually small) label space, which is usually unrealistic. (Qi et al., 2018) proposed a new framework for unsupervised multimodal domain adaptation, but it was not designed for the VQA tasks. Recently, several VQA domain adaptation methods have been proposed to address the multi-modal challenge. However, to the best of our knowledge, all the existing VQA domain adaptation methods focus on the multiple choice setting, where several answer candidates are provided and the model only needs to select one from them. In contrast, we focus on a more challenging open-ended setting where there is no prior knowledge of answer choices. In this paper, we address the aforementioned challenges by proposing a novel multi-modal domain adaptation framework, which learns a multi-modal feature embedding that simultaneously keeps each domain invariant and each individual modality discriminative, based on an adversarial loss and a classification loss. We additionally incorporate the maximum mean distance (MMD) to further reduce the domain mismatch by learning embeddings from different modalities. Our contributions are summarized as follows: 1) We propose a novel supervised multi-modal domain adaptation framework to tackle the more challenging open-ended VQA task. To the best of our knowledge, this is the first attempt of using domain adaptation for open-ended VQA. 2) We propose a method that learns a multi-modal feature embedding that simultaneously keeps each domain invariant and each individual modality discriminative, with an adversarial loss and a classification loss. At the same time, it minimizes the difference of cross-domain feature embeddings jointly over multiple modalities. 3) We conduct extensive experiments on two popular benchmark datasets (i.e., VQA 2.0 and VizWiz), and the results clearly show the effectiveness of our proposed method over the existing state-of-the-art baselines. Related Work VQA datasets: Over the past few years, several VQA datasets (Zhu et al., 2016; Goyal et al., 2019; Gurari et al., 2018; Krishna et al., 2017; Antol et al., 2015) and tasks were proposed to encourage researchers to develop algorithms that answer visual questions. One limitation of many existing datasets is that they were created either automatically or from an existing vision dataset like MSCOCO (Lin et al., 2014) with the questions either generated automatically or contrived by human annotators. This makes the images in these datasets typically of high quality and the questions less conversational, and thus might not be directly applicable to real-world applications such as (Gurari et al., 2018) which aims to answer the visual questions asked by blind people in their daily life. The main differences between (Gurari et al., 2018) and other VQA datasets are as follows: 1) Both the image and question quality of (Gurari et al., 2018) are lower as they suffer from poor lighting, out of focus and audio recording problems like clipping a question at either end or catching background audio content; 2) The questions can be unanswerable since blind people can hardly verify whether the images contain the visual content they are asking about, due to blurring, inadequate lighting, framing errors, finger covering the lens, etc. Our experiments also reveal that fine-tuning the model trained on the VQA 2.0 dataset provides limited improvement on VizWiz, due to the significant difference in bias between both datasets. VQA settings: There are two main VQA settings, namely multiple choice and open-ended following (Antol et al., 2015) 1 . Under the multiple choice setting, the model is provided with multiple candidates of answers and is expected to se-lect the correct one. VQA models following this setting take characteristics of all answer candidates like word embeddings as the input to make a selection (Sha et al., 2018; Jabri et al., 2016) . However, in the open-ended setting, there is neither prior knowledge nor answer candidates provided, and the model can respond with any freeform answers. This makes the open-ended setting more challenging and realistic (Kim et al., 2018; Kazemi and Elqursh, 2017; Singh et al., 2019; Anderson et al., 2017) . VQA models: Recently, a plethora of VQA models were proposed (Zhou et al., 2015; Kazemi and Elqursh, 2017; Anderson et al., 2017; Kim et al., 2018; Singh et al., 2019) . Most of them consist of image and question encoders, and a multi-modal fusion module followed by a classification module. (Kazemi and Elqursh, 2017) used an LSTM to encode the question and a residual network (He et al., 2015) to compute the image features with a soft attention mechanism. (Anderson et al., 2017) implemented a bottom-up attention using Faster R-CNN (Ren et al., 2015) to extract features of detected image regions, and then a top-down mechanism used task-specific context to predict an attention distribution over the image regions. The final output was generated by an MLP after fusing the image and question features. (Kim et al., 2018) used a bilinear attention between two groups of input channels on top of low-rank bilinear pooling which extracted the joint representations for each pair of channels. (Singh et al., 2019) proposed an approach that takes original image features, bottom-up attention features from object detection module, question features and the optical character recognition (OCR) strings detected from the image as the input, and answers either with an answer from the fixed answer vocabulary or by selecting one of the OCR strings detected in the image. Similar to the state-of-the-art model by (Singh et al., 2019) , our VQA base model also takes original image features, bottom-up attention features and question features to predict the final answer. Details of our VQA base model is described in the next section. Domain adaptation: Domain adaptation techniques have been proposed to learn a common domain invariant latent feature space where the distributions of two domains are aligned. Recent works typically focused on transferring knowledge from a labeled source domain to a tar-get domain where there is no or limited labeled data (Hoffman et al., 2015; Koniusz et al., 2017; Tzeng et al., 2017; Shen et al., 2017; Ganin and Lempitsky, 2015; Gong et al., 2012; Guo and Xiao, 2012) . (Hoffman et al., 2015) optimized for domain invariance to facilitate domain transfer and used a soft label distribution matching loss to transfer information between tasks. (Tzeng et al., 2017) proposed a framework which combines discriminative modeling, untied weight sharing and a GAN loss to reduce the difference between domains. (Shen et al., 2017) estimated empirical Wasserstein distance between the source and the target samples and optimized the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. (Ganin and Lempitsky, 2015) utilized gradient reversal layer (GRL) to incorporate the training process of domain classifier, label classifier and feature extractor to align domains. Similarly, (Guo and Xiao, 2012) simultaneously minimized the classification error, preserved the structure within and across domains, and restricted similarity on target samples. The major difference between our work and these works is that we propose a novel multi-modal domain adaptation framework, while these works assumed a single modality. Domain adaptation for VQA: Although domain adaptation has been successfully applied to computer vision, its applicability to VQA has yet to be well-studied. There was one recent work investigating domain adaptation for VQA by (Sha et al., 2018) . It reduces the difference in distributions by transforming the feature representation of the data in the target domain. However, one major limitation is the assumption of a multiple choice setting, where four answer candidates are provided as the input to the model. It is unrealistic because one can never guarantee that the ground truth answer is among four candidates. Moreover, it is unclear how to create answer candidates for an image-question pair. On the contrary, our model is only provided with an image-question pair and can generate any free-form answers. This makes our task more challenging and realistic. The VQA Framework In this section, we describe our base VQA framework. Given an image I and a question Q, the VQA model estimates the most likely answer \u00e2 from a large vocabulary based on the content of the image, which can be written as follows: \u00e2 = argmax a P (a|I, Q). (1) Our base framework consists of four components: 1) a question encoder; 2) an image encoder; 3) a multi-modal fusion module; and 4) a classification module. We will elaborate about each component in the following subsections. Question encoding: The question Q of length T is first tokenized and encoded using word embedding based on pre-trained GloVe (Pennington et al., 2014) as S = {x 0 , x 1 , ..., x T }. These embeddings are then fed into a GRU cell (Cho et al., 2014) . The encoded question is obtained from the last hidden state at time step T denoted as q = f q (Q; \u03b8 q ) \u2208 R dq , where f q (Q; \u03b8 q ) = h T , h t = GRU(x t , h t\u22121 ; \u03b8 q ) for 1 \u2264 t \u2264 T , and d q is the feature dimension. Image encoding: Similar to (Anderson et al., 2017) and (Singh et al., 2019) , we first feed the input image I to an object detector (Girshick et al., 2018) pre-trained on the Visual Genome dataset (Krishna et al., 2017) based on Feature Pyramid Networks (FPN) (Lin et al., 2016) with ResNeXt (Xie et al., 2017) as the backbone. The output from f c6 layer is used as the region-based features, i.e., V r = {v 1 , v 2 , ..., v K } with v i as the feature for i-th object. Meanwhile, we divide the entire image into a 7 \u00d7 7 grid, and obtain the grid-based features V g by average pooling features from the penultimate layer 5c of a pretrained ResNet-101 network (He et al., 2015) on ImageNet dataset. Finally, we combine V r and V g as well as the question embedding q to obtain the joint feature embedding in a multi-modal fusion module (see next paragraph for more details). Multi-modal fusion and classification: The question embedding q is used to obtain the attention weights on region-based image features V r . Then, the region-based features V r are averaged based on the attention weights to obtain the weighted region-based image features. Similarly, grid-based features V g are fused with question embedding q by concatenation. The fused grid-based features and the weighted region-based image features are concatenated to obtain the final image features v. We have also tried other combination schemes such as (Ben-younes et al., 2017; Yu et al., 2018 Yu et al., , 2017)) , but they fail to outperform concatenation and are much slower. Since our focus is on domain adaptation instead of the base VQA model, we use concatenation in our work. We denote the final image feature embedding as v = f v (q, I; \u03b8 v ). The final joint embedding e = f j (q, v) is calculated by taking the Hadamard product of q and v, and then is fed to an MLP f c (e; \u03b8 c ) for classification, i.e., a = f c (e; \u03b8 c ). The final answer is determined by \u00e2 = argmax a f c (e; \u03b8 c ). Multi-Modal Domain Adaptation In this section, we present our framework for supervised multi-modal domain adaptation. We assume there are two modalities 2 of source samples X s = [X a s , X b s ], which would be vision and language in the context of VQA, where a, b denote the two modalities, and labels Y s drawn from a source domain joint distribution P s (x, y), as well as the two modalities of target samples X t = [X a t , X b t ] and labels Y t drawn from a target joint distribution P t (x, y). We also assume there are sufficient source data so that a good pre-trained source model can be built, but the amount of target data is limited so that learning on only the target data leads to poor performance. Our goal is to learn the target representations for two modalities f a t , f b t , the multi-modal fusion f j t and the target classifier f c t with the help of the pre-trained source representations f a s , f b s , f j s and the source classifier f c s . For the VQA task in our work, a, b denote visual and textual modalities, respectively. A typical approach to achieve this goal is to regularize the learning of the source and target joint representations by minimizing the distance of empirical distributions between the source and target domains, i.e., between f j s f a s (X a s ; \u03b8 a s ), f b s (X b s ; \u03b8 b s ); \u03b8 j s and f j t f a t (X a t ; \u03b8 a t ), f b t (X b t ; \u03b8 b t ); \u03b8 j t . In this way, the data from the source domain and the target domain are projected onto a similar latent space, such that well-performing source model can lead to well-performing target model. Following this idea, we propose a novel multi-modal domain adaptation framework as shown in Figure 3 . Joint Embedding Alignment We propose to reduce the difference of joint embeddings between the source and the target domains by minimizing the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) . The intuition is that two distributions are identical if and only if all of their moments coincide. Empirically, we can minimize the following object function MMD(Xs,Xt)= 1 ns ns i=1 \u03d5(x s i )\u2212 1 n t n t i=1 \u03d5(x t i ) H . (2) We then define the loss function as L j = E Xs\u223cps,Xt\u223cpt MMD 2 (e s , e t ) , (3) where e s = f j s f a s (X a s ; \u03b8 a s ) , f b s (X b s ; \u03b8 b s ); \u03b8 j s and e t = f j t f a t (X a t ; \u03b8 a t ), f b t (X b t ; \u03b8 b t ); \u03b8 j t . By minimizing the difference between source and target joint embeddings, we enforce that the joint embeddings of both source domain and target domain will be projected onto a similar latent space. Multi-Modal Embedding Alignment It is more challenging to reduce multi-modal domain shift than conventional single-modal domain shift. The previous loss L j in Eq. ( 3 ) does not explicitly consider the multi-modal property. Aligning only the joint feature embedding is insufficient to adapt the source domain to the target domain. This is because the feature extractor for each modality has its own complexity of domain shift, which often differs from each other (e.g., visual vs. textual). Aligning only the fused features cannot fully reduce domain differences. Therefore, we introduce the following term to minimize the maximum mean discrepancy between every single modality, i.e., MMD (f a s (X a s ; \u03b8 a s ), f a t (X a t ; \u03b8 a t )) and MMD f b s (X b s ; \u03b8 b s ), f b t (X b t ; \u03b8 b t ) . Then, the loss function to minimize can be written as L mm = E Xs\u223cps,Xt\u223cpt \u03b3 a MMD 2 (f a s (X a s ; \u03b8 a s ), f a t (X a t ; \u03b8 a t )) + \u03b3 b MMD 2 f b s (X b s ; \u03b8 b s ), f b t (X b t ; \u03b8 b t ) , (4) where \u03b3 a and \u03b3 b are trade-off parameters. Classification While minimizing the distance between source and target embeddings, we also want to maintain the classification performance on both the source domain and the target domain. Similarly as in a standard supervised learning setting, we employ the cross entropy loss for classification: L c = E (Xt,Yt)\u223cpt [CE(f c t (e t ; \u03b8 c t ), Y t )] + \u03b3 c E (Xs,Ys)\u223cps [CE(f c s (e s ; \u03b8 c s ), Y s )] , (5) where CE denotes the cross entropy loss with \u03b3 c as the trade-off parameter between the two domains. Domain Discriminator We also propose to use a domain classifier f d to reduce the mismatch between the source domain and target domain by confusing the domain classifier from correctly distinguishing a sample from source domain or target domain. The domain classifier f d has a similar structure to f c t or f c s except the last layer outputs a scalar in [0, 1] with the value indicating how likely the sample comes from the source domain. Thus, f d can be optimized according to a standard cross-entropy loss. To make the features domain-invariant, the source and target mappings are optimized according to a constrained adversarial objective. The domain classifier minimizes this objective while the encoding model maximizes this objective. The generic formulation for domain adversarial technique is: L adv = \u2212 E Xs\u223cps log f d (e s ; \u03b8 d ) \u2212 E Xt\u223cpt log(1 \u2212 f d (e t ; \u03b8 d )) . (6) For simplicity, we denote \u03b8 F = \u03b8 a s , \u03b8 a t , \u03b8 b s , \u03b8 b t , \u03b8 j s , \u03b8 j t as the parameters of all feature mappings and \u03b8 C = (\u03b8 c s , \u03b8 c t ) as the parameters of all label predictors. The final objective function to minimize then becomes: L \u03b8 F ,\u03b8 C ,\u03b8 d=L c +\u03bb j L j +\u03bb mm L mm \u2212\u03bb adv L adv . (7) We seek a saddle point \u03b8F , \u03b8C , \u03b8d of L which satisfies the following conditions: \u03b8F , \u03b8C = argmin \u03b8 F ,\u03b8 C L(\u03b8 F , \u03b8 C , \u03b8d ) \u03b8d = argmax \u03b8 d L( \u03b8F , \u03b8C , \u03b8 d ). (8) At the saddle point, the parameters \u03b8 d of the domain classifier minimize the domain classification loss L adv (since we maximize \u2212L adv ) while the parameters \u03b8 C of the label predictor minimize the label prediction loss L c . The feature mapping parameters \u03b8 F minimize the label prediction loss such that the features are discriminative, while maximizing the domain classification loss such that the features are domain-invariant. In addition to MMD which explicitly aligns the distributions, domain discriminator implicitly aligns the distributions, leading to stronger regularization in the non-convex optimization problem. Experiments In this section, we validate our proposed method for the challenging open-ended VQA task, by comparing with a few state-of-the-art baselines. Datasets Two popular VQA benchmarks are used in our experiments, VQA 2.0 (Goyal et al., 2019) and VizWiz (Gurari et al., 2018) . A comparison of the statistics for both datasets are listed in Table 1 , which shows that the scale of VizWiz is much smaller in terms of the numbers of images and questions. Although VizWiz has more unique answers, only 824 out of its top 3,000 answers overlap with the top 3,000 answers in VQA 2.0. This explains why models trained on VQA 2.0 perform poorly on VizWiz, and their limited transferability. There are 28.63% of questions in VizWiz are even not answerable due to reasons mentioned before, making the domain gap even more significant. Figure 2 shows some examples from both VQA 2.0 and VizWiz datasets. The difficulty of the task can also be seen from the VizWiz samples: images are blurry, viewpoints are unusual, some questions are unanswerable, and ground truth answers are highly inconsistent (e.g., \"soda\", \"coca cola 0\", \"coke 0\"). Evaluation Metrics In VQA, each question is usually associated with 10 valid answers from 10 distinct annotators. We follow the conventional evaluation metric on the open-ended VQA setting to compute the accuracy using the following formula: Acc(ans) = min # humans said ans 3 , 1 . (9) Namely, an answer is considered correct if at least three annotators agree on the answer. Note that the true answers in VizWiz test set are not publicly available. In order to obtain the performance on the test set, results need to be uploaded to the official online submission system at https://evalai.cloudcv.org/web/ challenges/challenge-page/102. Implementation Details In all experiments, we extract K = 100 objects for each image to construct the region-based features V r and set the visual feature dimension to 2048. We also set the hidden dimension of GRU to 1024 and hidden dimension after fusion to 4096. The question length is truncated at 24. During training, we apply a warm-up strategy by gradually increasing the learning rate \u03b7 from 0.001 to 0.01 in the first 2000 iterations. It is then multiplied by 0.15 after every 4000 iterations. We use a batch size of 128. For domain adaptation, we let the source and target networks share the same parameters up to the penultimate layer, i.e., \u03b8 v = \u03b8 v s = \u03b8 v t and \u03b8 q = \u03b8 q s = \u03b8 q t . In multi-or single-modal alignment, we use Gaussian kernel k(x, y) = exp \u2212 ||x\u2212y|| 2 2\u03c3 2 to compute MMD, because the Gaussian kernel can approximate functions under mild assumptions (continuous, bounded) fairly well, while other kernels such as the polynomial kernel do not have such properties. The trade-off parameters are set as \u03bb j = 0.025, \u03bb mm = 0.008, \u03b3 v = 0.8, \u03b3 q = 1, \u03b3 c = 0.001, and \u03bb adv = 0.003. Experimental Setup First, we conduct experiments using VQA 2.0 as the source domain and VizWiz as the target domain, to evaluate the effectiveness of our proposed method for multi-modal domain adaptation. We also conduct experiments in the opposite way, i.e., using VizWiz as the source domain and VQA 2.0 as the target domain, to further demonstrate the effectiveness of our approach. We need to emphasize that we choose not to use an overly strong base model (i.e., question embedding from FastText, complex fusion techniques, OCR tokens etc.), as our focus is on multi-modal adaptation instead of the base model itself. Despite that, we will show that our proposed domain adaptation method with a weaker base model still outperforms the fine-tuned state-of-the-art model. Results and Analysis Adaptation from VQA 2.0 to VizWiz: As discussed in previous sections, we first pre-train a source model on the VQA 2.0 dataset, and then adapt it to the target dataset VizWiz. The results of our proposed method and other leading methods are shown in Table 2 . We first compare our method with the original VizWiz baseline proposed by (Gurari et al., 2018) , the previous state-of-the-art VQA model BAN by (Kim et al., 2018) and the current state-of-the-art VQA model Pythia by (Singh et al., 2019) . It is clear that our method outperforms the state-of-theart models by a significant margin from Table 2 . 3  In order to validate that the better performance of our method is not from a strong base model, we additionally report the results of our method in Table 3, with 1) training our single base model from scratch using only the VizWiz dataset (Target only), 2) fine-tuning from the model pre-trained on the VQA 2.0 dataset (Fine-tune), and 3) our proposed domain adaptation method (DA). From Method Accuracy VizWiz baseline (Gurari et al., 2018) 47.50 BAN (Kim et al., 2018) 51.40 Pythia 4 (Singh et al., 2019) 54.72 Ours 55.87 (Ganin and Lempitsky, 2015) , ADDA (Tzeng et al., 2017) , WDGRL (Shen et al., 2017), and SDT (Hoffman et al., 2015) . Note that DANN, ADDA and WDGRL were originally designed for unsupervised domain adaptation. For fair comparison, we fine-tune the model using target labels after unsupervised adaptation (hence they are indicated by a suffix '+'), and we also compare with a popular and effective supervised domain adaptation method SDT. The results shown in Table 6 illustrate that compared to direct fine-tuning, the existing domain adaptation methods do not help much (DANN performs even worse) in the multi-modal task, while our method outperforms both direct fine-tuning and existing domain adaptation methods by a notable margin. (Ganin and Lempitsky, 2015) 53.65 ADDA+ (Tzeng et al., 2017) 54.06 WDGRL+ (Shen et al., 2017) 54.28 SDT (Hoffman et al., 2015) 54.56 Ours 55.87 Adaptation with fewer target training samples: We also validate the robustness of our framework by reducing the target training dataset size. We experiment with various target sizes of 12.5% (2,500), 25% (5,000), 50% (10,000) and all data (20,000). The results are shown in Table 7 . We can observe that with the increase of the amount of training data, the performance gain over fine- tuning is decreasing. We conjecture that this is because when we have limited amount of target data, having more prior knowledge is beneficial to model performance, while having more target data will make prior knowledge less helpful. However, our method can stably improve the performance because it sufficiently makes use of target data and source data. It is more promising that our domain adaptation method using fewer samples can achieve comparable or even better performance compared with training from scratch using doubled amount of data (especially when target data is scarce), e.g., our method using 25% data (48.93%) outperforms training from scratch using 50% data (47.48%). Adaptation from VizWiz to VQA 2.0: In order to further validate the robustness of our method, we reverse the source domain and the target domain and perform adaptation. We pre-train the source model on VizWiz and adapt the source model to VQA 2.0. The results are shown in Table 8 , from which we still can observe a significant improvement for our method against fine-tuning. In comparison, the performance of MFH (Yu et al., 2018) , BAN and Pythia is 67.7%, 69.08% and 69.21%, respectively, all under-performing our proposed method. Our DA model achieves comparable performance to the state-of-the-art on VQA 2.0. Conclusion We have presented a novel supervised multi-modal domain adaptation framework for open-ended visual question answering. Under the proposed framework, we have developed a new method for VQA which can learn a multi-modal feature em- bedding that simultaneously keeps each domain invariant and each individual modality discriminative. We validate our proposed method on two popular VQA benchmark datasets, VQA 2.0 and VizWiz, in both directions of adaptation. The experimental results show our method outperforms the state-of-the-art methods.",
    "abstract": "We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain, with the goal to train a good target model. A straightforward solution is to fine-tune a pre-trained source model by using those limited labeled target data, but it usually cannot work well due to the considerable difference between the data distributions of the source and target domains. Moreover, the availability of multiple modalities (i.e., images, questions and answers) in VQA poses further challenges in modeling the transferability between various modalities. In this paper, we address the above issues by proposing a novel supervised multi-modal domain adaptation method for VQA to learn joint feature embeddings across different domains and modalities. Specifically, we align the data distributions of the source and target domains by considering those modalities both jointly and separately. Extensive experiments on VQA 2.0 and VizWiz datasets demonstrate that our proposed method outperforms the existing stateof-the-art baselines for open-ended VQA in this challenging domain adaptation setting.",
    "countries": [
        "United States",
        "China"
    ],
    "languages": [
        "Gan"
    ],
    "numcitedby": "9",
    "year": "2020",
    "month": "November",
    "title": "Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation"
}