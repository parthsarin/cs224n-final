{
    "article": "In order to build an automatic named entity recognition (NER) system for machine learning, a large tagged corpus is necessary. This paper describes the manual construction of a Chinese named entity tagged corpus (CNEC 1.0) that can be used to improve NER performance. In this project, we define five named entity tags: PER (person name), LOC (location name), ORG (organization name), LAO (location as organization), and OAL (organization as location) for named entity categories. In addition, we propose a special tag, DIFF (Difficulty), to annotate ambiguous cases during corpus construction. A, corpus-annotating procedure, a tagging tool, and an original corpus are also introduced. Finally, we demonstrate a part of our manual-tagged corpus. Introduction Named entity recognition (NER), which includes the identification and classification of certain proper nouns in a text, is an important task in information extraction. It is useful in many natural language processing systems for document indexing and managing data with named entities [Tsai et. al 2004] . Since numerous new proper nouns are generated every day, it is not enough for an IR system to index names from Internet documents or refer to gazettes. Therefore, NER has become an important method for information processing in recent years. Machine learning (ML) is one of the most popular methods in NER, due to its easy maintenance and portability [Tsai et. al 2004] . Typical machine learning approaches applied in NER include the Hidden Markov Model (HMM) [Bikel et. al 1997] , Support Vector Machine (SVM) [Asahara 2003 ], and Maximum Entropy (ME) [Borthwick 1998 ]. No matter which approach is used, a tagged named entity corpus with clear annotating criteria is needed in the training phase of building an NER system. However, constructing such a corpus is a labor-intensive task, so few researchers have focused on it. The Automatic Content Extraction program (ACE) executed by the Linguistic Data Consortium (LDC) [Http://wave.ldc.upenn.edu/] annotates seven common entities in English, simplified Chinese, and Arabic. Meanwhile, the IREX (Information Retrieval and Extraction Exercise) [Http://nlp.cs.nyu.edu/irex/] defines 8 kinds of named entities (NE) in Japanese [Sekine and Isahara 2000] , and the shared task in CoNLL 2002 and 2003 [Erik 2002 ] [Erik et. 2002 ] develops the NER system using four types of NE in English, German, Dutch, and Spanish. However, as none of these methods focus on traditional Chinese, there is an urgent need for a traditional Chinese NE corpus and NE annotating standards to support an automatic Chinese NER system like Mencius [Tsai et. al 2004] . The categories of named entities defined by Message Understanding Conferences (MUC) are the names of persons, organizations, locations, temporal expressions and number expressions [Grishman and Sundheim 1996] . Since temporal and number expressions, such as \"the past year\" and \"40 percent\", are generally used as adjectives to describe other entities, we disregard them and focus on the annotation of person names, organization names, and location names as NEs. We separate organizations and locations into four non-overlapping categories to accommodate common Chinese usage. We also propose a temporary tag, \"Difficulty\", to represent named entities that are ambiguous. The remainder of this paper is organized as follows. Section 2 discusses the main issues of labeling named entities. Section 3 introduces all the NE categories used in CNEC1.0. Section 4 describes our annotation procedure and environment. Finally, in Section 5, we present our conclusion and the direction of future research. Named entities annotation issues The applications of a corpus determine the kinds of entities to be tagged. In our project, the NER system should support information extraction, question answering, and information retrieval of new documents. The following three issues should be considered before tagging Chinese named entities. Proper nouns We think named entities should be proper nouns. Therefore, each named entity should denote a unique object, so words without uniqueness should not be annotated. For example, in the sentence \"\u4ed6\u628a\u8eca\u505c\u653e\u5728\u505c\u8eca \u5834/He parked his car in the parking lot.\", we cannot be sure which parking lot he parked in. Therefore, the term \"\u505c\u8eca\u5834/parking lot\" will not be labeled Inner feature and Outer feature Generally, a named entity can be determined in three ways: viewing its literal meaning, checking the context, and semantic understanding as shown in the following example: \"\u53f0\u5317\u5e02\u9577\u99ac\u82f1\u4e5d/ Mayor of Taipei City, Ma Ying-Jeou\" \"\u53f0\u5317\u8eca\u7ad9\u4eba\u6f6e\u6d36\u6e67/Taipei Main Station is crowded.\" \"\u6211\u807d\u8aaa\u9060\u6771\u642c\u5bb6\u4e86/I heard that Yuan-Dong has moved out.\" Obviously, we can easily determine that the term \"\u99ac\u82f1\u4e5d/ Ma Ying-jeou\" in Sentence (1) is a person name according to the position of the title \"\u53f0\u5317\u5e02\u9577/ Mayor of Taipei City\". \"\u53f0\u5317\u8eca\u7ad9/Taipei Main Station\" in sentence (2) can be identified as a location name because of the term \"\u8eca\u7ad9/Station\". However, sometimes we cannot identify or judge a word as a proper noun by its literal meaning. For example, in Sentence (3), we do not know if the term \"\u9060\u6771/ Yuan-Dong\" represents a person or a company. These kinds of inaccuracies are due to abbreviations or borrowings. For this reason, two types of feature are used to classify the recognition modes of named entities\uff1athe inner feature and the outer feature. In the above examples, sentence (1) is the outer feature type, while sentence (2) can be classified by its inner features. In our work, we do not limit the types of features annotators apply during tagging, but if a named entity cannot be identified by both inner and outer features, as in sentence (3), we ask annotators not to mark it. This eliminates confusing terms and keeps the corpus as clear as possible Maximum and minimum semantic unit matching Named entities are occasionally nested or appear next to one another in a text. In some cases we can combine them to form a larger entity because they may describe the same object. Therefore, determining the boundary of named entities is an important issue. We have found that different named entities have a corresponding annotation policy, which can be classified as maximum and minimum semantic unit matching. Minimum semantic unit matching is recommended for named entities such as person names and location names because these entities singly represent a unique item. For example, the sentence \"\u53f0\u5317\u7e23\u677f\u6a4b\u5e02/Ban-Qiao City, Taipei County\" is tagged as two place names because \"\u53f0\u5317\u7e23/Taipei County\" and \"\u677f\u6a4b\u5e02/Ban-Qiao City\" both denote specific independent entities. On the other hand, named entities such as organizations should apply the maximum unit policy. The term \"\u53f0\u5317\u5e02\u74b0\u4fdd\u5c40/Department of Environmental Protection, Taipei City Government\" cannot be separated into \"\u53f0\u5317\u5e02/Taipei City\" and \"\u74b0\u4fdd\u5c40/Department of Environmental Protection\" for retaining the original meaning. Named entity categories We propose five target named entity categories for annotating the \"unique identifiers\" of entities, including organizations, persons and locations, as well as one function tag. These are shown in Table 1 and explained in the following sub-sections. For practical purposes, we began our experiment with these NE tags. In our opinion, DIFF is essential for identifying ambiguous cases. Named entities that are difficult to classify are isolated from others for data cleansing to ensure that the content of the corpus is clear. DIFF entities will become the future expansion direction of our NER processing domain. Person name -PER Traditionally, the structure of Chinese person names follows the principle that the surname (one or two characters) is placed before the person's chosen names (one or two characters). In our research, the annotation of person names follows this principle. But some entities with \"person\" meaning as Diff tag such as nicknames, incomplete Chinese person names, foreigners' names and pronouns, are marked as Diff. These exceptions are discussed below. Nicknames Nicknames are not only given to people, but are sometimes given to pets or even objects like toys and vehicles. Because of their uniqueness, we mark nicknames as DIFF within a context, as the following example shows. [\u5c0f\u70b3<DIFF>] \u75bc\u611b\u7684\u5973\u5152 [\u592e\u592e<DIFF>] [xiao-bing<DIFF>] loves his daughter [yang-yang<DIFF>] Incomplete Chinese person names Following the full name principle, an incomplete Chinese person name indicates that the surname or chosen name may be omitted. For instance, \"Shin, Cheng-Wei\" is a full person name, but we sometimes only use the chosen name \"Cheng-Wei\" to address the person. Another example of an incomplete person name is a surname that follows a title or an appellation such as \"\u674e\u5148\u751f(Mr. Lee)\". Both of these cases are tagged as DIFF. [\u9673\u7e3d\u7d71<DIFF>] \u4e0a\u5348\u524d\u5f80\u65e5\u672c\u5317\u6d77\u9053\u904a\u73a9 [President Chen<DIFF>] went to Hokaido for sightseeing this morning. Foreigners' names Chinese NER has difficulty dealing with foreigners' names because of the following name constructions: direct translation, Japanese person names, and Korean person names. First, direct translation cannot normally meet the principle of Chinese person names. For example, the Chinese translation of \"Mel Gibson\" is \"\u6885\u723e\u5409 \u52c3\u905c(Mei-er-ji-bo-xun)\", which obviously doesn't meet the naming rule. Second, Japanese mostly uses Chinese characters for person names, but, there isn't a surname or composite first name as in Chinese person names. For example, in \"\u65e5\u672c\u9996\u76f8(Japanese prime minister)[\u5c0f\u6cc9\u7d14\u4e00\u90ce](Junichiro Koizumi)\", Koizumi is his surname and Junichiro is his first name. These names don't match the Chinese person naming principle. Finally, we treat Korean person names as PER because most of them match the Chinese naming rules, e.g. \"\u674e\u82f1\u611b /Lee-Ying-Ai\". As most foreigners' names may cause confusion in Chinese NER, we use DIFF tag as a temporary solution in CNEC 1.0 to solve the problem. [\u6885\u723e\u5409\u52c3\u905c<DIFF>] \u5c0e\u6f14\u53d7\u96e3\u8a18\u540d\u5229\u96d9\u6536 [Mel Gibson<DIFF>] has achieved both fame and wealth by directing the movie \"The Passion of the Christ.\" Pronouns Some NER researchers claim their systems can handle pronouns as named entities for person names. However, because of the \"uniqueness\" of NEs, pronouns are beyond our scope and we do not annotate them as NE tags. Location names -LOC Basically, a location name pinpoints a place's geographical position on an accurate map or in other reference material. Proper names like \"Hyde Park\", \"New York Art Theater\" or \"Berlin Wall\" are suitable for NER, but terms like \"a park\", \"a theater\", or \"a wall\" are not. So the main purpose in tagging location names is to recognize an existent location in geographic. A location name included in another compound word such as \"\u897f\u73ed\u7259\u6d77\u9bae\u98ef\"(Spanish seafood rice) is an issue in NE annotation. In Chinese, a term's noun and adjective forms are the same, In this case, \"Spanish\" and \"Spain\" are translated as the same Chinese word(\u897f\u73ed\u7259). Therefore, we suggest a syntactic frame: insert \"de (\u7684)\" between a possible location name and the other words close to (A de(\u7684) B) to solve such cases. Chinese Word 1: \u897f\u73ed\u7259\u6d77\u9bae\u98ef In English: Spanish seafood rice [A de B] in Chinese: [\u897f\u73ed\u7259][\u7684][\u6d77\u9bae\u98ef] In English: [Seafood Rice] [of] [Spain] Maximum and minimum semantic unit selection (section 2.3) is regarded as a Chinese segmentation problem. For example, in the phrase \"\u7f8e\u570b(United States)\u5fb7\u5dde(Texas)\u5967\u65af\u6c40(Austin)\", there are no spaces between the words in written Chinese. Location follows minimum semantic unit matching to tag the example as [\u7f8e\u570b(United Sate)<LOC>]\u3001[\u5fb7\u5dde(Texas)<LOC>]\u3001[\u5967\u65af\u6c40(Austin)<LOC>]. In addition to the basic tagging rule, several location types have to be labeled Roads, sections, and addresses Address' location information should be marked from country name, state, city, road (Boulevard, avenue, street, etc.,) to section. Other information in an address is excluded. [\u5fe0\u5b5d\u6771\u8def\u56db\u6bb5<LOC>] 100\u865f [Zhong-Xiao East Road, Section 4<LOC>], No. 100 Sometimes a road section can be described in a city-section or area-section. In this case the road name and its description should be tagged separately. [\u897f\u6ff1\u5feb\u901f\u9053\u8def<LOC>] [\u5609\u7fa9<LOC>] \u6bb5 [Xi-Bin Express Way<LOC>] [Jia-Yi<LOC>] Section Location abbreviations Location abbreviations are terms composed of two or more place names in a single entity. The other type of location abbreviation is multi-name expression containing conjoined modifiers. For instance, \u4e2d(Taichung)\u5f70 (Changhua)\u6295(Nantou)\u5730\u5340(area) is a common way to describe the location of three neighboring cities in Taiwan. We suggest that such cases should be tagged as DIFF. [\u6843\u7af9\u82d7<DIFF>] \u5730\u5340\u9023\u65e5\u8c6a\u96e8 [Tao-Chu-Miao<DIFF>] area it has been raining torrentially for a couple of days. # Tao-Taoyuan; Chu: Hsihchu; Miao: Miaoli World place names World place names can be divided into two sets: locations written in a foreign language and translated location names (written in Chinese). A translated name such as \"\u7d10\u7d04/New York\" can be considered as a location name in CNEC 1.0. But an original name, like Tokyo, should be tagged as DIFF. The following two sentences demonstrate the criteria we set. [\u5bc6\u8607\u91cc\u5dde(Missouri State)<LOC>] \u7684 [\u8056\u8def\u6613\u5e02(St. Louis City)<LOC>] [Missouri<DIFF>]\u5dde\u7684 [St. Louis City<DIFF>]. #[St. Louis City<LOC>] in [Missouri State<LOC>] Organization names -ORG In general, organizations include companies, government bodies, institutes, and other organized groups. We define an organization as having the ability to execute plans and projects. The tagging of ORG has to apply maximum semantic unit matching. A typical case is shown below. [\u88d5\u9686\u6c7d\u8eca\u4e09\u7fa9\u5ee0(Yulon Motor Sanyi plant)<ORG>] The maximum semantic unit is used because this entity cannot be separated into \"\u88d5\u9686\u6c7d\u8eca(Yulon Motor)\" and \"\u4e09\u7fa9\u5ee0(Sanyi plant)\". \"Sanyi plant\" cannot be tagged as a LOCATION according to the uniqueness characteristic of an NE. Like location names, some ambiguity may occur in the following cases. Organization abbreviations Organization abbreviation tagging follows the rules for location abbreviations described in Subsection 3.3.2. For example, [\u570b\u89aa\u65b0(Guo-Qin-Xin)<DIFF>] \u8b70\u54e1\u4e0b\u5348\u5230[\u5065\u4fdd\u5c40(BNHI)<ORG>] Guo-Qin-Xin councillors went to the Bureau of National Health Insurance this afternoon. # Guo: Kuomintang; Qin: People First Party; Xin: New Party Foreign organization names As in Section 3.3.3, an original organization name is regarded as DIFF in CNEC 1.0, but a translated name is tagged as a location name. [\u60e0\u666e<ORG>] \u8207 [\u5fae\u8edf<ORG>] \u7684\u5171\u540c\u79d8\u5bc6 The secret of [HP<ORG>] and [Microsoft<ORG>] Groups, bands, crowds, and teams Through the tagging process, we have found that most Chinese people have a problem tagging similar concepts like groups, bands, crowds and teams as organizations. Hence, we give a definition of an organization to help annotators determine if a term is an ORG. \"An organization has five fundamental parts: a founder, capital, structure (departments, section, class, etc.,), a hierarchy (chief, director, dean) and employees.\" According to this definition, we do not mark a term as an organization if it doesn't have an organized structure. LAO and OAL Location as Organization (LAO) and Organization as Location (OAL) are proposed for semantically meaningful NE in Chinese NEC. In some cases, \"location\" represents an organization-like role to make decisions or to perform some duties. Compare the following two examples from the China Times news corpus: \"[\u53f0\u5317\u5e02\u653f\u5e9c<ORG>]\u540c\u610f[\u7e3d\u7d71\u5e9c<OAL>]\u524d\u7684\u96c6\u6703\u904a\u884c\" [Taipei municipal government<ORG>] agreed to the protest in front of the [presidential plaza <LOC>]. [\u7e3d\u7d71\u5e9c<ORG>]\u767c\u8868\u4e00\u4e2d\u4e00\u53f0\u653f\u7b56 [Presidential Office <LAO>] announced \"one China, one Taiwan policy\". The above examples show that the term \"\u7e3d\u7d71\u5e9c(presidential plaza / presidential office)\" has a double meaning in Chinese: location and organization. The first term \"\u7e3d\u7d71\u5e9c\" is obviously a place name, but, the second \"\u7e3d\u7d71\u5e9c\" is an organization. This use of the same term to indicate a place name and an organization name, we call it \"borrowing\", is common in Chinese. Sometimes we cannot sure if a location entity is a real location name or just a borrowing. In order to avoid confusion, we separate the LAO tag and OAL tag in location and organization names. Differentiating between a borrowing and a true NE depends on an entity's category., By deciding which category an entity belongs to in common usage, we can tell whether it is a borrowing, or not. For example, a country's name can refer to its geographical position, but it may also be used as an organization name, as in: \"\u4e2d \u570b\u6628\u65e5\u8b66\u544a\u7f8e\u570b\u505c\u6b62\u5c0d\u8ecd\u552e/China warned the United States yesterday to stop selling advanced arms to Taiwan.\" Obviously, in this case we can tell the country names are borrowings and should be annotated as LAO as follows: OAL can be identified in the same way: \u9019\u8f1b\u5df4\u58eb\u6709\u5230 [\u884c\u653f\u9662<OAL>] This bus goes by [the Executive Yuan<OAL>] Manual Annotating Process The most famous corpus in Chinese NER is MET-2 made by MUC [Chinchor, 1998 ]. However, it only contains single domain data and is not large enough for building a machine-learning-based NER system [Tsai et. al 2004] . We, therefore, collected over a million sentences without any annotations from the online United Daily News (UDN) and China Times for the period December 2002 to December 2003 as raw data. The sentences extracted from raw data recorded in XML format shown in Figure 1 . Fig. 1. Original corpus We chose high school students as annotators and gave them basic training before they performed the annotations. The training process was: 1. All the students attended courses about the project, including an introduction to named entity recognition, segmentation, and parts-of-speech tagging. 2. Students took a qualifying test to select participants for the tagging task. 3. Participants had to acquaint themselves with the annotating criteria we suggest ed and the operation of the tagging tool program. (Figure 2 shows the interface of the tagging tool.) The participants were then divided into three groups. Each group was given the same sentence set containing 21,000 randomly extracted sentences; 13,208 from the UDN and 8,892 from the China Times. Table 2 shows the distribution of sentences. Participants were asked to finish the tagging task with the tagging tool program in two weeks. Figure 3 shows a tagged XML file in which the annotations are marked. Tagging results were then collected from each group and checked for consistency. Fig. 2. Tagging tool Conclusion and Future work In this paper we describe the construction of a tagged corpus for Chinese NER. We define the criteria of Chinese NE tagging, and design a standard tagging procedure for NE corpus annotation. We also demonstrate an annotator training procedure and the statistics of the corpus. The resulting corpus, CNEC 1.0, can be used to improve the performance of Mencius, our Chinese NER system. We do not use some ambiguous entities, labeled as DIFF, that involve issues such as abbreviations, cross-language loanwords and borrowings for training the NER model. As these entities need to be re-classified, advanced annotation will be executed in the next version of CNEC.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 6.704270752999619e-07,
        "foundation": 4.320199066265573e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, or foundations. Therefore, based on the information provided, it appears there was no disclosed funding for this research."
}