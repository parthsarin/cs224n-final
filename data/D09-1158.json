{
    "article": "Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier. It is often used in situations where labeled training data is scarce but unlabeled data is abundant. In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain. As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set. We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly. We call these instances bridges, as they are used to bridge the source domain to the target domain. We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task. Introduction Most recent researches on natural language processing (NLP) problems are based on machine learning algorithms. High performance can often be achieved if the system is trained and tested on data from the same domain. However, the performance of NLP systems often degrades badly when the test data is drawn from a source that is different from the labeled data used to train the system. For named entity recognition (NER), for example, Ciaramita and Altun (2005) reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set. The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation. In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain. Previous work in domain adaptation can be classified into two categories: [S+T+] , where a small, labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum\u00e9 III, 2007; Finkel and Manning, 2009) , or [S+T-], where no labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007) . In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain. In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time. Thus although [S+T+] is easier to handle, [S+T-] is of higher practical importance. In this paper, we propose a domain adaptive bootstrapping (DAB) approach to tackle the domain adaptation problem under the setting [S+T-]. Bootstrapping is an iterative process that uses a trained classifier to label and select unlabeled instances to add to the training set for retraining the classifier. It is often used when labeled training data is scarce but unlabeled data is abundant. In contrast, for domain adaptation problems, we may have a lot of training data but the target application domain has a different data distribution. Standard bootstrapping usually selects instances that are most confidently labeled from the unlabeled data. In domain adaptation situations, usually the most confidently labeled instances are the ones that are most similar to the source domain in-stances -these instances tend to contain very little information about the target domain. For domain adaptive bootstrapping, we propose a selection criterion that selects instances that are informative and easy to automatically label correctly. In addition, we propose a criterion for stopping the process of bootstrapping before it adds uninformative and incorrectly labeled instances that can reduce performance. Our approach leverages on instances in the target domain called bridges. These instances contain domain-independent features, as well as features specific to the target domain. As they contain domain-independent features, they can be classified correctly by classifiers trained on the source domain labeled data. We argue that these instances act as a bridge between the source and the target domain. We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007) , that has recently been proposed for domain adaptation. Related work One general class of approaches to domain adaptation is to consider that the instances from the source and the target domain are drawn from different distributions. Bickel et al. (Bickel et al., 2007) discriminatively learns a scaling factor for source domain training data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting. Daume III and Marcu (Daum\u00e9 III and Marcu, 2006) considers that the data distribution is a mixture distribution over general, source domain and target domain data. They learn the underlying mixture distribution using the conditional expectation maximization algorithm, under the [S+T+] setting. Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T-] settings. For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which was shown to outperform the standard bootstrapping algorithm. In this paper, we assume the [S+T-] settings, and we show that the approach proposed in this paper, domain adaptive bootstrapping (DAB), outperforms the balanced bootstrapping algorithm on NER. Another class of approaches to domain adaptation is feature-based. Daume III (Daum\u00e9 III, 2007) divided features into three classes: domainindependent features, source-domain features and target-domain features. He assumed the existence of training data in the target-domain (under the setting [S+T+]), so that the three classes of features can be jointly trained using source and target domain labeled data. This cannot be done in the setting [S+T-], where no training data is available in the target domain. Using a different approach, Blitzer et al. (2006) induces correspondences between feature spaces in different domains, by detecting pivot features. Pivot features are features that occur frequently and behave similarly in different domains. Pivot features are used to put domain-specific features in correspondence. In this paper, instead of pivot features, we attempt to leverage on pivot instances that we call bridges, which are instances that bridge the source and target domain. This will be illustrated in Section 3. It is generally recognized that adding informative and correctly labeled instances is more useful for learning. Active learning queries the user for labels of most informative or relevant instances. Active learning, which has been applied to the problem of NER in (Shen et al., 2004) , is used in situations where a large amount of unlabeled data exists and data labeling is expensive. It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007) . However, active learning requires human intervention. Here, we want to achieve the same goal without human intervention. Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting. A set of training data D S = {x i , y i } 1\u2264i\u2264|D S | is given in the source domain, where the notation |X| denotes the size of a set X. Each instance x i in D S has been manually annotated with a label, y i , from a given set of labels Y . The objective of domain adaptation is to label a set of unlabeled data, D T = {x i } 1\u2264i\u2264|D T | with labels from Y . A machine learning algorithm will take a labeled data set (for e.g. D S ) and outputs a classifier, which can then be used to classify unlabeled data, i.e. assign labels to unlabeled instances. A special class of machine learning algorithms, called transductive learning algorithms, is able to take the unlabeled data D T into account during the learning process (see e.g. (Joachims, 1999) ). However, such algorithms do not take into account the shift in domain of the test data. Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which we describe below. Standard and balanced bootstrapping We define a general bootstrapping algorithm in Algorithm 1. The algorithm can be applied to any machine learning algorithm that allows training instances to be weighted, and that gives confidence scores for the labels when used to classify test data. The bootstrapping procedure iteratively improves the performance of a classifier SC t over a number of iterations. In Algorithm 1, we have left a number of parameters unspecified. These parameters are (1) the selection-criterion for instances to be added to the training data, (2) the terminationcriterion for the bootstrapping process, and (3) the weights (w S , w T ) given to the labeled and bootstrapped training sets. Standard bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is based on selecting the top k most-confidently labeled instances in R t . The weight w S t is equal to w T t . The value of k is a parameter for the bootstrapping algorithm. Balanced bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is still based on selecting the top k most-confidently labeled instances in R t . Balanced bootstrapping was formulated for domain adaptation, and hence they set the weights to satisfy the ratio w S t w T t = |Tt| |D S | . This allows the small amount of target data added, T t , to have an equal weight to the large source domain training set D S . In this paper, we formulate a selection-criterion and a termination-criterion which are better than those used in standard and balanced bootstrapping. Regarding the selection-criterion, standard and balanced bootstrapping both select instances which are confidently labeled by SC t to be used for training SC t+1 , in the hope of avoiding using wrongly labeled data in bootstrapping. However, instances that are already confidently labeled by SC t may not contain sufficient information which is not in D S , and using them to train SC t+1 may result in SC t+1 performing similarly to SC t . This motivates us to select samples which are both informative and easy to automatically label correctly. Regarding the termination-criterion, which Algorithm 1 Bootstrapping algorithm Input: labeled data D S , test data D T and a machine learning algorithm. Output: the predicted labels of the set D T . Set T 0 = \u2205, R 0 = D T , and t = 0 Repeat 1. learn a classifier SC t with (D S , T t ) with weights (w S t , w T t ) 2. label the set R t with SC t 3. select S t \u2286 R t based on selection-criterion 4. T t+1 = T t \u222a S t , and R t+1 = R t \\ S t . Until termination-criterion Output the predicted labels of D T by SC t . is not mentioned in the paper (Jiang and Zhai, 2007) , we assume that bootstrapping is simply run for either a single iteration, or a small and fixed number of iterations. However, it is known that such simple criterion may result in stopping too early or too late, leading to sub-optimal performance. We propose a more effective terminationcriterion here. Domain adaptive bootstrapping (DAB) Our selection-criterion relies on the observation that in domain adaptation, instances (from the source or the target domain) can be divided into three types according to their information content: generalists are instances that contain only domainindependent information and are present in all domains; specialists are instances containing only domain-specific information and are present only in their respective domains; bridges are instances containing both domain-independent and domainspecific information, also present only in their respective domains but are useful as a \"bridge\" between the source and the target domains. The implication of the above observation is that when choosing unlabeled target domain data for bootstrapping, we should exploit the bridges, because the generalists are not likely to contain much information not in D S due to their domainindependence, and the specialists are difficult to be labeled correctly due to their domain-specificity. In contrast, the bridges are informative and easier to label correctly. Choosing confidently classified instances for bootstrapping, as in standard bootstrapping and balanced bootstrapping, is simple, but results in choosing mostly generalists, and is too conservative. We design a scoring function on instances, which has high value when the instance is informative and sufficiently likely to be correctly labeled in order to identify correctly labeled bridges. Intuitively, informativeness of an instance can be measured by the prediction results of the ideal classifier IS for the source domain and the ideal classifier IT for the target domain. If IS and IT are both probabilistic classifiers, IS should return a noninformative distribution while IT should return an informative one. The ideal classifier for the source domain is approximated with a source classifier SC trained on D S , while the ideal classifier for the target domain is approximated by training a classifier, T C, on target domain instances labeled by the source classifier. We also try to ensure that instances that are selected are correctly classified. As the label used is provided by the target classifier, we estimate the precision of the target classification. The final ranking function is constructed by combining this estimate with the informativeness of the instance. We show the algorithm for the instance selection in Algorithm 2. The notations used follow those used in Algorithm 1. For simplicity, we assume that w S t = w T t = 1 for all t. We expect T C to be a reasonable classifier on D T due to the presence of generalists and bridges. Note that the target classifier is constructed by randomly splitting D T into two partitions, training a classifier on each partition and using the prediction of the trained classifier on the partition it is not trained on. This is because classifiers tend to fit the data that they have been trained on too well making the probability estimates on their training data unreliable. Also, a random partition is used to ensure that the data in each partition is representative of D u . The scoring function : score(p (s) , p (t) ) The scoring function score(p (s) , p (t) ) in Algorithm 2 is simply implemented as the product of two components: a measure of the informativeness and the probability that SC's label is correct. We show how the intuitive ideas (described above) behind these two components are formalized. Informativeness of a distribution p on a set of discrete labels Y is measured by its entropy h(p) defined by h(p) = \u2212 y\u2208Y p(y) log p(y). i ). 6. Select top k instances from R t with the highest scores. h(p) is nonnegative; h(p) = 0 if and only if p has probability 1 on one of the labels; h(p) attains its maximum value when the distribution p is uniform over all labels. Hence, an instance is classified with high confidence when the distribution over its labels has low entropy. We measure the informativeness of an instance using h(p (s) ) \u2212 h(p (t) ), where p (s) and p (t) are as in Algorithm 2. We argue that a larger value of this expression implies that the instance is more likely to be a bridge instance. This expression has a high value when the source classifier is uncertain, and the target classifier is certain. Uncertain classification by the source classifier indicates that the instance is unlikely to be a generalist. Moreover, if the target classifier is certain on x i , it means that instances similar to the instance x i are consistently labeled with the same label by the source classifier SC t , indicating that it is likely to be a bridge instance. The probability that T C's label is correct cannot be estimated directly because we do not have labeled target domain data. Instead, we use the source domain to give an estimate. We do this with a simple pre-processing step: we split the data D S into two partitions of equal size, train a classifier on each partition, and test each classifier on the other partition. We then measure the resulting accuracy given each label: \u03c1(y) = # correctly labeled instances of label y # total instances of label y . Summarizing the above discussion, the scoring function is as shown below. score(p (s) , p (t) ) = \u03c1(y * ) h(p (s) )\u2212h(p (t) ) , where y * = arg max y\u2208Y p (s) (y) The scoring function has a high value when the information content of the example is high and the label has high precision. The termination criterion Intuitively, our algorithm terminates when there are not enough informative instances. Formally, we define the termination criterion as follows: we terminate the bootstrapping process when, there exists an instance x i in the top k instances satisfying the following condition: 1. h(p (s) i ) < h(p (t) i ), or 2. max y\u2208Y p (s) i (y) > max y\u2208Y p (t) i (y) The second case is used to check for instances where the classifier SC t is more confident than the target classifiers T C x t , on their respective predicted labels. This shows that the instance x i is more of a generalist than a bridge. NER task and implementation The algorithm described in Section 3 is not specific to any particular application. In this paper, we apply it to the problem of named entity recognition (NER). In this section, we describe the NER classifier and the features used in our experiments. NER features We used the features generated by the CRF package (Finkel et al., 2005) . These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word. We use the same set of features for all classifiers used in the bootstrapping process, and for all baselines used in the experimental section. Machine learning algorithms A base machine learning algorithm is required in bootstrapping approaches. We describe the two machine learning algorithms used in this paper. We chose these algorithms for their good performance on the NER task. Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is one of the most competitive methods for named entity recognition (Tjong and Meulder, 2003) . MaxEnt is a discriminative method that learns a distribution, p(y i |x i ), over the labels, y i , given the vector of features, x i . We used the implementation of MaxEnt classifier described in (Manning and Klein, 2003) . For NER, each instance represents a single word token within a sentence, with the feature vector x i derived from the sentence as described in the previous section. Max-Ent is not designed for sequence classification. To deal with sequences, each name-class (e.g. PER-SON) is divided into sub-classes: first token (e.g. PERSON-begin), unique token (e.g. PERSONunique), or subsequent tokens (e.g. PERSONcontinue) in the name-class. To ensure that the results returned by MaxEnt is coherent, we define deterministic transition probabilities that disallow transitions such as one from PERSON-begin to LOCATION-continue. A Viterbi parse is used to find the valid sequence of name-classes with the highest probability. Support vector machines (SVM): The basic idea behind SVM for binary classification problems is to consider the data points in their feature space, and to separate the two classes with a hyper-plane, by maximizing the shortest distance between the data points and the hyper-plane. If there exists no hyperplane that can split the two labels, the soft margin version of SVM will choose a hyperplane that splits the examples as cleanly as possible, while still maximizing the distance to the nearest cleanly split examples (Joachims, 2002) . We used the SVM light package for our experiments (Joachims, 2002) . For the multi-label NER classification with N classes, we learn N SVM classifiers, and use a softmax function to obtain the distribution. Formally, denoting by s(y) the confidence returned by the classifier for each label y \u2208 Y , the probability of the label y i is given by p(y i |x i ) = exp(s(y i )) y\u2208Y exp(s(y)) Similarly to MaxEnt, we subdivide name-classes into begin, continue, and unique sub-classes, and use a Viterbi parse for the sequence of highest probability. The SVM light package also implements a transductive version of the SVM algorithm. We also compare our approach with the transductive SVM (Joachims, 1999) in our experimental results. Experimental results In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program. The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he). In The evaluation measure used is the F-measure. F-measure is the harmonic mean of precision and recall, and is commonly used to evaluate NER systems. We use the scorer for CONLL 2003 shared task (Tjong and Meulder, 2003) where the F-measure is computed by averaging F-measures for name-classes, weighted by the number of oc- currences. Cross-source transfer The ACE 2005 data set consists of articles drawn from a variety of sources. We use the four categories shown in Table 1 . Each category is considered to be a domain, and we consider each pair of categories as the source and the target domain in turn. Figure 1 compares the performance of MaxEnt-SB, MaxEnt-BB and MaxEnt-DAB over multiple iterations. Figure 2 compares the performance of SVM, SVM-Trans and SVM-DAB. Each line in the figures represents the average F-measure across all the domains over many iterations. When the termination condition is met for one domain, its F-measure remains at the value of the final iteration. Despite a large number of iterations, both standard and balanced bootstrapping fail to improve performance. Supervised learning performance on each domain is shown in Table 3 (by 2-fold crossvalidation with random ordering) as a reference. In Table 5 , we compare the F-measures obtained by different algorithms at the last iteration they were run. We will discuss more on this in Section 5.3. Cross-topic transfer This data set is constructed from 175 articles from the ACE 2005 corpus. The data set is used to evaluate transfer across topics. We manually classify the articles into 4 categories: military operations (MO), political relationship or politicians (POL), terrorism-related (TER), and those which are not in the above categories (OTH). A detailed breakdown of the number of documents in the each topic is given in Table 2 . Supervised learning performance on each domain is shown in Table 4 (by 2-fold crossvalidation with random ordering) as a reference. Experimental results on cross-topic evaluation are shown in Table 6 . Figure 3 compares the performance of MaxEnt-SB, MaxEnt-BB and MaxEnt-   Table 4 : F-measure of supervised learning on the cross-topic target domains. Discussion We show in our experiments that DAB outperforms standard and balanced bootstrapping, as well as the transductive SVM. We have also shown DAB to be robust across two state-of-the-art classifiers, MaxEnt and SVM. Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. In contrast, we found that both methods perform poorly on domain adaptation for NER. In named entity classification, the names have already been segmented out and only need to be classified with the appropriate class. However, for NER, the names also need to be separated from not-a-name instances. We find that the addition of not-a-name instances changes the problem -the not-a-names form most of the instances classified with high confidence. As a result, we find that both standard and balanced bootstrapping fail to improve performance: the selection of the most confident instances no longer provide sufficient new information to improve performance. We also find that transductive SVM performs poorly on this task. This is because it assumes that the unlabeled data comes from the same distribution as the labeled data. In general, applying semi-supervised learning methods directly to [S+T-] type domain adaptation problems do not work and appropriate modifications need to be made to the methods. The ACE 2005 data set also contains a set of ariticles from the broadcast news (BN) source which is written entirely in lower case. This makes NER much more difficult. However, when BN is the source domain, the capitalization information can be discovered by DAB. Figures 5 and 6 show the average performance when BN is used as the source domain and all other domains in Table 1 as the target domains. The source domain classifier tends to have high precision and low recall, DAB results in an increase in recall, with a small decrease in precision. Testing the significance of the F-measure is not trivial because the named entities wrongly labeled by two classifiers are not directly comparable. We tested the labeling disagreements instead, using a McNemar paired test. The significance test is performed on the improvement of MaxEnt-DAB over MaxEnt and SVM-DAB over SVM. In most of the domains for the cross-source transfer, the improvements are significant at a significance level of 0.05, using MaxEnt classifier. The exceptional train-test pairs are NW-WL and WL-BC. In the case of WL-BC, this means the slight decrement in performance is not statistically significant. Similar result is achieved for the cross-source transfer using SVM classifier. In the cross-topic transfer, the source domain and the target domain are not very different. When we have a large amount of training data and little testing data, the gain of DAB can be not statistically significant, as in the case when we train with MO and POL domains. Conclusion We proposed a bootstrapping approach for domain adaptation, and we applied it to the named entity recognition task. Our approach leverages on instances that serve as bridges between the source and target domain. Empirically, our method outperforms baseline approaches including supervised, transductive and standard bootstrapping approaches. It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007) .",
    "abstract": "Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier. It is often used in situations where labeled training data is scarce but unlabeled data is abundant. In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain. As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set. We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly. We call these instances bridges, as they are used to bridge the source domain to the target domain. We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task.",
    "countries": [
        "Singapore"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "62",
    "year": "2009",
    "month": "August",
    "title": "Domain adaptive bootstrapping for named entity recognition"
}