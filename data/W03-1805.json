{
    "article": "We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be unified into a single score to rank extracted phrases. Introduction In many real world deployments of text mining technologies, analysts are required to deal with large collections of documents from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from text analysis tools. However, browsing data is not an efficient way to get an understanding of the topics and events which are particular to a domain. For example, an analyst concerned with the area of hybrid cars may harvest messages from online forums. They may then want to rapidly construct a hierarchy of topics based on the content of these messages. In addition, in cases where these messages are harvested via a search of some sort, there is a requirement to obtain a rich and effective set of search terms. The technology described in this paper is an example of a phrase finder capable of delivering a set of indicative phrases given a particular set of documents from a target domain. In the hybrid car example, the result of this process is a set of phrases like that shown in Figure 1 .  In order to capture domain-specific terms efficiently in limited time, the extraction result should be ranked with more indicative and good phrase first, as shown in this example. Phraseness and informativeness The word keyphrase implies two features: phraseness and informativeness. Phraseness is a somewhat abstract notion which describes the degree to which a given word sequence is considered to be a phrase. In general, phraseness is defined by the user, who has his own criteria for the target application. For instance, one user might want only noun phrases while another user might be interested only in phrases describing a certain set of products. Although there is no single definition of the term phrase, in this paper, we focus on collocation or cohesion of consecutive words. Informativeness refers to how well a phrase cap-tures or illustrates the key ideas in a set of documents. Because informativeness is defined with respect to background information and new knowledge, users will have different perceptions of informativeness. In our calculations, we make use of the relationship between foreground and background corpora to formalize the notion of informativeness. The target document set from which representative keyphrases are extracted is called the foreground corpus. The document set to which this target set is compared is called the background corpus. For example, a foreground corpus of the current week's news would be compared to a background corpus of an entire news article archive to determine that certain phrases, like \"press conference\" are typical of news stories in general and do not capture the particulars of current events in the way that \"national museum of antiquities\" does. Other examples of foreground and background corpora include: a web site for a certain company and web data in general; a newsgroup and the whole Usenet archive; and research papers of a certain conference and research papers in general. In order to get a ranked keyphrase list, we need to combine both phraseness and informativeness into a single score. A sequence of words can be a good phrase but not an informative one, like the expression \"in spite of.\" A word sequence can be informative for a particular domain but not a phrase; \"toyota, honda, ford\" is an example of a non-phrase sequence of informative words in a hybrid car domain. The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness. Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994) , the t-test (Church et al., 1991) , the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990) , and binomial loglikelihood ratio test (BLRT) (Dunning, 1993) . According to (Manning and Sch\u00fctze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair \"the the,\" and uses a hybrid of MI and BLRT. Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero). Keyphrase extraction Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al., 1999; Turney, 2000) . The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high. 1  We would like to have the ability to extract keyphrases from a totally new domain of text without building a training corpus. Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases. They took the intersection of each top 10% of phrases identified by MI and RIDF, but did not extend the approach to combining the two metrics into a unified score. Combining keyphrase and collocation Baseline method based on binomial log-likelihood ratio test We can use various statistics as a measure for phraseness and informativeness. For our baseline, we have selected the method based on binomial loglikelihood ratio test (BLRT) described in (Dunning, 1993) . The basic idea of using BLRT for text analysis is to consider a word sequence as a repeated sequence of binary trials comparing each word in a corpus to a target word, and use the likelihood ratio of two hypotheses that (i) two events, observed \u00a2\u00a1 times out of \u00a3 \u00a4\u00a1 total tokens and \u00a6\u00a5 times out of \u00a3 \u00a7\u00a5 total tokens respectively, are drawn from different distributions and (ii) from the same distribution. The BLRT score is calculated with \u00a2\u00a1 \u00a4\u00a3 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 \u00a1 \u00a1 \u00a3 \u00a1 \u00a7 \u00a9 \u00a5 \u00a5 \u00a3 \u00a5 \u00a7 \u00a9 \u00a1 \u00a3 \u00a4\u00a1 \u00a7 \u00a9 \u00a5 \u00a3 \u00a5 (1) where \"! \u00a6 $# \u00a3 % , &! '\u00a9 \u00a1 )( \u00a5 # 0\u00a9 \u00a3 \u00a4\u00a1 )( \u00a3 \u00a5 , and \u00a7 \u00a9 \u00a3 ! 1 32 4\u00a9 65 87 & @9 BA 2 (2) In the case of calculating the phraseness score of an adjacent word pair (C ED ), the null hypothesis is that C and D are independent, which can be expressed as \u00a2\u00a9 D %F C ! G )\u00a9 D \"F IH C . We can use Equation (1) to calculate phraseness by setting: \u00a1 P! RQ S\u00a9 TC ED U \u00a3 \u00a4\u00a1 V! WQ S\u00a9 TC \u00a5 X! RQ S\u00a9 H C ED Y ! WQ `\u00a9 D Y 7 aQ S\u00a9 TC ED U \u00a3 \u00a5 ! WQ S\u00a9 H C ! Wb dc eQ S\u00a9 Tf 7 1Q S\u00a9 TC (3) where Q `\u00a9 TC is the frequency of the word C and Q S\u00a9 TC ED Y is the frequency of D following C . For calculating informativeness of a word f , \u00a1 P! WQ hg pi B\u00a9 Tf \u00a3 \u00a1 ! Wb dc eQ hg pi B\u00a9 Tf \u00a5 ! WQ Pq ri B\u00a9 Tf \u00a3 \u00a5 8! Wb dc eQ Pq ri B\u00a9 Tf Combining a phraseness score s %t and an informativeness score s ) into a single score value is not a trivial task since the the BLRT scores vary a lot between phraseness and informativeness and also depending on data (c.f. Figure 6 (a) ). One way to combine those scores is to use an exponential model. We experimented with the following logistic function: s u! 5 5 V( av xw 0y \u00a9 67 8 Ys t 7 ps ( (5) whose parameters , , and are estimated on a heldout data set, given feedback from users (i.e. supervised). Figure 2 shows some example phrases extracted with this method from the data set described in Section 6.1, where the parameters, , , , are manually optimized on the test data. Although it is possible to rank keyphrases using this approach, there are a couple of drawbacks. Necessity of tuning parameters the existence of parameters in the combining function requires human labeling, which is sometimes an expensive task to do, and the robustness of learned weight across domains is unknown. We would like to have a parameter-free and robust way of combining scores. Inappropriate symmetry BLRT tests to see if two random variables are independent or not. This sometimes leads to unwanted phrases getting a high score. For example, when the background corpus happens to have many occurrences of phrase al jazeera which is an unusual phrase in the foreground corpus, then the phrase still gets high score of informativeness because the distribution is so different. What we would like to have instead is asymmetric scoring function to test the loss of the action of not taking the target phrase as a keyphrase. In the next section, we propose a new method trying to address these issues. Proposed method Language models and expected loss A language model assigns a probability value to every sequence of words ! df \u00a1 f \u00a5 \u00a2 p p f 9 . The probability `\u00a9 T can be decomposed as `\u00a9 T ! 9 \u00a1 `\u00a9 Tf F f \u00a1 Ef \u00a5 \u00a2 p p f A \u00a1 Assuming f 8 only depends on the previous words, N-gram language models are commonly used. The following is the trigram language model case. `\u00a9 T ! 9 \u00a1 \u00a9 Tf F f A \u00a5 f A \u00a1 Here each word only depends on the previous two words. Please refer to (Jelinek, 1990) and (Chen and Goodman, 1996) for more about N-gram models and associated smoothing methods. Now suppose we have a foreground corpus and a background corpus and have created a language model for each corpus. The simplest language model is a unigram model, which assumes each word of a given word sequence is drawn independently. We denote the unigram model for the foreground corpus as \u00a7 \u00a1 \u00a1 fg and for the background corpus as \u00a7 \u00a1 \u00a1 bg . We can also train higher order models \u00a7 \u00a1 \u00a3\u00a2 fg and \u00a7 \u00a1 \u00a3\u00a2 bg for each corpus, each of which is a -gram model, where a\u00a9 \u00a5\u00a4 5 is the order. Among those four models, \u00a7 \u00a1 \u00a2 fg will be the best model to describe the foreground corpus in the sense that it has the smallest cross-entropy or perplexity value over the corpus. \u00a6 \u00a7 phraseness \u00a7 \u00a8\u00a9 7 07 informativeness 7 07 \u00a7 \u00a1 \u00a2 fg \u00a7 \u00a1 \u00a3\u00a2 bg \u00a7 \u00a1 \u00a1 fg \u00a7 \u00a1 \u00a1 bg If we use one of the other three models instead, then we have some inefficiency or loss to describe the corpus. We expect the amount of loss between using \u00a7 \u00a1 \u00a3\u00a2 fg and \u00a7 \u00a1 \u00a1 fg is related to phraseness and the loss between \u00a7 \u00a1 \u00a2 fg and \u00a7 \u00a1 \u00a3\u00a2 bg is related to informativeness. Figure 3 illustrates these relationships. Pointwise KL-divergence between models One natural metric to measure the loss between two language models is the Kullback-Leibler (KL) divergence. The KL divergence (also called relative entropy) between two probability mass function )\u00a9 TC and Y\u00a9 TC is defined as \u00a9 ! \"! )\u00a9 TC \u00a1 \u00a3 \u00a6\u00a5 )\u00a9 TC Y\u00a9 TC KL divergence is \"a measure of the inefficiency of assuming that the distribution is when the true distribution is .\" (Cover and Thomas, 1991) You can see this by the following relationship: \u00a9 ! \"! )\u00a9 TC \u00a1 \u00a4\u00a3 \u00a6\u00a5 )\u00a9 TC 7 #! )\u00a9 TC \u00a1 \u00a4\u00a3 \u00a6\u00a5 Y\u00a9 TC ! \"! )\u00a9 TC 5 \u00a1 \u00a4\u00a3 \u00a6\u00a5 Y\u00a9 TC 7 $ \u00a9 &% The first term b ! )\u00a9 TC \u00a1 ' )( 10 32 54 ! 76 is the cross entropy and the second term $ \u00a9 &% is the entropy of the random variable % , which is how much we could compress symbols if we know the true distribution . We define pointwise KL divergence 8 @9 \u00a9 A B to be the term inside of the summation of Equation ( 6 ): 8 9 \u00a9 DC FE HG! ' \u00a2\u00a9 T \u00a1 \u00a4\u00a3 \u00a6\u00a5 )\u00a9 T Y\u00a9 T (7) Intuitively, this is the contribution of the phrase to the expected loss of the entire distribution. We can now quantify phraseness and informativeness as follows: Phraseness of is how much we lose information by assuming independence of each word by applying the unigram model, instead of thegram model. 8 9 \u00a9 r \u00a7 \u00a1 \u00a2 fg V \u00a7 \u00a1 \u00a1 fg (8) Informativeness of is how much we lose information by assuming the phrase is drawn from the background model instead of the foreground model. I9 \u00a9 r \u00a7 \u00a1 \u00a2 fg V \u00a7 \u00a1 \u00a2 bg or (9) 8 I9 \u00a9 r \u00a7 \u00a1 \u00a1 fg V \u00a7 \u00a1 \u00a1 bg (10) Combined The following is considered to be a mixture of phraseness and informativeness. I9 \u00a9 r \u00a7 \u00a1 \u00a2 fg V \u00a7 \u00a1 \u00a1 bg (11) Note that the KL divergence is always nonnegative 2 , but the pointwise KL divergence can be a negative value. An example is the phraseness of the bigram \"the the\". )\u00a9 the the \u00a1 \u00a4\u00a3 \u00a6\u00a5 )\u00a9 the the )\u00a9 the )\u00a9 the \u00a1 \u00a2 since \u00a2\u00a9 the the \u00a4\u00a3 )\u00a9 the )\u00a9 the . Also note that in the case of phraseness of a bigram, the equation looks similar to pointwise mutual information (Church and Hanks, 1990 ) , but they are different. Their relationship is as follows. I9 \u00a9 )\u00a9 TC ED U \" \u00a2\u00a9 TC )\u00a9 D U E ! 1 )\u00a9 TC ED Y \u00a1 \u00a4\u00a3 \u00a6\u00a5 )\u00a9 TC ED Y \u00a2\u00a9 TC )\u00a9 D U \u00a5 \u00a6 \u00a8 \u00a7 \u00a9 pointwise MI The pointwise KL divergence does not assign a high score to a rare phrase, whose contribution of loss is small by definition, unlike pointwise mutual information, which is known to have problems (as described in (Manning and Sch\u00fctze, 1999), e.g.). Combining phraseness and informativeness One way of getting a unified score of phraseness and informativeness is using equation ( 11 ). We can also calculate phraseness and informativeness separately and then combine them. We combine the phraseness score s %t and informativeness score s by simply adding them into a single score s . s u! Ws t ( Gs Intuitively, this can be thought of as the total loss. We will show some empirical results to justify this scoring in the next section. Experimental results In this section, we show some preliminary experimental results of applying our method on real data. Data set We used the 20 newsgroups data set 3 , which contains 20,000 messages (7.4 million words) between February and June 1993 taken from 20 Usenet newsgroups, as the background data set, and another 20,000 messages (4 million words) between June and September 2002 taken from rec.arts.movies.current-films newsgroup as the foreground data set. Each message's subject header and the body of the message (including quoted text) is tokenized into lowercase tokens on both data set. No stemming is applied. Finding key-bigrams The first experiment we show is to find key-bigrams, which is the simplest case requiring combination of phraseness and informativeness scores. Figure 4 outlines the extraction procedure. Inputs: foreground and background corpus. 1. create background language model from the background corpus. 2. count all adjacent word pairs in the foreground corpus, skipping pre-annotated boundaries (such as HTML tag boundaries) and stopwords. 3. for each pair of words (x,y) in the count, calculate phraseness from ! fg and \" fg # fg and informativeness from $ ! fg and $ ! bg . Add the two score values as the unified score. 4. sort the results by the unified score. Output: a list of key-bigrams ranked by unified score. Figure 5 shows the extracted key-bigrams using this method. Comparing to Figure 2 , you can see that those two methods extract almost identical ranked phrases. Note that we needed to tune three parameters to combine phraseness and informativeness in BLRT, but no parameter tuning was required in this method. The reason why \"message news\" becomes the top phrase in both methods is that it appears frequently enough in message citation headers such  as John Smith js@foo.com\u00a1 wrote in message news:1pk0a@foo.com, which was not common in the 20 newsgroup dataset. 5 A more sophisticated document analysis tool to remove citation headers is required to improve the quality further. Figure 6 shows the distribution of phraseness and informativeness scores of bigrams extracted using the BLRT and pointwise KL methods. One can see that there is little correlation between phraseness and informativeness in both ranking methods. Also note that the range of x and y axis is very different in BLRT, but in the pointwise KL method they are comparable ranges. That makes combining two scores easy in the pointwise KL approach. Ranking n-length phrases The next example is ranking \u00a3 -length phrases. We applied a phrase extension algorithm based on the APriori algorithm (Agrawal and Srikant, 1994) to the output of the key-bigram finder in the previous example to generate \u00a3 -length candidates whose frequency is greater than 5, then applied a linguistic filter which rejects phrases that do not occur in valid noun-phrase contexts (e.g. following articles or possessives) at least once in the corpus. We ranked resulting phrases using pointwise KL score, using the same smoothing method as in the bigram case. Figure 7 shows the result of re-ranking keyphrases extracted from the same movie corpus. We can see that bigrams and trigrams are interleaved in natural order (although not many long phrases are extracted from the dataset, since longer NP did not occur more than five times). Figure 1 was another example of the result of the same pipeline of methods. Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores, fg \u00a9 T # bg \u00a9 T . This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms (Damerau, 1993) . Figure 8 compares extracted keywords ranked with pointwise KL and likelihood ratio scores, both of which use the same foreground and background unigram language model. We used messages retrieved from the query Infiniti G35 as the foreground corpus and the same 20 newsgroup data as the background corpus. Katz smoothing is applied to both language models. As we can see, those two methods return very different ranked lists. We think the pointwise KL returns a set of keywords closer to human judgment. One example is the word \"infiniti\", which we expected to be one of the informative words since it is the query word. The pointwise KL score picked the word as the third informative word, but the likelihood score missed it. Whereas \"6mt\", picked up by the likelihood ratio, which occurs 37 times in the  Since the likelihood of \"6mt\" with respect to the background LM is so small, the likelihood ratio of the word becomes very large. But the pointwise KL score discounts the score appropriately by consider-ing that the frequency of the word is low. Likelihood ratio (or relative frequency ratio) has a tendency to pick up rare words as informative. Pointwise KL seems more robust in sparse data situations. One disadvantage of the pointwise KL statistic might be that it also picks up stopwords or punctuation, when there is a significant difference in style of writing, etc., since these words have significantly high frequency. But stopwords are easy to define or can be generated automatically from corpora, and we don't consider this to be a significant drawback. We also expect a better background model and better smoothing mechanism could reduce the necessity of the stopword list. Discussion Necessity of both phraseness and informativeness Although phraseness itself is domain-dependent to some extent (Smadja, 1994) , we have shown that there is little correlation between informativeness and phraseness scores. Combining method One way to calculate a combined score is directly comparing \u00a7 \u00a1 \u00a2 fg and \u00a7 \u00a1 \u00a1 bg in Figure 3 . We have tried both approaches and got a better result from combining separate phraseness and informativeness scores. We think this is due to data sparseness of the higher order ngram in the background corpus. Further investigation is required to make a conclusion. We have used the simplest method of combining two scores by adding them. We have also tried har-monic mean and geometric mean but they did not improve the result. We could also apply linear interpolation to put more weight on one score value, or use an exponential model to combine score, but this will require tuning parameters. Benefits of using a language model One benefit of using a language model approach is that one can take advantage of various smoothing techniques. For example, by interpolating with a character-based n-gram model, we can make the LM more robust with respect to spelling errors and variations. Consider the following variations, which we need to treat as a single entity: al-Qaida, al Qaida, al Qaeda, al Queda, al-Qaeda, al-Qa'ida, al Qa'ida (found in online sources) . Since these are such unique spellings in English, character n-gram is expected to be able to give enough likelihood score to different spellings as well. It is also easy to incorporate other models such as topic or discourse model, use a cache LM to capture local context, and a class-based LM for the shared concept. It is also possible to add a phrase length prior probability in the model for better likelihood estimation. Another useful smoothing technique is linear interpolation of the foreground and background language models, when the foreground and background corpus are disjoint. Conclusion We have explained that phraseness and informativeness should be unified into a single score to return useful ranked keyphrases for analysts. Our proposed approach calculates both scores based on language models and unified into a single score. The phrases generated by this method are intuitively very useful, but the results are difficult to evaluate quantitatively. In future work we would like to further explore evaluation of keyphrases, as well as investigate different smoothing techniques. Further extensions include developing a phrase boundary segmentation algorithm based on this framework and exploring applicability to other languages.",
    "abstract": "We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be unified into a single score to rank extracted phrases.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "312",
    "year": "2003",
    "month": "July",
    "title": "A Language Model Approach to Keyphrase Extraction"
}