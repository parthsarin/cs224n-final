{
    "article": "Frame-semantic annotations exist for a tiny fraction of the world's languages, Wikidata, however, links knowledge base triples to texts in many languages, providing a common, distant supervision signal for semantic parsers. We present WIKIBANK, a multilingual resource of partial semantic structures that can be used to extend pre-existing resources rather than creating new man-made resources from scratch. We also integrate this form of supervision into an off-the-shelf frame-semantic parser and allow cross-lingual transfer. Using Google's SLING architecture, we show significant improvements on the English and Spanish CoNLL 2009 datasets, whether training on the full available datasets or small subsamples thereof. Introduction Shallow semantic parsing comes in many varieties, including frame-semantic parsing (T\u00e4ckstr\u00f6m et al., 2015; Ringgaard et al., 2017) , semantic role labeling (SRL) (Surdeanu et al., 2008; Pradhan and Xue, 2009; Weischedel et al., 2013; Zhang et al., 2019) , and semantic dependency parsing (Oepen et al., 2014) . In this paper, we present WIK-IBANK, a multilingual resource with partial semantic dependency structures projected from an existing knowledge base. WIKIBANK is created automatically, and used to augment pre-existing resources, or reduce the annotation effort for low-resource languages. We show how it can be used to improve an off-the-shelf frame-semantic parser, but this resource could also be equally useful for semantic role labeling systems and other semantic parsing frameworks. The frames in frame-semantic parsing present extended predicate-argument structures that determine \"who did what to whom\", \"when\", \"where\" and \"why\". For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the \"giving\" frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013) , machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018) , discourse parsing (Mihaylov and Frank, 2016) , question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014) . Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013) . However, such annotations only exist for a small subset of the world's languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervision from lemmas, part-of-speech (POS) tags or syntactic parse trees.  However, they still require parallel corpora. This paper, in contrast, presents WIKIBANK, a multilingual resource of sentences partially annotated with semantic structures extracted from Wikidata, a knowledge base that contains decontextualized predicate-argument relations for hundreds of languages. We directly map relations from this knowledge base onto Wikipedia sentences to obtain partial semantic structures (see example in Table 1 ). We use WIK-IBANK with pre-existing resources to train an off-the-shelf frame-semantic parser for (simulated) low-resource languages. Contributions The main contribution of our work is WIKIBANK, a novel multilingual resource for semantic parsing obtained by aligning Wikidata knowledge base triples with Wikipedia sentences. This resource can be both used to augment current semantic parsing datasets, and also reduce the required annotations for new languages. Furthermore, we present experiments demonstrating how to use WIKIBANK to improve frame-semantic parsers for different languages. Using techniques from multi-task learning, we show how to train an off-the-shelf frame-semantic parser (Ringgaard et WikiBank WIKIBANK is our new partially annotated resource for the multilingual frame-semantic parsing task. It is based on a heuristics-driven extraction of mark-up from knowledge bases that has important similarities to linguistic structures, and which can therefore serve as auxiliary task data, enabling the leverage of potential synergies. WIKIBANK is composed of partially annotated sentences. Examples in WIKIBANK consists of semantically labelled sentences, where each sentence has partial-semantic annotation for the predicate and its semantic arguments (see example in Table 2 ). WIKIBANK is derived directly from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) , and Wikipedia. Wikidata is a collaborative knowledge base, containing triples (entity id, property id, value id) that define a type of relation holding between an entity and a value (which can also be an entity). Wikidata also contains labels, and aliases for the properties, entities, and values. Following (Hewlett et al., 2016) , for each Wikidata item, we replace the IDs in each statement with the text label for properties and values that are entities, and with a human readable version for numeric values (e.g., time-stamp is converted into readable date), obtaining triples (entity, property, value). The extraction of sentences from Wikipedia is performed using distant supervision, as in (Levy et al., 2017) . For each triple, we take the corresponding Wikipedia article for the entity then we extract the first sentence containing the entity, the property, and the value. To increase the amount of examples, we also use Wikidata aliases when matching sentences with triples. The matching is done using regular expressions; a sentence is extracted if the entity, property, and value or their aliases are contained in it. Moreover, we filter out the sentences where the alias of the property found in the sentence does  not contain a verb. The last step is to automatically annotate the extracted sentences with frame-semantic labels. We assign the label ARG0 to the entity, V for the target in the property, and ARG1 to the value. If a sentence contains multiple verbs, we consider each verb annotation as a different example. Table 3 shows the number of sentences and examples extracted for each language. We used a pre-existing semantic role labeling model, Deep-SRL (He et al., 2017) , pretrained on CoNLL 2005, to see how well examples in WIKIBANK are annotated. For the evaluation we used a sample of 10K English sentences, and obtained the results shown in Table 4 . We consider an argument label to be correct if the entity label from Wikidata is contained in the sequence extracted by DeepSRL. From Table 4 , we note that the class ARG0 has a lower accuracy compared to ARG1. After further data analysis, we realized that this is due to the ARG0 containing names of locations, dates, and in general entities that are labeled as ARG1 by the DeepSRL model. Baseline SLING (Ringgaard et al., 2017) conceptualizes each frame as a list of slots. Each slot has a semantic role and a value. Each frame is categorized by one verb, for which the most likely meaning will be annotated. SLING is a transitionbased parsing framework, not tied to any particular linguistic theory or knowledge ontology. Briefly put, SLING encodes the input text tokens using bidirectional Long Short Term Memories (LSTM). The encoding is then fed into a Transition Based Recurrent Unit (TBRU) in order to produce a sequence of transitions. The TBRU architecture is a single feed-forward unit, which takes the activations from the bidirectional-LSTM and combines them with the activations from the hidden layer from the previous step. The model then combines the transition system and the activation layer to create the input feature vector for the next step. The TBRU has multiple inputs such as the activations from both the left-to-right and right-to-left LSTMs, hidden layer activations of the transition steps, in order to have a continuous representation of the semantic context. SLING also in- corporates an attention mechanism based on neuro-science models of attention and awareness in (Nelson et al., 2017) and (Graziano, 2013) . Specifically, the attention mechanism focuses on encoding the frame representation that the parser has created rather than encoding the tokens themselves. 0 K 5 0 K 1 0 0 K 1 5 0 K 2 0 0 K 2 5 0 K 3 0 0 K 3 5 0 K 4 0 0 K 4 5 0 K 5 0 0 K N E x a mp l e s 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 R o l e F 1 (a) Simplified label set 0 K 5 0 K 1 0 0 K 1 5 0 K 2 0 0 K 2 5 0 K 3 0 0 K 3 5 0 K 4 0 0 K 4 5 0 K 5 0 0 K N E x Data and preprocessing Our experiments are designed to provide a proof of concept for the usefulness of integrating WIKIBANK into training frame-semantic parsers. To that end, we train SLING with the full CoNLL 2009 German, English, and Spanish corpora (Haji\u010d et al., 2009) . The monolingual models trained only on the CoNLL corpora provide our baselines. We use the standard data splits: training, validation, and heldout test data; but we also purposely experiment with subsampling the training data to simulate low-resource scenarios. Using the CoNLL corpora rather than OntoNotes, as (Ringgaard et al., 2017) do, enables us to explore multilingual sharing and using WIKIBANK as a bridge for crosslingual transfer. We facilitate cross-lingual transfer by using multilingual word embeddings (Lample et al., 2018) and, following (Johnson et al., 2017) , by prepending all sentences with a language ID -as well as a task ID (CoNLL or WIKIBANK) for integrating the WIKIBANK sentences. Furthermore, we experiment with reducing the full label sets in the corpora to a common, simplified label set, to facilitate cross-lingual transfer. The simplified labels disregard thematic role information. For both setups, we normalize labels to be conform with the PropBank (Palmer et al., 2005) notation (e.g., A1 becomes ARG1). However, as shown in Figure 1 , the experiments with the full label set have a slightly better accuracy than the ones with a simplified label set, so we will present only the results for the former. Protocol We experiment with three languages, and subsamples of the training data of size 0 (zero-shot), 100, 500, 1000, and 2000, as well as the full training set, and also with and without cross-lingual transfer. The cross-lingual training setup consists of various combination of source lan-guages, with both CoNLL and WIKIBANK, and the target language WIKIBANK. The transfer is achieved using multilingual embeddings, the language as well as the previouslymentioned task IDs. Training In our experiments we explore multiple strategies of training using different languages as source language. This cross-lingual setup is achieved using multilingual word representation that embeds words from all languages into a single semantic space so that words with similar meanings are close to each other despite of language. There are also specific tasks for CoNLL and WIKIBANK. We use the same hyperparameters described by (Ringgaard et al., 2017) . For the embeddings we use MUSE multilingual embeddings (Lample et al., 2018) . The number of steps is set to 100K. Evaluation We evaluate the quality of the model using Slot F1 and Role F1, following (Ringgaard et al., 2017) . SLING compares the produced frames with the gold standard frames from the evaluation corpus. The documents are matched using a graph where the document is the root. The document is connected to the spans, which are connected to the frames that they evoke. The graph is expanded using the span-to-span links defined by the roles. The graph of the produced output, and the gold standard one are aligned to produce the quality measures. The measures evaluate the performance for spans, frames, frame types, and roles that link to other frames (\"roles\"). They also define aggregated measures, the one used to define the best model is called slot, and is a combination of type and role. Note these metrics are not comparable to the F1-scores reported in the CoNLL 2009 shared task. Slot F1 is similar in the spirit to the CoNLL F1-score, but because of how frames are represented internally in SLING, there is no straight-forward mapping into the CoNLL format. We start by conducting experiments on all possible targetlanguage/target-size combinations on the development dataset, which we consider as a tuning step; results are reported in Table 5 . For instance, we note that for German as a target language, when the target CoNLL size is 100, the Spanish dataset helps significantly improve the results on both development and test sets. Moreover, we note that all the models without source language perform much worse than when adding WIKIBANK independently of the language. Results As described above, we experiment with different sizes, combination of languages and corpora. Results for the CoNLL development set are presented in Table 5 .We can notice how all three languages have an improvement in performance, except in 3 cases, when adding the extra data in the same language from WIKIBANK. Additionally, when adding another languages as source, with data from both CoNLL and WIKIBANK, the models achieves even better results. However, when using 2 languages as source, Background Frame semantics (Fillmore, 1982; Baker et al., 1998; Swayamdipta et al., 2017) is the study of how linguistic forms affect frame knowledge, and how these frames thus could be integrated into an understanding of sentences and documents (Baker et al., 1998) . Frame-semantic parsers typically rely on supervised learning to train complex mod-els on top of a syntactic parser, induced from manually annotated resources, such as FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2005) . One of the earliest works on frame-semantic parsing is by (Gildea and Jurafsky, 2000) , who proposed a discriminative model for semantic role labeling using frame-semantics. (Thompson et al., 2003) proposed a generative model trained on FrameNet for shallow semantic parsing and frame identifications tasks. (Shi and Mihalcea, 2004) proposed to identify frames and their elements using a rule-based approach. (Johansson and Nugues, 2007) proposed a framesemantic structure extraction model based on syntactic dependency parsing; they also propose a method to strengthen FrameNet by automatically adding new units to its lexical database. More recent work on frame-semantic parsing includes SE-MAFOR (Das et al., 2014; Kshirsagar et al., 2015) , a frame-semantic parser for identifying and labeling the semantic arguments of a given predicate that evokes a specific FrameNet frame. (Ringgaard et al., 2017) use a modified version of OntoNotes (Pradhan and Xue, 2009; Weischedel et al., 2013) in their experiments, and we use a similar modification of the CoNLL 2009 corpora. That said, our parsing experiments are merely meant to illustrate the usefulness of the WIKIBANK resource. To the best of our knowledge, there are two similar works regarding the creation of automatic labeled resources for semantic role labeling. In the first one (Exner et al., 2015) , the authors create a dataset using loosely parallel sentences from Wikipedia and transfer the predicate-argument structure from the source language (English) to the target language. The alignment is achieved using the Wikidata IDs, extracted using a dictionary and a named entity linker. In the second one (Hartmann et al., 2016) , the approach is to use the distant supervision paradigm to transfer the labels from a Linked Lexical Resource, a combination of several resource like WordNet, FrameNet and Wikitionary, to a large unlabeled corpus (e.g. web pages). Both of these solution require additional resource which are not always available for low-resource languages. (Exner et al., 2015) dictionary creation requires a POS tagger, languagedependent rules, and entity databases; while (Hartmann et al., 2016) use two Linked Lexical Resource, Uby and Sem-Link, which support very few languages. Conclusion We introduced WIKIBANK, a new multilingual resource for semantic parsing. WIKIBANK consists of partial semantic structures directly projected from Wikidata onto Wikipedia sentences. We presented a set of experiments meant to illustrate the usefulness of this resource. Specifically, we showed that when training Google's SLING frame-semantic parser on the CoNLL 2009 corpora, we can obtain significant improvements using WIKIBANK as auxiliary data. The multi-task learning method used in our experiments also facilitates cross-lingual transfer, and our experiments indicate that WIKIBANK acts as a bridge between languages, enabling joint training on multiple annotated corpora in different languages.",
    "abstract": "Frame-semantic annotations exist for a tiny fraction of the world's languages, Wikidata, however, links knowledge base triples to texts in many languages, providing a common, distant supervision signal for semantic parsers. We present WIKIBANK, a multilingual resource of partial semantic structures that can be used to extend pre-existing resources rather than creating new man-made resources from scratch. We also integrate this form of supervision into an off-the-shelf frame-semantic parser and allow cross-lingual transfer. Using Google's SLING architecture, we show significant improvements on the English and Spanish CoNLL 2009 datasets, whether training on the full available datasets or small subsamples thereof.",
    "countries": [
        "Denmark",
        "Germany"
    ],
    "languages": [
        "English",
        "Spanish",
        "German"
    ],
    "numcitedby": "3",
    "year": "2020",
    "month": "May",
    "title": "{W}iki{B}ank: Using {W}ikidata to Improve Multilingual Frame-Semantic Parsing"
}