{
    "article": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixedvocabulary. In this work, we present a largescale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions. Introduction Data visualizations such as bar charts and line charts have become popular in analyzing data and making informed decisions. To analyze data, often people ask complex reasoning questions about charts involving arithmetic and logical operations (Kim et al., 2020) . Answering such questions requires a significant amount of perceptual and cognitive efforts as people need to combine multiple operations such as retrieving values, comparing values, finding maximum, calculating sums and differences of values. For example, the question Q1 in Fig. 1 requires the user to compute the differences between the two lines for each year and find the year with the highest difference. The goal of a Chart Question Answering (ChartQA) system is to help users by taking a chart and a natural language question as input and pre- dicting the answer. This task differs from other QA tasks such as QA on texts (Rajpurkar et al., 2016) and tables (Pasupat and Liang, 2015) because the input for ChartQA is a visual representation of data that can draw a reader's attention to various prominent features such as trends and outliers (Kim et al., 2020 (Kim et al., , 2021)) . Also, people tend to ask questions by referring to visual attributes of marks. For example, in Fig. 1 , Q2 refers to the color of a mark ('line') and its attribute ('peak') in the chart. While the task of ChartQA has received growing attentions in recent years, existing datasets have several major limitations: (i) the questions are generated automatically using pre-defined templates (Kahou et al., 2017; Kafle et al., 2018; Chaudhry et al., 2020; Singh and Shekhar, 2020) which lack naturalness, (ii) the charts are created automatically using a programming tool like Matplotlib (Singh and Shekhar, 2020) which do not reflect the diverse styles of many real-world charts, and finally, (iii) in most datasets, the answer comes from a small fixed sized vocabulary (e.g., chart axis labels, 'yes', 'no'), ignoring many complex reasoning questions where the answer is derived through various mathematical operations such as aggregation and comparison. Since most datasets only support fixed vocabulary questions, existing models usually treat the task as a classification problem and rely on dynamic encoding techniques with the questions and answers encoded in terms of spatial positions of chart elements (e.g., x-axis-label-1). Such ap-proaches do not work when the OCR model generates errors or when the question refers to chart elements using synonyms (e.g., US vs. United States). PlotQA (Methani et al., 2020) attempts to support open vocabulary questions by applying a TableQA model (Pasupat and Liang, 2015) but it does not consider any visual features of a chart which are critical for answering visual reasoning questions. To address these limitations, we present a largescale benchmark covering 9,608 human-written questions focusing on logical and visual reasoning questions. Since human annotations are costly, we also generated another 23,111 questions automatically from human-written chart summaries using a T5 model (Raffel et al., 2020) and manually validated a subset of it for quality assurance. In this way, we collect a large number of questions automatically while maintaining rich variations in language as they were generated from human-written summaries. Our benchmark consists of 20,882 charts which are curated from four different online sources to ensure variety in visual styles and topics. To address the challenges introduced in our benchmark, where many questions involve complex reasoning and visual references to charts, we propose an approach that combines visual features and extracted data from the chart image. Our pipeline first extracts the underlying data table from the chart image by adapting the ChartOCR model (Luo et al., 2021) as well as the visual features from the chart image using neural models. Then, we adapt two transformer-based QA models where we utilize both the extracted data table and visual features of the chart in a unified way. Our models achieve the state-of-the-art results, or stands on par with the previous models on the previous datasets as well as on our newly created benchmark. In sum, our main contributions are: (i) A large-scale ChartQA dataset with real-world charts and human-authored question-answer pairs; (ii) a pipeline approach that combines visual features and automatically extracted data from charts to utilize in transformer-based QA models that provide state-of-the-art results; and (iii) an extensive analysis and evaluation of the performance of our models. Our code and dataset are publicly available at https://github.com/vis-nlp/ChartQA Related Work Existing Datasets ChartQA differs from previous datasets in two main aspects: the questions' types (human-authored vs. template-based) and the chart source (real-world vs. generated using a tool). A detailed comparison is shown in Table 1 . Earlier datasets such as FigureQA (Kahou et al., 2017) , DVQA (Kafle et al., 2018) , LEAF-QA (Chaudhry et al., 2020) and LEAF-QA++ (Singh and Shekhar, 2020) are mostly synthetic where the questions are generated using a small number of templates and the answers come from a fixed set of vocabulary (e.g. 'yes', 'no'). Moreover, their charts are created automatically using the same software. While FigureQA and DVQA use synthetically-generated data to plot the charts, LEAF-QA and LEAFQA++ use real-world data. PlotQA (Methani et al., 2020) is the only dataset with open-vocabulary questions that require applying aggregation operations on the underlying chart data. However, they do not have visual reasoning questions while their questions are still template-based and the charts are plotted using a software. Kim et al. (2020) ran a formative study with a very small human-authored dataset consisting of 52 charts and 629 QA pairs to understand how people ask questions about charts and explain answers. To our knowledge, there is no large-scale Chart QA dataset involving visual and logical reasoning questions written by humans on real-worlds charts which motivated us to build a new dataset. Existing Models There are two main approaches for Chart QA. The first approach uses classificationbased visual QA models that can only handle fixedvocabulary questions (Chaudhry et al., 2020; Singh and Shekhar, 2020; Kafle et al., 2019; Kahou et al., 2017; Kafle et al., 2018) . These models use encoders to encode the question and the chart image and an attention mechanism to combine the features of both the question and chart before applying a classification layer. These models mostly utilize dynamic encoding techniques to encode the question in terms of the positional information of the textual elements in the chart image that are prone to OCR noise. The second approach applies table QA methods by either assuming that the data table of the chart is given (Kim et al., 2020; Masry and Hoque, 2021) or by extracting it from the chart image using vision techniques (Methani et al., 2020) . Chart Data Extraction Early papers introduced semi-automatic systems to extract the data from the chart images (Savva et al., 2011; Jung et al., 2017) . Choi et al. (2019) , Liu et al. (2019) , and (Siegel et al., 2016) For the Pew dataset, we only crawled chart images since the underlying data tables are not available. For the other three, we extracted the underlying data tables, metadata (e.g., title, chart type), SVG file and associate text description. Finally, we extracted the bounding boxes information of the different chart elements (e.g., x-axis labels) from the SVG files to train our data extraction models. Data Annotation We have two main annotations procedures: (i) collect human-authored QA pairs using Amazon Mechanical Turk (AMT) and (ii) generate QA pairs from the Statista human-written summaries. \u2022 Human-authored QA annotation To create human-authored QA pairs, we designed an AMT task (see A.1 for details) in which we asked the crowdworkers to focus on two types of questions for each chart image: compositional and visual questions. Compositional questions contain at least two mathematical/logical operations like sum, difference and average, while visual questions refer to the visual attributes such as color, height, and length of graphical marks (e.g., bars) in the chart. We focus on these two types of questions because people tend to ask them commonly (Kim et al., 2020; Hoque et al., 2018) and previous datasets mostly do not focus on such complex visual and logical reasoning questions. For each chart, the workers provide two questions with the answers. The same questions are then answered by another annotator. If both workers' answers exactly match, we consider the answer to be correct. Otherwise, we manually check the answers to select the final correct answer. Overall, the agreement between the crowd workers based on exact matches was 61.04%. However, such exact match does not consider typos or lexical variations (e.g., 3$ vs. 3 dollars, 86.33 vs 86.3) that are common in human annotation. Hence, we have also manually checked the agreement on 500 random samples and found the agreement to be much higher (78.55%) when we consider typos and lexical variations. \u2022 Dataset Augmentation Prior work on QA has performed data augmentation by either creating template-based or machine generated questions, e.g., for visual QA (Kafle et al., 2017) and textual QA (Lewis et al., 2021 generally lack rich linguistic variations. On the other hand, large-scale language models like T5 (Raffel et al., 2020) which are trained on very large data from various web sources can learn general linguistic properties and variations (Brown et al., 2020) . Therefore, we opt for the latter. Specifically, we fine-tune a pre-trained T5 model on the SQuAD QA dataset (Rajpurkar et al., 2016) and apply to the human-written chart summaries that come with the charts from Statista to automatically generate questions that are human-like with sufficient lexical and syntactic variations. The process involves training and applying two T5 models: one for answer extraction and the other for answeraware question generation. For answer extraction, the T5 model is trained to generate possible answers separated by [SEP] token given the textual summary as input (i.e., trained on SQuAD's passage \u2192 answer pairs). For question generation, the proposed answer is first concatenated with the summary in the format: Answer: Answer Context: Chart Summary. Then, the T5 model is trained to generate a question from the given question using the chart summary. This model is trained on SQuAD's (passage, answer) \u2192 question pairs. Since the summaries are human-written, the generated questions are similar to the human-authored questions (see example questions in A.7). However, the T5 question generation model may still generate invalid questions because of the mismatch in training and test domains. We notice that some questions are either incomplete or not answerable from the chart (e.g., 'What province includes Cape Town?' is not answerable because it requires knowledge outside of the chart). To filter out such invalid questions, we developed a simple heuristic where we filter out the question if the answer cannot be found in the chart data table. This heuristic was inspired by the fact that most answers to the generated questions were values/labels of chart elements. After applying the heuristic, we manually analyzed 1,250 QA pairs and found that 86.64% of them were complete, answerable, and correct given  43.0 Both visual & compositional Between the second and the third age groups from the left, which opinion deviates the most? 33.3 Table 4 : Distribution of questions types of among 300 randomly chosen human written questions (blue-colored tokens make visual references to the chart). the chart. Moreover, for the sake of fair evaluation, we manually cleaned the test set of the machine generated dataset by removing invalid questions. \u2022 Data split We randomly split both of the human-written (ChartQA-H) and machine generated (ChartQA-M) QA pairs into train, validation, and test sets as shown in Table 2 . Dataset Analysis Our dataset has three commonly used chart types: bar, line, and pie charts (Table 3 ). Bar is the most common type of chart across all datasets as they are quite prevalent in real-world sources. We further categorize the bar and line charts into simple vs complex where data tables of simple charts have only two columns where complex charts involve multiple columns (e.g., stacked or grouped bars and multi-line charts). Among bar charts, 79.4% were simple and 29.6% were complex. For line charts, 61.0% were simple and 39.0% were complex. We have also analyzed the basic linguistic statistics about our benchmark (see A.2). Unlike previous datasets, our benchmark has more unique tokens on both types of QA pairs and on both questions and answers -6,150 and 4,319 unique tokens in questions and answers respectively in ChartQA-H whereas 12,379 and 11,979 unique tokens in questions and answers respectively in ChartQA-M. We also observe that questions cover a variety of syntactic structure and sometimes exhibit informal languages and typos. Overall, this suggests the richness of language variations which may introduce more challenges to the task. Finally, the topic distribution in our data is quite diverse as it is constructed from four different sources. Politics is a common topic among all sources but particularly in the Pew dataset where nearly half of charts are about U.S. Politics & Policy (45.4 %). Other common topics include economy, health, and society. To analyze the nature of questions, we randomly selected 300 QA pairs from our benchmark and categorized them into four types (Table 4 ). We see that the vast majority of questions (76.33% in total) are either compositional or both visual and compositional, which reflects the real-world scenarios where people ask complex reasoning questions. We also find that people make visual references to a variety of visual attributes of marks (see A.2), most commonly to color (e.g., 'orange line') and length (e.g., 'tallest bar') followed by size (e.g., 'largest slice') and position (e.g., 'leftmost bar'). Method Problem Formulation & Data Extraction The overall process of our ChartQA system is shown in Fig. 2 . We consider two problem settings for ChartQA. The first setting assumes that the underlying data table of the chart image is available. Formally, we are given a dataset with N examples D = {c i , t i , q i , a i } N i=1 , where c i represents a chart image, t i represents the underlying data table, q i represents a question over c i , and a i represents the answer to the question. The ChartQA models learn to predict the answer a i given c i , t i and q i . The gold data tables are not generally accessible in most real-world scenarios. Thus we consider the second setup where the underlying data table t i for chart image c i is extracted by adapting a state-ofthe-art ChartOCR (Luo et al., 2021) . ChartOCR first locates the main elements of the chart image (e.g., plot area, title) as well as data-encoding marks (e.g., bars ) using key-point detection networks. It then uses the detected keypoints of each mark along with axis-labels to estimate the data value of that mark. However, it does not associate the predicted data values with corresponding text labels (e.g., xaxis-label). Hence, we extend their approach to output the fully-structured data tables. We utilize the CRAFT (Baek et al., 2019) model to recognize the texts in the chart elements. Then, we associate the data values with their text labels using positional and color information (see A.3 for details). Models Our approach to ChartQA builds on two of the state-of-the-art TableQA models: T5 (Raffel et al., 2020; Nan et al., 2021) and TAPAS (Herzig et al., 2020) . The input to these models consists of the question q i and the data table t i . Different from TableQA, ChartQA often involves extracting visual information from chart images. For this, we also experiment with the visual counterparts of the TableQA models that also take the chart image features into account. While T5 has a visual variant, VL-T5 (Cho et al., 2021) , TAPAS does not. In this work, we extend Tapas to consider the image features and call it VisionTAPAS. More details on models are provided in A.5. \u2022 T5 (Raffel et al., 2020) is an encoder-decoder model which unifies the NLP tasks as text-totext generation using the same architecture and loss function. It has been pre-trained on massive amount of unlabelled data with a self-supervised denoising objective. To fine-tune T5 on our ChartQA task, we flatten the data table and feed it along with the question as: \"Question: Question tokens  \u2022 VL-T5 (Cho et al., 2021) is an extension of T5 that unifies the Vision-Language (VL) tasks as text generation conditioned on multimodal inputs. The input consists of both textual tokens and visual features of the objects extracted from the image using Faster R-CNN (Ren et al., 2015) . The model is pre-trained on multiple multimodal tasks such as language modeling, visual QA, and visual grounding. We utilize VL-T5 for our ChartQA task in the following manner. For the textual input, we do the same as T5 where we flatten the data table of the chart image and concatenate it with the question text. For the visual input, we extract the visual features of different marks in the chart image (e.g., bars, lines) using Mask R-CNN (He et al., 2017) with Resnet-101 as its backbone (see A.4 for details). Unlike the original VL-T5 where a fixed number of objects is provided (36), the number of elements varies from one chart to another. To account for this, we pad the extracted visual features with zeros to have a fixed length of 36. \u2022 TAPAS (Herzig et al., 2020 ) extends a BERT (Devlin et al., 2019) architecture with additional positional embeddings for rows and columns to encode a table. As shown in Fig. 3a , the input to the model has the following format: [CLS] Question tokens [SEP] Flattened table tokens. The tokens are encoded with the table-specific positional embeddings in addition to BERT's segment and positional embeddings. The model has two output heads: aggregation operation head and cell selection head. The aggregation operation head predicts an operation (e.g., COUNT, SUM, AVERAGE, NONE) which is then applied to the cell values selected by the cell selection head. Depending on the operation type, the selected cells can constitute the final answer or the input used to infer the final answer. TaPas is first pre-trained on masked language modeling objective using table-text pairs crawled from Wikipedia where table cells are randomly masked and the model is trained to predict them. It is then fine-tuned in a weakly-supervised manner (using answers as the only supervision) with endto-end differentiable objectives. \u2022 VisionTaPas is our extension of TaPas for QA over charts. It consists of three main components: a vision transformer encoder for encoding the chart image, a TaPas encoder for encoding the question and data table and a cross-modal encoder (Fig. 3b ). Vision Transformer or ViT (Dosovitskiy et al., 2021) utilizes the transformer encoder architecture (Vaswani et al., 2017) We initialize the ViT module with the pre-trained weights from (Dosovitskiy et al., 2021) . The TaPas encoder is utilized in the same manner as described above to encode the tokens in the question and the data table. For an input token sequence {w cls , w 1 , . . . , w m }, an L-layer TaPas generates the corresponding encodings Z = {z L cls , z L 1 , . . . , z L m }. This module is initialized with the TaPas weights (Herzig et al., 2020) pre-trained on the WikiTQ dataset (Pasupat and Liang, 2015) . The Cross-modality Encoder takes the output of ViT and TaPas encoders (H and Z) and compute multimodal encodings. It has four blocks, each containing a visual branch and a textual-tabular branch. The input first passes through the multiheaded cross attention layers in parallel, where in the visual branch the query vectors are the visual features, and the key and context vectors are the textual-tabular features and vice versa in the textual-tabular branch. The cross-attended features are then passed through a self-attention layer followed by a fully connected layer. Similar to the transformer model, each layer applies layer normalization (Ba et al., 2016) and is wrapped with a residual connection. Finally, we append the aggregation operation and the cell selection heads of TaPas to the final layer at the textual-tabular branch. Extension to Other Operations Many questions in our ChartQA dataset require performing a subtraction or ratio operation, which the original TaPas model does not support. We thus extend the operation head to add those two operations (Fig. 3b ). However, instead of training them in a weaklysupervised manner based on the final answer (as done in TaPas), we find it more effective when provided with more direct but potentially noisy supervision on the cells to consider. We rely on some heuristics to generate such supervision in our training data. For example, given a question \"What's the difference between A and B?\", an answer 5, and data values \"3, 6, 8\", we look for two values between which the difference is 5 (i.e. 8 and 3). While this may yield noisy supervision, similar approaches have been successfully exploited to inject reasoning capability in neural models (Geva et al., 2020; Saxton et al., 2019) ; on a random sample of 100 such questions, a manual checking shows 24% noise with our heuristics. To handle the fixed vocabulary answers (e.g. 'Yes', 'No'), we further extend the operation head to include those classes. Evaluation Datasets, Baselines & Metrics We evaluate our models on three datasets from previous work namely, FigureQA (Kahou et al., 2017) , PlotQA (Methani et al., 2020) and DVQA (Kafle et al., 2018) , as well as our newly created ChartQA dataset. We compare our benchmarking models ( \u00a74.2) with two following baselines 1 : \u2022 PREFIL (Kafle et al., 2019) is a classification approach that fuses the question and image features in parallel. The features are then aggregated and projected into a final classification layer. \u2022 PLOTQA* is our reimplementation of PlotQA (Methani et al., 2020) . It parses the chart image to extract the underlying data table and then employs a TableQA model from Pasupat and Liang (2015) . However, since their data extraction approach is specific to their synthetic dataset that does not generalize well to real-world charts, we use data tables extracted according to our method ( \u00a74.1) to evaluate their approach. Following Methani et al. (2020) , we use a relaxed accuracy measure for the numeric answers to allow a minor inaccuracy that may result from the automatic data extraction process. We consider an answer to be correct if it is within 5% of the gold answer. For non-numeric answers, we still need an exact match to consider an answer to be correct. Results Previous Datasets When the gold data table is provided, VisionTaPas and VL-T5 achieve near perfect results, however, the performance slightly decreases when it is not provided (Table 5 ). Still, VisionTaPas and VL-T5 achieve state-of-the-art results on DVQA (fully-automated setup) and PlotQA V1 datasets, respectively. For example, Vi-sionTaPas achieves 94.54% accuracy in the DVQA test set (14.5% margin over PReFIL). Moreover, our approach proved to be more robust to OCR noise. Unlike PReFIL whose performance significantly dropped by 16.49% when using OCR outputs instead of ORACLE, VisionTaPas only witnessed a marginal decrease in performance (0.92%). Similarly, in the PlotQA dataset, both models have outperformed the PlotQA model by wide margins. Another observation is that the improvement of VL-T5 over T5 is limited only to the PlotQA V1 dataset likely due to the lack of visual reasoning questions. In fact, the performance of both models is quite similar on PlotQA V2 test set where the majority of the questions are not visual. Finally, while the TaPas model achieves the best results on FigureQA (Gold Table setup), it does not perform very well on DVQA and PlotQA. This is likely because most questions in FigureQA are answerable from the data table alone. In PlotQA, however, questions are not always answerable from the data table alone and may involve the difference and ratio operations which are not supported by TaPas. This highlights the importance of the extensions we have made in the VisionTaPas model. ChartQA Dataset We observe that VisionTaPas achieves state-of-the-art performance on both problem scenarios. PReFIL performs pooly (4.8%) as it is a classification model which does not work well for the open-vocabulary questions in our dataset. We also notice VL-T5 does not necessarily improve over T5, likely because many visual questions in our new dataset involve multiple references to chart elements and VL-T5 cannot effectively capture such references. Overall, the accuracies of different models are generally lower in our dataset compared to previous datasets, suggesting the challenges introduced with the human-written visual and logical reasoning questions. Finally, the performance of our models decreases when the gold data table was not given. This highlights the increasing challenge of automatic data extraction from real-world charts with diversity in styles. We also evaluate the transferability of the models and the datasets, where we first pretrain the two top performing models (VisionTaPas and VL-T5) on the PlotQA dataset and then fine-tune them on ChartQA. From Table 6 , we notice that the accuracy increased from 41.56% to 51.84% for VL-T5 while the improvement for VisionTaPas was marginal (1.56%). One possible explanation is that VisionTaPas does not support nested arithmetic operations which are prevalent in ChartQA, so pretraining does not have a substantial effect. In contrast, we observe that the performance gain for VL-T5 were mainly for the compositional questions that do not require nested operations. Overall, this suggests that large datasets like PlotQA can be useful for pretraining the model even if the questions are generated from a small number of templates. We also performed another experiment in which we train the VL-T5 and VisionTaPas on the PlotQA dataset and evaluate directly on the ChartQA dataset without any fine-tuning. As shown in Table 6, the performance of the models decreased by wide margins when they are trained on the PlotQA dataset instead of the target dataset (e.g,. 45.52% to 31.96% for VisionTaPas). This supports our hypothesis that our newly created dataset, ChartQA, introduces more challenging visual and compositional questions and more lexical variations which the previous datasets lack. Ablation Studies To assess the importance of extensions we made in the VisionTaPas model, we conducted an ablation study in which we remove the supervision for 'difference' and 'ratio' operations from the model. The overall accuracy dropped by 1.80% and the accuracy on ChartQA-H (which have many such questions) dropped by 4.76% which suggests the usefulness of these operations (Table 6 ). Model ChartQA  We further analyze the performance by chart types and question types (see A.6). VisionTapas and VL-T5 perform better on bar charts while the performance decreases for other charts mainly due to higher data extraction errors, especially for pie charts which are less common in our dataset. To analyze question types, we randomly sampled 200 human-written questions. As expected, the performance is much higher on the data retrieval questions that do not require mathematical reasoning while the performance is lower for visual questions which refers to chart elements. Qualitative Analysis We have manually analyzed model predictions to investigate the key challenges existing models face (see sample predictions in A.7). Logical Inference with Nested Operations While VisionTaPas and VL-T5 handle various mathematical/logical operations, still they cannot effectively handle nested operations. For example, Q1 in fig. 4 requires the model to add two numbers and then subtract from another number, but our model only outputs the difference between two numbers. In future, we will extend the VisionTaPas model (by possibly training it in a sequential fashion (Cho et al., 2018) ) to address the issue. Input Representation Complex visual compositional questions may require a multi-stage reasoning process (e.g., Q2 in fig. 4 ). Currently, our models take the data table and the visual features of the chart separately and then combine them. Such representation does not fully capture the chart structure. In future, we will develop better representations including semantic graph representations (Teney et al., 2017) that can exploit the relations among the question, chart objects, and data values. Computer Vision Challenges Table 5 indicates that performance of our models decrease when the gold table is not given, suggesting the need for more accurate data extraction. Current approaches for automatic data extraction are modular and combine deep learning and rule-based methods which are error-prone. An end-to-end deep learning approach could help improve the performance and generalize well to different chart styles. Conclusion We present ChartQA, a new large-scale benchmark with human-written questions focusing on visual and logical reasoning. We also introduce a new approach that combines visual features and extracted data table from a chart to answer questions. While our evaluation highlights the promise of this approach, it also reveals several unique challenges emerge from the visual and logical reasoning questions asked by human which exhibit the informal, intricate, and nuanced nature of language. We hope that our benchmark will serve as a starting point for others to address these challenges. Ethical Considerations During the dataset collection and annotation process, we have considered several ethical issues. To respect the intellectual property of dataset sources, we only used the publicly available charts that comply with their terms and conditions. According to Statista publication rights, 2 users are given open access to the publicly available charts for academic purposes. According to the terms and conditions for Pew, 3 users are allowed to download and publish the content as long as they are attributed to the Center or are not attributed to a different party. According to OECD 4 terms and conditions, users can crawl and use the data in their own work for any purpose unless where restrictions apply. According to OWID 5 terms and conditions, all their data are open access and users can download or utilize the data in their own work. In order to fairly compensate the Mechanical Turk annotators, we considered the minimum wage in the United States at the time ($7.25 USD per hour). The estimated time taken for each task is 3-5 minutes. Hence, these annotators received $0.6 USD for each task. Additionally, to protect the privacy of these annotators, all of their annotations were anonymized. To ensure the reproducibility of our experimental results, our hyperparameters settings are provided in Appendix A.5. Our models can be abused to mislead the public about the charts content and implications. While our models provide state-of-the-art results on most of the existing datasets, we can not guarantee that their output will be correct all the time. A Appendices A.1 Additional Details on Data Annotation Amazon Mechanical Turk Task: In each HIT (Human Intelligent Task), the workers verify two previously asked questions by other workers and also provide two new QA pairs. To ensure quality, we selected workers with an acceptance rate of 95% and total accomplished HITs of 5000. Moreover, we further filtered the workers by giving them a pretest to select the best qualified workers for this task. The data collection interface is shown in Figure 5 . While presenting the chart, we ensure that the data labels of chart elements are visible to workers so that they can accurately perform the necessary arithmetic and logical operations to provide and answer the questions successfully. A.2 Dataset Analysis Table 7 shows some linguistic statistics about our benchmark. Also, Figure 6 shows the distribution of topics in our dataset for each of the four sources. Politics is a common topic among all sources but particularly in the Pew dataset where nearly half of charts are about U.S. Politics & Policy (45.4 %). The most frequent topic from OECD and OWID is Society (34.0 % and 26.0 % respectively). Furthermore, we analyzed how people make visual references to charts in their questions. Table 8 shows the usage of visual references made in the randomly selected 300 QA pairs. A.3 Automatic Chart Data Extraction Model: We extend ChartOCR (Luo et al., 2021) which relies on both deep-learning models and rulebased techniques to parse the chart image into the underlying data table. As described in Section ( \u00a74.1), the chart image is parsed in three main stages. In the first stage, key-point detection networks, adapted from (Law and Deng, 2019) , locates the chart visual marks (e.g. bars, plot area, line points). Ideally, the network locates the top-left point and bottom-right points for the rectangular objects (e.g. bar, plot area). In line charts, the detection network locates the coordinates of the points connecting the line segments. In pie charts, the network locates the intersection points between the pie segments along the pie perimeter. We extend their detection networks to also locate the chart textual elements (e.g. x-axis-label, legend-label ) as shown in Figure 7a and utilize the CRAFT model (Baek et al., 2019) to read their underlying texts. In the second stage, the chart scale is estimated using the y-axis-labels value for line and bar charts, Figure 7b . For pie charts, the value of each segment is estimated by calculating the angle between its borderlines. Finally, the model aggregates the extracted data values (using color and proximity heuristics) to output the final raw data values. We extend their approach to extract the fully-structured data table with the textual labels (e.g. column headers). As shown in Figure 7 , we associate the estimated bars data values (e.g., '17.13', '40.14') with their closest x-axis-label ('Snapchat'). Moreover, if the chart has more than one data series (dark bars or blue bars values), each data series is matched with its legend-label (e.g., '2016', '2014') based on the color of the legend mark and data-encoding marks (e.g., bars). If we cannot match data values with legends by colors (e.g., when all legend marks have the same color or there are no legend marks), we use other criteria that associate dataencoding marks with legend marks (e.g., proximity, alignment). For example, in Figure 8b , 'More' is matched with '17' and '29' since they are vertically aligned. Similarly, for line charts if there is no explicit legend mark for a line series we associate the legend labels with the points of their closest lines as shown in Figure 8a . Evaluation Metric: Our evaluation metric is adapted from ChartOCR (Luo et al., 2021) . The distance between any two data values is estimated as follows: D(gt, pr) = min(1, || gt \u2212 pr gt ||) where gt is the ground truth value and pr is the predicted value. For each chart, the cost matrix C, where C n,m = D(gt n , pr m ) is computed and the total minimum cost is calculated by solving the following linear sum assignment problem Cost = K \u2211 i=1 K \u2211 j=1 C i,j X i,j Where K = max(N, M ) and X is a binary assignment matrix. The final overall score is then estimated as follows: Overall Score = 1 L L \u2211 i=1 1 \u2212 cost K i where L is the total number of charts. Our evaluation results are shown in Table 9 . We have noticed that the accuracy is specifically lower on line and dot line charts in FigureQA and PlotQA. In DVQA, the extracted tables from logarithmic-scale charts were quite noisy since ChartOCR does not support them. Moreover, PlotQA has many charts with very large values (usually written in E notation). Hence, errors in such figures have higher impact on the overall accuracy. Overall, the accuracy on PlotQA and ChartQA are generally lower since they have more complex charts (PlotQA has numerous charts with very large values (e.g., 1e 6 ) and ChartQA has real-world challenging charts). A major limitation of evaluation metrics for the chart data extraction is that they do not take the extracted textual tokens into consideration (which are much more noisy in real-world figures). Hence, better metrics are still needed in the future. A.4 Visual Features Extraction in VL-T5 Object Detection (Mask R-CNN) We train the model to detect the following 15 objects: 'Legend', 'yAxisTitle', 'ChartTitle', 'xAxisTitle', ' Leg-endPreview', 'PlotArea', 'yAxisLabel', 'xAxisLabel', 'LegendLabel', 'PieLabel', 'bar', 'pie', 'pieSlice', 'line', and 'dotLine'. For the bounding boxes annotations, we use the available bboxes. For the Acknowledgement The authors would like to thank the anonymous reviewers for their helpful comments. This research was supported by the Natural Sciences & Engineering Research Council (NSERC) of Canada. masks, we generate them easily using the bounding boxes for all the rectangular objects. For 'pieSlice' and 'pie', we follow a similar approach to (Singh and Shekhar, 2020) where we generate the masks by projecting the radius along the pie perimeter from the starting to the ending points of each slice. We use the detectron2 library (Wu et al., 2019) and initialize the model with pre-trained wights on the COCO dataset (Lin et al., 2014) . We fine-tune the model with a batch size of 8 and an initial learning rate 0f 0.00025 for 50K iterations. A.5 ChartQA Baseline Models T5 and VL-T5 fine-tuning process setup is shown in Figure 9 . Our experiments were carried out on one 4-V100 GPU and one 4-A100 GPU machines. Fine-tuning VL-T5 on the PlotQA dataset was the longest experiment which took around 64-70 hours on 4 V100 GPUs. TaPas We follow the same settings as (Herzig et al., 2020) on the WikiTQ dataset (Pasupat and Liang, 2015) and fine-tune the TaPas-base-wtq for 40K iterations with a batch size 24 on DVQA, PlotQA, and our new dataset. For FigureQA, we follow similar settings to (Eisenschlos et al., 2020) and fine-tune the model with classification objective for 4 epochs with a batch size of 48 and initial learning rate of 0.00001. A.7 Sample Questions and Outputs Sample machine-generated questions with the human-written summaries are shown in Table 12 . Sample predictions from our model, VisionTaPas on ChartQA test set are shown in Figure 10 . Since what year has there been an increasing trend in arrivals? Data Retrieval The statistic shows the youth unemployment rate in the Gambia from 1999 to 2019. According to the source, the data are ILO estimates. In 2019, the estimated youth unemployment rate in the Gambia was at 12.44 percent. What was the youth unemployment rate in the Gambia in 2019? In what year did Portugal's population reach 10.29 million?",
    "abstract": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixedvocabulary. In this work, we present a largescale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
    "countries": [
        "Canada",
        "Singapore"
    ],
    "languages": [],
    "numcitedby": "1",
    "year": "2022",
    "month": "May",
    "title": "{C}hart{QA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"
}