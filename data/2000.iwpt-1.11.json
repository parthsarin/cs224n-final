{
    "article": "A transformation-based approach to robust parsing is presented, which achieves a strictly monotonic improvement of its current best hypothesis by repeatedly applying local repair steps to a complex multi-level representation. The transformation process is guided by scores derived from weighted constraints. Besides being interruptible, the procedure exhibits a performance profile typical for anytime procedures and holds great promise for the implementation of time-adaptive behaviour. Introduction Parsing procedures always have to be designed around a number of pre-specified requirements which arise from specific conditions of the individual application area in mind. Text retrieval tasks , for instance, can be accomplished already with a rather shallow analysis whereas speed and fail-soft behaviour are of utmost importance. Grammar checking and foreign language learning applications, on th e other hand, must provide for highly prec ise error detection ca pabilities but differ considerably in their coverage requirements. One of the most demanding combination of target spec ifications results from the development of sp oken language dialogue systems. Since this task is an attempt to model central capabilities of the human language fac ulty, rather strong criteria have to be met in order to achieve natural communicative behavior on a competitive level : \u2022 Robustness: A spoken language dialogue system is typically confronted with a rich va riety of linguistic constructs and will almost inevitably have to deal with extragrammatical input sooner or later. Also, repairs, hesitations, and other grammatical deviations will frequently produce ungrammatical utterances, while the recogn ition uncertainty inherent in spoken language input further increases the ambiguity. The parsing component must be able to cope with these problems in a robust way. Be sides being able to return (possibly partial) analyses even for unexpected and arbitrarily distorted input it is al so necessary to provide some kind of measure of how sure the parser is about its results. \u2022 C omplete d isambiguation: Na tural language utterances typically exh ibit ambiguity when treated in isolation. Nevertheless , a simple en umeration of different (structural) readings almost never can be considered a sensible contribution to a practical language processing task. Although interactive applications can en ga ge the speaker in a kind of clarification dialogue, usually this possibility brings many additional complications and should only be considered a measure of last resort. Instead, a well-designed system should make use of all the available information to obtain a single interpretation of the utterance , which is only abandoned if the user explicitly signals a communication failure. \u2022 Multiple-source d isambiguation: A va st va riety of knowledge sources can contribute to disambiguation : Syntactic constraints, semantic preferences, prosodic cues, domain knowledge , the dialogue history, etc. All the available knowledge should be put to use as soon as possible so that local ambiguity will not create a large space of useless hy potheses during processing. For rea sons of perspicuity and accessibility the integration of these knowledge sources should be organized in a way which maintains their modularity. Only then can the respective contributions of individual components be evaluated and properly balanced against each other. \u2022 Time-aware behavior: Three closely related aspects must be considered with respect to the temporal behavior of a language processing component: Efficiency, incremental processing and temporal adaptivity. Whereas efficiency always has been an issue of major In principle, such an anytime behaviour can be achieved by trading time against quality. Therefore a baseline performance will be required which allows the quality of available results to grow monotonically as more effort is made, and which is robust enough so that results of slightly reduced quality can still be considered being acceptable in a certain sense. Obviously, the most natural measure of external temporal pressure is given by the speaking rate of the dialogue partner. Thus temporal adaptivity makes sense first of all under an incremental processing scheme. Its basic mechanisms, however, can also be studied in the fa r simpler non-incremental case. This paper investigates a non-standard parsing approach, which attempts to reconcile two different kinds of robustness, namely robustness against unexpected and ill-formed input and robustness against external temporal pressure. It is based on the application of con straint satisfaction techniques to the problem of structural disambiguation and allows the parser to include a wide va riety of possibly contradicting informational contributions. Different solution procedures are presented and compared taking into account solution quality an d the observed temporal behavior. Parsing As A Consistent Labeling Problem Although most contemporary unification-based grammars can be said to employ con straints, none of them fulfill the traditional definition of a Constraint Satisfaction Problem (CSP) con sisting of a fixed number of variables, which receive their values from domains, i. e. sets of alternative va lue assignments. The global consistency of a va lue assignment is defined by means of local constraints, which can be understood as sets of admissible value tuples, specified  For this purpose, constraints are annotated with a weight or score between zero and one that determines how easily that constraint may be violated. Hard constraints have a weight of zero and must not be violated by any solution. Constraints wh ich reflect regularities of the grammar receive a small we ight greater than zero wh ile constraints that model mere preferences will be we ighted close to one . A solution candidate to the CSP can then be assigned a score by determining the product of the we ights of .all the constraints wh ich are violated somewhere in the structural description. Under these premises, parsing becomes a multidimensional optimization problem. For reasons of efficiency at most binary constraints can be allowed, hence the universal ex pressive powe r of constraints is not available in practice . This serious shortcoming , however, can be neutralized to a certain degree by approximating higher order constraints using binary ones. Another disadvantage is the lack of a variable binding mechanism like the one which is provided by a unification operator together with the missing notion of a constituent (which, however, is share d with most dependency grammar approaches). Experience with grammar writing has confirmed that nevertheless nontrivial subsets of grammar can be encoded suc cessfully, although some phenomena such as long-distance dependencies can only be modeled approximatively [SMFS] . Constraint dependency grammar is a purely declarative formalism . This property makes it amenable to a variety of problem solving strategies that can be compare d, e. g. with respect to their temporal behaviour. The possibility to add further representational levels supports the integration of knowledge contributions from ve ry different sources into a single solution space without sacrificing the strict modularity of the grammar and of the structural representation. Of course, this possibility is limited to only those knowledge sources that can meaningfully attach information to single word forms. A single interpretation of the incoming utterance can be obtained by using all available evidence, including minor preference indicators like ordering , distance or default cases . A truly ambiguous sentence will usually allow several analyses with only small differences between their scores , which can be ig nored if desired. The approach exhibits a remarkable robustness ag ainst unexpected and ill-formed np ut [MS98b] , which obviously can be attributed to three important ch aracteristics: \u2022 the use of weighted constraints , which provides for the accommodation of conflicting evidence and therefore makes the analysis of deviating structures possible , \u2022 the redundancy between loosely coupled representational levels, wh ich allows conflicting information on one level to be overridden by sufficient evidence from a complementary one , and \u2022 the possibility to license arbitrary categories as an acceptable, but in most cases highly disfavored, top node of a dependency tree , thus introducing a partial parsing scheme as a natural extension of the normal mode of operation. Note that the resulting robust behavior follows immediately from the fundamental principles of the approach and no error rules or special operations become necessary. Two other ch aracteristics of the approach contribute to the rich potential for obtaining the desired anytime behaviour. In contrast to other parsing approaches the space of all possible analyses for constraint dependency parsing is always finite and very reg ularly structured. Pars ing therefore becomes a process of selection between different analyses with virtually identical formal properties , which considerably facilitates their mutual comparison. Solution Procedures Consistency-based Methods The canonical method for solving a constraint satisfaction problem is to establish a certain degree of consistency in it by deleting incompatible values from its domains, and then select an assignment to all constraint variables from the remaining values. This approach contrasts strongly with common parsing methods that are constructive in nature. Usually, grammatical structures are built up recursively from simpler structures, and ultimately from the information associated with the lexical items present in an utterance. Thus, the number of structures available increases over time. In contrast, the achievement of consistency is an eliminative process: The more progress is made, the fewer values remain in the problem. An attractive property of this kind of parsing is that it can be exactly determined, at any time, how much progress has already been made and how much work remains to be done until disambiguation is completed. This information will be of great use to a time-aware solution procedure. Various well-defined degrees of consistency can be achieved in a CSP, and general algorithms exist to establish any desired degree of consistency. A suitable algorithm for consistency in a partial CSP should remove all those values that do not appear in the optimal solution-a property that is much more difficult to determine. The usual consistency algorithm will find a value that cannot appear in a solution by noting that it cannot appear in a valid n-ary assignment. A similar approach for partial CSP would be to select those values for deletion that only occur in n-ary assignments with low scores. The obvious method is to define a fixed limit and consider all scores below it unacceptable; this has much the same effect as employing the unmodified algorithms on a grammar in which more constraints are hard. Another method known as pruning [MS98b] goes one step further. While a consistency algorithm cannot guarantee how much progress it will achieve, a pruning method will invoke a selection function at regular intervals to select exactly one value for deletion. If this function uses a fixed amount of time, an exact appraisal can be given not only of the amount of work to be done, but also of the actual runtime left until termination. To guarantee that a value is selected for deletion within the allotted time, the selection function will usually have to be heuristic in nature. A simple selection function mimics the behaviour of a 2-consistency algorithm: Tables of mutual support are constructed for all pairs of domains in the problem. The support of a value v from another domain d can be defined as the maximal or the average compatibility of v with any value from d, or in a more elaborate way. The value whose maximal support by any other domain is smallest is selected for deletion. Since the globally optimal solution may consist of values that are locally suboptimal, in general this method of assessing values exclusively by local information may remove the wrong values from a problem. While the CDG formalism ensures that the remaining values form a complete assignment, in general it cannot be guaranteed that this assignment will be the optimal solution, or even a grammatically val id one. Thus, a heuristic consistency al gorithm may fail without result even though there is a val id solution, which defies the purpose of robust processing. Enumeration Most parsers use some kind of search al gorithm to enumerate all al ternatives for local or gl obal ambiguities arising in the analysis of their input. A great number of search variants has been invented for different parsing applications (top-down vs. bottom-up parsing, depth-first vs. breadth-first search , linear vs. island parsing) , and choosing the right method can have dram atic impact on th e efficiency of a parser. In general , considering every possible al ternative ensures that an al gorithm is complete as well as correct, but may require so many resources that it becomes impractical to apply. Since in a problem in CDG, consistency-based methods cannot gu arantee either complete or correct behaviour, a complete method of solution is desirable even if it has other disadvantages. For instance, a complete but inefficient al gorithm will still be of great use to the developer of a constraint grammar to check the val idity of their model, or to verify the results of an incomplete method. For the CSP, a complete search of all possible assignments can be conducted that is gu ar anteed to find the optimal solution. In the partial CSP, a normal best first search can be employed which finds the optimal solution without ever having to expand a partial solution with a lower score. The current implementation of the CDG parser provides a straightforward best-first search in which the variables of a problem are instantiated in a fixed order. This will usually be the order of the words corresponding to the constraint variables. Compared with other parsers, this would be classified as a heuristically driven left-to-right search . It resembles bottom-up parsing in that each word can immediately be integrated into a tree that forms part of the complete dependency structure. A top-down parsing method could be simulated by arranging the search so that every additional dependency edge must modify a word that has al ready been analyzed , starting with those words that can modify the root of the dependency tree. Since the CSP is NP-complete, probably any complete solution method will have an expo nential worst-case complexity. Although the ac tual runtime of a complete search al gorithm is usually far below the worst possible case, and heuristic re-ordering of both domains and values can greatly improve the efficiency, it is difficult to predict even approximately how long a par ticular instance of the problem will take to search . Therefore, a complete search is inadequate as a solution method when time-aware behaviour is required . However, in contrast to other methods a complete search can easily enumerate gl obally near-optimal structures such as those defined by syntactically ambiguous sentences. Transformation An obvious way to overcome the unacceptable temporal behaviour of complete al gorithms is to employ suboptimal methods. A strategy that works well in practice is that of heuristic repair. Rather than attempting to build the correct structure by selecting correct values step by step , this method first constructs an arbitrary dependency structure with errors in it and then tries to correct the errors. Suppose that the dependency structure shown in Figure 2 This method has several advantages as opposed to the previous ones: \u2022 Because a complete dependency analysis is maintained at all times, the algorithm may be interrupted at any time and still return a meaningful answer, though not always the optimal one. Thus, it automatically fulfills a strong anytime criterion. \u2022 The constraints that cause conflicts in a suboptimal assignment can suggest which value is inappropriate and what other value should be substituted. \u2022 Because all analyses of a given utterance comprise the same number of values, it is guaran teed that the optimal solution (if any exists) can be constructed from any other assignment by successively replacing one value at a time. A transformation step is usually defined as the exchange of one value of a constraint variable for an other. By this definition , the correct solution of a CSP of degree n can always be reached in not more than n transformation steps, if the correct replacement value is ch osen at any point. To accomplish this, however, every value in the problem has to be con sidered as an alternative in each step, whereas a backtracking search only has to try out ail values from one particular domain. Obviously the transformation alg orithm will encounter a much greater branching factor. However, the search space is now graph-shaped, an d so not every alternative must be pursued further because the order in wh ich several values are inserted into an an alysis does not matter. Instead, the number of alternatives that are tried out at all can be used as a parameter to speed up the computation. Obviously, the efficiency of such a repair algorithm depends on its ability to select the correct values for repair. Even a totally uninformed repair method can ultimately find the correct solution, since it is simply a random walk through the problem space. For better results, heuristic decision methods must be found to guide the selection. The simple hill climbing method will always ch oose the value that results in the best immediate improvement. The principal difficulty with th is method is that it can get stuck in local optima of the problem space where no immediate improvement is possible. Different methods exist to allow an algorithm to leave such misleading areas of the search space. \u2022 Occasional downhill steps may be allowed so that an algorithm may escape from a local maximum. For example, in the popular method of simulated annealing downhill steps are allowed but gradually discouraged as an alysis progresses. \u2022 In an other approach , hill climbing does not optimize the score of an an alysis itself, but an an cillary cost function that is adapted in each step, so that the local optimum can be turned into an ascent. \u2022 The definition of a transformation step can be ch anged so that several values may be replaced simultaneously. Assignments which differ in several variables will then become adjacent to the current assignment. Again , the number of values that may be ch anged in one step can be used as parameter to influence the speed of th e algorithm. A subsequent difficulty is that after such a repair alg orithm has converged to the optimal solution , it may not terminate. If the optimal solution still causes some minor conflicts, the alg orithm will continually try to repair these conflicts without success, since there is no simple way to distinguish a local optimum from the global one. In this case the individual constraints of the grammar can be used as a taboo criterion : If no repair step is allowed to re-introduce the conflict that prompted it in the first place, termination can be guaranteed. Although repair-based solution methods cannot guarantee to find the optimal solution in all cases, in practice they achieve results comparable to those of exh austive methods. This demonstrates that the values contained in a complete parse are helpful in selecting correct values even though some of them may themselves be incorrect [Minton92] . In this way, parsing by transformation can make use of global information without suffering the full combinatorial explosion of a complete search.                 With the observed increase of the solution quality the parser has a performance profile typical for anytime procedures [BD89]. In 90% of all problems, the solutions found were either identical to those found by a complete search or had even better scores (since the search uses a finite agenda, it may actually become incomplete in large problems). In the final analyses, 99. 7% of all dependency links were established correctly. In general, a near-optimal structure will be constructed after a short time. Finding the exact optimal analysis may take considerably more time in medium-sized and large optimization problems. However, particularly in these cases the algorithm will be consistently faster than a complete, search. Figure 5 gives the average time that the transformation-based solution method takes to terminate, measured in terms of the runtime of a best-first search of the same problems. Clearly, the repair method is faster in most problems except for the highest problem class (which only has two members in our corpus, however). Comparison of the time until the last successful repair with the total runtime shows that the algorithm will usually terminate not long after having reached the optimal analysis. : I j j \u2022--\u2022-\u2022-\u2022 -\u2022-\u2022---\u2022-\u2022-\u2022 i For applications that have to react to varying external time constraints, a parameter is provided that adjusts the number of alternatives to be tried out at each transformation step. In the cases investigated so far, decreasing this parameter can speed up the repair substantially, with an acceleration by a factor of about 3 reducing the accuracy to 86% of correct dependency links. Related work There is a striking analogy between constraint dependency parsing and customary approaches to the task of tagging natural language data: In both cases each word form in the utterance is annotated with a label from a finite set of alternatives and the approaches differ only in the information content of the labels. Although tagging usually. means to classify word forms into (syntactic and semantic) types, the idea can also be extended to the use of functional tags in a straightforward way. Karlsson et al. [KVHA95] use such functional descriptions in Constraint Grammar parsing. Their tagset contains elements like \"subject\" or \"a determiner modifying a noun to the right\". Usually these tags are underspecified, because the exact identity of the modified item is unknown. Constraint Dependency Grammar as discussed in this paper extends this approach to fully specified structural representations. Since the identity of the modified word is now included into the composite tags, the size of the tagset additionally depends on the number of word forms in the given utterance. Moreover, the extension to multi-level disambiguation allows to treat considerably richer representations as compared to what a usual tagger is taking into account. Another approach using complex tags is Supertagging [BJ99], where complex tree fragments are attached to word forms by means of a stochastic model. These tags represent considerably more structure than values in CDG, which correspond to single dependency edges. However, tags are treated in isolation and the compatibility between adjacent structures is modeled only probabilistically. In order to combine tags to complete parse trees an additional processing step has to be carried out after the tagging itself. The idea to obtain a structural description of natural language utterances by applying a sequence of transformations which successively modifies an intermediate representation has first been pursued within the framework of parsing as tree-to-tree transduction [BGQA82], although no explicit notion of scoring and quality improvement was involved at that time. The dynamics of the transformation process was fully under the control of the grammar writer, taking into account the precedence ordering implicit in a sequence of rules and some additional means to influence the degree of non-determinism. Transformation-based approaches have later been applied to the problem of syntactic tagging ( B ri95]. However, focus wa s on inducing an appropriate set of transformation rules from the information contained in an annotated corpus. Possibilities to model grammar by means of contradicting principles are inve stigated cur rently in the framework of optimality theory [PS91]. The grammatical principles postulated there are ranked rather than weighted , with higher ranked regularities completely overriding the influence of the lowe r ones. First applications have been identified in phonology and syntax. Conclusion A novel approach to parsing as constraint-based structural disambiguation has been presented. By combining techniques for robust parsing (graded constraints, multi-level-disambiguation and partial parsing) with the idea of a transformation-based problem solving mechanism, a parser can be created that shows the ty pical temporal behavior of an interruptible anytime algorithm. Further inve stigations will focus on 1. transferring these procedural characteristics to the case of incremental parsing, thus ad dressing particularly the problem of processing time for long utterances, 2. a more thorough inve stigation into time-adaptive behaviour, which should be able to speed up the convergence towards the optimum solution under temporal pressure , at the risk of missing it completely, and 3. the combination of different solution techniques to improve the termination behaviour. The re sulting parsing method mimics human language processing in that it is time-adaptive and robust and therefore lends itself to the implementation of human-machine dialogue systems.",
    "abstract": "A transformation-based approach to robust parsing is presented, which achieves a strictly monotonic improvement of its current best hypothesis by repeatedly applying local repair steps to a complex multi-level representation. The transformation process is guided by scores derived from weighted constraints. Besides being interruptible, the procedure exhibits a performance profile typical for anytime procedures and holds great promise for the implementation of time-adaptive behaviour.",
    "countries": [
        "Germany"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "38",
    "year": "2000",
    "month": "February 23-25",
    "title": "A Transformation-based Parsing Technique With Anytime Properties"
}