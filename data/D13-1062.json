{
    "article": "Chinese word segmentation and part-ofspeech tagging (S&T) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T. In this paper, we propose a unified model for Chinese S&T with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD). Then we regard the Chinese S&T with heterogeneous corpora as two \"related\" tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods. Introduction Currently, most of statistical natural language processing (NLP) systems rely heavily on manually annotated resources to train their statistical models. The more of the data scale, the better the performance will be. However, the costs are extremely expensive to build the large scale resources for some NLP tasks. Even worse, the existing resources are often incompatible even for a same task and the annotation guidelines are usually different for different projects, since there are many underlying linguistic theories which explain the same language with different perspectives. As a result, there often exist multiple heterogeneous annotated corpora for a same task with vastly different and incompatible annotation philosophies. These heterogeneous resources are waste on some level if we cannot fully exploit them. However, though most of statistical NLP methods are not bound to specific annotation standards, almost all of them cannot deal simultaneously with the training data with different and incompatible annotation. The co-existence of heterogeneous annotation data therefore presents a new challenge to utilize these resources. The problem of incompatible annotation standards is very serious for many tasks in NLP, especially for Chinese word segmentation and part-of-speech (POS) tagging (Chinese S&T). In Chinese S&T, the annotation standards are often incompatible for two main reasons. One is that there is no widely accepted segmentation standard due to the lack of a clear definition of Chinese words. Another is that there are no morphology for Chinese word so that there are many ambiguities to tag the parts-of-speech for Chinese word. For example, the two commonlyused corpora, PKU's People's Daily (PPD) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Xia, 2000) , use very different segmentation and POS tagging standards. For example, in CTB, into two words. The POS tagsets are also significantly different. For example, PDD gives diverse tags \"n\" and \"vn\" for the noun, while CTB just gives \"NN\". For proper names, they may be tagged as \"nr\", \"ns\", etc in PDD, while they are just tagged as \"NR\" in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The first step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the final taggers by using the outputs of the preliminary taggers as features. We call these methods as \"pipelinebased\" methods. In this paper, we propose a method for joint Chinese word segmentation and POS tagging with heterogeneous annotation corpora. We regard the Chinese S&T with heterogeneous corpora as two \"related\" tasks which can improve the performance of each other. Since it is impossible to establish an exact mapping between two annotations, we first automatically construct a loose and uncertain mapping the heterogeneous tagsets of CTB and PPD. Thus we can tag a sentence in one style with the help of the \"related\" information in another heterogeneous style. The proposed method can improve the performances of joint Chinese S&T on both corpora by using the shared information of each other, which is proven effective by experiments. There are three main contributions of our model: \u2022 First, we regard these two joint S&T tasks on different corpora as two related tasks which have interdependent and peer relationship. \u2022 Second, different to the pipeline-based methods, our model can be trained simultaneously on the heterogeneous corpora. Thus, it can also produce two different styles of POS tags. \u2022 Third, our model do not depend on the exactly correct mappings between the two heterogeneous tagsets. The correct mapping relations can be automatically built in training phase. The rest of the paper is organized as follows: We first introduce the related works in section 2 and describe the background of character-based method for joint Chinese S&T in section 3. Section 4 presents an automatic method to build the loose mapping function. Then we propose our method on heterogeneous corpora in 5 and 6. The experimental results are given in section 7. Finally, we conclude our work in section 8. Related Works There are some works to exploit heterogeneous annotation data for Chinese S&T. (Gao et al., 2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with different  underling annotation guidelines. They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al., 2012) . (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods regard one annotation as the main target and another annotation as the complementary/auxiliary purposes. For example, in their solution, an auxiliary tagger Tagger PPD is trained on a complementary corpus PPD, to assist the target CTB-style Tagger CTB . To refine the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1 . The related work in machine learning literature is multiple task learning (Ben-David and Schuller, 2003) , which learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005) . We also preliminarily verified that multiple task learning can improve the performance on this problem in our previous work (Zhao et al., 2013) , which is a simplified case of the work in this paper and has a relative low complexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models (Lafferty et al., 2001) , which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks (Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012) . The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated corpora, the relationship of their guidelines should be found. A mapping function should be established to represent the relationship between two different annotation guidelines. However, the exact mapping relations are hard to establish. As reported in (Sun and Wan, 2012) , there is no one-to-one mapping between their heterogeneous word classification, and the mapping between heterogeneous tags is very uncertain. Fortunately, there is a loose mapping can be found in CTB annotation guideline 1 (Xia, 2000) . Table 2 shows some  These loose mapping relations are many-to-many mapping. For example, the mapping may be \"NN/CTB\u2194{n,nt,nz}/PDD\", \"NR/CTB\u2194{nr,ns}/PDD\", \"v/PDD\u2194{VV, VA}/CTB\" and so on. We define T 1 and T 2 as the tag sets for two different annotations, and t 1 \u2208 T 1 and t 2 \u2208 T 2 are the corresponding tags in two tag sets respectively. We first establish a loose mapping function m : T 1 \u00d7 T 2 \u2192 {0, 1} between the tags of CTB and PDD. m(t 1 , t 2 ) = { 1 if t 1 and t 2 have mapping relation 0 else (1) The mapping relations are automatically build from the CTB guideline (Xia, 2000) . Due to the fact that the tag set of PPD used in the CTB guideline is just broad categories, we expand the mapping relations to include the sub categories. If a PPD's tag is involved in the mapping, all its sub categories should be involved. For example, for the mapping \"NR/CTB\u2194nr/PDD\", the relation of NR and nrf/nrg should be added in the mapping relations too (nrf/nrg belong to nr). Since we use the character-based joint S&T model, we also need to find the mapping function between the labels of characters. nese/posguide.3rd.ch.pdf In this paper, we employ the commonly used label set {B, I, E, S} for the segmentation part of cross-labels and the label of character can be in the form of {B-T}(T represents POS tag). Thus, each mapping relation t 1 \u2194 t 2 can be automatically transformed to four forms: Bt 1 \u2194B-t 2 , I-t 1 \u2194I-t 2 , E-t 1 \u2194E-t 2 and S-t 1 \u2194St 2 . (\"B-NR/CTB\u2194{B-nr,B-ns}/PPD\" for example). Beside the above transformation, we also give a slight modification to adapt the different segmentation guidelines. For instance, the person name \"\u83ab \u8a00 (Mo Yan)\" is tagged as \"B-NR, E-NR\" in CTB but \"S-nrf, S-nrg\" in PPD. So, some special mappings may need to be added like \"B-NR/CTB\u2194S-nrf/PPD\", \"E-NR/CTB\u2194{Snrg, E-nrg}/PPD\", \"M-NR/CTB\u2194{B-nrg, M-nrg}/PPD\" and so on. Although these special mappings are also established automatically with an exhaustive solution. In fact, we give segmentation alignment only to proper names due to the limitation of computing ability. Thus, we can easily build the loose bidirectional mapping function m for the labels of characters. An illustration of our construction flowchart is shown in Figure 2 . Finally, total 524 mappings relationships are established. Joint Chinese S&T with Heterogeneous Data with Multiple Task Learning Inspired by the multiple task learning (Ben-David and Schuller, 2003), we can regard the joint Chinese S&T with heterogeneous data as two \"related\" tasks, which can improve the performance of each other simultaneously with shared information. Sequence Labeling Model We first introduce the commonly used sequence labeling model in character-based joint Chinese S&T. Sequence labeling is the task of assigning labels y = y 1 , . . . , y n (y i \u2208 Y) to an input sequence x = x 1 , . . . , x n . Y is the set of labels. where w is the parameter of score function S(\u2022). The feature vector \u03a6(x, y) consists of lots of overlapping features, which is the chief benefit of discriminative model. Different algorithms vary in the definition of S(\u2022) and the corresponding objective function. S(\u2022) is usually defined as linear or exponential family function. For first-order sequence labeling, the feature can be denoted as \u03d5 k (x, y i\u22121:i ), where i stands for the position in the sequence and k stands for the number of feature templates. For the linear classifier, the score function can be rewritten in detail as \u0177 = arg max y L \u2211 i=1 (\u27e8u, f(x, y i )\u27e9 + \u27e8v, g(x, y i\u22121:i )\u27e9) , (3) where y i:j denotes label subsequence y i y i+1 \u2022 \u2022 \u2022 y j ; f and g denote the state and transition feature vectors respectively, u and v are their corresponding weight vectors; L is the length of x. The Proposed Model Different to the single task learning, the heterogeneous data have two sets of labels Y and Z. The heterogeneous datasets D s and D s consist of {x i , y i }(i = 0, \u2022 \u2022 \u2022 , m) and {x i , z i }(i = 0, \u2022 \u2022 \u2022 , n) respectively. For a sequence x = x 1 , . . . , x L with length L. , there may have two output sequence labels y = y 1 , . . . , y L and z = z 1 , . . . , z L , where y i \u2208 Y and z i \u2208 Z. We rewrite the loose mapping function m between two label sets into the following forms, \u03c6(y) = {z| m(y, z) = 1}, (4) \u03c6(z) = {y| m(y, z) = 1}, (5) where \u03c6(z) \u2282 Y and \u03c6(y) \u2282 Z are the subsets of Y and Z. Give a label y(or z) in an annotation, the loose mapping function \u03c6 returns the corresponding mapping label set in another heterogeneous annotation. Our model for heterogeneous sequence labeling can be write as \u0177 = arg max y,y i \u2208Y L \u2211 i=1 ( \u27e8u, f(x, y i )\u27e9 + \u27e8s, \u2211 z\u2208\u03c6(y i ) h(x, z)\u27e9 + \u27e8v 1 , g 1 (x, y i\u22121:i )\u27e9 + \u27e8v 2 , \u2211 z i\u22121 \u2208\u03c6(y i\u22121 ) z i \u2208\u03c6(y i ) g 2 (x, z i\u22121:i )\u27e9 ) , (6) and \u1e91 = arg max z,z i \u2208Z L \u2211 i=1 ( \u27e8u, \u2211 y\u2208\u03c6(z i ) f(x, y)\u27e9+ \u27e8s, h(x, z i )\u27e9 + \u27e8v 1 , \u2211 y i\u22121 \u2208\u03c6(z i\u22121 ) y i \u2208\u03c6(z i ) g 1 (x, y i\u22121:i )\u27e9 + \u27e8v 2 , g 2 (x, z i\u22121:i )\u27e9 ) , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq.( 6 ) and ( 7 ), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself.  The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables (Liang et al., 2006) (Yu and Joachims, 2009) . Most methods for structured learning with latent variables have not expand all possible mappings. In this paper, we also only expand the mapping that with highest according to the current model. Our model is shown in Figure 3 and the flowchart is shown in Algorithm 1. If given the output type of label T , we only consider the labels in T to initialize the Viterbi matrix, and the score of each is determined by all the involved heterogeneous labels according to the loose mapping function. input : character sequence x 1:L loose mapping function \u03c6 output type: T (T \u2208 {T y , T z }) output: label sequence ls if T == T y then calculate ls using Eq. ( 6 ); else if T == T z then calculate ls using Eq. ( 7 ) ; else return null; end return ls Algorithm 1: Flowchart of the Tagging process of the proposed model Training We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002) , the average strategy is used to avoid the overfitting problem. For the sake of simplicity, we merge the Eq.( 6 ) and ( 7 ) into a unified formula. Given a sequence x and the expect type of tags T , the merged model is \u0177 = arg max y t(y)=T \u27e8w, \u2211 z\u2208\u03c8(y) \u03a6(x, z)\u27e9, ( 8 ) where t(y) is a function to judge the type of output tags; \u03c8(y) represents the set {\u03c6(y 1 ) \u2297 \u03c6(y 2 ) \u2297 \u2022 \u2022 \u2022 \u2297 \u03c6(y L )} \u222a {y}, where \u2297 means Cartesian product; w = (u T , s T , v T 1 , v T 2 ) T and \u03a6 = (f T , h T , g T 1 , g T 2 ) T . We redefine the score function as S(w, x, y) = \u27e8w, \u2211 z\u2208\u03c8(y) \u03a6(x, z)\u27e9. (9) Thus, we rewrite the model into a unified formula \u0177 = arg max y t(y)=T S(w, x, y). (10) Given an example (x, y), \u0177 is denoted as the incorrect label sequence with the highest score \u0177 = arg max \u0233\u0338 =y t(\u0233)=t(y) S(w, x, \u0233). (11) The margin \u03b3(w; (x, y)) is defined as \u03b3(w; (x, y)) = S(w, x, y) \u2212 S(w, x, \u0177). ( 12 ) Thus, we calculate the hinge loss \u2113(w; (x, y)), (abbreviated as \u2113 w ) by \u2113 w = { 0, \u03b3(w; (x, y)) > 1 1 \u2212 \u03b3(w; (x, y)), otherwise (13 ) In round k, the new weight vector w k+1 is calculated by w k+1 = arg min w 1 2 ||w \u2212 w k || 2 + C \u2022 \u03be, s.t. \u2113(w; (x k , y k )) <= \u03be and \u03be >= 0 (14) where \u03be is a non-negative slack variable, and C is a positive parameter which controls the influence of the slack term on the objective function. Following the derivation in PA (Crammer et al., 2006) , we can get the update rule, w k+1 = w k + \u03c4 k e k , ( 15 ) where e k = \u2211 z\u2208\u03c8(y k ) \u03a6(x k , z) \u2212 \u2211 z\u2208\u03c8( \u0177k ) \u03a6(x k , z), \u03c4 k = min(C, \u2113 w k \u2225e k \u2225 2 ). As we can see from the Eq. ( 15 ), when we update the weight vector, the update information includes not only the features extracted from current input, but also that extracted from the loose mapping sequence of input. For each feature, the weights of its corresponding related features derived from the loose mapping function will be updated with the same magnitude as well as itself. Our method regards two annotations to be interdependence and peer relationship. Therefore, the two heterogeneous annotated corpora can be simultaneously used as the input of our training algorithm. Because of the tagging and training algorithm, the weights and tags of two corpora can be used separately with the only dependent part built by the loose mapping function. Our training algorithm based on PA is shown in Algorithm 2. Analysis Although our mapping function between two heterogeneous annotations is loose and uncertain, our online training method can automatically increase the relative weights of features from the beneficial mapping relations and decrease the relative weights of features from the unprofitable mapping relations. Consider an illustrative loose mapping relation \"NN/CTB\u2194n,nt,nz/PDD\". For an input sequence x and PDD-style output is expected. If the algorithm tagging a character as \"n/PDD\"(with help of the weight of \"NN/CTB\") and the right tag isn't one of input : mixed heterogeneous datasets: (x i , y i ), i = 1, \u2022 \u2022 \u2022 , N ; parameters: C, K; loose mapping function: \u03c6 ; output: w K Initialize: wTemp \u2190 0, w \u2190 0; for k = 0 \u2022 \u2022 \u2022 K \u2212 1 do for i = 1 \u2022 \u2022 \u2022 N do receive an example (x i , y i ); predict: \u0177i with Eq.( 11 ); if hinge loss \u2113 w > 0 then update w with Eq. ( 15 ); end end wTemp = wTemp + w ; end w K = wTemp/K ; Algorithm 2: Training Algorithm \"n,nt,nz/PDD\", the weight of \"NN/CTB\" will also be decreased, which is reasonable since it is beneficial to distinguish the right tag. And if the right tag is one of \"n,nt,nz/PDD\" but not \"n/PDD\" (for example, \"nt/PDD\"), which means it is a \"NN/CTB\", the weight of \"NN/CTB\" will remain unchanged according to the algorithm (updating \"n/PDD\" changes the \"NN/CTB\", but updating \"nt/PDD\" changes it back). Therefore, after multiple iterations, useful features derived from the mapping function are typically receive more updates, which take relatively more responsibility for correct prediction. The final model has good parameter estimates for the shared information. We implement our method based on Fu-danNLP (Qiu et al., 2013) . Experiments Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD) in our experiments. Dataset Partition Sections Words CTB- CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. \u2022 One is the partition criterion used in (Jin and Chen, 2008; Jiang et al., 2009; Sun and Wan, 2012) for CTB 5.0. \u2022 Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008). PPD Dataset For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoff 2008. The details of all datasets are shown in Table 3 . Our experiment on these datasets may lead to a fair comparison of our system and the related works. Setting We conduct two experiments on CTB-5 + PPD and CTB-S + PPD respectively. The form of feature templates we used is shown in Table 7 .2, where C represents a Chinese character, and T represents the characterbased tag. The subscript i indicates its position related to the current character. Our method can be easily combined with some other complicated models, but we only use the simple one for the purpose of observing the C i , T 0 (i = \u22122, \u22121, 0, 1, 2) C i , C i+1 , T 0 (i = \u22121, 0) T \u22121 , T 0 Table 4: Feature Templates sole influence of our unified model. The parameter C is tested on develop dataset, and we found that it just impact the speed of convergence and have no effect on the accuracy. Moreover, since we use the averaged strategy, we wish more iterations to avoid overfitting and set a small value 0.01 to it. The maximum number of iterations K is 50. The F 1 score is used for evaluation, which is the harmonic mean of precision P (percentage of predict phrases that exactly match the reference phrases) and recall R (percentage of reference phrases that returned by system). Evaluation on CTB-5 + PPD The experiment results on the heterogeneous corpora CTB-5 + PPD are shown in Table 5 . Our method obtains an error reductions of 24.08% and 90.8% over the baseline on CTB-5 and PDD respectively. Our method also gives better performance than the pipeline-based methods on heterogeneous corpora, such as (Jiang et al., 2009) and (Sun and Wan, 2012) . The reason is that our model can utilize the information of both corpora effectively, which can boost the performance of each other. Although the loose mapping function are bidirectional between two annotation tagsets, we may also use unidirectional mapping. Therefore, we also evaluate the performance when we use unidirectional mapping. We just use the mapping function \u03c8 PDD\u2192CTB , which means we obtain the PDD-style output without the information from CTB in tagging stage. Thus, in training stage, there are no updates for the weights of CTB-features for the instances from PDD corpus, while instances from CTB corpus can result to updates for PDD-features. Surprisedly, we find that the one-way mapping can also improve the performances of both corpora. The results are shown in Evaluation on CTB-S + PPD Table 6 shows the experiment results on the heterogeneous corpora CTB-S + PPD. Our method obtains an error reductions of 7.41% and 10.59% over the baseline on CTB-S and PDD respectively. Analysis As we can see from the above experiments, our proposed unified model can improve the performances of the two heterogeneous corpora with unidirectional or bidirectional loose mapping functions. Different to the pipeline-based methods, our model can use the shared information between two heterogeneous POS taggers. Although the mapping function is loose and uncertain, it is still can boost the performances. The features derived from the wrong mapping function take relatively less responsibility for prediction after multiple updates of their weights in training stage. The final model has good parameter estimates for the shared information. Another phenomenon is that the performance of one corpus can gains when the data size of another corpus increases. In our two experiments, the training set's size of CTB-S is larger than CTB-5, so the performance of PDD is higher in latter experiment. Conclusion We proposed a method for joint Chinese word segmentation and POS tagging with heterogeneous annotation data. Different to the previous pipeline-based works, our model is learned on heterogeneous annotation data simultaneously. Our method also does not require the exact corresponding relation between the standards of heterogeneous annotations. The experimental results show our method leads to a significant improvement with heterogeneous annotations over the best performance for this task. Although our work is for a specific task on joint Chinese word segmentation and POS, the key idea to leverage heterogeneous annotations is very general and applicable to other NLP tasks. In the future, we will continue to refine the proposed model in two ways: (1) We wish to use the unsupervised method to extract the loose mapping relation between the different annotation standards, which is useful to the corpora without loose mapping guideline. (2) We will analyze the shared information (weights of the features derived from the tags which have the mapping relation) in detail and propose a more effective model. Besides, we would also like to investigate for other NLP tasks which have different annotation-style corpora. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. This work was funded by NSFC (No.61003091)",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions funding from NSFC (No.61003091) at the end, which stands for the National Natural Science Foundation of China, a research agency. There is no mention of funding from defense, corporate entities, foundations, or an indication that there were no other funding sources.",
    "abstract": "Chinese word segmentation and part-ofspeech tagging (S&T) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T. In this paper, we propose a unified model for Chinese S&T with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD). Then we regard the Chinese S&T with heterogeneous corpora as two \"related\" tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": 18,
    "year": 2013,
    "month": "October",
    "title": "Joint {C}hinese Word Segmentation and {POS} Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning",
    "values": {
        "building on past work": " Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.  In this paper, we propose a unified model for Chinese S&T with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD).  These methods usually have a two-step process. The first step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the final taggers by using the outputs of the preliminary taggers as features.  They also reported that there is no one-to-one mapping between the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain.  ",
        "novelty": "Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T. In this paper, we propose a unified model for Chinese S&T with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD). Then we regard the Chinese S&T with heterogeneous corpora as two \"related\" tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods.",
        "performance": "The POS tagsets are also significantly different. For example, PDD gives diverse tags \"n\" and \"vn\" for the noun, while CTB just gives \"NN\". For proper names, they may be tagged as \"nr\", \"ns\", etc in PDD, while they are just tagged as \"NR\" in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The first step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the final taggers by using the outputs of the preliminary taggers as features. We call these methods as \"pipelinebased\" methods. For example, in their solution, an auxiliary tagger Tagger PPD is trained on a complementary corpus PPD, to assist the target CTB-style Tagger CTB . To refine the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1 . The correct mapping relations can be automatically built in training phase. The related work in machine learning literature is multiple task learning (Ben-David and Schuller, 2003) , which learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005) . We also preliminarily verified that multiple task learning can improve the performance on this problem in our previous work (Zhao et al., 2013) , which is a simplified case of the work in this paper and has a relative low complexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods. Introduction Currently, most of statistical natural language processing (NLP) systems rely heavily on manually annotated resources to train their statistical models. The more of the data scale, the better the performance will be. However, the costs are extremely expensive to build the large scale resources for some NLP tasks. Even worse, the existing resources are often incompatible even for a same task and the annotation guidelines are usually different for different projects, since there are many underlying linguistic theories which explain the same language with different perspectives. As a result, there often exist multiple heterogeneous annotated corpora for a same task with vastly different and incompatible annotation philosophies. These heterogeneous resources are waste on some level if we cannot fully exploit them. However, though most of statistical NLP methods are not bound to specific annotation standards, almost all of them cannot deal simultaneously with the training data with different and incompatible annotation. The co-existence of heterogeneous annotation data therefore presents a new challenge to utilize these resources. We define T 1 and T 2 as the tag sets for two different annotations, and t 1 \u2208 T 1 and t 2 \u2208 T 2 are the corresponding tags in two tag sets respectively. We first establish a loose mapping function m : T 1 \u00d7 T 2 \u2192 {0, 1} between the tags of CTB and PDD. m(t 1 , t 2 ) = { 1 if t 1 and t 2 have mapping relation 0 else (1) The mapping relations are automatically build from the CTB guideline (Xia, 2000) . Due to the fact that the tag set of PPD used in the CTB guideline is just broad categories, we expand the mapping relations to include the sub categories. If a PPD's tag is involved in the mapping, all its sub categories should be involved. For example, for the mapping \"NR/CTB\u2194nr/PDD\", the relation of NR and nrf/nrg should be added in the mapping relations too (nrf/nrg belong to nr). Since we use the character-based joint S&T model, we also need to find the mapping function between the labels of characters. An illustration of our construction flowchart is shown in Figure 2 . Finally, total 524 mappings relationships are established. For instance, the person name \"\u83ab \u8a00 (Mo Yan)\" is tagged as \"B-NR, E-NR\" in CTB but \"S-nrf, S-nrg\" in PPD. So, some special mappings may need to be added like \"B-NR/CTB\u2194S-nrf/PPD\", \"E-NR/CTB\u2194{Snrg, E-nrg}/PPD\", \"M-NR/CTB\u2194{B-nrg, M-nrg}/PPD\" and so on. Although these special mappings are also established automatically with an exhaustive solution. In fact, we give segmentation alignment only to proper names due to the limitation of computing ability. Thus, we can easily build the loose bidirectional mapping function m for the labels of characters. Sequence labeling is the task of assigning labels y = y 1 , . . . , y n (y i \u2208 Y) to an input sequence x = x 1 , . . . , x n . Y is the set of labels. where w is the parameter of score function S(\u2022). The feature vector \u03a6(x, y) consists of lots of overlapping features, which is the chief benefit of discriminative model. Different algorithms vary in the definition of S(\u2022) and the corresponding objective function. S(\u2022) is usually defined as linear or exponential family function. For first-order sequence labeling, the feature can be denoted as \u03d5 k (x, y i\u22121:i ), where i stands for the position in the sequence and k stands for the number of feature templates. For the linear classifier, the score function can be rewritten in detail as \u0177 = arg max y L \u2211 i=1 (\u27e8u, f(x, y i )\u27e9 + \u27e8v, g(x, y i\u22121:i )\u27e9) , (3) where y i:j denotes label subsequence y i y i+1 \u2022 \u2022 \u2022 y j ; f and g denote the state and transition feature vectors respectively, u and v are their corresponding weight vectors; L is the length of x. Our model for heterogeneous sequence labeling can be write as \u0177 = arg max y,y i \u2208Y L \u2211 i=1 ( \u27e8u, f(x, y i )\u27e9 + \u27e8s, \u2211 z\u2208\u03c6(y i ) h(x, z)\u27e9 + \u27e8v 1 , g 1 (x, y i\u22121:i )\u27e9 + \u27e8v 2 , \u2211 z i\u22121 \u2208\u03c6(y i\u22121 ) z i \u2208\u03c6(y i ) g 2 (x, z i\u22121:i )\u27e9 ) , (6) and \u1e91 = arg max z,z i \u2208Z L \u2211 i=1 ( \u27e8u, \u2211 y\u2208\u03c6(z i ) f(x, y)\u27e9+ \u27e8s, h(x, z i )\u27e9 + \u27e8v 1 , \u2211 y i\u22121 \u2208\u03c6(z i\u22121 ) y i \u2208\u03c6(z i ) g 1 (x, y i\u22121:i )\u27e9 + \u27e8v 2 , g 2 (x, z i\u22121:i )\u27e9 ) , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq. ( 6 ) and ( 7 ), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself. The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables (Liang et al., 2006) (Yu and Joachims, 2009) . Most methods for structured learning with latent variables have not expand all possible mappings. In this paper, we also only expand the mapping that with highest according to the current model. Our model is shown in Figure 3 and the flowchart is shown in Algorithm 1. If given the output type of label T , we only consider the labels in T to initialize the Viterbi matrix, and the score of each is determined by all the involved heterogeneous labels according to the loose mapping function. input : character sequence x 1:L loose mapping function \u03c6 output type: T (T \u2208 {T y , T z }) output: label sequence ls if T == T y then calculate ls using Eq. ( 6 ); else if T == T z then"
    }
}