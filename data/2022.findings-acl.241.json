{
    "article": "With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning method comprised of three auxiliary tasks to enhance the understanding of dialogue history, emotion and semantic meaning of stickers. Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines. Ablation study further verifies the effectiveness of each auxiliary task. Our code is available at https://github.com/ nonstopfor/Sticker-Selection. Introduction With the development of mobile messaging apps (e.g., WhatsApp and Messenger), visual content is getting more and more frequently used in our daily conversation, such as emojis and stickers. Compared with emojis, stickers are larger images consisting of drawing characters, symbolic icons, and text titles, and are hence more expressive and versatile (Konrad et al., 2020) . Users send stickers along with text to show intimacy, express strong emotion, and experience the enjoyment of creativity (Tang and Hew, 2019) . Despite the importance of stickers in daily communication, selecting stickers in open-domain dialogue hasn't been widely explored. In this paper, we address the task of selecting an appropriate sticker from a candidate set for an open-domain multi-turn dialogue. This task is a typical setting for various applications, e.g., automatically recommending stickers in messaging apps and building * Work done during internship at WeChat AI. Given a dialogue history, the model needs to add a sticker to the last textual message which is the most appropriate one among a collection of candidate stickers (the one marked in the red rectangle). The words below in red denote the emotion or meaning of each sticker. more interesting and human-like chatbots which could respond with stickers. As shown in Figure 1 , this task requires an understanding of dialogue context, emotion and semantic meaning of stickers, and a jointly modeling ability for the multimodal information. Only a few previous works have explored this task (Gao et al., 2020; Wang and Jurgens, 2021) . However, existing models are only trained on an end-to-end matching objective and lacks finer-grained supervision signals which could guide models to understand multimodal information better. Considering the challenges of this task and the shortcomings of previous work, we propose a novel multitask learning method to improve sticker selection in open-domain multi-turn dialogue. We design three auxiliary tasks: 1) masked context prediction, which uses multimodal context to predict masked tokens in the dialogue history, aiming to understand the dialogue in the presence of the sticker; 2) sticker emotion classification, which utilizes the sticker's contextualized representation to predict its emotion, aiming to improve the model's understanding of sticker emotion; 3) sticker semantic prediction, which explicitly instills semantic understanding of stickers by training the model to reconstruct a sticker's semantic label based on the multimodal inputs. Moreover, all these tasks help improve our model's joint modeling capability, as both our model architecture and task design require multimodal inputs and deep interactions between them. We evaluate the performance of our method on a recently proposed and challenging dataset. Extensive experiments show that our multitask method achieves state-of-the-art performance. There are two contributions of this paper: \u2022 We propose a multitask learning method to help select appropriate stickers in opendomain multi-turn dialogue. \u2022 Experiment results on a challenging dataset demonstrate the effectiveness of each auxiliary task and combining all the tasks achieves state-of-the-art performance. Related Work Sticker selection. Previous works proposed to recommend emojis in dialogue systems based on textual or multimodal context (Barbieri et al., 2018; Xie et al., 2016; Barbieri et al., 2017) . However, emojis are limited in variety and are much less expressive than stickers. Laddha et al. (2020) retrieved stickers for generated text utterances by simply matching the text tags of stickers. Several works have proposed improved matching methods for stickers. Gao et al. (2020) utilized co-attention to capture the interaction between a sticker and each utterance, and used a fusion network to combine the features. Wang and Jurgens (2021) followed the matching framework of CLIP (Radford et al., 2021) and designed a multimodal encoder for animated GIFs. Fei et al. (2021) proposed to generate special sticker tokens along with text utterances using one single GPT (Wang et al., 2020) for emotion prediction and retrieval of stickers. However, existing models are only trained on an end-to-end matching objective that implicitly guides the models to understand multimodal information. In our work, we design finer-grained auxiliary tasks that instill knowledge of stickers and their contextualized usage in a more efficient way. Visual Dialogue. Visual dialogue is a task to answer questions about the factual content of the real-world image (Liang et al., 2021; Das et al., 2017a,b) . In contrast, selecting appropriate stickers in open-domain dialogue requires understanding sentiment and semantic expression of usergenerated, artistic style images. 3 Method Task Definition We assume there is a multi-turn dialogue context C = {u 1 , ..., u N }, and a candidate sticker set S = {s 1 , ..., s M }, where u i represents the i-th utterance in the dialogue, and s i represents the i-th candidate sticker. N is the number of utterances in the dialogue and M is the number of candidate stickers. In this work, we suppose that there is only one appropriate sticker s * \u2208 S, and s * and u N belong to the same speaker. The goal is to train a model that can select the right sticker s * among all candidates S given the dialogue history C. Method Overview An overview of the design of our training tasks is shown in Figure 2 . Our main task is to decide whether the candidate sticker is appropriate given the dialogue context. To accomplish this task, we concatenate the embedded dialogue context and the sticker embedding as inputs to BERT. Then we apply a binary classification layer on top of the hidden state of the [CLS] token. In order to enhance the model's ability to understand the multimodal input, we design three auxiliary tasks: 1) masked context prediction, which improves the model's understanding of dialogue context; 2) sticker emotion classification, which aims to make the model better understand sticker's emotion; 3) sticker semantic prediction, which instills semantic information of stickers to the model. Next, we will introduce our three auxiliary tasks in detail. Task 1: Masked Context Prediction The masked context prediction task follows the masked language modeling (MLM) task in BERT (Devlin et al., 2019) . One difference is that we additionally append the embedding of the appropriate sticker to the input embeddings. In this way, the model can learn to utilize stickers for dialogue reconstruction, and thus the interaction between the two modalities is enhanced. The loss for this task is denoted as L ctx , and takes the same form of cross-entropy loss as in the original MLM task. Task 2: Sticker Emotion Classification In the dataset we used, stickers are annotated with one context-dependent emotion, which means one sticker could have different emotions in different dialogue contexts. Therefore, we design a sticker emotion classification task to enable the model to utilize the text and sticker information simultaneously for understanding sticker emotion. Specifically, we take the hidden state corresponding to the sticker and apply a softmax layer with crossentropy loss on top of it for emotion classification. The loss for this task is denoted as L emo . Task 3: Sticker Semantic Prediction Task 1 and Task 2 emphasize learning the implicit meaning of stickers and their correlation with dialogue text. However, many stickers express a clear intention that indicates their proper usage context, e.g., greetings and declines. We believe empowering our model to predict and utilize the semantic meaning of stickers is beneficial for our task. Hence, we further design a semantic label prediction task. We modify our model's inputs by inserting a fixed-length sequence of [MASK] tokens after the dialogue. The model is trained to recover the label text from the hidden states of the [MASK] tokens. The loss is formulated as the sum of cross-entropy loss for each token in label and is denoted as L sem . Since the dataset we used has no ground truth semantic labels for stickers, we take the textual information recognized by an OCR tool as semantic labels for stickers. Note that L sem is only applied for those stickers with text recognized. Total Loss Besides the above three auxiliary tasks, our main task is a binary classification of whether a candidate sticker is appropriate given the dialogue context. We take all dialogue-sticker pairs in the dataset as positive samples and randomly sample stickers to create an equal number of negative samples. The cross-entropy loss is denoted as L main . Our final loss is a combination of the four loss: L = L main + \u03b1L ctx + \u03b2L emo + \u03b3L sem (1) where \u03b1, \u03b2, \u03b3 are manually tuned hyperparameters. Experiments Dataset We use the Chinese version of the MOD dataset from DSTC10-Track1 1 . The dataset is grounded in a dialogue scenario and contains various stickers with contextualized emotion annotation. We split each dialogue into several samples, each containing a text sequence of dialogue history and an accompanying sticker. Note that this dataset is revised from the unpublished one used in Fei et al. (2021) . Baselines We compare our model with the following baselines from recent related work: 1) SRS (Gao et al., 2020) , which encodes dialogue history and candidate sticker separately, and then employs a deep interaction network and a fusion network to score each candidate sticker; 2) MOD-GPT (Fei et al., 2021) , which uses one single GPT to generate response text and match sticker; 3) CLIP, which finetunes pretrained CLIP (Radford et al., 2021) for sticker selection using the same contrastive loss. Results and Analysis The result is shown in Table 1 . Our full model (MMBERT+ctx+emo+sem) outperforms all baselines on two test sets, and achieves the best performance in almost all settings. As expected, all the results get worse on the hard test set and when selecting one amongst all stickers. As only one R10@1 R10@2 R10@5 MRR10 R ALL @1 R ALL @2 R ALL @5 MRR ALL or all available stickers respectively. R@k is the recall rate of top-k predicted stickers and MRR the Mean Reciprocal Rank of ground truth stickers. The abbreviations ctx, emo and sem correspond to the auxiliary task 1, 2 and 3 respectively in Section 3. A paired t-test is conducted between the full model (MMBERT+ctx+emo+sem) and CLIP ( * : p < 0.05, * * : p < 0.01). Bad guy, definitely. out of the numerous and various online stickers is considered correct, the task is inherently challenging. We find that CLIP is a strong baseline due to its better generalization ability on the hard set, compared with our base model which has no auxiliary task (MMBERT). This may be because CLIP is pretrained on a large number of imagetext pairs. However, with multitask learning, our full model outperforms CLIP, although BERT has never seen images during pretraining. Thus, we conclude that our multitask learning method can improve sticker selection by explicitly guiding the model to understand multimodal information. We also perform an ablation study to verify the effect of each auxiliary task. A clear trend emerges that the performance improves as each auxiliary training task is added to MMBERT, verifying the efficacy of our task design. One exception is that MMBERT+ctx+emo performs slightly better than our full model in terms of R 10 @2, R 10 @5, and MRR 10 . However, the inconsistency disappears when considering all stickers as candidates. Furthermore, our full model performs significantly better on the hard test set which contains unseen stickers. Hence, we conclude that introducing semantic information improves the model's generalization ability. We also find that our full model achieves 60% accuracy on the validation set for the auxiliary Figure 6 : A case in which our model's prediction is not the same as the answer but also appropriate. The leftmost sticker is selected by our model among the four candidate stickers. The appropriate sticker is marked with a red rectangle. The red words explain the stickers' emotions or meanings. sticker emotion classification task with 52 emotion labels in total, which is reasonable and confirms our model can learn from the auxiliary tasks. We visualize the saliency of different words in the dialogue history in Figure 3 , which shows that the more relevant words (e.g., guessed, good and modest) in the dialogue history contribute more to our model's prediction. Notably, our model could attend to some distant words (e.g., good), not just the words inside the previous utterance. We also analyze the prediction diversity of our full model. As shown in Figure 4 , the predictions of our model are diverse in general, covering almost all stickers in the whole candidate set. We note that a few stickers are predicted significantly more times than other stickers, which is because they appear much more frequently than other stickers in the training set. We leave addressing the imbalance problem of the training set as our future work. Case Study We present a successful case in Figure 1 , where the ground truth sticker has no OCR information, making it challenging for the model to understand its semantic meaning. Moreover, the model needs to understand that the dialogue is in a delighted context, and the stickers' emotions and meanings in order to distinguish the most appropriate sticker from the others. This case suggests our model has a good understanding of dialogue history and sticker emotion and semantic meaning with the help of auxiliary tasks. We show a failing case of our model in Figure 5 . In this case, the appropriate sticker never appears in the training set. Considering the hard test set is more challenging than the easy test set, improving the generalization ability of our model is thus an important direction of future work. The same is true for baselines. In the dataset we used, only one sticker is considered correct. However, we observe cases where the model's selection is not the same as the answer but is also appropriate. An example is shown in Figure 6 . Therefore, the results in Table 1 indicate a lower bound performance and our model may perform better in practice. Conclusion In this paper, we address the challenging task of selecting appropriate stickers in open-domain multiturn dialogue. We propose a multitask learning method with three auxiliary tasks to enhance the understanding of dialogues and stickers. Experiments show that our model outperforms strong baselines, confirming the effectiveness of our multitask learning method for sticker selection. Although our experiments are conducted on a Chinese dataset, our methods are expected to work for other languages. A Dataset Details Statistics of the dataset are shown in Table 3 . There are 307 stickers in total and 228 out of them have textual information extracted by OCR. For stickers without emotion labels or semantic labels, we simply ignore the emotion classification loss or the semantic prediction loss. A better way to deal with the missing labels is left as future work. For each dialogue sample, we ignore stickers in the middle of the dialogue history, as we found in preliminary experiments that removing them has no significant impact on the performance. B Implementation Details For all the models implemented by ourselves in our experiments, we set the batch size to 8 and use AdamW optimizer with cosine scheduler. For the CLIP baseline, as there is no available CLIP R10@1 R10@2 R10@5 MRR10 R ALL @1 R ALL @2 R ALL @5 MRR ALL  model especially pretrained in Chinese, we use a multilingual version adapted via knowledge distillation (Reimers and Gurevych, 2020) 3 . We cut the dialogue history to take only the last sentence in order to fit the length limit of CLIP's text encoder. We also tried to use the last two or more sentences, but found that the performance decreased. All BERT-based models and MOD-GPT use the image encoder in the CLIP baseline. We set the CLIP image encoder's learning rate to 5e-7 and the text encoder's learning rate to 9e-6. For BERTbased models, we set the learning rate to 9e-6 and 3 https://www.sbert.net/docs/ pretrained_models.html#image-text-models fix the image encoder following (Fei et al., 2021) . The maximum epoch is set to 10. For the total loss, \u03b1 is set to 0.05, \u03b2 is set to 0.2 and \u03b3 is set to 0.1. All the hyperparameters are selected based on the validation set. The maximum training time for one epoch is about 5 hours on one single V100 GPU. C Effect of Semantic Information In our full model, we also added semantic labels to other tasks' inputs, i.e., the main task of contextsticker matching, the masked context prediction task and the sticker emotion classification task. It raises an interesting question of how the performance will change if we remove this information. The result is shown in Table 2 . As we can see, the sticker semantic prediction task is more beneficial for the easy test set, while adding OCR information to other tasks is more beneficial for the hard test set. We conjecture that because of the relatively small number of stickers (less than 300), it could be easier for the model to memorize the meaning of all stickers in the dataset, which potentially damages the model's generalization ability on unseen stickers in the hard test set. Adding OCR information for other tasks greatly alleviates this phenomenon because it could offer semantic labels for unseen stickers and enhance the model's generalization ability. We also tried to enhance the model's generalization ability by incorporating additional stickerdescription pairs from another source into the 4 : Performance of our full model and CLIP on the divided dataset. The three values divided by / correspond to the performance on the sub dataset where stickers have text, the performance on the sub dataset where stickers don't have text and their difference. All the numbers are scaled by 100. The easy test set contains only the same stickers seen during training, while the hard test set has unseen stickers. R@k is the recall rate of top-k predicted stickers and MRR the Mean Reciprocal Rank of ground truth stickers. The abbreviations ctx, emo and sem correspond to the auxiliary task 1, 2 and 3 respectively in Section 3. sticker semantic prediction task. As Table 2 shows, this method could increase the performance on the hard test set as expected, but the performance on the easy test set drops significantly, which may be attributed to the distribution difference between the additional data and our original data. R ALL @1 R ALL @2 R ALL @ D Analysis of Sensitivity to the Text in Stickers To explore whether stickers have text or not could affect the model's performance, we split each test set into two parts, i.e., stickers with recognized text labels versus those without text labels. The number of samples in each part is 2164 and 1051 for the easy set, and 4429 and 2599 for the hard set. We compare the performance of our full model with that of CLIP on the divided test sets in Table 4 . To avoid randomness in candidate set construction, we only compare the two models with the whole candidate set. In general, our full model and CLIP work better when the stickers have text, which suggests that the text in the sticker could help the model better understand the sticker. 4 However, our model is less sensitive to whether stickers have text or not according to the smaller difference value compared with CLIP. This implies our model is more robust to different kinds of candidate stickers.",
    "abstract": "With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning method comprised of three auxiliary tasks to enhance the understanding of dialogue history, emotion and semantic meaning of stickers. Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines. Ablation study further verifies the effectiveness of each auxiliary task. Our code is available at https://github.com/ nonstopfor/Sticker-Selection.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Selecting Stickers in Open-Domain Dialogue through Multitask Learning"
}