{
    "article": "When strong partial-input baselines reveal artifacts in crowdsourced NLI datasets, the performance of full-input models trained on such datasets is often dismissed as reliance on spurious correlations. We investigate whether stateof-the-art NLI models are capable of overriding default inferences made by a partial-input baseline. We introduce an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model's sensitivity to edited contexts. Our results indicate that NLI models are still capable of learning to condition on context-a necessary component of inferential reasoning-despite being trained on artifact-ridden datasets. Introduction Natural language inference (NLI) is integral to building systems that are broadly capable of language understanding (White et al., 2017) . In a traditional NLI setup, models are provided with a premise as context and a corresponding hypothesis. They must then determine whether the premise entails, contradicts, or is neutral in relation to the hypothesis (Giampiccolo et al., 2007) . Researchers have shown that many NLI datasets contain statistical biases, or \"annotation artifacts\" (Gururangan et al., 2018; Herlihy and Rudinger, 2021) that systems leverage to correctly predict entailment. To diagnose such artifacts in datasets and provide a stronger alternative to majority-class baselines, Poliak et al. (2018) introduced partialinput baselines, a setting in which models are provided parts of NLI instances to predict an entailment relation. Poliak et al. (2018) , Gururangan et al. (2018) , and others posit that datasets containing artifacts may in turn produce models that are incapable of learning to perform true reasoning. In this paper, we re-examine such dismissal of the reasoning capabilities of models trained on datasets containing artifacts. While a competi-Figure 1 : Given NLI and \u03b4-NLI instances that partialinput baselines correctly label, we edit their context to induce a different gold label. We use the edited instances to probe a full-input model's ability to leverage context to deviate from the predicted partial-input label. on the inferential capabilities of full-input models trained on the same data, it is premature to conclude that full-input models are not capable of such reasoning at all. As a thought experiment, we imagine training a human to learn a task from a set of examples containing artifacts that allow them to cheat. If we force them to learn to perform the task by removing relevant context, they must rely on the artifacts to accurately perform the task. However, provided all the data, they may still learn to perform the task the \"right way\". Through two sets of experiments, we investigate whether NLI models are able to condition on the full input despite learning from artifact-ridden datasets. Section 3 investigates whether additional context strengthens a full-input model's confidence in the correct label, despite a partial-input model's correct prediction. In Section 4, we introduce an evaluation set to probe whether full-input models are sensitive to changes in context that flip the gold label in examples containing artifacts. We make our evaluation set and code publicly available. 1  Our results indicate that full-input models are still successfully learning to utilize context, overriding strong signal in the partial input. With this finding, we argue that while partial-input baselines are still a useful tool, they do not license the conclusion that models trained on datasets with artifacts do not learn to leverage context from the full input. Background Here we describe the two related tasks of natural language inference (NLI) and defeasible NLI; two corresponding datasets (SNLI and \u03b4-NLI, respectively); and annotation artifacts present in these datasets. Task Definitions. Natural language inference (Giampiccolo et al., 2007; MacCartney, 2009; Bowman et al., 2015) is the task of determining whether a premise sentence (P ) entails a hypothesis sentence (H). That is, given P , would a human conclude H is true (entailment), false (contradiction), or neither (neutral). When H is neutral, the task of defeasible natural language inference (Rudinger et al., 2020) asks whether a third update sentence (U ) makes H more likely to be true (U is a strengthener) or less likely to be true (U is a weakener). In the following example, H is neutral given P (NLI task), and U is a weakener given P and H (defeasible NLI task). Premise: A man is sitting in a dim restaurant. Hypothesis: He is eating food. Update: He is browsing a menu. In this work, we adopt the terms context (C) and target (T ) for clarity when describing partial-input baselines for the two tasks. In NLI, C = P and T = H; in defeasible NLI, C = (P, H), and T = U . Thus, for either task, the partial-input baseline we use looks only at T and ignores C. SNLI. SNLI (Bowman et al., 2015) is the first large-scale English NLI dataset, containing 570K labeled P -H pairs. In SNLI, premises are derived from image captions (Young et al., 2014) , and hypotheses for each label (entailment, neutral, contradiction) are elicited from crowdsource workers who are shown a premise. \u03b4-NLI. For the task of defeasible NLI, Rudinger et al. (2020) introduce the \u03b4-NLI dataset, which consists of extensions to three pre-existing English natural language reasoning datasets: SNLI (Bowman et al., 2015) , ATOMIC (Sap et al., 2019) , and SOCIAL-CHEM-101 (Forbes et al., 2020) . To extend each dataset, instances (e.g., P -H pairs) were presented to crowdworkers who then wrote an update sentence (U ) to strengthen or weaken the given hypothesis (as described in Task Definitions). The resulting binary classification task is to predict whether an update is a strengthener or weakener, given a (P, H, U ) triple. In this work, we focus our evaluation on \u03b4-SNLI, the SNLI portion of the \u03b4-NLI dataset. Artifacts and Partial-Input Baselines. Gururangan et al. (2018) and Poliak et al. (2018) observe that the crowdsourcing protocols adopted by Bowman et al. (2015) and others lead to the creation of data with annotation artifacts that enable partialinput baselines (e.g., hypothesis-only baselines) to perform well above a majority-class baseline. For SNLI, Poliak et al. (2018) report that an InferSent (Conneau et al., 2017) hypothesis-only baseline surpasses a majority baseline by 35 points. A similar effect is observed by Rudinger et al. (2020) in the \u03b4-NLI data, with an update-only RoBERTa (Liu et al., 2019) model achieving 15 points above a majority baseline. Experiment 1: Context in NLI An essential component of true reasoning is learning to leverage all parts of an example's input to make a determination of entailment. Strong partialinput models demonstrate that full-input models do not necessarily need to utilize context to make correct predictions. However, we explore whether they do at all, and how access to such context shifts a full-input model's confidence in the correct label. If, upon supplying C, a model strengthens its confidence in its prediction, we may conclude that it utilizes both C and T during inference as intended. Experimental Setup. We finetune two sets of RoBERTa (Liu et al., 2019 ) models (a partial-input and a full-input model) on the train splits of SNLI and \u03b4-NLI (see Appendix A for dataset sizes). We utilize roberta-base from the Hugging Face library (Wolf et al., 2020) , and finetune each model for two epochs. Appendix B further details our training setup. We then run inference on the test splits from SNLI and \u03b4-SNLI (the SNLI portion of \u03b4-NLI) using each pair of models. Table 1 reports accuracy of all models on the corresponding test splits. We calibrate RoBERTa models post-hoc using temperature scaling (Guo et al., 2017) as suggested by Desai and Durrett (2020) , and examine confidence shifts in the correct label to understand  whether full-input models utilize context at all. 2  As shown in Fig. 2 , we plot an ordered pair of each model's confidence in the correct label for examples in the test splits, with the partial-input model's confidence along the x-axis and the fullinput model's confidence along the y-axis. Density around the diagonal would indicate no change in confidence. Evidenced by the density above the diagonal in Figures 2a and 2b , full-input models (i.e access to both C and T ) are more confident in the correct label than partial-input models. 3 While this behavior may seem unsurprising, partial-input baselines illustrate that models may show confidence in the correct label without needing to condition on context at all. Our results hint that full-input models may be successfully learning to leverage additional context instead of overgeneralizing on artifacts in the target. To probe this behavior directly, we intro-duce an evaluation set crafted by editing contexts in the following section. Experiment 2: Context Editing We investigate a model's ability to leverage context despite the presence of artifacts by exploring how sensitive full-input models are to changes in non-target components of the input. We present an example modification scheme, illustrated in Figure 1 , in which we edit context sentences from examples where a model correctly predicts the label l from the target T alone. Namely, while holding T constant, we introduce an edited context sentence C \u2032 that induces a different label l \u2032 \u0338 = l on the new (C \u2032 , T ) pair. Using this scheme, we construct an evaluation set of 600 examples sourced from SNLI and \u03b4-NLI. Example Subselection. We select SNLI and \u03b4-SNLI test examples to edit by running the partialinput RoBERTa models from Section 3 and fullinput bag-of-words (BoW) models, implemented via fasttext with a maximum of 4-grams (Joulin et al., 2017) . See Appendix B for training details on the bag-of-words models. We select examples to edit for which either the partial-input model or the BoW model predicted the correct label. This identifies the subset of examples likeliest to contain artifacts in T , lexical or otherwise. Editing SNLI Examples. For a given SNLI example (P, H, l) and a new predefined target label 4 shows examples of editing contexts from both datasets. All examples were manually edited by one author and independently validated by another. During validation, we hide both the l and l \u2032 , and ask the annotator to label the text pair. Using Cohen's Kappa (Cohen, 1960) , we obtain an agreement measure of \u03ba = 0.78 and \u03ba = 0.76 for SNLI and \u03b4-SNLI examples in our test set respectively, indicating substantial agreement (Artstein and Poesio, 2008) . Results. Using the same full-input RoBERTa models trained in Section 3, we run inference on our edited evaluation set. Tables 2 and 3 show model performance stratified by original label l and target label l \u2032 . Our results show that full-input models are in fact sensitive to context modifications despite the presence of artifacts in T , consistently achieving above 70% accuracy on edited examples. Thus, we conclude that these models are not overgeneralizing on artifacts in the instance, learning to condition on context for prediction. Analyzing Post-Edit Model Confidence. Similar to the analysis in Section 3, we inspect shifts in confidence upon editing contexts to shed more light on a full-input model's utilization of C. For \u03b4-SNLI examples, we plot ordered pairs in Fig. 3 of a full-input model's confidence in the correct label pre-edit, and its confidence in the same label post-edit (i.e the confidence in the now-incorrect label). The majority of mass is under the diagonal, indicating that our model is indeed sensitive to changes in context. The green bottom-left quadrant delineates ideal performance (correct before and after editing examples). We attribute the small cluster of examples in the blue quadrant (previously highly confident in the correct label and subsequently remained confident, but in the wrong label) to strong, non-lexical artifacts overriding additional signal from the context. Appendix C visualizes shifts in SNLI examples using simplex plots to accommodate the ternary label. Lexical Model Performance. We do not retrain any of the models on edited examples prior to evaluating with our edited test set, precluding them from picking up on any newly-introduced non-lexical artifacts. However, to validate the absence of trivial lexical features that override the artifacts in T and to ensure sufficient difficulty, we run full-input BoW models on our edited evaluation set. A full- Context (C \u2192 C') Target (T) Label (l \u2192 l') SNLI Premise: A little girl in a pink hat is in a lush green field walking an ox. Edited Premise \u270e : A little girl in a pink hat sits on an ox carrying her through the middle of the Sahara. Hypothesis: A little girl is riding her ox in a desert. Contradiction \u2192 Entailment Premise: A girl in a yellow dress looks at a restaurant building. Edited Premise \u270e : A girl in a yellow dress enjoys some food at her favorite restaurant. Hypothesis: A girl debates eating at a certain restaurant. Neutral \u2192 Contradiction \u03b4-NLI Premise: A boy in a red hooded top is smiling whilst looking away from his reflection. Hypothesis: The boy doesn't want to see his reflection. Edited Hypothesis \u270e : The boy wants to see his reflection. Update: A woman's voice says to him 'Don't be afraid of it, it's harmless' Strengthener \u2192 Weakener Premise: A music concerto is happening on stage, with the lights on spot, while the audience listens in the dark. Hypothesis: The concert is being held at night. input fasttext model, often used for adversarial filtering (Zellers et al., 2018) , achieves 16% and 24.3% accuracy on the SNLI and \u03b4-SNLI portions of our evaluation set respectively. Edited Hypothesis Related Work In addition to work on partial-input baselines for NLI (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) , partial-input models have been studied for story completion (Cai et al., 2017) and reading comprehension (Kaushik and Lipton, 2018) . Feng et al. (2019) observe that low-scoring partial-input baselines do not preclude other artifacts and heuristics, while Glockner et al. (2018) and McCoy et al. (2019) demonstrate lexical and syntactic examples of NLI heuristics. Adversarial editing has been explored for non-NLI tasks as well (Jia and Liang, 2017; Ribeiro et al., 2018) . Finally, adversarial filtering has been proposed as a means of removing artifacts from datasets (Zellers et al., 2018; Le Bras et al., 2020) . Discussion and Conclusion While partial-input models are useful tools for analysis, often leveling fair criticism of datasets, our results show it is hasty to conclude that models trained on such datasets are not capable of reasoning. Even though high-scoring partial-input baselines show that full-input models could ignore context, our experiments show that they can leverage this context quite effectively. We argue that artifacts do not necessarily spell disaster for a model's reasoning capabilities. In particular, our context-editing experiments identify a set of instances that partial-input models fail (by design), but full-input models largely succeed at, displaying the capability of full-input models to leverage context to overcome SNLI and \u03b4-NLI artifacts in many, but not all, cases. Of course, we do not deny that artifacts can and do lead to models with exploitable heuristics, as demonstrated by Glockner et al. (2018) and McCoy et al. (2019) . While we do not attempt to define the sufficient conditions for a model to perform \"true inference,\" we demonstrate that these models can and do meet the necessary condition of leveraging the full input. Thus, we conclude that partial input baselines should be understood as agnostic warning signs: sufficient to conclude that full-input models might not be leveraging critical context, but insufficient to prove that they don't. This raises a number of interesting questions for follow-up work. If adversarial filtering (Le Bras et al., 2020) can identify instances containing artifacts, is it beneficial to remove these instances from the training set? Or could they be edited to flip the label and mitigate the spurious correlation? The edits we made in this work were done manually, but another interesting question is whether these edits could be made automatically or semi-automatically. Having a more efficient way of producing these examples would enable both rapid evaluation of models trained on datasets with artifacts (as in this work), as well as expansion of training sets to preemptively mitigate artifacts. A SNLI and \u03b4-NLI Dataset Sizes To train and tune our neural and lexical models, we utilize the train and validation splits from SNLI and \u03b4-NLI. We include train/validation/test split sizes in Table 4 . Our \u03b4-NLI RoBERTa models were finetuned on examples from all 3 portions of the \u03b4-NLI dataset (ATOMIC, SNLI, and SOCIAL-CHEM-101), however for our evaluation set and analysis, we exclusively use the SNLI portion of the \u03b4-NLI dataset, abbreviated as \u03b4-SNLI. We include the split sizes for \u03b4-SNLI in Table 4 as well. B Model Training Setup Neural Models. We use the Hugging Face library to train all of our RoBERTa models. We utilize roberta-base, which has 125M trainable parameters. All models were trained on a single NVIDIA 1080-TI GPU. After tuning on the validation set, all models were trained for two epochs with a learning rate of 2e-5 and a batch size of 32. Tables 2 and 3 report best accuracy across five runs. Lexical Models. We use the fasttext library to implement our bag-of-words models. fasttext is an off-the-shelf text classification library. All lexical models were trained for 5 epochs with 4-grams as the maximum length of word ngrams. We use the default learning rate of 0.1. C Visualizing Distribution Shifts in SNLI Edited Examples We are able to visualize confidence distribution shifts for \u03b4-NLI before and after editing using a 2D plane with ordered pairs, as in Figure 3 , due to the label set containing only two update types-weakener and strengthener (so, a probability score > 0.5 results in the predicted label). Since the SNLI label set consists of three labels (entailment, neutral, contradiction), we choose to visualize shifts in the confidence distribution before and after editing contexts via ternary plots. For each directional label pair (entailment \u2192 {neutral, con-tradiction}, neutral \u2192 {entailment, contradiction}, contradiction \u2192 {entailment, neutral}), we plot a heatmap of probabilities, or confidences, in each of the three classes on the simplex with Gaussian smoothing using python-ternary, a ternary plotting package (Harper, 2015) . Figures 5, 6 , and 7 show these visualizations. We include these plots mainly to help visualize information about the predicted labels of the incorrect examples ( D Dataset Limitations In this work, we choose to explore the role of context with respect to the SNLI and \u03b4-NLI datasets. In particular, the proven presence of strong artifacts in SNLI made it an appealing dataset to explore a model's behavior with respect to the utilization of context. We chose to include \u03b4-NLI in our analysis, since ultimately, we'd like reasoning systems to operate in complex and dynamic contexts. The ability to be sensitive to shifting contexts and understand when default inferences should be overridden by additional context (i.e more nuanced inference) is both central to our exploration and central to the task of defeasible reasoning itself. Our evaluation set does not include examples sourced from other NLI datasets such as MultiNLI (Williams et al., 2018) . It also does not contain datasets across domains, such as MedNLI (Romanov and Shivade, 2018) . However, we note that while the datasets may be different, others have shown artifacts present in such datasets (i.e (Herlihy and Rudinger, 2021; Gururangan et al., 2018) . Our goal was to utilize datasets containing high amounts of artifacts. Entailment \u2192 Neutral High Density Low Density  shows examples edited to induce a contradiction relation. Each l and l \u2032 is shown in the center box. We note that these distributions help visualize the accuracies presented in Table 2 . Neutral \u2192 Entailment (a) Confidence distribution heatmap for examples with original relation as neutral, edited to induce an entailment relation. We note that this was the hardest edit category for RoBERTa models to flip, and draw attention to a substantial amount of mass still in the neutral corner of the simplex. 2 .",
    "abstract": "When strong partial-input baselines reveal artifacts in crowdsourced NLI datasets, the performance of full-input models trained on such datasets is often dismissed as reliance on spurious correlations. We investigate whether stateof-the-art NLI models are capable of overriding default inferences made by a partial-input baseline. We introduce an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model's sensitivity to edited contexts. Our results indicate that NLI models are still capable of learning to condition on context-a necessary component of inferential reasoning-despite being trained on artifact-ridden datasets.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "Partial-input baselines show that {NLI} models can ignore context, but they don{'}t."
}