{
    "framing_question": "If this paper introduces a task or benchmark, what benefits of the task or benchmark does it state, if any?",
    "response": "The paper introduces the task of abstractive text summarization. The benefits of this task are not explicitly stated in the paper, but it can be inferred that abstractive text summarization can be used to:\n\n- Generate concise and informative summaries of long documents, which can be useful for a variety of applications such as news summarization, scientific literature review, and legal document analysis.\n\n\n- Help people understand the main points of a document quickly and easily, which can be useful for busy professionals, students, and anyone else who needs to stay informed about current events or research findings.\n\n\n- Automate the process of summarizing documents, which can save time and money for businesses and organizations.",
    "article": "Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive. This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss. Pseudo-labeling based methods are popular in sequence-tosequence model distillation. In this paper, we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models. Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods. Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive. Our code is available at https://github. com/Shengqiang-Zhang/plate. Introduction Automatic document summarization is the task of rewriting a long document into its shorter form while still retaining its most important content. In the literature, there are mainly two kinds of methods for summarization: extractive summarization and abstractive summarization (Nenkova and McKeown, 2011) . In this work, we focus on abstractive summarization, which is viewed as a sequence-tosequence (Seq2Seq) learning problem, since recent abstractive models outperform their extractive counterparts and can produce more concise summaries (Raffel et al., 2020; Lewis et al., 2020; Zhang et al., 2020; Liu and Lapata, 2019) . Recent progress of abstractive summarization largely relies on large pre-trained Transformer models (Raffel et al., 2020; Lewis et al., 2020; Zhang et al., 2020; Liu and Lapata, 2019; Bao et al., 2020) . With these extremely large models, we can obtain state-of-theart summarization results, but they are slow for online inference, which makes them difficult to be used in the production environment even with cutting-edge hardware. This paper aims to distill these large Transformer summarization models into smaller ones with minimal loss in performance. Knowledge distillation is a class of methods that leverage the output of a (large) teacher model to guide the training of a (small) student model. In classification tasks, it is typically done by minimizing the distance between the teacher and student predictions (Hinton et al., 2015) . As to Seq2Seq models, an effective distillation method is called pseudo-labeling (Kim and Rush, 2016) , where the teacher model generates pseudo summaries for all documents in the training set and the resulting document-pseudo-summary pairs are used to train the student model. In this paper, we argue that attention distributions of a Seq2Seq teacher model might be too sharp. As a result, pseudo labels generated from it are sub-optimal for student models. In the summarization task, we observe that 1) pseudo summaries generated from our teacher model copy more continuous text spans from original documents than reference summaries (56% 4-grams in pseudo summaries and 15% 4-grams in reference summaries are copied from their original documents on CNN/DailyMail dataset); 2) pseudo summaries tend to summarize the leading part of a document (measured on CNN/DailyMail, 74% of sentences in pseudo summaries and 64% of sentences in reference summaries are from the leading 40% sentences in original documents). We obtain the two numbers above by matching each sentence in a summary with the sentence in its original document that can produce maximum ROUGE (Lin, 2004 ) score between them. We call the two biases above the copy bias and the leading bias. In order to have an intuitive feeling, we select a rep-resentative example 1 and visualize its cross attention weights 2 (see the left graph in Figure 1 ). We observe that attention weights form three \"lines\", which indicates very time the decoder predicts the next word, its attention points to the next word in the input document. That may be the reason why multiple continuous spans of text are copied. Another phenomenon we observe is that all high-value attention weights (in deeper color) concentrate on the first 200 words in the input document, which reflects the leading bias. In either case, the attention distribution is too sharp (i.e., attention weights of the next word position or the leading part is much larger than other positions), which means our teacher model is over-confident. Based on the observations above, we propose a simple method called PLATE (as shorthand for Pseudo-labeling with Larger Attention TEmperature) to smooth attention distributions of teacher models. Specifically, we re-scale attention weights in all attention modules with a higher temperature, which leads to softer attention distributions. Figure 1 intuitively shows the effect of using higher attention temperatures. Compared with the left graph, the right graph with higher attention temperature has shorter lines (less copy bias) with high attention weights, and positions of high attention weights extend to the first 450 words (less leading bias). Less copy bias in pseudo summaries encourages student models to be more abstractive, while less leading bias in pseudo summaries encourages student models to take advantage of longer context in documents. Experiments on CNN/DailyMail, XSum, and New York Times datasets with student models of different sizes show PLATE consistently outperforms vanilla pseudo-labeling methods. Further empirical analysis shows that, with PLATE, both pseudo summaries generated by teacher models and summaries generated by student models are shorter and more abstractive, which matches the goal of abstractive summarization. Related Work Large pre-trained Seq2Seq Transformer models largely improve results of generation tasks including text summarization (Song et al., 2019; Lewis et al., 2020; Bao et al., 2020; Raffel et al., 2020; Token index in summary Zhang et al., 2020) . These models are pre-trained using unsupervised text-to-text objectives. For example, T5 (Raffel et al., 2020) is pre-trained by predicting corrupted text spans. BART (Lewis et al., 2020) employs denoising auto-encoding objectives such as text infilling and sentence permutation during its pre-training. The pre-training objective of PEGASUS (Zhang et al., 2020) is tailored for the summarization task, which predicts the most \"summary worthy\" sentences in a document. Our method aims to make these large models faster. In knowledge distillation, besides learning from gold labels in the training set, student models can learn from soft targets (Ba and Caruana, 2014; Hinton et al., 2015) , intermediate hidden states (Romero et al., 2014) , attentions (Zagoruyko and Komodakis, 2017; Wang et al., 2020) , and target output derivatives (Czarnecki et al., 2017) of teacher models. Recent work for distillation of pre-trained Transformers (e.g., DistilBERT (Sanh et al., 2019) , TinyBERT (Jiao et al., 2020) , Mobile-BERT (Sun et al., 2020) , BERT-of-Theseus (Xu et al., 2020a) , MINILM (Wang et al., 2020) ) focuses on natural language understanding tasks such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2016) benchmarks. Most methods above are designed for classification models. In Seq2Seq learning tasks such as summarization, we can apply distillation methods above to each step of sequence model predictions. However, the sequence-level knowledge of teacher mod-els is not well utilized. Therefore, Kim and Rush (2016) introduce a sequence-level knowledge distillation method (i.e., pseudo-labeling), where a student model is trained with pseudo labels generated by the teacher model using beam search decoding. Kim and Rush (2016) and later work (Kasai et al., 2020; Gu et al., 2017; Denkowski and Neubig, 2017) show pseudo-labeling achieves competitive performance for Seq2Seq tasks such as machine translation. Shleifer and Rush (2020) propose the shrink and fine-tune (SFT) approach for pre-trained summarization distillation, which re-finetunes a teacher model with some layers removed, and they show SFT outperforms pseudo-labeling and a modification of direct knowledge distillation (Jiao et al., 2020) on one of their datasets, but not others. Our method, which builds on top of pseudo-labeling, is conceptually simple and improves pseudo-labeling across different summarization datasets. There is an interesting line of work called selfdistillation or self-training (Furlanello et al., 2018; Xie et al., 2020; Deng et al., 2009; Liu et al., 2020; He et al., 2019) , where the size of the student model is identical to the size of the teacher model. Our method can also be applied in selfdistillation and can potentially be combined with the self-distillation methods above. Summarization Distillation Transformer based abstractive summarization Abstractive summarization aims to rewrite a document into its shorter form (i.e., summary), which is a typical Seq2Seq learning problem. We adopt the Seq2Seq Transformer (Vaswani et al., 2017) model. Given a document X = (x 1 , x 2 , . . . , x |X| ) and its gold summary Y = (y 1 , y 2 , . . . , y |Y | ), we estimate the following conditional probability: p(Y |X; \u03b8) = |Y | t=1 p(y t |y <t , X; \u03b8) (1) where \u03b8 is the model parameter and y <t stands for all tokens before position t (i.e., (y 1 , y 2 , . . . , y t\u22121 )). The Seq2Seq Transformer model can be trained by minimizing the negative log-likelihood of gold document-summary pairs: L G (\u03b8) = \u2212 1 |Y | log p(Y |X; \u03b8) (2) where |Y | is the number of tokens in summary Y . Distillation with pseudo labels Knowledge distillation refers to the task of transferring knowledge of a large teacher model (or a group of large teacher models) into a small student model. As to Seq2Seq learning tasks such as machine translation and summarization, pseudolabeling based methods are usually used to imitate teacher predictions at the sequence level. Specifically, suppose we have a document X, and \u0176 = (\u0177 1 , \u01772 , . . . , \u0177| \u0176 | ) is a pseudo summary generated by a teacher model using beam search. The student can be trained by minimizing the negative loglikelihood of document-to-pseudo-summary pairs. L PL (\u03b8) = \u2212 1 | \u0176 | | \u0176 | t=1 log p(\u0177 t |\u0177 <t , X; \u03b8) (3) Strictly, all possible pseudo summaries from X should be taken into account. Unfortunately, the computational cost is prohibitive. We therefore use a single sample \u0176 (which takes a large portion of probability mass from the teacher) instead as in Kim and Rush (2016) . Re-scaling attention temperatures Both our teacher and student models are Seq2Seq Transformer models. The core part of a Transformer model is the attention module: Attention(Q, K, V ) = softmax( QK T \u03c4 )V (4) where Q, K, V are linear projections of hidden states of a layer and \u03c4 is the temperature of the attention module which is usually \u221a d (d is the hidden dimension size of that attention head). Our distillation method PLATE works as follows. Assume we have a teacher model trained with \u03c4 = \u221a d. When the teacher generates pseudo labels with beam search, we use a higher attention temperature and set \u03c4 = \u221a \u03bb d where \u03bb > 1 (\u03bb is the attention temperature coefficient). Note that we only change the teacher's attention temperature during inference time. When we train our student model with pseudo labels, we still use a normal temperature (i.e., \u03c4 = \u221a d). We find that adjusting the student's attention temperature does not work. Probably because the student can easily adapt to the scaled attention temperature during training. We find that \u03bb = 1.5 or \u03bb = 2.0 usually works well in practice. To encourage teacher models to generate pseudo labels with more diversity, we further propose to use a random \u03bb for each input document (\u03bb \u223c U [a, b]). Note that U [a, b] is a uniform distribution and we typically set a = 1.0 and b = 2.0. Experiments Datasets We conduct our experiments on three popular document summarization datasets: CNN/DailyMail (Hermann et al., 2015) , XSum (Narayan et al., 2018) , and New York Times (Sandhaus, 2008) . All datasets are tokenized with the GPT-2 tokenizer (Radford et al., 2019) , which is based on UTF-8 BPE (Sennrich et al., 2016) . CNNDM The CNN/DailyMail dataset (CNNDM; Hermann et al., 2015) contains online news articles from the CNN and DailyMail websites paired with their associated highlights as reference summaries. We follow the standard pre-processing steps described in See et al. (2017) ; Liu and Lapata (2019) . 3 The resulting numbers of document-summary pairs for training, validation, and test are 287,227, 13,368, and 11,490, respectively. XSum The XSum dataset is collected by harvesting online articles from the BBC with single sentence summaries, which is professionally written. The summaries are extremely abstractive. We use the official splits of Narayan et al. (2018) Implementation details Teacher/Student model settings We use BART Large (Lewis et al., 2020 ) as our teacher model, which has 12 layers in the encoder and decoder. The hidden size of each layer is 1024, and each layer contains 16 attention heads with a hidden size of 64. We have four kinds of student models. The first three student models are initialized from BART weights (therefore, their hidden sizes are the same as that of BART). All the three students have the 12 layers of BART encoder and differ in the number of decoder layers. They are denoted by BART 12-6, BART 12-3, and BART 12-12 with 6, 3, and 12 decoder layers, respectively. For BART 12-6 (or BART 12-3), the decoder is initialized from the first 6 (or 3) layers or the maximally spaced 6 (or 3) layers of BART decoder. The fourth student is the Transformer base model (Vaswani et al., 2017) , which has 6 layers in each of the encoder and decoder. Each layer has a hidden size of 512 and 8 attention heads. This student is randomly initialized and denoted by Transformer. The latency statistics (Milliseconds) and numbers of parameters of above four models are in Table 1 . Training and inference Hyper-parameters for BART, BART 12-6, BART 12-3, and BART 12-12 are similar. Specifically, all models are optimized using Adam (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.999. Learning rates are tuned on validation sets (choose from 1e-5, 3e-5, 5e-5, 7e-5). We truncate all documents and summaries to 1024 sub-word tokens. We use a batch size of around 80 documents (we limit the max number of tokens on each GPU to 2048) and train our models for 20,000/15,000/6,000 steps with 500 warmup steps for CNNDM, XSum, and NYT, respectively. We also employ a weight decay of 0.01. For Transformer, the hyperparameters of the Adam optimizer is a bit different, and we use \u03b2 1 = 0.9, \u03b2 2 = 0.98. Learning rates are picked from 1e-4, 3e-4, 5e-4, 7e-4 accord-ing to validation sets. The weight decay is set to 0.0001. The warmup step we use is 4000. We train Transformer for 100 epochs and select the best model w.r.t. their ROUGE scores on validation sets. For all models above we apply a label smoothing of 0.1 to prevent overfitting (Pereyra et al., 2017) . During inference, as common wisdom, we apply beam search. The beam size, length penalty, and minimal length are 4, 2.0, and 55 on CNNDM; 6, 0.1, and 1 on XSum; and 4, 0.7, and 80 on NYT, respectively. All our models are trained on 8 NVIDIA V100 GPUs. The training is fairly fast. Training on CNNDM with the teacher model (i.e., BART) is most time-consuming. It takes about 45 minutes for one epoch, and we need 6 epochs in total. Evaluations We evaluate the quality of different summarization systems using ROUGE. On CNNDM and XSum datasets, we report full-length F1 based ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) scores. Following Durrett et al. (2016) ; Liu and Lapata (2019) , we report limited-length recall based ROUGE-1, ROUGE-2, and ROUGE-L, where generated summaries are truncated to the lengths of gold summaries. All ROUGE scores are computed using the ROUGE-1.5.5.pl script 4 . Summaries generated by abstractive models may be ungrammatical or unfaithful to the original document. Additionally, we also measure the quality of generated summaries by eliciting human judgements. We randomly sample 50 documents from the test set of CNNDM. 12 annotators are invited (they are either native English speakers or graduate students with IELTS test score over 6.5). In the evaluation, participants are presented with a document and a list of outputs by different models. First, they are asked to evaluate the summaries on three dimensions: fluency (is the summary grammatically correct?), faithfulness (is the summary faithful to the original document?), and coverage (does the summary coverage important information of the document?). Then, they are asked to rank the summaries from best to worst as a way of determining the overall quality of summaries. Each document is ensured to be annotated by 3 different subjects. Results Our main results are shown in Table 2 . The first block includes several recent abstractive summarization models based on large pre-trained Transformers. BERTSUM (Liu and Lapata, 2019) employs BERT (Devlin et al., 2019) as its encoder and uses randomly initialized decoder. T5 (Raffel et al., 2020) , PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020) are three popular large Seq2Seq Transformer models with different pretraining objectives. Our own fine-tuning version of BART (BART (ours)) is comparable or slightly better than the original reported BART results, and we use it as the teacher model on the three datasets. The second block presents results of student models. Shleifer and Rush (2020) compare pseudolabeling (BART-PL), knowledge distillation using both output and intermediate layers (BART-KD) as well as shrink and fine-tuning (BART-SFT) methods. They also use BART as teacher models. Note their settings of student models are BART 12-6 on CNNDM and BART 12-3 on XSum. Results of our BART 12-3 and BART 12-6 student models are in the third and fourth block. We present results of students trained with gold labels (Gold) and regular pseudo labels (Regular) as well as pseudo labels with higher and random attention temperatures (PLATE B12-3  \u03bb=1.5 , PLATE B12-3 \u03bb=2.0 and PLATE B12-3  rnd ). PLATE B12-3 \u03bb=1.5 means that the student uses attention temperature coefficient \u03bb = 1.5 with architecture setting BART 12-3. PLATE B12-3   rnd means that we use random attention temperature of \u03bb \u223c U [1.0, 2.0]. We observe that using pseudo-labeling methods with higher attention temperatures consistently improves over its counterpart with normal attention temperatures (Regular) across all three datasets, and the differences between them are almost always significant measured with the ROUGE script 5 (see details in Table 2 ). Interestingly, our student models PLATE B12-3  \u03bb=2.0 and PLATE B12-6  \u03bb=2.0 outperform all models in comparison (including student models and even the teacher model) on CNNDM. Our best performing student model PLATE B12-3  \u03bb=1.5 outperforms BART-PL, BART-SFT, and BART-KD on XSum. Meanwhile, our method is conceptually simpler and can further be combined with their methods with additional train-  ing objectives. In Section 3.3, we also propose a variant of our method, which employs random attention temperatures (PLATE rnd in Table 2 ). We can see that though random temperature based method is not as good as our best fixed-temperature method, it in general produces decent results. Therefore, we recommend using this method when the computing budget is limited. Note that we also tried more extreme \u03bb values as shown in Appendix B, and we find the value of 1.5 or 2.0 works better than others. In the fifth block, we additionally conduct selfdistillation experiments, which is not the focus of this work. Our method improves the teacher model on CNNDM; ROUGE-2/L scores are improved on XSum; while on NYT, there are improvements on ROUGE-1/L. Results ing power of Transformer without pre-training is not large enough to effectively model the differences in pseudo labels. It is also interesting to see that students distilled with pseudo-labeling do improve gold label based students using randomly initialized Transformer, but not with pre-trained models (i.e., BART 12-6 and BART 12-3), which may also be due to the strong modeling power of large pre-trained Transformers. Human evaluation We randomly sample 50 documents from the test set of CNNDM. We compare our best student model PLATE B12-6  \u03bb=2.0 against the 132 Attention Setting R1 R2 RL regular pseudo-labeling model (Regular), another model PLATE B12-6 \u03bb=1.5 and human reference (Ref). We ask human judges to rank the outputs of these models from best to worst. We convert the ranks to rank ratings (rank i to 5 \u2212 i) and further conduct student t-test on these ratings. As shown in Table 3 , PLATE B12-6  \u03bb=2.0 obtains the best ranking score and the difference between PLATE B12-6  \u03bb=2.0 and the regular pseudo-labeling based method Regular is significant (p < 0.05), which indicates our proposed method PLATE indeed produces better summaries. \u03bb enc = \u03bb cross = \u03bb dec = 2. Ablation study In a Transformer, there are three types of attention modules (i.e., encoder selfattention, decoder self-attention and decoder crossattention), and we can scale attention temperatures for all of them or some of them. Let \u03bb enc , \u03bb cross , and \u03bb dec denote the attention temperature coefficient of the encoder self-attention module, the decoder cross-attention module, and the decoder self-attention module, respectively. As shown in Table 4 , using large attention temperature coefficients (2.0) for all three types of attention modules leads to the best result. When setting the coefficient of the cross attention module to \u03bb cross = 1.0, the ROUGE scores drop most. Perhaps this is not surprising, since cross attentions are directly related to the selection of document contents for summarization. Besides, the attention temperature of the decoder self-attention is also crucial but not as important as the cross-attention (see the fourth row). Comparison with sampling and tuning output layer temperature Sampling based methods can produce more diverse and richer outputs than its beam search based counterpart and has been proven useful in back translation (Edunov et al., 2018) . We implement the sampling method in Edunov et al. (2018) and Nucleus Sampling (Holtzman et al., 2019) , a more advanced sampling method, to generate pseudo labels for distillation. We use the BART 12-6 as the student model, and the distillation results on CNNDM are in Table 5 . As can be seen, both of the sampling based methods above perform worse than the regular beam search based pseudolabeling method (Regular), let alone ours. Besides the attention temperatures, we can also tune the temperature T in the decoder output softmax layer. With a proper T (i.e., T = 0.5) during pseudo label generation, the resulting student model slightly outperforms the baseline student model with regular pseudo labeling method on ROUGE-2/L (see Table 5 ), but worse than PLATE \u03bb=2.0 . More results with different T s are in Appendix C. Analysis Why does our distillation method work? To answer this question, we first try to analyze the reasons from both the external characteristics of the summaries generated by the teacher model and the internal characteristics of the teacher's attention mechanism. Then, we will give an in-depth explanation. Length and novel n-grams We first analyze the pseudo summaries generated by the teacher models. We calculate novel n-grams and lengths of generated summaries. Note that if an n-gram appears in the summary, but not in the original document, we call it a novel n-gram. Proportions of novel n-grams are used to measure the abstractiveness of summaries (See et al., 2017; Liu and Lapata, 2019) . As shown in Table 6 , when using a larger \u03bb, pseudo summaries are shorter 6 and contain a larger portion of novel n-grams. It indicates that the teachers can produce more concise and abstractive summaries, which matches the goal of abstractive summarization. Are these pseudo summaries of good quality? The performance of the teacher with different attention temperatures on CNNDM test set is shown in Table 7 . Their results are all decent and close to each other (at least for ROUGE-1 and ROUGE-L). Interestingly, compared with \u03bb = 1.0, the performance of the teacher with \u03bb = 2.0 is worse, but the resulting student is much better (see Table 2 ). Perhaps not surprisingly, the styles of summaries from students are similar with these from their teachers. Concise and abstractive teachers lead to concise and abstractive students (see Table 6 ). Conciseness and abstractiveness are good properties for summarization, which however may not be the case for other generation tasks such as machine translation. We apply PLATE to the WMT16 (Bojar et al., 2016) English-German translation task and use Transformer-big as the teacher and Transformer-base as the student. With \u03bb = 1.5, we obtain a BLEU of 27.90, while the result of the regular pseudo-labeling is 27.79 (more details are in Appendix A). Attention We have shown earlier in Figure 1 that with higher attention temperature, cross-attention modules of a teacher can attend to later parts in documents. We observe that students behave similarly, and we put more cross attention visualization of students in Appendix F. To obtain corpus-level >@ >@ >@ >@ >@ 1RUPDOL]HGWRNHQSRVLWLRQWRLQWHUYDOVLQGRFXPHQWV 3URSRUWLRQRIHYLGHQWDWWHQWLRQZHLJKWV! = 1.0 = 1.5 = 2.0 statistics, we further calculate the evident crossattention weight distributions of the teacher when generating pseudo labels on the training set of CN-NDM. Note that an attention weight is evident if it is greater than 0.15, and these evident attention weights account for around 15% of all attention weights. Specifically, we normalize the token positions of each document to (0.0, 1.0] and divide the normalized positions into five bins. The mean proportions of evident attentions for all bins are shown in Figure 2 . Compared to the teacher with normal attention temperature (pink bar), teachers with higher attention temperatures (blue and green bars) attend less on the heading parts of documents while more on the tail parts of documents. To sum up, teachers with higher attention temperatures can generate more concise and abstractive pseudo summaries, which makes the teacher provide more summary-like pseudo labels to students. High-temperature teachers can alleviate the leading bias problems by providing pseudo labels with better coverage of source documents to students. More explanation According to the study of Xu et al. (2020b) , the prediction entropy correlates strongly with whether the model is copying or generating, as well as where in the sentence the token is (content selection). The decoder tends to copy when the model has a low prediction entropy and generate novel bigrams when the model has a high prediction entropy. They also find that high entropy of attention distribution strongly correlates with the model's high prediction entropy. Our method with a higher attention temperature makes attention distributions of the teacher model smoother and leads to a higher entropy of attention distributions, which results in a higher prediction entropy. Therefore, the model with higher attention temperature tends to copy less and generate more novel tokens. The conclusion from Xu et al. (2020b) is in accordance with our observation in Table 6 . Conclusions In this work, we propose a simple but effective extension of pseudo-labeling method PLATE for summarization distillation. Experiments on three datasets demonstrate that our method can consistently outperform the vanilla pseudo-labeling method. Further empirical analysis shows that by using our method, teacher models can generate more concise and abstractive summaries. As a result, summaries produced by student models also become more concise and abstractive. In the future, we would like to explore our method to other generation tasks as well as self-training with unlabeled data. We are also interested in combining our method with other distillation methods and extending our method for better teacher model training. A Experiments of Applying PLATE to the Machine Translation Task Model We apply our method on the WMT16 En-De translation task. We use Transformer-Big model as the teacher and Transformer-Base as the student. Our results on newstest2014 are shown in Table 8 . The student models with our method (\u03bb = 1.5 and \u03bb = 2.0) slightly outperform the student with regular pseudo-labeling method (\u03bb = 1.0). Note that the improvement is not as significant as in summarization tasks. We speculate the reason may be that, unlike summarization, outputs of the machine translation task are relatively fixed. The strength of our methodconciseness and abstractiveness are good properties for summarization but seem not very beneficial to the translation task. B Experiments of More \u03bb Values Besides the \u03bb values of 1.5 and 2.0, we also try more values in a broader range. Table 9 shows the distillation performance of BART 12-6 student models with more values of \u03bb we try on CNNDM dataset (we also include the values of 1.0, 1.5, and 2.0 in table for convenient comparison). As can be seen, both lower and larger \u03bb values are not helpful to the distillation. Though the suitable \u03bb values may vary across datasets, we recommend considering the \u03bb value 1.5 or 2.0 firstly in most cases. equation 5 to some other values rather than the default value 1.0. C Experiments of q i = exp(z i /T ) j exp(z j /T ) (5) However, our experiments demonstrate that this method does not help summarization distillation much. We use BART teacher models with different softmax temperatures in the final decoder layers to generate pseudo summaries and use the BART 12-6 as student models. The experiment results are shown in table 10 . D Experiments of Shorter Pseudo Summaries with Smaller Length Penalty Our method can make pseudo summaries shorter and more abstractive, so one natural idea is that whether just changing the inference hyperparameter length penalty to a smaller value, which can also make pseudo summaries shorter, can benefit abstractive summarization distillation. The experiment results are shown in Table 11 , where the teacher is BART, and the student is BART 12-6. As can be seen from the table, teachers with smaller length penalty (i.e., 1.0 or 0.5) cannot teach better students than the Regular pseudo-labeling or our method. E The Example in Section 1 We present the detailed content of the example in Section 1 in table 12 . F Attention Visualization We present more examples of student models' outputs and cross attention visualization here. The student models are with the BART 12-6 setting and are trained on CNNDM and the following examples are from the validation set of CNNDM. Example 1 Table 13 shows system outputs from different student models and Figure 3 illustrates the corresponding cross attention weights of these student models. Compared with the regular pseudolabeling method ([Regular]), the summary generated by our method PLATE B12-6 \u03bb=1.5 omits the modifier \"Nirvana frontman\" and \"Nirvana bassist\" of the person \"Kurt Cobain\" and \"Krist Novoselic\", respectively and the resulting summary is shorter and more abstractive. The summary generated by our method PLATE B12-6  \u03bb=2.0 contains the text \"will premiere on HBO on May 4\", which is at the end of the source document and included in the reference (i.e., summary worthy), but is ignored by [Regular] . It indicates that our method can alleviate the leading bias problem. Figure 3 also shows that PLATE B12-6  \u03bb=2.0 can access the tail part of the document. Example 2 The second example is shown in Table 14 (outputs) and Figure 4 (attention visualization). In this example, the source document is relatively long (over 700 words). As shown in Figure 4 , the summary generated with the regular pseudo-labeling method Regular mainly focuses on the heading part of the source document (around the first 150 words), but our method PLATE B12-6   \u03bb=2.0 takes into account the tokens in the front, middle and tail of the source document. In Table 14 , the [Reference]: Mentally ill inmates in Miami are housed on the \"forgotten floor\" </s> Judge Steven Leifman says most are there as a result of \"avoidable felonies\" </s> While CNN tours facility, patient shouts: \"I am the son of the president\" </s> Leifman says the system is unjust and he's fighting for change. [PseudoLBL]: Mentally ill inmates in Miami are housed on the \"forgotten floor\" of a pretrial detention facility. </s> Judge Steven Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. </s> He says they face drug charges or charges of assaulting an officer, which are \"avoidable felonies\" </s> He says the arrests often result from confrontations with police, which exacerbate their illness. [Smoothed ]: Mentally ill inmates in Miami are housed on the \"forgotten floor\" </s> Judge Steven Leifman says they are there because of \"avoidable felonies\" </s> He says many of them are in jail for drug or assault charges. </s> He says the system is unjust and he's trying to change it. summary from PLATE B12-6  \u03bb=2.0 contains the key sentence \"Peter Bergen: Pilots are not different from other people, but they can be careless, lazy, inattentive and reckless\", which is similar to the reference sentence \"Peter Garrison: Pilots don't exist on different moral plane than the rest of us\". The sentence \"the human mind is the blackest of boxes\" in the reference, which appears at the tail of the source document, is also included in summaries of PLATE B12-6  \u03bb=2.0 . This example again demonstrates that our method can alleviate the leading bias problem and can make the generated summary have better coverage of source documents.  [Reference]: The trailer surfaced on Wednesday. The documentary is winning raves. It premieres on HBO on May 4. [Regular ]: \"Montage of Heck\" is directed by Brett Morgen and offers an intimate portrayal of Nirvana frontman Kurt Cobain. The trailer features interviews with those closest to the performer, including family members, Courtney Love and Nirvana bassist Krist Novoselic. A companion book containing art and archival documents from Cobain is being produced to accompany the film. [PLATE B12-6  \u03bb=1.5 ]: \"Montage of Heck\" is directed by Brett Morgen and offers an intimate portrayal of Kurt Cobain. The trailer features bits of interviews with those closest to the performer, including family members, Courtney Love and Krist Novoselic. A companion book containing art and archival documents from Cobain is being produced to accompany the film. [PLATE B12-6  \u03bb=2.0 ]: \"Montage of Heck\" is directed by Brett Morgen and will premiere on HBO on May 4. A companion book containing art and archival documents from Cobain is being produced to accompany the documentary. The soundtrack will include \"a mind-blowing 12minute acoustic Cobain unheard track,\" Morgen says. Table 13 : Example 1 of reference summary ([Reference]), summary generated from student with the regular pseudo-labeling method ([Regular]), and summaries generated from students with PLATE ([PLATE B12-6  \u03bb=1.5 ] and [PLATE B12-6  \u03bb=2.0 ]. [Reference]: Experts suspect first officer Andreas Lubitz locked pilot out of the cockpit of plane. Peter Garrison: Pilots don't exist on different moral plane than the rest of us, and the human mind is the blackest of boxes. [Regular ]: Germanwings first officer Andreas Lubitz is one of a handful of airline pilots who have used their airplanes to combine suicide with mass murder. Frida Ghitis: Why is this thought at once so fascinating and so horrifying? It is because of the incompatibility between what we want to believe about flying and what we now see. [PLATE B12-6  \u03bb=1.5 ]: Andre Lubitz joins the short and infamous list of airline pilots who have used their airplanes to combine suicide with mass murder. Frida Ghitis: Why is this thought at once so fascinating and so horrifying? It is because of the incomp compatibility between what we want to believe about flying and what we now see. [PLATE B12-6  \u03bb=2.0 ]: Germanwings first officer Andreas Lubitz is one of a handful of pilots who have used their airplanes to combine suicide with mass murder. Peter Bergen: Pilots are not different from other people, but they can be careless, lazy, inattentive and reckless. He says the human mind is the blackest of boxes; no one can peer inside it. Table 14: Example 2 of reference summary ([Reference]), summary generated from student with the regular pseudo-labeling method ([Regular]), and summaries generated from students with PLATE ([PLATE B12-6  \u03bb=1.5 ] and [PLATE B12-6  \u03bb=2.0 ]. ",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 6.704270752999619e-07,
        "foundation": 0.0,
        "none": 1.0
    }
}