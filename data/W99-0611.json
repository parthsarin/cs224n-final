{
    "article": "This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. It differs from existing methods in that it views corererence resolution as a clustering task. In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%~ placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem. The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes. Introduction Many natural language processing (NLP) applications require accurate noun phrase coreference resolution: They require a means for determining which noun phrases in a text or dialogue refer to the same real-world entity. The vast majority of algorithms for noun phrase coreference combine syntactic and, less often, semantic cues via a set of hand-crafted heuristics and filters. All but one system in the MUC-6 coreference performance evaluation (MUC, 1995) , for example, handled coreference resolution in this manner. This same reliance on complicated hand-crafted algorithms is true even for the narrower task of pronoun resolution. Some exceptions exist, however. Ge et al. (1998) present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993) . Dagan and Itai (1991) develop a statistical filter for resolution of the pronoun \"it\" that selects among syntactically viable antecedents based on relevant subject-verb-object cooccurrences. Aone and Bennett (1995) and McCarthy and Lehnert (1995) employ decision tree algorithms to handle a broader subset of general noun phrase coreference problems. This paper presents a new corpus-based approach to noun phrase coreference. We believe that it is the first such unsupervised technique developed for the general noun phrase coreference task. In short, we view the task of noun phrase coreference resolution as a clustering task. First, each noun phrase in a document is represented as a vector of attribute-value pairs. Given the feature vector for each noun phrase, the clustering algorithm coordinates the application of context-independent and context-dependent coreference constraints and preferences to partition the noun phrases into equivalence classes, one class for each real-world entity mentioned in the text. Context-independent coreference constraints and preferences are those that apply to two noun phrases in isolation. Context-dependent coreference decisions, on the other hand, consider the relationship of each noun phrase to surrounding noun phrases. In an evaluation on the MUC-6 coreference resolution corpus, our clustering approach achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system (McCarthy and Lehnert, 1995) employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set. Furthermore, our approach has a number of important advantages over existing learning and non-learning methods for coreference resolution: \u2022 The approach is largely unsupervised, so no annotated training corpus is required. \u2022 Although evaluated in an information extraction context, the approach is domainindependent. \u2022 As noted above, the clustering approach provides a flexible mechanism for coordinating context-independent and context-dependent coreference constraints and preferences for partitioning noun phrases into coreference equivalence classes. As a result, we believe that viewing noun phrase coreference as clustering provides a promising framework for corpus-based coreference resolution. The remainder of the paper describes the details of our approach. The next section provides a concrete specification of the noun phrase coreference resolution task. Section 3 presents the clustering algorithm. Evaluation of the approach appears in Section 4. Qualitative and quantitative comparisons to related work are included in Section 5. Noun Phrase Coreference It is commonly observed that a human speaker or author avoids repetition by using a variety of noun phrases to refer to, the same entity. Our approach to the coreference task stems from the observation that each group of coreferent noun phrases defines an equivalence class 1. Therefore, it is natural to view the problem as one of partitioning, or clustering, the noun phrases. Intuitively, all of the noun phrases used to describe a specific concept will be \"near\" or related in some way, i.e. their conceptual \"distance\" will be small. Given a description of each noun phrase and a method for measuring the distance between two noun phrases, a clustering algorithm can then group noun phrases together: Noun phrases with distance greater than a clustering radius r are not placed into the same partition and so are not considered coreferent. The subsections below describe the noun phrase 1The coreference 'relation is symmetric, transitive, and reflexive. John Simon, Chief Financial Officer of Prime Corp. since 1986, saw his pay jump 20%, to $1.3 million, as the 37-year-old also became the financial-services company's president.  representation, the distance metric, and the clustering algorithm in turn. Instance Representation Given an input text, we first use the Empire noun phrase finder (Cardie and Pierce, 1998) Each noun phrase in the input text is then represented as a set of 11 features as shown in Table 1 . This noun phrase representation is a first approximation to the feature vector that would be required for accurate coreference resolution. All feature values are automatically generated and, therefore, are not always perfect. In particular, we use very simple heuristics to approximate the behavior of more complex feature value computations: Individual Words. The words contained in the noun phrase are stored as a feature. Head noun. The last word in the noun phrase is considered the head noun.  Appositive. Here we use a simple, overly restrictive heuristic to determine whether or not the noun phrase is in a (post-posed) appositive construction: If the noun phrase is surrounded by commas, contains an article, and is immediately preceded by another noun phrase, then it is marked as an appositive. Number. If the head noun ends in an 's', then the noun phrase is marked PLURAL; otherwise, it is considered SINGular. Expressions denoting money, numbers, or percentages are also marked as PLURAL. Proper Name. Proper names are identified by looking for two adjacent capitalized words, optionally containing a middle initial. Semantic Class. Here we use WordNet (Fellbaum, 1998) Distance Metric Next, we define the following distance metric between two noun phrases: dist(NPi, NPj) ---- ~ f e F W f * incompatibility f ( N P i, N P y ) where F corresponds to the NP feature set described above; incompatibilityf is a function that returns a value between 0 and l inclusive and indicates the degree of incompatibility of f for NPi and NPj; and wf denotes the relative importance of compatibility w.r.t, feature f. The incompatibility functions and corresponding weights are listed in Table 2 . 2 In general, weights are chosen to represent linguistic knowledge about coreference. Terms with a weight of cc represent filters that rule out impossible antecedents: Two noun phrases can never corefer when they have incompatible values for that term's feature. In the current version of our system, the Terms with a weight of r --the clustering radius threshold --implement a preference that two NPs not be coreferent if they are incompatible w.r.t. that term's feature. As will be explained below, however, two such NPs can be merged into the same equivalence class by the clustering algorithm if there is enough other evidence that they are similar (i.e. there are other, coreferent noun phrase(s) sufficiently close to both). All other terms obtain weights selected using the development corpus. Although additional testing 2Note that there is not currently a one-to-one correspondence between NP features and distance metric terms: The distance metric contains two terms that make use of the WORDS feature of the noun phrase representation.  When computing a sum that involves both oc and -c~, we choose~the more conservative route, and the oc distance takes precedence (i.e. the two noun phrases are not considered coreferent). An example of where this might occur is in the following sentence: [1 Reardon Steel Co.] manufactures several thousand tons of [2 steed each week. Here, NP1 subsumes NP2, giving.them a distance of -c<) via the word substring term; however, NPi's semantic class is COMPANY, and NP2's class is OB-JECT, generating a distance of cx) via the semantic class feature. Therefore, dist(NP1,NP2) = oc and the two noun phrases are not considered coreferent. The coreference distance metric is largely contextindependent in {hat it determines the distance between two noun ~ phrases using very little, if any, of their intervening or surrounding context. The clustering algorithm described below is responsible for coordinating these local coreference decisions across arbitrarily long contexts and, thus, implements a series of context-dependent coreference decisions. Clustering Algorithm The clustering algorithm is given in Figure 2 . Because noun phrases generally refer to noun phrases that precede them, we start at the end of the document and work backwards. Each noun phrase is compared to all preceding noun phrases. If the distance between two noun phrases is less than the clustering radius r, then their classes are considered for possible merging. Two coreference equivalence classes can be merged unless there exist any incompatible NPs in the classes to be merged. It is useful to consider the application of our algorithm to an excerpt from a document: 3 . Initially, NP1, NP2, and NP3 are all singletons and belong to coreference classes cl, c2, and c3, respectively. We begin by considering NP3. Dist(NP2,NP3) = oc due to a mismatch on gender, so they are not considered for possible merging. Next, we calculate the distance from NP1 to NP3. Pronouns are not expected to match when the words of two noun phrases are compared, so there is no penalty here for word (or head noun) mismatches. The penalty for their difference in position is dependent on the length of the document. For illustration, assume that this is less than r. Thus, dist(NP1,NP3) < r.  2. Return TRUE. Figure 2 : Clustering Algorithm The algorithm then considers NP2. Dist(NPi,NP2) = 11.0 plus a small penalty for their difference in position. If this distance is > r, they will not be considered coreferent, and the resulting equivalence classes will be: {{The chairman, he}, {Ms. White}}. Otherwise, the distance is < r, and the algorithm considers cl and c~ for merging. However, cl contains NP3, and, as calculated above, the distance from NP2 to NP3 is oc. This incompatibility prevents the merging of c~ and c2, so the resulting equivalence classes would still be {{The chairman, he}, {Ms. White}}. In this way, the equivalence classes grow in a flexible manner. In particular, the clustering algorithm automatically computes the transitive closure of the coreference relation. For instance, if dist(NPi,NPj) < r and dist(NPj,NPk) < r then (assuming no incompatible NPs), NPi, NPj, and NPk will be in the same class and considered mutually coreferent. In fact, it is possible that dist(NPi, NPk) > r, according to the distance measure; but as long as that distance is not c~, NPi can be in the same class as NPk. The distance measure operates on two noun phrases in isolation, but the clustering algorithm can and does make use of intervening NP information: intervening noun phrases can form a chain that links otherwise distant NPs. By separating context-independent and context-dependent computations, the noun phrase representation and distance metric can remain fairly simple and easily extensible as additional knowledge sources are made available to the NLP system for coreference resolution. Evaluation We developed and evaluated the clustering approach to coreference resolution using the \"dry run\" and \"formal evaluation\" MUC-6 coreference cotpora. Each corpus contains 30 documents that have been annotated with NP coreference links. We used the dryrun data for development of the distance measure and selection of the clustering radius r and reserved the formal evaluation materials for testing. All results are reported using the standard mensures of recall and precision or F-measure (which combines recall and precision equally). They were calculated automatically using the MUC-6 scoring program (Vilaln et al., 1995) . Table 4 summarizes our results and compares them to three baselines. For each algorithm, we show the F-measure for the dryrun evaluation (column 2) and the formal evaluation (column 4). (The \"adjusted\" results are described below.) For the dryrun data set, the clustering algorithm obtains 48.8% recall and 57.4% precision. The formal evaluation produces similar scores: 52.7% recall and 54.6% precision. Both runs use r = 4, which was obtained by testing different values on the dryrun corpus. Table 5 summarizes the results on the dryrun data set for r values from 1.0 to 10.0. 3 As expected, increasing r also increases recall, but decreases precision. Subsequent tests with different values for r on the formal evaluation data set also obtained optimal performance with r= 4. This provides partial support for our hypothesis that r need not be recalculated for new corpora. The remaining rows in Table 4 show the performance of the three baseline algorithms. The first baseline marks every pair of noun phrases as coreferent, i.e. all noun phrases in the document form one class. This baseline is useful because it establishes an upper bound for recall on our clustering algorithm (67% for the dryrun and 69% for the formal evaluation). The second baseline marks as coreferent any two noun phrases that have a word in common. The third baseline marks as coreferent any two noun phrases whose head nouns match. Although the baselines perform better one might expect (they outperform one MUC-6 system), the clustering algorithm performs significantly better. In part because we rely on base noun phrases, our aNote that r need not be an integers especially when the distance metric is returning non-integral values. Algorithm ] Dryrun Data Set Official Adjusted The lack of discourse-related topic and focus information also limits System performance. In addition, we currently make no special attempt to handle reflexive pronouns and pleonastic \"it\". Lastly, errors arise from the greedy nature of the clustering algorithm. Noun phrase NPj is linked to every preceding noun phrase NP~ that is compatible and within the radius r, and that link can never be undone. We are considering three possible ways to make the algorithm less aggressively greedy. First, for each NPj, instead of considering every previous noun phrase, the algorithm could stop on finding the first compatible antecedent. Second, for each NPj, the algorithm could rank all possible antecedents and then choose the best one and link only to that one. Lastly,: the algorithm could rank all possible coreference links (all pairs of noun phrases in the document) and then proceed through them in ranked order, thus progressing from the links it is most confident about to those it is less certain of. Future work will include a more detailed error analysis. Related Work Existing systems for noun phrase coreference resolution can be broadly characterized as learning and non-learning approaches. All previous attempts to view coreference as a learning problem treat coreference resolution as a classification task: the algorithms classify a pair of noun phrases as coreferent or not. Both MLR (Aone and Bennett, 1995) NPi and NPj as coreferent, and NPj and NPk as coreferent, but NPi and NPk as not coreferent. In an evaluation on the MUC-6 data set (see Table 6 ), RESOLVE achieves an F-measure of 47%. The MUC-6 evaluation also provided results for a large number of non-learning approaches to coreference resolution. Table 6 provides a comparison of our results to the best and worst of these systems. Most implemented a series of linguistic constraints similar in spirit to those employed in our system. The main advantage of our approach is that all constraints and preferences are represented neatly in the distance metric (and radius r), allowing for simple modification of this measure to incorporate new 4Whether or not clustering can be considered a \"learning\" approach is unclear. The algorithm uses the existing partitions to process each successive NP, but the partitions generated for a document are not useful for processing subsequent documents. OWe do use training data to tune r, but as noted above, it is likely that r need not be recalculated for new corpora. There is also a growing body of work on the narrower task of pronoun resolution. Azzam et al. (1998) , for example, describe a focus-based approach that incorporates discourse information when resolving pronouns. Lappin and Leass (1994) make use of a series of filters to rule out impossible antecedents, many of which are similar to our ooincompatibilities. They also make use of more extensive syntactic information (such as the thematic role each noun phrase plays), and thus require a fuller parse of the input text. Ge et al. (1998) present a supervised probabilistic algorithm that assumes a full parse of the input text. Dagan and Itai (1991) present a hybrid full-parse/unsupervised learning approach that focuses on resolving \"it\". Despite a large corpus (150 million words), their approach suffers from sparse data problems, but works well when enough relevant data is available. Lastly, Cardie (1992a; 1992b) presents a case-based learning approach for relative pronoun disambiguation. Our clustering approach differs from this previous work in several ways. First, because we only require the noun phrases in any input text, we do not require a fifll syntactic parse. Although we would expect increases in performance if complex noun phrases were used, our restriction to base NPs does not reflect a limitation of the clustering algorithm (or the distance metric), but rather a self-imposed limitation on the preprocessing requirements of the approach. Second, our approach is unsupervised and requires no annotation of training data, nor a large corpus for computing statistical occurrences. Finally, we handle a wide array of noun phrase coreference, beyond just pronoun resolution. 6 Conclusions and Future Work We have presented a new approach to noun phrase coreference resolution that treats the problem as a clustering task. In an evaluation on the MUC-6 coreference resolution data set, the approach achieves very promising results, outperforming the only other corpus-based learning approach and producing recall and precision scores that place it firmly between the best and worst coreference systems in the evaluation. In contrast to other approaches to coreference resolution, ours is unsupervised and offers several potential advantages over existing methods: no annotated training data is required, the distance metric can be easily extended to account for additional linguistic information as it becomes available to the NLP system, and the clustering approach provides a flexible mechanism for combining a variety of constraints and preferences to impose a partitioning on the noun phrases in a text into coreference equivalence classes. Nevertheless, the approach can be improved in a number of ways. Additional analysis and evaluation on new corpora are required to determine the generality of the approach. Our current distance metric and noun phrase instance representation are only first, and admittedly very coarse, approximations to those ultimately required for handling the wide variety of anaphoric expressions that comprise noun phrase coreference. We would also like to make use of cues from centering theory and plan to explore the possibility of learning the weights associated with each term in the distance metric. Our methods for producing the noun phrase feature vector are also overly simplistic. Nevertheless, the relatively strong performance of the technique indicates that clustering constitutes a powerful and natural approach to noun phrase coreference resolution. Acknowledgments This work was supported in part by NSF Grant IRI-9624639 and a National Science Foundation Graduate fellowship. We would like to thank David Pierce for his formatting and technical advice.",
    "abstract": "This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. It differs from existing methods in that it views corererence resolution as a clustering task. In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%~ placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem. The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "214",
    "year": "1999",
    "month": "",
    "title": "Noun Phrase Coreference as Clustering"
}