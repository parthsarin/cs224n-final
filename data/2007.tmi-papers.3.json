{
    "article": "Orthographic variation can be a serious problem for many natural language-processing applications. Japanese in particular contains orthographic variation, because the large quantity of transliteration from other languages causes many possible spelling variations. To manage this problem, this paper proposes a support vector machine (SVM)-based classifier that can determine whether two terms are equivalent. We automatically collected both positive examples (sets of equivalent term pairs) and negative examples (sets of inequivalent term pairs). Experimental results yielded high levels of accuracy (87.8%), demonstrating the feasibility of the proposed approach. Introduction Orthographic variation can be a serious problem for many natural language-processing (NLP) applications, such as information extraction (IE), question answering (QA), and machine translation (MT). For example, many example-based machine translation (EBMT) (Nagao, 1984) methods, such as (Somers, 1999; Richardson et al., 2001; Sumita, 2001; Carl and Way, 2003; Aramaki and Kurohashi, 2004; Nakazawa et al., 2006) , utilize a translation dictionary during bilingual text alignment. Also, several statistical machine translation (SMT) (Brown et al., 1993) methods set initial translation parameters using a translation dictionary. When consulting a dictionary, a system must disambiguate orthographic variation. The following terms are an example of Japanese orthographic variation, corresponding to the term \"Avogadro's number\": 1. \u30a2\u30f4\u30a9\u30ac\u30c9 \u30ed\u6570 (A VO GA DO RO SU), 2. \u30a2\u30dc\u30ac\u30c9 \u30ed\u6570 (A BO GA DO RO SU). Although both terms are frequently used (term (1) resulted in 25,700 Google hits and Term (2) resulted in 25,000 Google hits 1 ), translation dictionaries contain only one of the terms, resulting in low levels of accuracy with dictionary-based bilingual text alignment. This paper focuses on Japanese orthographic disambiguation. Japanese orthographic variance is closely related to transliteration, because transliteration relies on pronunciation, the great differences between the sounds made in Japanese and in Western languages (mainly English) results in a variety of possible spellings. Researchers have already proposed methods to solve this problem. For example, Knight(1998) developed a backtransliteration method using a probabilistic A BO GA DO RO SU A VO GA DO RO SU Avogadro's number T r a n s l i t e r a t i o n B a c kt r a n s l i t e r a t i o n T r a n s l i t e r a t i o n B a c k -t r a n s l i t e r a t i o n Orthographicdisambiguation Figure 1 : Transliteration and Orthographic Variation. model. Goto et al.(2004) also developed a probabilistic model, which takes into account surrounding context. Lin and Chen(2002) developed a perceptron learning algorithm for back-transliteration. While these methods differ, they all share the same goal: being able to back-transliterate a given term into another language. By contrast, this paper proposes a new task schema: given two Japanese terms, the system determines whether they are equivalent. Figure 1 illustrates our task schema; a foreign term can be transliterated into Japanese in several ways. While previous methods can yield suitable back-transliteration for a term, our system determines whether a pair of Japanese terms originates from the same foreign word. We expect our task-setting is more direct and practical for many applications, such as dictionary consulting in MT, IE, and so on. For this process, our proposed method uses a machine learning technique (support vector machine, hereafter SVM (Vapnik, 1999) ), which requires the two following types of data:  incorporates negative examples. Both examples can be generated automatically from translation dictionaries using spelling similarity and heuristic rules. Experimental results yielded high accuracy (87.8%), demonstrating the feasibility of the proposed approach. Although we investigated the performance in the medical terms, the proposed method does not depend on the target domain. Section 2 of this paper describes how training data are built. Section 3 describes the learning method, and Section 4 presents the experimental results. Section 5 discusses related work, and Section 6 presents our conclusions. Automatic Example Building This section describes how training data are built; Section 2.1 discusses positive examples, and Section 2.2 discusses negative examples. Note that the latter is a novel task. Positive Examples Our method uses a standard approach to extract positive examples. The basic idea is that orthographic variants should (1) have similar spelling, and (2) share the same English translation. The method consists of the following two steps: STEP 1: First, using two or more translation dictionaries, we extract a set of Japanese terms with the same English translation. STEP 2: Then, for each extracted set, we generate possible two term pairs (term \uf731 and term \uf732 ), and calculate the spelling similarity between them. Spelling similarity is measured using the following edit-distance based similarity SIM (term \uf731 , term \uf732 ): SIM (term \uf731 , term \uf732 ) = 1 \u2212 EditDistance(term \uf731 , term \uf732 ) \u00d7 \uf732 len(term \uf731 ) + len(term \uf732 ) , where len(term \uf731 ) is the length (the number of characters) of term \uf731 , len(term \uf732 ) is the length (the number of characters) of term \uf732 , EditDistance(term \uf731 , term \uf732 ) is the minimum number of point mutations required to change term \uf731 into term \uf732 , where a point mutation is one of: (1) a change in a character, (2) the insertion of a character, and (3) the deletion of a character. For details, see (Levenshtein, 1965) . Any term pair with more than a threshold (T H) similarity is considered a positive example 2 . Negative Examples As mentioned in Section 1, generating negative examples is a novel process in this field. One simple way is to select two words from a dictionary randomly. However, such a simple method would generate a huge quantity of meaningless examples. Therefore, as in our collection of positive examples, we collected only term pairs with similar spellings. Another problem is a balance of the example quantity. In the preliminary experiments, the number of negative examples was about three times as the number positive examples, leading to a negative bias. Therefore, we investigated the Google hits of each term pair by using a query, such as \" \u30a2\u30f4\u30a9\u30ac\u30c9 \u30ed\u6570 \u30a2\u30dc\u30ac\u30c9 \u30ed\u6570\". Then, we utilize only negative examples with many Google hits, and reject low-hits examples, because of the following two reasons: 1. Popularity: We expect that a more popular term pair is more informative. Reliability: We hypothesize that an orthographic pair rarely appears in one document, because one document usually has an orthographic consistency. Therefore, we can expect that if two terms cooccur in one document, they are not orthographic variants, ensuring reliability for negative examples. The detailed steps are as follows: STEP 1: First, using two or more translation dictionaries, we extract a set of Japanese terms with different English translations. STEP 2: Then, for each extracted set, we generate possible pairs, and calculate the spelling similarity between them. Any term pair exceeding a threshold (T H) similarity is considered a negative example candidate. STEP 3: Finally, we investigate the Google hits for each candidate. We only use the top K-hits candidates as negative examples 3 . Leaning Method Application of the method described in Section 2 yields training data, consisting of triple expressions < term \uf731 , term \uf732 , +\uf731/ \u2212 \uf731 >, in which \"+1\" indicates a positive example (orthographic variants), and \"-1\" indicates a negative example (different terms). Table 1 provides some examples. The next problem is how to convert training data into machine learning features. We regard the different parts and context (window size \u00b11) as features: 1. Diff: differing characters between two translations; 2. Pre-context: previous character of Diff; and 3. Post-context: subsequent character of Diff. Figure 2 provides examples of these features. Since the different part is a gray area (\"VO(\u30f4\u30a9)\" and \"BO(\u30dc)\"), we consider Diff to be \"VO:BO (\u30f4\u30a9:\u30dc)\" itself, Precontext to be \"A (\u30a2)\" in a dotted box, and Post-context to be \"GA (\u30ac)\" also in a dotted box. Figure 3 provides another example; the insertion/deletion of a character can be considered the Diff using \u03c6, such as \" \u03c6 :A ( \u03c6 :\u30fc)\". In addition, the start ( SOT ) or end ( EOT ) of a term can be considered a character. Note that both Pre-context and Postcontext consist of one character pair, while the Diff can be a pair of n : m characters (n \u2265 0, m \u2265 0). In learning, we can use a back-off technique to prevent problems related to data sparseness. As a result, each different point utilizes the following four features: \u2022 Diff + Pre-context + Post-context \u2022 (1-back-off-a) Diff + Pre-context \u2022 (1-back-off-b) Diff + Post-context \u2022 (2-back-off) Diff Figure 4 presents some examples. Experiments Test-set To evaluate the performance of our system, we manually built a test-set as follows: First, we extracted 5,013 similar spelling term pairs, that have more than (SIM > 0.8), from two dictionaries (Nanzando, 2001b) , (Ito et al., 2003) . Then, for each pair, we annotated whether it is an equivalent pair (orthographic variants) or not (different terms). Finally, we randomly extracted 883 pairs form it. We regard it as a test-set. Training-set By using the proposed method (in Section 2), we automatically built a training-set from two translation dictionaries (Japan Medical Terminology English-Japanese (Nanzando, 2001a) For SVM learning, we used TinySVM 4 with a linear kernel 5 . Evaluation To evaluate our method, we used three measures, precision, recall and accuracy, defined 4 http://chasen.org/ taku/software/TinySVM/ 5 Although we tried a polynomial kernel and an RBF kernel, their performance are almost equal to a linear kernel. as follows: P recision = # of pairs found and correct total # of pairs found , Recall = # of pairs found and correct total # of pairs correct , Accuracy = # of pairs correct total # of pairs in test-set . Results First, we checked the performance of ED-ITDISTANCE(TH) in various T H values. Figure 5 presents the results. While the precision is basically proportional to the spelling similarity (T H), it drops down in the high T H (T H 0.96), indicating a highly similar spelling term pair not always have to be the orthographic variants. Table 2 presents the performance of all methods. AUTOMATIC did not obtain a higher accuracy than BYHAND, the combination of them is the highest accuracy, demonstrating the basic feasibility of our approach. The precision-recall graph (Figure 6 ) also shows the advantage of COMBINATION . Error Analysis We investigated the errors from COMBINA-TION, and found that many errors came from a verbal omission, which is different phenomenon from transliteration. For example, a test-set has the following positive example: 1. \u30ab\u30eb\u30b7\u30a6\u30e0\u30fb\u30c1\u30e3\u30cd\u30eb (calcium channel; KA RU SI U MU CHA NE RU), 2. \u30ab\u30eb\u30b7\u30a6\u30e0\u30a4\u30aa\u30f3\u30fb\u30c1\u30e3\u30cd\u30eb (calcium ion channel; KA RU SI U MU I O N CHA NE RU). Because a term \"ion\" is without saying inferable in this case, it can be omitted. Capturing such an operation requires a very high level of understanding of the meaning of the terms. To focus on a transliteration problem, we manually removed such examples from our test-set, and built a sub-set of it, consisting of only transliterations. The result is shown in Table 3 . The accuracy of COMBINATION is higher than 90%. It is difficult to compare this accuracy to that of the previous studies because (1) their corpus were different from ours and (2) previous studies focused on back-transliteration. However, we can say that the present accuracy is, at least, not behind from the previous researchers (64% by (Knight and Graehl, 1998) and 87.7% by (Goto et al., 2004) ). We expect that the present accuracy is practical in many applications. Finally, we investigate the differences between AUTOMATIC and BYHAND results (the AUTOMATIC accuracy is much lower than the BYHAND by 8.5 points in Table 2 ). One of the reasons is dictionary specific styles, such as numerous expression variants (\" \"), hyphenation variants (\" \") and so on. Because the BYHAND training-set and the test-set came from the same dictionaries, BYHAND already knows such variants are meaningless differences. However, AU-TOMATIC, using different dictionaries, sometimes suffered from unseen number expression/hyphenation variants. Note that in transliteration accuracy (in Table 3 ), their accuracies (BYHAND and AU-TOMATIC) are not so different. Related Works As noted in Section 1, transliteration is the field most relevant to our work, because many orthographic variations come from borrowed words. Our proposed method differs from previous studies in the following three ways: (1) task setting, ( 2 ) negative examples, and (3) target scope. Task Setting Most previous studies have involved finding the most suitable back-transliteration of a term. For example, given an observed Japanese string o by optical character recognition (OCR) software, Knight and Graehl (1998) finds a suitable English word w. For this process, they developed a probabilistic model that decomposed a transliteration into suboperations as follows: P (w)P (e|w)P (j|e)P (k|j)P (o|k), where P (w) generates written English word sequences, P (e|w) pronounces English word sequences, P (j|e) converts English sounds into Japanese sounds, P (k|j) converts Japanese sounds to KATAKANA writing, and P (o|k) introduces misspellings caused by OCR. While this method is phoneme-based, Bilac and Tanaka(2004) combined phoneme-based and graphme-based transliteration. Goto et al.(2004) proposed a similar method, utilizing the surrounding context. Such methods are not only applicable to Japanese; it can also be used for Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007) , Chinese (Li et al., 2007) , Persian (Karimi et al., 2007) . The task-setting involved in our method differs from previous methods. Our methodology involves determining whether two terms in the same language are equivalent, making our task-setting more direct and suitable than previous methods for many applications, such as dictionary consulting in MT and information retrieval. Note that Yoon et al.( 2007 ) also proposed a discriminative transliteration method, but their system determines whether a target term is transliterated from a source term or not. Negative Examples Our task setting requires negative examples, consisting of term pairs with similar spellings, but different meanings. By contrast, previous research involved only positive examples. For example, Masuyama et al.(2004) collected 178,569 Japanese transliteration variants (positive examples) from large corpora. However, they paid little attention to negative examples. Target Scope As mentioned above, orthographic variation in Japanese results mainly from transliteration. However, our target includes several different phenomena, such as verbal omissions mentioned in Section 4.6. Although the accuracy for omissions is not enough, our method addresses it easily, while previous methods are unable to handle this kind of phenomenon. Conclusion In this paper, we proposed a SVM-based orthographic disambiguation method. We also proposed a method for collecting both positive and negative examples. Experimental results yielded high levels of accuracy (87.8%), demonstrating the feasibility of the proposed approach. Acknowledgments Part of this research is supported by Grantin-Aid for Scientific Research of Japan Society for the Promotion of Science (Project Number:16200039, F.Y. 2004-2007 and 18700133,  F.Y.2006-2007)  and the Research Collaboration Project (#047100001247) with Japan Anatomy Laboratory Co.Ltd.",
    "funding": {
        "defense": 0.0,
        "corporate": 1.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article mentions support from Grant-in-Aid for Scientific Research of Japan Society for the Promotion of Science and a Research Collaboration Project with Japan Anatomy Laboratory Co.Ltd. The Japan Society for the Promotion of Science is a research agency, and Japan Anatomy Laboratory Co.Ltd. represents corporate funding."
}