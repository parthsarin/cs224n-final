{
    "article": "The merit of phrase-based statistical machine translation is often reduced by the complexity to construct it. In this paper, we address some issues in phrase-based statistical machine translation, namely: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit. We present Level-Of-Detail (LOD) approach, an agglomerative approach for learning phrase-level alignment. Our experiments show that LOD approach significantly improves the performance of the word-based approach. LOD demonstrates a clear advantage that the phrase translation table grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches. Introduction Early approach to statistical machine translation relies on the word-based translation model to describe the translation process [1] . However, the underlying assumption of word-to-word translation often fails to capture all properties of the language, i.e. the existence of the phrase where a group of words often function together as a unit. Many researchers have proposed to move from the word-based to the phrase-based translation model [2] [3] [4] . A phrase-based approach offers many advantages as a phrase translation captures word context and local reordering inherently [3] . It has become popular in statistical machine translation applications. There are typically two groups of approaches to constructing the phrasebased model. The first group learns phrase translation directly from the sentence pair. It learns both word and phrase units simultaneously. Although these approaches appear intuitive, it usually suffers from a prohibitive computational cost. It might have to consider all possible multi-word sequences as phrase candidates and all possible pairings as phrase translations at the same time. The second group of approaches learns phrase translations through word-level alignment: alignment template [2] and projection extension [6] , just to name a few. In general, these approaches take the word-level alignment, a by-product of the word-based translation model, as their input and then utilize a heuristic measurement to learn the phrase translation. The heuristic measurement contains all possible configurations of word-level alignment on a phrase translation. It is noted that the underlying word-level alignment is just an approximation to the exact alignment. The approximation is reflected by a probability produced by the word-based translation model. The majority of approaches do not make use of this probability, whereas it may provide a valuable clue leading to a better phrase translation from a statistical point of view. Koehn, et. al [8] compared the representative of both groups and reported that learning phrase translation using a simple heuristic from word alignment yields a better translation performance than learning phrase translation directly from the sentence pair. Many approaches try to learn all phrase translations in one step, either directly from the sentence pair or through word alignment. As a result, they may encounter a huge amount of phrase translation candidates at once. Usually, they limit the maximum phrase length to reduce the choice of candidates. Although this method is sufficient to satisfy the computational requirement, it comes with the cost of not finding the good phrases longer than the imposed limit. Additionally, to reduce the candidates, those approaches use a threshold to separate good phrase translation from the rest. The threshold is ad-hoc and often not capable of making a clear separation. Therefore, the use of threshold often comes with the cost of the inclusion of undesired phrase translations and the absence of good phrase translations in the phrase translation table. The cost may be reflected from the size of the phrase translation table that often grows almost linearly over the phrase length limit [6] [8] . The growth implies a non-intuitive behavior: two phrases with different length introduce an equal number of additional entries to the phrase translation table. As longer phrases occur less often, there should be fewer entries introduced into the phrase translation table. We propose an agglomerative approach to learn phrase translations. Our approach is motivated by the second group, which is to learn phrase translation through word-alignment, while addressing the common issues: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit. Only a few approaches move away from one-step learning. Melamed [13] presented an agglomerative approach to learn the phrases progressively from a parallel corpus by using sub-phrase bigram statistics. Moore [14] proposed a similar approach which identifies the phrase candidates by parsing the raw training data. Our idea differs from these approaches in that we look into the association of the alignments rather than the association of the words to discover the phrases. In this paper, we propose the Level of Detail (LOD) approach for learning of phrase translations in phrase-based statistical machine translation. Section 2 discusses the background and motivation and then formulates the LOD approach while section 3 describes the learning process in details. Section 4 describes the experimental results. In this section, we compare LOD with state-of-the-art word-based approach in translation tasks. Finally, section 5 concludes this paper by providing some discussion in comparison with other related works. Statistical Machine Translation: A Level of Detail Motivation and Background It is often not intuitive to model the translation of a phrase using the word-based translation model. First, the literal translation of phrase constituents is often inappropriate from a linguistic point of view. The word-based translation model treats a phrase as a multi-word. One such example is the case where a phrase appears as an idiom. The translation of an idiom cannot be synthesized from the literal translation of its constituents but rather from the semantic translation of the whole. Besides, the literal translation of an idiom detracts from the intended meaning. In one such example, the literal translation of French \"manger sur le pouce\" is \"to eat on the thumb\". This detracts from the correct translation \"to grab a bite to eat \". In addition, to produce the correct translation, the word-based translation model might have to learn that \"manger \" is translated as \"eat \" or \"pouce\" is translated as \"thumb\". Although it may serve the translation purpose, it will introduce many non-intuitive entries to the dictionary. Second, even if it is possible to translate a phrase verbatim, modeling phrase translation using the word-based translation model suffers from a disadvantage: the number of word alignments required to synthesize the phrase translation is large. It requires four word alignments to model the translation between \"une minute de silence\" and \"one minute of silence\", whereas one phrase alignment is adequate. The introduction of more alignments also implies the requirement to estimate more parameters for the translation model. The implication often comes with the cost of learning wrong word alignments. Third, a phrase often constitutes some spurious words. The word-based translation model often has trouble in modeling spurious words, such as function words. Function words may appear freely in any position and often may not be translated to any word. We observe that many of these function words appear inside a phrase. It is beneficial to realize these spurious words inside a phrase unit so as to improve statistical machine translation performance and also to remove the necessity to model them explicitly. All these suggest that, ideally, a phrase translation should be realized as a phrase alignment, where the lexical correspondence is established on phrase level rather than on its word constituents. The discussion above suggests that phrase-based translation is a wise choice. Practically, as a phrase is not a well defined lexical entry, a mechanism is needed to judge what constitutes a phrase in the context of statistical machine translation. In this paper, we advocate an approach to look into the phrase discovery process at different level of details. The level of detail refers to the size of a phrase unit. At its finest level of detail, a phrase translation uses the word-based translation model where a phrase is modeled through its word constituent. At a coarser level of detail, a sub-phrase unit is introduced as a sequence of words, making it a constituent of the phrase. The coarsest level of detail refers to the status of a phrase where all word constituents converge into a whole unit. Our Level-Of-Detail (LOD) approach views the problem of phrase-based translation modeling through a LOD process. It starts from the finest wordlevel alignment and transforms the phrase translation into its coarsest level of detail. Formulation Let < e, f > be a sentence pair of two sequences of words with e as an English sentence and f as its translation in French 1 . Let < \u1ebd, f > represents the same sentence pair but with the phrase as its atomic unit rather than the word. To generalize the notation, we treat word and phrase unit similarly by considering a word as a phrase of length one. Therefore, < e, f > hereafter will be referred as < \u1ebd, f > (0) , which represents the finest level of detail, and < \u1ebd, f > as < \u1ebd, f > (N ) , which represents the coarsest level of detail. Let each tuple in the sentence pair of any level of detail n, < \u1ebd, f > (n) be \u1ebd(n) = {\u1ebd (n) 0 , \u1ebd(n) 1 , . . . , \u1ebd(n) i , . . . , \u1ebd(n) l (n) } and f(n) = { f (n) 0 , f (n) 1 , . . . , f (n) j , . . . , f (n) m (n) } where \u1ebd(n) 0 , f (n) 0 represent the special token N U LL as suggested in [1] and l (n) ,m (n) represent the length of the corresponding sentence. Let T (n) be a set of alignment defined over the sentence pair < \u1ebd, f > (n)  with t (n) ij = [\u1ebd (n) i , f (n) j ] as its member. The superscript in all notations denotes the level of detail where 0 represents the finest and N represents the coarsest level of detail. LOD algorithm iteratively transforms < \u1ebd, f > (0) to < \u1ebd, f > (N ) through re-alignment of phrases and re-estimation of phrase translation probability. At n-th iteration, LOD harvests all bi-directional alignments from the sentence pair < \u1ebd, f > (n) . The alignment is obtained by a typical word-based translation model, such as the IBM model, while treating a sub-phrase at n-th iteration as a word. We refer to those alignments as B (n) , a pool of sub-phrase alignments unique to the particular iteration. Afterwards, LOD generates all possible phrase alignment candidates C (n) for a coarser level of detail from these sub-phrase alignments. A resulting phrase alignment candidate is basically a joining of two adjacent sub-phrase alignments subject to a certain criterion. It represents the future coarser level alignment. Up to this point, two sets of alignment are obtained over< \u1ebd, f > (n) : a pool of sub-phrase alignments B (n) at the current level and a pool of phrase alignment candidates C (n) at a coarser level. From these two sets of alignments B (n) \u222a C (n) , we would like to derive a new set of alignments T (n+1) that best describes the training corpus with the re-estimated statistics obtained at n-th iteration. LOD constructs < \u1ebd, f > (n+1) from the new set of alignment. Algorithm 1 provides the general overview of LOD algorithm. Algorithm 1. An overview of LOD approach in learning phrase translation. The LOD approach takes a sentence pair at its finest level of detail as its input, learns the phraselevel alignment iteratively and outputs the same sentence pair at its coarsest level of detail along with its phrase translation table. input \u1ebd, f (0) for n = 0 to (N \u2212 1) do -Generate bi-directional sub-phrase level alignments B (n) from \u1ebd, f (n) -Identify phrase-level alignment candidates C (n) from B (n) -Estimate the alignment probability in B (n) and C (n) -Learn coarser level alignment T (n+1) from B (n) \u222a C (n) and construct \u1ebd, f (n+1) output \u1ebd, f (N) and T (N) Learning Phrase Translation In this section, we discuss the steps of LOD algorithm in detail. As presented in Algorithm 1, moving from one level of alignment to its coarser level, LOD follows four simple steps: 1. Generation of bi-directional sub-phrase level alignments 2 2. Identification of phrase level alignment candidates 3. Estimation of alignment probability 4. Learning coarser level alignment Generation of Bi-directional Sub-phrase Level Alignments LOD follows the common practice to utilize the IBM translation model for learning the phrase translation. That is to harvest all alignments from both translation directions. For the sake of clarity, LOD defines the following notation for these alignments, as follows: Let \u0393 (n) ef : \u1ebd(n) i \u2212\u2192 f (n) j be an alignment function represents all alignments from translating the source English sentence to the target French sentence, and \u0393 (n) fe : f (n) j \u2212\u2192 \u1ebd(n) i be the reversed translation direction. Then, bi-directional sub-phrase alignment B (n) includes all possible alignment by both functions: B (n) = {t (n) ij = [\u1ebd (n) i , f (n) j ]|(\u0393 (n) ef (\u1ebd (n) i ) = f (n) j ) \u222a (\u0393 (n) fe ( f (n) j ) = \u1ebd(n) i )} Let us denote N U LL alignments, N (n) , a subset of alignments in B (n) in which the special token N U LL is involved. Identification of Phrase Alignment Candidates LOD applies a simple heuristic to identify a phrase alignment candidate. First, LOD considers every combination of two distinct sub-phrase alignments and assesses its candidacy. Here, we define a phrase alignment candidate < t (n) ij , t (n) i j >\u2208 C (n) as follows: Let < t (n) ij , t (n) i j > be a set of two tuples, where t (n) ij \u2208 B (n) and t (n) i j \u2208 B (n) . Then < t (n) ij , t (n) i j > is a phrase aligment candidate if and only if 1. not ((i, i ) = 0) or (|i \u2212 i | = 1) 2. not ((t (n) ij \u2208 N (n) ) and (t (n) i j \u2208 N (n) )) In the definition above, the first clause defines a candidate as a set of two whose source sub-phrases are adjacent. The second clause forbids the consideration of two N U LL alignments. As LOD considers only two alignments for each phrase alignment candidate, it implies that, at the n-th iteration, the length of the longest possible phrase is bounded by 2 n . Apparently, we do not have to examine sub-phrase alignment trunks of more than two sub-phrases because the iteration process guarantees LOD to explore phrases of any length given sufficient iteration. This way, the search space at each iteration can be manageable at each iteration. Estimation of Alignment Probability Joining the alignment set B (n) derived in Section 3.1 and the coarser level alignment C (n) derived in Section 3.2, we form a candidate alignment set B (n) \u222a C (n) . Assuming that there are two alignments x \u2208 B (n) , y \u2208 B (n) , and a candidate alignment < x, y >\u2208 C (n) , we derive the probability p(x) and p(y) from the statistics as the count of x and y normalized by the number of alignments in the corpus, and we derive the joint probability p(< x, y >) in a similar way. If there is a genuine association between the two alignments, x and y, then we expect that p(< x, y >) p(x)p(y). If there is no interesting relationship between x and y, then p(< x, y >) \u2248 p(x)p(y) where we say that x and y are independent. If x and y are in a complementary relationship, then we expect to see that p(< x, y >) p(x)p(y). These statistics allow us to discover a genuine sub-phrase association. The probability is estimated by the count of observed events normalized by the corpus size. Note that the alignment from the IBM translation model is derived using a Viterbi-like decoding scheme. Each observed event is counted as one. This is referred to as hard-counting. As the alignment is done according to probability distribution, another way of counting the event is to use the fractional count that can be derived from the translation model. We refer to it as softcounting. Learning a Coarser Level Alignment From section 3.1 to 3.3, we have prepared all the necessary alignments with their probability estimates. The next step is to re-align < \u1ebd, f > (n) into < \u1ebd, f > (n+1) using alignment phrases in B (n) \u222a C (n) with their newly estimated probability distribution. The re-alignment is considered as a constrained search process. Let p(t (n) ij ) be the probability of a phrase alignment t (n) ij \u2208 (B (n) \u222a C (n) ) as defined in Section 3.3, T (n) be the potential new alignment sequence for < \u1ebd, f > (n) , we have the likelihood for T (n) as log P (< \u1ebd, f > (n) |T (n) ) = t (n) ij \u2208T (n) log p(t (n) ij ) ( 1 ) The constrained search is to decode an alignment sequence that produces the highest likelihood possible in the current iteration, subject to the following constraints: 1. to preserve the phrase ordering of the source and target languages 2. to preserve the completeness of word or phrase coverage in the sentence pair 3. to ensure the mutual exclusion between alignments (except for the special N U LL tokens) The constrained search can be formulated as follows: T (n+1) = argmax \u2200T (n) log P (< \u1ebd, f > (n) |T (n) ) ( 2 ) In Eq.( 2 ), we have T (n+1) as the best alignment sequence to re-align sentence pair < \u1ebd, f > (n) to < \u1ebd, f > (n+1) . The constraints are to ensure that the search leads to a valid alignment result. The search is essentially a decoding process, which traverses the sentence pair along the source language and explores all the possible phrase alignments with the target language. In practice, LOD tries to find a phrase translation table that maximizes Eq.( 2 ) as formulated in Algorithm 2. As the existing alignment for < \u1ebd, f > (n) in the n-th iteration is a valid alignment subject to three Algorithm 2. A stack decoding algorithm to explore the best alignment path between source and target languages by considering all alignment candidates in B (n) \u222a C (n) at n-th iteration. 1. Initialize a lattice of l (n) slots for l (n) sub-phrase in source language. 2. Starting from i=1, for all phrases in source language ei; 1) Register all the alignments t (n) ij that map source phrases ending with ei, including ei itself, into slot i in the lattice; 2) Register the probability of alignment p(t (n) ij ) together with the alignment entry t (n) ij 3) Repeat 1) and 2) until i=l (n)  3. Apply stack decoding [15] process to find the top n-best paths subject to the three constraints. During the decoding processing, the extension of partial path is subject to a connectivity test to enforce the three constraints. 4. Output the top best alignment result as the final result. constraints, it also serves as one resolution to the search. In the worst case, if the constrained search can not discover any new alignment other than the existing one, then the existing alignment in the current iteration will stand through the next iteration. In Algorithm 2, we establish the lattice along the source language. In the case of English to French translation, we follow the phrases in the English order. However, it can be done along the target language as well since our approach follows a symmetric many-to-many word alignment strategy. This step ends with the promotion of all phrase alignment candidates in the best alignment sequence T (n+1) . The promotion includes the merging of the two sub-phrase alignments and the concerning sub-phrases. The merged unit will be considered as a unit in the next iteration. Experiments The objective of our experiments is to validate our LOD approach in machine translation task. Additionally, we are interested in investigating the following: the effect of soft-counting in probability estimation, and the behavior of LOD approach in every iteration, in terms of the length of the phrase unit and the size of the phrase translation table. We report all our experiments using BLEU metrics [10] . Furthermore, we report confidence intervals with 95% statistical significance level of each experiments, as suggested by Koehn [16] . We validate our approach through several experiments using English and French language pairs from the Hansard corpus. We restrict the sentence length to at most 20 words to obtain around 110 thousands sentence pairs. Then we randomly select around 10 thousands sentence pair as our own testing set. In total, the French corpus consists of 994,564 words and 29,360 unique words; while the English corpus consists of 1,055,167 words and 20,138 unique words. Our experiment is conducted on both English-to-French (e2f) and French-to-English (f2e) tasks under open testing set-up. We use these available tools: GIZA++ 3 for word-based IBM 4 model training and ISI ReWrite 4 for translation test. For measuring the BLEU score and deriving the confidence intervals, we use the publicly available tools 5 . Soft-Counting vs. Hard-Counting Table 1 summarizes our experiments in analyzing the effect of soft-counting and hard-counting in the probability estimation on the BLEU score. Case I demonstrates the BLEU score of the experiment using the underlying translation model probability or soft-counting, while Case II demonstrates the score of hard-counting. The experimental results suggest that the use of the underlying translation model probability is beneficial as it gives consistently higher BLEU scores in all the iterations. The comparison using paired bootstrap resampling [16] also confirms the conclusion. LOD Behavior over Iteration Table 2 summarizes the performance of our LOD approach for the first 10 iterations in comparison with the baseline IBM 4 word-based approach. The results show that the LOD approach produces a significant improvement over IBM 4 consistently. The first iteration yields the biggest improvement. We achieve an absolute BLEU score improvement of 5.01 for the English-to-French task and 5.48 for the French-to-English task from the first iteration. The subsequent improvement is obtained by performing more iterations and capturing longer phrase translation, however, the improvement gained is less significant compared to that of the first iteration. Table 2 also summarizes the maximum phrase length and the behavior of the phrase translation table: its size and its increment over iteration. It shows that the phrase length is soft-constrained by the maximum likelihood criterion in Eq. ( 2 ) rather than limited. As iteration goes on, longer phrases are learnt but their probabilities are less probable than shorter one. Consequently, longer phrases introduce fewer entries to the phrase translation table. Table 2 Discussion In this paper, we propose LOD approach to phrase-based statistical machine translation. The LOD approach addresses three issues in the phrase-based translation framework: the size of phrase translation table, the use of underlying translation model probability and the length of the phrase unit. In terms of the size of the phrase translation table, our LOD approach presents a sub-linear growth of the phrase translation table. It demonstrates a clear advantage over other reported attempts, such as in [6] [8] where the phrase translation table grows almost linearly over the phrase length limit. The LOD approach manages the phrase translation table size in a systematic way as a result of the incorporation of maximum likelihood criterion into the phrase discovery process. In terms of the use of underlying translation model probability, we propose to use soft-counting instead of hard-counting in the re-estimation processing of probability estimation. In the projection extension algorithm [6] , the phrases are learnt based on the presence of alignment in certain configurations. In alignment template [2] , two phrases are considered to be translation of each other, if the word alignments exist within the phrases and not to the words outside. Both methods are based on hard-counting of translation event. Our experiment results suggest the use of soft-counting. In terms of the length of the phrase unit, we move away from the window-like limit for phrase candidacy [4] [9] . The LOD approach is shown to be more flexible in capturing phrases of different length. It gradually explores longer phrases as iteration goes, leading any reasonable length given sufficient iteration as long as they are statistically credible. It is known that statistical machine translation relies very much on the training corpus. A larger phrase translation table means more training data are needed for the translation model to be statistically significant. In this paper, we successfully introduce the LOD approach to control the process of new phrase discovery process. The results are encouraging.",
    "abstract": "The merit of phrase-based statistical machine translation is often reduced by the complexity to construct it. In this paper, we address some issues in phrase-based statistical machine translation, namely: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit. We present Level-Of-Detail (LOD) approach, an agglomerative approach for learning phrase-level alignment. Our experiments show that LOD approach significantly improves the performance of the word-based approach. LOD demonstrates a clear advantage that the phrase translation table grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches.",
    "countries": [
        "Singapore"
    ],
    "languages": [
        "French",
        "English"
    ],
    "numcitedby": "9",
    "year": "2005",
    "month": "",
    "title": "Phrase-Based Statistical Machine Translation: A Level of Detail Approach"
}