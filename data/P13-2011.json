{
    "article": "Uncertainty text detection is important to many social-media-based applications since more and more users utilize social media platforms (e.g., Twitter, Facebook, etc.) as information source to produce or derive interpretations based on them. However, existing uncertainty cues are ineffective in social media context because of its specific characteristics. In this paper, we propose a variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets. We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification. Introduction Social media is not only a social network tool for people to communicate but also plays an important role as information source with more and more users searching and browsing news on it. People also utilize information from social media for developing various applications, such as earthquake warning systems (Sakaki et al., 2010) and fresh webpage discovery (Dong et al., 2010) . However, due to its casual and word-of-mouth peculiarities, the quality of information in social media in terms of factuality becomes a premier concern. Chances are there for uncertain information or even rumors flooding in such a context of free form. We analyzed a tweet dataset which includes 326,747 posts (Details are given in Section 3) collected during 2011 London Riots, and result reveals that at least 18.91% of these tweets bear uncertainty characteristics 1 . Therefore, distinguishing uncertain statements from factual ones is crucial for users to synthesize social media information to produce or derive reliable interpretations, and this is expected helpful for applications like credibility analysis (Castillo et al., 2011) and rumor detection (Qazvinian et al., 2011) based on social media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987) , the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that \"Uncertainty -in its most general sense -can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information\". In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004) . However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012) . Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain statements. But, neither of the uncertainty expressions can be represented based on the existing types of uncertainty defined in the literature. Therefore, a different uncertainty classification scheme is needed in social media context. In this paper, we propose a novel uncertainty classification scheme and construct the first uncertainty corpus based on social media data -tweets in specific here. And then we conduct experiments for uncertainty post identification and study the effectiveness of different categories of features based on the generated corpus. Related work We introduce some popular uncertainty corpora and methods for uncertainty identification. Uncertainty corpus Several text corpora from various domains have been annotated over the past few years at different levels (e.g., expression, event, relation, sentence) with information related to uncertainty. Sauri and Pustejovsky ( 2009 ) presented a corpus annotated with information about the factuality of events, namely Factbank, which is constructed based on TimeBank 2 containing 3,123 annotated sentences from 208 news documents with 8 different levels of uncertainty defined. Vincze et al. (2008) constructed the BioSocpe corpus, which consists of medical and biological texts annotated for negation, uncertainty and their linguistic scope. This corpus contains 20,924 sentences. Ganter et al. (2009) generated Wikipedia Weasels Corpus, where Weasel tags in Wikipedia articles is adopted readily as labels for uncertainty annotation. It contains 168,923 unique sentences with 437 weasel tags in total. Although several uncertainty corpora exist, there is not a uniform set of standard for uncertainty annotation. Szarvas et al. (2012) normalized the annotation of the three corpora aforementioned. However, the context of these corpora is different from that of social media. Typically, these documents annotated are grammatically correct, carefully punctuated, formally structured and logically expressed. Uncertainty identification Previous work on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-Of-Speech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010) . The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (Kiefer, 2005) . For Epistemic, there are two sub-classes Possible and Probable. For Hypothetical, there are four sub-classes including Investigation, Condition, Doxastic and Dynamic. The detail of the classification is described as below (Kiefer, 2005) : Epistemic: On the basis of our world knowledge we cannot decide at the moment whether the statement is true or false. Hypothetical: This type of uncertainty includes four sub-classes: \u2022 Doxastic: Expresses the speaker's beliefs and hypotheses. \u2022 Investigation: Proposition under investigation. \u2022 Condition: Proposition under condition. \u2022 Dynamic: Contains deontic, dispositional, circumstantial and buletic modality. Compared to the existing uncertainty corpora, social media authors enjoy free form of writing. In order to study the difference, we annotated a small set of 827 randomly sampled tweets according to the scheme of uncertainty types above, in which we found 65 uncertain tweets. And then, we manually identified all the possible uncertain tweets, and found 246 really uncertain ones out of these 827 tweets, which means that 181 uncertain tweets are missing based on this scheme. We have the following three salient observations: -Firstly, there is no tweet found with the type of Investigation. We find people seldom use words like \"examine\" or \"test\" (indicative words of Investigation category) when posting tweets. Once they do this, the statement should be considered as highly certain. For example, @dobibid I have tested the link, it is fake! -Secondly, people frequently raise questions about some specific topics for confirmation which expresses uncertainty. For example, @ITVCentral Can you confirm that Birmingham children's hospital has/hasn't been attacked by rioters? -Thirdly, people tend to post message with external information (e.g., story from friends) which reveals uncertainty. For example, Friend who works at the children's hospital in Birmingham says the riot police are protecting it. Based on these observations, we propose a variant of uncertainty types in social media context by eliminating the category of Investigation and adding the category of Question and External under Hypothetical, as shown in Table 3 .1. Note that our proposed scheme is based on Kiefer's work (2005) which was previously extended to normalize uncertainty corpora in different genres by Szarvas et al. (2012) . But we did not try these extended schema for specific genres since even the most general one (Kiefer, 2005) was proved unsuitable for social media context. Annotation result The dataset we annotated was collected from Twitter using Streaming API during summer riots in London during August 6-13 2011, including 326,747 tweets in total. Search criteria include hashtags like #ukriots, #londonriots, #prayforlondon, and so on. We further extracted the tweets relating to seven significant events during the riot identified by UK newspaper The Guardian from this set of tweets. We annotated all the 4,743 extracted tweets for the seven events 3 . Two annotators were trained to annotate the dataset independently. Given a collection of tweets T = {t 1 , t 2 , t 3 ...t n }, the annotation task is to label each tweet t i as either uncertain or certain. Uncertainty assertions are to be identified in terms of the judgements about the author's intended meaning rather than the presence of uncertain cue-phrase. For those tweets annotated as uncertain, sub-class labels are also required according to the classification indicated in Table 3 .1 (i.e., multi-label is allowed). The Kappa coefficient (Carletta, 1996) indicating inter-annotator agreement was 0.9073 for the certain/uncertain binary classification and was 0.8271 for fine-grained annotation. The conflict labels from the two annotators were resolved by a third annotator. Annotation result is displayed in Experiment and evaluation We aim to identify those uncertainty tweets from tweet collection automatically based on machine learning approaches. In addition to n-gram features, we also explore the effectiveness of three categories of social media specific features including content-based, user-based and Twitter-specific ones. The description of the three categories of features is shown in Table 4 . Since the length of tweet is relatively short, we therefore did not carry out stopwords removal or stemming. Our preliminary experiments showed that combining unigrams with bigrams and trigrams gave better performance than using any one or two of these three features. Therefore, we just report the result based on the combination of them as n-gram features. Five-fold cross validation is used for evaluation. Precision, recall and F-1 score of uncertainty category are used as the metrics. Overall performance The overall performance of different approaches is shown in Table 4 .1. We used uncertainty cuephrase matching approach as baseline, denoted by CP. For CP, we labeled tweets containing at least one entry in uncertainty cue-phrase list (described in Section 3) as uncertain. All the other approaches are supervised methods using SVM based on different feature sets. n-gram stands for n-gram feature set, C means content-based feature set, U denotes user-based feature set, T represents   Twitter-specific feature set and ALL is the combination of C, U and T. Table 4 .1 shows that CP achieves the best recall but its precision is the lowest. The learning based methods with different feature sets give some similar recalls. Compared to CP, SVM n\u2212gram increases the F-1 score by 43.9% due to the salient improvement on precision and small drop of recall. The performance improves in terms of precision and F-1 score when the feature set is expanded by adding C, U or T onto n-gram, where +C brings the highest gain, and SVM n\u2212gram+ALL performs best in terms of precision and F-1 score. We then study the effectiveness of the three content-based features, and result shows that the presence of uncertain cue-phrase is most indicative for uncertainty tweet identification. Error analysis We analyze the prediction errors based on SVM n\u2212gram+ALL . The distribution of errors in terms of different types of uncertainty is shown Conclusion and future work In this paper, we propose a variant of classification scheme for uncertainty identification in social media and construct the first uncertainty corpus based on tweets. We perform uncertainty identification experiments on the generated dataset to explore the effectiveness of different types of features. Result shows that the three categories of social media specific features can improve uncertainty identification. Furthermore, content-based features bring the highest improvement among the three and the presence of uncertain cue-phrase contributes most for content-based features. In future, we will explore to use uncertainty identification for social media applications. Acknowledgement This work is partially supported by General Research Fund of Hong Kong (No. 417112). 61",
    "abstract": "Uncertainty text detection is important to many social-media-based applications since more and more users utilize social media platforms (e.g., Twitter, Facebook, etc.) as information source to produce or derive interpretations based on them. However, existing uncertainty cues are ineffective in social media context because of its specific characteristics. In this paper, we propose a variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets. We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification.",
    "countries": [
        "Qatar",
        "Hong Kong",
        "United Kingdom"
    ],
    "languages": [],
    "numcitedby": "25",
    "year": "2013",
    "month": "August",
    "title": "An Empirical Study on Uncertainty Identification in Social Media Context"
}