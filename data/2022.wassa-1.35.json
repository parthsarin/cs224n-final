{
    "article": "Machine-learned models for author profiling in social media often rely on data acquired via self-reporting-based psychometric tests (questionnaires) filled out by social media users. This is an expensive but accurate data collection strategy. Another, less costly alternative, which leads to potentially more noisy and biased data, is to rely on labels inferred from publicly available information in the profiles of the users, for instance self-reported diagnoses or test results. In this paper, we explore a third strategy, namely to directly use a corpus of items from validated psychometric tests as training data. Items from psychometric tests often consist of sentences from an I-perspective (e.g., \"I make friends easily.\"). Such corpora of test items constitute 'small data', but their availability for many concepts is a rich resource. We investigate this approach for personality profiling, and evaluate BERT classifiers fine-tuned on such psychometric test items for the big five personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism) and analyze various augmentation strategies regarding their potential to address the challenges coming with such a small corpus. Our evaluation on a publicly available Twitter corpus shows a comparable performance to in-domain training for 4/5 personality traits with T5-based data augmentation. Introduction The field of author profiling originally emerged from the study of stylometry (Lutoslawski, 1898) and, with the rise of social media (Bilan and Zhekova, 2016) , now considers a variety of attributes, including demographic data such as age, sex, gender, nationality (Schwartz et al., 2013) , personality traits (Golbeck et al., 2011) , or psychological states such as emotions, or medical conditions like mental disorders (De Choudhury et al., 2013) . Such automatic methods enable large-scale social media data analyses even for (combinations of) variables for which results from surveys are not available. Therefore, personality profiling in social media helps to paint a more comprehensive, complete, and timely picture for parts of a society. State-of-the-art models reconstruct personality traits or mental health states from posts of social media users by relying on ground-truth data that links such posts to the correct annotation (Guntuku et al., 2017) . The ground-truth data is typically obtained by either (1) asking participants to complete a validated survey that measures the desired variable and asking the participants to share their social media profiles, (2), by relying on self-reports of users, e.g., disclosure of a condition in the user's profile description, or (3), by having experts annotate profiles for particular properties. The quality of data obtained might therefore suffer from socialdesirability bias, from being a non-representative subsample, from a lack of validated diagnoses, or from noise stemming from the challenge that annotators do not have access to the actual characteristics of users (Ernala et al., 2019) . We explore another route for which we hypothesize that it addresses these issues, but at the cost of only having access to very small data sets: We propose to leverage the existing set of high-quality, validated, and reliable psychometric instruments to measure psychological traits directly. Psychometric tests often come in the form of questionnaires which contain items, allowing a person to report (Lee and Ashton, 2018) . 'Cor.' indicates if the item has been shown to correlate positively or negatively to the respective concept. about themselves. These items are sentences formulated as descriptions of the self (Table 1 shows some examples). This structure motivates our hypothesis that such psychometric tests can be used directly to induce classifiers that profile individuals in social media without the existence of designated, manually annotated in-domain training data. If indeed possible, this would lead to a straightforward route to develop a myriad of classifiers for all those concepts for which psychometric tests exist. To dampen the issue of these sets of items being comparably small, we make use of pre-trained language models (Howard and Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) to transfer knowledge acquired through pretraining rich semantic representations. Some subtypes of such models can be considered few-shot learners (Brown et al., 2020; Ruder et al., 2019) , however, the transfer might not be successful to data outside of the pretraining domain. Therefore, we evaluate if various data augmentation methods can further leverage the challenges coming with such small corpora. Thus, our contributions in this paper are that we (1) assemble a corpus from publicly available psychometric tests for the 'Big Five' variables of openness, conscientiousness, extraversion, agreeableness, and neuroticism (Costa and McCrae, 1992) , which have been shown to be principled factors of personality (Cattell, 1945) . Based on these data, we (2) fine-tune BERT (Devlin et al., 2019) and evaluate it on an existing personality trait corpus (Rangel et al., 2015) . Furthermore, (3) we evaluate three data augmentation methods, namely paraphrasing with T5 (Raffel et al., 2020) , and item generation with GPT-2 (Radford et al., 2019) and synonym replacements with Easy Data Augmentation (Wei and Zou, 2019) . Our results, (4), show that the models perform en par with in-domain training for 4/5 personality trait variables. Related Work Psychometric Personality Tests. A psychometric test is a standardized instrument used to measure the cognitive, behavioral, or emotional characteristics of a person. One possible form are questionnaires, which can be designed for self-reporting. For each item the information is available if it is correlated positively or negatively with the concept to be measured. Publicly available psychometric tests can be found in various online repositories. 1  An established test for personality traits following the so-called 'big five' variables is the International Personality Item Pool Representation of the NEO PI-R with 300 items 2 (IPIP-NEO-300, Goldberg et al., 1999) . This test is a proxy of the Revised NEO Personality Inventory (NEO PI-R) by Costa and McCrae (1992) , which is copyrighted and can only be ordered by professionals and used with permission. We use all items of the IPIP-NEO-300 as the source of our training corpus. Another test of personality traits would be the HEXACO Personality Inventory-Revised (Lee and Ashton, 2008) . It measures six factors of personality (Ashton et al., 2004) with 200 items, namely Honesty-Humility, Emotionality, Extraversion, Agreeableness, Conscientiousness, and Openness to experience. Data. Psychometric tests found application in the analysis of social media user's personality in the past. An influential study has been the work by Schwartz et al. (2014) , who collected Facebook data with a dedicated application (Stillwell and Kosinski, 2004) in which users completed the 100item IPIP-NEO-100 questionnaire (Goldberg et al., 1999) . The users further shared access to their status updates. This data is not available any longer. The data for the PAN-author-profiling shared task in 2015 has been collected in a similar way (Rangel et al., 2015) . 3 It consists of Tweets of 294 English Twitter profiles (besides Spanish, Italian and Dutch Twitter profiles), which are annotated with gender, age, and the 'Big Five' personality traits. The personality traits were self-assessed by the Twitter users with the BFI-10 ( Rammstedt and John, 2007) , which is an economic psychometric test that allows the personality to be recorded with only 10 items. We use this corpus for evaluation. Combining Tests and Social Media Data. An interesting combination of psychometric tests with social media posts, which is likely the one most similar to our paper, is the work by Vu et al. (2020) . The authors make use of social media data of users to automatically fill the IPIP-NEO (Goldberg et al., 1999) psychometric test to predict the social media user's 'Big Five' personality traits. They do so by embedding sentences and items with BERT into the same distributional space, followed by a k-nearest- neighbor classification. This approach constitutes the opposing approach that we chose in our paper - Vu et al. (2020) use social media data to fill a psychometric test. We use psychometric tests to classify social media data. We refer the reader to Stajner and Yenikent (2020) for a more comprehensive overview of related work. Methods Workflow We depict the general workflow in Figure 1 . The original items from the questionnaire are first augmented. The resulting augmented items inherit the labels from the respective original items. We finetune BERT with these items which leads to a model to make predictions for comparably short instances, like Tweets. From the labeled corpus of Twitter profiles, we obtain labels for each individual tweet with the BERT-based model and then aggregate the individual labels to obtain a label for the whole profile. In the evaluation, this predicted profile label is compared to the annotated gold label. Corpora We use all items of the psychometric test IPIP-NEO-300 (Goldberg et al., 1999) as training data and label each item following the evaluation guidelines accompanying the IPIP-NEO-300 (see also Table 1 ). These guidelines provide the information if a confirmative answer to the item indicates a positive correlation or negative correlation with the target variable, which leads to a binary label. For evaluation, we use the English subset of the PAN-author-profiling-2015 data (Rangel et al We extracted all profiles with positive or negative scores and excluded profiles with neutral scores. 2015) with annotated Twitter profiles. Table 2 summarizes the corpus statistics. Note that the distribution of the items from the test data is skewed towards positive instances -this might be a direct consequence of people with particular personality traits being more likely to share particular information on social media. Classification Model As our source domain, we consider a set of items Q C = {(q i , y i )} n i=1 from a reliable psychometric test. Each of these items corresponds to one psychological concept C and consists of the item text q i and the label y i \u2208 {pos, neg} which stems from the evaluation guidelines for this test. The task is to find a parameterized function f C,\u03bb (u) which takes as input all posts of a user u and predicts a label for each concept C. The important aspect in our setup is that the parameters \u03bb are only optimized on the psychometric data Q C . This is a mismatch -we train a classifier to label short texts but need as output a prediction for a set of tweets which represents the user. Hence, to obtain a label for each user, we aggregate the labels for all their posts by accepting the majority class, for each concept separately. To obtain the text classifier, we fine-tune BERT (Devlin et al., 2019) to approximate each function f C,\u03bb , based on bert-base-uncased. The sequence classification head is randomly initialized on top of the encoder. 4 For each concept C, we fine-tune a separate BERT model (no multi-task learning). Data Augmentation With 60 items per personality trait, our training corpora are small. To address this issue, we perform data augmentation with three different methods. For every n instances (q i , y i ) \u2208 Q C , we perform each data augmentation m times (obtaining n \u2022 m instances). Thus, we generate m augmented items q a i for each q i . Each newly generated instance inherits the label y i of its original instance q i . We show examples for automatically generated items in the Appendix A.3. Easy Data Augmentation. Easy Data Augmentation (EDA, Wei and Zou, 2019 ) consists of four operations on the sentence level: synonym replacement, random insertion, random deletion, and random swap. We use the default parameter of 10% of words in the sentence being changed (30% for random deletion) to perform each operation of EDA on each sentence (item) 5 times, hence generate 20 instances out of each original instance. This leads to 1,160 items for openness, 1,130 for conscientiousness, 1,080 for extraversion, 1,160 for agreeableness, and 1080 for neuroticism. T5 item paraphrasing. We use T5 (Raffel et al., 2020) to paraphrase each item, based on the T5ForConditionalGeneration model provided by HuggingFace 5 . We do not perform fine-tuning to our domain, but rely on the original pre-trained parameters. For each item, we generate up to 50 paraphrases which leads to 2,285 items for openness, 2,383 for conscientiousness, 2,149 for extraversion, 2,126 for agreeableness, 2,130 for neuroticism. GPT-2 item generation. We fine-tune GPT-2 (Radford et al., 2019) for each personality trait separately in 150 epochs, based on gpt-2-simple 6 . We generate 3000 items for each class label with a sentence length of 100 tokens and a temperature of 1.5. This leads to 6,279 items for openness, 6,177 for conscientiousness, 6,204 for extraversion, 6,271 for agreeableness, 6,242 for neuroticism. Experiments Experimental Settings We split the psychometric test data to 80 % for training and use 20 % for hyperparameter optimization, while we ensure that augmented items stay in one set with their original item. 7 To avoid overfitting, we apply early stopping via observing the loss on the validation data. The maximum number of epochs is set to 200. For a comparison to an \"upper-bound\" of indomain training on Twitter, we split the corpora of social media profiles such that 50% of the Twitter profiles are in the test set. The remaining 50 % are used for training and further split into 90 % for training and 10 % for hyperparameter optimization of the in-domain model. The settings for finetuning the in-domain models are identical to the settings of the psychometric models. Results We show our main results as weighted F 1 values in Figure 2 (complete results in Table 4 in the Appendix). We compare the \"plain\" models without data augmentation to the augmented methods (as bar plots) and a random baseline (as horizontal line). We further show the performance of the indomain model. All \"plain\", non-data augmented models get outperformed by the random baseline, except for the personality trait neuroticism (F 1 =.63 versus F 1 =.46 random baseline). The plain psychometric models are inferior to the in-domain models for all concepts, but to various extends: Neuroticism is the only trait where the plain model shows a performance en par with the in-domain model. Regarding the augmentation methods, T5 shows considerable improvements for conscientiousness, extraversion, and agreeableness (F 1 =.89, F 1 =.82, F 1 =. 65, respectively, vs. .73, .87, .84 for in-domain models) . This is also the best-performing augmentation method for conscientiousness and extraversion, however, EDA shows a further improvement for agreeableness (.84). T5 does not harm the performance for neuroticism in comparison to the plain model. Therefore, we conclude that T5 augmentation is a promising choice for 4/5 traits, while the other augmentation methods appear less stable in their contribution. In summary, we obtain a substantial model performance without the use of in-domain training data for Conscientiousness, Extraversion, Agreeableness, and Neuroticism. The transfer to or the difficulty of these concepts appears not to be the same, the performance for Conscientiousness is substantially higher than for Neuroticism. These results can only be partially compared to previous work due to the differences in the evaluation setup. However, it should be noted that the concepts that appear to be more challenging in our setup show also lower evaluation measures in related work (see for instance Table 3 in Rangel et al., 2015 , note that their evaluation measure is an RSME, lower is therefore better). Model Introspection To provide some insights on the decision process by the classification models, we provide one example for each personality trait from the Tweet corpus with LIME explanations (Ribeiro et al., 2016) in Table 3 . In the example for openness, the classifier relies on the word \"love\" as a positive indicator. This word can indeed be found in items from the test, namely in \"Love to daydream\", \"Love flowers\", and \"Love to read challenging material\". It is also a term that appears frequently in augmented data, such as in \"Love problem solving\" or in \"Love flowers. Is it not hard to tell if you like something that's especially beautiful?\". A positive indicator for conscientiousness is the word \"August\" and \"year\". This is interesting, given that these words appear not to be directly related to conscientiousness, and they do not appear in the original items of the test. However, the augmented data contains items that refer to \"year\", such as in \"I truly love Excel and have used it for years.\". T Tweet O @username What my love life will hold instore for me in the future. I'd never ask when I'm gonna die...???????? C \"@username: @username I like your profile photo. Very nice!!! You look very pretty. :)\" THANK YOU! Took this photo in August this year. E @username Slade!!! Cool memories of my grammar school days!! A @username I rocked so much to their music! N \"@username: Karma has no menu. You get served what you deserve.\" Table 3 : Examples of LIME explanations. Green indicates a positive contribution of the word, and red a negative contribution. The augmentation approach used in each example is the best-performing method for the respective concept. All examples are true positives. Conclusion & Future Work We outlined a novel methodology for automatic author profiling in social media users without a costly collection of annotated social media data. Instead, we directly train on items from validated psychometric tests. This data selection procedure has some advantages: items of psychometric tests are carefully validated textual instances. Such corpora of such items constitute \"small data\", but are available for a large number of concepts. Therefore, developing a method to induce classifiers directly from psychometric tests is also a promising avenue for future research. For the tasks of developing models measuring the big five personality traits, we tested on Twitter data that has been collected by asking users to fill out a (different) test. The transfer appears to be achievable, we obtain results for four out of five personality traits which are en par with in-domain models, using T5 data augmentation (except Openness, which has very few test instances). An important remaining research question is how models can be obtained that show consistently good results across concepts. In a real-world setup, test data from the target domain would not be available to make model selection decisions. One way to go might be to combine various augmentation methods. Another approach would be to use items as prompts in a zero-shot learning setup. Ethical considerations The fact that the current research deals with the sensitive topic of personality warrants for some ethical considerations. First, the study has been conducted with anonymized publicly available data. We did not collect data ourselves and importantly the data did not allow to identify subjects. Therefore, it is neither required nor possible to request IRB approval for the current research, given that IRB is concerned with the protection of human subjects. We had no reasons to doubt that the parties, who originally collected the data got IRB approval and informed consent form the participants who provided their data. However, we acknowledge that automatic systems for personality trait analysis can be misused. Further, the application of our proposed model creation strategy can also be used for other more sensible concepts, for instance regarding mental health. We propose that such systems are only made available in such a manner that no personalized results can be retrieved. 54 .54 .54 .48 .45 .42 .42 .38 .19 .44 .50 .47 .44 .35 .34 w-avg .01 .10 .02 .83 .84 .84 .80 .56 .65 .70 .20 .24 .79 .89 .84 .76 .44 .55 Neur.  Table 4 : Detailed results for Psychometric Models vs. in-domain Models vs. Random Baseline for psychological traits in Twitter users. The random baseline generates predictions by respecting the training sets' class distribution. A Appendix \u2212 The weighted average values for P, R, F 1 correspond to the average across all labels considering the proportion for each label in the data set. The bold typo highlights our best performing model w.r.t. the highest w-avg. \u2212: scored negative, +: scored positive. A.2 Implementation Details We performed the experiments on 4 NVIDIA GeForce GTX 1080 Ti GPUs with Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GH. The number of parameters is defined by the base model that we used, namely BERT base, with 110 M parameters. We show the run-time of models (training + testing) in Table 5 . The numbers do not include startup/loading times. Note that the test data is (sometimes dramatically) larger than the training data. Acknowledgements This work was supported by Deutsche Forschungsgemeinschaft (project CEAT, KL 2869/1-2).",
    "abstract": "Machine-learned models for author profiling in social media often rely on data acquired via self-reporting-based psychometric tests (questionnaires) filled out by social media users. This is an expensive but accurate data collection strategy. Another, less costly alternative, which leads to potentially more noisy and biased data, is to rely on labels inferred from publicly available information in the profiles of the users, for instance self-reported diagnoses or test results. In this paper, we explore a third strategy, namely to directly use a corpus of items from validated psychometric tests as training data. Items from psychometric tests often consist of sentences from an I-perspective (e.g., \"I make friends easily.\"). Such corpora of test items constitute 'small data', but their availability for many concepts is a rich resource. We investigate this approach for personality profiling, and evaluate BERT classifiers fine-tuned on such psychometric test items for the big five personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism) and analyze various augmentation strategies regarding their potential to address the challenges coming with such a small corpus. Our evaluation on a publicly available Twitter corpus shows a comparable performance to in-domain training for 4/5 personality traits with T5-based data augmentation.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "Italian",
        "Spanish",
        "Dutch",
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Items from Psychometric Tests as Training Data for Personality Profiling Models of {T}witter Users"
}