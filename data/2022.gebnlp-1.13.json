{
    "article": "The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size. 1 Introduction The growing size of pretrained language models has led to large improvements on a variety of NLP tasks (Raffel et al., 2020; He et al., 2021; Brown et al., 2020) . However, the success of these models comes with a price-they are trained on vast amounts of mostly web-based data, which often contains social stereotypes and biases that the models might pick up (Bender et al., 2021; Dodge et al., 2021; De-Arteaga et al., 2019) . Combined with recent evidence that the memorization capacity of training data grows with model size (Magar and Schwartz, 2022; Carlini et al., 2022) , the risk of Figure 1 : We study the effect of model size on occupational gender bias in two setups: using prompt based method (A), and using Winogender as a downstream task (B). We find that while larger models receive higher bias scores on the former task, they make less gender errors on the latter. We further analyse the models' behaviour on Winogender and show that larger models express more biased behavior in those two setups. language models containing these biases is even higher. This can have negative consequences, as models can abuse these biases in downstream tasks or applications. For example, machine translation models have been shown to generate outputs based on gender stereotypes regardless of the context of the sentence (Stanovsky et al., 2019) , and models rated male resumes higher than female ones (Parasurama and Sedoc, 2021) . There is an increasing amount of research dedicated to evaluating this problem. For example, several works studied the bias in models using downstream tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018) , natural language inference (NLI) (Poliak et al., 2018; Sharma et al., 2021) and machine translation (Stanovsky et al., 2019) . Other works measured bias in language models directly using masked language modeling (MLM) (Nadeem et al., 2021; Nangia et al., 2020; de Vassimon Manela et al., 2021) . In this paper, we examine how model size af-fects gender bias (Fig. 1 ). We focus on occupationspecific bias which corresponds to the real-world employment statistics (BLS). 2 We measure the bias in three model families (RoBERTa; Liu et al., 2019, DeBERTa; He et al., 2021 and T5; Raffel et al., 2020) in two different ways: using MLM prompts and using the Winogender benchmark (Rudinger et al., 2018) . We start by observing a potentially conflicting trend: although larger models exhibit more gender bias than smaller models in MLM, 3 their Winogender parity score, which measures gender consistency, is higher, indicating a lower level of gender errors. To bridge this gap, we further analyze the models' Winogender errors, and present an alternative approach to investigate gender bias in downstream tasks. First, we estimate the probability that an error is caused due to gender bias, and find that within all three families, this probability is higher for the larger models. Then, we distinguish between two types of gender errors-stereotypical and anti-stereotypical-and compare their distribution. We find that stereotypical errors, which are caused by following the stereotype, are more prevalent than anti-stereotypical ones, and that the ratio between them increases with model size. Our results demonstrate a potential risk inherent in model growth-it makes models more socially biased. 2 Are Larger Models More Biased? The connection between model size and gender bias is not fully understood; are larger models more sensitive to gender bias, potentially due to their higher capacity that allows them to capture more subtle biases? or perhaps they are less biased, due to their superior language capabilities? In this section we study this question in a controlled manner, and observe a somewhat surprising trend: depending on the setup for measuring gender bias, conflicting results are observed; on the one hand, in MLM setup larger models are more sensitive to gender bias than smaller models. On the other, larger models obtain higher parity score on a downstream task (Winogender), which hints that they might be less sensitive to bias in this task. We describe our findings below. We measure the occupational gender bias in three models' families, using two methods- prompt based method (Kurita et al., 2019) and Winogender schema (Rudinger et al., 2018) . To maintain consistency, we use the same list of occupations in all our experiments. The gender stereotypicality of an occupation is determined by the U.S. Bureau of Labor Statistics (BLS). 4   Pretrained Models Unless stated otherwise, we experiment with three families of pretrained language models: RoBERTa-{base,large} (Liu et al., 2019) , DeBERTa-{base,large,xlarge} (He et al., 2021) and T5-{base,large,3B} (Raffel et al., 2020) . We provide implementation details in App. B. Sensitivity to Gender Bias in MLM Increases with Model Size To examine the model's sensitivity to gender bias we directly query the model using a simple prompt: \"[MASK] worked as a/an [OCCUPATION] .\" 5 This prompt intentionally does not provide much context, in order to purely measure occupational biases. As a measure of bias, we adopt Kurita et al. (2019)'s log probability bias score. We compare the normalized predictions 6 that the model assigns to \"he\" and \"she\", given the above prompt: for male occupations (according to BLS) we compute the difference with respect to \"he\", and for female occupations we compute the difference with respect to \"she\". Positive scores indicate the model assigns higher normalized predictions to the pronoun that matches the occupation's stereotypical gender. We experiment with RoBERTa and T5, 7 evaluating gender bias using two measures: 1. agreement: the percentage of occupations with positive bias score. 2. average bias score: the average bias score of the occupations. agreement enables us to evaluate the general preference towards one gender, while average bias score measures the magnitude of the preference. Results Fig. 2 presents our results. For both model families, the average bias score increases along with the model size. Further, the agreement measure increases with model size for T5 models, and is the same for both RoBERTa models. These findings indicate that models are becoming more biased as they grow in size. This is consistent with prior work (Nadeem et al., 2021; Vig et al., 2020) . Larger Models Exhibit Less Bias in Winogender We have so far observed that larger models express higher sensitivity to gender bias in an MLM setup. We now examine gender bias using a downstream task-Winogender-an evaluation dataset designed to measure occupational gender bias in coreference resolution. sentence type The engineer informed the client that she would need more time to complete the project. gotcha The engineer informed the client that he would need more time to complete the project. not gotcha Table 1 : Examples of \"gotcha\" and \"not gotcha\" sentences from Winogender. In both sentences the pronoun refers to the engineer. Each example in the dataset contains an occupation (one of the occupations on the BLS list), a Winogender consists of \"gotcha\" and \"not gotcha\" sentences. Roughly speaking, \"gotcha\" sentences are the ones in which the stereotype of the occupation might confuse the model into making the wrong prediction. Consider the \"gotcha\" sentence in Tab. 1. The pronoun \"she\" refers to the \"engineer\" which is a more frequent occupation for men than for women. This tendency could cause the model to misinterpret \"she\" as \"the client\". In contrast, in \"not gotcha\" sentences, the correct answer is not in conflict with the occupation distribution (a male engineer in Tab. 1). The Winogender instances are arranged in minimal pairs-the only difference between two paired instances is the gender of the pronoun in the premise (Tab. 1). Importantly, the label for both instances is the same. We use the casting of Winogender as an NLI task (Poliak et al., 2018) , which is part of the Su-perGLUE benchmark (Wang et al., 2019) . Performance on Winogender is measured with both NLI accuracy and gender parity score: the percentage of minimal pairs for which the predictions are the same. Low parity score indicates high level of gender errors (errors which occur when a model assigns different predictions to paired instances). These errors demonstrate the presence of gender bias in the model. We use all three families (RoBERTa, DeBERTa, T5), all fine-tuned on MNLI (Williams et al., 2018) and then fine-tuned again with RTE (Dagan et al., 2013) . Results Our results are shown in Fig. 3 . We first notice, unsurprisingly, that larger models outperform smaller ones on the NLI task. Further, when considering parity scores, we also find that the scores increase with model size. Combined with our results in Sec. 2.1, we observe a potential conflict: while our findings in the MLM experiment show that the larger the model the more sensitive it is to gender bias, when considering our Winogender results, we find that larger models make less gender errors. We next turn to look differently at the Winogender results, in an attempt to bridge this gap. Winogender Errors Analysis Unravels Biased Behavior We have so far shown that larger models make fewer gender errors compared to smaller models (Sec. 2.2), but that they also hold more occupational gender bias compared to their smaller counterparts (Sec. 2.1). In this section we argue that parity score and accuracy do not show the whole picture. Through an analysis of the models' gender errors, we offer an additional viewpoint on the Winogender results, which might partially bridge this gap. The probability that an error is gendered increases with model size Our first observation is that while larger models make fewer errors, and fewer gender errors in particular, the proportion of the latter in the former is higher compared to smaller models. We evaluate the probability that an error is caused by the gender of the pronoun (i.e., that an error is gendered). We estimate this probability by the proportion of gender errors in total errors: p(error is gendered) \u2248 |gender errors| |errors| We find for both DeBERTa and RoBERTa that this probability increases with model size (Tab. 2, gender column). In the extreme case (DeBERTaxlarge), 41% of the errors are gendered. Our results indicate that for larger models, the rate in which the total amount of errors drop is higher than the rate of gender errors drop. Larger models make more stereotypical errors We next distinguish between two types of gender errors: stereotypical and anti-stereotypical. As described in Sec. to \"gotcha\" and \"not gotcha\" instances. The key characterization of a \"gotcha\" sentence is that the occupation's stereotype can make it hard for the model to understand the coreference in the sentence. Thus, we will refer to the gender errors on \"gotcha\" sentences as stereotypical errors. 8 Accordingly, we will refer to gender errors on \"not gotcha\" sentences as anti-stereotypical errors. Note that the number of gender errors is equal to the sum of stereotypical and anti-stereotypical errors. We present in Tab. 2 both probabilities that an error is stereotypical and anti-stereotypical. Within all three model families, the probability that an error is stereotyped rises with model size, while the probability that an error is anti-stereotyped decreases with model size. This observation indicates that the increase in proportion of gendered errors is more attributed to stereotypical errors in larger models compared to smaller ones. Indeed, when considering the distribution of gender errors (Fig. 4 ), we find that the larger models obtain a higher stereotypical to anti-stereotypical error ratio; in some cases, the larger models are making up to 20 times more stereotypical errors than antistereotypical. This indicates that even though they make fewer gender errors, when they do make them, their mistakes tend to be more stereotypical. Our results provide a deeper understanding of the models' behavior on Winogender compared to only considering accuracy and parity score. Combined with our MLM results (Sec. 2.1), we conclude that larger models express more biased behavior than smaller models. Related work Measuring bias in pretrained language models Earlier works presented evaluation datasets such as WEAT/SEAT, which measure bias in static word embedding using cosine similarity of specific target words (Caliskan et al., 2017; May et al., 2019) . Another line of work explored evaluation directly in pretrained masked language models. Kurita et al. (2019) presented an association relative metric for measure gender bias. This metric incorporates the probability of predicting an attribute (e.g \"programmer\") given the target for bias (e.g \"she\"), in a generic template such as \"<target> is [MASK]\". They measure how much more the model prefers the male gender association with an attribute. Nadeem et al. (2021) presented StereoSet, a large-scale natural dataset to measure four domains of stereotypical biases in models using likelihood-based scoring with respect to their language modeling ability. Nangia et al. (2020) introduced CrowS-Pairs, a challenge set of minimal pairs that examines stereotypical bias in nine domains via minimal pairs. They adopted psuedolikelihood based scoring (Wang and Cho, 2019; Salazar et al., 2020) that does not penalize less frequent attribute term. In our work, we build upon Kurita et al. (2019) 's measure in order to examine stereotypical bias to the specifics occupations we use, in different sizes of models. Another method to evaluate bias in pretrained models is through downstream tasks, such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018) and sentiment analysis (Kiritchenko and Mohammad, 2018) . Using this method, the bias is determined by the performance of the model in the task. This allows for investigation of how much the bias of the model affects its performance. Bias sensitivity of larger pretrained models Most related to this work, Nadeem et al. (2021) measured bias using the StereoSet dataset, and compared models of the same architecture of different sizes. They found that as the model size increases, its stereotypical score increases. For autocomplete generation, Vig et al. (2020) analyzed GPT-2 (Radford et al., 2019) variants through a causal mediation analysis and found that larger models contain more gender bias. In this work we found a similar trend with respect to gender occupational bias measured via MLM prompts, and a somewhat different trend when considering Winogender parity scores. Our error analysis on Winogender was able to partially bridge the gap between these potential conflicting findings. Conclusion We investigated how a model's size affects its gender bias. We presented somewhat conflicting results: the model bias increases with model size when measured using a prompt based method, but the amount of gender errors decreases with size when considering the parity score in the Winogender benchmark. To bridge this gap, we employed an alternative approach to investigate bias in Winogender. Our results revealed that while larger models make fewer gender errors, the proportion of these errors among all errors is higher. In addition, as model size increases, the proportion of stereotypical errors increases in comparison to antistereotypical ones. Our work highlights a potential risk of increasing gender bias which is associated with increasing model sizes. We hope to encourage future research to further evaluate and reduce biases in large language models. Bias Statement In this paper, we examine how model size affects gender bias. We focus on occupations with a gender stereotype, and examine stereotypical associations between male and female gender and professional occupations. We measure bias in two setups: MLM (Kurita et al., 2019; Nadeem et al., 2021) and Winogender (Rudinger et al., 2018) , and build on the enclosed works' definition of gender bias. 9 We show how these different setups yield conflicting results regarding gender bias. We aim to bridge this gap by working under a unified framework of stereotypical and anti-stereotypical associations. We find that the models' biases lead them to make errors, and specifically more stereotypical then antistereotypical errors. Systems that identify certain occupations with a specific gender perpetuate inappropriate stereotypes about what men and women are capable of. Furthermore, if a model makes wrong predictions because it associates an occupation with a specific gender, this can cause significant harms such as inequality of employment between men and women. In this work, we highlight that those potential risks become even greater as the models' size increase. Finally, we acknowledge that our binary gender labels, which are based on the resources we use, do not reflect the wide range of gender identities. In the future, we hope to extend our work to nonbinary genders as well. with batch size 8. We use AdaFactor (Shazeer and Stern, 2018) optimizer with learning rate of 1e-4 and default parameters: \u03b2 1 = 0.0, \u03f5 = 1e-3, without weight decay. We selected the highest performing models on the validation set among five random trials. All our experiments were conducted using the following GPUs: nvidia RTX 5000, Quadro RTX 6000, A10 and A5000. Acknowledgements We would like to thank Elad Shoham, Yuval Reif, Gabriel Stanovsky, Daniel Rotem, and Tomasz Limisiewicz for their feedback and insightful discussions. We also thank the anonymous reviewers for their valuable comments. This work was supported in part by the Israel Science Foundation (grant no. 2045/21) and by a research gift from the Allen Institute for AI. A Additional Prompts for MLM Setup As pretrained models sensitive to prompts, we experiment with two other prompts: \"[MASK] is a/an [OCCUPATION]\" (Fig. 5 ) and \"Complete the sentence: [MASK] is a/an [OCCUPATION] .\" (Fig. 6 ). 10 The top predictions of T5-base were irrelevant to the given prompt. In particular, \"she\" and \"he\" were not among the top ten predictions of the model for any of the occupations. Therefore it is not presented. The last prompt is inspired by the task prefix that was used during T5's pretraining. In all the prompts we use, the models predicted \"she\" and \"he\" in the top ten predictions, for at least 75% of the occupations. The results show in almost all cases (except agreement score for T5-3B in \"[MASK] is a/an [OCCUPATION]\") an increasing trend for both families. B Implementation Details For Sec. 2.2 We implemented the experiments with the huggingface package (Wolf et al., 2020) , using both run_glue (for RoBERTa and Deberta) and run_summarization (for T5) scrips for masked language models. We used the official MNLI checkpoints for RoBERTa and Deberta and then finetuned again with RTE with the following standard procedure and hyperparameters. We fine-tuned RoBERTa and DeBERTa on RTE for 6 epochs with batch size 32. We use AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 2e-5 (for RoBERTa-{base,large}) and DeBERTa-{base}) and 1e-5 (for DeBERTa-{large,xlarge} and default parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03f5 = 1e-6, with weight decay of 0.1. For T5 we used the T5 1.0 checkpoint, which is trained on both unsupervised and downstream task data. We fine-tuned T5 11 on RTE for 6 epochs",
    "abstract": "The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size. 1",
    "countries": [
        "Israel"
    ],
    "languages": [],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias"
}