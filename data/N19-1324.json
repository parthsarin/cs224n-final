{
    "article": "We propose a new type of representation learning method that models words, phrases and sentences seamlessly. Our method does not depend on word segmentation and any humanannotated resources (e.g., word dictionaries), yet it is very effective for noisy corpora written in unsegmented languages such as Chinese and Japanese. The main idea of our method is to ignore word boundaries completely (i.e., segmentation-free), and construct representations for all character n-grams in a raw corpus with embeddings of compositional sub-ngrams. Although the idea is simple, our experiments on various benchmarks and real-world datasets show the efficacy of our proposal. Introduction Most existing word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) take a sequence of words as their input. Therefore, the conventional models are dependent on word segmentation (Yang et al., 2017; Shao et al., 2018) , which is a process of converting a raw corpus (i.e., a sequence of characters) into a sequence of segmented character n-grams. After the segmentation, the segmented character n-grams are assumed to be words, and each word's representation is constructed from distribution of neighbour words that co-occur together across the estimated word boundaries. However, in practice, this kind of approach has several problems. First, word segmentation is difficult especially when texts in a corpus are noisy or unsegmented (Saito et al., 2014; Kim et al., 2018) . For example, word segmentation on social network service (SNS) corpora, such as Twitter, is a challenging task since it tends to include many misspellings, informal words, neologisms, and even emoticons. This problem becomes more severe in unsegmented languages, such as Chinese and Japanese, whose word boundaries are not explicitly indicated. Second, word segmentation has ambiguities (Luo et al., 2002; Li et al., 2003) . For example, a compound word \u7dda\u5f62\u4ee3\u6570\u5b66 (linear algebra) can be seen as a single word or sequence of words, such as \u7dda\u5f62|\u4ee3\u6570\u5b66 (linear | algebra). Word segmentation errors negatively influence subsequent processes (Xu et al., 2004) . For example, we may lose some words in training corpora, leading to a larger Out-Of-Vocabulary (OOV) rate (Sun et al., 2005) . Moreover, segmentation errors, such as segmenting \u304d\u306e\u3046 (yesterday) as \u304d|\u306e\u3046 (tree | brain), produce false co-occurrence information. This problem is crucial for most existing word embedding methods as they are based on distributional hypothesis (Harris, 1954) , which can be summarized as: \"a word is characterized by the company it keeps\" (Firth, 1957) . To enhance word segmentation, some recent works (Junyi, 2013; Sato, 2015; Jeon, 2016) made rich resources publicly available. However, maintaining them up-to-date is difficult and it is infeasible for them to cover all types of words. To avoid the negative impacts of word segmentation errors, Oshikiri (2017) proposed a word embedding method called segmentation-free word embedding (sembei). The key idea of sembei is to directly embed frequent character n-grams from a raw corpus without conducting word segmentation. However, most of the frequent n-grams are non-words (Kim et al., 2018) , and hence sembei still suffers from the OOV problems. The fundamental problem also lies in its extension (Kim et al., 2018) , although it uses external resources to reduce the number of OOV. To handle OOV problems, Bojanowski et al. (2017) proposed a novel compositional word embedding method with subword modeling, called subword-information skipgram (sisg). The key idea of sisg is to extend the notion of vocabulary to include subwords, namely, substrings of words, for enriching the representations of words by the embeddings of its subwords. In sisg, the embeddings of OOV (or unseen) words are computed from the embedings of their subwords. However, sisg requires word segmentation as a prepossessing step, and the way of collecting co-occurrence information is dependent on the results of explicit word segmentation. For solving the issues of word segmentation and OOV, we propose a simple but effective unsupervised representation learning method for words, phrases and sentences, called segmentation-free compositional n-gram embedding (scne). The key idea of scne is to train embeddings of character n-grams to compose representations of all character n-grams in a raw corpus, and it enables treating all words, phrases and sentences seamlessly (see Figure 1 for an illustrative explanation). Our experimental results on a range of datasets suggest that scne can compute high-quality representations for words and sentences although it does not consider any word boundaries and is not dependent on any human annotated resources. 2 Segmentation-free Compositional n-gram Embedding (scne) Our method scne successfully combines a subword model (Zhang et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Zhao et al., 2018) with an idea of character n-gram embedding (Os-  hikiri, 2017; Kim et al., 2018) . In scne, the vector representation of a target character n-gram is defined as follows. Let x 1 x \u2022 \u2022 \u2022 x N be a raw unsegmented corpus of N characters. For a range i, i + 1, . . . , j specified by index t = (i, j), 1 \u2264 i \u2264 j \u2264 N , we denote the substring x i x i+1 x i nleft , \u2022 \u2022 \u2022 , x i 2 , x i 1 , x i , x i+1 , \u2022 \u2022 \u2022 , x j , x j , x j+1 , x j+2 , \u2022 \u2022 \u2022 , x j+nright Target n-gram Context n-gram Context n-gram x i , x i+1 , x i+2 , x i+3 , \u2022 \u2022 \u2022 , x j , x j 2 , x j 1 , x j Sub-n-grams 2 S(x (i:j)) \u21e2 V < l a t e x i t s h a 1 _ b a s e 6 4 = \" O R T R U v l J d R d 2 J X 8 j S k V a 8 + P V K F s = \" > A A A C k 3 i c h V F B S + N A F P 6 M u q v V X e s u g u A l W J V 6 K S 9 e V t y L u B 6 8 C G p t F a y U J D v t z p o m I Z k W N f Q P 7 H n B g y c F E f E f e N X D / o E 9 + B P E o 4 I X D 7 6 m A V F R X 8 i 8 b 7 5 5 3 5 t v Z i z f k a E i u u z Q O r u 6 P 3 z s 6 U 3 1 9 X / 6 P J A e / F I M v X p g i 4 L t O V 6 w b p m h c K Q r C k o q R 6 z 7 g T B r l i P W r K 0 f r f W 1 h g h C 6 b m r a s c X m z W z 6 s q K t E 3 F V D m d G y t J V 8 9 n S 0 p s K 6 s S x V m q a L v Z L E d Z O f N 7 s j m p l 8 K 6 F Q q l F 8 f K 6 Q z l K A 7 9 J T A S k E E S S 1 7 6 G C X 8 h A c b d d Q g 4 E I x d m A i 5 G 8 D B g g + c 5 u I m A s Y y X h d o I k U a + t c J b j C Z H a L x y r P N h L W 5 X m r Z x i r b d 7 F 4 T 9 g p Y 5 x + k 8 n d E P / 6 J S u 6 P 7 V X l H c o + V l h 7 P V 1 g q / P P B n O H / 3 r q r G W e H X o + p N z w o V T M d e J X v 3 Y 6 Z 1 C r u t b + z u 3 e R n V s a j C T q k a / Z / Q J d 0 z i d w G 7 f 2 0 b J Y 2 U e K H 8 B 4 f t 0 v Q X E q Z 1 D O W J 7 K z M 4 l T 9 G D E Y w i y / f 9 D b N Y w B I K v O 9 f n O E c F 9 q Q 9 l 2 b 0 + b b p V p H o v m K J 6 E t P g A b L p s 9 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" O R T R U v l J d R d 2 J X 8 j S k V a 8 + P V K F s = \" > A A A C k 3 i c h V F B S + N A F P 6 M u q v V X e s u g u A l W J V 6 K S 9 e V t y L u B 6 8 C G p t F a y U J D v t z p o m I Z k W N f Q P 7 H n B g y c F E f E f e N X D / o E 9 + B P E o 4 I X D 7 6 m A V F R X 8 i 8 b 7 5 5 3 5 t v Z i z f k a E i u u z Q O r u 6 P 3 z s 6 U 3 1 9 X / 6 P J A e / F I M v X p g i 4 L t O V 6 w b p m h c K Q r C k o q R 6 z 7 g T B r l i P W r K 0 f r f W 1 h g h C 6 b m r a s c X m z W z 6 s q K t E 3 F V D m d G y t J V 8 9 n S 0 p s K 6 s S x V m q a L v Z L E d Z O f N 7 s j m p l 8 K 6 F Q q l F 8 f K 6 Q z l K A 7 9 J T A S k E E S S 1 7 6 G C X 8 h A c b d d Q g 4 E I x d m A i 5 G 8 D B g g + c 5 u I m A s Y y X h d o I k U a + t c J b j C Z H a L x y r P N h L W 5 X m r Z x i r b d 7 F 4 T 9 g p Y 5 x + k 8 n d E P / 6 J S u 6 P 7 V X l H c o + V l h 7 P V 1 g q / P P B n O H / 3 r q r G W e H X o + p N z w o V T M d e J X v 3 Y 6 Z 1 C r u t b + z u 3 e R n V s a j C T q k a / Z / Q J d 0 z i d w G 7 f 2 0 b J Y 2 U e K H 8 B 4 f t 0 v Q X E q Z 1 D O W J 7 K z M 4 l T 9 G D E Y w i y / f 9 D b N Y w B I K v O 9 f n O E c F 9 q Q 9 l 2 b 0 + b b p V p H o v m K J 6 E t P g A b L p s 9 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" O R T R U v l J d R d 2 J X 8 j S k V a 8 + P V K F s = \" > A A A C k 3 i c h V F B S + N A F P 6 M u q v V X e s u g u A l W J V 6 K S 9 e V t y L u B 6 8 C G p t F a y U J D v t z p o m I Z k W N f Q P 7 H n B g y c F E f E f e N X D / o E 9 + B P E o 4 I X D 7 6 m A V F R X 8 i 8 b 7 5 5 3 5 t v Z i z f k a E i u u z Q O r u 6 P 3 z s 6 U 3 1 9 X / 6 P J A e / F I M v X p g i 4 L t O V 6 w b p m h c K Q r C k o q R 6 z 7 g T B r l i P W r K 0 f r f W 1 h g h C 6 b m r a s c X m z W z 6 s q K t E 3 F V D m d G y t J V 8 9 n S 0 p s K 6 s S x V m q a L v Z L E d Z O f N 7 s j m p l 8 K 6 F Q q l F 8 f K 6 Q z l K A 7 9 J T A S k E E S S 1 7 6 G C X 8 h A c b d d Q g 4 E I x d m A i 5 G 8 D B g g + c 5 u I m A s Y y X h d o I k U a + t c J b j C Z H a L x y r P N h L W 5 X m r Z x i r b d 7 F 4 T 9 g p Y 5 x + k 8 n d E P / 6 J S u 6 P 7 V X l H c o + V l h 7 P V 1 g q / P P B n O H / 3 r q r G W e H X o + p N z w o V T M d e J X v 3 Y 6 Z 1 C r u t b + z u 3 e R n V s a j C T q k a / Z / Q J d 0 z i d w G 7 f 2 0 b J Y 2 U e K H 8 B 4 f t 0 v Q X E q Z 1 D O W J 7 K z M 4 l T 9 G D E Y w i y / f 9 D b N Y w B I K v O 9 f n O E c F 9 q Q 9 l 2 b 0 + b b p V p H o v m K J 6 E t P g A b L p s 9 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" O R T R U v l J d R d 2 J X 8 j S k V a 8 + P V K F s = \" > A A A C k 3 i c h V F B S + N A F P 6 M u q v V X e s u g u A l W J V 6 K S 9 e V t y L u B 6 8 C G p t F a y U J D v t z p o m I Z k W N f Q P 7 H n B g y c F E f E f e N X D / o E 9 + B P E o 4 I X D 7 6 m A V F R X 8 i 8 b 7 5 5 3 5 t v Z i z f k a E i u u z Q O r u 6 P 3 z s 6 U 3 1 9 X / 6 P J A e / F I M v X p g i 4 L t O V 6 w b p m h c K Q r C k o q R 6 z 7 g T B r l i P W r K 0 f r f W 1 h g h C 6 b m r a s c X m z W z 6 s q K t E 3 F V D m d G y t J V 8 9 n S 0 p s K 6 s S x V m q a L v Z L E d Z O f N 7 s j m p l 8 K 6 F Q q l F 8 f K 6 Q z l K A 7 9 J T A S k E E S S 1 7 6 G C X 8 h A c b d d Q g 4 E I x d m A i 5 G 8 D B g g + c 5 u I m A s Y y X h d o I k U a + t c J b j C Z H a L x y r P N h L W 5 X m r Z x i r b d 7 F 4 T 9 g p Y 5 x + k 8 n d E P / 6 J S u 6 P 7 V X l H c o + V l h 7 P V 1 g q / P P B n O H / 3 r q r G W e H X o + p N z w o V T M d e J X v 3 Y 6 Z 1 C r u t b + z u 3 e R n V s a j C T q k a / Z / Q J d 0 z i d w G 7 f 2 0 b J Y 2 U e K H 8 B 4 f t 0 v Q X E q Z 1 D O W J 7 K z M 4 l T 9 G D E Y w i y / f 9 D b N Y w B I K v O 9 f n O E c F 9 q Q 9 l 2 b 0 + b b p V p H o v m K J 6 E t P g A b L p s 9 < / l a t e x i t > 2 V < l a t e x i t s h a 1 _ b a s e 6 4 = \" m S z r L h 9 h 7 i Z Y O g X V x 9 c s m E A 7 W y w = \" > A A A C a n i c h V G 7 S g N B F D 1 Z 3 / E V Y 6 O k C S Y R q 3 D X R r E S b S x 9 5 Q G J y O 4 6 6 u B m d 9 n d B D T 4 A 3 Z W g q k U R M T P s P E H L P I J o l 0 E G w t v N g u i Q b 3 D z J w 5 c 8 + d M z O 6 Y 0 r P J 2 p G l J 7 e v v 6 B w a H o 8 M j o 2 H h s I p 7 3 7 K p r i J x h m 7 Z b 1 D V P m N I S O V / 6 p i g 6 r t A q u i k K + t F q e 7 9 Q E 6 4 n b W v b P 3 b E T k U 7 s O S + N D S f q U K 6 L K 1 k P r 0 b S 1 G W g k h 2 A z U E K Y S x b s d u U c Y e b B i o o g I B C z 5 j E x o 8 b i W o I D j M 7 a D O n M t I B v s C p 4 i y t s p Z g j M 0 Z o 9 4 P O B V K W Q t X r d r e o H a 4 F N M 7 i 4 r k 8 j Q E 9 1 R i x 7 p n p 7 p 4 9 d a 9 a B G 2 8 s x z 3 p H K 5 z d 8 b O p r f d / V R W e f R x + q f 7 0 7 G M f i 4 F X y d 6 d g G n f w u j o a y c X r a 2 l z U x 9 l q 7 p h f 1 f U Z M e + A Z W 7 c 2 4 2 R C b D U T 5 A 9 S f z 9 0 N 8 v N Z l b L q x n x q e S X 8 i k E k M I M 5 f u 8 F L G M N 6 8 g F 7 s 5 x i U b k V Y k r 0 0 q i k 6 p E Q s 0 k v o W S / g T O b o u r < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" m S z r L h 9 h 7 i Z Y O g X V x 9 c s m E A 7 W y w = \" > A A A C a n i c h V G 7 S g N B F D 1 Z 3 / E V Y 6 O k C S Y R q 3 D X R r E S b S x 9 5 Q G J y O 4 6 6 u B m d 9 n d B D T 4 A 3 Z W g q k U R M T P s P E H L P I J o l 0 E G w t v N g u i Q b 3 D z J w 5 c 8 + d M z O 6 Y 0 r P J 2 p G l J 7 e v v 6 B w a H o 8 M j o 2 H h s I p 7 3 7 K p r i J x h m 7 Z b 1 D V P m N I S O V / 6 p i g 6 r t A q u i k K + t F q e 7 9 Q E 6 4 n b W v b P 3 b E T k U 7 s O S + N D S f q U K 6 L K 1 k P r 0 b S 1 G W g k h 2 A z U E K Y S x b s d u U c Y e b B i o o g I B C z 5 j E x o 8 b i W o I D j M 7 a D O n M t I B v s C p 4 i y t s p Z g j M 0 Z o 9 4 P O B V K W Q t X r d r e o H a 4 F N M 7 i 4 r k 8 j Q E 9 1 R i x 7 p n p 7 p 4 9 d a 9 a B G 2 8 s x z 3 p H K 5 z d 8 b O p r f d / V R W e f R x + q f 7 0 7 G M f i 4 F X y d 6 d g G n f w u j o a y c X r a 2 l z U x 9 l q 7 p h f 1 f U Z M e + A Z W 7 c 2 4 2 R C b D U T 5 A 9 S f z 9 0 N 8 v N Z l b L q x n x q e S X 8 i k E k M I M 5 f u 8 F L G M N 6 8 g F 7 s 5 x i U b k V Y k r 0 0 q i k 6 p E Q s 0 k v o W S / g T O b o u r < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" m S z r L h 9 h 7 i Z Y O g X V x 9 c s m E A 7 W y w = \" > A A A C a n i c h V G 7 S g N B F D 1 Z 3 / E V Y 6 O k C S Y R q 3 D X R r E S b S x 9 5 Q G J y O 4 6 6 u B m d 9 n d B D T 4 A 3 Z W g q k U R M T P s P E H L P I J o l 0 E G w t v N g u i Q b 3 D z J w 5 c 8 + d M z O 6 Y 0 r P J 2 p G l J 7 e v v 6 B w a H o 8 M j o 2 H h s I p 7 3 7 K p r i J x h m 7 Z b 1 D V P m N I S O V / 6 p i g 6 r t A q u i k K + t F q e 7 9 Q E 6 4 n b W v b P 3 b E T k U 7 s O S + N D S f q U K 6 L K 1 k P r 0 b S 1 G W g k h 2 A z U E K Y S O p r f d / V R W e f R x + q f 7 0 7 G M f i 4 F X y d 6 d g G n f w u j o a y c X r a 2 l z U x 9 l q 7 p h f 1 f U Z M e + A Z W 7 c 2 4 2 R C b D U T 5 A 9 S f z 9 0 N 8 v N Z l b L q x n x q e S X 8 i k E k M I M 5 f u 8 F L G M N 6 8 g F 7 s 5 x i U b k V Y k r 0 0 q i k 6 p E Q s 0 k v o W S / g T O b o u r < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" m S z r L h 9 h 7 i Z Y O g X V x 9 c s m E A 7 W y w = \" > A A A C a n i c h V G 7 S g N B F D 1 Z 3 / E V Y 6 O k C S Y R q 3 D X R r E S b S x 9 5 Q G J y O 4 6 6 u B m d 9 n d B D T 4 A 3 Z W g q k U R M T P s P E H L P I J o l 0 E G w t v N g u i Q b 3 D z J w 5 c 8 + d M z O 6 Y 0 r P J 2 p G l J 7 e v v 6 B w a H o 8 M j o 2 H h s I p 7 3 7 K p r i J x h m 7 Z b 1 D V P m N I S O V / 6 p i g 6 r t A q u i k K + t F q e 7 9 Q E 6 4 n b W v b P 3 b E T k U 7 s O S + N D S f q U K 6 L K 1 k P r 0 b S 1 G W g k h 2 A z U E K Y S (i,j) = x i x i+1 \u2022 \u2022 \u2022 x j in the cor- pus, scne constructs its representation v x (i,j) \u2208 R d by summing the embeddings of its sub-ngrams as follows: v x (i,j) = s\u2208S(x (i,j) ) z s , where S(x (i,j) ) = {x (i ,j ) \u2208 V | i \u2264 i \u2264 j \u2264 j} consists of all sub-n-grams of target x (i,j) , and the embeddings of sub-n-grams z s \u2208 R d , s \u2208 V are model parameters to be learned. The objective of scne is similar to that of Mikolov et al. (2013) , t\u2208D \uf8f1 \uf8f2 \uf8f3 c\u2208C(t) log \u03c3 v xt u xc + k s\u223cPneg log \u03c3 \u2212v xt u s \uf8fc \uf8fd \uf8fe , where \u03c3(x) = 1 1+exp(\u2212x) , D = {(i, j) | 1 \u2264 i \u2264 j \u2264 N, j \u2212 i + 1 \u2264 n target }, and C((i, j)) = {(i , j ) | x (i ,j ) \u2208 V, j = i \u2212 1 or i = j + 1}. D is the set of indexes of all possible target n-grams in the raw corpus with n \u2264 n target , where n target is a hyperparameter. C(t) is the set of indexes of contexts of the target x t , that is, all character ngrams in V that are adjacent to the target (see Figures 1 and 2). The negative sampling distribution P neg of s \u2208 V is proportional to its frequency in the corpus. The model parameters z s , u s \u2208 R d , s, s \u2208 V , are learned by maximizing the objective. We set n target = n max in our experiments.  Although we examine frequent n-grams for simplicity, incorporating supervised word boundary information or byte pair encoding into the construction of compositional n-gram set would be an interesting future work (Kim et al., 2018; Sennrich et al., 2016; Heinzerling and Strube, 2018) . Comparison to Oshikiri (2017) To avoid the problems of word segmentation, Oshikiri (2017) proposed segmentation-free word embedding (sembei) (Oshikiri, 2017 ) that considers the M -most frequent character n-grams as individual words. Then, a frequent n-gram lattice is constructed, which is similar to a word lattice used in morphological analysis (see Figure 3 ). Finally, the pairs of adjacent n-grams in the lattice are considered as target-context pairs and they are fed to existing word embedding methods, e.g., skipgram (Mikolov et al., 2013) . Although sembei is simple, the frequent n-gram vocabulary tends to include a vast amount of nonwords (Kim et al., 2018) . Furthermore, its vocabulary size is limited to M , hence, sembei can not avoid the undesirable issue of OOV. The proposed scne avoids these problems by taking all possible character n-grams as embedding targets. Note that the target-context pairs of sembei are fully contained in those of scne (see Figure 1 ). Comparison to Kim et al. (2018) To overcome the problem of OOV in sembei, Kim et al. (2018) proposed an extension of sembei called word-like n-gram embedding (wne). In wne, the n-gram vocabulary is filtered to have more vaild words by taking advantage of a supervised probabilistic word segmenter. Although wne reduce the number of non-words, there is still the problem of OOV since its vocabulary size is limited. In addition, wne is dependent on word segmenter while scne does not. Comparison to Bojanowski et al. (2017) To deal with OOV words as well as rare words, Bojanowski et al. (2017) proposed subword information skip-gram (sisg) that enriches word embeddings with the representations of its subwords, i.e., sub-character n-grams of words. In sisg, a vector representation of a target word is encoded as the sum of the embeddings of its subwords. For instance, subwords of length n = 3 of the word where are extracted as <wh, whe, her, ere, re>, where \"<\",\">\" are special symbols added to the original word to represent its left and right word boundaries. Then, a vector representation of where is encoded as the sum of the embeddings of these subwords and that of the special sequence <where>, which corresponds to the original word itself. Although sisg is powerful, it requires the information of word boundaries as its input, that is, semantic units need to be specified when encoding targets. Therefore, it cannot be directly applied to unsegmented languages. Unlike sisg, scne does not require such information. The proposed scne is much simpler, but due to its simpleness, the embedding target of scne should contains many non-words, which seems to be a problem (see Figure 1 ). However, our experimental results show that scne successfully captures the semantics of words and even sentences for unsegmented languages without using any knowledge of word boundaries (see Section 3). Experiments In this section, we perform two intrinsic and two extrinsic tasks at both word and sentence level, focusing on unsegmented languages. The implementation of our method is available on GitHub 1 . Common Settings Baselines: We use skipgram (Mikolov et al., 2013) , sisg (Bojanowski et al., 2017) and sembei (Oshikiri, 2017) as word embedding baselines. For sentence embedding, we first test simple baselines obtained by averaging the word vectors over a word-segmented sentence. In addition, we examine several recent successful sentence embedding methods, pv-dbow, pv-dm (Le and Mikolov, 2014) and sent2vec (Pagliardini et al., 2018) in an extrinsic task. Note that both scne and sembei have embeddings of frequent character n-grams as their model parameters, but the differences come from training strategies, such as embedding targets and the way of collecting cooccurrence information (see Section 2.1 for more details). For contrasting scne with sembei, we also propose a variant of sembei (denoted by sembei-sum) as one of baselines, which composes word and sentence embeddings by simply summing up the embeddings of their sub-n-grams which are learned by sembei. Hyperparameters Tuning: To see the effect of rich resources for the segmentation-dependent baselines, we employ widely-used word segmenter with two settings: Using only a basic dictionary (basic) or using a rich dictionary together (rich). The dimension of embeddings is 200, the number of epochs is 10 and the number of negative samples is 10 for all the methods. The n-gram vocabulary size M = 2 \u00d7 10 6 is used for sisg, sembei and scne. The other hyperparameters, such as learning rate and n max , are carefully adjusted via a grid search in the validation set. In the word similarity task, 2-fold cross validation is used for evaluation. In the sentence similarity task, we use the provided validation set. In the downstream tasks, vector representations are combined with a supervised logistic regression classifier. We repeat training and testing of the classifier 10 times, while the prepared dataset is randomly split into train (60%) and test (40%) sets at each time, and the hyperparameters are tuned by 3-fold cross validation in the train set. We adopt mean accuracy as the evaluation metric. See Appendix A.1 for more experimental details. Word and Sentence Similarity We measure the ability of models to capture semantic similarity for words and sentences in Chinese; see Appendix A.2 for the experiment in Japanese. Given a set of word pairs, or sentence pairs, and their human annotated similarity scores, we calculated Spearman's rank correlation between the cosine similarities of the embeddings and the scores. We use the dataset of Jin and Wu (2012) and Wang et al. (2017) for Chinese word and sentence similarity respectively. Note that the conventional models, such as skipgram, cannot provide the embeddings for OOV words, while the compositional models, such as sisg and scne, can compute the embeddings by using their subword modeling. In order to show comparable results, we use the null vector for these OOV words following Bojanowski et al. (2017) . Results: To see the effect of training corpus size, we train all models on portions of Wikipedia 2 . The results are shown in Figure 4 . As it can be seen, the proposed scne is competitive with or outperforms the baselines for both word and sentence similarity tasks. Moreover, it is worth noting that scne provides high-quality representations even when the size of training corpus is small, which is crucial for practical real-world settings where rich data is not available. For a next experiment to see the effect of noisiness of training corpus, we test both noisy SNS corpus and the Wikipedia corpus 3 of the same size. The results are reported in Table 1 . As it can be seen, the performance of segmentation-dependent methods (skipgram, sisg) are decreased greatly by the noisiness of the corpus, while scne degrades only marginally. The other two segmentation-free methods (sembei, sembei-sum) performed poorly. This shows the efficacy of our method in the noisy texts. On the other hand, in preliminary experiments on English (not shown), scne did not get better results than our segmentation-dependent baselines and it will be a future work to incorporate easily obtainable word boundary information into scne for segmented languages. Noun Category Prediction As a word-level downstream task, we conduct a noun category prediction on Chinese, Japanese and Korean 4 . Most settings are the same as those of Oshikiri (2017) . Noun words and their semantic categories are extracted from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) with a predetermined semantic category set 5 , and the classifier is trained to predict the semantic category of words from the learned word representations, where unseen words are skipped in training and treated as errors in testing. To see the effect of the noisiness of corpora, both noisy SNS corpus and Wikipedia corpus of the same size are examined as training corpora 6 . Results: The results are reported in Table 2 . Since the set of covered nouns (i.e., non-OOV words) depends on the methods, we calculate accuracies in two ways for a fair comparison: Using all the nouns and using the intersection of the covered nouns. scne achieved the highest accuracies in all the settings when using all the nouns, and also 4 Although Korean has spacing, word boundaries are not obviously determined by space. 5 {food, song, music band name, manga, fictional character name, television series, drama, chemical compound, disease, taxon, city, island, country, year, business enterprise, public company, profession, university, language, book} 6 For each language, we use 100MB of Wikipedia and SNS data as training corpora. For the SNS data, we use Sina Weibo for Chinese and Twitter for the rest. performed well when using the intersection of the covered nouns, especially for the noisy corpora. Sentiment Analysis As a sentence-level evaluation, we perform sentiment analysis on movie review data. We use 101k, 56k and 200k movie reviews and their scores respectively from Chinese, Japanese and Korean movie review websites (see Appendix A.1.6 for more details). Each review is labeled as positive or negative by its rating score. Sentence embedding models are trained using the whole movie reviews as training corpus. Among the reviews, 5k positive and 5k negative reviews are randomly selected, and the selected reviews are used to train and test the classifiers as explained in Section 3.1. Results: The results are reported in Table 3 . The accuracies show that scne is also very effective in the sentence-level application. In this experiment, we observe that the larger n max contributes to the performance improvement in sentence-level application by allowing our model to capture composed representations for longer phrases or sentences. Conclusion We proposed a simple yet effective unsupervised method to acquire general-purpose vector representations of words, phrases and sentences seamlessly, which is especially useful for languages whose word boundaries are not obvious, i.e., unsegmented languages. Although our method does not rely on any manually annotated resources or word segmenter, our extensive experiments show that our method outperforms the conventional approaches that depend on such resources. A Appendices A.1 Experimental Details A.1.1 Hyperparameters Tuning For skipgram, we performed a grid search over (h, \u03b3) \u2208 {1, 5, 10} \u00d7 {0.01, 0.025}, where h is the size of context window and \u03b3 is the initial learning rate. For sisg, we performed a grid search over (h, \u03b3, n min , n max ) \u2208 {1, 5, 10} \u00d7 {0.01, 0.025} \u00d7 {1, 3} \u00d7 {4, 8, 12}, where h is the size of context window, \u03b3 is the initial learning rate, n min is the minimum length of character n-gram and n max is the maximum length of character n-gram. For pv-dbow, pv-dm and sent2vec, we performed a grid search over (h, \u03b3) \u2208 {5, 10} \u00d7 {0.01, 0.05, 0.1, 0.2, 0.5}, where h is the size of context window and \u03b3 is the initial learning rate. For sembei and scne, we used the initial learning rate 0.01 and n min = 1. The maximum length of n-gram to consider n max is grid searched over {4, 6, 8} in the word and sentence similarity tasks. In the noun category prediction task, we used n max = 8 for sembei and the n max of scne is grid searched over {4, 6, 8}. For sentiment analysis task, we tested both n max = 8 and n max = 16 for sembei and scne to see the effect of large n max . After carefully monitoring the loss curve and the performance in the word and sentence similarity tasks, we set the number of epochs 10 for all methods. In preliminary experiments, we also tested the number of epochs 20 for the wordsegmentation-dependent baselines but there were no significant differences. In the two supervised downstream tasks, the learned vector representations are combined with the logistic regression classifier. The parameter C, which is the inverse of regularization strength of the classifier, is adjusted via a grid search over C \u2208 {0.1, 0.5, 1, 5, 10}. Again, as explained in the main paper, the hyperparamters are grid searched on the determined validation set for all experiments. A.1.2 Implementations Here we provide the list of implementations of baselines which are used in our experiments. For skipgram 7 , sisg 8 , sembei 9 , and sent2vec 10 , we use the official implementations provided by the authors. Meanwhile, as for pv-dbow and pv-dm, we employ a widely-used implementation of Gensim library 11 . A.1.3 Word Segmenters and Word Dictionaries for Unsegmented Languages Below we list the word segmentation tools and word dictionaries which are used in our experiments. We employed a widely-used word segmentation tool for each language. For Chinese language, we used jieba 12 with its default dictionary 13 or with an extended dictionary 14 , which fully supports both traditional and simplified Chinese characters. For Japanese, we used MeCab 15 with its default dictionary called IPADIC 15 along with specially designed neologisms-extended dictionary called mecab-ipadic-NEologd 16 . Note that, because this extended dictionary mecab-ipadic-NEologd is specially designed to include many neologisms, there is a significant word coverage improvement by using this word dictionary as it can be seen in the Japanese noun category prediction task in the main paper. For Korean, we used mecab-ko 17 with its default dictionary called mecab-ko-dic 18 along with another extended dictionary called NIADic 19 . A.1.4 Training Corpora We prepared Wikipedia corpora and SNS corpora for Chinese, Japanese and Korean for our experiments. For the Wikipedia corpora, we used the first 10, 50, 100, 200 and 300MB of texts from the publicly available Wikipedia dumps 20 . The texts are extracted by using WikiExtractor tool 21 . For Chinese SNS corpus, we used 100MB of Leiden Weibo Corpus (van Esch, 2012) from the head. For Japanese and Korean SNS corpora, we collected Japanese and Korean tweets using Twitter Streaming API. We removed usernames and URLs from the SNS corpora. There were many informal words, emoticons and misspellings in the SNS corpora. We preserved them without preprocessing to see the effect of the noisiness of training corpora in our experiments. A.1.5 Preprocess of Wikidata For the noun category prediction task, we extracted noun words and their semantic categories from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) following Oshikiri (2017) . We determined the semantic category set used in our experiments as follows: First, we collected Wikidata objects that have Chinese, Japanese, Korean and English labels. Next, we sorted the categories by the number of noun words, and removed categories (e.g., Wikimedia category or Wikimedia template) that do not represent any semantic category. We also removed out several categories that contain too many noun words (e.g., human) or too few noun words (e.g., academic discipline). Since there were several duplicated labels for different Wikidata objects, the number of nouns for each language is slightly different. Each category has at least 0.1k words and no more than 5k words. The numbers of extracted noun words that are used in our experiments were 22,468, 22,396 and 22,298 for Chinese, Japanese and Korean, respectively. A.1.6 Movie Review Datasets In the main paper, three movie review datasets are used to evaluate the quality of sentence embeddings. We used 101,114, 55,837 and 200,000 movie reviews and their rating scores from Yahoo\u5947\u6469\u96fb\u5f71 22 , Yahoo!\u6620\u753b 23 and Naver Movies 24 for Chinese, Japanese and Korean, respectively. A.2 Additional Experiment on Japanese In this section, we show the results of Japanese word similarity experiments. We use the datasets of Sakaizawa and Komachi (2018) . It contains 4427 pairs of words with human similarity scores. We omit sentence similarity task since there is no public widely-used benchmark dataset for Japanese yet. Following the main paper, given a set of word pairs and their human annotated similarity scores, we calculated Spearman's rank correlation between the cosine similarities of the embeddings and the human scores. We use 2-fold Acknowledgments We would like to thank anonymous reviewers for their helpful advice. This work was partially supported by JSPS KAKENHI grant 16H02789 to HS and 18J15053 to KF. ",
    "abstract": "We propose a new type of representation learning method that models words, phrases and sentences seamlessly. Our method does not depend on word segmentation and any humanannotated resources (e.g., word dictionaries), yet it is very effective for noisy corpora written in unsegmented languages such as Chinese and Japanese. The main idea of our method is to ignore word boundaries completely (i.e., segmentation-free), and construct representations for all character n-grams in a raw corpus with embeddings of compositional sub-ngrams. Although the idea is simple, our experiments on various benchmarks and real-world datasets show the efficacy of our proposal.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Chinese",
        "Japanese"
    ],
    "numcitedby": "4",
    "year": "2019",
    "month": "June",
    "title": "Segmentation-free compositional $n$-gram embedding"
}