{
    "framing_question": "To what extent do the authors engage with the communities that are the subject of their paper, and do they claim a participatory approach?",
    "response": "The authors do not mention engaging with the communities that are the subject of their paper, nor do they claim a participatory approach.",
    "article": "Although the task of semantic textual similarity (STS) has gained in prominence in the last few years, annotated STS datasets for model training and evaluation, particularly those with fine-grained similarity scores, remain scarce for languages other than English, and practically non-existent for minor ones. In this paper, we present the Serbian Semantic Textual Similarity News Corpus (STS.news.sr)an STS dataset for Serbian that contains 1192 sentence pairs annotated with fine-grained semantic similarity scores. We describe the process of its creation and annotation, and we analyze and compare our corpus with the existing news-based STS datasets in English and other major languages. Several existing STS models are evaluated on the Serbian STS News Corpus, and a new supervised bag-of-words model that combines part-of-speech weighting with term frequency weighting is proposed and shown to outperform similar methods. Since Serbian is a morphologically rich language, the effect of various morphological normalization tools on STS model performances is considered as well. The Serbian STS News Corpus, the annotation tool and guidelines used in its creation, and the STS model framework used in the evaluation are all made publicly available. Introduction Semantic Textual Similarity (STS), sometimes also referred to as Short-text Semantic Similarity (STSS), is the task of assigning a numerical score to a given pair of short texts based on the level of semantic equivalence between them. The minimal numerical score in a given range indicates complete semantic independence, while the maximal score indicates full semantic equality. Although STS has important implications for a whole range of other natural language processing tasks, including information retrieval, question answering, machine translation, textual entailment, etc., research on this topic started appearing only around a decade ago (Corley and Mihalcea, 2005; Mihalcea, Corley, and Strapparava, 2006; Islam and Inkpen, 2008) . Semantic Textual Similarity has gained in prominence since 2012, with its inclusion in the annual SemEval shared tasks (Agirre et al., 2012 (Agirre et al., , 2013 (Agirre et al., , 2014 (Agirre et al., , 2015 (Agirre et al., , 2016;; Cer et al., 2017) . A large collection of datasets with fine-grained semantic similarity scores has been annotated in this series of shared tasks, using a standardized methodology, and has been made publicly available. The move from binary similarity scores (Dolan and Brockett, 2005) to fine-grained ones has allowed for more precise model training and evaluation. However, most of this development has been limited to English. Several other major languages have been considered recently, including Spanish (Agirre et al., 2014 (Agirre et al., , 2015;; Cer et al., 2017) , French (Vu et al., 2014) , Portuguese (Fonseca et al., 2016) , Chinese and Japanese (Hayashi and Luo, 2016) , Arabic (Cer et al., 2017) , and Hindi (Agarwal et al., 2017) . Among them, only the datasets in Spanish, Portuguese, and Arabic have been made publicly available. To the best of our knowledge, there has been no development of STS datasets with fine-grained similarity scores for minor languages so far. In this paper, we present the Serbian Semantic Textual Similarity News Corpus (STS.news.sr) 1 -a publicly available STS dataset for Serbian annotated with finegrained semantic similarity scores. Although there has been some recent work on the broader task of semantic 1 http://vukbatanovic.github.io/STS.news.sr/ relatedness in Polish (Wr\u00f3blewska and Krasnowska-Kiera\u015b, 2017) , our dataset is, as far as we know, the first STS dataset for a Slavic language. The remainder of this paper is structured as follows: in Section 2 we describe the creation and annotation of STS.news.sr, while in Section 3 we analyze and compare it with the available STS datasets in other languages. Section 4 provides some baseline model results on STS.news.sr, as well as an evaluation of several supervised bag-of-words STS models. Within this section, we also assess the impact of morphological normalization methods for Serbian -a language with rich morphology -on STS models. Finally, in Section 5 we present our conclusions and some potential avenues of future research. Dataset Creation and Annotation The initial step in STS dataset creation is the acquisition of a suitable collection of short-text pairs. We deemed the existing Serbian Paraphrase Corpus (paraphrase.sr) 2 (Batanovi\u0107, Furlan, and Nikoli\u0107, 2011; Furlan, Batanovi\u0107, and Nikoli\u0107, 2013) , a set of 1194 sentence pairs gathered from the news domain, to be a suitable source for this purpose. Firstly, we went through the corpus and manually corrected any typographical errors and restored any missing diacritical marks. Two sentence pairs were removed from the dataset since one was found to be a duplicate and the other included a text longer than one sentence. The remaining 1192 sentence pairs were then given to five annotators who independently assigned a semantic similarity score to each pair. For the sake of standardization, we chose to follow the annotation methodology established in the SemEval STS tasks, and we adopted the scoring scheme (a 0 -5 Likert scale) and the general annotation guidelines used therein (Agirre et al., 2013) . However, our initial consultations with the annotators showed that the sentence pair examples for each score that are included in the SemEval annotation instructions can be somewhat unclear, particularly those for scores 2 -4. This issue had an effect on lowering task comprehension and annotation quality. To rectify this, we replaced all examples with new ones, and we increased the number of examples from one to three per score. In order to limit our own bias in the selection of new examples, we chose suitable pairs from the 2012 MSRPar and the 2013-2016 HDL SemEval STS corpora in English (since they all belong to the news domain), and we had them professionally translated into Serbian. We considered only those pairs whose averaged scores are integers -an integer average usually means that all annotators assigned the same similarity score to a particular pair, indicating its unambiguity. The final selection was made in consultation with the annotators to ensure the representativeness of each example. Our annotation guidelines and examples, in both Serbian and English, are available on the STS.news.sr repository. Once the instructions were finalized, all annotators first scored a subset of 60 randomly selected pairs from the corpus (~5% of the total), before proceeding to annotate the entire dataset. This initial batch was subsequently used to calculate the annotator self-agreement scores. The annotation process was completed within approximately two months. In order to make the annotation quicker and easier, we created STSAnno 3 , a simple offline annotation tool. STSAnno allows an annotator to view in parallel the texts in a pair, assign a semantic similarity score to them, and change or erase existing scores. Annotators can also assign a special symbol to a pair to temporarily skip it, which can be useful when faced with difficult examples. Scored, unscored, and skipped pairs are highlighted in different colors to easily distinguish between them. Sentence pairs can be scored in the order in which they are given, or in any other order chosen by the annotator. At all times, a statistical overview of the annotation progress is displayed. A screenshot of STSAnno during the annotation of STS.news.sr is shown in Figure 1 . Dataset Analysis The annotator self-agreements and the inter-annotator agreements were calculated using the Pearson correlation coefficient r. Table 1 contains the self-agreement scores and Table 2 the inter-annotator agreements. In addition to the pairwise inter-annotator scores, we also measure the agreement of each annotator with the average of the scores of all other annotators. The agreement scores are generally very high. Even though the annotators had different backgrounds (annotator #1 is a computational linguist, annotators #2 and #3 are linguists, while annotators #4 and #5 are non-linguists) there is no major difference in correlation values due to this. This indicates that with well-chosen example pairs and clear guidelines, even non-experts can achieve very high levels of annotation quality on STS corpora. Our average interrater agreement between an annotator and the average of the scores of all other annotators is 0.92, which is therefore the upper bound for STS model performance on this dataset. This agreement is higher than the ones reported for SemEval datasets from the news domain (Agirre et al., 2013 (Agirre et al., , 2014 (Agirre et al., , 2015) ) by around 0.05 -0.1, most likely due to the increased number and quality of the examples in our annotation instructions. The final similarity score for each sentence pair was obtained by averaging the scores of all five annotators. Figure 2 shows the distribution of sentence pairs within the Serbian STS News Corpus across the range of similarity score values. It is moderately balanced, with the exception of a large peak regarding the pairs with the score 3.0. However, similar distributional irregularities are also present in other news-based STS datasets. A comparison between our dataset and other publicly available STS corpora created from the news domain is shown in Table 3 . We consider the following corpora: \u2022 In English: the 2012 SemEval MSRPar corpus (Agirre et al., 2012) , the combined 2013-2016 collection of SemEval HDL corpora (Agirre et al., 2013 (Agirre et al., , 2014 (Agirre et al., , 2015 (Agirre et al., , 2016)) , and the 2014 SemEval Deft-news corpus (Agirre et al., 2014) . \u2022 In Spanish: the combined 2014-2015 SemEval News corpora (Agirre et al., 2014 (Agirre et al., , 2015)) . \u2022 In Portuguese: the 2016 ASSIN corpus, divided into European and Brazilian Portuguese portions (Fonseca et al., 2016) . \u2022 In Arabic: the 2017 SemEval translation of a part of the MSRPar corpus into Arabic (Cer et al., 2017) . The size of the Serbian STS News Corpus is average when compared to the other available STS corpora, both in terms of the number of sentence pairs and in terms of token count (we counted only alphanumerical tokens). The average length of a sentence in STS.news.sr is greater than in most other STS datasets, while the average similarity score is 2.51 -almost ideal given the 0 -5 score scale. In fact, STS.news.sr is much more balanced than the English SemEval MSRPar corpus, which is the one most similar to it in terms of source material, type, and size. However, nearly all of the considered STS corpora exhibit strong distribution peaks around score values 3 and 4, in case of the 0 -5 score scale, and scores 2 and 3 in case of the 0 -4 scale. The ASSIN corpora score distribution is heavily skewed toward the central 2 -4 values. The only Evaluation We evaluate several STS models on the Serbian STS News Corpus. As a performance metric, we utilize the Pearson correlation coefficient between the model output and the averaged annotated similarity scores, which we consider the gold standard. We first consider unsupervised models, and evaluate them on the entire STS.news.sr. Then, we move on to supervised algorithms, which are evaluated using 10-fold cross-validation with sorted stratification. Unsupervised Models The first unsupervised model we consider is the one used as a standard baseline in all SemEval STS shared tasks -a simple word overlap technique in which sentences are split into tokens using white space and then represented as bagof-words vectors in the multidimensional token space (Agirre et al., 2012) . Token counts in a sentence are binarized, so that each vector dimension has a value of one if that token appeared in the sentence, and zero otherwise. Cosine similarity is used to compute the similarity between such sentence vectors. We also improve upon this baseline by lowercasing the text, removing punctuation, and using the tokenizer for Serbian included in the ReLDI (Regional Linguistic Data Initiative) project repository 4 (Samard\u017ei\u0107, Ljube\u0161i\u0107, and Mili\u010devi\u0107, 2015; Ljube\u0161i\u0107, Erjavec, et al., 2016) . Since it proves to be highly beneficial, this improved tokenization approach is utilized for all subsequent models. The second baseline model that we use is one based on averaging the embeddings of words in a sentence and calculating the cosine similarity of the mean vectors. We employ the word2vec algorithm (Mikolov, Chen, et al., 2013; Mikolov, Sutskever, et al., 2013) , as implemented in the gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010) embedding models when used in this context. The skipgram word2vec architecture is trained on the Serbian web corpus srWaC (Ljube\u0161i\u0107 and Klubi\u010dka, 2014) , the largest publicly available text corpus in Serbian, containing 555 million tokens. The srWaC corpus is parsed to remove punctuation marks and words that are not in Serbian, and is then lowercased. This reduces the corpus to around 470 million tokens, with a vocabulary of around 3.8 million entries. We use 100-dimensional vectors and a window size of 10 for the skip-gram model. All other parameters are kept at the gensim default settings. In both baseline models, we experiment with a simple negation-marking technique in which a single word after a negation word is marked with a special prefix in order to distinguish it from its non-negated form. Such techniques were previously found useful in simple models for other semantic tasks, such as sentiment analysis, both in English (Pang, Lee, and Vaithyanathan, 2002) and in Serbian (Batanovi\u0107, Nikoli\u0107, and Milosavljevi\u0107, 2016) . The results of basic unsupervised STS model evaluations are presented in Table 4 . We find that the word overlap model outperforms the embedding-based one, indicating a higher level of string similarity between the sentences in STS.news.sr, which is to be expected given the method used to collect sentence pairs for the original paraphrase.sr corpus (Furlan, Batanovi\u0107, and Nikoli\u0107, 2013) . We therefore also consider a joint model in which the word2vec mean vector and the binarized bag-of-words vector of a sentence are concatenated and used in cosine similarity calculation. This joint baseline proves superior to both individual models. The proposed negation-marking technique is found beneficial on the word2vec baseline. However, since it has a slightly detrimental effect on the superior word overlap and joint models, we do not use it in further experiments. 4.1.1 Morphological Normalization Next, having in mind the morphological complexity of Serbian, we evaluate the impact of morphological normalization methods on our baseline STS models. The results are presented in Table 5 . Three stemming algorithms developed for Serbian are considered -the optimal and the greedy algorithm of Ke\u0161elj and \u0160ipka (2008) , and the improvement of the greedy algorithm by Milo\u0161evi\u0107 (2012) . We also evaluate a stemmer for Croatian, a language closely related to Serbian, by Ljube\u0161i\u0107 and Pand\u017ei\u0107, which is a refinement of the approach presented in (Ljube\u0161i\u0107, Boras, and Kubelka, 2007) . We use the SCStemmers package (Batanovi\u0107, Nikoli\u0107, and Milosavljevi\u0107, 2016) in which all of the aforementioned algorithms are implemented. Similarly, we consider two publicly available lemmatizers for Serbian and one for Croatian. The first lemmatizer for Serbian is BTagger, which is available in two variants -one that only normalizes word suffixes (Gesmundo and Samard\u017ei\u0107, 2012b) , and another that also deals with word prefixes, allowing for full lemmatization (Gesmundo and Samard\u017ei\u0107, 2012a) . In addition, we assess a lemmatization model for Croatian developed by Agi\u0107, Ljube\u0161i\u0107, and Merkler (2013) for the CST lemmatizer (Jongejan and Dalianis, 2009) . The final lemmatizer that is evaluated is the one for Serbian by Ljube\u0161i\u0107, Klubi\u010dka, et al. (2016) , which relies on a large inflectional lexicon and an improved part-of-speech tagger.   The results show that the application of morphological normalization has a consistently positive impact on the performance of word overlap and joint baseline models, and a consistently detrimental one on the purely embedding-based method. On average, stemmers tend to have a better effect on STS models than lemmatizers do. The best overall stemmer is Ljube\u0161i\u0107 and Pand\u017ei\u0107's stemmer for Croatian, although the optimal stemmer of Ke\u0161elj and \u0160ipka is a close second. Ljube\u0161i\u0107 and Pand\u017ei\u0107's stemmer was also found to be the best option for sentiment classification in Serbian (Batanovi\u0107 and Nikoli\u0107, 2016, 2017) , making it a good choice in The lemmatizer of Ljube\u0161i\u0107 et al. proves to be the best one in this setting, but it is still outmatched by the top two stemming algorithms. Supervised Models We limit the examination of supervised models to those that do not require more advanced syntactic tools, like dependency parsers, since the development of such tools for Serbian has only recently begun (Samard\u017ei\u0107 et al., 2017) . We therefore evaluate the performance of several bag-of-words models. The approach proposed by Islam and Inkpen (2008) is the most basic one we consider. Within it, each word from the shorter sentence is paired to its most similar word in the longer sentence, and word pair similarities are calculated as a mixture of three string similarity metrics and one corpus-based semantic similarity measure. Supervision is used in this method to determine the optimal balance between the string and the corpusbased measures in the final score. As the corpus-based measure, we utilize the cosine similarity of the same word2vec 100-dimensional vectors as before. We also evaluate three models derived from this basic approach. The first is LInSTSS (Language-independent Short-text Semantic Similarity), proposed by Furlan, Batanovi\u0107, and Nikoli\u0107 (2013) , in which word pair similarities are weighted according to the term frequencies of the words in question. We calculate the TF values using the srWaC corpus. The second one is POST STSS (Part-of-speech Tagsupported Short-text Semantic Similarity), proposed by Batanovi\u0107 and Boji\u0107 (2015) , which utilizes similarity weighting based on the part of speech of each word in a pair. In order to obtain POS tags we use the Serbian morphosyntactic tagger developed by Ljube\u0161i\u0107, Klubi\u010dka, et al. (2016) . This tagger produces morphosyntactic descriptors (MSDs) and POS tags according to the MULTEXT-East (Erjavec, 2017) M_s) . There are also MSD categories, such as adpositions (S), which are not divided into types -in these cases one weight is assigned to an entire category. The only exceptions to this classification scheme are the residuals (X), where a single weight is assigned to the entire category since only one type of residual (foreign) appears in STS.news.sr, and the punctuation category (Z), which is ignored, since punctuation is filtered out during tokenization. However, this category/type classification is applied only to those types for which actual MSD values are specified in the MULTEXT-East version 5 standard. For example, the standard allows for a separate type of copular verbs (Vc), but no tags are specified under this type and the utilized tagger does not employ it, so this category/type combination does not necessitate a separate weight. POST STSS requires a nested cross-validation during the first optimization phase in order to tune the model hyperparameters -the initial POS weight values, the initial POS interaction values, the initial string similarity weight, the choice of the POS weighting function, and the option of using a special weight value minimization process at the end of the first optimization phase. Here, for the sake of efficiency, we only optimize the initial POS weights and the initial POS interaction values in a nested three-fold CV. We consider the same options for their initial values as in (Batanovi\u0107 and Boji\u0107, 2015) . For the remaining hyperparameters we use the settings found optimal in previous experiments (Batanovi\u0107 and Boji\u0107, 2015) -the initial string similarity weight is set to 0.5, the arithmetic mean is the chosen POS weighting function, and the value minimization process is not used. Finally, we propose and evaluate a mixture of LInSTSS and POST STSS that uses both TF-based and POS-based weighting of word similarities. Within this model, similarities between words \ud835\udc56\ud835\udc56 and \ud835\udc57\ud835\udc57 are calculated as follows: \ud835\udc46\ud835\udc46\ud835\udc56\ud835\udc56\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) = (\ud835\udc64\ud835\udc64 \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 \u00d7 \ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) + \ud835\udc64\ud835\udc64 \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 \u00d7 \ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57)) \u00d7 \ud835\udc47\ud835\udc47\ud835\udc47\ud835\udc47(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) \u00d7 \ud835\udc43\ud835\udc43\ud835\udc43\ud835\udc43\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) where \ud835\udc64\ud835\udc64 \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 and \ud835\udc64\ud835\udc64 \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 are the string and the semantic similarity weights (\ud835\udc64\ud835\udc64 \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 + \ud835\udc64\ud835\udc64 \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 = 1), \ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) is the mixture of three string similarity metrics as defined in (Islam and Inkpen, 2008) , and \ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) is the corpusbased semantic similarity measure (as noted, we use the cosine similarity of word2vec vectors in this paper). \ud835\udc47\ud835\udc47\ud835\udc47\ud835\udc47(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) is the term frequency weighting function, as defined in (Furlan, Batanovi\u0107, and Nikoli\u0107, 2013) , while \ud835\udc43\ud835\udc43\ud835\udc43\ud835\udc43\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56, \ud835\udc57\ud835\udc57) is the part-of-speech weighting function (as noted, in this paper we always use the arithmetic mean of the weights for the parts of speech of words \ud835\udc56\ud835\udc56 and \ud835\udc57\ud835\udc57). The optimization procedure for this approach, which we name POS-TF STSS, is identical to the one used for POST STSS, since term frequencies are obtained from the srWaC corpus in an unsupervised way. In all supervised models, weight values are optimized in steps of 0.1. The string similarity weight is chosen from the [0.3, 0.7] range, while the POS weights are optimized in the [0.7, 1.3] range. In order to minimize the chance of overfitting to the training set, the hill climbing part of the POST/POS-TF STSS optimization is stopped heuristically, when there are no hill climbing moves left whose error reduction on the training set is at least 5% of the error reduction of the first move made in the climb. The 10-fold cross-validation results for all models are shown in Table 6 . We repeat the unsupervised model evaluation using 10-fold CV to be able to make a fair comparison between the performances of the unsupervised and the supervised models. Furthermore, we measure the impact of morphological normalization on supervised models, but we limit the scope to the tools that were previously found to be the best in each category -the stemmer of Ljube\u0161i\u0107 and Pand\u017ei\u0107, and the lemmatizer of Ljube\u0161i\u0107 et al. All of the evaluated STS models, both supervised and unsupervised, are made available as parts of STSFineGrain 6 , a collection of STS models and a unified framework for their evaluation, implemented in Java. Results show that supervised models perform noticeably better than unsupervised ones on non-normalized text, but the gap between the two narrows when stemming or lemmatization is applied. Stemming has a clearly positive effect on almost all models, while lemmatization only brings an improvement to word overlap methods, with mixed effects on supervised ones. The three models derived from Islam and Inkpen's approach consistently outperform the original algorithm. LInSTSS generally achieves results similar to POST STSS, but the POS-TF STSS mixture model performs better than both LInSTSS and POST STSS independently. In fact, when used in conjunction with the stemmer of Ljube\u0161i\u0107 and Pand\u017ei\u0107, POS-TF STSS achieves the best result among all the models that we considered. 4.2.1 Optimal Parameters Naturally, the optimal parameter values for supervised models vary somewhat from one morphological normalization approach to another, but there are consistent patterns that can be observed. The optimal string similarity weight in the basic Islam and Inkpen approach tends to be 0.7, resulting in an optimal semantic similarity weight of 0.3. This is not surprising given the higher level of string similarity between the sentences in STS.news.sr. Nevertheless, in the LInSTSS model the optimal value of the string similarity weight is a bit lower (0.6) which indicates that the addition of TF weighting increases the importance of non-surface forms of similarity. Some variation between the optimal POS weight settings of POST STSS and those of POS-TF STSS does exist. However, we did not encounter any systematic differences between the optimal parameters of these two algorithms, nor between their chosen optimal hyperparameter values. The optimal initial POS weights are most often set to the neutral value of 1.0, while the optimal initial POS interaction setting is usually to allow word pairings between all parts of speech. Common nouns typically retain a neutral POS weight value of 1.0 and are found to be more important than proper nouns, whose weight revolves around the 0.8 -0.9 mark. The weight for main verbs is almost universally set to the 1.3 maximum, indicating the central role of a verb in conveying the meaning of a sentence. This effect was also evident when applying POST STSS to data in English (Batanovi\u0107 and Boji\u0107, 2015) , and was previously noted by other researchers as well (Wiemer-Hastings, 2004 content and are therefore assigned a lower weight, most often in the 0.7 -0.9 range. With regard to this, participial adjectives are consistently found to be the most important kind of adjectives, with the maximum POS weight value. The weight of possessive adjectives 7 also tends to be augmented, but to a lesser extent, while other adjectives are most often assigned the 0.9 weight value. The weights within the adverb category follow a similar patternadverbial participles are assigned higher weights, around 1.2, while the weight of adverbs proper is usually 1.0 or 1.1. Numerals, particularly ordinal ones, are found to be quite important -their weight values usually approach the upper POS weight bound. The high weight values assigned to numerals, adverbs, and most adjectives probably indicate their importance in correctly measuring the exact level of semantic similarity between sentences whose main actions/verbs are the same. The POS weights of pronouns are generally lower, ranging between 0.7 and 1.0, while the weight value assigned to abbreviations tends to fluctuate between 0.8 and 1.0. The remaining parts of speech mostly consist of functional words, such as conjunctions, adpositions, interjections, etc., which do not contain salient semantic content and are, thus, assigned low weight values in the 0.7 -0.8 range. The optimized POS interaction matrix allows the pairing of words belonging to different parts of speech in most cases, but nonsensical pairings are generally prohibited, like the one of pronouns and purely functional words like conjunctions. However, the fact that most pairings remain permitted shows that such strict prohibitions are only useful in a very limited number of cases, and that, in performance optimization, the POST/POS-TF STSS models rely primarily on the modification of POS weight values. This conclusion is further validated by the fact that the optimal string similarity weight in these models usually remains at the starting value of 0.5. Consequently, the optimal semantic similarity weight has the same value. Conclusion In this paper, we have presented the Serbian STS News Corpus, the first STS corpus with fine-grained semantic similarity scores in a Slavic language. We have compared it to similar STS corpora in other languages and have evaluated several unsupervised baseline STS models on it. A number of previously presented supervised models have also been considered. In addition, we have proposed POS-TF STSS, a new bag-of-words method that uses both term frequency weighting and part-of-speech weighting, and outperforms similar algorithms on STS.news.sr. The effects of various morphological normalization techniques on STS model performances have also been evaluated. In particular, we have found that using the stemmer for Croatian by Ljube\u0161i\u0107 and Pand\u017ei\u0107 alongside the POS-TF STSS approach yields the best results among the evaluated models. Finally, the optimal values of supervised model parameters have been discussed. In the future, we plan to construct additional, topically distinct STS corpora in Serbian, and to use them to conduct a more thorough model evaluation. We also aim to examine the influence of gold score distribution irregularities on the behavior of STS models. Acknowledgements This work was partially supported by the III 44009 research grant of the Ministry of Education, Science, and Technological Development of the Republic of Serbia. The annotation of the Serbian STS News Corpus was supported by the Regional Linguistic Data Initiative (ReLDI) via the Swiss National Science Foundation grant no. 160501. The authors would like to thank Aleksandar Milinkovi\u0107 for assisting in srWaC lemmatization, and Ana Bjelogrli\u0107, Jelena Bo\u0161njak, Marko Jankovi\u0107, and Ognjen Kre\u0161i\u0107 for annotating the STS.news.sr corpus. The first author also served as an annotator.",
    "funding": {
        "military": 0.0,
        "corporate": 3.128162811005808e-07,
        "research agency": 0.8633899637765114,
        "foundation": 0.9999397295583361,
        "none": 0.0
    }
}