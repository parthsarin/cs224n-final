{
    "framing_question": "What are the listed limitations of the work, if any?",
    "response": "The limitations of the work are not explicitly stated in the paper. However, some potential limitations can be identified:\n\n1. **Dataset Size**: The PANACEA dataset, while comprehensive, is still limited in size compared to some other existing datasets. This may affect the generalizability of the findings and the performance of the proposed veracity assessment approaches on larger datasets.\n\n2. **Data Homogenization**: The authors mention the challenges faced in homogenizing the data from different sources, which may have resulted in the exclusion of valuable information or introduced biases in the dataset.\n\n3. **Narrow Focus on COVID-19**: The work is focused specifically on misinformation related to the COVID-19 pandemic. The approaches and findings may not generalize well to other domains or types of misinformation.\n\n4. **Lack of Evaluation on Adversarial Attacks**: The paper does not discuss or evaluate the proposed approaches' robustness against adversarial attacks, which are a growing concern in the context of automated veracity assessment.\n\n5. **Limited Analysis of Challenging Cases**: While the authors mention encountering challenging cases during evaluation, they do not provide a detailed analysis or discussion of these cases. This limits the understanding of the specific scenarios where the approaches may struggle and hinders the development of strategies to address these challenges.\n\n6. **Reliance on Pre-trained Models**: The proposed approaches rely on pre-trained language models and natural language inference models, which have inherent limitations and biases. The performance of the approaches may be affected by these factors.\n\nIt is important to note that these limitations are potential and may not necessarily impact the overall findings or the usefulness of the work. However, they highlight areas where future research and improvements can be made to strengthen the veracity assessment approaches and the PANACEA dataset.",
    "article": "We present a comprehensive work on automated veracity assessment from dataset creation to developing novel methods based on Natural Language Inference (NLI), focusing on misinformation related to the COVID-19 pandemic. We first describe the construction of the novel PANACEA dataset consisting of heterogeneous claims on COVID-19 and their respective information sources. The dataset construction includes work on retrieval techniques and similarity measurements to ensure a unique set of claims. We then propose novel techniques for automated veracity assessment based on Natural Language Inference including graph convolutional networks and attention based approaches. We have carried out experiments on evidence retrieval and veracity assessment on the dataset using the proposed techniques and found them competitive with SOTA methods, and provided a detailed discussion. Introduction In recent years, and particularly with the emergence of the COVID-19 pandemic, significant efforts have been made to detect misinformation online with the aim of mitigating its impact. With this objective, researchers have proposed numerous approaches and released datasets that can help with the advancement of research in this direction. Most existing datasets (D'Ulizia et al., 2021) focus on a single medium (e.g., Twitter, Facebook, or specific websites), a unique information domain (e.g., health information, general news, or scholarly papers), a type of information (e.g., general claims or news), or a specific application (e.g., verifying claims, or retrieving useful information). This inevitably results in a limited focus on what is a complex, multi-faceted phenomenon. With the aim of furthering research in this direction, the contributions of our work are twofold: (1) creating a new comprehensive dataset of misinformation claims, and (2) introducing two novel approaches to veracity assessment. In the first part of our work, we contribute to the global effort on addressing misinformation in the context of COVID-19 by creating a dataset for PANdemic Ai Claim vEracity Assessment, called the PANACEA dataset. It is a new dataset that combines different data sources with different foci, thus enabling a comprehensive approach that combines different media, domains and information types. To this effect our dataset brings together a heterogeneous set of True and False COVID claims and online sources of information for each claim. The collected claims have been obtained from online fact-checking sources, existing datasets and research challenges. We have identified a large overlap of claims between different sources and even within each source or dataset. Thus, given the challenges of aggregating multiple data sources, much of our efforts in dataset construction has focused on eliminating repeated claims. Distinguishing between different formulations of the same claim and nuanced variations that include additional information is a challenging task. Our dataset is presented in a large and a small version, accounting for different degrees of such similarity. Finally, the homogenisation of datasets and information media has presented an additional challenge, since fact-checkers use different criteria for labelling the claims, requiring a specific review of the different kinds of labels in order to combine them. In the second part of our work, we propose NLI-SAN and NLI-graph, two novel veracity assessment approaches for automated fact-checking of the claims. Our proposed approaches are centred around the use of Natural Language Inference (NLI) and contextualised representations of the claims and evidence. NLI-SAN combines the inference relation between claims and evidence with attention techniques, while NLI-graph builds on graphs considering the relationship between all the different pieces of evidence and the claim. Specifically we make the following contributions: \u2022 We describe the development of a comprehensive COVID fact-checking dataset, PANACEA, as a result of aggregating and de-duplicating a set of heterogeneous data sources. The dataset is available in the project website 1 , as well as a fully operational search platform to find and verify COVID-19 claims implementing the proposed approaches. \u2022 We propose two novel approaches to claim verification, NLI-SAN and NLI-graph. \u2022 We perform an evaluation of both evidence retrieval and the application of our proposed veracity assessment methods on our constructed dataset. Our experiments show that NLI-SAN and NLI-graph have state-of-the-art performance on our dataset, beating GEAR (Zhou et al., 2019) and matching KGAT (Liu et al., 2020) . We discuss challenging cases and provide ideas for future research directions. 2 Related Work (Shu et al., 2020) collecting not only claims from news content, but also social context and spatio-temporal information; NELA-GT-2018 (N\u00f8rregaard et al., 2019) with 713,534 articles from 194 news outlets; FakeHealth (Dai et al., 2020) collecting information from HealthNewsReview, a project critically analysing claims about health care interventions; PUBHEALTH (Kotonya and Toni, 2020) with 11,832 claims related to health topics; FEVER (Thorne et al., 2018a) as well as its later versions FEVER 2.0 (Thorne et al., 2018b) and FEVER-OUS (Aly et al., 2021) , containing claims based on Wikipedia and therefore constituting a well-defined, informative and non-duplicated information corpus; SciFact (Wadden et al., 2020 ) also from a very different domain, containing 1,409 scientific claims. Our dataset is a real-world dataset bringing together heterogeneous sources, domains and information types. Approaches to claim veracity assessment. We employ our dataset for automated fact-checking and veracity assessment (Zeng et al., 2021 (Thorne et al., 2018a; Nie et al., 2019; Stammbach and Neumann, 2019) . Different pieces of evidence have been previously combined using graph neural networks (Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020) . Many of these authors have centred their techniques on the use of NLI (Chen et al., 2017; Ghaeini et al., 2018; Parikh et al., 2016; Li et al., 2019) to verify the claim. In our work we also make use of NLI results of claim-evidence pairs, but propose alternative approaches built on a self-attention network and a graph convolutional network for veracity assessment. Dataset Construction This section describes our dataset construction by selecting COVID-19 related data sources ( \u00a73.1), and applying information retrieval and re-ranking techniques to remove duplicate claims ( \u00a73.2). Data Sources We first identified a set of COVID-19 related data sources to build our dataset. Our aim is to have the largest compilation of non-overlapping, labelled and verified claims from different media and information domains (Twitter, Facebook, general websites, academia), and used for different applications (media reporting, veracity evaluation, information retrieval challenges, etc.). We have included any large dataset or media, to our knowledge, related to that objective that includes claims together with their information sources. The data sources identified are shown in processing and combining these sources we obtained 20,689 initial claims. Claim De-duplication We processed claims and removed: exact duplicates; claims making only a direct reference to existing content in other media (audio, video, photos); automatically obtained content not representing claims; entries with claims or fact-checking sources in languages other than English. The similarity of claims was then analysed using: BM25 (Robertson et al., 1995; Crestani et al., 1998; Robertson and Zaragoza, 2009) and BM25 with MonoT5 re-ranking (Nogueira et al., 2020) . BM25 is a commonly-used ranking function that estimates the relevance of documents to a given query. MonoT5 uses a T5 model trained using as input the template 'Query:[query] Document:[doc] Relevant:', fine-tuned to produce as output the token 'True' or 'False'. A softmax layer applied to those tokens gives the respective relevance probabilities. These methods are used to identify not only claims similar in content, but also distinct claims that are sufficiently relevant when searching for information about them. This ensures that the claims presented are unique, and avoids overlap between training and testing cases when using the data to train veracity assessment models. These methods were carried out using Pyserini 2 and PyGaggle 3 . The set of claims was indexed and a search was performed for each of the claims to detect similar claims. We created two versions of the dataset by varying the similarity threshold between claims. The LARGE dataset excludes claims with a 90% probability of being similar, while in the SMALL dataset the probability is increased to 99%, as obtained through the MonoT5 model. These thresholds were chosen empirically by manual inspection of the results with simultaneous consideration of the efficiency of the method. As a further assessment of the uniqueness of the claims, we evaluated the de-duplication process using BERTScore 4 (Zhang et al., 2019) on the resulting datasets. We used the linked code with a RoBERTa-large model with baseline rescaling. We compared each claim with all the other claims in the dataset and kept the score of the most similar match. The mean and standard deviation, and the 90th percentile of claim similarity values are shown in the upper part of Table 3 . The average claim similarity has been drastically reduced in the LARGE dataset compared to the original dataset and further reduced in the SMALL dataset. To illustrate the difference between the two ver-   Example claims contained in the dataset are shown in Table 4 . Each of the entries in the dataset contains the following information: \u2022 Claim. Text of the claim. \u2022 Claim label. The labels are: False, and True. \u2022 Claim source. The sources include mostly fact-checking websites, health information websites, health clinics, public institutions sites, and peer-reviewed scientific journals. \u2022 Original information source. Information about which general information source was used to obtain the claim. \u2022 Claim type. The different types, explained in Section A.2, are: Multimodal, Social Media, Questions, Numerical, and Named Entities. Claim Veracity Assessment We develop a pipeline approach consisting of three steps: document retrieval, sentence retrieval and veracity assessment for claim veracity evaluation. Given a claim, we first retrieve the most relevant documents from COVID-19 related sources and then further retrieve the top N most relevant sentences. Considering each retrieved sentence as evidence, we train a veracity assessment model to assign a True or False label to the claim. Document Retrieval Document Dataset. In order to retrieve documents relevant to the claims, we first construct an additional dataset containing documents obtained from reliable COVID-19 related websites. These information sources represent a real-world comprehensive database about COVID-19 that can be used as a primary source of information on the pandemic. We have selected four organisations from which to collect the information: ( web content was downloaded using the Beautiful-Soup 5 and Scrapy 6 packages. Social networking sites and non-textual content were discarded. In total 19,954 web pages have been collected. The list of websites and the full content of each website constitute this additional dataset used for document retrieval. This dataset is enhanced with some additional websites used only in the document retrieval experiments, detailed in Section 5.1. Method. Information sources were indexed by creating a Pyserini Lucene index and PyGaggle was used to implement a re-ranker model on the results. The documents were split into paragraphs of 300 tokens segmented with a BERT tokenizer. To retrieve the information we first used a BM25 score. Additionally, we tested the effect of multistage retrieval by re-ranking the initial results using MonoBERT (Nogueira et al., 2019) and MonoT5 models, and query expansion using RM3 pseudorelevance feedback (Abdul-Jaleel et al., 2004) on the BM25 results (Lin, 2019; Yang et al., 2019) . MonoBERT uses a BERT model trained using as inputs the query and each of the documents to be re-ranked encoded together ([CLS]query[SEP]doc[SEP]), and then the [CLS] output token is passed to a single layer fully-connected network that produces the probability of the document being relevant to the query. Sentence Retrieval For each claim, once documents are retrieved using BM25 and MonoT5 re-ranking of the top 100 BM25 results, we then further retrieve the N most similar sentences obtained from the 10 most relevant documents. The relevance of the sentences is 5 https://www.crummy.com/software/ BeautifulSoup/ 6 https://scrapy.org/ calculated using cosine similarity in relation to the original claim. The similarity is obtained with the pre-trained model MiniLM-L12-v2 (Wang et al., 2020b) , using Sentence-Transformers 7 (Reimers and Gurevych, 2019) to encode the sentences. Veracity Assessment We propose two veracity assessment approaches built on the NLI results of claim-evidence pairs. For each of the most similar sentences (pieces of evidence) retrieved for a claim, we apply the pretrained NLI model RoBERTa-large-MNLI 8 (Liu et al., 2019) . This model acts as a cross-encoder on pairs of sentences, trained to detect the relationship between the two sentences: contradiction, neutrality, or entailment. The model is trained on the Multi-Genre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018) . The inference results are then used in our proposed approaches described below. NLI-SAN. The first approach, named NLI-SAN, incorporates the inference results of claim-evidence pairs into a Self-Attention Network (SAN) (See Figure 1a ). First, a claim is paired with each piece of retrieved relevant evidence. Each pair (c, e i ) is fed into a RoBERTa-large 8 model, and the last hidden layer output S i is used as its representation. Additionally, each pair is also fed to the mentioned RoBERTa-large-MNLI 8 model obtaining I i , a triplet containing the probability of contradiction, neutrality, or entailment. S i = RoBERTa(c, e i ) I i = RoBERTa NLI (c, e i ) (1) \u2026 + ! Evidence \" ! \u2026 RoBERTa RoBERTa MNLI \u2026 RoBERTa RoBERTa MNLI Claim ! \" \" ! \" ! ! \" \" # ! # \" $ ! $ \" (a) NLI-SAN ! Evidence \" ! Evidence \" \" \u2026 RoBERTa RoBERTa MNLI \u2026 NLI output MLP + Softmax Veracity classification output RoBERTa RoBERTa MNLI \u2026 \u2026 \" \" \" ! \" # \" $ + + \u2026 ! Claim ! RoBERTa \" ! ! \" \" Last hidden layer The sentence representation is combined with the NLI output through a Self Attention Network (SAN) (Galassi et al., 2020; Bahdanau et al., 2015) . + [0, 0, 1] contradict, neutral, entail ( \" ( ! ) \" ) ! * + % + & ! + &\" (b) NLI-graph The RoBERTa-encoded claim-evidence representation S i with length n S = n K = n V is mapped onto a Key K \u2208 R n K \u00d7d K and a Value V \u2208 R n V \u00d7d V , while the NLI output I i of each claim-evidence pair is mapped onto a Query Q \u2208 R n Q \u00d7d Q . The representation dimensionality is d K = d V = d Q = 1024. The attention function is defined as: Att(Q, K, V) = softmax(QK \u22a4 / \u221a d)V (2) While standard attention mechanisms use only the sentence representation information for the Key, Value and Query, here the inference information is used in the Query. This attention mechanism is applied to each of the claim-evidence pairs, and the outputs are concatenated into an output O SAN that is passed through a Multi-Layer Perceptron (MLP) with hidden size d h and a Softmax layer to generate the veracity classification output. \u0177 = softmax(MLP ReLU (O SAN )) (3) NLI-graph. We propose an alternative approach based on Graph Convolutional Networks (GCN). First, for each claim-evidence pair, we derive RoBERTa-encoded representations for the claims and evidence separately (using the pooled output of the last layer) and obtain NLI results of the pairs as before. C i = RoBERTa(c); E i = RoBERTa(e i ) (4) I i = RoBERTa NLI (c, e i ) (5) Next, we build an evidence network in which the central node is the claim and the rest of the nodes are the evidence. Two nodes are linked if their similarity value exceeds a pre-defined threshold, which is empirically set to 0.9 by comparing the results of the experimental evaluation described in the following section using different thresholds. The similarity is considered between claim and evidence, but also between pieces of evidence. Similarity calculation is performed following the same approach as in Section 4.2. The features considered in each evidence node are the concatenation of E i and I i . For the claim node we use its representation C i and a unity vector (0, 0, 1) for the inference. The network is implemented with the package PyTorch Geometric (Fey and Lenssen, 2019) , using in the first layer the GCNConv operator (Kipf and Welling, 2016) with 50 output channels and self-loops to the nodes, represented by: X \u2032 = D\u22121/2 \u00c2 D\u22121/2 XW, ( 6 ) where X is the matrix of node feature vectors, \u00c2 = A + I denotes the adjacency matrix with inserted self-loops, Dii = j=0 \u00c2ij its diagonal degree matrix, and W is a trainable weight matrix. Once the node representation is updated via GCN, all the node representations are averaged and passed to the MLP and the Softmax layer to generate the final veracity classification output. \u0177 = softmax(MLP ReLU (O graph )) (7) 5 Experiments In this section, we perform a twofold evaluation: We first evaluate our document retrieval methods (presented in \u00a74.1) on obtaining information relevant to the dataset claims from a database of COVID-19 related websites. We subsequently present an evaluation of the veracity assessment approaches for the claims (described in \u00a74.3). Document Retrieval In order to evaluate our document retrieval methods, we need the gold-standard relevant document for each claim. Therefore, in the documents dataset described in section 4.1 we additionally include the web content referenced in each of the information sources used to compile our claim dataset: The CoronaVirus Alliance Database. All web pages from the websites referenced as factchecking sources for the claims have been downloaded from 151 different domains. CoAID dataset. We downloaded the websites used as fact-checking sources of false claims and the websites where correct information on true claims is gathered from 68 different domains. MM-COVID. We collected both fact-checking sources and reliable information related to the claims of this dataset from 58 web domains. CovidLies dataset. We include the web content used as fact-checking sources of the misconceptions from 39 domains. We have not included web content from the TREC Challenges, as each of them is performed on a very large dataset specific to each challenge (CORD19 and Common Crawl corpus), as explained previously. Note that in our subsequent experiments, we have excluded all fact-checking websites to avoid finding directly the claim references. The results of the document retrieval are presented in Table 5 . For each claim, the precision@k is defined as 1 if the relevant result is retrieved in the top k list and 0 otherwise. We can see that by using BM25, it is possible in many cases to retrieve the relevant results at the very top of our searches. Combining BM25 with MonoBERT did not offer any improvement. It even introduced noise to the retrieval results, leading to inferior performance compared to using BM25 only on AP@5 and AP@10. MonoT5 appears AP@5 AP@10 AP@20 AP@100 BM25 to be more effective, consistently improving the retrieval results across all metrics. Moreover for this dataset the use of query expansion using RM3 pseudo-relevance feedback on the BM25 results does not improve the results. Veracity Assessment Evaluation Here we evaluate our proposed NLI-SAN and NLI-graph veracity assessment approaches. To gain a better insight into the benefits of the proposed architectures, we conducted additional experiments on the variants of the models including: \u2022 NLI, using only the NLI outputs of the claimevidence pairs. The outputs are concatenated and then passed through the final classification layer to generate veracity classification results. \u2022 NLI+sent, this is the ablated version of NLI-SAN without the self-attention layer. Here, the RoBERTa-encoded claim-evidence representations are concatenated with the NLI results and then fed to the classification layer to produce the veracity classification output. \u2022 NLI+PSent, this is similar to the previous ablated version, but using the pooled representation of the claim-evidence pair to concatenate with the NLI result. \u2022 NLI-graph \u2212abl , this is the ablated version of NLI-graph in which the node representation is the NLI result of the corresponding claim-evidence pair without its RoBERTaencoded representation. For NLI, NLI+sent and NLI-SAN, we consider the 5 most similar sentences for each claim, obtained from the 10 most relevant documents of the information source database. Those documents are retrieved using BM25 and MonoT5 re-ranking of the top 100 BM25 results. For NLI-graph, NLI-graph \u2212abl and NLI+PSent, in order to have enough nodes to benefit from the network structure, the number of retrieved sentences is increased to 30 for each claim, selected as the 3 most similar sentences from the top 10 retrieved documents. The retrieval procedure is as in sections 4.1 and 4.2. Details of parameter settings can be found in Appendix B. We compare against the SOTA methods GEAR 9 (Zhou et al., 2019) and KGAT 10 (Liu et al., 2020), with settings as described by the authors. For all approaches we perform 5-fold crossvalidation and report the averaged results on the SMALL dataset in Table 6 . By using the NLI information alone it is possible to obtain reasonable results for the True claims, however, this is not the case for the most relevant False claims. Once we add sentence representations the efficiency of the method increases significantly. Using NLI-SAN instead of simply concatenating contextualised claim-evidence representations and NLI outputs further improves the results. A similar observation can be made in the results generated by NLI-graph and its variants; the contextualised representations of claim-evidence pairs are much more important than merely using the corresponding NLI values. We also note that using the graph version NLI-graph obtains better scores 9 https://github.com/thunlp/GEAR 10 https://github.com/thunlp/KernelGAT than a non-graph model with the same information NLI+PSent, however the scores are still lower than the NLI-SAN method. Our method performs on a par with KGAT, while being simpler, and outperforms GEAR. Complementing the results for the SMALL dataset, Table 7 presents the results for the LARGE dataset. In general, we observe improved performance for all models across all metrics for both classes compared to the results on the SMALL dataset. The previous results in the SMALL dataset constitute a more challenging case, since the uniqueness of the claims is increased and therefore the veracity assessment models are not able to learn from similar claims when performing the assessment. Discussion Our results show that in document retrieval, we have obtained values of around 0.6 from a simple term scoring and re-ranking retrieval model. However, this baseline represents only a rough measure of quality using this technique, since we have only evaluated the retrieval of a single document specific to each claim; we have not evaluated the quality of other retrieved documents. The distinction into True and False claims can be rather coarse-grained. We note that initially we considered a larger number of veracity labels, including more nuanced cases that could be interesting to analyse (see A.1). However, we have not found a clear separation between complex cases and it would seem that different fact checkers do not follow the same conventions when labelling such cases. The development of datasets especially focused on such nuanced cases may be therefore an important line of work in the future, together with the development of techniques for these more complex situations. In analysing misclassified claims, we note some interesting cases. The scope and globality of the pandemic imply that similar issues are mentioned repeatedly on multiple occasions, yet claims to be verified may include nuances or specificities. This is challenging as it is easy to retrieve information that omits relevant nuances. E.g. The claim \"Barron Trump had COVID-19, Melania Trump says\" retrieves sentences such as \"Rudy Giuliani has tested positive for COVID-19, Trump says.\" with a similar structure and mentions but missing the key name. This type of situation could be addressed by using Named Entity Recognition (NER) methods that prioritise matching between the entities involved in the claim and the information sources. See e.g. (Taniguchi et al., 2018; Nooralahzadeh and \u00d8vrelid, 2018) . Other interesting cases involve claims for which documents with adequate information are retrieved, but the sentences containing evidence cannot be identified because they are too different from the original claim. E.g. The claim \"Vice President of Bharat Biotech got a shot of the indigenous COV-AXIN vaccine\" retrieves correct documents on the issue. Similar sentences are retrieved such as \"Covaxin which is being developed by Bharat Biotech is the only indigenous vaccine that is approved for emergency use.\". Despite being similar such retrieved sentences give no information about the claimed situation. In the retrieved document, the sentence \"The pharmaceutical company, has in a statement, denied the claim and said the image shows a routine blood test.\" contains the essential information to debunk the original claim but is missed by the sentence retrieval engine as it is very different from the claim (See Table A1 in Appendix C for other examples). Such cases are more difficult to deal with, as the similarity between claim and evidence is cer-tainly a good indicator of relevance. Nevertheless, these cases are very interesting for future work using more complex approaches. We have made an initial attempt to address this problem by representing claims and retrieved documents using Abstract Meaning Representation (Banarescu et al., 2013) in order to better select relevant information. Although the results were not satisfactory, it may be an interesting avenue for future exploration. Another line of future work is the design of strategies against adversarial attacks to mitigate possible risks to our system. Conclusions We have presented a novel dataset that aggregates a heterogeneous set of COVID-19 claims categorised as True or False. Aggregation of heterogeneous sources involved a careful deduplication process to ensure dataset quality. Fact-checking sources are provided for veracity assessment, as well as additional information sources for True claims. Additionally, claims are labelled with sub-types (Multimodal, Social Media, Questions, Numerical, and Named Entities). We have performed a series of experiments using our dataset for information retrieval through direct retrieval and using a multi-stage re-ranker approach. We have proposed new NLI methods for claim veracity assessment, attention-based NLI-SAN and graph-based NLI-graph, achieving in our dataset competitive results with the GEAR and KGAT state-of-the-art models. We have also discussed challenging cases and provided ideas for future research directions. A Data Sources Here we present detailed information of the data sources introduced in section 3.1. It is worth noting that for the construction of our dataset, we have only included sources or datasets that contain explicit veracity labels of specific claims, thus we have not included collections of tweets related to COVID that do not have veracity labels (Chen et al., 2020; Lamsal, 2021; Abdul-Mageed et al., 2021; Huang et al., 2020; Dimitrov et al., 2020; Kerchner and Wrubel, 2020; Qazi et al., 2020) . We have not included claims without independent fact-checking sources (Memon and Carley, 2020; Shahi et al., 2021) and information sources without formulated claims such as the collections of scholarly articles (Wang et al., 2020a; Chen et al., 2021) , news articles (Zhou et al., 2020) The data sources that we have used for the construction of our dataset are: \u2022 The CoronaVirusFacts/DatosCoronaVirus Alliance Database 11 . Published by Poynter 12 , this online publication combines factchecking articles from more than 100 factcheckers from all over the world, being the largest journalist fact-checking collaboration on the topic worldwide 13 . The publication is presented as an online portal, thus we had to develop scripts to crawl the content and extract the relevant claims, categories, and information sources. \u2022 CoAID dataset 14 . The dataset (Cui and Lee, 2020) contains fake news from fact-checking websites and real news from health information websites, health clinics, and public institutions. Unlike most other datasets, it contains a wide selection of true claims. \u2022 MM-COVID 15 . The multilingual dataset (Li et al., 2020) contains fake and true news collected from Poynter and Snopes 16 , being a good complement to the first data source. \u2022 CovidLies dataset 17 . The dataset (Hossain et al., 2020) contains a curated list of common misconceptions about COVID appearing in social media, carefully reviewed to contain very relevant and unique claims unlike other automatically collected datasets. \u2022 TREC Health Misinformation track 18 . Research challenge using claims on the health domain focused on information retrieval from general websites through the Common Crawl corpus 19 . This dataset is specialized in a very specific domain, and has been used for a very different application than the previous data sources. \u2022 TREC COVID challenge 20 . Research challenge (Voorhees et al., 2021; Roberts et al., 2020) using claims on the health domain focused on information retrieval from scholarly peer-reviewed journals through the CORD19 dataset (Wang et al., 2020a) , the largest existing compilation of such articles. Similar to the last source, but focused on scholarly papers unlike the other sources. A.1 Pre-processing A separate pre-processing step was carried out for each of the selected data sources: TREC Health Misinformation track. The claims used in the track were obtained and reformulated manually by us as affirmative claims (e.g., \"Can vitamin D cure COVID-19?\" was changed to \"Vitamin D cures COVID-19\") for consistency with the rest of the data sources and to allow claim veracity assessment. True and False claims are used. TREC COVID challenge. The claims used in the challenge were obtained and reformulated manually by us as full sentences using the explanations related to each query (e.g., for a given query \"coronavirus immunity\", and its explanation \"will SARS-CoV2 infected people develop immunity?\", we form the following claim, \"coronavirus infected people develop immunity\"). True and False claims are used. The above processed data sources were combined to provide 20,689 initial claims. A.2 Claim Categorisation The claims were analysed to identify types of claims that may be of particular interest, either for inclusion or exclusion depending on the type of analysis. The following types were identified: (1) Multimodal; (2) Social media references; (3) Claims including questions; (4) Claims including numerical content; (5) Named entities, including: PERSON \u2212 People, including fictional; ORGA-NIZATION \u2212 Companies, agencies, institutions, etc.; GPE \u2212 Countries, cities, states; FACILITY \u2212 Buildings, highways, etc. These entities have been detected using a RoBERTa base English model (Liu et al., 2019) trained on the OntoNotes Release 5.0 dataset (Weischedel et al., 2013) using Spacy 22 . B Parameter Setting In our veracity assessment experiments, the parameters of the initial RoBERTa models are frozen during the training. The inputs are padded and truncated to the longest sequence, and a ReLU function is used as the activation function for the hidden layer. The GCNConv outputs are padded to the longest graph size. The loss function used is cross-entropy. The size of the hidden layer is 50, the batch size is 30, and the training is performed for 100 epochs for NLI-SAN and its variants, and 200 epochs for NLI-graph and its variants. The optimizer used is AdamW (Loshchilov and Hutter, 2019) with \u03b2 1 = 0.9, \u03b2 2 = 0.999, a weight decay of 0.01, and a learning rate of 10 \u22122 for NLI, 10 \u22124 for NLI+Sent and NLI-SAN, and Acknowledgements This work was supported by the UK Engineering and Physical Sciences Research Council (grant no. EP/V048597/1, EP/T017112/1). ML and YH are supported by Turing AI Fellowships funded by the UK Research and Innovation (grant no. EP/V030302/1, EP/V020579/1). Claim Description \"Sugar causes a cytokine storm in the lungs that promotes COVID-19\" Retrieved documents are relating COVID and its cytokine storm effects, but without the specific mention of sugar, which does not cause a cytokine storm. However, being similar they give no information about the claimed situation. \"Barron In the retrieved document, the sentence \"The pharmaceutical company, has in a statement, denied the claim and said the image shows a routine blood test.\" contains the essential information to debunk the original claim. But it is missed by the sentence retrieval engine as it is very different from the claim. \"Masks can be sanitized in microwave\" Correct documents are retrieved with similar sentences such as \"Claiming masks can be sanitized in microwave resurfaces\". However, sentences such as \"The study authors cautioned health care workers against trying to clean masks this way. Microwaves melted the masks, making them useless.\" or \"He also warns people against using microwaves or ovens to heat their masks.\" that are present in the retrieved documents but are not similar enough to the claim are missed. Table A1 : Examples of errors in document or sentence retrieval. a learning rates of 10 \u22124 for NLI-graph, 10 \u22123 for NLI-graph \u2212abl , and 10 \u22125 for NLI+PSent, these last three with a step size of 0.1 after 100 epochs. C Additional Examples of Document or Sentence Retrieval Errors Here we expand on the examples mentioned in Section 5.3 related to difficulties in the document or sentence retrieval parts of the process. Table A1 presents in more detail the cases previously mentioned, and includes new examples.",
    "funding": {
        "military": 1.9361263126072004e-07,
        "corporate": 6.704270752999619e-07,
        "research agency": 0.9796672037507729,
        "foundation": 8.756606586823867e-05,
        "none": 0.0002532015890656103
    }
}