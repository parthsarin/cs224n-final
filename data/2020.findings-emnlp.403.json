{
    "article": "Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020) , and use a paraphraser to simulate multiple responses per training prompt. We find SMRT improves over a strong Transformer baseline as measured by human and automatic quality scores and lexical diversity. We also find SMRT is comparable to pretraining in human evaluation quality, and outperforms pretraining on automatic quality and lexical diversity, without requiring related-domain dialog data. Table 17: An example training pair with 20 independent random paraphrase samples of the response. Sampling is limited to the top 100 tokens per time-step. During training a new sample is taken each epoch. Introduction Non-task-oriented dialog is a low-resource NLP task. While large and noisy related corpora exist (e.g. movie subtitles, social media, and irclogs; Serban et al., 2018) , the publicly-released curated corpora are small. Serban et al. note that smaller corpora have lower lexical diversity and topic coverage, leading to models with poor quality non-diverse responses. Pretraining on larger data may improve performance, but requires a large dialog corpus in the right language and related domain. We leverage Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020) to overcome sparse dialog data. SMRT uses a wordlevel knowledge distillation-inspired objective and a paraphraser to simulate multiple references per training example. Khayrallah et al. introduce SMRT for machine translation (MT) and simulate training on all translations for a source sentence, assuming: (1) all paraphrases of a target are translations of the source; and (2) all translations of the source are paraphrases of the target. ( 1 ) is true for dialog, but (2) is not-valid chatbot responses vary in meaning. SMRT The tree of paraphrases includes some . . . . . . . . possible . . . . . . . . . . . . paraphrases of the original promt, a sampled path and some of the other tokens also considered in the training objective. We apply SMRT to chatbots and find that it: (1) improves human and automatic quality scores; (2) improves lexical diversity; (3) performs as well as pretraining in human evaluation with better performance on automatic measures of diversity and quality. Method We model the non-task-oriented dialog system (chatbot) task as conditional language modeling. These models are typically trained using Negative Log Likelihood (NLL) with respect to a single reference. An alternative approach is Knowledge Distillation (Hinton et al., 2015; Kim and Rush, 2016) which assumes access to a teacher distribution (q(y | x)) and minimizes the cross entropy with the teacher's probability distribution. Simulated Multiple Reference Training SMRT is structured similarly to word-level Knowledge Distialltion, but uses a paraphraser as the teacher distribution (q(y | y)). The paraphraser conditions on the reference y (rather than the source x) and generates a paraphrase y . Additionally, SMRT samples a new paraphrase of the reference every epoch. The SMRT training objective for the i th target word in the reference y, given the prompt x, with a target vocabulary V is: L SMRT = \u2212 v\u2208V p PARAPHRASER (y i = v | y, y j<i ) \u00d7 log p CHATBOT (y i = v | x, y j<i ) The paraphraser and chatbot each condition on the previously sampled paraphrase tokens (y j<i ). Experimental Setup Dialog models We train Transformer (Vaswani et al., 2017) chatbots in FAIRSEQ using parameters from the FLO-RES 1 benchmark for low-resource MT (Guzm\u00e1n et al., 2019) for both a standard NLL baseline and SMRT. 2 Following Khayrallah et al. (2020) , we sample from the 100 highest probability tokens from the paraphraser distribution at each time-step (Fan et al., 2018) . We train and evaluate on DailyDialog (Li et al., 2017) , a high quality corpus with multiple references for evaluation. We train on the \u223c 80,000 turns of English-learners practicing 'daily dialogues' in various contexts, e.g., chatting about vacation or food. See Appendix A for full details for replication. Paraphraser We use the state-of-the-art PRISM multilingual paraphraser Thompson and Post (2020a,b) . 3 It is trained as a multilingual MT model on \u223c 100 million sentence pairs in 39 languages. Paraphrasing is treated as zero-shot translation (e.g., English to English). Evaluation Protocols Human Evaluation We use Amazon Mechanical Turk to collect human judgments. For every HIT we display a prompt and two responses; the worker indicates their preferred response (or tie). Following Baheti et al. (2018) , we employ the pairwise bootstrap test (Efron and Tibshirani, 1994) and report statistical significance at the 95% confidence level. Automatic Quality Evaluation We use MUL-TIREFEVAL for DailyDialog (Gupta et al., 2019) . In \u00a7 4 we report METEOR, ROUGE-L, and GREEDY MATCH for the original and multiple references. See Appendix B for all 14 metrics. For reading ease we report metrics scaled 0 to 100. Automatic Diversity Evaluation To measure lexical diversity, we use the type/token ratio of unigrams, bigrams, and trigrams (Li et al., 2016) . Results SMRT is preferred over the baseline system in human evaluation, as shown in Table 2 . It outperforms the baseline in automatic quality too: see Table 3 . Our baseline outperforms nearly all systems in Gupta et al. (2019) for these metrics, 4 suggesting it is a strong baseline. SMRT has higher lexical diversity than the baseline, though not as high as the human reference response (Table 4 ). baseline SMRT tie 35.8% 43.5% 20.6% Table 2 : Human preference judgments. The output of SMRT is preferred over the baseline system. This preference is statistically significant at the 95% confidence level. Multi-Ref Single-Ref M R GM M R GM baseline 12.8 34.0 76.9 6.9 20.9 71.2 SMRT 13.8 36.1 77.7 8.1 24.0 72.5 Analysis SMRT outperforms a strong baseline; here we analyze it in additional settings: pretraining and MMI. Pretraining Pretraining is another way of incorporating auxiliary data in the model. We pretrain on the OpenSubtitles corpus (OS; Lison and Tiedemann, 2016) , 5 which consists of \u223c 200 million turns from movie subtitles. Similar to DailyDialog, it consists of conversational data on a variety of topics. After pretraining on OS, we fine-tune on DailyDialog. Results In the human evaluation (Table 5 ), SMRT performs comparably to baseline pretraining. In automatic evaluation (Table 6 ), SMRT outperforms pretraining. We combine SMRT with pretraining 6 and find that this again performs comparably to baseline pretraining in human evaluation, and pretraining with SMRT performs better in the automatic evaluation. Finally, we compare SMRT with and without pretraining, and find with pretraining is preferred in human evaluation, while they perform similarly on the automatic metrics. Pretraining improves the NLL baseline's diversity, but SMRT's diversity is still better. Combining SMRT with pretraining improves diversity compared to pretraining alone: see Table 7 . Overall, SMRT performs on par with pretraining in terms of human evaluation of quality, with better diversity and automatic metrics of quality. 7 Discussion It can be hard to find dialog corpora that are large, domain relevant, and in-language. Unlike pretraining, SMRT incorporates nondialog data. PRISM was trained to translate, and leveraged as a paraphrase model using zero-shot translation. It is not trained to generate dialog, yet we still leverage it to improve a chatbot. The paraphraser is trained on less data (\u223c 100 million sentences pairs, with \u223c 17 million English sentences) than is used for OpenSubtitles pretraining (\u223c 200 million turns-all in English), thus competitive performance is not a result of more data. PRISM was trained on formal text: Wikipedia, news (Global Voices, and SETimes) MMI Maximum Mutual Information (MMI) decoding, (1\u2212\u03bb) log p(y|x)+\u03bb log p(x|y), is commonly used in dialog to increase response diversity (Li et al., 2016) , however we did not find it helpful in our experiments. Following MMI-bidi, we rerank a 100-best list with a reverse model. 8 When comparing both models with MMI, we find human prefer SMRT to the baseline, see Table 9 . MMI degrades automatic measures of quality (Table 10 ) and diversity (Table 11 ) of both the baseline and SMRT models compared to standard decoding. The quality degradation is similar for both, but the degradation in diversity is more pronounced for SMRT. Examples For a training pair and paraphrased responses, see Related work Paraphrasing Neural paraphrasing is actively improving (Wieting et al., 2017 (Wieting et al., , 2019;; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c; Thompson and Post, 2020a,b) ; we expect future improved paraphrasers will improve SMRT. Since it trains toward a distribution rather than a 1-hot vector, SMRT may have more reasonable confidence levels. Simulated Multiple Reference Training Conclusion SMRT improves upon a strong Transformer baseline in quality and diversity. It also has human evaluation quality comparable to pretraining, with better automatic quality and lexical diversity. This method, which works even in settings where pretraining is impractical due to a lack of in-domain same language dialog data, has a high potential for impact in creating chatbots for more languages. A Experiment Setup A.1 Dialog Models We train Transformer conditional language models in FAIRSEQ using parameters from the FLORES 9 benchmark for low-resource machine translation (Guzm\u00e1n et al., 2019) for both the baseline and SMRT. We use the publicly released SMRT fork of FAIRSEQ (Ott et al., 2019; Khayrallah et al., 2020) , 10 along with the PRISM M39V1 paraphraser (Thompson and Post, 2020a) . 11 We use a 5-layer encoder and decoder, 512 dimensional embeddings, and 2 encoder and decoder attention heads. We regularize with 0.2 label smoothing, and 0.4 dropout. We optimize using Adam with a learning rate of 10 \u22123 . We train 100 epochs, and select the best checkpoint based on validation set perplexity. We generate with a beam size of 10, and no length penalty. Figure 1 shows the train command for SMRT, Figure 2 shows the train command for the NLL baseline. We train and evaluate on the DailyDialog corpus (Li et al., 2017) , as released by ParlAI (Miller et al., 2017) . 12 We pretrain on the OpenSubtitles corpus (OS; Lison and Tiedemann, 2016) . 13   Since SMRT compares the distribution over tokens from the paraphraser and chatbot their vocabularies must match, so we apply the PRISM Sen-tencePiece model (Kudo and Richardson, 2018) to the DailyDialog and OpenSubtitles corpora. The ParlAI release of DailyDialog is tokenized and lowercased. Since the data the paraphraser is trained on is not, we detokenize and recase the DailyDialog data. We then provide the PRISM dictionary when running FAIRSEQ-PREPROCESS (see Figure 3 ). For MMI we use SMRT for the reverse model as well. For pretraining + SMRT we use standard NLL for pretraining on OpenSubtitles, and fine-tune on DailyDialog with SMRT. A.2 Evaluation Protocols A.2.1 Human Evaluation We randomly sample 500 prompt-response pairs from the test set, and filter out any that are not distinct, leaving 482 pairs. A.2.2 Automatic Quality Evaluation In Appendix B we report the full automatic evaluation results of the 14 metrics across both the single reference and multi-reference evaluation from the the multi-reference automatic evaluation framework for DailyDialog released by Gupta et al. (2019) , which is computed using NLG-EVAL 14 (Sharma et al., 2017) . This include wordoverlap metrics: BLEU (Papineni et al., 2002) , ME-TEOR (Lavie and Agarwal, 2007) , and ROUGE-L (Lin, 2004) as well as embedding based metrics: SkipThought (Kiros et al., 2015) , embedding average (Forgues et al., 2014) , vector extrema and Greedy Matching (Rus and Lintean, 2012) . For reading ease, we reports metrics scaled between 0 and 100 rather than 0 and 1. A.2.3 Automatic Diversity Evaluation We compute the type/token ratio on tokenized text, using the same spaCy 15 tokenization used in the quality evaluation scripts. 16  python fairseq-smrt/train.py \\ $DATADIR \\ --source-lang src \\ --target-lang tgt \\ --seed 10 \\ --save-dir $SAVEDIR --paraphraser-lang-prefix \"<en>\" \\ --patience 50 --criterion smrt_cross_entropy \\ --paraphraser-model prism/m39v1/checkpoint.pt \\ --paraphraser-data-dir prism/m39v1/ \\ --paraphraser-sample-topN 100 \\ --prob-use-smrt 1.0 \\ --label-smoothing 0.2 \\ --share-all-embeddings \\ --arch transformer --encoder-layers 5 --decoder-layers 5 \\ --encoder-embed-dim 512 --decoder-embed-dim 512 \\ --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\ --encoder-attention-heads 2 --decoder-attention-heads 2 \\ --encoder-normalize-before --decoder-normalize-before \\ --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \\ --weight-decay 0.0001 \\ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \\ --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \\ --lr 1e-3 --min-lr 1e-9 --no-epoch-checkpoints \\ --max-tokens 4000 \\ --max-epoch 100 --save-interval 10 --update-freq 4 \\ --log-format json --log-interval 100 python fairseq-smrt/train.py \\ $DATADIR \\ --source-lang src \\ --target-lang tgt \\ --seed 10 \\ --save-dir $SAVEDIR \\ --patience 50 --criterion label_smoothed_cross_entropy \\ --label-smoothing 0.2 \\ --share-all-embeddings \\ --arch transformer --encoder-layers 5 --decoder-layers 5 \\ --encoder-embed-dim 512 --decoder-embed-dim 512 \\ --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\ --encoder-attention-heads 2 --decoder-attention-heads 2 \\ --encoder-normalize-before --decoder-normalize-before \\ --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \\ --weight-decay 0.0001 \\ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \\ --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \\ --lr 1e-3 --min-lr 1e-9 --no-epoch-checkpoints \\ --max-tokens 4000 \\ --max-epoch 100 --save-interval 10 --update-freq 4 \\ --log-format json --log-interval 100 python fairseq-smrt/preprocess.py \\ --source-lang src --target-lang tgt \\ --trainpref $path_to_sentencepieced_data/train.sp \\ --validpref $path_to_sentencepieced_data/valid.sp \\ --testpref $path_to_sentencepieced_data/test.sp \\ --srcdict prism/m39v1/dict.tgt.txt \\ --tgtdict prism/m39v1/dict.tgt.txt \\ --destdir $databin Table 12 and Table 13 show the evaluation against the multiple references for the word based and embedding based metrics. Table 14 sure, what about some lambs? PT + baseline: yes, I'd like to share some kebabs. PT + SMRT: yes, I would like to share some kebabs. prompt: Believe it or not, tea is the most popular beverage in the world after water. baseline: Really? What's the nature of this city? SMRT: Really? I am so surprised. PT + baseline: That's true. Tea is the most popular in the world. PT + SMRT: Yes, I agree with you, but I don't know what to do. Table 20 : An evaluation prompt and example output. The baseline is off-topic, but SMRT and baseline pretraining give good (but very different) responses. Pretrained SMRT is less ideal, though could be a fine response in some contexts. prompt: Oh, good. How old is your niece? baseline: I'm afraid I'm not sure. SMRT: Eighteen. PT + baseline: She's 42. PT + SMRT: She is ten years old. You can buy a ticket from the buyer's cheque. PT + baseline: Sure. Go straight ahead and turn left. PT + SMRT: Go straight ahead and turn left, turn left, turn left and turn left.   Oh, we never speak Spanish. PT + baseline: I don't know. PT + SMRT: No, I don't speak English in the class. Table 24 : An evaluation prompt and example output. The baseline pretraining gives an 'I don't know' variant. The baseline is okay, the SMRT systems have better output. Acknowledgments We thank Patrick Xia, Claire Daniele, Nathaniel Weir, Carlos Aguirre, and additional anonymous proofreaders for their helpful comments and feedback on the paper. We additionally thank the reviewers for their insightful comments.",
    "abstract": "Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020) , and use a paraphraser to simulate multiple responses per training prompt. We find SMRT improves over a strong Transformer baseline as measured by human and automatic quality scores and lexical diversity. We also find SMRT is comparable to pretraining in human evaluation quality, and outperforms pretraining on automatic quality and lexical diversity, without requiring related-domain dialog data. Table 17: An example training pair with 20 independent random paraphrase samples of the response. Sampling is limited to the top 100 tokens per time-step. During training a new sample is taken each epoch.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "1",
    "year": "2020",
    "month": "November",
    "title": "{SMRT} Chatbots: {I}mproving Non-Task-Oriented Dialog with {S}imulated {M}ultiple {R}eference {T}raining"
}