{
    "article": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -making for more human-like annotations than previous approaches. Introduction Gaining a better understanding of natural language, and especially natural language associated with images helps drive research in both computer vision and natural language processing (e.g., Barnard et al. (2003) , Pastra et al. (2003) , Feng and Lapata (2010b) ). In this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language. Automatic generation of image descriptions differs from automatic image tagging (e.g., Leong et al. (2010) ) in that we aim to generate complex phrases or sentences describing images rather than predicting in-dividual words. These natural language descriptions can be useful for a variety of applications, including image retrieval, automatic video surveillance, and providing image interpretations for visually impaired people. Our work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et al. (2010) ), or summarizing existing text fragments associated with an image (e.g., Aker and Gaizauskas (2010) , Feng and Lapata (2010a) ). Second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related (but subtly different) work in automatic caption generation creates news-worthy text (Feng and Lapata (2010a)) or encyclopedic text (Aker and Gaizauskas (2010) ) that is contextually relevant to the image, but not closely pertinent to the specific content of the image. Third, we aim to build a general image description method as compared to work that requires domain specific hand-written grammar rules (Yao et al. (2010) ). Last, we allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011) . In this work, we propose a novel surface realization technique based on web-scale n-gram data. Our approach consists of two steps: (n-gram) phrase selection and (n-gram) phrase fusion. The first step phrase selection -collects candidate phrases that may be potentially useful for generating the description of a given image. This step naturally accommodates uncertainty in image recognition inputs as well as synonymous words and word re-ordering to improve fluency. The second step -phrase fusion -finds the optimal compatible set of phrases using dynamic programming to compose a new (and more complex) phrase that describes the image. We compare the performance of our proposed approach to three baselines based on conventional techniques: language models, parsers, and templates. Despite its simplicity, our approach is highly effective for composing image descriptions: it generates mostly appealing and presentable language, while permitting creative writing at times (see Figure 5 for example results). We conclude from our exploration that (1) it is viable to generate simple textual descriptions that are germane to the specific image content, and that (2) world knowledge implicitly encoded in natural language (e.g., web-scale ngram data) can help enhance image content recognition. In this section, we briefly describe the image recognition system that extracts visual information and encodes it as a set of triples. For a given image, the image recognizer extracts objects, attributes and spatial relationships among objects as follows: Image Recognition 1. Objects: including things (e.g., bird, bus, car) and stuff (e.g., grass, water, sky, road) are detected. 2. Visual attributes (e.g., feathered, black) are predicted for each object. 3. Spatial relationships (e.g., on, near, under) between objects are estimated. In particular, object detectors are trained using state of the art mixtures of multi-scale deformable parts models (Felzenszwalb et al., 2010) . Our set of objects encompasses the 20 PASCAL 2010 object challenge 1 categories as well as 4 additional categories for flower, laptop, tiger, and window trained on images with associated bounding boxes from Imagenet (Deng et al., 2009) . Stuff detectors are trained to detect regions corresponding to non-part based object categories (sky, road, building, tree, water, and grass) using linear SVMs trained on the low level region features of (Farhadi et al., 2009) . These are also trained on images with labeled bounding boxes from ImageNet and evaluated at test time on a coarsely sampled grid of overlapping square regions over whole images. Pixels in any region with a classification probability above a fixed threshold are treated as detections. We select visual attribute characteristics that are relevant to our object and stuff categories. Our attribute terms include 21 visual modifiers -adjectives -related to color (e.g. blue, gray), texture (e.g. striped, furry), material (e.g. wooden, feathered), general appearance (e.g. rusty, dirty, shiny), and shape (e.g. rectangular) characteristics. The attribute classifiers are trained on the low level features of (Farhadi et al., 2009) using RBF kernel SVMs. Preposition functions encoding spatial relationships between objects are hand designed to evaluate the spatial relationships between pairs of regions in an image and provide a score for 16 prepositions (e.g., above, under, against, in etc). From these three types of visual output, we construct a meaning representation of an image as a set of triples (one triple for every pair of detected objects). Each triple encodes a spatial relation between two objects in the following format: <<adj1, obj1>, prep, <adj2, obj2>>. The generation procedure is elaborated in the following two sections. Baseline Approaches to Surface Realization This section explores three baseline surface realization approaches: language models ( \u00a73.1), randomized local search ( \u00a73.2), and template-based ( \u00a73.3). Our best approach, phrase fusion using web-scale ngrams follows in \u00a74. Language Model Based Approach For each triple, as described in \u00a72, we construct a sentence. For instance, given the triple <<white, cloud>, in, <blue, sky>>, we might generate \"There is a white cloud in the blue sky\". We begin with a simple decoding scheme based on language models. Let t be a triple, and let V t be the set of words in t. We perform surface realization by adding function words in-between words in V t . As a concrete example, suppose we want to determine whether to insert a function word x between a pair of words \u03b1 \u2208 V t and \u03b2 \u2208 V t . Then, we need to compare the length-normalized probability p(\u03b1x\u03b2) with p(\u03b1\u03b2), where p takes the n'th root of the probability p for n-word sequences. We insert the new function word x if p(\u03b1x\u03b2) \u2265 p(\u03b1\u03b2) using the n-gram models, where the probability of any given sequence w 1 , ..., w m is approximated by p(w 1 , ..., w m ) = m i=1 p(w i |w i\u2212(n\u22121) , ..., w i\u22121 ) Note that if we wish to reorder words in V t based on n-gram based language models, then the decoding problem becomes an instance of asymmetric traveler's salesman problem (NP-hard). For brevity, we retain the original order of words in the given triple. We later lift this restriction using the web-scale ngram based phrase fusion method introduced in \u00a74. enforce long distance regularities for more grammatically correct generation. However, optimizing both language-model-based probabilities and parser-based probabilities is intractable. Therefore, we explore a randomized local search approach that makes greedy revisions using both language models and parsers. Randomized local search has been successfully applied to intractable optimization problems in AI (e.g., Chisholm and Tadepalli (2002) ) and NLP (e.g., White and Cardie (2002) ). Randomized Local Search Approach Table 1 shows the skeleton of the algorithm in our study. Iterating through a loop, it chooses an edit location and an edit operation (insert, delete, or replace) at random. If the edit yields a better score, then we commit the edit, otherwise we jump to the next iteration of the loop. We define the score as score(X) = pLM (X)p P CF G (X) where X is a given sentence (image description), pLM (X) is the length normalized probability of X based on the language model, and pP CF G (X) is the length normalized probability of X based on the probabilistic context free grammar (PCFG) model. The loop is repeated until convergence or a fixed number of iterations is reached. Note that this approach can be extended to simulated annealing to allow temporary downward steps to escape from local maxima. We use the PCFG implementation of Klein and Manning (2003) . Template Based Approach The third approach is a template-based approach with linguistic constraints, a technique that has often been used for various practical applications such as summarization (Zhou and Hovy, 2004 ) and dia-  logue systems (Channarukul et al., 2003) . Because the meaning representation produced by the image recognition system has a fixed pattern of <<adj1, obj1>, prep, <adj2, obj2>>, it can be templated as \"There is a [adj1] [obj1] [prep] the [adj2] [obj2].\" We also include templates that encode basic discourse constraints. For instance, the template that generated the first sentences in Figure 3 and 4 is: [PREFIX] [#(x 1 )] [x 1 ], [#(x 2 )] [x 2 ], ... and [#(x k )] [x k ] , where x i is the name of an object (e.g. \"cow\"), #(x i ) is the number of instances of x i (e.g. \"one\"), and PREFIX \u2208 {\"This picture shows\", \"This is a picture of\", etc}. Although this approach can produce good looking sentences in a limited domain, there are many limitations. First, a template-based approach does not allow creative writing and produces somewhat stilted prose. In particular, it cannot add interesting new words, or replace existing content words with better ones. In addition, such an approach does not allow any reordering of words which might be necessary to create a fluent sentence. Finally, hand-written rules are domain-specific, and do not generalize well to new domains. Surface Realization by Phrase Fusion using Web-scale N-gram We now introduce an entirely different approach that addresses the limitations of the conventional ap-proaches discussed in \u00a73. This approach is based on web-scale n-gram, also known as Google Web 1T data, which provides the frequency count of each possible n-gram sequence for 1 \u2264 n \u2264 5. [Step I] -Candidate Phrase Selection We first define three different sets of phrases for each given triple <<adj1, obj1>, prep, <adj2, obj2>>: \u2022 O 1 = {(x, f ) | x is an n-gram phrase describing the first object using the words adj1 and obj1, and f is the frequency of x} \u2022 O 2 = {(x, f ) | x is an n-gram phrase describing the second object using the words adj2 and obj2, and f is the frequency of x} \u2022 R = {(x, f ) | x is an n-gram describing the relation between the two objects using the words obj1 and obj2, and f is the frequency of x} We find n-gram phrases for O 1 , O 2 , and R from the Google Web 1T data. The search patterns for O 1 is: \u2022 [adj 1 ] [\u2663] n\u22122 [obj 1 ] \u2022 [obj 1 ] [\u2663] n\u22122 [adj 1 ] where [\u2663] is a wildcard word, and [\u2663] n\u22122 denotes a sequence of n-2 number of wildcard words in a ngram sequence. For wildcards, we only allow a limited set of function words, and verbs in the gerund form 2 for reasons that will become clearer in the next step -phrase fusion in \u00a74.2. Note that it is the second pattern that allows interesting re-ordering of words in the final sentence generation. For instance, suppose adj1=green, obj1=person. Then it is more natural to generate a phrase using the reverse pattern such as, \"person in green\" or \"person wearing green\" than simply concatenating adj1 and obj1 to generate \"green person\". Similarly, given obj1=bicycle and obj2=man, generating a phrase using the reverse pattern, e.g., \"man with a bicycle\" would be more natural than \"bicycle with a man\". Our hypothesis is that such ordering preference is implicitly encoded in the web-scale n-grams via frequencies. It is worthwhile to note that our pattern matching is case sensitive, and we only allow patterns that are Template This picture shows one cow, one building, one grass and one sky. The black cow is by the shiny building, and by the furry grass, and by the blue sky. The shiny building is by the furry grass, and by the blue sky. The furry grass is below the blue sky. Simple decoding the black cow or by the furry grass. the shiny building up by the blue sky. the furry grass be below one blue sky.  all lower-case. From our pilot study, we found that n-grams with upper case characters are likely from named entities, which distort the n-gram frequency distribution that we rely on during the phrase fusion phase. To further reduce noise, we also discard any n-gram that a character that is not an alphabet. Image Recognition Accommodating Uncertainty We extend candidate phrase selection in order to cope with uncertainty from the image recognition. In particular, for each object detection obj i , we include its top 3 predicted modifiers adj i1 , adj i2 , adj i3 determined by the attribute classifiers (see \u00a72) to expand the set O 1 and O 2 accordingly. For instance, given adj i =(shiny or white) and obj i = sheep, we can consider both <shiny,sheep> and <white,sheep> pairs to predict more compatible pairs of words. Accommodating Synonyms Additionally, we augment each modifier adj i and each object name obj i with synonyms to further expand our sets O 1 , O 2 , and R. These expanded sets of phrases enable resulting generations that are more fluent and creative. [Step II] -Phrase Fusion Given the expanded sets of phrases O 1 , O 2 , and R described above, we perform phrase fusion to generate simple image description. In this step, we find the best combination of three phrases, ( x1 , f1 ) \u2208 O 1 , ( x2 , f2 ) \u2208 O 2 , and ( xR , fR ) \u2208 R as follows: Computational Efficiency One advantage of our phrase fusion method is its efficiency. If we were to attempt to re-order words with language models in a naive way, we would need to consider all possible permutations of words -an NP-hard problem ( \u00a73.1). However, our phrase fusion method is clever in that it probes reordering only on selected pairs of words, where reordering is likely to be useful. In other words, our approach naturally ignores most word pairs that do not require reordering and has a time complexity of only O(K 2 n), where K is the maximum number of candidate phrases of any phrase type, and n is the number of phrase types in each sentence. K can be kept as a small constant by selecting K-best candidate phrases of each phrase type. We set K = 10 in this paper. ( x1 , x2 , xR ) = argmax x 1 ,x 2 ,x R score(x 1 , x 2 , x R ) (1) score(x 1 , x 2 , x R ) = \u03c6(x 1 ) \u00d7 \u03c6(x 2 ) \u00d7 \u03c6(x R ) (2) Experimental Results To construct the training corpus for language models, we crawled Wikipedia pages that describe our object set. For evaluation, we use the UIUC PAS-CAL sentence dataset 3 which contains upto five human-generated sentences that describing 1000 images. Note that all of the approaches presented in   Section 3 and 4 attempt to insert function words for surface realization. In this work, we limit the choice of function words to only those words that are likely to be necessary in the final output. 4 For instance, we disallow function words such as \"who\" or \"or\". Before presenting evaluation results, we present some samples of image descriptions generated by 4 different approaches in Figure 3 and 4 . Notice that only the PHRASE FUSION approach is able to include interesting and adequate verbs, such as \"eating\" or \"looking\" in Figure 3 , and \"operating\" in Figure 4 . Note that the choice of these action verbs is based only on the co-occurrence statistics encoded in n-grams, without relying on the vision component that specializes in action recognition. These examples therefore demonstrate that world knowledge implicitly encoded in natural language can help enhance image content recognition. Automatic Evaluation: BLEU (Papineni et al., 2002) is a widely used metric for automatic evaluation of machine translation that measures the ngram precision of machine generated sentences with respect to human generated sentences. Because our task can be viewed as machine translation from images to text, BLEU (Papineni et al., 2002) like a reasonable choice. However, there is larger inherent variability in generating sentences from images than translating a sentence from one language to another. In fact two people viewing the same picture may produce quite different descriptions. This means BLEU could penalize many correctly generated sentences, and be poorly correlated with human judgment of quality. Nevertheless we report BLEU scores in absence of any other automatic evaluation method that serves our needs perfectly. The results are shown in Table 2 -first column shows BLEU score considering exact matches, second column shows BLEU with full credit for synonyms. To give a sense of upper bound and to see some limitations of the BLEU score, we also compute the BLEU score between human-generated sentences by computing the BLEU score of the first human sentence with respect to the others. There is one important factor to consider when interpreting Table 2 . The four approaches explored in this paper are purposefully prolific writers in that they generate many more sentences than the number of sentences in the image descriptions written by humans (available in the UIUC PASCAL dataset). In this work, we do not perform sentence selection to reduce the number of sentences in the final output. Rather, we focus on the quality of each generated sentence. The consequence of producing many  more sentences in our output is overall lower BLEU scores, because BLEU precision penalizes spurious repetitions of the same word, which necessarily occurs when generating more sentences. This is not an issue for comparing different approaches however, as we generate the same number of sentences for each method. From Table 2 , we find that our final approach -PHRASE FUSION based on web-scale n-grams performs the best. Notice that there are two different evaluations for PHRASE FUSION: the first one is evaluated for the best combination of phrases (Equation (1)), while the second one is evaluated for the best combination of phrases that contained at least one gerund. Human Evaluation: As mentioned earlier, BLEU score has some drawbacks including obliviousness to correctness of grammar and inability to evaluate the creativity of a composition. To directly quantify these aspects that could not be addressed by BLEU, we perform human judgments on 120 instances for the four proposed methods. Evaluators do not have any computer vision or natural language generation background. We consider the following three aspects to evaluate the our image descriptions: creativity, fluency, and relevance. For simplicity, human evaluators assign one set of scores for each aspect per image. The scores range from 1 to 3, where 1 is very good, 2 is ok, and 3 is bad. 5 The definition and guideline for each aspect is: [Creativity] How creative is the generated sentence? 1 There is creativity either based on unexpected words (in particular, verbs), or describing things in a poetic way. 2 There is minor creativity based on re-ordering words that appeared in the triple 3 None. Looks like a robot talking. [Fluency] How grammatically correct is the generated sentence? 1 Mostly perfect English phrase or sentence. 2 There are some errors, but mostly comprehensible. 3 Terrible. [Relevance] How relevant is the generated description to the given image? 1 Very relevant. 2 Reasonably relevant. 3 Totally off. Notice that the relevance score of TEMPLATE is better than that of LANGUAGE MODEL, even though both approaches generate descriptions that consist of an almost identical set of words. This is presumably because the output from LANGUAGE MODEL contains grammatically incorrect sentences that are not comprehendable enough to the evaluators. The relevance score of PHRASE FUSION is also slightly worse than that of TEMPLATE, presumably because PHRASE FUSION often generates poetic or creative expressions, as shown in Figure 5 , which can be considered a deviation from the image content. Error Analysis There are different sources of errors. Some errors are due to mistakes in the original visual recognition input. For example, in the 3rd image in Figure 5 , the color of sky is predicted to be \"golden\". In the 4th image, the wall behind the table is recognized as \"sky\", and in the 6th image, the parrots are recognized as \"person\". Other errors are from surface realization. For instance, in the 8th image, PHRASE FUSION selects the preposition \"under\", presumably because dogs are typically under the chair rather than on the chair according to Google n-gram statistics. In the 5th image, an unexpected word \"burning\" is selected to make the resulting output idiosyncratic. Word sense disambiguation sometimes causes a problem in surface realization as well. In the 3rd image, the word \"way\" is chosen to represent \"path\" or \"street\" by the image recognizer. However, a different sense of way -\"very\" -is being used in the final output. Related Work There has been relatively limited work on automatically generating natural language image descriptions. Most work related to our study is discussed in \u00a71, hence we highlight only those that are closest to our work here. Yao et al. (2010) present a comprehensive system that generates image descriptions using Head-driven phrase structure (HPSG) grammar, which requires carefully written domainspecific lexicalized grammar rules, and also demands a very specific and complex meaning representation scheme from the image processing. In contrast, our approach handles images in the opendomain more naturally using much simpler techniques. We use similar vision based inputs -object detectors, modifier classifiers, and prepositional functions -to some very recent work on generating simple descriptions for images (Kulkarni et al., 2011) , but focus on improving the sentence generation methodology and produce descriptions that are more true to human generated descriptions. Note that the BLEU scores reported in their work of Kulkarni et al. (2011) are not directly comparable to ours, as the scale of the scores differs depending on the number of sentences generated per image. Conclusion In this paper, we presented a novel surface realization technique based on web-scale n-gram data to automatically generate image description. Despite its simplicity, our method is highly effective in generating mostly appealing and presentable language, while permitting creative writing at times. We conclude from our study that it is viable to generate simple textual descriptions that are germane to the specific image content while also sometimes producing almost poetic natural language. Furthermore, we demonstrate that world knowledge implicitly encoded in natural language can help enhance image content recognition. Acknowledgments This work is supported in part by NSF Faculty Early Career Development (CAREER) Award #1054133.",
    "abstract": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -making for more human-like annotations than previous approaches.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "330",
    "year": "2011",
    "month": "June",
    "title": "Composing Simple Image Descriptions using Web-scale N-grams"
}