{
    "article": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest humanannotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of documentlevel RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https: //github.com/thunlp/DocRED. Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role in large-scale knowledge graph construction. Most existing RE Figure 1 : An example from DocRED. Each document in DocRED is annotated with named entity mentions, coreference information, intra-and inter-sentence relations, and supporting evidence. 2 out of the 19 relation instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014 Zeng et al., , 2015;; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a) . Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In order to identify the relational fact (Riddarhuset, country, Sweden), one has to first identify the fact that Riddarhuset is located in Stockholm from Sentence 4, then identify the facts Stockholm is the capital of Sweden and Sweden is a country from Sentence 1, and finally infer from these facts that the sovereign state of Riddarhuset is Sweden. The process requires reading and reasoning over multiple sentences in a document, which is intuitively beyond the reach of sentence-level RE methods. According to the statistics on our human-annotated corpus sampled from Wikipedia documents, at least 40.7% relational facts can only be extracted from multiple sentences, which is not negligible. Swampillai and Stevenson (2010) and Verga et al. (2018) have also reported similar observations. Therefore, it is necessary to move RE forward from sentence level to document level. The research on document-level RE requires a large-scale annotated dataset for both training and evaluation. Currently, there are only a few datasets for document-level RE. Quirk and Poon (2017) and Peng et al. (2017) build two distantly supervised datasets without human annotation, which may make the evaluation less reliable. BC5CDR (Li et al., 2016 ) is a humanannotated document-level RE dataset consisting of 1, 500 PubMed documents, which is in the specific domain of biomedicine considering only the \"chemical-induced disease\" relation, making it unsuitable for developing general-purpose methods for document-level RE. Levy et al. (2017) extract relational facts from documents by answering questions using reading comprehension methods, where the questions are converted from entityrelation pairs. As the dataset proposed in this work is tailored to the specific approach, it is also unsuitable for other potential approaches for document-level RE. In summary, existing datasets for document-level RE either only have a small number of manually-annotated relations and entities, or exhibit noisy annotations from distant supervision, or serve specific domains or approaches. In order to accelerate the research on document-level RE, we urgently need a largescale, manually-annotated, and general-purpose document-level RE dataset. In this paper, we present DocRED, a large-scale human-annotated document-level RE dataset constructed from Wikipedia and Wikidata (Erxleben et al., 2014; Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) . Do-cRED is constructed with the following three features: (1) DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents, making it the largest human-annotated document-level RE dataset. (2) As at least 40.7% of the relational facts in Do-cRED can only be extracted from multiple sentences, DocRED requires reading multiple sentences in a document to recognize entities and inferring their relations by synthesizing all information of the document. This distinguishes DocRED from those sentence-level RE datasets. (3) We also provide large-scale distantly supervised data to support weakly supervised RE research. To assess the challenges of DocRED, we implement recent state-of-the-art RE methods and conduct thorough experiments on DocRED under various settings. Experimental results show that the performance of existing methods declines significantly on DocRED, indicating the task documentlevel RE is more challenging than sentence-level RE and remains an open problem. Furthermore, detailed analysis on the results also reveals multiple promising directions worth pursuing. Data Collection Our ultimate goal is to construct a dataset for document-level RE from plain text, which requires necessary information including named entity mentions, entity coreferences, and relations of all entity pairs in the document. To facilitate more RE settings, we also provide supporting evidence information for relation instances. In the following sections, we first introduce the collection process of the human-annotated data, and then describe the process of creating the large-scale distantly supervised data. Human-Annotated Data Collection Our human-annotated data is collected in four stages: (1) Generating distantly supervised annotation for Wikipedia documents. (2) Annotating all named entity mentions in the documents and coreference information. (3) Linking named entity mentions to Wikidata items. (4) Labeling relations and corresponding supporting evidence. Following ACE annotation process (Doddington et al., 2004) , both Stage 2 and 4 require three iterative passes over the data: (1) Generating named entity using named entity recognition (NER) models, or relation recommendations us-ing distant supervision and RE models. (2) Manually correcting and supplementing recommendations. (3) Reviewing and further modifying the annotation results from the second pass for better accuracy and consistency. To ensure the annotators are well trained, a principled training procedure is adopted and the annotators are required to pass test tasks before annotating the dataset. And only carefully selected experienced annotators are qualified for the third pass annotation. To provide a strong alignment between text and KBs, our dataset is constructed from the complete English Wikipedia document collection and Wikidata 1 , which is a large-scale KB tightly integrated with Wikipedia. We use the introductory sections from Wikipedia documents as the corpus, as they are usually high-quality and contain most of the key information. Stage 1: Distantly Supervised Annotation Generation. To select documents for human annotation, we align Wikipedia documents with Wikidata under the distant supervision assumption (Mintz et al., 2009) . Specifically, we first perform named entity recognition using spaCy 2 . Then these named entity mentions are linked to Wikidata items, where named entity mentions with identical KB IDs are merged. Finally, relations between each merged named entity pair in the document are labeled by querying Wikidata. Documents containing fewer than 128 words are discarded. To encourage reasoning, we further discard documents containing fewer than 4 entities or fewer than 4 relation instances, resulting in 107, 050 documents with distantly supervised labels, where we randomly select 5, 053 documents and the most frequent 96 relations for human annotation. Stage 2: Named Entity and Coreference Annotation. Extracting relations from document requires first recognizing named entity mentions and identifying mentions referring to the same entities within the document. To provide high-quality named entity mentions and coreference information, we ask human annotators first to review, correct and supplement the named entity mention recommendations generated in Stage 1, and then merge those different mentions referring to the same entities, which provides extra coreference information. The resulting intermediate corpus con-tains a variety of named entity types including person, location, organization, time, number and names of miscellaneous entities that do not belong to the aforementioned types. Stage 3: Entity Linking. In this stage, we link each named entity mention to multiple Wikidata items to provide relation recommendations from distant supervision for the next stage. To be specific, each named entity mention is associated with a Wikidata item candidate set 3 consisting of all Wikidata items whose names or aliases literally match it. We further extend the candidate set using Wikidata items hyperlinked to the named entity mention by the document authors, and recommendations from an entity linking toolkit TagMe (Ferragina and Scaiella, 2010) . Specially, numbers and time are semantically matched. Stage 4: Relation and Supporting Evidence Collection. The annotation of relation and supporting evidence is based on the named entity mentions and coreference information in Stage 2, and faces two main challenges. The first challenge comes from the large number of potential entity pairs in the document. On the one hand, given the quadratic number of potential entity pairs with regard to entity number (19.5 entities on average) in a document, exhaustively labeling relations between each entity pair would lead to intensive workload. On the other hand, most entity pairs in a document do not contain relations. The second challenge lies in the large number of fine-grained relation types in our dataset. Thus it is not feasible for annotators to label relations from scratch. We address the problem by providing human annotators with recommendations from RE models, and distant supervision based on entity linking (Stage 3). On average, we recommend 19.9 relation instances per document from entity linking, and 7.8 from RE models for supplement. We ask the annotators to review the recommendations, remove the incorrect relation instances and supplement omitted ones. We also ask the annotators to further select all sentences that support the reserved relation instances as supporting evidence. Relations reserved must be reflected in the document, without relying on external world knowledge. Finally 57.2% relation instances from entity linking and 48.2% from RE models are reserved. Distantly Supervised Data Collection In addition to the human-annotated data, we also collect large-scale distantly supervised data to promote weakly supervised RE scenarios. We remove the 5, 053 human-annotated documents from the 106, 926 documents, and use the rest 101, 873 documents as the corpus of distantly supervised data. To ensure that the distantly supervised data and human-annotated data share the same entity distribution, named entity mentions are reidentified using Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) that is fine-tuned on the human-annotated data collected in Sec. 2.1 and achieves 90.5% F1 score. We link each named entity mention to one Wikidata item by a heuristic-based method, which jointly considers the frequency of a target Wikidata item and its relevance to the current document. Then we merge the named entity mentions with identical KB IDs. Finally, relations between each merged entity pair are labeled via distant supervision. Data Analysis In this section, we analyze various aspects of Do-cRED to provide a deeper understanding of the dataset and the task of document-level RE. Data Size. Table 1 shows statistics of DocRED and some representative RE datasets, including sentence-level RE datasets SemEval-2010 Task 8 (Hendrickx et al., 2010 ), ACE 2003 -2004 (Doddington et al., 2004) , TACRED (Zhang et al., 2017) , FewRel (Han et al., 2018b ) and documentlevel RE dataset BC5CDR (Li et al., 2016) . We find that DocRED is larger than existing datasets in many aspects, including the number of docu-  text. (4) A similar proportion of relation instances (16.6%) has to be identified based on commonsense reasoning, where readers need to combine relational facts from the document with commonsense to complete the relation identification. In summary, DocRED requires rich reasoning skills for synthesizing all information of the document. Inter-Sentence Relation Instances. We find that each relation instance is associated with 1.6 supporting sentences on average, where 46.4% relation instances are associated with more than one supporting sentence. Moreover, detailed analysis reveals that 40.7% relational facts can only be extracted from multiple sentences, indicating that DocRED is a good benchmark for document-level RE. We can also conclude that the abilities of reading, synthesizing and reasoning over multiple sentence are essential for document-level RE. Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in  The first challenge comes from the rich reasoning skills required for performing document-level RE. As shown in Sec. 3, about 61.1% relation instances depend on complex reasoning skills other than pattern recognition to be extracted, which requires RE systems to step beyond recognizing simple patterns in a single sentence, and reason over global and complex information in a document. The second challenge lies in the high computational cost of modeling long documents and the massive amount of potential entity pairs in a document, which is quadratic with regard to entity number (19.5 entities on average) in a document. As a result, RE systems that model context information with algorithms of quadratic or even higher computational complexity such as (Sorokin and Gurevych, 2017; Christopoulou et al., 2018) are not efficient enough for document-level RE. Thus the efficiency of context-aware RE systems needs to be further improved to be applicable in document-level RE. Weakly Supervised Setting. This setting is identical to the supervised setting except that the training set is replaced with the distantly supervised data (Sec. 2.2). In addition to the aforementioned two challenges, the inevitable wrong labeling problem accompanied with distantly supervised data is a major challenge for RE models under weakly supervised setting. Many efforts have been devoted to alleviating the wrong labeling problem in sentence-level RE (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Lin et al., 2016) . However, noise in document-level distantly supervised data is significantly more than its counterpart in sentencelevel. For example, for the recommended relation instances whose head and tail entities co-occur in the same sentence (i.e. intra-sentence relation instance) in Stage 4 of human-annotated data collection (Sec. 2.1), 41.4% are labeled as incorrect, while 61.8% inter-sentence relation instances are labeled as incorrect, indicating the wrong labeling problem is more challenging for weakly supervised document-level RE. Therefore, we believe offering distantly supervised data in DocRED will accelerate the development of distantly supervised methods for document-level RE. Moreover, it is also possible to jointly leverage distantly supervised data and human-annotated data to further improve the performance of RE systems. Experiments To assess the challenges of DocRED, we conduct comprehensive experiments to evaluate state-ofthe-art RE systems on the dataset. Specifically, we conduct experiments under both supervised and weakly supervised benchmark settings. We also assess human performance and analyze the performance for different supporting evidence types. In addition, we conduct ablation study to investigate the contribution of different features. Through detailed analysis, we discuss several future directions for document-level RE. Models. We adapt four state-of-the-art RE models to document-level RE scenario, including a CNN (Zeng et al., 2014) based model, an LSTM (Hochreiter and Schmidhuber, 1997) based model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {w i } n i=1 consisting of n words into a hidden state vector sequence {h i } n i=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014) , entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity mention m k ranging from the s-th word to the t-th word, we define its representation as m k = 1 t\u2212s+1 t j=s h j . And the representation of an entity e i with K mentions is computed as the average of the representations of these mentions: e i = 1 K k m k . We treat relation prediction as a multi-label classification problem. Specially, for each entity pair (e i , e j ), we first concatenate the entity representations with relative distance embeddings, and then use a bilinear function to compute the probability for each relation type: \u00eai = [e i ; E(d ij )], \u00eaj = [e j ; E(d ji )] (1) P (r|e i , e j ) = sigmoid(\u00ea T i W r \u00eaj + b r ) (2) where Feature Ablations. We conduct feature ablation studies on the BiLSTM model to investigate the contribution of different features in documentlevel RE, including entity types, coreference information, and the relative distance between entities (Eq. 1). Table 6 shows that the aforementioned features all have a contribution to the performance. Specifically, entity types contribute most due to their constraint on viable relation types. Coreference information and the relative distance between entities are also important for synthesizing information from multiple named entity mentions. This indicates that it is important for RE systems to leverage rich information at document level. Supporting Evidence Prediction. We propose a new task to predict the supporting evidence for relation instances. On the one hand, jointly predicting the evidence provides better explainability. On the other hand, identifying supporting evidence and reasoning relational facts from text are nat- urally dual tasks with potential mutual enhancement. We design two supporting evidence prediction methods: (1) Heuristic predictor. We implement a simple heuristic-based model that considers all sentences containing the head or tail entity as supporting evidence. (2) Neural predictor. We also design a neural supporting evidence predictor. Given an entity pair and a predicted relation, sentences are first transformed into input representations by the concatenation of word embeddings and position embeddings, and then fed into a BiL-STM encoder for contextual representations. Inspired by Yang et al. (2018) , we concatenate the output of the BiLSTM at the first and last positions with a trainable relation embedding to obtain a sentence's representation, which is used to predict whether the sentence is adopted as supporting evidence for the given relation instance. As Table 7 shows, the neural predictor significantly outperforms heuristic-based baseline in predicting supporting evidence, which indicates the potential of RE models in joint relation and supporting evidence prediction. (2018b) further combine external recommendations with human annotation to build large-scale high-quality datasets. However, these RE datasets limit relations to single sentences. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015 Mitamura et al., , 2017)) , fact extraction and verification (Thorne et al., 2018) , reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017) , sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010) , summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018) . Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017) . In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. Conclusion To promote RE systems from sentence level to document level, we present DocRED, a large-scale document-level RE dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly supervised data offered for facilitating the development of weakly supervised document-level RE. Experiments show that human performance is significantly higher than RE baseline models, which suggests ample opportunity for future improvement. A Appendices A.1 Experimental Details In this section, we provide more details of our experiments. To fairly compare the results of different models, we optimized all baselines using Adam, with learning rate of 0.001, \u03b2 1 = 0.9, \u03b2 1 = 0.999. The other experimental hyper-parameters used in our experiments are shown in Table 8 . Additionally, due to the document-level distance between entities, distances are first divided into several bins {1, 2, .., 2 k }, where each bin is associated with a trainable distance embedding. A.2 Types of Named Entities In this paper, we adapt the existing types of named entities used in Tjong Kim Sang and De Meulder (2003) to better serve DocRED. These types include \"Person (PER)\", \"Organization (ORG)\", \"Location (LOC)\", \"Time (TIME)\", \"Number (NUM)\", and \"other types (MISC)\". The types of named entities in DocRED and their covered contents are shown in Table 9 .   sibling the subject has the object as their sibling (brother, sister, etc.). Use \"relative\" (P1038) for siblingsin-law (brother-in-law, sister-in-law, etc.) and step-siblings (step-brothers, step-sisters, etc.)",
    "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest humanannotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of documentlevel RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https: //github.com/thunlp/DocRED.",
    "countries": [
        "China"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "171",
    "year": "2019",
    "month": "July",
    "title": "{D}oc{RED}: A Large-Scale Document-Level Relation Extraction Dataset"
}