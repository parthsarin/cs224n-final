{
    "article": "For some time now, we have been witnessing a gradual momentum which is remodelling the translation profession and transforming it from craft into an automated industrial process. In this context, the need for a coherent, reproducible model for translation tools evaluation emerges as a key element in the overall strategy for project management. This model should integrate the translation process into the evaluation procedure, considering at the same time stakeholders' needs and requirements. The evaluation framework I propose is based on adopting a user-oriented perspective. This essential principle translates into defining, first, the context of use of the particular tool, and, then, into checking whether the system conforms to these specifications. The present paper rests on the preceding assumptions and is structured along the following lines: o Scenario test outline. This is broken down into (1) definition of stakeholders' profile as a set of characteristics covering such aspects as text type, translation environment, leveraging needs, team description and terminology requirements; and (2) definition of system performance criteria, using ISO 9126 as the starting point. o Criteria weighting. This implies assigning weighted values to the characteristics previously defined so as to decide their relative importance in the evaluation of the system performance. o Metrics and Measurement. A three element rating is defined for allocating scores to the system under evaluation. Introduction Current trends in the language industry indicate that the new methods implemented in information management are pervading the translation world. In this respect, the field of information management is already directing its procedures towards managing interdependencies, since the complexity of producing documentation impels us \"to abandon the silo perspective of product development, marketing communication, technical writing, translation, and product support\" (Hofmann and Mehnert, 2000: 60) . A translation project's life-cycle must comply, then, with the demands of a wider production environment which considers multilingual information management in the form of information objects (IO). An IO is a collection of information identified as a unit, and defined by its communicative purpose, the specific user it is addressed to, the business entity it represents (a product line or a corporate function), the information it provides (in a specific format and for a target audience) and some publishing restrictions (61). One of the immediate consequences of this tendency is the high demand imposed on translation project management. This demand is higher than ever and introduces a challenge as new stakeholders enter the translation arena: clients, global teams of translators, cost, time, quality and resources, project's life-cycle, technology, and budget, among others. At the core of the whole process, and providing a means to face this challenge, we find Computer Assisted Translation (CAT) tools, a key resource with a constant presence in the translation workflow. From groundwork, term extraction and glossaries management, to text alignment and leverage of materials extracted from translation databases; from machine translation to post-editing; the operational environment of the translator is directly affected by technology. In this context, the need for a coherent, reproducible model for translation tools evaluation emerges as a key element in the overall strategy for project management. This model should integrate the translation process into the evaluation procedure, considering at the same time stakeholders' needs and requirements. This should be a user-oriented framework for evaluation which could account for different translation scenarios: the industry, public administrations, agencies and freelancers. Up to the present, many evaluations on translation tools have been conducted, specially in the case of translation databases (or translation memories) (see Benis, 1999; Assenat-Falcone, 2000) but, to my knowledge, these are not easily reproducible since they do not supply a set of parameters which could serve as a comprehensible model. In a sense, while these are certainly useful and valuable instances of evaluation, they concentrate on particular products used in a particular setting and under a specific environment which, in turn, restrains the system assessment to a predefined and specific set of needs. Acknowledging the benefits of such an evaluation, I believe that the current trends in the translation industry, as stated above, call for a broader model, one which can be easily reproduced and implemented according to different translation settings. This paper is an attempt at presenting such a model. The point of departure is EAGLES 1 document EAG-EWG-PR.2 on Evaluation of Natural Language Processing Systems (1996) which outlines a comprehensive framework for evaluation and, in turn, can be adapted to particular, case-specific tools. In fact, this document advances a set of features for evaluation of translator's aids, concentrating on the description of user profiles, types of translator's aids and the functionality of translation memories. The evaluation framework I propose is based on adopting a user-oriented perspective. This essential principle translates into defining, first, the context of use of the particular tool, and, then, into checking whether the system conforms to these specifications. This golden rule of evaluation implies the following: (a) designing an scenario test according to stakeholders' needs; (b) deciding how each feature of the scenario contributes to the final assessment of the system; and (c) executing the evaluation and assessing system performance. There is no such thing as a best system, but a best system for a particular situation. Translation Scenario and Stakeholders Any translation project, no matter how big or small, is defined, primarily, by two inseparable qualities: product and process. Translation is a product in the form of the final work the client is handed over, and it is defined at the same time as process as all the tasks that lead to this product. These two qualities conform the essential paradigm around which any translation scenario should be designed and directly affect the requirements and needs translation stakeholders may impose. The adequate identification of these forces, together with a correct estimation of their true contribution to satisfactory project completion, is the key to an objective evaluation of any translation tool. In fact, one of the main problems in evaluation is how to avoid subjective statements which, in any case, do not contribute to establishing relevant and efficient conclusions. While the core task in translation is apparently the same, methods vary, clients' expectations differ, teams of translators evolve and companies' profiles are different. Consequently, the evaluation of a particular tool in such a context and with so many forces interacting becomes a complex task which, in any case, should not lead to hasty results. Consequently, the set of features to be considered when designing the scenario test are distributed along four main categories: client, product, process and system. Client A translation assignment is only originated when someone commissions it, imposing certain requirements on the job to be done. This means that there is no translation job as an abstract entity since there is always a client, usually more interested in the final product rather than on the process leading to it. In some instances what our client wants is speed and may even be willing to accept some trade-off with quality. It may also be the case that the customer places a strong interest in the means of communication with the translator, or the use of in-house specific terminology. No matter what the client's needs are, it is certainly true that listening to them and acting accordingly is a key to successful project completion. In this sense, the client-translator relationship affects the definition of the translation scenario in the following aspects: Product In general terms, we consider that the product of translation is the text. As a result of the advances in information technology and telecommunications, the text, as was traditionally conceived, has now evolved towards a typology previously unknown: Web sites, hypertexts, software to be localised, on-line product documentation and interactive guides, among others. The new translation products pose new challenges, both on the technical and the linguistic side, because content is now interactive, dynamic and subject to constant change, it is increasingly integrated in the final product and it has to be tailored to specific audiences in an unprecedented rush for reaching global markets. While in translation theory there is no general agreement on a definition for the notion of text, it is still possible to identify three main characteristics: it is a coherent and cohesive unit, it is the product of a precise communicative action and the elements that conform the text are mutually relevant instead of a random collection of units (Cerezo, 2001) . This definition accounts for all products the translation industry is currently dealing with, calling for new and different aspects to be considered for evaluation: Process The traditional appreciation of a translator's work environment has been that of a person who works alone, fenced behind a stack of dictionaries, paper and all sorts of documentation material. Not that this has ever been completely true, but it is certainly a picture surviving in the minds of some aliens to the profession. As a service, translation has customarily been postponed until the last phases of production, when all other aspects of product development have been completed. Translation has inevitably been tied up to the end of the process, either as an update of a manual or the manual itself, as a letter to a client, a best-seller to be introduced to a new cultural community, a contract establishing legal bounds between two parties or the marketing material of an international corporation. Managing the translation process involves coordination, team work, planning and control techniques and these definitely affect modes of work and operational environments. On the one hand, there is no doubt the translation market is facing a dramatic increase as the figures demonstrate: the worldwide market for translation and localization is set to reach US$20 billion in 2004, Microsoft executes over 1,000 localization projects a year with a revenue in fiscal 1998 of US 5$ billion, there are practically 100,000 people translating professionally in Western Europe and 317,000 full-and part-time translators worldwide (Sprung, 2000: ix) . On the other, the working environment of the translator is evolving towards decentralized global teams where technology is at the core of the process. In any case, it is certainly true that project management has lately gained a name in the translation profession due, mainly, to market growth and virtual teams. When translation is subcontracted to teams communicating through the Internet, productivity becomes the focus and it is in the area of planning, tracking and measuring for volume and quality where project management offers a wide experience (Devaux, 2000: 12) . In this complex paradigm and with so many forces interacting in the translation process, project management emerges as the key element marrying crafts, needs and expertise. Translation teams share data, clients, targets, projects, people and resources, and it is the task of project managers to plan, instruct, monitor and control large amounts of data, quickly and accurately, while facilitating the problem-solving and decision-making process (Burke, 1999: 1) System ISO 9126 standard (\"Information Technology -Software product evaluation -Quality Characteristics and Guidelines for their Use) establishes six characteristics for software evaluation which provide an adequate framework for designing the translation scenario in terms of system characteristics. These six characteristics are summarised as follows 4 : a. Functionality: accuracy, suitability, interoperability, compliance and security b. Usability: understandability, learnability, operability c. Maintainability: analysability, changeability, stability, testability d. Reliability: maturity, fault tolerance, recoverability e. Efficiency: time behaviour, resource utilisation f. Portability; adaptability, installability, conformance, replaceability In fact, the Expert Advisory Group on Language Engineering Standards (EAGLES) on its Guidelines for Evaluation of Natural Processing Systems adopts ISO terms and guidelines as its starting point and adds a seventh aspect, customizability, defined as \"the ability to modify a product in order to satisfy a particular customer's needs\" (Manzi et al, 1996) . For the purposes of this paper I will adapt EAGLES framework so as to present and adaptable model for system performance. o Functionality: this is broken down into: \u2022 Accuracy: measure system performance in terms of precisionpercentage of valid segments from all those retrieved-and recall -percentage of segments retrieved from all those valid (for instance, when retrieving pairs of translated sentences from a TM). \u2022 Interoperability: check whether the system allows interaction with other translation aids. \u2022 Compliance with standards: check if the system supports different file formats. \u2022 Security: check whether the system covers your needs for translation validation and control of consistency over your team. If so, what are the mechanisms for translation validation? o Portability: if you need to reuse materials in other environments, does the system comply with the required standards? o Usability: how much effort is needed for recognizing the logical concept behind the system tasks and workflow? Usability also measures the effort needed for learning the application. What is the learning curve for effective use of the TM? What level of retraining does the TM impose on translation staff? o Efficiency: measure here time behavior in terms of retrieval time. o Maintainability: how does the system respond to fault tolerance and recoverability? o Back up and service: does the company selling the product offer a good service? o Pricing policy: can you afford to buy the system? o Investment in technical equipment: what level of investment is required before you can actually have the system running? o Customization: does the system allow easy customization? o Updates: is there any updating policy? Does it suit your needs? Figure 1 illustrates a sample checklist with all the features contributing to the design of the translation scenario. Criteria estimation Once the set of features that contribute to designing the translation scenario have been identified, the following step is to decide their relative value in the overall evaluation, i.e. assigning weighted values to each of the characteristics previously defined. This procedure rests on the assumption, as stated above, that different translation scenarios call for different translation needs and, therefore, different translation tools' requirements. Consider, for instance, how different teammanagement needs might be for freelance translators working on their own compared to those of an a translation department at an international corporation. The former would probably think that this feature is not applicable to their context of use, while the latter would assign a high relevance to this same characteristic. Figure 1. The translation scenario: Sample evaluation checklist The model assumes that the ideal CAT tool scores a maximum of 100 points, this meaning the optimum performance. In order to find out how the different features contribute to the final score, the evaluator should go over each of them an assign a value according to the user's own needs. This value will later be weighted so that all weighted values are added, they account for the expected optimum performance, i.e. 100 points 5 . In order to illustrate how different scenarios are outlined in the evaluation model, figure 2 shows the values assigned to features under two different translation scenarios 6 . Scenario 1 (S1) would correspond to the translation department of an international organization whose clients are, on the one hand, internal departments who produce confidential reports, economic reviews and summaries of conferences or papers. On the other hand, the department produces official documentation such as agreements, reports on technical issues or legally binding documents. Confidential and urgent translations are done in-house while lower priority documentation is outsourced. Scenario 2 (S2) depicts translation management at a telecommunications company shipping their products and their corresponding literature and software simultaneously worldwide. The documentation is integrated in the product as an interactive guide but some printed material is also included. Translation requirements consist of software development, translation, localization and testing. In this sense, translation is part of product development and is always carried out in-house. Comparing the values assigned to each of the features in the checklist, we see how differently those contribute to outlining the translation scenarios. For instance, the value assigned to exchange of materials is 0 for S1 and 100 for S2 to indicate that in S1 the possible exchange of translation materials is not an essential feature for a CAT tool evaluation, while in S2 this is a key characteristic. Conversely, the values given to outsourcing needs indicate, for S2, that all translation is done in-house, whereas for S1, the assigned value (30 points) means that some 30% of the translation work is done externally. Metrics and Measurement In their evaluation of two dictation systems, Canelli et al (2000) applied a quality model that rested on ISO/EAGLES methodology and developed a three element rating scale used to indicate whether the metric applied counted as good, acceptable or unacceptable. Each metric would then produce a score and \"the number of points awarded to the product is determined by its position on the three point scale, so that if the score is good, the total number of points is given, if acceptable, half of the points, if poor, none at all\". 5 To calculate the weighted values, multiply each value (V) by a constant (C) defined as C = W \uf053 V where C is the constant, W is the optimum performance (100) and V is the value assigned to each feature. 6 Both scenarios are simulated examples.  Similarly, the present model for CAT tools evaluation assumes that once the translation scenario has been defined as a set of characteristics and all their corresponding weights have been established, the actual analysis of the system will produce a score for each of the features, ranging from the assignment of the maximum weighted value of an attribute when the performance is optimum, through half this value for partially satisfactory performance, to zero for bad or unsatisfactory performance. In executing the evaluation, the evaluator should proceed by checking, in turn, all the features in the checklist and then allocating scores according to the following metrics: 1. Optimum performance: allocate maximum score as stated in the weighted value. 2. Partially satisfactory performance: give half the possible score. 3. Poor performance: allocate no points at all (0). When all scores have been assigned, the evaluator adds them up and, for the purposes of the evaluation report, will consider that the best system scored closer to 100 points. Discussion Following King's (1999) recipe for designing an evaluation framework for machine translation we find that \"the ultimate question is whether the object being evaluated fits what the customer of the evaluation wants or needs\". The present paper outlines the means to answer this question and, in order to show the practicalities of the model proposed, I will now check it against the seven steps for evaluation as stated in King. Step 1. Why is the evaluation being done? This is a stage prior to the actual evaluation but still an essential one. The key is to find the purpose of the evaluation so that from the very beginning the user and the evaluator work on common grounds. At this point, we should consider why we are to evaluate a particular tool: is it for the sake of speed in translation?, for an increase in quality?, because our client is supplying us with certain materials in a certain format?, or because we want to set up our own workstation? Step 2. Elaborate a task model Our task model is defined by the translation scenario where the tool under evaluation will be used. Step 3. Define top level quality characteristics These are defined as the set of translation characteristics corresponding to the four translation stakeholders, i.e. client, product, process and system. Since not all attributes are of equal importance each of the characteristics is assigned a weighted value, as shown above. Step 4. Produce detailed requirements In King's words, \"a valid and reliable way of measuring how a system fares with respect to the attributes must be found\". Nevertheless, finding attributes which are measurable is sometimes impossible as many of the characteristics in the translation scenario are not numerically quantifiable. This is where the evaluation framework introduces some unavoidable subjectivity which I try to compensate by defining a three element rating scale to account for excellent performance, partially satisfactory performance and poor performance. Step 5. Define metrics Metrics and measurement in the model are conceived with the challenging task of converting into a numerical score some observed characteristics which are, otherwise, qualitative. As is the case in many evaluation methodologies (especially in evaluation of machine translation systems), finding agreement across a representative community of users on what counts as good or poor performance is normally difficult (King, 1996) . In this case, metrics are based on \"internal validity\" rather than on \"external validity\", which means that \"their interpretation is constrained only by the individual's notion of what the metric means\" (193) . At best, agreement can be negotiated among users under the same translation scenario. Steps 6 and 7. Design the execution of evaluation and Execute evaluation So far, the model presented here has been tested in simulated scenarios and a through exploration of its practicality under a real translation setting is the subject for further work. Conclusion The evaluation model presented in this paper is inspired by a user-oriented perspective which could account for different translation scenarios such as the industry, public administrations, agencies and freelancers, and their different translation needs. The model acknowledges these differences and brings up a series of characteristics that correspond to the four main actors in translation, i.e. the client, the product, the process and the CAT tool. Under this framework, the translation characteristics that conform the scenario are easily extensible to allow for other aspects not originally considered because the process of assigning weights and estimating values remains unchanged. Author Celia Rico, Ph.D., is the head of the Department of Translation Technologies at the Universidad Europea CEES (Madrid, Spain), where she trains future translators on the fundamentals of CAT tools as well as on the principles of translation strategies. Dr. Rico's publications have concentrated on areas such as translation memory evaluation, machine translation and translator competence, and the impact of new technologies on the translation profession. Dr. Rico can be reached at celia.rico@ti.fil.uem.es",
    "abstract": "For some time now, we have been witnessing a gradual momentum which is remodelling the translation profession and transforming it from craft into an automated industrial process. In this context, the need for a coherent, reproducible model for translation tools evaluation emerges as a key element in the overall strategy for project management. This model should integrate the translation process into the evaluation procedure, considering at the same time stakeholders' needs and requirements. The evaluation framework I propose is based on adopting a user-oriented perspective. This essential principle translates into defining, first, the context of use of the particular tool, and, then, into checking whether the system conforms to these specifications. The present paper rests on the preceding assumptions and is structured along the following lines: o Scenario test outline. This is broken down into (1) definition of stakeholders' profile as a set of characteristics covering such aspects as text type, translation environment, leveraging needs, team description and terminology requirements; and (2) definition of system performance criteria, using ISO 9126 as the starting point. o Criteria weighting. This implies assigning weighted values to the characteristics previously defined so as to decide their relative importance in the evaluation of the system performance. o Metrics and Measurement. A three element rating is defined for allocating scores to the system under evaluation.",
    "countries": [
        "Spain"
    ],
    "languages": [],
    "numcitedby": "11",
    "year": "2001",
    "month": "November 29-30",
    "title": "Reproducible Models for {CAT} Tools Evaluation: A User-Oriented Perspective"
}