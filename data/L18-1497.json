{
    "article": "Discriminating between similar languages (DSL) on conversational texts is a challenging task. This paper aims at discriminating between limited-resource languages on short conversational texts, like Uyghur and Kazakh. Considering that Uyghur and Kazakh data are severely imbalanced, we leverage an effective compensation strategy to build a balanced Uyghur and Kazakh corpus. Then we construct a maximum entropy classifier based on morphological features to discriminate between the two languages and investigate the contribution of each feature. Empirical results suggest that our system achieves an accuracy of 95.7% on our Uyghur and Kazakh dataset, which is higher than that of the CNN classifier. We also apply our system to the out-of-domain subtasks of VarDial'2016 DSL shared tasks to test the system's performance on short conversational texts of other similar languages. Though with much less preprocessing, our system outperforms the champions on both test sets B1 and B2. Introduction Automatic language identification (LID) aims to identify the language a document is written in, which is an important branch in Natural Language Processing (NLP) (Zampieri et al., 2015a) . The past two decades had witnessed fast development in LID and state-of-the-art systems have achieved high accuracy (Sim\u00f5es et al., 2014) and wide coverage (Brown, 2014) on standard texts. However, identifying languages from very little data, from multilanguages input or discriminating between extremely similar languages are bottlenecks of this field (Ljube\u0161ic and Kranjcic, 2014; Zampieri et al., 2015b) . What's more, identifying similar languages with limited resource is unsolved. Uyghur and Kazakh, widely used in Middle East and North West of China, are similar languages. They both belong to the Turkic group of Altaic family and are agglutinative languages. According to Wang et al. (2013) , the similarity between Uyghur and Kazakh at sentence and word level are over 80% and 90% respectively. They have many characteristics in common: (1) They are both written in Arabic alphabets in the right-to-left order. (2) Theoretically there are 32 letters in Uyghur and 33 in Kazakh. The two languages share 26 letters and encoding areas with another 2 letters look exactly the same. (3) There is a large overlap of vocabulary and syntax between the two languages. It is very difficult to identify them by looking up the words in dictionaries. (4) In both languages, a great amount of prefixes and suffixes are attached to a word, which makes word stemming and recognition difficult. Here we define \"short conversational texts\" as short texts people used to communicate with each other through mobile devices, communicational software and social-media platforms. They can be (1) short messages people send to each other through cell phones; (2) chatting records of communicational software such as Wechat and MSN; (3) post-s and comments on social-media platforms such as Twitter, Facebook and Microblog. These texts are obstacles for NLP tasks for the following reasons: (1) Each text is pretty short. Lengths of most sentences range from 3 to 9 words. (2) There exist enormous spelling and grammatical mistakes in the texts, which make it time and energy consuming in word stemming and error correction. (3) Abbreviations and colloquial expressions are widely used. (4) It takes much time and energy to collect short conversational texts, resulting in the imbalance and inadequacy of the corpus. (5) Without unified input methods, people use various characters other than standard ones. In fact, more than 100 letters of different encoded bytes are found in our corpus. This strengthened the difficulty of discriminating between Uyghur and Kazakh short conversational texts. Related Work Since more and more researchers are concerned with discriminating between similar languages (DSL), a series of shared tasks were organized by the workshop series for Similar Languages, Varieties and Dialects (VarDial), which was collocated with either COLING, RANLP or EACL. According to Malmasi et al. (2016) , high-order character n-grams were the most successful feature, and the best classification models included SVM, logistic regression, and language models, while deep learning approaches did not perform very well. To deal with short and sparse texts, solutions (Phan et al., 2008; Rehurek and Kolkus, 2009; Tromp and Pechenizkiy, 2011; Dai et al., 2013) were proposed to enrich short text representation by bringing in additional semantics. The additional semantics could be from data collection itself or be derived from a much larger external knowledge base. Dealing with tweets, Zubiaga et al. (2014) summarized the TweetLID shared task and workshop held at SEPLN 2014 and pointed out several shortcomings in current researches. When it comes to discriminating between Uyghur and Kazakh, Hasimu et al. (2015) employed unique characters to identify the Uyghur , Kazakh and Kyrgyz languages. They carried out experiments on the written texts longer than 70 words and achieved a 96.67% accuracy. But in the web corpus of less than 10 words, the precision of Kazakh fell dramatically to only 65.31%. In this paper, we made two contributions: (1) We constructed a corpus of conversational texts in Uyghur and Kazakh for similar languages identification and proposed a method for corpus augmentation. (2) We designed a system that can effectively discriminate between similar languages on conversational texts. Data Construction Data Collection With the popularization of social network and chatting applications on mobile phones, people are more likely to communicate with each other via short instant messages. Thus natural languge processing on short conversational messages is of great significance. We collected 48680 texts from the chatting messages sent by mobile phones and used as our training set after anonymization. Likewise, 973 colloquial messages that were sent in a day were collected as our test set. Then all the texts were tagged by linguistic experts. In our training set, we found that 48432 samples were written in Uyghur while 148 samples were Kazakh. As for test set, 687 and 286 texts were annotated as Uyghur and Kazakh separately. The scales of Uyghur and Kazakh texts in the training set were severely imbalanced, which exceeded the proportion of 327:1. Data Augmentation Since a highly imbalanced training corpus may hinder the effectiveness of discrimination between the two similar languages, we decided to balance the corpus by supplementing Kazakh texts. We did not collect more Kazakh short messages in the same way because of inefficiency since the linguistic experts have to skim more than 300 Uyghur samples to get a Kazakh sample. To obtain data that are similar to short messages which are conversational, informal and short, we decided to crawl data from Kazakh forums instead of the Kazakh news web pages and Twitter. The reasons are as follows: (1) News are formal written texts, which have little overlap of words and characters with short communicational messages. (2) Although tweets are short and informal, Twitter is rarely used in China. (3) Posts on Kazakh forums are informal and conversational, which resemble the nature of short messages. What's more, the contents are almost entirely written in Kazakh. We crawled 70909 web pages from a Kazakh forum 1 . However, some texts in these web pages were longer than the chatting messages. To make the crawled texts more similar to the short messages, we picked out 339,609 samples of no more than 14 words. Then we randomly chose 48000 texts from the filtered samples to match the number of Uyghur Our System Feature Extraction Since all texts in the corpus are extremely short, we assume the lexical n-gram features cannot play an important role in DSL in short conversational texts. Based on linguistic, in particular morphological analysis of the two languages, we mainly used the following features to discriminate between the two languages: \u2022 Unique characters. Once a unique character is found in a text, we can determine that the text is written in the language the unique character belongs to. \u2022 Character n-grams. The sequence and combination of characters is different among various languages, even though the languages share a lot of characters. The Uyghur Latin word \"men\" corresponds to \"man\" in Kazakh Latin for the same meaning. (For the convenience of typing and visualization, here we use Latin charcters to embody the Uyghur and Kazakh instead of the Arabic letters.) \u2022 Prefixes and suffixes. As agglutinative languages, both Uyghur and Kazakh have numerous affixes. On many occasions, affixes of the two languages are different. For example, to express the same meaning, suffix \"lar\" is used in Uyghur, while \"dar\" is used in Kazakh. Likewise, \"o\" can be the first letter in Kazakh but cannot be found at this position in Uyghur. One thing we should note is that misspelling problems make this feature hard to extract, thus we use the the first and last n characters of the words as a substitute of prefixes and suffixes. Here n ranges from 1 to 3. \u2022 Word unigrams. The frequency of a word represents how likely it belongs to a language. If a text contains a high-frequency word of a language, it is more likely to belong to the corresponding language. \u2022 Bin on text length. We can divide the texts into different bins according to the lengths of the texts. Models trained in certain bin length will be more accurate. Classifiers Nowadays there are many state-of-the-art classifiers that achieve steady and desirable performance, no matter whether they are based on machine learning or neural networks. The maximum entropy (MaxEnt) classifier is one of the best models among the machine learning algorithms. The MaxEnt classifier computes the conditional likelihood and relativity of the features mutually for each category in the training step. Based on the statistics, for each sample, the classifier adjusts the weights of corresponding features to maximize the max entropy of the sentence under the constraints of all the conditional likelihood above. When predicting, scores of samples of each category is computed and the class of the highest score is chosen as its label. Therefore, feature dependence is taken into account in MaxEnt. In this paper, we applied a MaxEnt classifier in our system using the Stanford classifier toolkit 2 . With the convolutional neural networks (CNN) successfully applied to image recognition (Krizhevsky et al., 2012) and text classification (Kim, 2014) , CNN became one of the most popular deep learning classifier algorithms. We also built a CNN classifier based on character embeddings considering the features are mainly of the character level and then compared the performances of the two classifiers. Evaluation Since we take the DSL task as an issue of classification, we use the evaluation metrics of classification systems. Precision (P), recall (R) and accuracy (Acc) are used to evaluate the performance of our system. Experiments and Discussion In this section, we conducted four experiments to examine the effect of the supplemented Kazakh samples, the con- Experiment on the Supplemented Data In this experiment, we use the MaxEnt classifier based on all the features except bin. Table 3 Experiment on the Features' Contribution To investigate the contribution of each feature, we evaluate the performance of our system using all the features, and without each one of them each time separately, e.g. using all the features without unique characters or character ngrams. In this way, we can see how importance each feature is by observing the decrease of performance, compared to that of using all the features. As we can see, with each feature removed, the performance of our system decreases to different extent except for the word unigrams. It proves that all the features except word unigrams are useful in this task. We also observe the sharpest decline in accuracy when the character n-grams are removed, which implies it contributes the most among all the features. On the contrast, the accuracy increases slightly without the word unigrams, which reveals that word level features are helpless and even undermine the performance of language identification on the data set. Therefore, we stop using this feature in the following experiments. Experiment on the Classifiers In this experiment we respectively use the CNN classifier and MaxEnt classifier trained on the final Uyghur and Kazakh corpus to compare their performance. For the Max-Ent classifier, all the features except word unigrams are used. For the CNN classifier, the samples are represented at the character level with each character mapped into an embedding of 50 dimensions. Convolutional kernel widths are set to [1, 2, 3, 4] to resemble the character n-grams of size 1 to 4 used in MaxEnt classifier. Numbers of kernels are set to be [50, 200, 300, 500] separately since there is no improvement when using more kernels. A dropout layer with a 0.5 dropout rate is applied. The character embeddings are randomly initialized between (-0.05,0.05) under the uniform distribution. The performances of the two classifiers using the best parameters are listed in Table 5 . Classifiers Experiment on the VarDial'2016 DSL shared task Since the MaxEnt classifier using the morphological features achieved a high accuracy in discriminating between Ughur and Kazakh, we intended to test the performance of our system in discriminating other similar languages on short conversational texts. Data Description We chose the two social media data (B1 and B2) of Subtask 1 in VarDial'2016 DSL shared task 3 as the test materials. The training set consists of 18000 instances of journalistic data per language for training and 2000 instances for development. Each of the test sets includes 100 Twitter users' tweets per language. A varying number of tweets from a user are concatenated as a test sample (98.88 and 50.47 tweets per user for B1 and B2 in average separately). The two test sets cover two groups of closely-related languages : South-Slavic (Bosnian, Croatian, Serbian) and Portuguese (Brazilian and European). For each sample in the test set which contains five language/variants in a messed order, we have to find out which language it belongs to. Evaluation In the DSL shared task, average accuracy (Acc) and macroaveraged F1-score (F1) were used as the official scores. Therefore we use the same metrics in this experiment. Since the DSL datesets of the subtask are balanced with the same number of examples for each language variety, we mainly use the average accuracy for comparison in the following subsection. Results and Discussion We applied the MaxEnt classifier and with the character ngram feature (n ranges from 1 to 7) to compare with other participant systems. Results of our system as well as the top participant systems in B1 and B2 in the VarDial'2016 DSL shared tasks are listed in Table 7 and 8 As is shown in Table 7 and 8 , GW LT3 ranked first in the subtask of discriminating between similar languages on the tweets dataset. It used character n-gram (n=2-6) and word n-gram (n=1-3) with term-frequency weighting, and took many preprocessing measures. Our system outperforms it by 1.0% in B1 and 1.2% in B2 in accuracy. It is implied that, besides Uyghur and Kazakh, our system is also highly efficient in DSL tasks in other similar languages on short conversational texts. Compared with tubasfs, which also used character n-grams as a feature (n=1-7), the accuracies of our system in B1 and B2 are both 6.8% higher. This indicates that MaxEnt is better than SVM in this task. In addition, while our system achieved the accuracy of 95.7% on the Uyghur and Kazakh dataset, we just set n to be 1 to 4 in the character n-gram feature. When dealing with B1 and B2 test sets, we set n to be 1 to 7, and the accuracies we got were 93.0% and 89.0% respectively, which are lower than that we got in dealing with Uyghur and Kazakh. The reason for the unsatisfying result is that the training set of DSL 2016 subtask1 are journalistic news, which are different from short conversational texts to some extent. That can also show that when discriminating between similar languages on short conversational texts, contents in related forums is a better resource than news as the training data. Conclusion In this paper, we have constructed a corpus of short conversational Uyghur and Kazakh texts used for DSL. To solve the severe imbalance problem of the two languages with limited resource, we proposed a data augmentation method. That was to crawl a Kazakh forum and choose the materials which were short, informal as supplemental data. It is suggested that our augmentation strategy is effective and texts from forums are more suitable than news texts for the DSL task. Then we designed a MaxEnt classifier with morphological features to discriminate between Uyghur and Kazakh conversational texts. Our empirical study shows that the character level features we exploited are helpful while employing the word unigrams led to worse performance. Experimental results also indicate that our system can not only discriminate between Uyghur and Kazakh on short conversational texts at a high accuracy of 95.7%, but also outperforms the state-of-the-art systems in DSL on Tweets with out-of-domain training data in the VarDial'2016 DSL task. It is also implied that CNN is not a competitive model for this task and the MaxEnt performs better than the SVM classifier using the same features. Acknowledgements This work is partially supported by the National Natural Science Foundation of China (Nos. 11461141004,  61271426, U1536117, 11504406, 11590770-4, 11590771)",
    "abstract": "Discriminating between similar languages (DSL) on conversational texts is a challenging task. This paper aims at discriminating between limited-resource languages on short conversational texts, like Uyghur and Kazakh. Considering that Uyghur and Kazakh data are severely imbalanced, we leverage an effective compensation strategy to build a balanced Uyghur and Kazakh corpus. Then we construct a maximum entropy classifier based on morphological features to discriminate between the two languages and investigate the contribution of each feature. Empirical results suggest that our system achieves an accuracy of 95.7% on our Uyghur and Kazakh dataset, which is higher than that of the CNN classifier. We also apply our system to the out-of-domain subtasks of VarDial'2016 DSL shared tasks to test the system's performance on short conversational texts of other similar languages. Though with much less preprocessing, our system outperforms the champions on both test sets B1 and B2.",
    "countries": [
        "China"
    ],
    "languages": [
        "Uyghur",
        "Kazakh"
    ],
    "numcitedby": "1",
    "year": "2018",
    "month": "May",
    "title": "Discriminating between Similar Languages on Imbalanced Conversational Texts"
}