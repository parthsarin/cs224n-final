{
    "article": "This paper describes the participation of team DUCS at SemEval 2022 Task 6: iSarcasmEval -Intended Sarcasm Detection in English and Arabic. Team DUCS participated in SubTask A of iSarcasmEval which was to determine if the given English text was sarcastic or not. In this work, emojis were utilized to capture how they contributed to the sarcastic nature of a text. It is observed that emojis can augment or reverse the polarity of a given statement. Thus sentiment polarities and intensities of emojis, as well as those of text, were computed to determine sarcasm. Use of capitalization, word repetition, and use of punctuation marks like '!' were factored in as sentiment intensifiers. An NLP augmenter was used to tackle the imbalanced nature of the sarcasm dataset. Several architectures comprising of various ML and DL classifiers, and transformer models like BERT and Multimodal BERT were experimented with. It was observed that Multimodal BERT outperformed other architectures tested and achieved an F1-score of 30.71%. The key takeaway of this study was that sarcastic texts are usually positive sentences. In general emojis with positive polarity are used more than those with negative polarities in sarcastic texts. Introduction According to the Collins Dictionary 1 \"Sarcasm is speech or writing which actually means the opposite of what it seems to say\". Fox Tree et al. (2020) report that sarcasm is challenging to identify, even for humans. People often use gestures like rolling of eyes or heavy tonal stress to express sarcasm (Pandey et al., 2019) , in speech or in-person communication. Intonation and stress in speech, too, are strong indicators of sarcasm (Castro et al., 2019) . Also, the context in the form of shared knowledge between the speaker and the audience can be leveraged to detect sarcasm (Amir et al., 2016) . But most of the time the tone or the context is missing in text data, especially tweet data. Some users attempt to use capitalization like \"OH YEAH\" or repetitions like \"woowww\" to indicate tonal intensity. But, most of the time these cues are not enough, especially in absence of context to gauge sarcasm. This makes sarcasm detection a challenging task. In this work, team DUCS participated in SemEval 2022 Task 6 : iSarcasmEval Subtask A, (Abu Farha et al., 2022) , to determine if the given text is sarcastic or not. The English text training dataset contains 3468 sentences. Each text is labeled as sarcastic or non-sarcastic by the text-author. Of the 3468 sentences 867 are sarcastic and 2601 are non-sarcastic. In the dataset corresponding to a sarcastic text, a rephrased non-sarcastic version has also been provided. These sarcastic texts are labeled as one of sarcasm, irony, satire, understatement, overstatement, and rhetorical question. These ironic speech categories are determined by a linguist expert. The target task is to determine whether a text is sarcastic or not on the Test dataset which contains 1400 sentences. For sarcasm detection in the above-discussed dataset, an attempt was made to derive cues from the text itself. It is observed that in real scenarios, the absence of labels by users like #not or #sarcasm or any other contextual information makes sarcasm detection challenging (Chaudhari and Chandankhede, 2017) . Thus, in this work, the authors attempted to use the information available in the text itself. Emoji occurrences in the text are captured to study their impact in discerning sarcasm. Sentiment intensity and polarity of both the emojis and the text were computed to train the classifiers for sarcasm classification. This paper is organized as follows. Section 2 discusses related work in sarcasm detection. In Section 3, System Overview, the proposed approach to incorporate polarity and intensity of emojis along with text sentiment polarity and intensity is discussed. Section 4, describes the Experimental Setup and the data augmentation approach used for the given dataset. The results of the experiments conducted are reported in Section 5. In the last section Conclusion, the key takeaways and learnings are discussed. Related work The sarcastic nature of the text can deceive classifiers as well as humans, given it conveys the opposite of what it means. Many researchers have attempted to use text sentiment and emojis for sarcasm detection. Subramanian et al. (2019) use word and emoji embeddings simultaneously to train deep learning models with attention layer for sarcasm detection. Pamungkas and Patti (2018) utilize structural features and affective features of tweets for irony detection. Structural features include the presence of hashtags, links, emojis, quotes, etc. The counts of mentions, exclamations, upper-case, intensifiers, links, along with the counts of verbs, nouns, and adjectives are also used along with the use various affective resources to capture sentiment polarity and emotions. Lemmens et al. (2020) used an ensemble approach with LSTM for representing emojis and hashtags, CNN-LSTM for representation of cases, stop-words, punctuation, and sentiments. MLP was trained with Facebook's InferSent embeddings (Conneau et al., 2017) while SVM was trained with emotion and stylometric features. A decision tree with Adaboost served as the base estimator for the ensemble. This ensemble was used to predict sarcasm on the conversational text where context was available. Sundararajan and Palanisamy (2020) propose a rule-based classifier with an ensemble of 20 different features. They observed that the features based on sentiment can predict sarcasm better in combination with contradictory features. In most work so far researchers have tried to capture sentiment and linguistic features to identify sarcasm. A few researchers have attempted to include emojis in sarcasm classification. But, how the usage of emojis with text impacts or conveys sarcasm is yet to be explored in depth. It needs to be studied how frequent is sentiment incongruence of emojis in the text they are used with and can the polarity of emojis and text help deduce sarcasm. Hence in this work, the authors focus on these features -emoji sentiment and text sentiment as well as their polarities to understand how this combination impacts sarcasm. This approach is discussed in the next section. System overview In the given dataset, situational context is not available. Although linguistic markers are provided, they may not be available in real scenarios, eg when the users post on social media. In the absence of context, it is important to take into account other factors that may point to sarcasm. For instance, the surface sentiment of a text is often used in many sarcasm classifiers (Joshi et al., 2016) . Sulis et al. (2016) , report that most often sarcasm is used with an apparently positive statement to produce a negative impact. It is noted that the users may use intensity in the form of capitalization, repeated letters, etc. to express sarcasm (Chaudhari and Chandankhede, 2017) . In this work, to identify sarcasm, the sentiment polarity of the text and intensity in the form of sentiment score was incorporated for the sarcasm classification task. Intensity as a feature is used as it may help capture intonation or stress a user may want to express. It has been reported that features like emojis can augment as well as alter the sentiment polarity (Grover, 2021) . Pamungkas and Patti (2018) observed that emoji incongruity with the text may help improve the irony classification. Thus, this work investigates whether in sarcastic texts users use in-congruent emojis with the text to express sarcasm. And whether the use of emojis in the text helps in detecting sarcasm? Data preparation The text dataset was cleaned to remove URLs as they do not contribute to the text sentiment. Most text preprocessing approaches in sentiment classification convert characters to lower case, reduce repeating letters, and perform lemmatization. To compute the sentiment score and polarities, special care was taken to not remove repetition of letters, capitalization or punctuation as they may serve as intensifiers. These special usages can later be removed during the experimentation while training the classifiers. Emoji extraction In the next step, the emojis are separated from text. The training dataset for this task contained 672 text sentences with emojis of which 195 were sarcastic. The demoji 1.1.0 Python package 2 , is used for emoji extraction. Text sentiment polarity and intensity To evaluate intensity or the sentiment score of the text and text polarity the VADER sentiment analyzer (Hutto and Gilbert, 2014) , available in Python's natural language toolkit is employed. VADER is observed to work better in the language used by the users on social media (Illia et al., 2021) , (Bonta and Janardhan, 2019) . Users tend to communicate informally, using slang, abbreviations, capitalization and punctuation marks, etc. on social media. VADER can capture these components while computing sentiment polarity and sentiment scores. The compound scores provided by VADER are used as the sentiment score. If the compound score is \u2264 -0.5 the sentiment polarity is considered negative, between (-0.5, 0.5), the polarity is considered as neutral, while the score \u2265 = 0.5 is considered to be positive. Emoji sentiment polarity As discussed above, since emojis are known to contribute to text sentiment and in many cases, they may even reverse the sentiment polarity, the emojis extracted for each text are stored separately in the list. A total of 672 texts have emojis in the provided dataset. For extracting emoji sentiment, Emoji Sentiment Ranking (Novak et al., 2015) is employed. Emoji Sentiment Ranking gives the sentiment score of the emojis. A text may contain numerous emojis which is an indicator of both the context as well as associated emotion with the text. For example, a single emoji used several times with a text may be considered as a sentiment intensifier. Thus, all the emojis in the text were considered while computing emoji polarity and emoji sentiment score. Emoji sentiment scores were captured for all emojis. If an emoji did not appear in the Emoji Sentiment Ranking, it was demojized using the emoji 1.6.3 Python package 3 . Demojizing here means replacing the emojis with their Unicode Consortium description in English, (Unicode, 2022). These emoji descriptions are passed to the VADER sentiment analyzer to compute the sentiment score of the emojis. Again the compound score provided by VADER is used to compute the final score. Scores of all the emojis are then added to compute the total emoji score for all the emoji scores associated with the text. These emojis are then normalized as done in VADER and corresponding emoji polarities are computed. normalizedScore = score \u221a score 2 + \u03b1 (1) where default value of \u03b1 is 15. Each text with emojis is labeled with the computed emoji score and emoji polarity. For the text with no emojis, the emoji sentiment score is assigned to zero, and polarity is assigned as neutral. For example if the text is \"@AsdaServiceTeam imagine your delivery being 2 hours late, and imagine calling up your service team only for them to hang up at 10pm, coincidentally the same time the office closes. But it's okay, my \u00a33 delivery fee is being refunded though \". The VADER text score is computed as with \"+\" text polarity. The VADER emoji sentiment score is computed as 0.0150371002692231 with \"+\" emoji polarity. The prepared dataset with the derived features of text sentiment scores, text polarity, emoji sentiment score, and emoji polarity is now used to train the different classifiers discussed in the next section. Experimental setup For sarcasm detection, this work employs various machine and deep learning classifiers. The proposed set up is also tested with transformer models like BERT and Multimodal BERT. First, the text is cleaned to remove stop words, newlines, and spaces. The cleaned text is then converted to lower case and then tokenized to prepare for loading the word embeddings. Pre-trained GloVe embeddings, (Pennington et al., 2014) of 100 dimensions are used to represent the tokenized text. 80% of the dataset was used for training while the remaining 20% was used for validation. Data augmentation This dataset was highly imbalanced with 867 sarcastic and 2601 nonsarcastic texts, i.e. (3:1 ratio for non-sarcastic: sarcastic texts). So the dataset was augmented to improve this ratio of sarcastic and non-sarcastic texts. The sarcastic texts were oversampled with nlpaug python library 4 for text augmentation. The contextual BERT embeddings were used to synthesize new texts from existing texts in the dataset. Two different augmented datasets were created. In the first augmented dataset (Aug1 Training Set), each sarcastic text was synthesized once to create one more text. In this set, the ratio of non-sarcastic: sarcastic texts was 2:1. In the next augmented dataset (Aug2 Training Set), each sarcastic text was synthesized to create two more texts, thus resulting in the ratio of 1:1 for non-sarcastic to sarcastic texts. For example, if the tweet was, \"See Brexit is going well\". Then in the first augmented text (Aug1 Training Set) the original tweet (mentioned above) was retained and a synthesized tweet in the form \"can see brexit negotiations is going extremely well\" was added to the dataset. This was done for every sarcastic tweet, making the ratio of non-sarcastic:sarcastic texts as 2:1. In the Aug2 Training Set) the number of synthesized texts was increased to two. So two synthesized tweets apart from the original tweet were appended to the dataset making the ratio of non-sarcastic:sarcastic as 1:1. The computed sentiment polarity, text sentiment score, emoji sentiment polarity, and emoji sentiment score were replicated from the original texts in the synthesized texts. Model architecture The different architectures experimented with are as follows: \u2022 Gaussian Naive Bayes (Perez et al., 2006) \u2022 Support Vector Machine with Linear Kernel (Wang, 2005) \u2022 Logistic Regression (Wright, 1995) \u2022 Sequential model with 1 dense fully connected layer (Sutskever et al., 2014) \u2022 LSTM with Adadelta optimiser, 128 batch size, run over 10 epochs (Sundermeyer et al., 2012) \u2022 Bi-LSTM (Graves and Schmidhuber, 2005) with Adadelta optimiser, 128 batch size, run over 10 epochs \u2022 BERT (Devlin et al., 2018) with the learning rate of 1e-5, batch size 32 run over 4 epochs with the base uncased classifier \u2022 Multimodal BERT with the batch size 32 run over 6 epochs with the base uncased classifier Results This section reports the results of various classifiers on which the proposed approach was tested with. The results of models trained with text and emoji sentiment scores and their respective polarities on Original, Aug1, and Aug2 are reported in 4, 5, 6 respectively. The Multimodal BERT trained with 1736 sarcastic and 2601 non-sarcastic sentences was eventually used to determine sarcastic text in the test data. With this work the authors achieved an overall F1-Score of 30.71% on the test dataset and was ranked 24 out of the 43 participating teams.  It was observed that in general, models trained with sentiment scores of text and emojis alongwith their polarities performed better than those trained only with text Figure 1 shows the distribution of emojis across sarcastic text in the train and test dataset. From Figure 1 of iSarcasmEval dataset \u2022 It is observed that sarcastic texts mostly have emojis with positive polarity. \u2022 Sarcastic texts generally demonstrate positive sentiment. Some other observations noted during the experiments are reported as follows. \u2022 Sarcastic text when augmented twice results in overfitting and results are not satisfactory on the test set. The best F1-score on the test dataset is achieved when each sarcastic text is augmented once.   \u2022 Overall the Multi-modal BERT, which can handle tabular data, performs the best. \u2022 Models trained with sentiment scores and polarities of text and emojis performed slightly better than those trained with only text. \u2022 In the test dataset only 18 sarcastic sentences had emojis. \u2022 F1-score on test dataset for text with emojis was found to be 21.05% while that of text without emojis was 31.02%. This performance may be improved if more sentences with emojis are available. Conclusion In this work, the authors took their first steps in understanding how the sentiment of emojis alongwith the text sentiment helps in detecting sarcasm. Since sarcasm is difficult to detect without context, authors attempted to uncover information implicit in the text. For this the intensity and polarity of both the available emojis and the text itself were used to identify sarcasm. It was observed that sarcastic texts generally have positive polarity. Sarcastic texts employed emojis with positive polarities more than the negative emojis. This information can be used further to improve results of sarcasm classification. The given dataset had relatively fewer texts with emojis, thus, more work is required to fully capitalize on the proposed approach. This motivates the authors to explore more sarcasm datasets with emojis to exhaustively study the impact of emojis in identifying sarcasm. Acknowledgement The authors thank the organizers of SemEval-22 organizers and iSarcasmEval for the conduct of this competition. We thank fellow competitors and peer reviewers for contributing to our learning.",
    "abstract": "This paper describes the participation of team DUCS at SemEval 2022 Task 6: iSarcasmEval -Intended Sarcasm Detection in English and Arabic. Team DUCS participated in SubTask A of iSarcasmEval which was to determine if the given English text was sarcastic or not. In this work, emojis were utilized to capture how they contributed to the sarcastic nature of a text. It is observed that emojis can augment or reverse the polarity of a given statement. Thus sentiment polarities and intensities of emojis, as well as those of text, were computed to determine sarcasm. Use of capitalization, word repetition, and use of punctuation marks like '!' were factored in as sentiment intensifiers. An NLP augmenter was used to tackle the imbalanced nature of the sarcasm dataset. Several architectures comprising of various ML and DL classifiers, and transformer models like BERT and Multimodal BERT were experimented with. It was observed that Multimodal BERT outperformed other architectures tested and achieved an F1-score of 30.71%. The key takeaway of this study was that sarcastic texts are usually positive sentences. In general emojis with positive polarity are used more than those with negative polarities in sarcastic texts.",
    "countries": [
        "India"
    ],
    "languages": [
        "Arabic",
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "{DUCS} at {S}em{E}val-2022 Task 6: Exploring Emojis and Sentiments for Sarcasm Detection"
}