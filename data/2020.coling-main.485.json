{
    "article": "In this paper, we propose a new few-shot text classification method. Compared with supervised learning methods which require a large corpus of labeled documents, our method aims to make it possible to classify unlabeled text with few labeled data. To achieve this goal, we take advantage of advanced pre-trained language model to extract the semantic features of each document. Furthermore, we utilize an edge-labeling graph neural network to implicitly models the intra-cluster similarity and the inter-cluster dissimilarity of the documents. Finally, we take the results of the graph neural network as the input of a prototypical network to classify the unlabeled texts. We verify the effectiveness of our method on a sentiment analysis dataset and a relation classification dataset and achieve the state-of-the-art performance on both tasks. Introduction In recent years, deep learning has achieved great success in many fields such as computer vision, speech recognition and natural language processing. However, to train a deep learning model usually requires a large scale of manually annotated data, which greatly limits the practicality and scalability of the model. In the text classification task, the problem of greed for large amounts of labeled data also exists. Text classification is an important task in natural language processing field. Over the years, many text classification methods have been proposed, varying from CNN based methods (Kim, 2014; Johnson and Zhang, 2015; Zhang et al., 2015) to RNN, including LSTM and GRU-based methods (Zhou et al., 2015; Yang et al., 2016) . But all the above supervised learning methods require a large corpus of labeled data, making these models hindered in practical application. In recent years, some scholars have contributed to solving the few-shot text classification problem. Han (2018) proposed a dataset FewRel and tested the performance of several typical few-shot learning models proposed in recent years on this dataset. Yu (2018) proposed an adaptive metric learning model, which can automatically determine the best weighted combination of a set of metrics obtained from a meta-learning process for a newly arrived few-shot text classification task. Gao (2019) proposed a prototypical network model (Snell et al., 2017) based on a hybrid attention mechanism, which has a better performance for noisy data scenes. Sun (2019) improved prototypical network by adopting hierarchical attention mechanism, which is applied in feature level, word level and instance level to enhance the expressive ability of semantic space. Geng (2019) applied the dynamic routing algorithm in meta-learning and proposed an induction network, which achieves a better generalization ability on different few-shot text classification tasks. We believe that previous few-shot text classification methods generally have two flaws. First, all the methods mentioned above adopt a GloVe word embedding combined with CNN or RNN structure for text embedding instead of more advanced pre-trained language models proposed in recent years, such as ELMo (Peters et al., 2018) , GPT (Radford et al., 2018) and BERT (Devlin et al., 2018) . Second, these methods focus more on the semantic features of the texts itself, ignoring the potential relationships between texts. In this paper we propose Edge-Labeling Graph Neural Network-Based Prototypical Network (EGNN-Proto) to further tackle the few-shot text classification task. EGNN-Proto takes advantage of the advanced pre-trained language model BERT, and improves the prototypical network (Snell et al., 2017) by combining an edge-labeling graph neural network (Kim et al., 2019) to better characterize and utilize the relationship between texts to achieve a better performance on few-shot text classification task. The main contributions of our work are as follows: \u2022 We propose Edge-Labeling Graph Neural Network-Based Prototypical Network (EGNN-Proto) for few-shot text classification. To our knowledge, our model is the first to combine a graph neural network with a prototypical network and the first to utilize an edge-labeling graph neural network to solve the few-shot text classification problem. \u2022 Our method outperforms the current state-of-the-art models on two few-shot text classification datasets, i.e. Amazon Review Sentiment Classification (ARSC) and Few-shot Relation classification dataset (FewRel). Related Works Few-shot learning (FSL) aims to learn classifiers for new classes with only a few training examples per class. Given a support set of labeled instances, the goal is to classify instances from query set. The seminal work on few-shot learning dates back to the early 2000s (Fei-Fei et al., 2003; Fei-Fei et al., 2006) . More recent work can be divided into two types: similarity-based methods (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018) and optimization-based methods (Ravi and Larochelle, 2016; Munkhdalai and Yu, 2017) . Researchers have also studied FSL in various NLP tasks (Yu et al., 2018; Gu et al., 2018; Gao et al., 2019; Sun et al., 2019; Geng et al., 2019; Hu et al., 2019) . Graph neural networks (GNN) were first proposed by Gori (2005) and Scarselli (2008) . Li (2015) further extended it with gated recurrent units. Generalized convolution-based propagation rules also have been directly applied to graphs (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016) , offering a balance between expressivity and sample complexity. In recent years, A few approaches (Garcia and Bruna, 2017; Kim et al., 2019; Gidaris and Komodakis, 2019) further explored GNNs for few-shot learning. Method Few-shot Text Classification In few-shot text classification task, we define the set of labeled samples as support set S and the set of unlabeled samples as query set Q. Following previous works (Sun et al., 2019; Geng et al., 2019) , we adopt the N -way, k-shot setting, where the support set S contains k labeled samples for each of N classes, noting that k is rather small in a few-shot learning task. Our goal is to perform meta-learning on the training set, then extract transferable knowledge that will allow us to deliver better few-shot learning on the support set S and classify the samples from query set Q as accurately as possible. Method Overview As illustrated in Figure 1 , our model consists of three major components, i.e., text embedding component, edge-labeling graph neural network component and prototypical network component. Text embedding component is mainly used to extract semantic features from the text and transform the original text sequences into text embedding. Then we adopt edge-labeling graph neural network to model the intra-cluster similarities and inter-cluster dissimilarities of the texts, thus we can measure the underlying relationships between a text sample and another. Finally, we classify the samples from query set using a prototypical network. Text Embedding In a few-shot text classification task, only a small amount of annotated data can be used to train the classifier. So we choose to make use of a pre-trained language model to help use better extract the feature of the text. Here we adopt a transformer encoder from BERT (Devlin et al., 2018) . A transformer encoder stack consists of 12 layers of encoders, each of which consists of a bidirectional self-attention layer and a fully connected layer. Every token in the input of the stack is first embedded into a learned d-dimensional embedding, and then transformed progressively every time it traverses one of the BERT Encoder layers. Theoretically, each word embedding output from each encoder layer contains the feature of the entire text. Here we adopt the word embedding of the special symbol [CLS], which marks the beginning of a text, from the penultimate layer as the text embedding of the whole text t, represented as e = f emb (t|\u03b8 emb ), where \u03b8 emb represents parameters in text embedding component, and are fine-tuned while training. Edge-labeling Graph Neural Network We introduce the edge-labeling graph neural network, which is initially proposed by Kim (2019) for few-shot image classification task, to better characterize the potential relationships between texts. Given the text embedding of all samples of a task, a fully connected graph is initially constructed, where each node represents each sample and each edge represents relationship between the connected nodes. Node features, which are initialized by the text embeddings of texts, i.e. e, and edge features, which are initialized based on whether the connected nodes belong to the same class, are alternately updated till convergence. Finally, the updated node features e f = f egnn (e|\u03b8 egnn ) are obtained for further processing. Prototypical Network Instead of computing the prediction probability of query nodes directly through edge features and node features as Kim (2019) , we introduce prototypical network (Snell et al., 2017) to classify samples from a more general perspective. Each sample feature e f is initially transformed through a linear layer to get a new feature representation e l = f linear (e f |\u03b8 linear ). Then prototypical network computes a prototype vector as the representation of each class, which is the mean vector representations of the support samples from the class. We compare the distances between all prototype vectors and a query vector, then classify the query sample to the nearest one. Experiments Implementation Details In implementation, we adopt BERT-base as text encoder, thus a sample is represented by a 768dimensional vector after text embedding. The episode training strategy that Vinyals (2016) proposed is adopted in training procedure. Datasets We evaluate our method and compare it with other methods on two few-shot task classification datasets, i.e. Amazon Review Sentiment Classification (ARSC) and Few-shot Relation classification dataset (FewRel). ARSC dataset (Blitzer et al., 2007) consists of English reviews of 23 types of products on Amazon. Following the experiment setting of Yu (2018) , we creat a 12-way 5-shot text classification task on this dataset. FewRel (Han et al., 2018 ) is a large-scale supervised dataset consisting of 70000 instances on 100 relations derived from Wikipedia. In our experiment, we follow the settings of Sun ( 2019 ) and evaluate our method on 5-way 5-shot and 10-way 5-shot settings. Results and Analysis Model Conclusion This work addresses the problem of few-shot text classification. We take advantage of advanced pretrained language model BERT to extract the semantic feature of the text. Then we introduce edgelabeling graph neural network to further model the potential relationships between texts. Finally we utilize a prototypical network to classify the query text. In the experiments, our method achieves the state-of-the-art performance on both ARSC and FewRel datasets. For future work, we can consider generalizing our method to other few-shot NLP problems, and even to non-NLP tasks.",
    "abstract": "In this paper, we propose a new few-shot text classification method. Compared with supervised learning methods which require a large corpus of labeled documents, our method aims to make it possible to classify unlabeled text with few labeled data. To achieve this goal, we take advantage of advanced pre-trained language model to extract the semantic features of each document. Furthermore, we utilize an edge-labeling graph neural network to implicitly models the intra-cluster similarity and the inter-cluster dissimilarity of the documents. Finally, we take the results of the graph neural network as the input of a prototypical network to classify the unlabeled texts. We verify the effectiveness of our method on a sentiment analysis dataset and a relation classification dataset and achieve the state-of-the-art performance on both tasks.",
    "countries": [
        "China"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "3",
    "year": "2020",
    "month": "December",
    "title": "Few-Shot Text Classification with Edge-Labeling Graph Neural Network-Based Prototypical Network"
}