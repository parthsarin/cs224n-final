{
    "article": "Supervised methods for Word Sense Disambiguation (WSD) benefit from highquality sense-annotated resources, which are lacking for many languages less common than English. There are, however, several multilingual parallel corpora that can be inexpensively annotated with senses through cross-lingual methods. We test the effectiveness of such an approach by attempting to disambiguate English texts through their translations in Italian, Romanian and Japanese. Specifically, we try to find the appropriate word senses for the English words by comparison with all the word senses associated to their translations. The main advantage of this approach is in that it can be applied to any parallel corpus, as long as large, highquality inter-linked sense inventories exist for all the languages considered. Introduction Cross-lingual Word Sense Disambiguation (CL-WSD) is an approach to Word Sense Disambiguation (WSD) that exploits the similarities and the differences across languages to disambiguate text in an automatic fashion. Using existing multilingual parallel corpora for this purpose is a natural choice, as shown by a long series of works in the literature; see for instance Brown and Mercer (1991) , Gale et al. (1992) , Ide et al. (2002) , Ng et al. (2003) , Chan and Ng (2005) , and Khapra et al. (2011) more recently. As Diab and Resnik (2002) showed, the translation correspondences in a parallel corpus provide valuable semantic information that can be exploited to perform WSD. For instance, Tufis \u00b8et al. (2004) used parallel corpora to validate the interlingual alignments in different WordNets (WNs). Specifically, they looked at the sense intersection between the lexical items found in all the reciprocal translations of a parallel corpus. Gliozzo et al. (2005) showed how CL-WSD can help to sense-annotate a bilingual corpus by looking at the semantic differences in a language pair. Bentivogli and Pianta (2005) , on the other hand, focused on how meaning is somehow preserved despite those differences, which allows us to transfer the semantic annotation of a text in a certain language to its translation in another language. The sense projection procedure that they used is simple yet powerful, but it can only be applied on corpora in which at least one parallel text is annotated with senses. Nevertheless, given the difficulty to come across sense-annotated data, any way to produce such data is of great benefit to WSD. The knowledge acquisition bottleneck is still a challenge to address for most languages. Given the task of annotating an ambiguous word in a multilingual parallel corpus, some valuable information can be derived through the comparison of the set of senses of each of the word's translations. If fewer senses (or one only, in the optimal case) are retained across languages, then the cross-lingual information has helped reducing (or solving) the ambiguity. In previous work (Bond and Bonansinga, 2015) we employed sense intersection (SI) to annotate a trilingual parallel corpus in English, Italian and Romanian built upon SemCor (SC) (Landes et al., 1998) . We summarize the data used and our findings in Section 2. In Section 3 we continue investigating in the same strand by introducing a further language, Japanese, to disambiguate English text. In Section 4 we show how an annotation task can benefit from coarser sense distinctions. In Section 5 we examine thoroughly how and how much each additional language helps the automatic sense disambiguation process. We conclude in Section 6. Multilingual Sense Intersection In Bond and Bonansinga (2015) we explored the cross-lingual approaches pioneered by Gliozzo et al. (2005) and Bentivogli and Pianta (2005) to annotate the SC corpus (Landes et al., 1998) and two corpora built upon it from its Italian and Romanian translations. This parallel corpus, though rather small (see Subsection 2.1), is ideal for the task as it is sense-annotated in all its translations, thus making the evaluation of alternative sense annotation methods straightforward. We briefly present the data used back then and introduce the last component of the corpus, the Japanese SemCor (Bond et al., 2012) , which is included in the analysis presented in this paper. Data Developed at Princeton University, SC is a subset of the Brown Corpus of Standard American English (Ku\u010dera and Francis, 1967) enriched with sense annotations referring to the WN sense inventory (see Section 2.2). Bentivogli and Pianta (2005) manually translated 116 SC texts and automatically aligned them to their English counterparts. Then the sense annotations of the English words were automatically transferred following the word alignment, thus leading to the creation of a sense-annotated English-Italian corpus, MultiSemCor (MSC). With the purpose of providing a Romanian version of SC, Lupu et al. (2005) developed the Romanian SemCor (RSC) (Lupu et al., 2005; Ion, 2007) , which shares 50 texts with MSC. Unfortunately, RSC is not word-aligned to any other component of the parallel corpus, which is a requirement to perform sense mapping with any of the mentioned procedures. Nevertheless, as the sentence alignment is available and as we are only interested in content words, we attempted a word alignment based upon the information already available. First, we aligned all the reciprocal translations in the same sentence pair having identical sense annotation. Then, we aligned the remaining content words, if any, using heuristics that exploit PoS information and path similarity in the WN ontology. Finally, we manually checked a sample of the alignment found in this fashion and we observed a precision of 97%; of course, errors can only be introduced in the second step, when the heuristics used to align the remaining unaligned content words come into play. Bond et al. (2012) built a Japanese SemCor (JSC) matching the texts covered in MSC, after porting the sense annotations to WN 3.0 using the mappings provided by Daude et al. (2003) . The sense annotation was carried out through sense projection by exploiting the word alignment, similarly to what Bentivogli and Pianta (2005) did for Italian. JSC follows the Kyoto Annotation Format (KAF) (Bosma et al., 2009) and is released under the same license as SC. 1  In Table 1 we remind the basic statistics of each corpus. For English and Italian we also specify the number of the target words after the migration to WordNet 3.0 (WN 3.0). In Table 2 Sense Inventories When MSC was released, MultiWordNet 2 (MWN) (Pianta et al., 2002) , a multilingual WordNet aligned to Princeton WN 1.6, was used. As described in Bond and Bonansinga (2015) , we ported all senses annotations in MSC to WN 3.0, so to make it possible a comparison between the different components of the parallel corpus. To this aim, we used automatically inferred mappings (Daud\u00e9 et al., 2000; Daud\u00e9 et al., 2001) . However, the changes occurred between WN versions 1.6 and 3.0 led to the loss of 4,631 sense annotations (1,204 types, half of which are adjective satellites). The Romanian WordNet (RW), created within the BalkaNet project (Stamou et al., 2002) and then consistently grown independently (Barbu Mititelu et al., 2014) was aligned to WN 3.0 with precision of 95% (Tufis \u00b8et al., 2013) . The Japanese WN (JWN) (Isahara et al., 2008; Bond et al., 2009) In Table 3 we give basic coverage statistics for the WNs of our target languages. The Open Multilingual WordNet (OMW) 3 is an open-source multilingual database that connects all open WNs linked to the English WN, including Italian (Pianta et al., 2002) among the 28 languages supported (Bond and Paik, 2012; Bond and Foster, 2013) . A convenient interface to OMW is provided in the Python module NLTK 4 (Bird et al., 2009) . Findings For the sake of completeness, in previous work we performed sense projection on the Italian and Romanian corpora using English as pivot, scoring a precision of over 90% in both cases. As for SI, we report the previous precision and coverage scores obtained through trilingual SI in Table 4, along with the Most Frequent Sense (MFS) baseline, that assigns each word its most frequent sense. In this step, sense frequency statistics (SFS) are therefore necessary, but unfortunately there are Multilingual Sense Intersection with languages from different families The theoretical justification behind Multilingual Sense Intersection (SI) is in that an ambiguous word will often be translated in different words in another language. As a consequence, the knowledge of all the senses associated to its translation can help detect the sense actually intended in the original text. More commonly, such a comparison will help reduce the ambiguity, but it will not identify one single, shared sense. On the other hand, a text whose ambiguity has been progressively reduced through automatic methods can be completely disambiguated by a human annotator at a lesser cost. Moreover, the more the languages available for comparison in the parallel corpus, the more likely is that SI actually manages to discern the correct sense in context. Differently from our previous work, where we disambiguated all the texts that were aligned with at least one other language, in the following section we show results computed over 49 texts. Those constitute the subset of the corpus shared across all four components and for which we have alignments. As a result, we use an even smaller corpus through which, nevertheless, we can show more effectively the contribution of up to three languages. Given an ambiguous word, all its translations provide their 'set of sense', as retrieved from the shared sense inventory. Then, intersection is performed over every non-empty set and successes when the final overlap contains only one sense, meaning that the target word has been disambiguated. Otherwise, the overlap is further intersected with the top most frequent senses available for the target lemma. We take note whether the sense selected was the most frequent one. As before, we resort to sense frequency statistics (SFS) whenever the target word is not yet disambiguated after SI. These frequencies were calculated over all texts in the corpus except the one being annotated. Introducing coarse-grained senses Sense inventories are a crucial part of this approach. Not only are a sufficient coverage and the alignment to the Princeton WN necessary: when it comes to deciding how to define close, very specific senses, a trade-off between the detail of the sense description and its actual usability in real contexts is highly desirable. The fine granularity of WN senses can occasionally, depending on the application, be more of a practical disadvantage than a quality. In this analysis, for instance, error analysis suggested that the senses found through SI were often very close, but it may happen that they are discarded as wrong outputs just because one language has a WN more developed and granular than another. We should also bear in mind that the correct senses against which we evaluate were picked by trained human annotators in the first place, and human annotators tend to describe a word as precisely as possible. Conscious of this limit, Navigli (2006) devised an automatic methodology to find a reasonable sense clustering for the senses in WN 2.1. Sense clustering can be of great help in tasks where minor sense distinctions can be ignored, allowing a coarse-grained evaluation. They found 29,974 main clusters, some of which were manually validated by an expert lexicographer for the Semeval all-word task. We mapped the senses in the clusters found to WN 3.0, losing 101 of them in the process (typically one-element clusters). When evaluating the results of SI, we performed a coarse-grained eval-uation; in particular, whenever the sense found by SI was not correct, we checked whether it was part of a sense cluster and whether the correct sense was in it. If so, we considered the output of the algorithm correct. Evaluation In Table 4 we show the improvement in precision obtained thanks to coarse-grained evaluation with respect to the results in Bond and Bonansinga (2015) . English and Italian show respectively a significant improvement of 0.1 and 0.11. In the case of Romanian, the improvement is not as big, but still meaningful (0.07). Of course, coarsegrained evaluation causes the MFS baseline to improve as well. In the case of English -which, again, is the component most subjected to the bias introduces by SFS -the difference between MFS and SI decreases a little, but MFS still performs better. The case of Italian is unique, in that SI obtains better precision scores with both fine and coarsegrained senses. For Romanian, on the other hand, SI performs better until coarse-grained evaluation is employed, and the improvement achieved by MFS is striking. In Table 5 we show our latest attempt to disambiguate English text by using the semantic information of its aligned translation in a parallel corpus. The languages that contribute to the dis-ambiguation process are Italian, Romanian and Japanese, and all together they manage to beat MFS, if coarse-grained senses are considered. Conclusions For future work, it is important to analyze the progressive improvement that we can achieve by taking into account semantic information from one language at the time, so as to verify if it is true that it is the very diverse languages that contribute the most to the disambiguation process. As for the sense inventories, it would be interesting to compare different lexical resources for Italian, that is MWN and ItalWordNet (ITW) (Roventini et al., 2002) . ITW was born as the Eu-roWordNet Italian database, but even though compatible to a certain extent with EuroWordNet, it is released in XML format. ITW includes about 47.000 lemmas, 50.000 synsets and 130.000 semantic relations and is currently maintained by the Institute for Computational Linguistics (ILC) at the National Research Council (CNR). An updated version is freely available online. 5  Finally, we could easily address, at least for English, the lack of unbiased sense frequency statistics by computing them over the WordNet Gloss Corpus, in which glosses are sense-annotated. 6  This corpus alone would provide sense frequencies for 157,300 lemma-pos pairs. Acknowledgments",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 5.51223498068687e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not provide any specific information regarding funding from defense, corporate entities, research agencies, foundations, or any other sources. Without explicit mentions of funding, it is not possible to accurately determine the sources of financial support for the research presented in the article.",
    "abstract": "Supervised methods for Word Sense Disambiguation (WSD) benefit from highquality sense-annotated resources, which are lacking for many languages less common than English. There are, however, several multilingual parallel corpora that can be inexpensively annotated with senses through cross-lingual methods. We test the effectiveness of such an approach by attempting to disambiguate English texts through their translations in Italian, Romanian and Japanese. Specifically, we try to find the appropriate word senses for the English words by comparison with all the word senses associated to their translations. The main advantage of this approach is in that it can be applied to any parallel corpus, as long as large, highquality inter-linked sense inventories exist for all the languages considered.",
    "countries": [
        "Italy",
        "Singapore"
    ],
    "languages": [
        "English",
        "Romanian",
        "Italian",
        "Japanese"
    ],
    "numcitedby": 5,
    "year": 2016,
    "month": "27--30 January",
    "title": "Multilingual Sense Intersection in a Parallel Corpus with Diverse Language Families"
}