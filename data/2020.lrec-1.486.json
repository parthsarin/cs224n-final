{
    "article": "Data-driven segmentation of words into subword units has been used in various natural language processing applications such as automatic speech recognition and statistical machine translation for almost 20 years. Recently it has became more widely adopted, as models based on deep neural networks often benefit from subword units even for morphologically simpler languages. In this paper, we discuss and compare training algorithms for a unigram subword model, based on the Expectation Maximization algorithm and lexicon pruning. Using English, Finnish, North Sami, and Turkish data sets, we show that this approach is able to find better solutions to the optimization problem defined by the Morfessor Baseline model than its original recursive training algorithm. The improved optimization also leads to higher morphological segmentation accuracy when compared to a linguistic gold standard. We publish implementations of the new algorithms in the widely-used Morfessor software package. Introduction Subword segmentation has become a standard preprocessing step in many neural approaches to natural language processing (NLP) tasks, e.g Neural Machine Translation (NMT) (Sennrich et al., 2015) and Automatic Speech Recognition (ASR) (Smit et al., 2017) . Word level modeling suffers from sparse statistics, issues with Out-of-Vocabulary (OOV) words, and heavy computational cost due to a large vocabulary. Word level modeling is particularly unsuitable for morphologically rich languages, but subwords are commonly used for other languages as well. Subword segmentation is best suited for languages with agglutinative morphology. While rule-based morphological segmentation systems can achieve high quality, the large amount of human effort needed makes the approach problematic, particularly for low-resource languages. The systems are language dependent, necessitating use of multiple tools in multilingual setups. As a fast, cheap and effective alternative, data-driven segmentation can be learned in a completely unsupervised manner from raw corpora. Unsupervised morphological segmentation saw much research interest until the early 2010's; for a survey on the methods, see Hammarstr\u00f6m and Borin (2011) . Semi-supervised segmentation with already small amounts of annotated training data was found to improve the accuracy significantly when compared to a linguistic segmentation; see Ruokolainen et al. (2016) for a survey. While this line of research has been continued in supervised and more grammatically oriented tasks (Cotterell et al., 2017) , the more recent work on unsupervised segmentation is less focused on approximating a linguistically motivated segmentation. Instead, the aim has been to tune subword segmenta-tions for particular applications. For example, the simple substitution dictionary based Byte Pair Encoding segmentation algorithm (Gage, 1994) , first proposed for NMT by Sennrich et al. (2015) , has become a standard in the field. Especially in the case of multilingual models, training a single language-independent subword segmentation method is preferable to linguistic segmentation (Arivazhagan et al., 2019) . In this study, we compare three existing and one novel subword segmentation method, all sharing the use of a unigram language model in a generative modeling framework. The previously published methods are Morfessor Baseline (Creutz and Lagus, 2002) , Greedy Unigram Likelihood (Varjokallio et al., 2013) , and Sen-tencePiece (Kudo, 2018) . The new Morfessor variant proposed in this work is called Morfessor EM+Prune. The contributions of this article are (i) a better training algorithm for Morfessor Baseline, with reduction of search error during training, and improved segmentation quality for English, Finnish and Turkish; (ii) comparing four similar segmentation methods, including a close look at the SentencePiece reference implementation, highlighting details omitted from the original article (Kudo, 2018) ; (iii) and showing that the proposed Morfessor EM+Prune with particular hyper-parameters yields SentencePiece. Morphological Segmentation with Unigram Language Models Morphological surface segmentation is the task of splitting words into morphs, the surface forms of meaningbearing sub-word units, morphemes. The concatena-  This study focuses on segmentation methods applying a unigram language model. In the unigram language model, an assumption is made that the morphs in a word occur independently of each other. Alternatively stated, it is a zero-order (memoryless) Markov model, generalized so that one observation can cover multiple characters. The probability of a sequence of morphs decomposes into the product of the probabilities of the morphs of which it consists. \u2713 \u2713 \u2713 N-best decoding \u2713 - \u2713 \u2713 Sampling decoding - - \u2713 \u2713 Count dampening \u2713 - - \u2713 Semi-supervised \u2713 - - \u2713 Requires pretokenization \u2713 \u2713 - \u2713 Reference implementation Python C++ C++ Python P \u03b8 (s) = N \u220f i=1 P \u03b8 (m i ) (1) The Expectation Maximization (EM) algorithm (Dempster et al., 1977) is an iterative algorithm for finding Maximum Likelihood (ML) or Maximum a Posteriori (MAP) estimates for parameters in models with latent variables. The EM algorithm consists of two steps. In the E-step (2), the expected value of the complete data likelihood including the latent variable is taken, and in the M-step (3), the parameters are updated to maximize the expected value of the E-step: Q(\u03b8, \u03b8 (i\u22121) ) = \u222b y log P (D, y | \u03b8)P (y | D, \u03b8 (i\u22121) )dy (2) \u03b8 i = arg max \u03b8 Q(\u03b8, \u03b8 (i\u22121) ). (3) When applied to a (hidden) Markov model, EM is called the forward-backward algorithm. Using instead the related Viterbi algorithm (Viterbi, 1967) is sometimes referred to as hard-EM. Related Work In this section we review three previously published segmentation methods that apply a unigram language model. Table 1 summarizes the differences between these methods. Morfessor Baseline Morfessor is a family of generative models for unsupervised morphology induction (Creutz and Lagus, 2007) . Here, consider the Morfessor 2.0 implementation (Virpioja et al., 2013) of Morfessor Baseline method (Creutz and Lagus, 2002) . A point estimate for the model parameters \u03b8 is found using MAP estimation with a Minimum Description Length (MDL) (Rissanen, 1989) inspired prior that favors lexicons containing fewer, shorter morphs. The MAP estimate yields a two-part cost function, consisting of a prior (the lexicon cost) and likelihood (the corpus cost). The model can be tuned using the hyperparameter \u03b1, which is a weight applied to the likelihood (Kohonen et al., 2010) : \u03b8 = arg min \u03b8 {\u2212 log prior P (\u03b8) \u2212\u03b1 log likelihood P (D | \u03b8)} (4) The \u03b1 parameter controls the overall amount of segmentation, with higher values increasing the weight of each emitted morph in the corpus (leading to less segmentation), and lower values giving a relatively larger weight to a small lexicon (more segmentation). The prior can be further divided into two parts: the prior for the morph form properties and the usage properties. The form properties encode the string representation of the morphs, while the usage properties encode their frequencies. Morfessor Baseline applies a non-informative prior for the distribution of the morph frequencies. It is derived using combinatorics from the number of ways that the total token count \u03bd can be divided among the \u00b5 lexicon items: P (\u03c4 1 , . . . , \u03c4 \u00b5 | \u00b5, \u03bd) = 1/ ( \u03bd \u2212 1 \u00b5 \u2212 1 ) . ( 5 ) Morfessor Baseline is initialized with a seed lexicon of whole words. The Morfessor Baseline training algorithm is a greedy local search. During training, in addition to storing the model parameters, the current best segmentation for the corpus is stored in a graph structure. The segmentation is iteratively refined, by looping over all the words in the corpus in a random order and resegmenting them. The resegmentation is applied by recursive binary splitting, leading to changes in other words that share intermediary units with the word currently being resegmented. The search converges to a local optimum, and is known to be sensitive to the initialization (Virpioja et al., 2013) . In the Morfessor 2.0 implementation, the likelihood weight hyper-parameter \u03b1 is set either with a grid search using the best evaluation score on a held-out development set, or by applying an approximate automatic tuning procedure based on a heuristic guess of which direction the \u03b1 parameter should be adjusted. Greedy Unigram Likelihood Varjokallio et al. ( 2013 ) presents a subword segmentation method, particularly designed for use in ASR. It applies greedy pruning based on unigram likelihood. The seed lexicon is constructed by enumerating all substrings from a list of common words, up to a specified maximum length. Pruning proceeds in two phases, which the authors call initialization and pruning. In the first phase, a character-level language model is trained. The initial probabilities of the subwords are computed using the language model. The probabilities are refined by EM, followed by hard-EM. During the hard-EM, frequency based pruning of subwords begins. In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold. SentencePiece SentencePiece (Kudo and Richardson, 2018; Kudo, 2018) is a subword segmentation method aimed for use in any NLP system, particularly NMT. One of its design goals is use in multilingual systems. Although (Kudo, 2018) implies a use of maximum likelihood estimation, the reference implementation 2 uses the implicit Dirichlet Process prior called Bayesian EM (Liang and Klein, 2007) . In the M-step, the count normalization is modified to P (z) = exp(\u03a8(C z )) exp(\u03a8( \u2211 z \u2032 C z \u2032 )) (6) where \u03a8 is the digamma function. The seed lexicon is simply the e.g. one million most frequent substrings. SentencePiece uses an EM+Prune training algorithm. Each iteration consists of two subiterations of EM, after which the lexicon is pruned. Pruning is based on Viterbi counts (EM+Viterbiprune). First, subwords that do not occur in the Viterbi segmentation are pre-pruned. The cost function is the estimated change in likelihood when the subword is removed, estimated using the assumption that all probability mass of the removed subword goes to its Viterbi segmentation. Subwords are sorted according to the cost, and a fixed proportion of remaining subwords are pruned each iteration. Single character subwords are never pruned. A predetermined lexicon size is used as the stopping condition. Morfessor EM+Prune Morfessor EM+Prune 3 uses the unigram language model and priors similar to Morfessor Baseline, but combines them with EM+Prune training. Prior The prior must be slightly modified for use with the EM+Prune algorithm. The prior for the frequency distribution (5) is derived using combinatorics. When using real-valued expected counts, there are infinite assignments of counts to parameters. Despite not being theoretically motivated, it can still be desirable to compute an approximation of the Baseline frequency distribution prior, in order to use EM+Prune as an improved search to find more optimal parameters for the original cost. To do this, the real valued token count \u03bd is rounded to the nearest integer 4 . Alternatively, the prior for the frequency distribution can be omitted, or a new prior with suitable properties could be formulated. We do not propose a completely new prior in this work, instead opting to remain as close as possible to Morfessor Baseline. In Morfessor EM+Prune, morphs are explicitly stored in the lexicon, and morphs are removed from the lexicon only during pruning. This differs from Morfessor Baseline, in which a morph is implicitly considered to be stored in the lexicon if it has non-zero count. The prior for the morph form properties does not need to be modified. During the EM parameter estimation, the prior for the morph form properties is omitted as the morph lexicon remains constant. During pruning, the standard form prior is applicable. Additionally we apply the Bayesian EM implicit Dirichlet Process prior (Liang and Klein, 2007) . We experiment with four variations of the prior: 1. the full EM+Prune prior, 2. omitting the Bayesian EM (noexp\u03a8), 3. omitting the approximate frequency distribution prior (nofreqdistr), 4. and omitting the prior entirely (noprior). Seed Lexicon The seed lexicon consists of the one million most frequent substrings, with two restrictions on which substrings to include: pre-pruning of redundant subwords, and forcesplit. Truncating to the chosen size is performed after pre-pruning, which means that prepruning can make space for substrings that would otherwise have been below the threshold. Pre-pruning of redundant subwords is based on occurrence counts. If a string x occurs n times, then any substring of x will occur at least n times. Therefore, if the substring has a count of exactly n, we know that it is not needed in any other context except as a part of x. Such unproductive substrings are likely to be poor candidate subwords, and can be removed to make space in the seed lexicon for more useful substrings. This prepruning is not a neutral optimization, but does affect segmentation results. We check all initial and final substrings for redundancy, but do not pre-prune internal substrings. To achieve forced splitting before or after certain characters, e.g. hyphens, apostrophes and colons, substrings which include a forced split point can be removed from the seed lexicon. As EM+Prune is unable to introduce new subwords, this pre-pruning is sufficient to guarantee the forced splits. While Morfessor 2.0 only implements force splitting certain characters to single-character morphs, i.e. force splitting on both sides, we implement more fine-grained force splitting separately before and after the specified character. Training Algorithm We experiment with three variants of the EM+Prune iteration structure: 1. EM, 2. Lateen-EM, EM+Viterbi-prune EM+Viterbi-prune is an intermediary mode between EM and lateen-EM in the context of pruning. The pruning decisions are made based on counts from a single iteration of Viterbi training, but these Viterbi counts are not otherwise used to update the parameters. In effect, this allows for the more aggressive pruning using the Viterbi counts, while retaining the uncertainty of the soft parameters. Each iteration begins with 3 sub-iterations of EM. In In (\u03b1-weighted) Minimum Description Length (MDL) pruning, subwords are pruned until the estimated cost starts rising, or until the pruning quota for the iteration is reached, whichever comes first. A subword lexicon of a predetermined size can be used as pruning criterion in two different ways. If the desired \u03b1 is known in advance, or if the prior is omitted, subwords are pruned until the desired lexicon size is reached, or until the pruning quota for the iteration is reached, whichever comes first. To reach a subword lexicon of a predetermined size while using the Morfessor prior, the new automatic tuning procedure can be applied. For each subword, the estimated change in prior and likelihood are computed separately. These allow computing the value of \u03b1 that would cause the removal of each subword to be cost neutral, i.e. the value that would cause MDL pruning to terminate at that subword. For subwords with the same sign for both the change in prior and likelihood, no such threshold \u03b1 can be computed: if the removal decreases both costs the subword will always be removed, and if it increases both costs it will always be kept. Sorting the list of subwords according to the estimated threshold \u03b1 including the always kept subwords allows automatically tuning \u03b1 so that a sub- word lexicon of exactly the desired size is retained after MDL pruning. The automatic tuning is repeated before the pruning phase of each iteration, as retraining the parameters affects the estimates. Sampling of Segmentations Morfessor EM+Prune can be used in subword regularization (Kudo, 2018) , a denoising-based regularization method for neural NLP systems. Alternative segmentations can be sampled from the full data distribution using Forward-filtering backward-sampling algorithm (Scott, 2002) or approximatively but more efficiently from an n-best list. SentencePiece as a Special Case of Morfessor EM+Prune Table 1 contains a comparison between all four methods discussed in this work. To recover SentencePiece, Morfessor EM+Prune should be run with the following settings: The prior should be omitted entirely, leaving only the likelihood \u03b8 = arg min \u03b8 {\u2212 log P (D | \u03b8)} (7) As the tuning parameter \u03b1 is no longer needed when the prior is omitted, the pruning criterion can be set to a predetermined lexicon size, without automatic tuning of \u03b1. Morfessor by default uses type-based training; to use frequency information, count dampening should be turned off. The seed lexicon should be constructed without using forced splitting. The EM+Viterbi-prune training scheme should be used, with Bayesian EM turned on. Experimental Setup English, Finnish and Turkish data are from the Morpho Challenge 2010 data set (Kurimo et al., 2010a; Kurimo et al., 2010b) . The training sets contain ca 878k, 2.9M and 617k word types, respectively. As test sets we use the union of the 10 official test set samples. For North S\u00e1mi, we use a list of ca 691k word types extracted from Den samiske tekstbanken corpus (Sametinget, 2004) . and the 796 word type test set from version 2 of the data set collected by (Gr\u00f6nroos et al., 2015; Gr\u00f6nroos et al., 2016) . In most experiments we use a grid search with a development set to find a suitable value for \u03b1. The exception is experiments using autotuning or lexicon size criterion, and experiments using SentencePiece. We use type-based training (dampening counts to 1) with all Morfessor methods. For English, we force splits before and after hyphens, and before apostrophes, e.g. \"women's-rights\" is force split into \"women + +'s + +-+ +rights\". For Finnish, we force splits before and after hyphens, and after colons. For North S\u00e1mi, we force splits before and after colons. For Turkish, the Morpho Challenge data is preprocessed in a way that makes force splitting ineffectual. Evaluation The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their \u03b1-weighted sum. The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary F 1 -score (Virpioja et al., 2011) . The boundary F 1score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score. Error Analysis We perform an error analysis, with the purpose of gaining more insight into the ability of the methods to model particular aspects of morphology. We follow the procedure used by Ruokolainen et al. (2016) . It is based on a categorization of morphs into the categories prefix, stem, and suffix. The category labels are derived from the original morphological analysis labels in the English and Finnish gold standards, and directly correspond to the annotation scheme used in the North S\u00e1mi test set. We first divide errors into two kinds, over-segmentation and under-segmentation. Over-segmentation occurs when a boundary is incorrectly assigned within a morph segment. In under-segmentation, the a correct morph boundary is omitted from the generated segmentation. We further divide the errors by the morph category in which the over-segmentation occurs, and the two morph categories surrounding the omitted boundary in under-segmentation. Results Figure 1 compares the cost components of the Morfessor model across different \u03b1 parameters. The lowest costs for the mid-range settings are obtained for the EM+Prune algorithm, but for larger lexicons, the Baseline algorithm copes better. As expected, using forced splits at certain characters increase the costs, and the increase is larger than between the training algorithms. As Turkish preprocessing causes the results to be unaffected by the forced splits, we only report results without them. Tables 2 to 5 show the Morfessor cost of the segmented training data for particular \u03b1 values. Again, the proposed Morfessor EM+Prune reaches a lower Morfessor cost than Morfessor Baseline. Using the lateen-EM has only minimal effect to the costs, decreasing the total cost slightly for English and increasing for the other languages. Turkish results include the \"keepredundant\" setting discussed below in more detail. Figure 2 shows the Precision-Recall curves for the primary systems, for all four languages. While increasing the Morfessor cost, forced splitting improves BPR. Tables 6 to 9 show test set Boundary Precision, Recall and F 1 -score (BPR) results at the optimal tuning point (selected using a development set) for each model, for English, Finnish, Turkish and North S\u00e1mi, respectively 6 . The default Morfessor EM+Prune configuration (\"soft\" EM, full prior, forcesplit) significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi, for which there is no significant difference between the methods. Morfessor EM+Prune is less responsive to tuning than Morfessor Baseline. This is visible in the shorter lines in Figures 1 and 2 , although the tuning parameter takes values from the same range. In particular, EM+Prune can not easily be tuned to produce very large lexicons. Table 10 : Error analysis for English (eng, \u03b1 = 0.9), Finnish (fin, \u03b1 = 0.02), and North S\u00e1mi (sme, \u03b1 = 1.0). All results without forcesplit. Over-segmentation and under-segmentation errors reduce precision and recall, respectively. Pre-pruning of redundant substrings gives mixed results. For Turkish, both Morfessor cost and BPR are degraded by the pre-pruning, but for the other three languages the pre-pruning is beneficial or neutral. When tuning \u03b1 to very high values (less segmentation), pre-pruning of redundant substrings improves the sensitivity to tuning. The same effect may also be achievable by using a larger seed lexicon. We perform most of our experiments with pre-pruning turned on. To see the effect of pre-pruning on the seed lexicon, we count the number of subwords that are used in the gold standard segmentations, but not included in seed lexicons of various sizes. Taking Finnish as an example, we see 203 subword types missing from a 1 million substring seed lexicon without pre-pruning. Turning on pre-pruning decreases the number of missing types to 120. To reach the same number without using prepruning, a much larger seed lexicon of 1.7M substrings must be used. Omitting the frequency distribution appears to have little effect on Morfessor cost and BPR. Turning off Bayesian EM (noexp\u03a8) results in a less compact lexicon resulting in higher prior cost, but improves BPR for two languages: English and Turkish. Table 10 contains the error analysis for English, Finnish and North S\u00e1mi. For English and North S\u00e1mi, EM+Prune results in less under-segmentation but worse over-segmentation. For Finnish these results are reversed. However, the suffixes are often better modeled, as shown by lower under-segmentation on SUF-SUF (all languages) and STM-SUF (English and North S\u00e1mi). Conclusion We propose Morfessor EM+Prune, a new training algorithm for Morfessor Baseline. EM+Prune reduces search error during training, resulting in models with lower Morfessor costs. Lower costs also lead to improved accuracy when segmentation output is compared to linguistic morphological segmentation. We compare Morfessor EM+Prune to three previously published segmentation methods applying unigram language models. We find that using the Morfessor prior is beneficial when the reference is linguistic morphological segmentation. In this work we focused on model cost and linguistic segmentation. In future work the performance of Morfessor EM+Prune in applications will be evaluated. Also, a new frequency distribution prior, which is theoretically better motivated or has desirable properties, could be formulated. Acknowledgements This study has been supported by the MeMAD project, funded by the European Union's Horizon 2020 research and innovation programme (grant agreement \u2116 780069), and the FoTran project, funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement \u2116 771113) Computer resources within the Aalto University School of Science \"Science-IT\" project were used."
}