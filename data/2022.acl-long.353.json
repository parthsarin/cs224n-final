{
    "article": "Program understanding is a fundamental task in program language processing. Despite the success, existing works fail to take human behaviors as reference in understanding programs. In this paper, we consider human behaviors and propose the PGNN-EK model that consists of two main components. On the one hand, inspired by the \"divide-and-conquer\" reading behaviors of humans, we present a partitioningbased graph neural network model PGNN on the upgraded AST of codes. On the other hand, to characterize human behaviors of resorting to other resources to help code comprehension, we transform raw codes with external knowledge and apply pre-training techniques for information extraction. Finally, we combine the two embeddings generated from the two components to output code embeddings. We conduct extensive experiments to show the superior performance of PGNN-EK on the code summarization and code clone detection tasks. In particular, to show the generalization ability of our model, we release a new dataset that is more challenging for code clone detection and could advance the development of the community. Our codes and data are publicly available at https://github.com/ RecklessRonan/PGNN-EK. Introduction The past decades have witnessed the prosperity of programming platforms, such as Github and Stack Overflow. These platforms generate massive open-source code 1 data that is named as \"Big Code\" in (Allamanis et al., 2018a) . To automate the software development and maintenance, based on the \"Software Naturalness\" hypothesis (Hindle et al., 2016) , natural language processing (NLP) techniques have been applied in program understanding. After that, a series of downstream programming language processing (PLP) tasks can be performed, including code summarization (Zhang et al., 2020; Ahmad et al., 2020; Liu et al., 2021) and code clone detection (Zhang et al., 2019; Wang et al., 2020) . Existing works for understanding programs mainly utilize three types of information: code context, code structure and external knowledge. Specifically, code context refers to the token sequence in the code. For code structure, each code can be parsed into various types of intermediate representations, such as AST (Abstract Syntax Tree), CFG (Control Flow Graph) and PDG (Program Dependence Graph). These representations capture the structural information of codes. Further, there also exists external knowledge associated with codes, such as API documentation and other exemplary codes. Despite the success, all these models ignore considering human behaviors in reading programs. Recently, (Bengio et al., 2021) suggest the potential futures of deep learning by comparing current AI methods with human learning abilities. This further prompts us to revisit program understanding: Can we develop a model that understands programs like humans? In the domain of programming education, how people understand codes is a topic that has been studied. For example, based on knowledge base including syntactical knowledge (e.g., programming basics) and semantic knowledge (e.g., API documentation), (Schulte et al., 2010) offer a bottom-up reading technique, which assumes that people begin with individual code lines and chunks, and then combine them into higher-level abstractions. Further, (Park et al., 2016) state that when people read codes, reasoning about the hierarchical relationship of blocks, statements, expressions and variables is necessary. Based on these studies, we conclude three key points for human understanding codes. First, the transition of defined variables has to be traced. Second, humans usually adopt a \"divideand-conquer\" strategy, which divides codes based on statements and then understands codes from a local-to-global view. Third, humans resort to external knowledge to comprehend codes, such as API documentation and code examples written by experts. In this paper, inspired by human behaviors for code comprehension, we propose a novel Partitioning-based Graph Neural Network with External Knowledge (PGNN-EK). To capture code context and structure, PGNN-EK upgrades the traditional AST and defines a novel subtoken-based AST called S-AST. In S-AST, we add edges between variables to trace the variable transitions, edges between adjacent tree leaves from left to right to enrich the context and structure information, and edges between sub-nodes corresponding to subtokens tokenized from user-defined identifiers to handle the Out of Vocabulary (OOV) problem (Karampatsis et al., 2020) . Details will be illustrated later. After that, we first apply graph neural network (GNN) models on the S-AST to derive a code embedding. To further implement the \"divide-and-conquer\" reading strategy, we partition the S-AST into multiple subgraphs, which follow the sequence of statements in the original code. For each subgraph, we use GNN models to generate the subgraph embedding. Then, these subgraph embeddings are fused to generate another code embedding. For these two code embeddings, since they are both derived from S-AST, we further aggregate them. On the other hand, to characterize the dependence on external knowledge for code comprehension, we traverse the AST of the original code to derive a sequence of tokens for syntactic knowledge and then add the API descriptions to the end for semantic knowledge. We then apply Code-BERT (Feng et al., 2020) on the token sequence to capture external knowledge. Finally, PGNN-EK generates the output code embedding by combining the embedding derived from S-AST and the one from external knowledge. To evaluate the model performance, we conduct experiments on the code summarization task and code clone detection task, respectively. Before we apply PGNN-EK on the code clone detection benchmarks in CodeXGLUE (Shi et al., 2021) extracted from the BigCloneBench 2014 dataset (Svajlenko et al., 2014) , we notice from the leaderboard 2 that the results are incredibly high, where the minimum F1 score is 0.949. Then we dive into the characteristics of the dataset and find that the functionalities of codes in the test set have all appeared in the training set. Therefore, the dataset is very simple. To further test the model's generalization ability, we construct a new dataset, where the test set contains codes whose functionality has never appeared in the training set. This new dataset provides an insightful reference for further research in the community. Our main contributions are summarized as follows: \u2022 We construct a new code structure representation S-AST that can be used to handle the OOV problem in PLP. \u2022 We follow human behaviors in understanding codes and propose a novel model PGNN-EK that leverages code context, structure and external knowledge. Specifically, we put forward a novel partitioning-based graph neural network model that can effectively use code context and structure. We also present a code transformation method to utilize external knowledge in boosting comprehension. \u2022 We conduct extensive experiments on code summarization and code clone detection tasks to demonstrate the effectiveness of our model. In particular, we identify the limitation of a benchmark dataset for code clone detection and release a new dataset that is more challenging. 2 Related Work Program Understanding Program understanding is a topic that has received wide attention. Early works use either code context or structure information. For example, taking codes as raw texts, some works use language models (Raychev et al., 2014; Allamanis et al., 2015) , RNN-series (Zaremba and Sutskever, 2014; Dam et al., 2016) and attention (Iyer et al., 2016) to represent codes. However, different from natural language, programs are more structural, which can be parsed into intermediate graphs, such as AST. Many works for code analysis are then proposed based on AST, such as AST-based LSTM (Wei and Li, 2017) , AST-based CNN (Yu et al., 2019) , ASTNN (Zhang et al., 2019) To simplify the graph, we create a code snippet (top left), whose variables are defined with only one character, such as \"a\" and \"b\". In real tasks, the codes are longer and user-defined identifiers are more semantically complex. This could add more subtoken nodes and edges. The figure is better viewed in color. 2019b), and code2seq (Alon et al., 2019a) . Recently, GNN models have also been applied in code understanding. Since the original AST is actually a tree that is sparse, these works (Allamanis et al., 2018b; Wang et al., 2020; Wang and Li, 2021) first add edges to AST to make it more connected and then apply GNN models. Further, there are also works (Yu et al., 2020; Cummins et al., 2021; Liu et al., 2021) that utilize other intermediate graphs such as CFG, PDG and CPG (Yamaguchi et al., 2014) . Recently, approaches that use both code context and structure are proposed. For example, Hellendoorn et al. (2020) and Z\u00fcgner et al. (2021) incorporate the structure information derived from AST, such as edge weights and node distances, into the context attention computation in Transformer (Vaswani et al., 2017) . Despite the success, all these methods only consider the code context and structure information. There are also approaches that utilize the external knowledge associated with codes. For example, some methods apply pre-training techniques in NLP to boost comprehension, such as Code-BERT (Feng et al., 2020) , GPT-C (Svyatkovskiy et al., 2020) and PLBART (Ahmad et al., 2021) . There are also works that incorporate code characteristics into pre-training models, such as Graph-CodeBERT (Peng et al., 2021) , OSCAR (Peng et al., 2021) and InferCode (Bui et al., 2021) . Further, API is another external source for program understanding, which has been introduced in many works (Hu et al., 2018; Xu et al., 2020) . How-ever, all these methods ignore considering human behaviors in program understanding. Code Summarization and Code Clone Detection In this paper, we focus on two program understanding downstream tasks: code summarization and code clone detection. For code summarization, some works (Iyer et al., 2016; Ahmad et al., 2020) use code context only, some methods (LeClair et al., 2019; Alon et al., 2019a) use code structure only, while there are also models (Hellendoorn et al., 2020; Z\u00fcgner et al., 2021) that use both information. Further, Liu et al. (2021) introduce external knowledge for performance improvement. For code clone detection, existing works mainly employ code structure (Wei and Li, 2017; Zhang et al., 2019; Wang et al., 2020) and pre-training models (Feng et al., 2020; Ahmad et al., 2021) . S-AST Construction In this section, we construct S-AST. The original AST has two main limitations: \u2022 Low connectivity. The original AST is actually tree-structured, where every two nodes are minimally connected with only one path. This could lead to a long distance between leaf nodes. As pointed out in (Alon and Yahav, 2021) , directly applying GNN models in tree-shaped graphs could cause the long-range problem. \u2022 OOV problem. User-defined identifiers in codes can be arbitrarily complex and most of them are compound words, which could induce a large vocabulary size. For example, the training set size in the benchmark dataset CodeXGLUE (Lu et al., 2021) for code summarization is 164, 814, while the vocabulary size for AST nodes is 620, 256. After we split the nodes by camel case and underscores (Cvitkovic et al., 2019) , the vocabulary size is still as high as 201, 286. A very large vocabulary could cause the OOV problem (Jean et al., 2015) and thus adversely affect the model performance. To improve the connectivity of the AST, there exist some works (Allamanis et al., 2018b; Wang et al., 2020; Wang and Li, 2021) that add edges to the AST. However, these methods cannot address the OOV problem. Therefore, we propose a new code intermediate graph S-AST, as shown in Figure 1 . Similar as in (Allamanis et al., 2018b; Wang et al., 2020) , we add data flow edges to trace variable transitions and connect adjacent leaf nodes to encourage learning from contexts. To solve the OOV problem, we further reduce the vocabulary size by using the tokenizer of RoBERTa (Liu et al., 2019) to tokenize every leaf node in the AST. When a leaf node can be tokenized into multiple subtokens, we keep the first subtoken as the parent node and take other subtokens as its children. For example, the token \"getLarger\" is divided into the parent node \"get\" and the children nodes \"L\" and \"arger\". These new parent-children connections are defined as subtoken edges. With these three types of edges added, we increase the number of edges in the AST and improve the graph connectivity. Further, the vocabulary size could be significantly reduced. In our experiments, we use javalang 3 to generate Java AST and reduce the vocabulary size to 50, 336, where 50, 265 is the size of original RoBERTa vocabulary and 71 is the number of keywords in non-leaf nodes defined by javalang. Algorithm In this section, we introduce the PGNN-EK model, which is composed of two main components. On the one hand, the partitioning-based graph neural network model (PGNN) is proposed to follow the \"divide-and-conquer\" behaviours of humans to 3 https://github.com/c2nes/javalang understand programs. On the other hand, PGNN-EK leverages external knowledge to enhance the model's capability. The overall architecture of PGNN-EK is summarized in Figure 2 . Partitioning-based Graph Neural Networks As illustrated in (Schulte et al., 2010) and (Park et al., 2016) , the bottom-up reasoning on the hierarchical relationship of statements plays an essential role in human understanding. Therefore, we propose a statement-based partitioning algorithm to divide S-AST into multiple subgraphs. Since S-AST is no longer a tree, for convenience, we first keep subtokens and their edges in-between in S-AST, and remove edges linking variables and those connecting adjacent leaf nodes, to derive a tree structure. After that, we calculate the number of nodes in each subtree of the root node and each subtree corresponds to a statement of the raw code. Then, we accumulate the number of nodes in subtrees from left to right. When the sum exceeds the pre-defined threshold \u03bb, we group these subtrees into one subgraph and reset the sum to zero. If the current subgraph is not the first one, for each variable node in it, we also add to the subgraph the closest node indicating the same variable in previous subgraphs to trace the variable transition. After the subgraph is derived, we add edges between nodes that represent the same variable and also connect adjacent leaf nodes as in the original S-AST. We repeat this process until all subtrees are visited. Note that if the node number of the last subgraph is smaller than \u03bb/2, we merge the last subgraph into the penultimate subgraph. Finally, we summarize the pseudocodes of the partitioning algorithm in Alg. 1. After subgraphs are derived, as in (Hellendoorn et al., 2020), we adopt GGNN (Li et al., 2016) as the graph embedding model, which uses a multi-layer perceptron (MLP) and a gated recurrent unit (GRU) to perform message passing and embedding updating. Specifically, at the (l + 1)-th layer, to update the embedding h l+1 i of node x i , we have: m l+1 i = j\u2208N i MLP(h l j , e ij ), h l+1 i =GRU(m l+1 i , h l i ), where N i is the neighbor set of x i and e ij is the feature vector of the edge between x i and x j . After node embeddings are generated, we use a READ-OUT function to obtain the graph embedding G: G = READOUT({h i }). We repeat the above process on each subgraph to derive a list of subgraph embeddings L = [G 1 , G 2 , \u2022 \u2022 \u2022 , G n ], where n is the number of subgraphs. Next, we keep the order of the subgraph list and feed L into an unidirectional LSTM: O = LSTM(L). Inspired by the skip connection (He et al., 2016) , we also perform GGNN on the whole S-AST graph to derive a code embedding C. Finally, we concatenate C and the last output O[\u22121] of LSTM. We further feed the result into a fully connected layer to get the output code embedding E p : E p = FC(Concat(C, O[\u22121])). External Knowledge To help understand programs, people often resort to external knowledge. For example, humans usually learn from massive exemplary codes written by experts for better syntactic comprehension, which are in the format of programming language. Further, API documentation is written in natural language and provides semantic details on functions. Therefore, a research question arises: how to fuse these external syntactic and semantic knowledge into our model? To address the problem, we use pre-training techniques in programming language processing (PLP), which are trained on massive code corpus to learn programming basics. In particular, we adopt Code-BERT (Feng et al., 2020) , which is a bimodal pretrained model for both programming language and natural language. Before CodeBERT is applied, we first combine the raw code and API descriptions. To enrich the syntactic information contained in the raw code, we perform pre-order traversal on the AST of the code to obtain a sequence of tokens and replace the raw code. This is because the AST includes extra coderelated information, such as statements, variables and operations. Then we append the corresponding API description to the end. A toy example of transformation is shown in Figure 3 . Finally, we feed the transformed context T into the pre-trained CodeBERT 4 and obtain the embedding E e : E e = CodeBERT(T). Finally, we concatenate the output embeddings of PGNN and CodeBERT, and feed the result into a fully connected layer to obtain the final embedding E f : E f = FC(Concat(E p , E e )). Experiments In this section, we evaluate the performance of PGNN-EK. We conduct experiments on two program understanding tasks: code summarization and code clone detection. For each task, we use two benchmark datasets, whose statistics are listed in Table 1 . Implementation details In our experiments, we use the AdamW optimizer and linear schedule from (Wolf et al., 2020) to update model parameters. For fair comparison, we run all experiments on 2 Tesla V100 with 32G memory. For PGNN, we set the number of GNN layers, the number of LSTM layers, the embedding size of GNN node, and the embedding size of LSTM hidden layer to 3, 2, 768 and 768, respectively. We choose the mean operator as the READ-OUT function. To avoid overfitting, we set the dropout rate to 0.2 in PGNN. We implement GNNs In the code clone detection task, as by (Neculoiu et al., 2016) , we double the PGNN-EK to a siamese neural network to calculate code similarity. We set learning rate to 0.00005, batch size to 4, training steps to 200, 000 and maximum code length to 400, respectively. Code Summarization Code summarization aims at generating natural language comments for codes. We evaluate the performance of PGNN-EK on two benchmark datasets, which are TL-CodeSum (shorted as TLC) (Hu et al., 2018) and the Java subset of CodeSearchNet (shorted as CSN) (Husain et al., 2019) . For TLC, we use the original dataset. For CSN, we use the version provided by CodeXGLUE (Lu et al., 2021) . For fair comparison, we use the smoothed BLEU-4 score (Lin and Och, 2004) as in CodeXGLUE. The larger the score, the better the model performance. We compare our model with five representative baselines, including CodeNN (Iyer et al., 2016) , NCS (Ahmad et al., 2020) , Rencos (Zhang et al., 2020) , CodeBERT (Feng et al., 2020) and PLBART (Ahmad et al., 2021) . Due to the space limitation, we move the details of these baselines to Appendix C. Table 2 shows the code summarization results. Note that the results of CodeNN, NCS and Rencos are directly taken from (Shi et al., 2021) . Also, the results of CodeBERT and PLBART on CSN are 5 https://www.oracle.com/java/ technologies/javase-jdk8-doc-downloads. html derived from the leaderboard of CodeXGLUE. For their results on TLC, we run the codes released by the authors of the paper and set hyper-parameters according to the original paper. From the table, we see that, due to the fusion of external knowledge, pre-training models CodeBERT, PLBART and PGNN-EK outperform other models on both datasets. Further, PGNN-EK performs the best. The gaps between PGNN-EK and the runner-up model PLBART on CSN and TLC are 0.5 and 1.05, respectively. This shows the importance of considering human behaviors for code comprehension. We also observe that scores on TLC are substantially larger than that on CSN. This is because codes in the training set and the test set of TLC are considerably more similar in functionalities, which will be elaborated in the next section. Code Clone Detection The goal of code clone detection is to detect whether two code fragments implement the same functionality. Following (Zhang et al., 2019; Wang et al., 2020) , we use the BigCloneBench 2014 dataset (Svajlenko et al., 2014) and adopt the version provided by CodeXGLUE. We short it as BCB. Before we apply PGNN-EK on BCB, we notice from the leaderboard of CodeXGLUE that the results on BCB are incredibly high, where the mini-mum F1 score is 0.949. Then we dive into the characteristics of the dataset and compare BCB with the original benchmark (Svajlenko et al., 2014) . We find that the functionalities of codes in the test set have all appeared in the training set of BCB. Therefore, BCB is a very simple dataset. To test the model's generalization ability, we construct a new dataset, named BCB-F, where the test set contains codes whose functionality has never appeared in the training set. We first extract codes from the version benckmark (Svajlenko and Roy, 2015 ) that has more code fragments and code functionalities. We next split training/validation/test set based on code functionalities. Specifically, we construct training/validation/test set with 22/11/10 code functionalities. For details on the functionality splits of BCB and BCB-F, see Appendix D. We keep the same number of positive and negative samples in all the three sets. The comparison between BCB and BCB-F is given in Table 3 . In addition to the pre-training models Code-BERT and PLBART, we further compare our model with two representative methods in code clone detection, which are ASTNN (Zhang et al., 2019) and FA-AST (Wang et al., 2020) (For the details of these baselines, see Appendix C). Table 4 shows the evaluation results on the two datasets. For BCB, we take the results of other baseline methods from CodeXGLUE 6 . For BCB-F, we run the source codes released by their authors to obtain the results. From the table, we observe: 1) All models perform very well on BCB, indicating that the dataset is very simple. However, the best F1 score on BCB-F is only 0.724, which shows that this dataset is very challenging. 2) The non-pretraining models ASTNN and FA-AST predict all samples to be positive and perform poorly on BCB-F, while pre-training models perform better. This 6 Specifically, we take the results of ASTNN and FA-AST from https://github.com/ microsoft/CodeXGLUE/tree/main/Code-Code/ Clone-detection-BigCloneBench and that of CodeBERT and PLBART from the CodeXGLUE leaderboard. Note that PLBART only reports the F1 score on BCB. further demonstrates the importance of introducing external knowledge. 3) PGNN-EK achieves the best results on both datasets. This shows that considering human behaviors in program understanding enhances the generalization ability of PGNN-EK. Ablation Study We further conduct ablation study to verify the importance of its main components in PGNN-EK, including subtokens, the S-AST graph, the partitioning-based GNN and the external knowledge. Specifically, one variant employs only the S-AST graph without using external knowledge. This helps us realize the importance of external knowledge in program understanding. We call this variant PGNN only. Meanwhile, we define another variant that ignores the hierarchical relationships in code structure and uses only external knowledge. We call this variant EK only. To further show the significance of S-AST in code understanding, we replace S-AST with the original AST in the variant PGNN-EK with AST. We also implement a variant that does not use the subtoken tokenizer to generate extra subtoken nodes and edges. We call it PGNN-EK without subtoken. This variant can be used to show the importance of subtokens in addressing the OOV problem. To show the advantage of the partitioning strategy, we propose a variant GNN-EK that discards the partitioning step. Finally, we consider a variant that feeds the raw code into the pre-trained CodeBERT without transforming it with external knowledge. We call this variant PGNN-CodeBERT. Table 5 summarizes the ablation study results. From the table, we see that: 1) S-AST contains richer information than AST and can serve as an effective code intermediate representation in program understanding. The introduction of subtokens nodes and edges alleviates the OOV problem The Influence of Subgraph Size We end this section with a hyper-parameter sensitivity analysis. In PGNN-EK there is a key hyper-parameter \u03bb that is used to control the size of subgraphs. Here, we investigate the sensitivity of \u03bb. We vary the value of \u03bb from {10, 30, 50, 70, 90, 110, 130, 150, 170, 190} , and the final prediction results of PGNN-EK on 4 datasets are shown in the Figure 4 . The results indicate that 1) the model performance first increases and then drops, with the increase of the subgraph size. When the subgraph size is too small, each subgraph is a code fragment that no longer represents a code statement and thus contains less information. Further, when the subgraph is too large, each subgraph could be composed of statements that are of different semantic meanings, which thus degrades the model performance. 2) PGNN-EK performs the best at \u03bb = 30 on CSN and TLC while it achieves the best results at \u03bb = 70 on BCB and BCB-F. We further investigate the reason and show the average 2 0 4 0 6 0 8 0 1 0 0 1 2 0 1 4 0 1 6 0 1 8 0 2 0 0 0 . 9 0 0 . 9 1 0 . 9 2 0 . 9 3 0 . 9 4 0 . 9 5 0 . 9 6 0 . 9 7 0 . 9 8 0 . 9 9 number of nodes in S-AST on the four datasets in Table 6 . From the table, BCB and BCB-F contain \u223c 2.5 times more nodes than that in CSN and TLC. This empirically suggests that setting \u03bb to be about 1 5 to 1 4 of the average node number in S-AST could be a reasonable choice. 1 . 0 0 B C B F 1 \u03bb 0 2 0 4 0 6 0 8 0 1 0 0 1 2 0 1 4 0 1 6 0 1 8 Conclusion In this paper, we followed human understandings for programs and proposed the PGNN-EK model. To enrich the code structure information and alleviate the OOV problem, we presented the S-AST graph based on AST, which uses a subtoken tokenizer to generate subtoken nodes and edges between them. Inspired by the \"divide-and-conquer\" strategy, we proposed the partitioning-based graph neural network model on S-AST that employs code context and structure. To leverage the external knowledge to boost comprehension, we transformed the raw code to fuse syntactic and semantic knowledge and utilized pre-training techniques for information extraction. We performed extensive experiments to show the effectiveness of our model PGNN-EK on the code summarization and code clone detection tasks. In particular, to show the generalization ability of the model, we released a new benchmark that is more challenging.  A Partitioning S-AST Algorithm See Algorithm 1. B Examples of API-Description Pairs In the experiment. we obtain 51, 191 method description pairs after preprocessing, and Table 7 gives some examples. C Baselines Introduction We compare our model with five representative models in code summarization task: \u2022 CodeNN (Iyer et al., 2016) is the first method that applies deep neural networks in code summarization. It uses a classical attention-based encoder-decoder framework from Neural Machine Translation (NMT). \u2022 NCS (Ahmad et al., 2020) applies Transformer (Vaswani et al., 2017) to model the pairwise relationship between code tokens and capture their long-term dependencies. \u2022 Rencos (Zhang et al., 2020) proposes an attention-based encoder-decoder model and enhance it with the most similar code snippets retrieved from the training set. \u2022 CodeBERT (Feng et al., 2020 ) is a bimodal pre-training model for programming and natural languages based on RoBERTa (Liu et al., 2019) . \u2022 PLBART (Ahmad et al., 2021) is a sequenceto-sequence pre-training model based on BART (Lewis et al., 2020) . In addition to the pre-training models Code-BERT and PLBART, we further compare our model with two representative model in code clone detection task: \u2022 ASTNN (Zhang et al., 2019) proposes an ASTbased neural network that splits AST into a sequence of statement trees and applies a bidirectional RNN model to produce source code representation. However, it ignores external knowledge associated with codes. \u2022 FA-AST (Wang et al., 2020) \u2022 Val: Same to Train. \u2022 Test: Same to Train. For BCB-F, the functionalities in Train/Val/Test set are, where the emphasis discloses the whole 10 functionalities that exist in BCB:",
    "abstract": "Program understanding is a fundamental task in program language processing. Despite the success, existing works fail to take human behaviors as reference in understanding programs. In this paper, we consider human behaviors and propose the PGNN-EK model that consists of two main components. On the one hand, inspired by the \"divide-and-conquer\" reading behaviors of humans, we present a partitioningbased graph neural network model PGNN on the upgraded AST of codes. On the other hand, to characterize human behaviors of resorting to other resources to help code comprehension, we transform raw codes with external knowledge and apply pre-training techniques for information extraction. Finally, we combine the two embeddings generated from the two components to output code embeddings. We conduct extensive experiments to show the superior performance of PGNN-EK on the code summarization and code clone detection tasks. In particular, to show the generalization ability of our model, we release a new dataset that is more challenging for code clone detection and could advance the development of the community. Our codes and data are publicly available at https://github.com/ RecklessRonan/PGNN-EK.",
    "countries": [
        "China"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "A Neural Network Architecture for Program Understanding Inspired by Human Behaviors"
}