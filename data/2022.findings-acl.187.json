{
    "article": "The recent large-scale vision-language pretraining (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data-and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7\u00d7 fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks. Introduction Recent large-scale dual-stream Vision-Language Pre-training (VLP) models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) , have shown remarkable performance on various downstream multimodal alignment tasks, e.g., imagetext retrieval and image classification. These models are pre-trained using cross-modal contrastive learning on tremendous image-text pairs and learn strong multimodal representations. Despite their success, as mentioned by Radford et al. (2021) , their text encoder is relatively weak by only having a discriminative multimodal pre-training objective, ... ? Answer: A napkin. \ud835\udc41\ud835\udc3f\ud835\udc43 \ud835\udc38\ud835\udc5b\ud835\udc50 \ud835\udc41\ud835\udc3f\ud835\udc43 \ud835\udc37\ud835\udc52\ud835\udc50 \ud835\udc49\ud835\udc3f\ud835\udc43 \ud835\udc3c\ud835\udc5a\ud835\udc54 A picture of [mask] . A picture of a bouquet of white flowers in a glass vase. \ud835\udc41\ud835\udc3f\ud835\udc43 \ud835\udc38\ud835\udc5b\ud835\udc50 \ud835\udc41\ud835\udc3f\ud835\udc43 \ud835\udc37\ud835\udc52\ud835\udc50 \u00d7 \ud835\udc8e Visual Question Answering Image Captioning \u00d7 \ud835\udc8f Figure 1 : Intuition of our proposed approach. After VLKD, the model can fill in the masked locations with meaningful words to describe the image without further finetuning. Moreover, it can answer questions with proper reasoning over the given images and pre-trained knowledge inside PLMs, e.g., a napkin is for wiping the face at meals. which makes them incompetent on generative multimodal tasks such as image captioning and openended visual question answering (VQA). Meanwhile, the Transformer-based (Vaswani et al., 2017) auto-regressive large-scale pre-trained language models (PLMs), such as GPT (Radford and Narasimhan, 2018; Brown et al., 2020) , have been dominating in the natural language generation (NLG) tasks. These models are usually trained with causal self-attention, which only allows the model to attend to past outputs (unidirectional) to satisfy their generative nature. More recently, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) propose to augment the auto-regressive decoder with a bidirectional Transformer encoder to further capture bidirectional information of the input. These encoder-decoder architectures excel on not only NLG but also understanding (NLU) tasks. To tackle the aforementioned limitations of dualstream VLP models and fully utilize PLMs, in this paper, we present Vision-Language Knowledge Distillation (VLKD), a simple yet effective approach to enable CLIP to perform generative multimodal tasks through knowledge distillation. Specifically, we align the BART encoder to CLIP's joint multimodal embedding space to gain the understanding of multimodal knowledge, along with an image-conditioned language modeling loss to consort BART encoder and decoder. During training, we freeze CLIP's weights to keep its learned multimodal space. For the finetuning and inference of downstream tasks, the original CLIP text encoder is discarded, which can be interpreted as being replaced by the distilled BART. Therefore, we leverage the strengths from both sides, the expressive multimodal representation space of CLIP and the strong text generation capability of BART. Compared to VLP from scratch, VLKD uses several magnitudes fewer image-text pairs and computational resources. As depicted in Figure 1 , after VLKD pre-training, the model exhibits strong zero-shot performance on generative multimodal tasks, including open-ended VQA and image captioning. Without finetuning, it has the ability to generate answers by reasoning over the question, the visual information, and the textual knowledge embedded in the pre-trained BART. Furthermore, it can also directly generate a plausible caption given an image. Empirical results show that our model achieves 44.5% accuracy on the VQAv2 dataset and 84.6 CIDEr on COCO image caption dataset in a zero-shot manner. Moreover, the original NLU and NLG ability of BART is maintained, which makes the model versatile for both multimodal and unimodal tasks. To summarize, our contributions are: 1) We introduce an efficient approach to distill knowledge from the dual-stream VLP model CLIP to BART. The resulting model shows strong zero-shot performance on generative multimodal tasks, as well as pure NLP tasks; 2) We exhaustively quantify these capabilities on six benchmarks under various settings; and 3) We conduct comprehensive analysis and ablation study to provide insights and grease future work on this direction. Related Work Vision-language Pre-training Based on how the two modalities interact, recent VLP models mainly fall into two categories: singlestream and dual-stream models. Single-stream models (Chen et al., 2020; Li et al., 2019; Ramesh et al., 2021; Lin et al., 2021; Kim et al., 2021a; Shen et al., 2022) concatenate the patch-wise or regional visual features and textual embeddings and feed them into a single model. Dual-stream models (Lu et al., 2019; Radford et al., 2021; Jia et al., 2021; Zhai et al., 2021; Yao et al., 2022) use separate encoders for images and texts, allowing efficient inference for downstream multimodal alignment tasks like image-text retrieval, by pre-computing image/text features offline. However, these models can not be directly used for multimodal generation tasks. In this paper, we propose an efficient method to align the dual-stream VLP model CLIP's multimodal embedding space with a powerful PLM BART to gain multimodal generation ability. There are also VLP models that can perform multimodal generation tasks, by expensive pretraining with objective of image-conditioned autoregressive language modeling (Lin et al., 2021; Wang et al., 2021; Hu et al., 2021; Li et al., 2022) . However, the pre-training of these models requires a large number of image-text pairs and numerous computation resources. Other models like (Agrawal et al., 2019; Li et al., 2019 Li et al., , 2020;; Cho et al., 2021; Li et al., 2021) rely on an extra pretrained object detector such as Faster-RCNN with labeled bounding-box data to extract image regional features offline and are less scalable. Knowledge Distillation Knowledge distillation (KD) in deep learning is first proposed by Hinton et al. (2015) , which transfers knowledge embedded in the logits learned in a cumbersome teacher model to a smaller student model without sacrificing too much performance. Besides logits, other forms of knowledge like the intermediate representations and attentions (Jiao et al., 2019; Hou et al., 2020) have also been used in transferring the knowledge embedded in Transformer-based models. Recently, contrastive representation distillation (Tian et al., 2019) distills the knowledge from the teacher network to the student network by maximizing the mutual information between the two networks, and is recently extended to transfer the knowledge from the pre-trained multimodal model CLIP for zero-shot detection (Gu et al., 2021) and multilingual setting (Jain et al., 2021) . In this paper, we apply the conventional KD as well as the contrastive KD to transfer the knowledge from the pre-trained CLIP to BART. Besides, we also propose to transfer the knowledge in CLIP image encoder to BART decoder through the cross-attention. Proposed Method We propose to distill multimodal knowledge from CLIP to BART for generative multimodal tasks, which takes the strengths from both sides (powerful multimodal representations of CLIP and text generation ability of BART). To this end, we propose three objectives (Section 3.2). The overall architecture is illustrated in Figure 2 . 3.1 Model Architecture CLIP. CLIP (Radford et al., 2021) is a dualstream VLP model pre-trained with a contrastive loss on 400 million image-text pairs. It consists of a text encoder which is a GPT (Radford et al., 2019) style Transformer model, and an image encoder which can be either a Vision Transformer (ViT) (Dosovitskiy et al., 2020) or Residual Convolutional Neural Network (ResNet) (He et al., 2016) . CLIP learns a joint multimodal embedding space with its text encoder and image encoder aligned. Given an input image-text pair, the image encoder first reshapes the image into a sequence of 2D patches and then maps them into 1D embeddings with a prepended [CLS] token using a trainable linear projection. These embeddings are fed into the CLIP image encoder together with positional encodings. The output embedding of the [CLS] token can represent the whole image. For the text sentence, it is bracketed with [SOS] and [EOS] tokens, and the output embedding of the latter is used as the sentence-level representation. In this paper, we explore four CLIP variants, including ViT-B/16, ViT-L/14, RN50\u00d716, and RN50\u00d764. BART. BART is a Transformer-based (Vaswani et al., 2017) sequence-to-sequence model that has a bi-directional encoder and a uni-directional (leftto-right) decoder, which can be seen as a generalization of the BERT (Devlin et al., 2019) and GPT (Radford and Narasimhan, 2018) . It is pretrained on 160GB text data in a self-supervised way by performing the text span infilling task with the input sentences corrupted and shuffled. Similar to the CLIP text encoder, BART also tokenizes and converts the input text into a sequence of embeddings, which are then fed into the BART encoder. BART excels at both NLG (e.g., abstractive summarization) and NLU tasks. Training Objectives To distill multimodal knowledge from CLIP to BART, we propose three objective functions: 1) Text-Text Distance Minimization (TTDM); 2) Image-Text Contrastive Learning (ITCL); and 3) Image-Conditioned Text Infilling (ICTI). During training, the model parameters of CLIP are frozen constantly, i.e. no gradients will be backpropagated through them (marked as SG in Figure 2 ), to ensure its two encoders are still aligned and the multimodal knowledge is not forgotten. For each training batch with B image-text pairs, denote the k-th image-text pair as x k = {x k I , x k T }, and the output of multimodal encoders of CLIP and BART encoder as CLIP I (x k I ) \u2192 V k = [v k cls , v k 1 , . . . , v k n 1 ], CLIP T (x k T ) \u2192 T k = [t k sos , t k 1 , . . . , t k n 2 , t k eos ], BART enc (x k T ) \u2192 E k = [e k bos , e k 1 , . . . , e k n 3 , e k eos ]. Here, n 1 is the number of image patches, n 2 and n 3 denote the sequence lengths of the text encoder of CLIP and BART, respectively. v k * , t k * \u2208 R d 1 represents the \u2113 2 -normalized output embedding from the CLIP image and text encoder at a certain position. e k * is the unnormalized raw output embedding from the BART encoder. In the following, we elaborate on the three distillation objectives. Text-Text Distance Minimization To align the CLIP text encoder and BART encoder, i.e. making their output representations close given the same input text, we propose to minimize the \u2113 2 distance between their sequence-level output representations. Specifically, for the k-th input text, it can be formulated as \u0113k norm = W e \u0113k /\u2225W e \u0113k \u2225 2 , L T T DM = 1 B B k=1 \u2225t k eos \u2212 \u0113k norm \u2225 2 , where \u0113k \u2208 R d 2 is the average of all output embeddings from the BART encoder, and W e \u2208 R d 1 \u00d7d 2 is a weight matrix to linearly project the output of BART encoder to CLIP's multimodal space. \ud835\udc70\ud835\udc6a\ud835\udc7b\ud835\udc70 \ud835\udc73\ud835\udc90\ud835\udc94\ud835\udc94 SG SG SG A large airplane is on the airport runaway \u2113 2 \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a \u2113 2 \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a \u2113 2 \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a \ud835\udc5b 1 \ud835\udc5b 1 (a) The TTDM and ITCL losses. \ud835\udc70\ud835\udc6a\ud835\udc7b\ud835\udc70 \ud835\udc73\ud835\udc90\ud835\udc94\ud835\udc94 SG SG SG A large airplane is on the airport runaway \u2113 2 \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a \u2113 2 \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a \u2113 2 \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a \ud835\udc5b 1 \ud835\udc5b 1 (b) The ICTI loss. Image-Text Contrastive Learning Contrastive training has been shown to be very effective in cross-modal representation learning (Tian et al., 2020; Sigurdsson et al., 2020; Zhang et al., 2020; Radford et al., 2021) . To further adapt the BART encoder to CLIP's multimodal space, we optimize a symmetric InfoNCE loss between the output representations of the BART encoder and CLIP image encoder. The image-to-text contrastive loss L i2t is formulated as L i2t = \u2212 1 B B k=1 log exp v k\u22a4 cls \u0113k norm /\u03c4 j exp \u00c4 v k\u22a4 cls \u0113j norm /\u03c4 \u00e4 , where \u03c4 is a learnable temperature parameter. Different from Radford et al. (2021) , we find that not clamping the \u03c4 shows a slight improvement. Similarly, the text-to-image contrastive loss L t2i is L t2i = \u2212 1 B B k=1 log exp v k\u22a4 cls \u0113k norm /\u03c4 j exp \u00c4 v j\u22a4 cls \u0113k norm /\u03c4 \u00e4 . Then, the ITCL loss can be calculated as L IT CL = 1 2 (L i2t + L t2i ). Note that when computing the ITCL and TTDM losses, we do not introduce any new linear projections to the CLIP output features to avoid destroying the pre-trained alignment between its image and text encoders. Instead, we add one linear layer (parameterized by W e ) to project the BART encoder to CLIP's representation space and match their feature dimension. Image-Conditioned Text Infilling With only TTDM and ITCL, the BART decoder is not updated at all. To consort BART encoder and decoder, we propose to perform the text span infilling task conditioned on the corresponding image features. As depicted in Figure 2b , for the k-th image-text pair, following Lewis et al. (2020) , we corrupt the input text by masking 15% of wholeword tokens with span lengths drawn from a Poisson Distribution with \u03bb = 3. Considering that V k and W e E k are already aligned in the CLIP's multimodal space through TTDM and ITCL, and having a different feature dimension with the BART decoder, we further project them to the BART decoder dimension with W i and W \u2032 e . Then, we concatenate them together as C k before feeding into the BART decoder as shown in Eq.( 1 ). As mentioned in Section 3.1, we explore two variants of CLIP. With a slight abuse of notation, for ResNet-based CLIP, V k is composed of representations of all image patches {v k i } n 1 i=1 , while for ViT-based CLIP, V k consists of the representation of the [CLS] token v k cls only. Note that the weight matrix W \u2032 e is initialized to be the pseudo-inverse of W e , such that text representations after the two projections W \u2032 e W e E k are the closest to the original pre-trained BART encoder space at initialization 1 . The BART decoder then interacts with C k through standard Transformer cross-attention layers. We optimize a lan- 1 The pseudo inverse matrix W \u2032 e satisfies W \u2032 e = arg min X \u2225WeX \u2212 I\u2225 2 F , where I is the identity matrix and \u2225 \u2022 \u2225F denotes the Frobenius Norm. guage modeling loss L ICT I by minimizing the negative log-likelihood in Eq.( 2 ), in which w j denotes the token to be predicted at each decoding step. C k = concat(W i V k , W \u2032 e W e E k ), (1) L ICT I = \u2212 1 B B k=1 j log P (w k j |w k <j , C k ). (2) The ICTI loss is crutial for for our methodology to work, as it not only coordinates the BART encoder and decoder, but also enables the BART decoder to understand the multimodal information by recovering texts with visual clues. Finally, we simultaneously optimize the summation of three losses L as L = \u03b3L T T DM + L IT CL + L ICT I , where \u03b3 is set to 10 3 by default, as L IT CL , L ICT I are about three magnitudes larger than L T T DM . Datasets for VLKD Our model is trained on the Conceptual Captions (CC3M) (Sharma et al., 2018) dataset, which contains 3 million image-text pairs crawled from the Internet. For larger model variants (ViT-L/14 and RN50x64), we further include the Visual Genome Caption data which contains \u223c700K image-text pairs. No images for pre-training appear in the downstream datasets. Compared to previous VLP work (Radford et al., 2021; Jia et al., 2021; Wang et al., 2021) , VLKD is much cheaper by leveraging several magnitudes less data. Furthermore, we experiment with even smaller data (1M, 100K) by uniformly sampling a subset of CC3M to test the limit of dataset size of VLKD, with results discussed in Section 5. Experiments To demonstrate the effectiveness of VLKD, we evaluate it on generative multimodal tasks for both zero-shot and finetuning. Specifically, we test the image captioning task, and also the VQA task under the open-ended scenario. Furthermore, we also run the model on NLU and NLG tasks to investigate the influence of VLKD on the text processing ability of the original pre-trained BART. Finetuning Datasets Image Captioning. Image captioning requires the model to generate a relevant description given an image. We use the COCO image caption dataset (Lin et al., 2014) with the Karpathy split (Karpathy and Fei-Fei, 2017) . Additionally, we use the NoCaps (Agrawal et al., 2019) dataset to test the model performance when there are outof-domain objects. Open-Ended VQA. Unlike previous works (Anderson et al., 2018; Chen et al., 2020; Li et al., 2020; Yu et al., 2021a; Zhang et al., 2021; Kim et al., 2021b) that treat the VQA task as a discriminative problem, we let the model generate answers freely, which is more aligned with the real-world scenario of this task. We use the standard VQAv2 (Goyal et al., 2017) , and also OK-VQA (Marino et al., 2019) which requires knowledge to answer questions correctly. NLU and NLG. For NLU, we test our model on the GLUE benchmark (Wang et al., 2019) , which consists of nine text classification tasks. We exclude the WNLI task as it is problematic 2 . For NLG, we test the abstractive summarization task on XSUM (Narayan et al., 2018) dataset, which requires the model to comprehend long texts and generate short summaries with key information. Implementation Details We use BART-large as the pre-trained backbone NLP model, which has 12 layers in both encoder and decoder with a hidden size of 1024 and 16 heads in each multi-head attention (MHA) layer. In total, it contains 406M parameters. For the pre-trained CLIP (Radford et al., 2021 ) model, we report four variants with different visual backbones, including ViT-B/16, ViT-L/14, RN50\u00d716, and RN50\u00d764. We use 64 Nvidia V100 GPUs for VLKD and 8 for the finetuning of downstream tasks. In total, we pre-train the model for 10 epochs, which takes about 5 hours. We use a batch size of 4608 for ViT-B/16 and ViT-L/14, 4096 for RN50x16 and 3840 for RN50x64. All of the models are optimized by the AdamW (Loshchilov and Hutter, 2019) optimizer. The learning rate is warmed up to 2.4e \u22124 within the first 2% steps and then linearly decay to 0. More information of VLKD pre-training and the finetuning of each downstream task can be found in Appendix A. Reference caption: Two people sit on the beach with surfboards at their sides. Generated caption: A couple sitting on the beach with their surfboards in the background. Reference caption: A cat is laying next to a blue book. Generated caption: A cat reading a book on a couch in the li ving room. Reference caption: A woman sitting on a bench with a dog. Generated caption: A young woman sitting on a bench with her dog in the background. What's reflecting from the mirror? Candidate answer(s): Light; Wall; Shower. Generated answer: Light. Reference caption: A man holds a stick during a hockey game. Generated caption: A young man in the middle of a hockey game. Reference caption: Two people sit on the beach with surfboards at their sides. Generated caption: A couple sitting on the beach with their surfboards in the background. Reference caption: A cat is laying next to a blue book. Generated caption: A cat reading a book on a couch in the li ving room. Reference caption: A woman sitting on a bench with a dog. Generated caption: A young woman sitting on a bench with her dog in the background. Multimodal Zero-Shot Evaluation Benefit from the knowledge distillation, especially the ICTI loss, our model can perform various downstream multimodal tasks in a zero-shot manner. Zero-Shot Image Captioning During knowledge distillation, the ICTI loss can be seen as a simple version of the image captioning task, which asks the model to fill in the corrupted locations of image descriptions. If the masking ratio increases to 100%, it reduces to the image captioning task. Therefore, it is intuitive to test the zero-shot performance of our model. Following Radford et al. (2021) and Wang et al. (2021) , we compose the input with a text prompt and also m mask tokens, i.e., \"A picture of [MASK]\u00d7m.\", for the model to generate the caption for the image. The zero-shot results are included in Table 1 . Our zero-shot model achieves comparable overall performance to the finetuned UpDown (Agrawal et al., 2019) model on NoCaps dataset. As shown in Figure 3b , the zero-shot generated captions are plausible with correct objects, relationships, and actions. However, sometimes details like colors could be omitted. In our experiments, we use m = 6 for COCO and m = 8 for NoCaps. Although it could poten-tially limit the length of generation, we find that it has negligible influence to the performance, as for each [MASK] token, the model is learned to fill one to three tokens depending on the context. Furthermore, this could be used to control the length of generated texts for different senarios. See Section 5 for a more detailed discussion about the effects of number of the masks. Zero-Shot VQA Zero-shot VQA is much more challenging than image captioning, as it requires reasoning over both the image and question, which is very different from the ICTI loss during the knowledge distillation. As illustrated in Figure 1 , we construct the input by appending a text prompt \"Answer: [MASK]\u00d7n.\" to the question Given the context (image+question+prompt), the model is required to predict the answer by recovering the textual token in the [MASK] positions. In our experiments, we use n = 2 for the VQAv2, which is found performing best among n \u2208 {1, 2, 3}. In Table 2 , compared to the strong baseline Frozen (Tsimpoukelli et al., 2021) (Anderson et al., 2018; Li et al., 2020; Zhang et al., 2021; Cho et al., 2021; Hu et al., 2021; Wang et al., 2021) . Models marked by \u2020 additionally use the constrained beam search (CBS) (Anderson et al., 2017) questions. For example, it connects the visual object Turkey with the traditional food people usually eat at the Thanksgiving festival. Multimodal Finetuning Evaluation When finetuning VLKD on downstream multimodal tasks, we keep the same input format as zero-shot to obtain outputs in a generative way. The CLIP model parameters are still frozen during finetuning. Finetuning Image Captioning In Table 1 , we demonstrate that our model can achieve decent performance when finetuned on the COCO dataset. The SCST CIDEr optimization method (Rennie et al., 2017) is used to further improve the performance. Our model outperforms VL-T5/BART (Cho et al., 2021) without using an extra object detector, which is fairly timeconsuming as explained by Kim et al. (2021b) . (Lewis et al., 2020) , \u2020 are from (Iki and Aizawa, 2021) , and \u2021 are from (Wang et al., 2021) . Compared to other VLP models, our VLKD model has a great advantage in text-only NLP tasks. there is still a small performance gap, which we conjecture is mainly due to their usage of object detector/tags and much more pre-training image-text pairs. We also evaluate our VLKD models with ResNet visual backbones on the NoCaps dataset (Table 1 ). For zero-shot image caption, the CIDEr score on the out-of-domain set is even higher than the in-and near-domain sets, which shows the generalization of our knowledge distillation method to common visual objects. After finetuned on the COCO training set, the performance on NoCaps of our model with the RN50\u00d764 backbone is comparable to the state-of-the-art models. Finetuning VQA From Table 2 , the best performance of VQAv2 is achieved by VLP models that tackle this task in a discriminative way with a set of pre-defined answers. However, this approach does not generalize to real-world scenarios and cannot be directly applied to more diverse datasets (e.g., OK-VQA). Differently, Frozen (Tsimpoukelli et al., 2021) and our proposed VLKD formulate VQA as a generative problem to generate answers conditioned on the questions and images in an open-ended manner, which also enables zero-shot VQA. Specifically, for each question-answer pair in the VQAv2 dataset, we optimize the model to generate the answer with the cross-entropy loss and a label-smoothing of 0.1. The loss is weighted by the weight of each answer candidate. In addition, we augment the training data with VG-QA (Krishna et al., 2016) . Furthermore, following (Cho et al., 2021) , we test the performance on out-of-domain questions with rare answers using the Karpathy test-split. As shown in Table 3 , our method shows a salient advantage on out-of-domain questions due to the benefit from VLKD and its generative nature without defining the answer list. Evaluation of NLU and NLG Table 4 shows results on the GLUE benchmark. Although prior VLP models are either initialized from the pre-trained BERT model, or trained by a text-only language modeling loss together with the vision-language (VL) losses, they generally suffer from the weakened performance of NLU. For example, SIMVLM performs significantly worse than BART, though trained with five times more textual data. We speculate that the weakened NLU ability of these models is caused by the catastrophic forgetting of the pre-trained BERT weights during the multimodal pre-training. Moreover, simultaneous optimization of multimodal and text-only objectives potentially shifts the latter to be an auxiliary loss, making the NLP ability not as effective. On the other hand, the resulting model of VLKD performs only slightly worse than the original BART and significantly outperforms BERT, as the original knowledge embedded in BART is well maintained. Additionally, as presented in Table 5 , we also run VLKD on the abstractive summarization task to evaluate its NLG performance, since BART-based methods excel on the summarization (Lewis et al., 2020; Dou et al., 2021; Yu et al., 2021b) . The gap between VLKD and its backbone BART is negligible. Overall, we empirically demonstrate that VLKD enables the backbone PLM to perform multimodal tasks without hurting its original NLP ability. Ablation Study Knowledge Distillation Objectives. Table 6 shows the ablation on the knowledge distillation objectives, except the ICTI loss which is necessary for our method to work. Without TTDM or ITCL, we observe a clear degradation of zero-shot performance on both VQAv2 and COCO image caption datasets. It is worth noting that ITCL contributes more to the image captioning task, which requires a deeper perception of visual features to generate captions. Oppositely, TTDM helps more for the VQA task, which involves reasoning over the question and image features. Removing both of them incurs a large performance drop, which demonstrates the importance of aligning the embedding space between CLIP and BART. We speculate that unfreezing CLIP harms its pretrained multimodal space, which further downgrades the performance of VLKD. Conclusion Recent dual-stream VLP models (e.g., CLIP) are powerful in various multimodal classification and retrieval tasks. However, their ability of multimodal generation or pure NLP tasks is highly restricted. In this paper, we propose a novel knowledge distillation method to efficiently align CLIP's multimodal encoders and BART's textual encoder to the same mutlimodal space, as well as a crossmodal LM loss to consort BART encoder and decoder. This enables multimodal generation under zero-shot and also fully-finetuned settings without losing the original BART's NLP ability. Empirical results show that our model achieves new stateof-the-art zero-shot performance on VQA and excellent performance on both NLP and multimodal tasks when finetuned, demonstrating the effectiveness of our proposed method. A Hyper-parameters In this section, we show the hyper-parameters of vision-language knowledge distillation (VLKD), as well as downstream task finetuning. For VLKD, the hyper-parameters are shown in Table 9 , for both two CLIP variants we explored. For finetuning multimodal downstream tasks, we use the hyper-parameters shown in Table 10. Within each task, we use the same setting for multiple datasets. For the GLUE benchmark, we use the LAMB optimizer (You et al., 2020) to train for 10 epochs. We conduct a hyper-parameter grid search with batch size={16, 32, 64}, lr={1e-4, 5e-4, 1e-3}, weight decay={1e-4, 1e-3}. We warm up the learning rate in the first epoch, then linearly decay it to zero. For XSUM, we directly follow the hyperparameters used in Lewis et al. (2020) . B More Examples of Zero-shot Inference In Figure 4 , we show more examples of zero-shot image captioning. In Figure 5 , we depict more cases of the results of zero-shot open-ended VQA. Reference caption: A big cat laying down in a chair on a porch. Generated caption: A cat lounging on a chair in a hammock. Reference caption: A little girl holding up a pink umbrella. Generated caption: A girl holding a pink umbrella in the rain. Reference caption: A white boat out in the middle of the ocean. Generated caption: A small fishing boat in the middle of the ocean. Reference caption: A small herd of elephants standing in the grass. Generated caption: A herd of elephants in a field of grasses.",
    "abstract": "The recent large-scale vision-language pretraining (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data-and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7\u00d7 fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
    "countries": [
        "Hong Kong"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "4",
    "year": "2022",
    "month": "May",
    "title": "Enabling Multimodal Generation on {CLIP} via Vision-Language Knowledge Distillation"
}