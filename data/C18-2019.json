{
    "article": "We present Appraise, an open-source framework for crowd-based annotation tasks, notably for evaluation of machine translation (MT) output. This is the software used to run the yearly evaluation campaigns for shared tasks at the WMT Conference on Machine Translation. It has also been used at IWSLT 2017 and, recently, to measure human parity for machine translation for Chinese to English news text. The demo will present the full end-to-end life cycle of an Appraise evaluation campaign, from task creation to annotation and interpretation of results. Motivation Human evaluation of machine translation is the ultimate measure of translation quality. However, due to data collection effort and annotation cost, many experiments and publications do not report results from human evaluation and rely on scores computed by automated metrics such as BLEU (Papineni et al., 2002) instead. We believe that machine translation researchers should be able to conduct manual annotation campaigns at scale, without having to re-implement the necessary infrastructure from scratch. Since 2007, development of the Appraise evaluation framework for machine translation has supported the research community, trying to bring more human evaluation into MT research. Introduction The Appraise framework has become a standard tool for machine translation evaluation. It is used for shared tasks at the yearly Conference on Machine Translation (WMT) (Bojar et al., 2017) and has been adopted at last year's IWSLT 2017 workshop (Cettolo et al., 2017) . The Microsoft Translator team utilises the software for its internal quality monitoring. Figure 1 shows the annotation user interface.  In 2018, Appraise was used as part of a research project which proved human parity for machine translation of Chinese to English news text (Awadalla et al., 2018) , based on a large-scale evaluation campaign run using an Appraise system hosted on Azure. Figure 2 shows a graph visualising results from this work, comparing score distributions for the human parity system (COMBO-6) and the original WMT17 reference translation (WMT). Our system demonstration provides an end-to-end overview on all aspects of a machine translation evaluation campaign within Appraise. We first describe input data, task creation, and campaign setup, including best practices regarding user and team management. Then, we show the annotation interface and discuss how annotator reliability is measured and monitored, allowing to detect spammers assigning random scores to candidate translations. We describe how statistical significance testing (Wilcoxon, 1945; Mann and Whitney, 1947; Riezler and Maxwell, 2005) can help to solve this problem. Lastly, we explain how final campaign results can be computed, extracted and visualised effectively, so that results are easily interpretable. We also describe the annotation system's Python-based architecture and highlight implementation details as well as lessons learnt during ten years of human evaluation campaigns based on Appraise. License Appraise source code is available on GitHub 1 and is shared under a permissive license 2 . Conclusion Our system demonstration explains the full end-to-end life cycle of an Appraise evaluation campaign. It gives an in-depth look into a decade of research on machine translation evaluation, including in-sights from several WMT campaigns as well as the evaluation part of Microsoft's recent human parity research breakthrough. This should lead to interesting discussions. Acknowledgements The author thanks the anonymous reviewers for their feedback. Appraise is developed incorporating feedback from the research community, participants of WMT and IWSLT shared tasks, and Microsoft. This support is crucial for the evolution of Appraise and, thus, much appreciated. Thank you!",
    "funding": {
        "defense": 0.0,
        "corporate": 5.7961679021945045e-06,
        "research agency": 0.0,
        "foundation": 9.088342269869543e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not explicitly mention any funding sources, including defense, corporate, research agencies, or foundations. It focuses on the development and application of the Appraise framework without attributing its support to any specific external funding entities. Therefore, based on the information provided, it appears there were no disclosed funding sources.",
    "abstract": "We present Appraise, an open-source framework for crowd-based annotation tasks, notably for evaluation of machine translation (MT) output. This is the software used to run the yearly evaluation campaigns for shared tasks at the WMT Conference on Machine Translation. It has also been used at IWSLT 2017 and, recently, to measure human parity for machine translation for Chinese to English news text. The demo will present the full end-to-end life cycle of an Appraise evaluation campaign, from task creation to annotation and interpretation of results. Motivation Human evaluation of machine translation is the ultimate measure of translation quality. However, due to data collection effort and annotation cost, many experiments and publications do not report results from human evaluation and rely on scores computed by automated metrics such as BLEU (Papineni et al., 2002) instead. We believe that machine translation researchers should be able to conduct manual annotation campaigns at scale, without having to re-implement the necessary infrastructure from scratch. Since 2007, development of the Appraise evaluation framework for machine translation has supported the research community, trying to bring more human evaluation into MT research.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": 15,
    "year": 2018,
    "month": "August",
    "title": "Appraise Evaluation Framework for Machine Translation",
    "values": {
        "ease of implementation": "We believe that machine translation researchers should be able to conduct manual annotation campaigns at scale, without having to re-implement the necessary infrastructure from scratch. Since 2007, development of the Appraise evaluation framework for machine translation has supported the research community, trying to bring more human evaluation into MT research.",
        "reproducibility": "We believe that machine translation researchers should be able to conduct manual annotation campaigns at scale, without having to re-implement the necessary infrastructure from scratch. Since 2007, development of the Appraise evaluation framework for machine translation has supported the research community, trying to bring more human evaluation into MT research. This should lead to interesting discussions."
    }
}