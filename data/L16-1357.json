{
    "article": "In this paper, we present a freely available corpus of human and automatic translations of subtitles. The corpus comprises the original English subtitles (SRC), both human (HT) and machine translations (MT) into German, as well as post-editions (PE) of the MT output. HT and MT are annotated with errors. Moreover, human evaluation is included in HT, MT, and PE. Such a corpus is a valuable resource for both human and machine translation communities, enabling the direct comparison -in terms of errors and evaluation -between human and machine translations and post-edited machine translations. Introduction This paper describes a freely available corpus 1 consisting of original English subtitles (SRC) translated into German. The translations are produced by human translators and by SUMAT (Volk, 2008; M\u00fcller and Volk, 2013; Etchegoyhen et al., 2014 ), a MT system developed and trained specifically for the translation of subtitles. Both human (HT) and machine translations (MT) are annotated with translation errors (HT ERROR, MT ERROR) as well as evaluation scores at subtitle level (HT EVAL, MT EVAL). Additionally, machine translations are completed with post-editions (PE) which are also evaluated (PE EVAL). The corpus was compiled as part of a course on subtitling targeted at students enrolled in the BA Translation Studies programme at Saarland University. The students carried out the human translations and the error and evaluation annotation. Though novice translators, this kind of evaluation is already an improvement, since human evaluation of MT datasets is often carried out by lay translators (computational linguists or computer scientists). To the best of our knowledge, this is the first attempt to apply human error annotation and evaluation to subtitles produced by humans and a MT system, taking into account the distinctive features of subtitling such as temporal and spatial constrains. Such a resource can be useful for both human and machine translation communities. For (human) translation scholars a corpus of translated subtitles can be very helpful from a pedagogical point of view. In the field of MT, the possibility of comparing between human and machine translations, especially using manual metrics like error analysis and evaluation scores, can enhance MT development, in particular, applied to the specific task of subtitling. Subtitling and Quality Assessment in Human and Machine Translation Subtitling implies taking into consideration that translated text has to be displayed synchronously with the image. This means that certain constrains on space (number of lines on 1 http://hdl.handle.net/11858/ 00-246C-0000-0023-8D18-7 screen, number of characters per line) and time (synchrony, time displayed on screen) have to be respected when performing subtitling. According to D\u00edaz Cintas and Remael (2007) subtitling is defined as a translation practice that consists of presenting a written text, generally on the lower part of the screen, that endeavours to recount the original dialogue of the speakers as well as the discursive elements that appear in the image (letters, inserts, graffiti, inscriptions, placards, and the like), and the information that is contained on the soundtrack (songs, voices off). These restrictions have an impact in the translation leading to text reduction phenomena. According to D\u00edaz Cintas and Remael ( 2007 ) the text reduction for subtitling can be either: (i) partial (implying condensation and reformulation of the information) or (ii) total (implying omission of information). Subtitling and Machine Translation MT systems developed specifically for the translation of subtitles have to take into account these aspects. The improvement of MT technology in the last years has lead to its successful deployment by increasing the speed and amount of text translated. MT has also been applied to subtitling taking into account its distinctive features (Volk, 2008; M\u00fcller and Volk, 2013; Etchegoyhen et al., 2014) . Nevertheless, the usage of MT to translate subtitles still implies post-editing the MT output, a process which has been proven to be faster and more productive than translating from scratch (Guerberof, 2009; Zampieri and Vela, 2014) . Quality Assessment in Human Translation Manual evaluation and analysis of translation quality (HT and MT) has proven to be a challenging and demanding task. According to Waddington (2001) , there are two main approaches to human evaluation in translation: a) analytic, and b) holistic. Analytic approaches (Vela et al., 2014a; Vela et al., 2014b) tend to focus on the description of the translational phenomena observed following a given error taxonomy and, sometimes, by considering the impact of the errors. Holistic approaches (House, 1981; Halliday, 2001) tend to focus on how the translation as a whole is perceived according to a set of criteria established in advance. Pioneering proposals (House, 1981; Larose, 1987) focus on the errors made in combination with linguistic analysis at textual level (Vela and Hansen-Schirra, 2006; Hansen-Schirra et al., 2006; Vela et al., 2007; Hansen-Schirra et al., 2012; Lapshinova-Koltunski and Vela, 2015) . Williams (1989) goes a step further proposing a combined method to measure the quality of human translations by taking into consideration the severity of the errors. According to him, there are two types of errors: \u2022 major, which is likely to result in failure in the communication, or to reduce the usability of the translation for its intended purpose \u2022 minor, which is not likely to reduce the usability of the translation for its intended purpose; although, it is a departure from established standards having little bearing on the effective use of the translation Depending on the number and impact of the errors, he proposes a four-tier scale to holistically evaluate human translations: (i) superior: no error at all, no modifications needed; (ii) fully acceptable: some minor errors, but no major error, it can be used without modifications; (iii) revisable: one major error or several minor ones, requiring a cost-effective revision; and (iv) unacceptable: the amount of revision to fix the translation is not worth the effort, re-translation is required Quality Assessment in Machine Translation The evaluation of machine translation is usually performed by lexical-based automatic metrics such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) . Evaluation metrics such as Meteor (Denkowski and Lavie, 2014) , Asiya (Gonz\u00e0lez et al., 2014) , and VERTa (Comelles and Atserias, 2014) , incorporate lexical, syntactic and semantic information into their scores. More recent evaluation methods are using machine learning approaches (Stanojevi\u0107 and Sima'an, 2014; Gupta et al., 2015; Vela and Tan, 2015; Vela and Lapshinova-Koltunski, 2015) to determine the quality of machine translation. The automatically produced scores have been correlated with human evaluation judgements, usually carried out by ranking the output of the MT system (Bojar et al., 2015; Vela and van Genabith, 2015) or by performing post-editing (Gupta et al., 2015; Scarton et al., 2015; Zampieri and Vela, 2014) The evaluation of machine translated subtitles was performed by Etchegoyhen et al. (2014) on the SUMAT output, by letting professional subtitlers post-edit and rate the post-editing effort. Moreover, the same authors measure the MT output quality by running BLEU on the MT output and hBLEU by taking the post-edited MT output as reference translation, showing that the metrics correlate with human ratings. Learner Corpora Learner corpora annotated with errors have been built before, either by collecting and annotating translations of trainee translators as described by Castagnoli et al. (2006) or by training professional translators as specified by Lommel et al. (2013) . Castagnoli et al. (2006) collected human translations of trainee translators and got them annotated by translation lecturers, based on a previously established error annotation scheme. The goal was mainly pedagogical: to learn from translation errors made by students enrolled in European translation studies programmes. A learner corpus similar to ours has been reported by Wisniewski et al. (2014) . Their corpus contains English to French machine translations which were post-edited by students enrolled in a Master's programme in specialised translation. Parts of the MT output have undergone error annotation based on an error typology including lexical, morphological, syntactic, semantic, and format errors, and errors that could not be explained. Building the Corpus In this section, we describe the corpus: the participants, the materials, the annotation schema, and the tasks carried out to compile it. Participants The participants of the experiments were undergraduate students enrolled in the BA Translation Studies programme at Saarland University. The subtitling corpus is the output of a series of tasks fulfilled during a subtitling course. Metadata about the participants were collected documenting: command of source (English) and target (German) languages, knowledge of other languages, professional experience as translator, and other demographic information such as nationality, gender and birth year. We collected metadata from 50 students who carried out 52 English to German translations. Materials The source text used for this experiment is the documentary film Joining the Dots by Pablo Romero Fresco (Romero Fresco, 2013) . A master template of the original subtitles in English was provided by the film maker. Therefore, no spotting was required. Using the same master subtitle template for all translators eased comparability of subtitles across translations and translators. The source text contained 132 subtitles amounting to 1557 words. The students produced the human translations from English into German. The machine translations were produced with SUMAT's online demo (Del Pozo, 2014) . The MT output was finally post-edited by translation students. We arranged the quality assessment annotation as two different tasks: 1) error analysis and 2) evaluation. Each task had its own annotation schema. We were interested in obtaining an analytical description of the translational phenomena observed, and a general idea of the impact of such errors on the translation units taken as a whole. Annotators were provided with guidelines illustrating where to mark the errors, how to mark the appropriate text spans, and typical cases for each error category. In addition, all annotators practised in class both the translation, the post-editing of the MT output, as well as the error annotation and evaluation of the human and machine translation. Error Analysis We developed an error annotation schema and an evaluation instrument based partly on MQM and Mellange TLC taxonomies. The error annotation schema consists of 4 dimensions: 1) content, 2) language, 3) format, and 4) semiotics. The first two categories correspond to classical error types described in the literature: \u2022 content: omission, addition, content shift, untrans-lated, terminology; and \u2022 language: syntax, morphology, function words, orthography. The last two categories are our contribution aimed at describing specific features of subtitling: \u2022 format: punctuation, font-style, capitalisation, number of characters per line, number of lines per subtitle, number of seconds per subtitle, line breaks, positioning of subtitle, colour of subtitle, audio synchronisation, video synchronisation; and \u2022 semiotics: cases where there is a contradiction between other channels contributing to the meaning of the text and the translation. Table 2 provides an overview on the error annotation scheme used. An additional field labeled other was provided for each category, in case annotators found a phenomenon not already listed. Moreover, it was possible to add a description of the annotation to provide a more insightful feedback. Evaluation An approach similar to SICAL ratings described in Williams (1989) was adopted for the evaluation task of both HT and MT. The quality of a translation is measured in four levels in light of its acceptability. In our case, the labels were: \u2022 perfect \u2022 acceptable \u2022 revisable \u2022 unacceptable The evaluation was carried out for each dimension of analysis: 1) content, 2) language, 3) format and 4) semiotics. Tasks The students were asked to carry out the following tasks: \u2022 a human translation of the source text (HT)  the machine translation (MT error analysis and evaluation) a post-edition (PE evaluation) The human translation was done as a homework assignment. It was not a timed translation, students were allowed to make use of any documentation resource, and they employed Aegisub 4 as subtitle editor. The post-edition of the machine translation produced by SUMAT was performed with PET (Aziz et al., 2012) as a class assignment at the computer labs. Students had 60 minutes to revise the content and linguistic aspects of 132 subtitles. In a second phase, they produced a version where subtitling formal conventions were considered. The quality assessment of the translations was carried out with MMAX2 (M\u00fcller and Strube, 2003) . The assessment consisted of two tasks: error analysis and evaluation at subtitle level. Both of them were homework assignments and without any restrictions regarding time and documentation available. Short supervised training was provided before the quality assessment assignments in order to get the students acquainted with the usage of the annotation schemas and tools. Corpus Details and Statistics The corpus described here was built over two semesters during a subtitling course offered to students enrolled in the BA Translation Studies programme at Saarland University. Figure 4 depicts the structure of the corpus and Table 1 gives an overview of the students taking part in the courses during the summer and winter terms in 2014.  We collected 50 human and SUMAT produced machine translations. 21 machine translations were post-edited, 25 machine translations and 24 human translations were annotated with errors. The human translations were evaluated 68 times, meaning that some students evaluated more than one translation, but never their own. Machine translations were evaluated 46 times. The English original subtitles were translated into German with SUMAT and were post-edited by 21 students and annotated in terms of errors by 25 students. These post-edited machine translations were evaluated 21 times. A preliminary analysis has been carried out on the subset made up of: HT ERROR, HT EVAL, MT ERROR, MT EVAL for the summer term 2014. We use box plots to visualise a summary of the distribution underlying the samples and to compare central measure values and spread of the data across groups. Moreover, notched box plots help to check if the differences observed are significant: if the notches of two box plots overlap, there is no evidence that their medians differ (Chambers et al., 1983) . Box plots in Figure 5 illustrate the amount of subtitles per text considered perfect, acceptable, revisable and unacceptable. HT (white) typically shows a much higher number of perfect subtitles per text than MT (grey). By contrast, MT shows more unacceptable subtitles per text than HT. Subtitles qualified as acceptable exhibit no clear differences, whereas revisable MT segments outnumber HT. Human output shows wider IQR (higher spread of variation) probably because each evaluation is performed on a different target rendering of the source text, while MT evaluation is based on the same target version. The semiotic dimension reveals a different behaviour in contrast, where differences between HT and MT are negligible, intersemiotic coherence errors seem to be quite infrequent. Humans tend to produce mostly perfect and acceptable subtitles. SUMAT however shows a more homogeneous distribution. All in all, MT output seems to require more revision effort than human translations for all four dimensions. content format language semiotic q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 50 100  Error Analyses complete our understanding of typical pitfalls for each dimension. Most frequent content errors (as Figure 6 points up) are content shifts, omissions and additions no matter the mode of translation. By and large, the machine makes more errors than the humans. Figure 7 shows that the most frequent format error is number of characters per line, specially for machine translations, followed by seconds per subtitle (which captures the ratio characters/second, often been too high if lines are too long). Punctuation and capitalisation errors also have a small share, probably due to the peculiar semiotics of capital letters and some punctuation marks in subtitling, differences are negligible though. Box plots for language error analysis (see Figure 8 ) show that SUMAT have in general more difficulties in this area than human beings, but for orthography. The greatest pitfall for MT is syntax, followed by morphology and function words. In all three categories, the differences with human performance are significant. Figure 9 discloses a very similar behaviour regarding semiotic errors for both modes. In absolute terms, they are negligible. We conclude this analysis of errors with a methodological remark -other is a category barely used. This might indi-q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 50 100 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 30 60 cate that only a few cases were not taken into account by our error taxonomy. Conclusion We presented here SubCo, a corpus of human and machine translation subtitles. The machine translation was carried out with SUMAT and post-edited thereafter. Both human and machine translations were annotated in terms of errors. Moreover, human and machine translations as well as the post-editions of SUMAT were manually evaluated. Although human error annotation and evaluation are very time-consuming tasks, we have shown in Section 4. that this kind of data can provide interesting insights on the nature of human and machine translation in general, and subtitling in particular. Therefore, this resource is a valuable contribution for automatic error detection and MT systems developers, who can benefit from this freely available resource. q q q q q q q q q q q q q q q q q q q 0 50 100 q q q q q q q q q 0 2 4 6 coherence other Future work will involve a thorough evaluation of interannotator agreement, and a more fine grained study at subtitle level with a two-fold goal: 1) to identify the most difficult subtitles, and 2) to obtain a more detailed knowledge of the relationship between some types of errors and their impact in the quality of translations. Acknowledgments This work has partially been supported by the CLARIN-D 5 (Common Language Resources and Technology Infrastructure) project. Bibliographical References",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article mentions support from the CLARIN-D project, which is a research infrastructure initiative funded by national and European research funding agencies. There is no mention of funding from defense, corporate entities, foundations, or an indication that there was no funding."
}