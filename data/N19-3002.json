{
    "article": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora-Penn Treebank, WikiText-2, and CNN/Daily Mail-resulting in similar conclusions. Introduction Dealing with discriminatory bias in training data is a major issue concerning the mainstream implementation of machine learning. Existing biases in data can be amplified by models and the resulting output consumed by the public can influence them, encourage and reinforce harmful stereotypes, or distort the truth. Automated systems that depend on these models can take problematic actions based on biased profiling of individuals. The National Institute for Standards and Technology (NIST) evaluated several facial recognition algorithms and found that they are systematically biased based on gender (Ngan and Grother, 2015) . Algorithms performed worse on faces labeled as female than those labeled as male. Models automating resume screening have also proved to have a heavy gender bias favoring male candidates (Lambrecht and Tucker, 2018) . Such data and algorithmic biases have become a growing concern. Evaluation and mitigation of biases in data and models that use the data has been a growing field of research in recent years. One natural language understanding task vulnerable to gender bias is language modeling. The task of language modeling has a number of practical applications, such as word prediction used in onscreen keyboards. If possible, we would like to identify the bias in the data used to train these models and reduce its effect on model behavior. Towards this pursuit, we aim to evaluate the effect of gender bias on word-level language models that are trained on a text corpus. Our contributions in this work include: (i) an analysis of the gender bias exhibited by publicly available datasets used in building state-of-the-art language models; (ii) an analysis of the effect of this bias on recurrent neural networks (RNNs) based word-level language models; (iii) a method for reducing bias learned in these models; and (iv) an analysis of the results of our method. Related Work A number of methods have been proposed for evaluating and addressing biases that exist in datasets and the models that use them. Recasens et al. (2013) studies the neutral point of view (NPOV) edit tags in the Wikipedia edit histories to understand linguistic realization of bias. According to their study, bias can be broadly categorized into two classes: framing and epistemological. While the framing bias is more explicit, the epistemological bias is implicit and subtle. Framing bias occurs when subjective or one-sided words are used. For example, in the  sentence-\"Usually, smaller cottage-style houses have been demolished to make way for these Mc-Mansions.\", the word McMansions has a negative connotation towards large and pretentious houses. Epistemological biases are entailed, asserted or hedged in the text. For example, in the sentence-\"Kuypers claimed that the mainstream press in America tends to favor liberal viewpoints,\" the word claimed has a doubtful effect on Kuypers statement as opposed to stated in the sentence-\"Kuypers stated that the mainstream press in America tends to favor liberal viewpoints.\" It may be possible to capture both of these kinds of biases through the distributions of co-occurrences. In this paper, we deal with identifying and reducing gender bias based on words co-occurring in a context window. Bolukbasi et al. (2016) propose an approach to investigate gender bias present in popular word embeddings, such as word2vec (Mikolov et al., 2013) . They construct a gender subspace using a set of binary gender pairs. For words that are not explicitly gendered, the component of the word embeddings that project onto this subspace can be removed to debias the embeddings in the gender direction. They also propose a softer variation that balances reconstruction of the original embeddings while minimizing the part of the embeddings that project onto the gender subspace. We use the softer variation to debias the embeddings while training our language model. Zhao et al. (2017) look at gender bias in the context of using structured prediction for visual object classification and semantic role labeling. They ob-serve gender bias in the training examples and that their model amplifies the bias in its predictions. They impose constraints on the optimization to reduce bias amplification while incurring minimal degradation in their model's performance. Word embeddings can capture the stereotypical bias in human generated text leading to biases in NLP Applications. Caliskan et al. (2017) conduct Word Embedding Association Test (WEAT). It is based on the hypothesis that word embeddings closer together in high dimensional space are semantically closer. They find strong evidence of social biases in pretrained word embeddings. Rudinger et al. (2018) introduce Winogender schemas 1 and evaluate three coreference resolution systems-rule-based, statistical and neural systems. They find that these systems' predictions strongly prefer one gender over the other for occupations. Font and Costa-Juss\u00e0 (2019) study the impact of gender debiasing techniques by Bolukbasi et al. (2016) and Zhao et al. (2018) in machine translation. They find these methods to be effective, and even a noted BLEU score improvement for the debiased model. Our work is closely related but while they use debiased pretrained embeddings, we train the word embeddings from scratch and debias them while the language model is trained. May et al. (2019) extend WEAT to state-ofthe-art sentence encoders: the Sentence Encoder Association Test (SEAT). They show that these tests can provide an evidence for presence of bias. However, the cosine similarity between sentences can be an inadequate measure of text similarity in sentences. In this paper, we attempt to minimize the cosine similarity between word embeddings and gender direction. Gonen and Goldberg (2019) conduct experiments using the debiasing techniques proposed by Bolukbasi et al. (2016) and Zhao et al. (2018) . They show that bias removal techniques based on gender direction are inefficient in removing all aspects of bias. In a high dimensional space, spatial distribution of the gender neutral word embeddings remain almost same after debiasing. This enables a gender-neutral classifier to still pick up the cues that encode other semantic aspects of bias. We use softer variation of the debiasing method proposed by Bolukbasi et al. (2016) and attempt to measure the debiasing effect from the minimal changes in the embedding space. Methods We first examine the bias existing in the datasets through qualitative and quantitative analysis of trained embeddings and cooccurrence patterns. We then train an LSTM word-level language model on a dataset and measure the bias of the generated outputs. As shown in Figure 1 , we then apply a regularization procedure that encourages the embeddings learned by the model to depend minimally on gender. We debias the input and the output embeddings individually as well as simultaneously. Finally, we assess the efficacy of the proposed method in reducing bias. We observe that when both input and output embeddings are debiased together, the perplexity of the model shoots up by a much larger number than the input or the output embeddings debiased individually. We report our results when only input embeddings are debiased. This method, however, does not limit the model to capture other forms of bias being learned in other model parameters or output embeddings. The code implementing our methods can be found in our GitHub repository. 2 Datasets and Text Preprocessing We compare the model on three datasets-Penn Treebank (PTB), WikiText-2 and CNN/Daily Mail. The first two have been used in language modeling for a long time. We include CNN/Daily 2 https://github.com/BordiaS/language-model-bias Mail dataset in our experiments as it contains a more diverse range of topics. PTB Penn Treebank comprises of articles ranging from scientific abstracts, computer manuals, etc. to news articles. In our experiments, we observe that PTB has a higher count of male words than female words. Following prior language modeling work, we use the Penn Treebank dataset (PTB; Marcus et al., 1993) preprocessed by Mikolov et al. (2010) . WikiText-2 WikiText-2 is twice the size of the PTB and is sourced from curated Wikipedia articles. It is more diverse and therefore has a more balanced ratio of female to male gender words than PTB. We use preprocessed WikiText-2 (Wikitext-2; Merity et al., 2016) . CNN/Daily Mail This dataset is curated from a diverse range of news articles on topics like sports, health, business, lifestyle, travel etc. This dataset has an even more balanced ratio of female to male gender words and thus, relatively less biased than the above two. However, this does not mean that the use of pronouns is not biased. This dataset was released as part of a summarization dataset by Hermann et al. (2015) , and contains 219,506 articles from the newspaper the Daily Mail. We subsample the sentences by a factor of 100 in order to make the dataset more manageable for experiments. Word-Level Language Model We use a three-layer LSTM word-level language model (AWD-LSTM; Merity et al., 2018) with 1150 hidden units implemented in PyTorch. 3  These models have an embedding size of 400 and a learning rate of 30. We use a batch size of 80 for Wikitext-2 and 40 for PTB. Both are trained for 750 epochs. The PTB baseline model achieves a perplexity of 62.56. For WikiText-2, the baseline model achieves a perplexity of 67.67. For CNN/Daily Mail, we use a batch size of 80 and train it for 500 epochs. We do early stopping for this model. The hyperparameters are chosen through a systematic trial and error approach. The baseline model achieves a perplexity of 118.01. All three baseline models achieve reasonable perplexities indicating them to be good proxies for standard language models. Quantifying Biases For numeric data, bias can be caused simply by class imbalance, which is relatively easy to quantify and fix. For text and image data, the complexity in the nature of the data increases and it becomes difficult to quantify. Nonetheless, defining relevant metrics is crucial in assessing the bias exhibited in a dataset or in a model's behavior. Bias Score Definition In a text corpus, we can express the probability of a word occurring in context with gendered words as follows: P (w|g) = c(w, g)/\u03a3 i c(w i , g) c(g)/\u03a3 i c(w i ) where c(w, g) is a context window and g is a set of gendered words that belongs to either of the two categories: male or female. For example, when g = f , such words would include she, her, woman etc. w is any word in the corpus, excluding stop words and gendered words. The bias score of a specific word w is then defined as: bias train (w) = log P (w|f ) P (w|m) This bias score is measured for each word in the text sampled from the training corpus and the text corpus generated by the language model. A positive bias score implies that a word cooccurs more often with female words than male words. For an infinite context, the words doctor and nurse would cooccur as many times with a female gender as with male gender words and the bias scores for these words will be equal to zero. We conduct two sets of experiments where we define context window c(w, g) as follows: Fixed Context In this scenario, we take a fixed context window size and measure the bias scores. We generated bias scores for several context window sizes in the range (5, 15). For a context size k, there are k words before and k words after the target word w for which the bias score is being measured. Qualitatively, a smaller context window size has more focused information about the target word. On the other hand, a larger window size captures topicality (Levy and Goldberg, 2014) . By choosing an optimal window of k = 10, we give equal weight of 5% to the ten words before and the ten words after the target word. Infinite Context In this scenario, we take an infinite window of context with weights diminishing exponentially based on the distance between the target word w and the gendered word g. This method emphasizes on the fact that the nearest word has more information about the target word. The farther the context gets away from a word, the less information it has about the word. We give 5% weight to the words adjacent to the target word as in Fixed Context but reduce the weights of the words following by 5% and 95% to the rest; this applied recursively gives a base of 0.95. This method of exponential weighting instead of equal weighting adds to the stability of the measure. Bias Reduction Measures To evaluate debiasing of each model, we measure the bias for the generated corpus. bias \u03bb (w) = log( P (w|f ) P (w|m) ) To estimate the amplification or reduction of the bias, we fit a univariate linear regression model over bias scores of context words w as follows: bias \u03bb (w) = \u03b2 * bias train (w) + c where \u03b2 is the scaled amplification measure relative to the training data. Reducing \u03b2 implies debiasing the model. We also look at the distribution of the bias by evaluating mean absolute bias and deviation in bias scores for each context word in each of the generated corpora. \u00b5 \u03bb = mean(abs(bias \u03bb )); \u03c3 \u03bb = stdev(bias \u03bb ) We take the mean of absolute bias score as the word can be biased in either of the two directions. Model Debiasing Machine learning techniques that capture patterns in data to make coherent predictions can unintentionally capture or even amplify the bias in data (Zhao et al., 2017) . We consider a gender subspace present in the learned embedding matrix in our model as introduced in the Bolukbasi et al. ( 2016 ) paper. We train these embeddings on the word level language model instead of using the debiased pretrained embeddings (Font and Costa-Juss\u00e0, 2019) . We conduct experiments for the three cases where we debias-input embeddings, output embeddings, and both the embeddings simultaneously. Let w \u2208 S W be a word embedding corresponding to a word in the word embedding matrix W . Let D i , . . . , D n \u2282 S W be the defining sets 4 that contain gender-opposing words, e.g. man and woman. The defining sets are designed separately for each corpus since certain words may not appear in another corpus. We consider it a defining set if both gender-opposing words occur in the training corpus. If u i , v i are the embeddings corresponding to the words man and woman, then {u i , v i } = D i . We consider the matrix C which is defined as a stack of difference vectors between the pairs in the defining sets. We have: C = \uf8ee \uf8ef \uf8f0 ( u 1 \u2212v 1 2 ) . . . ( un\u2212vn 2 ) \uf8f9 \uf8fa \uf8fb = U \u03a3V The difference between the pairs encodes the gender information corresponding to the gender pair. We then perform singular value decomposition on C, obtaining U \u03a3V . The gender subspace B is then defined as the first k columns (where k is chosen to capture 50% of the variation) of the right singular matrix V : B = V 1:k Let N be the matrix consisting of the embeddings for which we would like the corresponding words to exhibit unbiased behavior. If we want the embeddings in N to have minimal bias, then its projection onto the gender subspace B should be small in terms its the squared Frobenius norm. Target Word \u03bb Sample From Generated Text 0.0 \"she was put on her own machine to raise money for her own wedding <unk> route which saw her crying and <unk> down a programme today . effects began by bottom of her marrow the <unk>\" crying 0.5 \"he <unk> in the americas with the <unk> which can spread a <unk> circumcision ceremony made last month . as he <unk> his mother s <unk> crying to those that\" 1.0 \"he discovered peaceful facebook remains when he was caught crying officers but was arrested after they found the crash hire a man <unk> brown shocked his brother <unk> over\" 0.0 \"camilla said she talked to anyone and had previously left her love of two young children . it all comes with her family in conviction of her son s death . it s been fragile . the <unk> and retail boy that was rik s same maker identified genuinely <unk> attacked all\" fragile 0.5 \"his children at nearby children s hospital in <unk> and went <unk> years after he was arrested on <unk> bail . she spent relaxed weeks in prison after being sharply in fragile <unk> while she was jailed and strangled when she was born in <unk> virginia\" 1.0 \"could they possibly have a big barrier to jeff <unk> and <unk> my son all intelligence period that will contain the east country s world from all in the world the truth is when we moved clear before the split twenty days earlier that day . none of the distributed packs on the website can never <unk> re able to <unk> it the second time so that fitting fragile <unk> are and less the country is <unk> . it came as it was once <unk> million lead jobs mail yorkshire . adoption of these first product is ohio but it is currently almost impossible for the moon to address and fully offshore hotly \" 0.0 \"mr <unk> worked traditions at the squadron base in <unk> rbs to marry the us government .he referring to the mainland them in february <unk> he kept communist leadership from undergoing\" leadership 0.5 \"obama s first wife janet had a chance to run the opposition for a superbowl event for charity the majority of the south african people s travel stage <unk> leadership while it was married off christmas\" 1.0 \"the woman s lungs and drinking the ryder of his daughters s leadership morris said businesses . however being of his mouth around wiltshire and burn talks from the hickey s <unk> employees\" 0.0 \"his legs and allegedly killed himself by suspicious points . in the latest case after an online page he left prisoner in his home in <unk> near <unk> manhattan on saturday when he was struck in his car operating in <unk> bay smoking <unk> and <unk> <unk> when he had\" prisoner 0.5 \"it is something that the medicines can target prisoner and destroy <unk> firms in the uk but i hope that there are something into the on top getting older people who have more branded them as poor .\" 1.0 \"the ankle follows a worker <unk> her <unk> prisoner she died this year before now an profile which clear her eye borrowed for her organ own role . it was a huge accident after the drugs she had\" Therefore, to reduce the bias learned by the embedding layer in the model, we can add the following bias regularization term to the training loss: L B = \u03bb N B 2 F where \u03bb controls the importance of minimizing bias in the embedding matrix W (from which N and B are derived) relative to the other components of the model loss. The matrices N and C are updated each iteration during the model training. We input 2000 random seeds in the language model as starting points to start word generation. We use the previous words as an input to the language model and perform multinomial selection to generate up the next word. We repeat this up to 500 times. In total, we generate 10 6 tokens for all three datasets for each \u03bb and measure the bias. Experiments Model After achieving the baseline results, we run experiments to tune \u03bb as hyperparameter. We report an in-depth analysis of bias measure on the models with debiased input embeddings. Results and Text Examples We calculate the measures stated in Section 3.3 for the three datasets and the generated corpora using the corresponding RNN models. The results are shown in Tables 1, 2 and 3. We see that the \u00b5 consistently decline as we increase \u03bb until a point, beyond which the model becomes unstable. So there is a scope of optimizing the \u03bb values. The detailed analysis is presented in Section 4.3 Table 4 shows excerpts around selected target words from the generated corpora to demonstrate the effect of debiasing for different values of \u03bb. We highlight the words crying and fragile that are typically associated with feminine qualities, along with the words leadership and prisoners that are stereotyped with male identity. These biases are reflected in the generated text for \u03bb = 0. We notice increased mention of the less probable gender in the subsequent generated text with debiasing (\u03bb = 0.5, 1.0). For fragile, the generated text at \u03bb = 1.0 has reduced the mention of stereotyped female words but had no mentions of male words; resulting in a large chunk of neutral text. Similarly, in prisoners, the generated text for \u03bb = 0.5 has no gender words. However, these are small snippets and the bias scores presented in the supplementary table quantifies the distribution of gender words around the target word in the entire corpus. These target words are chosen as they are commonly perceived gender biases and in our study, they show prominent debiasing effect. 5 Analysis and Discussion We consider a text corpus to be biased when it has a skewed distribution of words cooccuring with one gender vs another. Any dataset that has such demographic bias can lead to (potentially unintended) social exclusion (Hovy, 2015) . PTB and WikiText-2 consist of news articles related to business, science, politics, and sports. These are all male dominated fields. However, CNN/Daily Mail consists of articles across diverse set of categories like entertainment, health, travel etc. Among the three corpora, Penn Treebank has more frequent mentions of male words with respect to female words and CNN/Daily Mail has the least. As defined, bias score of zero implies perfectly neutral word, any value higher/lower implies female/male bias. Therefore, the absolute value of bias score signifies presence of bias. Overall bias in a dataset can be estimated as the average of absolute bias score (\u00b5). The aggregated absolute bias scores \u00b5 of the three datasets-Penn Treebank, WikiText-2, and CNN/Daily Mail-are 0.83, 0.80, and 0.72 respectively. Higher \u00b5 value in this measure means on-an-average the words in the entire corpus are more gender biased. As per the Tables 1, 2 , and 3, we see that the \u00b5 consistently decline as we increase \u03bb until a point, beyond which the model becomes unstable. So there is a scope of optimizing the \u03bb values. The second measure we evaluated is the standard deviation (\u03c3) of the bias score distribution. 5 For more examples, refer to the supplement Less biased dataset should have the bias score concentrating closer to zero and hence lower \u03c3 value. We consistently see that, with the initial increase of \u03bb, there is a decrease in \u03c3 of the bias score distribution. The final measure to evaluate debiasing is comparison of bias scores at individual word level. We regress the bias scores of the words in generated text against their bias scores in the training corpus after removing the outliers. The slope of regression \u03b2 signifies the amplification or dampening effect of the model relative to the training corpus. Unlike the previous measures, this measure gives clarity at word level bias changes. A drop in \u03b2 signifies reduction in bias and vice versa. A negative \u03b2 signifies inversion in bias assuming there are no other effects of the loss term. In our experiments, we observe \u03b2 to increase with higher values of \u03bb possibly due to instability in model and none of those values go beyond 1. We observe that corpus level bias scores like \u00b5, \u03c3 are less effective measures to study efficacy of debiasing techniques because they fail to track the improvements at word level. Instead, we recommend a word level score comparison like \u03b2 to evaluate robustness of debiasing at corpus level. To choose the context window in a more robust manner, we take exponential weightings to the cooccurrences. The results for aggregated average of absolute bias and standard deviation show the same pattern as in fixed context window. As shown in the results above, we see that the standard deviation (\u03c3), absolute mean (\u00b5) and slope of regression (\u03b2) reduce for smaller \u03bb relative to those in training data and then increase with \u03bb to match the variance in the original corpus. This holds for the experiments conducted with fixed context window as well as with exponential weightings. Conclusion In this paper, we quantify and reduce gender bias in word level language models by defining a gender subspace and penalizing the projection of the word embeddings onto that gender subspace. We device a metric to measure gender bias in the training and the generated corpus. In this study, we quantify corpus level bias in two different metrics-absolute mean (\u00b5) and standard deviation (\u03c3). However, for evaluating debiasing effects, we propose a relative metric (\u03b2) Acknowledgements We are grateful to Yu Wang and Jason Cramer for helping to initiate this project, to Nishant Subramani for helpful discussion, and to our reviewers for their thoughtful feedback. Bowman acknowledges support from Samsung Research. to study the change in bias scores at word level in generated text vs. training corpus. To calculate \u03b2, we conduct an in-depth regression analysis of the word level bias measures in the generated text corpus over the same for the training corpus. Although we found mixed results on amplification of bias as stated by Zhao et al. (2017) , the debiasing method shown by Bolukbasi et al. (2016) was validated with the use of novel and robust bias measure designed in this paper. Our proposed methodology can deal with distribution of words in a vocabulary in word level language model and it targets one way to measure bias, but it's highly likely that there is significant bias in the debiased models and data, just not bias that we can detect on this measure. It can be concluded different bias metrics show different kinds of bias (Gonen and Goldberg, 2019) . We additionally observe a perplexity bias tradeoff as a result of the additional bias regularization term. In order to reduce bias, there is a compromise on perplexity. Intuitively, as we reduce bias the perplexity is bound to increase due to the fact that, in an unbiased model, male and female words will be predicted with an equal probability.",
    "abstract": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora-Penn Treebank, WikiText-2, and CNN/Daily Mail-resulting in similar conclusions.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "123",
    "year": "2019",
    "month": "June",
    "title": "Identifying and Reducing Gender Bias in Word-Level Language Models"
}