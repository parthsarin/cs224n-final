{
    "article": "We present a debiased dataset for the Personcentric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an image and a caption, PCVG requires pairing up a person's name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who's Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. Naturally, models trained on these biased data lead to over-estimation of performance on the benchmark. To enforce models being correct for the correct reasons, we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context, such as those with no verb or with a long chain of conjunct names in their captions. Our experiments show that our new subsampled dataset 1 contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods. We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased (and larger) training set on our debiased test set. We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements. Introduction A newly released task called Person-centric Visual Grounding (Cui et al., 2021) poses an interesting angle into contextual reasoning in vision-language. The task is motivated by humans' reasoning ability. Humans viewing an image with a caption as shown in Figure 1 can reason (and if needed, speculate) which name refers to which person in the image. This reasoning task involves multiple abilities, such as perceiving characteristics and behaviors of people, understanding their actions in context, speculating about their intentions and effects human of actions (Fang et al., 2020) , and connecting visually perceived characteristics with grounded descriptions in natural language (Kazemzadeh et al., 2014; Yu et al., 2016; Zellers et al., 2019) . In many cases, this task can be performed without knowing the names of the people; for instance in the example on the right, one person is signing and the other is not, as such it is possible to predict which person refers to President and Secretary of State respectively. However, in cases such as the example on the left, if all persons are performing the same action (run-ning on a track), then it is hard to match names with these runners without any additional information. Progress in the PCVG task can thus help better capture what exact contextual cues are needed to learn about a person's characteristics in a scenario, and can aid improvements in visual understanding about human interactions and behaviors. To support this task, Cui et al. ( 2021 ) offer a large-scale dataset called Who's Waldo which consists of 272K annotated real life images. Ideally, the dataset should consist of input-output pairs (such as the example on the right in Figure 1 ) which are 'solvable' as opposed to the one on the left which is ambiguous. However, as we explore the original Who's Waldo dataset, we encounter a great portion of cases that resemble the left example in Figure 1 , unsolvable data with insufficient contextual cues. Given such context, if we do not recognize who exactly is in the picture, even we human beings cannot tell which name is who. We can then only make predictions with biased assumptions, such as the first named person would always be on the leftmost, or the main subject would always make up the largest area. Such biases in the original dataset may explain why the heuristic methods perform very strongly, outperforming random guessing by a big 27% increase in test accuracy and trailing the top benchmark only by 6%. We believe a fair dataset should not encourage approaches to adopt biases to such an extent, and thus the original baseline model overestimates its performance. Inspired by dataset debiasing works such as VQA-CP (Agrawal et al., 2018) and GQA-OOD (Kervadec et al., 2021) , we create a debiased collection of 84K annotated image-captions out of the Who's Waldo dataset by filtering out all biased data with insufficient context. We evaluate the quality of our new dataset by applying the original heuristic methods as well as Who's Waldo's benchmark model. Results show that our debiased dataset greatly reduces the heuristic biases from the original dataset and provides the PCVG task a more practical baseline for future developments. Related Work Dataset Debiasing. We take many inspirations from previous studies on uncurated datasets. A task dataset if not curated properly could lead to methods that cheat their ways through without learning generalized information. For example, VQAv2 (Goyal et al., 2017) addresses the imbalance be-tween language and images in VQAv1 (Antol et al., 2015) which results in visual information being ignored and inflated model performance. VQA-CP (Agrawal et al., 2018) and GQA-OOD (Kervadec et al., 2021) were designed to test model performance if spurious correlations exist in the training dataset. Cadene et al. (2019) ; Chen et al. (2020a) ; Gokhale et al. (2020) are bias-aware techniques that mitigate dataset bias with modeling and data augmentation. Ye and Kovashka (2021) introduce exploits by matching repeated texts in questions and answers to achieve high scores in Visual Commonsense Reasoning (Zellers et al., 2019) . We also learn from various techniques to amend priors, biases, or shortcuts in datasets. REPAIR (Li and Vasconcelos, 2019) uses resampling to fix representation biases in image datasets. Dasgupta et al. (2018) incorporate compositional information into sentence embeddings for Natural Language Inference. DQI (Mishra et al., 2020) offers quantitative metrics to assess biases in automated dataset creation in Natural Language Processing. Le Bras et al. ( 2020 ) introduce adversarial measures to mitigate biases in various Natural Language Processing and Computer Vision tasks. Visual Grounding. The PCVG task adapts previous supervised Visual Grounding models as its original baselines. The Visual Grounding task is defined as locating specific objects in an image from a textual description. First established by Karpathy et al. (2014) , following researches have evolved into extracting attention information such as works by Deng et al. (2018) and Endo et al. (2017) . A huge variation of datasets for Visual Grounding have also been created, including Flicker30k (Plummer et al., 2015) , Visual Genome (Krishna et al., 2017) , and RefCOCO (Yu et al., 2016) . Referring Expression Comprehension (REC). An active branch from Visual Grounding, the Referring Expression Comprehension task (Rohrbach et al., 2016) is no longer restricted to object categories. Instead its goal is to relate a free region in an image to a sentence description. Mattnet (Yu et al., 2018) (Lin et al., 2014) . Recent works such as DarkPose (Zhang et al., 2020 ) also attempt to utilize human pose information to better single out human traits from complex background. Method In this section, we introduce the Person-centric Visual Grounding task, discuss the original Who's Waldo dataset, and provide our analysis of shortcuts, biases, and other issues that we discovered in the dataset. We describe the process via which we curate, debias, and filter the dataset. The Task The Person-centric Visual Grounding task is defined as follows. The givens are an image I, a set of m \u2265 1 person detections B (in form of bounding boxes), and a corresponding image caption T where its tokens contain references to n \u2265 1 persons. For each referred person, we look for the best matching detection from the givens. We also assume no two persons can be matched with the same detection. The Who's Waldo Dataset The dataset consists of 272K real-life captioned images sourced from the free Wikimedia Commons repository. Each image pictures individuals under the 'People by name' category on Wikimedia Commons, while its caption describes the scene and explicitly mentions the featured people in real names. Key dataset creation procedures, text pre-processing, identifying person entities in captions, detecting bounding boxes of people in images, and generating ground truths linking bounding boxes and names, are all done with existing automated tools such as FLAIR (Akbik et al., 2019) and MMDetection (Chen et al., 2019) . To prevent misuse, in the publicly released version, all the real names in the captions are replaced with the [NAME] token, but references between bounding boxes and token indices are given in individual annotation files. This is equivalent to masking each name with indexed placeholders such as PERSON1, PERSON2, etc. Amongst the entirety of 272K annotated samples, 179K samples are used for training, 6.7K for validation, and 6.7K for testing. Each test sample is supposed to either mention at least two persons or choose from at least two bounding boxes. The original test set is further validated manually on Amazon Mechanical Turk. Biases in Who's Waldo The premise of the Person-centric Visual Grounding task is to use ONLY the caption text and the image as the cues to find out the correct bounding box from the image per mentioned name. However, we observe a large portion of the original Who's Waldo dataset does not provide sufficient contexts and can only be solved by heuristic methods. We discuss two major types of biases that we discover in the following sections. The first type no-verb is that the caption text contains zero detectable verbs. Since linguistically a verb is the crucial part of an action that assigns participants with semantic roles, we technically have no way to tell who performs or who receives an action without verbs. For example in Figure 2 (a), we are unable to tell who is who from the image and the no-verb caption alone, unless we recognize Vladimir Putin or the Georgian President with external knowledge. The second type conjunct-names is that the caption contains a long chain of conjunct referred names. Shown in Figure 2 (b), all the referred names share the verb perform, joined together only with conjunct words such as and or along with. With no indication of the order amongst these persons, we can only resort to a naive positional order such as left-to-right. But since we may also have extra bounding boxes as choices, such naive assumption is indeed unreliable. Figure 2 (b) is such an example that the first mentioned name is not always the one in the left-most bounding box. Data Curation for De-biasing In order to resolve the aforementioned limitations of the original dataset, we utilize two pipelines in SpaCy ver 3.0 (Honnibal et al., 2020) to filter out the biased data. We apply the POS-Tagging pipeline to find out if sentences in an image cap- For both pipelines, we replace the [NAME] tokens that refer to the same person in a caption with a random popular first name, so that the natural language-based SpaCy pipelines can yield more accurate results. Both pipelines use the state-ofthe-art en-web-core-trf model which is built on RoBERTa (Liu et al., 2019) . Ultimately, our filtering procedure produces 84K qualifying image-caption pairs. Table 1 shows the distribution of samples sourced from each split of the original through our two debiasing pipelines. We utilize data from the unused yet legitimately annotated 79K samples of the original dataset. We reorganize and split all the qualifying 84K samples into 74K for training, 5K for validation, and 5K for test. Our new test set does not overlap with the original training set. Similarly to the design of the original, we enforce that all samples in our new test set involves no trivial case that contains exactly one referred name and exactly one bounding box. We also make sure that any test set sample always has at least one name-to-bounding-box pair as ground truth. Experiments and Baselines Setup. We evaluate the quality of our debiased dataset with the same heuristic and Transformerbased methods from the original paper. We also train the benchmark model on both the original and our new training set. We report the accuracies obtained from our new test set as the new baselines. Heuristics. We inherit the original heuristic measures to study the potential biases of our debiased dataset versus those of the original dataset. Alongside Random guessing, we assign the names in the caption to the bounding boxes sorted by: (a) decreasing area size (Big \u2192 Small), (b) left-to-right upper-left coordinates (L \u2192 R (All)), and (c) left-toright upper-left coordinates of the largest d bounding boxes, d being the larger between the number of bounding boxes and the number of names in a test case (L \u2192 R (Largest)). Transformer-based Models. We adapt the original benchmark Who's Waldo model to our debiased dataset and see how well it can perform under the updated contexts. The benchmark model is a multilayer multi-modal Transformer (Vaswani et al., 2017) . Based on UNITER (Chen et al., 2020b) , it learns to maximize the similarities between the corresponding person names and bounding boxes while minimize the similarities between those that do not match up. We fine-tune the Who's Waldo model with pre-trained weights from UNITER. Analysis of Results. ased dataset. We find that the heuristic measures have overall lower performance on our new dataset, meaning we have successfully reduced the effects of the positional and the size-based biases from the original dataset. Most significantly, we have lowered L \u2192 R (All) from +7.5% to +1.4%, almost equal to randomness. Even the strongest L \u2192 R (Largest) heuristic has been lowered from +26.8% all the way down to +13.3% as well. Our dataset is thus proven less biased compared to the original. We also show that our dataset has better practicality for the task. Measured with our new test set, the performance of the Who's Waldo benchmark model trained with the original training set performs 3.8% lower than that trained with our new, smaller training set. Meanwhile, the test accuracy gap between the Transformer-based method and the heuristic methods has become larger using our debiased dataset, widened from 5.8% to 9.7%. In addition, using the filtered biased samples from the original test set on our new trained model yields an even lower performance at 48.2%, which indicates our new baseline model now adopts fewer biases during training compared to the original. Altogether with the lowered new baseline accuracy of 54.0%, we argue that our debiased dataset improves the quality of contextual cues that su-pervised models can learn from, and leaves more applicable room for improvements in the future. Conclusion We present a refined dataset for the PCVG task with samples that contain contextual information required for the task. We address prominent biases that we identified in the original task dataset by filtering out a large number of unsolvable cases, and report new baseline performances on the new benchmark. Our refined dataset can serve as a more reliable benchmark to enable fair comparisons for new modeling techniques and training protocols. Acknowledgements This research was supported by grants from DARPA SAIL-ON, DARPA KAIROS, NSF 1816039, and NSF 2132724. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers. Ethical Considerations Our curated dataset is available at https:// github.com/fpsluozi/tofindwaldo . We will also follow the same licensing and data sharing policy as the original Who's Waldo dataset.",
    "funding": {
        "defense": 1.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions funding from DARPA (Defense Advanced Research Projects Agency), which is a branch of the United States Department of Defense, and NSF (National Science Foundation), which is a government-funded research agency. Therefore, the research was funded by defense and a research agency.",
    "abstract": "We present a debiased dataset for the Personcentric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an image and a caption, PCVG requires pairing up a person's name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who's Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. Naturally, models trained on these biased data lead to over-estimation of performance on the benchmark. To enforce models being correct for the correct reasons, we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context, such as those with no verb or with a long chain of conjunct names in their captions. Our experiments show that our new subsampled dataset 1 contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods. We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased (and larger) training set on our debiased test set. We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": 0,
    "year": 2022,
    "month": "May",
    "title": "To Find Waldo You Need Contextual Cues: Debiasing Who{'}s Waldo"
}