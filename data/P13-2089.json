{
    "article": "Recommendation dialog systems help users navigate e-commerce listings by asking questions about users' preferences toward relevant domain attributes. We present a framework for generating and ranking fine-grained, highly relevant questions from user-generated reviews. We demonstrate our approach on a new dataset just released by Yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain. Introduction Recommendation dialog systems have been developed for a number of tasks ranging from product search to restaurant recommendation (Chai et al., 2002; Thompson et al., 2004; Bridge et al., 2005; Young et al., 2010) . These systems learn user requirements through spoken or text-based dialog, asking questions about particular attributes to filter the space of relevant documents. Traditionally, these systems draw questions from a small, fixed set of attributes, such as cuisine or price in the restaurant domain. However, these systems overlook an important element in users' interactions with online product listings: usergenerated reviews. Huang et al. (2012) show that information extracted from user reviews greatly improves user experience in visual search interfaces. In this paper, we present a dialog-based interface that takes advantage of review texts. We demonstrate our system on a new challenge corpus of 11,537 businesses and 229,907 user reviews released by the popular review website Yelp 1 , focusing on the dataset's 4724 restaurants and bars (164,106 reviews) . This paper makes two main contributions. First, we describe and qualitatively evaluate a frame-1 https://www.yelp.com/dataset_challenge/ work for generating new, highly-relevant questions from user review texts. The framework makes use of techniques from topic modeling and sentiment-based aspect extraction to identify finegrained attributes for each business. These attributes form the basis of a new set of questions that the system can ask the user. Second, we use a method based on informationgain for dynamically ranking candidate questions during dialog production. This allows our system to select the most informative question at each dialog step. An evaluation based on simulated dialogs shows that both the ranking method and the automatically generated questions improve recall. 2 Generating Questions from Reviews Subcategory Questions Yelp provides each business with category labels for top-level cuisine types like Japanese, Coffee & Tea, and Vegetarian. Many of these top-level categories have natural subcategories (e.g., ramen vs. sushi). By identifying these subcategories, we enable questions which probe one step deeper than the top-level category label. To identify these subcategories, we run Latent Dirichlet Analysis (LDA) (Blei et al., 2003) on the reviews of each set of businesses in the twenty most common top-level categories, using 10 topics and concatenating all of a business's reviews into one document. 2 Several researchers have used sentence-level documents to model topics in reviews, but these tend to generate topics about finegrained aspects of the sort we discuss in Section 2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010) . We then manually labeled the topics, discarding junk topics and merging similar topics. to a subcategory based on the topic with highest probability in that business's topic distribution. Finally, we use these subcategory topics to generate questions for our recommender dialog system. Each top-level category corresponds to a single question whose potential answers are the set of subcategories: e.g., \"What type of Japanese cuisine do you want?\" Questions from Fine-Grained Aspects Our second source for questions is based on aspect extraction in sentiment summarization (Blair-Goldensohn et al., 2008; Brody and Elhadad, 2010) . We define an aspect as any noun-phrase which is targeted by a sentiment predicate. For example, from the sentence \"The place had great atmosphere, but the service was slow.\" we extract two aspects: +atmosphere and -service. Our aspect extraction system has two steps. First we develop a domain specific sentiment lexicon. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with 'strongly subjective' adjectives from the OpinionFinder lexicon (Wilson et al., 2005) , yielding 1342 seeds. Like Brody and Elhadad (2010) , we then construct a coordination graph that links adjectives modifying the same noun, but to increase precision we require that the adjectives also be conjoined by and (Hatzivassiloglou and McKeown, 1997) . This reduces problems like propagating positive sentiment to orange in good orange chicken. We marked adjectives that follow too or lie in the scope of negation with special prefixes and treated them as distinct lexical entries. Sentiment Propagation Negative and positive seeds are assigned values of 0 and 1 respectively. All other adjectives begin at 0.5. Then a standard propagation update is computed iteratively (see Eq. 3 of Brody and Elhadad (2010) ). In Brody and Elhadad's implementation of this propagation method, seed sentiment values are fixed, and the update step is repeated until the nonseed values converge. We found that three modifications significantly improved precision. First, we omit candidate nodes that don't link to at least two positive or two negative seeds. This eliminated spurious propagation caused by one-off parsing errors. Second, we run the propagation algorithm for fewer iterations (two iterations for negative terms and one for positive terms). We found that additional iterations led to significant error propagation when neutral (italian) or ambiguous (thick) terms were assigned sentiment. 3 Third, we update both non-seed and seed adjectives. This allows us to learn, for example, that the negative seed decadent is positive in the restaurant domain. Table 2 Evaluative Verbs In addition to this adjective lexicon, we take 56 evaluative verbs such as love and hate from admire-class VerbNet predicates (Kipper-Schuler, 2005) . Extraction Patterns To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003) . Table 3 shows a sample of the aspects generated by these methods. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non-business targets (Wayne is the target of excellent job in (1)). Second, hypothetical contexts lead to spurious extractions. In (2), the extraction +service is clearly wrong-in fact, the opposite sentiment is being expressed. (1) Wayne did an excellent job addressing our needs and giving us our options. (2) Nice and airy atmosphere, but service could be more attentive at times. We address these problems by filtering out sentences in hypothetical contexts cued by if, should, could, or a question mark, and by adopting the following, more conservative extractions rules: i) [BIZ + have + adj. + NP] Sentiment adjective modifies NP, main verb is have, subject is business name, it, they, place, or absent. (E.g., This place has some really great yogurt and toppings). ii) [NP + be + adj.] Sentiment adjective linked to NP by be-e.g., Our pizza was much too jalapeno-y. \"Good For\" + NP Next, we extract aspects using the pattern BIZ + positive adj. + for + NP, as in It's perfect for a date night. Examples of extracted aspects include +lunch, +large groups, +drinks, and +quick lunch. Verb + NP Finally, we extract NPs that appear as direct object to one of our evaluative verbs (e.g., We loved the fried chicken). Aspects as Questions We generate questions from these extracted aspects using simple templates. For example, the aspect +burritos yields the question: Do you want a place with good burritos? Question Selection for Dialog To utilize the questions generated from reviews in recommendation dialogs, we first formalize the dialog optimization task and then offer a solution. Problem Statement We consider a version of the Information Retrieval Dialog task introduced by Kope\u010dek (1999) . Businesses b \u2208 B have associated attributes, coming from a set Att. These attributes are a combination of Yelp categories and our automatically extracted aspects described in Section 2. Attributes att \u2208 Att take values in a finite domain dom(att). We denote the subset of businesses with an attribute att taking value val \u2208 dom(att), as B| att=val . Attributes are functions from businesses to subsets of values: att : B \u2192 P(dom(att)). We model a user information need I as a set of attribute/value pairs: I = {(att 1 , val 1 ), . . . , (att |I| , val |I| )}. Given a set of businesses and attributes, a recommendation agent \u03c0 selects an attribute to ask Chinese: Mexican: +beef +egg roll +sour soup +orange chicken +salsa bar +burritos +fish tacos +guacamole +noodles +crab puff +egg drop soup +enchiladas +hot sauce +carne asade +breakfast burritos +dim sum +fried rice +honey chicken +horchata +green salsa +tortillas +quesadillas Japanese: American (New) +rolls +sushi rolls +wasabi +sushi bar +salmon +environment +drink menu +bar area +cocktails +brunch +chicken katsu +crunch +green tea +sake selection +hummus +mac and cheese +outdoor patio +seating area +oysters +drink menu +sushi selection +quality +lighting +brews +sangria +cheese plates H = H \u222a {(att, val)} end Return (H, B) Algorithm 1: Procedure for evaluating a recommendation agent the user about, then uses the answer value to narrow the set of businesses to those with the desired attribute value, and selects another query. Algorithm 1 presents this process more formally. The recommendation agent can use both the set of businesses B and the history of question and answers H from the user to select the next query. Thus, formally a recommendation agent is a function \u03c0 : B \u00d7 H \u2192 Att. The dialog ends after a fixed number of queries K. Information Gain Agent The information gain recommendation agent chooses questions to ask the user by selecting question attributes that maximize the entropy of the resulting document set, in a manner similar to decision tree learning (Mitchell, 1997) . We follow the standard approach of using the attributes of an individual business as a simulation of a user's preferences (Chung, 2004; Young et al., 2010) . For every business b \u2208 B we form an information need composed of all of b's attributes: I b = {att\u2208Att|att(b) =\u2205} (att, att(b)) To evaluate a recommendation agent, we use the recall metric, which measures how well an information need is satisfied. For each information need I, let B I be the set of businesses that satisfy the questions of an agent. We define the recall of the set of businesses with respect to the information need as recall(B I , I) = |B I ||I| We average recall across all information needs, yielding average recall. We compare against a random agent baseline that selects attributes att \u2208 Att uniformly at random at each time step. Other recommendation dialog systems such as Young et al. (2010) select questions from a small fixed hierarchy, which is not applicable to our large set of attributes. Results Figure 1 shows the average recall for the random agent versus the information gain agent with varying sets of attributes. 'Top-level' repeatedly queries the user's top-level category preferences, 'Subtopic' additionally uses our topic modeling subcategories, and 'All' uses these plus the aspects extracted from reviews. We see that for sufficiently long dialogs, 'All' outperforms the other systems. The 'Subtopic' and 'Top-level' systems plateau after a few dialog steps once they've asked all useful questions. For instance, most businesses only have one or two top-level categories, so after the system has identified the top-level category that the user is interested in, it has no more good questions to ask. Note that the information gain agent starts dialogs with the top-level and appropriate subcategory questions, so it is only for longer dialogs that the fine-grained aspects boost performance. Below we show a few sample output dialogs from our 'All' information gain agent. Conclusion We presented a system for extracting large sets of attributes from user reviews and selecting relevant attributes to ask questions about. Using topic models to discover subtypes of businesses, a domain-specific sentiment lexicon, and a number of new techniques for increasing precision in sentiment aspect extraction yields attributes that give a rich representation of the restaurant domain. We have made this 1329-term sentiment lexicon for the restaurant domain available as useful resource to the community. Our information gain recommendation agent gives a principled way to dynamically combine these diverse attributes to ask relevant questions in a coherent dialog. Our approach thus offers a new way to integrate the advantages of the curated hand-build attributes used in statistical slot and filler dialog systems, and the distributionally induced, highly relevant categories built by sentiment aspect extraction systems. Acknowledgments Thanks to the anonymous reviewers and the Stanford NLP group for helpful suggestions. The authors also gratefully acknowledge the support of the Nuance Foundation, the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-13-2-0040, ONR grants N00014-10-1-0109 and N00014-13-1-0287 and ARO grant W911NF-07-1-0216, and the Center for Advanced Study in the Behavioral Sciences.",
    "funding": {
        "defense": 1.0,
        "corporate": 0.867032991886486,
        "research agency": 2.4749261732015526e-05,
        "foundation": 0.029312519751639066,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article mentions support from the Defense Advanced Research Projects Agency (DARPA), which is a branch of the U.S. Department of Defense, and the Office of Naval Research (ONR), which is also part of the U.S. Department of Defense. It also mentions support from the Nuance Foundation, which is a corporate foundation, and the Army Research Office (ARO), which is part of the U.S. Army, another defense entity. There is no mention of funding from a research agency not associated with defense, nor from any other type of corporate, foundation (non-corporate), or an indication that there was no funding.",
    "abstract": "Recommendation dialog systems help users navigate e-commerce listings by asking questions about users' preferences toward relevant domain attributes. We present a framework for generating and ranking fine-grained, highly relevant questions from user-generated reviews. We demonstrate our approach on a new dataset just released by Yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Japanese",
        "Chinese"
    ],
    "numcitedby": 37,
    "year": 2013,
    "month": "August",
    "title": "Generating Recommendation Dialogs by Extracting Information from User Reviews",
    "values": {
        "building on past work": "This paper makes two main contributions. The framework makes use of techniques from topic modeling and sentiment-based aspect extraction to identify finegrained attributes for each business. These attributes form the basis of a new set of questions that the system can ask the user. This allows our system to select the most informative question at each dialog step. An evaluation based on simulated dialogs shows that both the ranking method and the automatically generated questions improve recall.",
        "novelty": "Our information gain recommendation agent gives a principled way to dynamically combine these diverse attributes to ask relevant questions in a coherent dialog.",
        "performance": "Figure 1 shows the average recall for the random agent versus the information gain agent with varying sets of attributes. 'Top-level' repeatedly queries the user's top-level category preferences, 'Subtopic' additionally uses our topic modeling subcategories, and 'All' uses these plus the aspects extracted from reviews. We see that for sufficiently long dialogs, 'All' outperforms the other systems. The 'Subtopic' and 'Top-level' systems plateau after a few dialog steps once they've asked all useful questions. For instance, most businesses only have one or two top-level categories, so after the system has identified the top-level category that the user is interested in, it has no more good questions to ask. Note that the information gain agent starts dialogs with the top-level and appropriate subcategory questions, so it is only for longer dialogs that the fine-grained aspects boost performance."
    }
}