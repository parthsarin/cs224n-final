{
    "article": "In this paper, we present the methods we used to extract bacteria and biotopes names and then to identify the relation between those entities while participating to the BioNLP'13 Bacteria and Biotopes Shared Task. We used machine-learning based approaches for this task, namely a CRF to extract bacteria and biotopes names and a simple matching algorithm to predict the relations. We achieved poor results: an SER of 0.66 in sub-task 1, and a 0.06 F-measure in both sub-tasks 2 and 3. Introduction The BioNLP'13 Bacteria and Biotopes shared task aims at extracting bacteria names (bacterial taxa) and biotopes names (bacteria habitats; geographical and organization entities). The task comprises three sub-tasks (Bossy et al., 2012b) . \u2022 Sub-task 1 aims at extracting habitat names and linking those names to the relevant concept from the OntoBiotope ontology. \u2022 Sub-task 2 aims at identifying relations between bacteria and habitats among two kinds of relations (localization, part-of) based on a ground truth corpus of bacteria and habitat names. The \"localization\" relation is the link between a bacterium and the place where it lives while the \"part-of\" relation is the relation between hosts and host parts (bacteria) (Bossy et al., 2012a) . \u2022 Sub-task 3 aims at extracting all bacteria and biotopes names (including both habitat and geographical names), and then identifying relations between these concepts. In this paper, we present the methods we designed as first time participant to the BioNLP Bacteria Biotopes Shared Task. Background Scientific documents provide useful information in many domains. Because processing those documents is time-consuming for a human, NLP techniques have been designed to process a huge amount of documents quickly. The microorganisms ecology domain involves a lot of microorganisms (bacteria, living and dead cells, etc.) and habitats (food, medical, soil, water, hosts, etc.) that have been described in details in the literature. NLP techniques would facilitate the access to information from scientific texts and make it available for further studies. Bacteria and biotopes identification has been addressed for the first time during the BioNLP 2011 Bacteria Biotopes shared task (Bossy et al., 2012a; Kim et al., 2011) . This task consisted in extracting bacteria location events from texts among eight categories (Host, HostPart, Geographical, Environment, Food, Medical, Water and Soil) . Three teams participated in this task. All systems followed the same process: in a first stage, they detected bacteria names, detected and typed locations; then, they used co-reference to link the extracted entities; the last stage focused on the event extraction. Bj\u00f6rne et al. (2012) adapted an SVM-based Named Entity Recognition system and used the list of Prokaryotic Names with Standing in Nomenclature. Nguyen and Tsuruoka (2011) used a CRF-based system and used the NCBI web page about the genomic BLAST. Ratkovic et al. (2012) designed an ad hoc rule-based system based on the NCBI Taxonomy. The participants obtained poor results (Table 1 ) which underlines the complexity of this task. Corpus analysis The bacteria names appear in the texts, either in their longer form (Xanthomonas axonopodis pv. citri), in a partial form (Xanthomonas) or in their abbreviated form (Xac). The abbreviations are case-sensitives since they follow the original form: MmmSC is derived from M. mycoides ssp mycoides SC. 1 A few bacteria names can appear in the text followed by a trigger word: Spirillum bacteria, but it will be abbreviated in the remainder of the text, sometimes with a higher degree of specificity: S. volutans standing for Spirillum volutans. 1 Mycoplasma mycoides subspecies mycoides Small Colony in its longer form. Methods This year, the BioNLP organizers encouraged the participants to use supporting resources in order to reduce the time-investment in the challenge. Those resources encompass sentence splitting, tokenization, syntactic parsing, and biological annotations. Moreover, a specific ontology has been released for the Bacteria Biotopes task. We used some of the resources provided and combined them with additional resources, in a machine-learning framework we specifically designed for this task. Linguistic resources The OntoBiotope Ontology OntoBiotope 2 is an ontology tailored for the biotopes domain. The BioNLP-ST 2013 version has been released in the OBO format. This ontology integrates 1,756 concepts. Each concept has been given a unique ID and is associated with exact terms and related synonyms. The concept is also defined in a \"is a\" relation. The normalization of the habitat names in the first sub-task must be based on this ontology. For example, the concept microorganism (unique id MBTO:00001516) is a living organism which unique id is MBTO:00000297. For this concept, microbe is an exact synonym while microbial is a related synonym (see Figure 1 ). [Term] id: MBTO:00001516 name: microorganism exact synonym: \"microbe\" [TyDI:23602] related synonym: \"microbial\" [TyDI:23603] is a: MBTO:00000297 ! living organism In order to help our system to identify the bacteria names, we built a list of 357,387 bacteria taxa based on the NCBI taxonomy database 3 (Federhen, 2012) . This taxonomy describes a small part (about 10%) of the living species on earth, based on public sequence databases. It includes twelve categories of information from the biological domain (bacteria, invertebrates, mammals, phages, plants, primates, rodents, synthetics, unassigned, viruses, vertebrates and environmental samples) . We extracted from this taxonomy all names belonging to the Bacteria category, which represent 24.3% of the content. This output includes a few variants of bacteria names (see Table 3 The Cocoa annotations Cocoa is a WebAPI annotator tool for biological text. 4 We used the Cocoa annotations provided by the organizers as part of the supporting resources. These annotations emphasize 37 pre-defined categories. We noticed a few categories are often tied with one of the three kinds of entities we have to process: \u2022 Bacteria: Cell, Chemical, Mutant Organism, Organism, Protein, Unknown; \u2022 Habitat: Body part, Cell, Cellular component, Chemical, Disease, Food, Geometrical part, Habitat, Location, Multi-tissue structure, Organism, Organism subdivision, Pathological formation, Tissue; \u2022 Geographical: Company, Habitat, Technique, Unknown. We believe these categories should be useful to identify bacteria and biotopes entities in the texts, and we used them as features in the CRF model (see column #10 in Table 4 ). System Formalisms Depending on the sub-task to process, we used two distinct formalisms implemented in the Wapiti tool (Lavergne et al., 2010) to build our models: \u2022 Conditional Random Fields (CRF) (Lafferty et al., 2001; Sutton and McCallum, 2006) to identify bacteria and biotopes names (subtasks 1 and 3). \u2022 Maximum Entropy (MaxEnt) (Guiasu and Shenitzer, 1985; Berger et al., 1996) to process the relationships between entities (subtasks 2 and 3). Bacteria biotopes features set We used several sets of features, including \"classical\" internal features (columns #4 to #7 in Table 4 : typographic, digit, punctuation, length) and a few semantic features. In table 4, we present a sample tabular file produced in order to train the CRF model. \u2022 Presence of the token in the NCBI taxonomy (column #9); \u2022 Presence of the token in the OntoBiotope ontology (column #8); \u2022 Category of the token based on the Cocoa annotations (column #10); \u2022 Unsupervised clusters (column #11) created using Brown's algorithm (Brown et al., 1992 ) with Liang's code 5 (Liang, 2005) . Taxonomy feature. We noticed that 1,169 tokens out of 1,229 (95.1%) tokens we identified in the NCBI taxonomy in both corpora correspond to a Bacteria name in the reference (Table 5 ). This characteristic should be useful to identify the bacteria names. OntoBiotope feature. Regarding the presence of the token in the OntoBiotope ontology, we noticed that 1,487 tokens out of 1,906 (78.0%) from both corpora correspond to a habitat name in the reference (Table 6 ). The identification of habitat names will benefit from this characteristic. \u2022 Bacterium and Localization (Habitat) for a \"localization\" relation, \u2022 Host and Part for a \"PartOf\" relation (between two entities being of the same type). For example, Bifidobacterium is a bacteria name, human and human gastrointestinal tract are two habitats (localizations). A \"localization\" relation can occur between Bifidobacterium and human while a \"PartOf\" relation occurs between human and human gastrointestinal tract. Basic approach. For the official submission, we did not use this model because of the following remaining problems: (i) a few relations we produced were not limited to the habitat category but also involved the geographical category, (ii) we did not manage the relations we produced in duplicate, and (iii) the weight our CRF system gave to each relation was not relevant enough to be used (for a relation involving A with B, C, and D, the same weight was given in each relation). All of those problems led us to process the relations between entities using a too much simple approach: we only considered if the relation between two entities from the test exists in the training corpus. This approach is not robust as it does not consider unknown relations. Results and Discussion Identification of bacteria and biotopes In this subsection, we present the results we achieved on the development corpus (Table 2 ) to identify bacteria and biotopes names without linking those names to the concept in the OntoBiotope ontology. We built the model on the training corpus and applied it on the development corpus. The evaluation has been done using the conlleval.pl script 6 (Tjong Kim Sang and Buchholz, 2000) that has been created to evaluate the results in the CoNLL-2000 Shared Task. We chose this script because it takes as input a tabular file which is commonly used in the machine-learning process. Nevertheless, the script does not take into account the offsets to evaluate the annotations, which is the official way to evaluate the results. We give in Table 7 the results we achieved. Those results show our system succeed to correctly identify the bacteria and biotopes names. Nevertheless, the biotopes names are more difficult to process than the bacteria names. Similarly, Kolluru et al. (2011) achieved better results on the bacteria category rather than on the habitat, confirming this last category is more difficult to process. There is still room for improvement, especially in order to improve the recall in each category. We plan to define some post-treatments so as to identify new entities and thus, increase the recall in those three categories. Official results SER Habitat entities normalization General results. The first sub-task is evaluated using the Slot Error Rate (Makhoul et al., 1999) , based on the exact boundaries of the entity to be detected and the semantic similarity of the concept from the ontology between reference and hypothesis (Bossy et al., 2012b) . This semantic similarity is based on the \"is a\" relation between two concepts. We achieved a 0.66 SER which places us 4th out of four participants. Other participants obtained SERs ranging from 0.46 to 0.49. Our system achieved high precision (0.62) but low recall (0.35). It produced two false positives and 144 false negatives. Out of 283 predicted habitats, 175.34 are correct. There was also a high number of substitutions (187.66). Correct entity, incorrect categorization. On the entity boundaries evaluation, our system SER (0.45) was similar to that of the other participants (from 0.46 to 0.42). We achieved a 1.00 precision, a 0.56 recall and a 0.71 F-measure (the best from all participants). Those results are consistent with those we achieved on the development corpus (Table 7 ) and confirm the benefit of using a CRF-based system for entity detection. While we correctly identified the habitat entities, the ontology categorization proved difficult: we achieved an SER of 0.62 while other participants obtained SERs ranging from 0.38 to 0.35. For this task, we relied on exact match for mapping the concept to be categorized and the concepts from the ontology, including both singular and plural forms match. When no match was found, because the categorization was mandatory, we provided a default identifier-the first identifier from the ontology-which is rarely correct. 7 Relationships between entities General results. The relation sub-task is evaluated in terms of recall and precision for the predicted relations. On both second and third subtasks, due to our too basic approach, we only achieved a 0.06 F-measure. Obviously, because considering only existing relations is not a robust approach, the recall is very low (R=0.04). The precision is not as high as we expected (P=0.19), which indicates that if a relation exists in the training corpus for two entities, this relation does not necessarily occur within the test for the two same entities (two entities can occur in the same text without any relation to be find between them). On the second sub-task, other participants obtained F-measures ranging from 0.42 to 0.27, while on the third sub-task, the other participants obtained a 0.14 F-measure, which underlines the difficulty of the relation task. Out of the two types of relation to be found, this simple approach yielded better results for the Localization relation (F=0.07) than for the PartOf relation (F=0.02). While our results are probably too bad to yield a definite conclusion, the results of other participant also reflect a difference in performance for relation Localization and PartOf. Improvements. After fixing the technical problems we encountered, we plan to test other algorithms such as SVM, which may be more adapted for this kind of task. Additional experiments After the official submission, we carried out additional experiments. Habitat entities normalization Beyond exact match The improvements we made on the habitat entities normalization are only based on the mapping between the predicted concept and the ontology. In our official submission, we only used an exact match. We tried to produce a more flexible mapping in several ways. First, we tried to normalize the mention gathering all words from the mention into a single word. Indeed, the concept \"rain forest\" is not found in the ontology while the concept \"rainforest\" in one word exists. Second, we split the mention into single words and tried matching based on the features listed below, in order to manage the subsumption of concepts. \u2022 all words except the first one: \"savannah\" instead of \"brazilian savannah\", \u2022 all words except the last one: \"glossina\" instead of \"glossina brevipalpis\", \u2022 the last three words (we did not find example in the corpus), \u2022 the first three words: \"sugar cane fields\" instead of \"sugar cane fields freshly planted with healthy plants\", \u2022 the last two words: \"tsetse fly\" instead of \"blood-sucking tsetse fly\", \u2022 and the first two words: \"tuberculoid granulomas\" instead of \"tuberculoid granulomas with caseous lesions\". If two parts of a mention can be mapped to two concepts in the ontology, we added both concepts in the output. We also extended the coverage of the ontology using the reference normalization from both training and development corpora, adding 316 entries in the ontology. Those new concepts can be considered either as synonyms or as hyponyms: \u2022 synonyms: \"root zone\" is a synonym of \"rhizosphere\". While only the second one occurs in the ontology, we added the first concept with the identifier from the second concept; \u2022 hyponyms: \"bacteriologist\" and \"entomologist\" are both hyponyms of \"researcher\". We gave the hypernym identifier to the hyponym concepts. At last, if no concept was found in the ontology, instead of using the identifier of the first concept in the ontology, we gave as a default identifier the one of the more frequent concept in the corpora. 8 This strategy improves system performance. Results The improvements we made allowed us to achieved better results on the test corpus (table 9 ). While on the official submission we achieved a 0.66 Slot Error Rate, we obtained a 0.53 SER thanks to the improvements we made. This new result does not lead us to obtain a better rank, but it is closer to the ones the other participants achieved (from 0.49 to 0.46). These improvements led us to obtain better recall, precision and F-measure. While our recall is still the lowest of all participants (0.48 vs. [0.60;0.72]), our precision is the highest (0.85 vs. [0.48;0.61]) and our F-measure is equal to the highest one (0.61 vs. [0.57;0.61]). Category Relationships between entities Processing On the relationships, as a first step, we fixed the problems that prevented us to use the Max-Ent model during the submission stage: (i) we produced correct files for the algorithm, removing the geographical entities from our processing accordingly with the guidelines, (ii) when dealing with all possible combinations of entities that can be linked together, we managed the relations so as not to produce those relations in duplicate, and (iii) we better managed the confidence score given by the CRF on each relation. Results We produced new models on the training corpus based on the following features: entities to be linked, category of each entity, and whether a relation between those entities exists in the training corpus. We performed two evaluations of those models: (i) on the development corpus, using the official evaluation script, and (ii) on the test corpus via the evaluation server. 9 As presented in We also noticed that we achieved very poor results on the test corpus while the evaluation on the development corpus provided promising results, with a F-measure decreasing from 0.26 to 0.02 on the first experiment, and from 0.66 to 0.04 on the second one. The difference between the results from both development and test corpora is hard to understand. We have to perform additional analyses on the outputs we produced to identify the problem that occurred. Moreover, we plan to use more contextual features (specific words that indicate the relation, distance between two entities, presence of relative pronouns, etc.) to improve the model. Indeed, in relations between concepts, not only the concepts must be studied but also the context in which they occur as well as the linguistic features used in the neighborhood of those concepts. Conclusion In this paper, we presented the methods we used as first time participant to the BioNLP Bacteria Biotopes Shared Task. To detect bacteria and biotopes names, we used a machine-learning approach based on CRFs. We used several resources to build the model, among them the NCBI taxonomy, the OntoBiotope ontology, the Cocoa annotations, and unsupervised clusters created through Brown's algorithm. The normalization of the habitat names with the concepts in the OntoBiotope ontology was performed with a Perl script based on exact match of the entity to be found, taking into account its plural form. On this sub-task, we achieved a 0.66 Slot Error Rate. In order to process the relationships between entities, our MaxEnt model was not ready for the official submission. The simple approach we used relies on the identification of the relation between entities only if the relation exists in the training corpus. This simple approach is not robust enough to correctly process new data. On the relation subtasks, due to the approach we used, we achieved a 0.06 F-measure. On the first sub-task, we enhanced our habitat entities normalization process, which led us to improve our Slot Error Rate from 0.66 (official submission) to 0.53 (additional experiments). On the relation detection, first, we plan to make new tests with more features, including contextual features. Second, we plan to test new algorithms, such as SVM which seems to be relevant to process relationships between entities. Acknowledgments This work has been done as part of the Quaero program, funded by Oseo, French State Agency for Innovation. I would like to thank the organizers for their work and Aur\u00e9lie N\u00e9v\u00e9ol for the proofread of this paper.",
    "abstract": "In this paper, we present the methods we used to extract bacteria and biotopes names and then to identify the relation between those entities while participating to the BioNLP'13 Bacteria and Biotopes Shared Task. We used machine-learning based approaches for this task, namely a CRF to extract bacteria and biotopes names and a simple matching algorithm to predict the relations. We achieved poor results: an SER of 0.66 in sub-task 1, and a 0.06 F-measure in both sub-tasks 2 and 3.",
    "countries": [
        "France"
    ],
    "languages": [
        "French"
    ],
    "numcitedby": "13",
    "year": "2013",
    "month": "August",
    "title": "Building A Contrasting Taxa Extractor for Relation Identification from Assertions: {BIO}logical Taxonomy {\\&} Ontology Phrase Extraction System"
}