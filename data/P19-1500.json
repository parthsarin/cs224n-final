{
    "article": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments a previously proposed Transformer architecture (Liu  et al., 2018)  with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines. 1 Introduction Automatic summarization has enjoyed renewed interest in recent years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018) . Multi-document summarization -the task of producing summaries from clusters of themati-cally related documents -has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are relatively small (in the range of a few hundred examples) for training neural models. In an attempt to drive research further, Liu et al. (2018) tap into the potential of Wikipedia and propose a methodology for creating a large-scale dataset (WikiSum) for multidocument summarization with hundreds of thousands of instances. Wikipedia articles, specifically lead sections, are viewed as summaries of various topics indicated by their title, e.g.,\"Florence\" or \"Natural Language Processing\". Documents cited in the Wikipedia articles or web pages returned by Google (using the section titles as queries) are seen as the source cluster which the lead section purports to summarize. Aside from the difficulties in obtaining training data, a major obstacle to the application of end-to-end models to multi-document summarization is the sheer size and number of source documents which can be very large. As a result, it is practically infeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them. Liu et al. (2018) propose a two-stage architecture, where an extractive model first selects a subset of salient passages, and subsequently an abstractive model generates the summary while conditioning on the extracted subset. The selected passages are concatenated into a flat sequence and the Transformer (Vaswani et al., 2017) , an architecture well-suited to language modeling over long sequences, is used to decode the summary. Although the model of Liu et al. (2018) takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000) . The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014) . Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information across multiple documents as opposed to simply concatenating text spans and feeding them as a flat sequence to the model. In this way, the model automatically learns richer structural dependencies among textual units, thus incorporating well-established insights from earlier work. Advantageously, the proposed architecture can easily benefit from information external to the model, i.e., by replacing inter-document attention with a graph-matrix computed based on the basis of lexical similarity (Erkan and Radev, 2004) or discourse relations (Christensen et al., 2013) . We evaluate our model on the WikiSum dataset and show experimentally that the proposed architecture brings substantial improvements over several strong baselines. We also find that the addition of a simple ranking module which scores documents based on their usefulness for the target summary can greatly boost the performance of a multi-document summarization system. Related Work Most previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013) , and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015) . Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018) , the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018) , where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2018) . Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summarization as a supervised machine learning prob- lem and for this purpose assume access to large, labeled datasets (i.e., source documents-summary pairs). In contrast to their approach, we use a learning-based ranker and our abstractive model can hierarchically encode the input documents, with the ability to learn latent relations across documents and additionally incorporate information encoded in well-known graph representations. Model Description We follow Liu et al. (2018) in treating the generation of lead Wikipedia sections as a multidocument summarization task. The input to a hypothetical system is the title of a Wikipedia article and a collection of source documents, while the output is the Wikipedia article's first section. Source documents are webpages cited in the References section of the Wikipedia article and the top 10 search results returned by Google (with the title of the article as the query). Since source documents could be relatively long, they are split into multiple paragraphs by line-breaks. More formally, given title T , and L input paragraphs {P 1 , \u2022 \u2022 \u2022 , P L } (retrieved from Wikipedia citations and a search engine), the task is to generate the lead section D of the Wikipedia article. Our summarization system is illustrated in Figure 1. Since the input paragraphs are numerous and possibly lengthy, instead of directly applying an abstractive system, we first rank them and summarize the L -best ones. Our summarizer follows the very successful encoder-decoder architecture (Bahdanau et al., 2015) , where the encoder encodes the input text into hidden representations and the decoder generates target summaries based on these representations. In this paper, we focus exclusively on the encoder part of the model, our decoder follows the Transformer architecture in-troduced in Vaswani et al. (2017) ; it generates a summary token by token while attending to the source input. We also use beam search and a length penalty (Wu et al., 2016) in the decoding process to generate more fluent and longer summaries. Paragraph Ranking Unlike Liu et al. (2018) who rank paragraphs based on their similarity with the title (using tf-idfbased cosine similarity), we adopt a learningbased approach. A logistic regression model is applied to each paragraph to calculate a score indicating whether it should be selected for summarization. We use two recurrent neural networks with Long-Short Term Memory units (LSTM; Hochreiter and Schmidhuber 1997) to represent title T and source paragraph P : {u t1 , \u2022 \u2022 \u2022 , u tm } = lstm t ({w t1 , \u2022 \u2022 \u2022 , w tm }) (1) {u p1 , \u2022 \u2022 \u2022 , u pn } = lstm p ({w p1 , \u2022 \u2022 \u2022 , w pn }) (2) where w ti , w pj are word embeddings for tokens in T and P , and u ti , u pj are the updated vectors for each token after applying the LSTMs. A max-pooling operation is then used over title vectors to obtain a fixed-length representation \u00fbt : \u00fbt = maxpool({u t1 , \u2022 \u2022 \u2022 , u tm }) (3) We concatenate \u00fbt with the vector u pi of each token in the paragraph and apply a non-linear transformation to extract features for matching the title and the paragraph. A second max-pooling operation yields the final paragraph vector p: p i = tanh(W 1 ([u pi ; \u00fbt ])) (4) p = maxpool({p 1 , \u2022 \u2022 \u2022 , p n }) (5) Finally, to estimate whether a paragraph should be selected, we use a linear transformation and a sigmoid function: s = sigmoid(W 2 ( p)) ( 6 ) where s is the score indicating whether paragraph P should be used for summarization. All input paragraphs {P 1 , \u2022 \u2022 \u2022 , P L } receive scores {s 1 , \u2022 \u2022 \u2022 , s L }. The model is trained by minimizing the cross entropy loss between s i and ground-truth scores y i denoting the relatedness of a paragraph to the gold standard summary. We adopt ROUGE-2 recall (of paragraph P i against gold target text D) as y i . In testing, input paragraphs are ranked based on the model predicted scores and an ordering {R 1 , \u2022 \u2022 \u2022 , R L } is gener- ated. The first L paragraphs {R 1 , \u2022 \u2022 \u2022 , R L } are selected as input to the second abstractive stage. Paragraph Encoding Instead of treating the selected paragraphs as a very long sequence, we develop a hierarchical model based on the Transformer architecture (Vaswani et al., 2017) to capture inter-paragraph relations. The model is composed of several local and global transformer layers which can be stacked freely. Let t ij denote the j-th token in the i-th ranked paragraph R i ; the model takes vectors x 0 ij (for all tokens) as input. For the l-th transformer layer, the input will be x l\u22121 ij , and the output is written as x l ij . Embeddings Input tokens are first represented by word embeddings. Let w ij \u2208 R d denote the embedding assigned to t ij . Since the Transformer is a nonrecurrent model, we also assign a special positional embedding pe ij to t ij , to indicate the position of the token within the input. To calculate positional embeddings, we follow Vaswani et al. (2017) and use sine and cosine functions of different frequencies. The embedding e p for the p-th element in a sequence is: e p [i] = sin(p/10000 2i/d ) (7) e p [2i + 1] = cos(p/10000 2i/d ) (8) where e p [i] indicates the i-th dimension of the embedding vector. Because each dimension of the positional encoding corresponds to a sinusoid, for any fixed offset o, e p+o can be represented as a linear function of e p , which enables the model to distinguish relative positions of input elements. In multi-document summarization, token t ij has two positions that need to be considered, namely i (the rank of the paragraph) and j (the position of the token within the paragraph). Positional embedding pe ij \u2208 R d represents both positions (via concatenation) and is added to word embedding w ij to obtain the final input vector x 0 ij : pe ij = [e i ; e j ] (9) x 0 ij = w ij + pe ij (10) Local Transformer Layer A local transformer layer is used to encode contextual information for tokens within each paragraph. The local transformer layer is the same as the vanilla transformer layer (Vaswani et al., 2017) , and composed of two sub-layers: h = LayerNorm(x l\u22121 + MHAtt(x l\u22121 )) (11) x l = LayerNorm(h + FFN(h)) ( 12 ) where LayerNorm is layer normalization proposed in Ba et al. (2016) ; MHAtt is the multihead attention mechanism introduced in Vaswani et al. ( 2017 ) which allows each token to attend to other tokens with different attention distributions; and FFN is a two-layer feed-forward network with ReLU as hidden activation function. Global Transformer Layer A global transformer layer is used to exchange information across multiple paragraphs. As shown in Figure 2 , we first apply a multi-head pooling operation to each paragraph. Different heads will encode paragraphs with different attention weights. Then, for each head, an inter-paragraph attention mechanism is applied, where each paragraph can collect information from other paragraphs by selfattention, generating a context vector to capture contextual information from the whole input. Finally, context vectors are concatenated, linearly transformed, added to the vector of each token, and fed to a feed-forward layer, updating the representation of each token with global information. Multi-head Pooling a z ij = W z a x l\u22121 ij (13) b z ij = W z b x l\u22121 ij (14) \u00e2z ij = exp(a z ij )/ n j=1 exp(a z ij ) (15) where W z a \u2208 R 1 * d and W z b \u2208 R d head * d are weights. d head = d/n head is the dimension of each head. n is the number of tokens in R i . We next apply a weighted summation with another linear transformation and layer normalization to obtain vector head z i for the paragraph: head z i = LayerNorm(W z c n j=1 a z ij b z ij ) (16) where W z c \u2208 R d head * d head is the weight. The model can flexibly incorporate multiple heads, with each paragraph having multiple attention distributions, thereby focusing on different views of the input. Inter-paragraph Attention We model the dependencies across multiple paragraphs with an inter-paragraph attention mechanism. Similar to self-attention, inter-paragraph attention allows for each paragraph to attend to other paragraphs by calculating an attention distribution: q z i = W z q head z i ( 17 ) k z i = W z k head z i (18) v z i = W z v head z i ( 19 ) context z i = m i=1 exp(q z i T k z i ) m o=1 exp(q z i T k z o ) v z i ( 20 ) where q z i , k z i , v z i \u2208 R d head * d head are query, key, and value vectors that are linearly transformed from head z i as in Vaswani et al. (2017) ; context z i \u2208 R d head represents the context vector generated by a self-attention operation over all paragraphs. m is the number of input paragraphs. Figure 2 provides a schematic view of inter-paragraph attention. Feed-forward Networks We next update token representations with contextual information. We first fuse information from all heads by concatenating all context vectors and applying a linear transformation with weight W c \u2208 R d * d : We then add c i to each input token vector x l\u22121 ij , and feed it to a two-layer feed-forward network with ReLU as the activation function and a highway layer normalization on top: c i = W c [context 1 i ; \u2022 \u2022 \u2022 ; context n head i ] (21) g ij = W o2 ReLU(W o1 (x l\u22121 ij + c i )) (22) x l ij = LayerNorm(g ij + x l\u22121 ij ) (23) where W o1 \u2208 R d f f * d and W o2 \u2208 R d * d f f are the weights, d f f is the hidden size of the feed-forward later. This way, each token within paragraph R i can collect information from other paragraphs in a hierarchical and efficient manner. Graph-informed Attention The inter-paragraph attention mechanism can be viewed as learning a latent graph representation (self-attention weights) of the input paragraphs. Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks (Liu and Lapata, 2018; Kim et al., 2017; Williams et al., 2018; Niculae et al., 2018; Fernandes et al., 2019) , much work in multi-document summarization has taken advantage of explicit graph representations, each focusing on different facets of the summarization task (e.g., capturing redundant information or representing passages referring to the same event or entity). One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model, to generate better summaries. We experimented with two well-established graph representations which we discuss briefly below. However, there is nothing inherent in our model that restricts us to these, any graph modeling relationships across paragraphs could have been used instead. Our first graph aims to capture lexical relations; graph nodes correspond to paragraphs and edge weights are cosine similarities based on tf-idf representations of the paragraphs. Our second graph aims to capture discourse relations (Christensen et al., 2013) ; it builds an Approximate Discourse Graph (ADG) (Yasunaga et al., 2017) over paragraphs; edges between paragraphs are drawn by counting (a) co-occurring entities and (b) discourse markers (e.g., however, nevertheless) connecting two adjacent paragraphs (see the Appendix for details on how ADGs are constructed). We represent such graphs with a matrix G, where G ii is the weight of the edge connecting paragraphs i and i . We can then inject this graph into our hierarchical transformer by simply substituting one of its (learned) heads z with G. Equation (20) for calculating the context vector for this head is modified as: context z i = m i =1 G ii m o=1 G io v z i (24) 4 Experimental Setup WikiSum Dataset We used the scripts and urls provided in Liu et al. (2018) to crawl Wikipedia articles and source reference documents. We successfully crawled 78.9% of the original documents (some urls have become invalid and corresponding documents could not be retrieved). We further removed clone paragraphs (which are exact copies of some parts of the Wikipedia articles); these were paragraphs in the source documents whose bigram recall against the target summary was higher than 0.8. For both ranking and summarization stages, we encode source paragraphs and target summaries using subword tokenization with Sentence-Piece (Kudo and Richardson, 2018) . Our vocabulary consists of 32, 000 subwords and is shared for both source and target. Paragraph Ranking To train the regression model, we calculated the ROUGE-2 recall (Lin, 2004) of each paragraph against the target summary and used this as the ground-truth score. The hidden size of the two LSTMs was set to 256, and dropout (with dropout probability of 0.2) was used before all linear layers. Adagrad (Duchi et al., 2011) with learning rate 0.15 is used for optimization. We compare our ranking model against the method proposed in Liu et al. (2018) who use the tf-idf cosine similarity between each paragraph and the article title to rank the input paragraphs. We take the first L paragraphs from the ordered paragraph set produced by our ranker and the similarity-based method, respectively. We concatenate these paragraphs and calculate their ROUGE-L recall against the gold target text. The results are shown in Table 1 . We can see that our ranker effectively extracts related paragraphs and produces more informative input for the downstream summarization task. Training Configuration In all abstractive models, we apply dropout (with probability of 0.1) before all linear layers; label smoothing (Szegedy et al., 2016) with smoothing factor 0.1 is also used. Training is in traditional sequence-to-sequence manner with maximum likelihood estimation. The optimizer was Adam (Kingma and Ba, 2014) with learning rate of 2, \u03b2 1 = 0.9, and \u03b2 2 = 0.998; we also applied learning rate warmup over the first 8, 000 steps, and decay as in (Vaswani et al., 2017) . All transformer-based models had 256 hidden units; the feed-forward hidden size was 1, 024 for all layers. All models were trained on 4 GPUs (NVIDIA TITAN Xp) for 500, 000 steps. We used gradient accumulation to keep training time for all models approximately consistent. We selected the 5 best checkpoints based on performance on the validation set and report averaged results on the test set. During decoding we use beam search with beam size 5 and length penalty with \u03b1 = 0.4 (Wu et al., 2016) ; we decode until an end-of-sequence token is reached. Comparison Systems We compared the proposed hierarchical transformer against several strong baselines: Lead is a simple baseline that concatenates the title and ranked paragraphs, and extracts the first k tokens; we set k to the length of the ground-truth target. LexRank (Erkan and Radev, 2004 ) is a widelyused graph-based extractive summarizer; we build a graph with paragraphs as nodes and edges weighted by tf-idf cosine similarity; we run a PageRank-like algorithm on this graph to rank and select paragraphs until the length of the ground-truth summary is reached. Flat Transformer (FT) is a baseline that applies a Transformer-based encoder-decoder model to a flat token sequence. We used a 6-layer transformer. The title and ranked paragraphs were concatenated and truncated to 600, 800, and 1, 200 tokens. T-DMCA is the best performing model of Liu et al. (2018) and a shorthand for Transformer Decoder with Memory Compressed Attention; they only used a Transformer decoder and compressed the key and value in selfattention with a convolutional layer. The model has 5 layers as in Liu et al. (2018) . Its hidden size is 512 and its feed-forward hidden size is 2, 048. The title and ranked paragraphs were concatenated and truncated to 3,000 tokens. Hierarchical Transformer (HT) is the model proposed in this paper. The model architecture is a 7-layer network (with 5 localattention layers at the bottom and 2 global attention layers at the top). The model takes the title and L = 24 paragraphs as input to produce a target summary, which leads to approximately 1, 600 input tokens per instance. Results Automatic Evaluation We evaluated summarization quality using ROUGE F 1 (Lin, 2004) . We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. forms FT, and even T-DMCA when the latter is presented with 3, 000 tokens. Adding an external graph also seems to help the summarization process. The similarity graph does not have an obvious influence on the results, while the discourse graph boosts ROUGE-L by 0.16. We also found that the performance of the Hierarchical Transformer further improves when the model is presented with longer input at test time. 2  As shown in the last row of Table 2 , when testing on 3, 000 input tokens, summarization quality improves across the board. This suggests that the model can potentially generate better summaries without increasing training time. Table 3 summarizes ablation studies aiming to assess the contribution of individual components. Our experiments confirmed that encoding paragraph position in addition to token position within each paragraph is beneficial (see row w/o PP), as well as multi-head pooling (w/o MP is a model where the number of heads is set to 1), and the global transformer layer (w/o GT is a model with only 5 local transformer layers in the encoder). Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments on 20 randomly selected test instances. Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018) . We created a set of questions based on the gold summary under the assumption that it contains the most important information from the input paragraphs. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary. The more questions a system can answer, the better it is at summarization. We created 57 questions in total varying from two to four questions per gold summary. Examples of questions and their answers are given in Table 5 . We adopted the same scoring mechanism used in Clarke and Lapata (2010) , i.e., correct answers are marked with 1, partially correct ones with 0.5, and 0 otherwise. A system's score is the average of all question scores. Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them taking into account the following criteria: Informativeness (does the summary convey important facts about the topic in question?), Fluency (is the summary fluent and grammatical?), and Succinctness (does the summary avoid repetition?). We used Best-Worst Scaling (Louviere et al., 2015) , a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017) . Participants were presented with the gold summary and summaries generated from 3 out of 4 systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard, taking into account the criteria mentioned above. The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst. Ratings range from \u22121 (worst) to 1 (best). Both evaluations were conducted on the Amazon Mechanical Turk platform with 5 responses per hit. Participants evaluated summaries produced by the Lead baseline, the Flat Transformer, T-DMCA, and our Hierarchical Transformer. All evaluated systems were variants that achieved the best performance in automatic evaluations. As shown in Table 4 , on both evaluations, participants overwhelmingly prefer our model (HT). All pairwise comparisons among systems are statistically significant (using a one-way ANOVA with posthoc Tukey HSD tests; p < 0.01). Examples of system output are provided in Table 5 . Pentagoet Archeological District GOLD The Pentagoet Archeological District is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Castine, Maine. It is the site of Fort Pentagoet, a 17th-century fortified trading post established by fur traders of French Acadia. From 1635 to 1654 this site was a center of trade with the local Abenaki, and marked the effective western border of Acadia with New England. From 1654 to 1670 the site was under English control, after which it was returned to France by the Treaty of Breda. The fort was destroyed in 1674 by Dutch raiders. The site was designated a National Historic Landmark in 1993. It is now a public park. QA What is the Pentagoet Archeological District? [a National Historic Landmark District] Where is it located? [Castine , Maine] What did the Abenaki Indians use the site for? [trading center] LEAD The Pentagoet Archeological District is a National Historic Landmark District located in Castine, Maine. This district forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In the colonial period, Abenakis frequented the fortified trading post at this site, bartering moosehides, sealskins, beaver and other furs in exchange for European commodities. \"Pentagoet Archeological district\" is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda. The Australian golden whistler (Pachycephala pectoralis) is a species of bird found in forest, woodland, mallee, mangrove and scrub in Australia (except the interior and most of the north) Most populations are resident, but some in south-eastern Australia migrate north during the winter. FT The Melanesian whistler (P. Caledonica) is a species of bird in the family Muscicapidae. It is endemic to Melanesia. T-DMCA The Australian golden whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae, which is endemic to Fiji. HT The Melanesian whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae, which is endemic to Fiji. Table 5 : GOLD human authored summaries, questions based on them (answers shown in square brackets) and automatic summaries produced by the LEAD-3 baseline, the Flat Transformer (FT), T-DMCA (Liu et al., 2018) and our Hierachical Transformer (HT). Conclusions In this paper we conceptualized abstractive multidocument summarization as a machine learning problem. We proposed a new model which is able to encode multiple input documents hierarchically, learn latent relations across them, and additionally incorporate structural information from well-known graph representations. We have also demonstrated the importance of a learning-based approach for selecting which documents to summarize. Experimental results show that our model produces summaries which are both fluent and in-formative outperforming competitive systems by a wide margin. In the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks. Acknowledgments We would like to thank Laura Perez-Beltrachini for her help with preprocessing the dataset. This research is supported by a Google PhD Fellowship to the first author. The authors gratefully acknowledge the financial support of the European Research Council (award number 681760). A Appendix We describe here how the similarity and discourse graphs discussed in Section 3.2.4 were created. These graphs were added to the hierarchical transformer model as a means to enhance summary quality (see Section 5 for details). A.1 Similarity Graph The similarity graph S is based on tf-idf cosine similarity. The nodes of the graph are paragraphs. We first represent each paragraph p i as a bag of words. Then, we calculate the tf-idf value v ik for each token t ik in a paragraph: where N w(t) is the count of word t in the paragraph, N d is the total number of paragraphs, and N dw (t) is the total number of paragraphs containing the word. We thus obtain a tf-idf vector for each paragraph. Then, for all paragraph pairs < p i , p i >, we calculate the cosine similarity of their tf-idf vectors and use this as the weight S ii for the edge connecting the pair in the graph. We remove edges with weights lower than 0.2. A.2 Discourse Graphs To build the Approximate Discourse Graph (ADG) D, we follow Christensen et al. (2013) and Yasunaga et al. (2017) . The original ADG makes use of several complex features. Here, we create a simplified version with only two features (nodes in this graph are again paragraphs). Co-occurring Entities For each paragraph p i , we extract a set of entities E i in the paragraph using the Spacy 3 NER recognizer. We only use entities with type {PERSON, NORP, FAC, ORG, GPE, LOC, EVENT, WORK OF ART, LAW}. For each paragraph pair < p i , p j >, we count e ij , the number of entities with exact match. Discourse Markers We use the following 36 explicit discourse markers to identify edges between two adjacent paragraphs in a source webpage: again, also, another, comparatively, furthermore, at the same time,however, immediately, indeed, instead, to be sure, likewise, meanwhile, moreover, nevertheless, nonetheless, notably, otherwise, regardless, similarly, unlike, in addition, even, in turn, in exchange, in this case, in any event, finally, later, as well, especially, as a result, example, in fact, then, the day before 3 https://spacy.io/api/entityrecognizer If two paragraphs < p i , p i > are adjacent in one source webpage and they are connected with one of the above 36 discourse markers, m ii will be 1, otherwise it will be 0. The final edge weight D ii is the weighted sum of e ii and m ii D ii = 0.2 * e ii + m ii (26)",
    "abstract": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments a previously proposed Transformer architecture (Liu  et al., 2018)  with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines. 1",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "Dutch",
        "French",
        "English"
    ],
    "numcitedby": "184",
    "year": "2019",
    "month": "July",
    "title": "Hierarchical Transformers for Multi-Document Summarization"
}