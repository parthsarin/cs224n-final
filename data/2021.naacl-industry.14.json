{
    "framing_question": "To what extent do the authors engage with the communities that are the subject of their paper, and do they claim a participatory approach?",
    "response": "The authors do not mention engaging with the communities that are the subject of their paper, nor do they claim a participatory approach.",
    "article": "Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, current neural machine translation systems usually deliver unsatisfying translation results of dialogue texts. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (ProDrop), punctuation dropping (PunDrop), and typos (DialTypo). In response to these challenges, we propose a joint learning method to identify omission and typo in the process of translating, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09% to 47.16%. The code and dataset are publicly available at https://github.com/rgwt123/DialogueMT. Introduction Remarkable progress has been made in Neural Machine Translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) in recent years, which has been widely applied in everyday life. A typical scenario for such application is translating dialogue texts, in particular the record of group chats or movie subtitles, which helps people of different languages understand cross-language chat and improve their comprehension capabilities. However, traditional NMT models translate texts in a sentence-by-sentence manner and focus on the formal text input, such as WMT news translation * Corresponding author. ( (Barrault et al., 2020) , while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues from parallel corpora, such as OpenSubtitles (Lison et al., 2019) , or leveraging speaker information for integrating dialogue context into neural models. Also, the lack of both training data and benchmark test set makes current dialogue translation models far from satisfying and need to be further improved. In this paper, we try to alleviate the aforementioned challenges in dialogue translation. We first analyze a fraction of a dialogue corpus and summarize three critical issues in dialogue translation, including ProDrop, PunDrop, and DialTypo. Then we design a Multi-Task Learning (MTLDIAL) approach that learns to self-correct sentences in the process of translating. The model's encoder part automatically learns how to de-noise the noise input via explicit supervisory signals provided by additional contextual labeling. We also propose three strong baselines for dialogue translation, including repair (REPAIRDIAL) and robust (ROBUSTDIAL) model. To alleviate the challenges arising from the scarcity of dialogue data, we use sub-documents in the bilingual parallel corpus to enable the model to learn from crosssentence context. Additionally as for evaluation, the most commonly used BLEU metric (Papineni et al., 2001) for NMT is not good enough to provide a deep look into the translation quality in such a scenario. Thus, we build a Chinese-English test set containing sentences with the issues in ProDrop, PunDrop and DialTypo, attached with the human translation and annotation. Finally, we get a test set of 300 dialogues with 1,931 parallel sentences. The main contributions of this paper are as follows: a) We analyze three challenges ProDrop, PunDrop and DialTypo, which greatly impact the understanding and translation of a dialogue. b) We propose a contextual multi-task learning method to tackle the analyzed challenges. c) We create a Chinese-English test set specifically containing those problems and conduct experiments to evaluate proposed method on this test set. Analysis on Dialogue Translation There were already some manual analyses of translation errors, especially in the field of discourse translation. Voita et al. (2019) study English-Russian translation and find three main challenges for discourse translation: deixis, ellipsis, and lexical cohesion. For Chinese-English translation, tense consistency, connective mismatch, and content-heavy sentences are the most common issues (Li et al., 2014) . Different from previous works, we mainly analyze the specific phenomena in dialogue translation. We begin with a study on a bilingual dialogue corpus (Wang et al., 2018) . 1 We translate source sentences into the target language at sentence level and compare translation results with reference at dialogue level. Around 1,000 dialogues are evaluated, and the results are reported in Table 2 . From the statistic, we observe two persistent dialogue translation problems: pronoun dropping (ProDrop), punctuation drop-  ping(PunDrop). The phenomenon is consistent with the issue we collect in practical Instant Messaging (IM) chat scenarios, except for typos since the analyzed dialogue corpus has been proofread to remove typos. Pronoun Dropping Pronouns are frequently omitted in pro-drop languages (Huang, 1989) , such as Chinese, Japanese, Korean, Vietnamese, and Slavic languages. Such phenomenon are more frequent in dialogue, where the interlocutors are both aware of what's omitted in the context. However, when translating a pro-drop language into a non-pro-drop language (e.g., English) 2 , it is hard to translate those omitted pronouns, resulting in grammatical errors or semantic inaccuracies in the target language. The first conversation in Table 1 is an example. Punctuation Dropping In dialogue scenarios, such as IM software, punctuation is often omitted and users tend to segment sentences with spaces. The problem becomes much serious in languages with no spaces, such as Chinese, Japanese, Korean, and Thai. Table 1 shows this phenomenon in Example (2). Dialogue Typos Typo repairing is another fundamental but very challenging practical problem. In dialogue translation, typos or misspellings are very common, which dramatically undermine the quality of translation output produced by machine translation. Table 1 shows this phenomenon in Example (3). Approach to NMTDIAL This section aims to propose a unified framework that facilitates NMT to correct noisy inputs in dialogue neural machine translation (NMTDIAL). The framework includes three different methods, which are REPAIRDIAL, ROBUSTDIAL and MTL-DIAL. 107 Nancy \u600e\u4e48 \u4e86 \uff1f \u5979 \u662f\u4e0d\u662f \u54ed \u4e86 \u554a Nancy \u600e\u4e48 \u4e86 \uff1f<sep> \u5979 \u662f\u4e0d\u662f \u54ed \u4e86 \u554a <eos> Nancy \u600e\u4e48 \u4e86 <sep> \u662f\u4e0d\u662f \u54ed \u4e86 \u963f Contextual Perturbation Example Generation The most challenging problem for NMTDIAL is the data distribution gap between training and inference stage, where the training data are clean sentence-level pairs while the test data are noisy dialogue-level conversations. To bridge the distribution gap, the first step is to generate perturbation examples based on training instances. The data generation mainly consists of two steps. The first step is to obtain sub-documents with cross-sentence context, and the second step is to generate examples with word perturbations within sub-documents. Figure 1a shows a complete process. Cross-sentence Context It is difficult to acquire dialog-level parallel training data. As an alternative approach, we use parallel document data to catch dependencies across sentences. Formally, let x d = {x (1) , x (2) , \u2022 \u2022 \u2022 , x (M ) } be a source-language document containing M source sentences. And y d = {y (1) , y (2) , \u2022 \u2022 \u2022 , y (M ) } is the corresponding target-language document containing the same number of sentences as that of the source document. To get more context information, we randomly sample consecutive sub-document pairs (x d , y d ) of N sentences (i.e., snippet pairs from aligned documents). We set N \u2208 [1, 10] in this paper. We use a special token <sep> as the separator to concatenate sentences into a parallel subdocument {(x d , y d )}, as shown in Figure 1a For ProDrop and PunDrop, we traverse source sentences of x d , discard pronouns/punctuation in these sentences with a probability of 30% and record deletion positions with corresponding labels (see details below); to construct a typo, we choose a word with a probability of 1%, of which 80% is replaced with one of its homophones according to T DialTypo and 20% is replaced with another random word. We determine these percentages by observing the generated perturbation data. For annotation labels, we tag correct words with 0, words of DialTypo with 1, ProDrop words with 2 and PunDrop words with 3. Finally we get x d , x d and their corresponding label sequences x , x . x is a sequence of all 0s. NMTDIAL Base Models With the created training data, we first introduce two methods for NMTDIAL as our strong baselines, which will be elaborated here for model comparison. REPAIRDIAL A natural way for NMTDIAL is to train a dialog repair model to transform dialogue inputs into forms that an ordinary NMT system can deal with. REPAIRDIAL involves training a repair model to transform x d to x d and a clean translation model that translates x d to y d . As a pipeline method, REPAIRDIAL may suffer from error propagation. ROBUSTDIAL We extend the robust NMT (Cheng et al., 2018) to dialogue-level translation. Specifically, we take both the original (x d , y d ) and the perturbated (x d , y d ) bilingual pairs as training instances. So the model is more resilient on dialogue translation. During the inference stage, the robust model directly translates raw inputs into the target language. MTLDIAL ROBUSTDIAL has the potential to handle translation problems caused by noisy dialogue inputs. However, the internal mechanism is rather implicit and in a black box. Therefore, the improvement is limited, and it is not easy to analyze the improvement. To address this issue, we introduce a contextaware multi-task learning method MTLDIAL for NMTDIAL. As shown in \u2462 of Figure 1b , the only difference is that we have a contextual labeling module based on the encoder. We denote the final layer output of the Transformer encoder as H. For each token h i in H = (h 1 , h 2 , ..., h m ), the probability of contextual labeling is defined as: P (p i = j|X) = sof tmax(W \u2022 h i + b)[j] (1) where X = (x 1 , x 2 , ..., x m ) is the input sequence, P (p i = j|X) is the conditional probability that token x i is labeled as j (j \u2208 0, 1, 2, 3 as defined above). Here we make the labeling module as simple as possible, so that the Transformer encoder can behave like BERT (Devlin et al., 2019) , learning more information related to perturbation and guiding the decoder to find desirable translations. During the training phrase, the model takes (x d , x d , x , x , y d ) as the training data. The learning process is driven by optimizing two objectives, corresponding to sequence labeling as auxiliary loss (L SL ) and machine translation as the primary loss (L M T ) in a multi-task learning framework. L SL = \u2212log(P ( x |x d ) + P ( x |x d )) (2) L M T = \u2212log(P (y d |x d ) + P (y d |x d )) (3) The two objective are linearly combined as the overall objective in learning. L = L M T + \u03bb \u2022 L SL (4) \u03bb is coefficient. During experiments, we set as follows according the best practice: \u03bb = max(1.0 \u2212 update_num 10 5 , 0.2) ( 5 ) where update_num is the number of updating steps during training. We introduce multi-task learning for two reasons: 1) The labeling performance reflects the model's understanding of sentences containing the mentioned phenomena. 2) Contextual Labeling can be seen as a pre-training process based on the BERTlike model, and explicit guidance can enable the encoder to learn more about the information we annotate. Modeling Dialogue Context The modes for exploring dialogue context during decoding can be divided into offline and online. For the offline setting, all sentences in a dialogue are concatenated one by one with <sep>. The concatenated sequence is translated, and the target translation for each sentence can be easily detected according to the separator <sep>. The offline mode can be used for dialogue translation where the entire source dialogue has already been available before translation (e.g., movie subtitles). However, we continuously get new source sentences for online chat and need to generate corresponding translations immediately. We refer to this mode as the online setting. We experiment with two online methods. One is online-cut where the current sentence is concatenated to the previous context with the separator <sep>. The trained NMTDIAL model then translates the concatenated sequence and the last target segment is used as the translation for the current source sentence. The other is online-fd. Online-fd is a force decoding method. It forces the decoder to use translated history and continues decoding instead of re-translating the entire concatenated sequence. Online-fd brings more consistent translation. Experiments Test Set For better evaluation of NMTDIAL, we create a Chinese-English test set covering all issues discussed above based on the corpus we analyze in the second section. Statistics on the built test set are displayed in Table 3 . Building such a test set is hard and time-consuming as we need to perform manual selection, translation and annotation. As for translation quality evaluation, we use other metrics in addition to BLEU. For PunDrop and DialTypo, we evaluate BLEU scores on sen- tences containing missing punctuation or typos according to the annotation information. As for ProDrop, we evaluate the translation quality by the percentage of correctly recovering and translating the dropped pronouns. Settings We adopt the Chinese-English corpus from WMT2020 3 , with about 48M sentence pairs, as our bilingual training data D. We select newstest2019 as the development set. After splicing, we get D doc with 1.2M pairs and corresponding perturbated dataset D and D doc with 48M and 1.2M pairs respectively. We use byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and limit the number of merge operations to a maximum of 30K. In our studies, all translation models are Transformer-big, including 6 layers for both encoders and decoders, 1024 dimensions for model, 4096 dimensions for FFN layers and 16 heads for attention. During training, we use label smoothing = 0.1 (Szegedy et al., 2016) , attention dropout = 0.1 and dropout (Hinton et al., 2012) with a rate of 0.3 for all other layers. We use Adam (Kingma and Ba, 2015) to train the NMT models. \u03b21 and \u03b22 of Adam are set to 0.9 and 0.98, the learning rate is set to 0.0005, and gradient norm 5. The models are trained with a batch size of 32,000 tokens on 8 Tesla V100 GPUs during training. During decoding, we employ beam search algorithm and set the beam size to 5. We use sacrebleu (Post, 2018) to calculate uncased BLEU-4 (Papineni et al., 2001) . Results of Offline Setting The offline mode aims at using the entire source dialogue for translation. We experiment with all the methods in the offline setting, and the results  We believe that this is due to the error propagation caused by the pipeline. From the specific indicators, we can draw the following conclusions: 1) DialTypo has a very obvious impact on BLEU, and the gap between BASE and GOLD+BASE is more than 12 points; 2) The recovery of ProDrop is a relatively difficult task. Although compared with BASE, the current best result of 47.16% has been greatly improved, but is still far away from the golden result 97.32%; 3) PunDrop seems to be a relatively easy task for each method to address. Results of Online Setting The online mode only makes use of previous context during translation. An extreme situation of online setting is that there is no context, that is, sentence-level translation. We show the results of all the methods on the test set at the sentence level in Table 5 . Despite the lack of context, our approaches can still bring general benefits. We find that ProDrop relies heavily on context, especially for MTLDIAL, where the absence of context results in a 12.38% drop in performance. This is in line 110  with our expectations, as in many cases machine translation system heavily depends on context to fulfill the dropped pronouns. We further experiment on how context lengths can affect NMTDIAL. The results are shown in Figure 2. In the online-cut setting, we can see that using previous few sentences as context may improve overall BLEU score, but continuously adding more preceding texts will lead to a continuous decline. Online-fd performs well because using historical translation records to continue decoding can bring more consistent translation results. For the recovery accuracy of ProDrop, online-cut is better than online-fd in contrast, because forced decoding may cause wrong pronoun transmission. Analysis Labeling Performance To better understand how our proposed MTLDIAL make sense, we calculate the labeling performance on both validation and test set. PunDrop, and 73.2% for DialTypo. When testing on the real test data, the performance on ProDrop has declined a lot because of the difference between synthetic training/validation data and real test data. Especially noteworthy is the fact that F1 score of DialTypo drops the most, reaching 26%, because of its low recall. It may be due to the considerable difference between the typos generated by our automatic method and the actual distribution. Effects of Pronoun Correcting We further explore the auto-correction of specific pronouns. As shown in Figure 3 , we can find that pronouns such as I/you, which occur mostly in the corpus, generally have a higher recovery success rate. We believe this is due to the data imbalance. Compared with BASE, MTLDIAL has a much better performance. While ProDrop recovery accuracy has been improved, it still has not achieved 50%. The most common error is that the model does not capture any context or captures previous inappropriate context. We summarize frequentlyoccurring recovery errors in Table 7 . Related Work Our work is related with both dialogue translation and robust training. Dialogue Translation There has been some work on building bilingual dialogue data sets for the translation task in recent years. Wang et al. (2016) propose a novel approach to automatically construct parallel discourse corpus for dialogue machine translation and release around 100K parallel discourse data with manual speaker and dialogue boundary annotation. Maruf et al. (2018) propose the task of translating Bilingual Multi-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task -machine translation for chats, 4 focusing on bilingual customer support chats (Farajian et al., 2020) . Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019) . Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construction, and the anonymous reviewers for suggestions. Deyi Xiong is partially supported by the Natural Science Foundation of Tianjin (Grant No. 19JCZDJC31400) and the Royal Society (London) (NAF\\R1\\180122).",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.006797894068009125,
        "foundation": 0.0003514892131878389,
        "none": 0.9999998063873687
    }
}