{
    "article": "In this paper, we apply a weakly-supervised learning approach for slot tagging using conditional random fields by exploiting web search click logs. We extend the constrained lattice training of T\u00e4ckstr\u00f6m et al. ( 2013 ) to non-linear conditional random fields in which latent variables mediate between observations and labels. When combined with a novel initialization scheme that leverages unlabeled data, we show that our method gives significant improvement over strong supervised and weakly-supervised baselines. Introduction A key problem in natural language processing (NLP) is to effectively utilize large amounts of unlabeled and partially labeled data in situations where little or no annotations are available for a task of interest. Many recent work tackled this problem mostly in the context of part-of-speech (POS) tagging by transferring POS tags from a supervised language via automatic alignment and/or constructing tag dictionaries from the web (Das and Petrov, 2011; Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) . In this work, we attack this problem in the context of slot tagging, where the goal is to find correct semantic segmentation of a given query, which is an important task for information extraction and natural language understanding. For instance, answering the question \"when is the new bill murray movie release date?\" requires recognizing and labeling key phrases: e.g., \"bill murray\" as actor and \"movie\" as media type. The standard approach to slot tagging involves training a sequence model such as a conditional random field (CRF) on manually annotated data. An obvious limitation of this approach is that it relies on fully labeled data, which is both difficult to adapt and changing tasks and schemas. Certain films, songs, and books become more or less popular over time, and the performance of models trained on outdated data will degrade. If not updated, models trained on live data feeds such as movies, songs and books become obsolete over time and their accuracy will degrade. In order to achieve high accuracy continuously data and even model schemas have to be refreshed on a regular basis. To remedy this limitation, we propose a weakly supervised framework that utilizes the information available in web click logs. A web click log is a mapping from a user query to URL link. For example, users issuing queries about movies tend to click on links from the IMDB.com or rottentomatoes.com, which provide rich structured data for entities such as title of the movie (\"The Matrix\"), the director (\"The Wachowski Brothers\"), and the release date (\"1999\") . Web click logs present an opportunity to learn semantic tagging models from large-scale and naturally occurring user interaction data (Volkova et al., 2013) . While some previous works (Li et al., 2009) have applied a similar strategy to incorporate click logs in slot tagging, they do not employ recent advances in machine learning to effectively leverage the incomplete annotations. In this paper, we pursue and extend learning from partially labeled sequences, in particular the approach of T\u00e4ckstr\u00f6m et al. (2013) . Instead of projecting labels from a high-resource to a low-resource languages via parallel text and word alignment, we project annotations from structured data found in click logs. This can be seen as a benefit since typically a much larger volume of click log data is available than parallel text for low-resource languages. We also extend the constrained lattice training method of T\u00e4ckstr\u00f6m et al. (2013) from linear CRFs to non-linear CRFs. We propose a perceptron training method for hidden unit CRFs (Maaten et al., 2011) that allows us to train with partially labeled sequences. We show that combined with a novel pretraining methodology that leverages large quantities of unlabeled data, this training method achieves significant improvements over several strong baselines. Model definitions and training methods In this section, we describe the two sequence models in our experiments: a conditional random field (CRF) of Lafferty et al. (2001) and a hidden unit CRF (HUCRF) of Maaten et al. (2011) . Note that since we only have partially labeled sequences, we need a technique to learn from incomplete data. For a CRF, we follow a variant of the training method of T\u00e4ckstr\u00f6m et al. (2013) . In addition, we make a novel extension of their method to train a HU-CRF from partially labeled sequences. The resulting perceptron-style algorithm (Figure 2 ) is simple but effective. Furthermore, we propose an initialization scheme that naturally leverages unlabeled data for training a HUCRF. Partially Observed CRF A first-order CRF parametrized by \u03b8 \u2208 R d defines a conditional probability of a label sequence y = y 1 . . . y n given an observation sequence x = x 1 . . . x n as follows: p \u03b8 (y|x) = exp(\u03b8 \u03a6(x, y)) y \u2208Y(x) exp(\u03b8 \u03a6(x, y )) where Y(x) is the set of all possible label sequences for x and \u03a6(x, y) \u2208 R d is a global feature function that decomposes into local feature functions \u03a6(x, y) = n j=1 \u03c6(x, j, y j\u22121 , y j ) by the first-order Markovian assumption. Given fully labeled sequences {(x (i) , y (i) )} N i=1 , the standard train-ing method is to find \u03b8 that maximizes the log likelihood of the label sequences under the model with l 2 -regularization: \u03b8 * = arg max \u03b8\u2208R d N i=1 log p \u03b8 (y (i) |x (i) ) \u2212 \u03bb 2 ||\u03b8|| 2 Unfortunately, in our problem we do not have fully labeled sequences. Instead, for each token x j in sequence x 1 . . . x n we have the following two sources of label information: \u2022 A set of allowed label types Y(x j ). (Label dictionary) \u2022 A label \u1ef9j transferred from a source data. (Optional: transferred label) Y(x j , \u1ef9j ) = {\u1ef9 j } if \u1ef9j is given Y(x j ) otherwise In addition to these existing constraints, we introduce constraints on the label structure. In our segmentation problem, labels are structured (e.g., some label types cannot follow certain others). We can easily incorporate this restriction by disallowing invalid label types as a post-processing step of the form: Y(x j , \u1ef9j ) \u2190 Y(x j , \u1ef9j ) \u2229 Y(x j\u22121 , \u1ef9j\u22121 ) where Y(x j\u22121 , \u1ef9j\u22121 ) is the set of valid label types that can follow Y(x j\u22121 , \u1ef9j\u22121 ). T\u00e4ckstr\u00f6m et al. (2013) define a conditional probability over label lattices for a given observation sequence x: p \u03b8 (Y(x, \u1ef9)|x) = y\u2208Y(x,\u1ef9) p \u03b8 (y|x) Given a label dictionary Y(x j ) for every token type x j and training sequences {(x (i) , \u1ef9(i) )} N i=1 where \u1ef9(i) is (possibly non-existent) transferred labels for x (i) and, the new training method is to find \u03b8 that maximizes the log likelihood of the label lattices: \u03b8 * = arg max \u03b8\u2208R d N i=1 log p \u03b8 (Y(x (i) , \u1ef9(i) )|x (i) ) \u2212 \u03bb 2 ||\u03b8|| 2 Since this objective is non-convex, we find a local optimum with a gradient-based algorithm. The gradient of this objective at each example (x (i) , \u1ef9(i) ) takes an intuitive form: \u2202 \u2202\u03b8 log p \u03b8 (Y(x (i) , \u1ef9(i) )|x (i) ) \u2212 \u03bb 2 ||\u03b8|| 2 = y\u2208Y(x (i) ,\u1ef9) p \u03b8 (y|x (i) )\u03a6(x (i) , y) \u2212 y\u2208Y(x (i) ) p \u03b8 (y|x (i) )\u03a6(x (i) , y) \u2212 \u03bb\u03b8 This is the same as the standard CRF training except the first term where the gold features \u03a6(x (i) , y (i) ) are replaced by the expected value of features in the constrained lattice Y(x (i) , \u1ef9). Partially Observed HUCRF While effective, a CRF is still a linear model. To see if we can benefit from nonlinearity, we use a HU-CRF (Maaten et al., 2011) : a CRF that introduces a layer of binary-valued hidden units z = z 1 . . . z n \u2208 {0, 1} for each pair of label sequence y = y 1 . . . y n and observation sequence x = x 1 . . . x n . A HUCRF parametrized by \u03b8 \u2208 R d and \u03b3 \u2208 R d defines a joint probability of y and z conditioned on x as follows: p \u03b8,\u03b3 (y, z|x) = exp(\u03b8 \u03a6(x, z) + \u03b3 \u03a8(z, y)) z \u2208{0,1} n y \u2208Y(x,z ) exp(\u03b8 \u03a6(x, z ) + \u03b3 \u03a8(z , y )) where Y(x, z) is the set of all possible label sequences for x and z, and \u03a6(x, z) \u2208 R d and \u03a8(z, y) \u2208 R d are global feature functions that decompose into local feature functions: \u03a6(x, z) = n j=1 \u03c6(x, j, z j ) \u03a8(z, y) = n j=1 \u03c8(z j , y j\u22121 , y j ) In other words, it forces the interaction between the observations and the labels at each position j to go through a latent variable z j : see Figure 1 for illustration. Then the probability of labels y is given by marginalizing over the hidden units, p \u03b8,\u03b3 (y|x) = z\u2208{0,1} n p \u03b8,\u03b3 (y, z|x) As in restricted Boltzmann machines (Larochelle and Bengio, 2008) , hidden units are conditionally independent given observations and labels. This allows for efficient inference with HUCRFs despite their richness (see Maaten et al. (2011) for details). Training with partially labeled sequences We extend the perceptron training method of Maaten et al. (2011) to train a HUCRF from partially labeled sequences. This can be viewed as a modification of the constrained lattice training method of T\u00e4ckstr\u00f6m et al. (2013) for HUCRFs. A sketch of our training algorithm is shown in Figure 2 . At each example, we predict the most likely label sequence with the current parameters. If this sequence does not violate the given constrained lattice, we make no updates. If it does, we predict the most likely label sequence within the con- Input: constrained lattices {(x (i) , \u1ef9(i) )} N i=1 , step size \u03b7 Output: HUCRF parameters \u0398 := {\u03b8, \u03b3} 1. Initialize \u0398 randomly. 2. Repeatedly select i \u2208 {1 . . . N } at random: (a) y * \u2190 arg max y\u2208Y(x (i) ) p \u0398 (y|x (i) ) (b) If y * \u2208 Y(x (i) , \u1ef9(i) ): i. y + \u2190 arg max y\u2208Y(x (i) ,\u1ef9 (i) ) p \u0398 (y|x (i) ) ii. Make parameter updates: \u0398 \u2190 \u0398 + \u03b7 \u00d7 \u2202 \u2202\u0398 p \u0398 (y + , z + |x (i) )\u2212 p \u0398 (y * , z * |x (i) ) where the following hidden units are computed in closed-form (see Gelfand et al. (2010) ): strained lattice. We treat this as the gold label sequence, and perform the perceptron updates accordingly (Gelfand et al., 2010) . Even though this training algorithm is quite simple, we demonstrate its effectiveness in our experiments. z + := arg max z p \u0398 (z|x (i) , y + ) z * := arg max z p \u0398 (z|x (i) , y * ) Initialization from unlabeled data Rather than initializing the model parameters randomly, we propose an effective initialization scheme (in a similar spirit to the pre-training methods in neural networks) that naturally leverages unlabeled data. First, we cluster observation types in unlabeled data and treat the clusters as labels. Then we train a fully supervised HUCRF on this clustered data to learn parameters \u03b8 for the interaction between observations and hidden units \u03a6(x, z) and \u03b3 for the interaction between hidden units and labels \u03a6(z, y). Finally, for task/domain specific training, we discard \u03b3 and use the learned \u03b8 to initialize the algorithm in Figure 2 . We hypothesize that if the clusters are nontrivially correlated to the actual labels, we can capture the interaction between observations and hidden units in a meaningful way. Mining Click Log Data We propose using search click logs which consist of queries and their corresponding web documents. Clicks are an implicit signal for related entities and information in the searched document. In this work, we will assume that the web document is structured and generated from an underlying database. Due to the structured nature of the web, this is not an unrealistic assumption (see Adamic and Huberman (2002) for discussion). Such structural regularities make obtaining annotated queries for learning a semantic slot tagger almost cost-free. As an illustration of how to project annotation, consider Figure 3 , where we present an example taken from queries about video games. In the figure, the user queries are connected to a structured document via a click log, and then the document is parsed and stored in a structured format. Then annotation types are projected to linked queries through structural alignment. In the following subsections we describe each step in our log mining approach in detail. Click Logs Web search engines keep a record of search queries, clicked document and URLs which reveal the user behavior. Such records are proven to be useful in improving the quality of web search. We focus on utilizing query-to-URL click logs that are essentially a mapping from queries to structured web documents. In this work, we use a year's worth of query logs (from July 2013 to June 2014) at a commercial search engine. We applied a simple URL normalization procedure to our log data including trimming and removal of prefixes, e.g. \"www\". Parsing Structured Web Document A simple wrapper induction algorithm described in Kushmerick (1997) is applied for parsing web documents. Although it involves manually engineering a rule-based parser and is therefore website-specific, a single wrapper often generates large amounts of data for large structured websites, for example IMDB. Furthermore, it is very scalable to large quantities of data, and the cost of writing such a rule-based sys- tem is typically much lower than the annotation cost of queries. Figure 4 shows the statistics of parsed web documents on 24 domains with approximately 500 template rules. One of the chosen domains in our experiment, Music, has over 130 million documents parsed by our approach. Annotation Projection via Structural Alignment We now turn to the annotation projection step where structural alignment is used to transfer type annotation from structured data to queries. Note that this is different from the word-based or phrase-based alignment scenario in machine translation since we need to align a word sequence to a type-value pair. Let us assume that we are given the user query as a word sequence, w = w 1 , w 2 , . . . , w n and a set of structured data, s = {s 1 , s 2 , . . . , s m }, where s i is a pair of slot-type and value. We define a measurement of dissimilarity between word tokens and slots, dist(w i , s j ) = 1 \u2212 sim(w i , s j ) where sim(\u2022, \u2022) is cosine similarity over character trigrams of w i and s j . Next we construct a n-by-n score matrix S of which element is max j dist(w t ...t , s j ) meaning that a score of the most similar type-value s j and a segment {t . . . t} where 1 \u2264 t < t \u2264 n. Finally, given this approximate score matrix S, we use a dynamic programming algorithm to find the optimal segments to minimize the objective function: T (t) = min t <t T (t )S(t , t). Our approach results in a large amount of high-quality partially-labeled data: 314K, 1.2M, and 1.1M queries for the Game, Movie and Music domain, respectively. Experiments To test the effectiveness of our approach, we perform experiments on a suite of three entertainment domains for slot tagging: queries about movies, music, and games. For each domain, we have two types of data: engineered data and log data. Engineered data is a set of synthetic queries to mimic the behavior of users. This data is created during development at which time no log data is available. Log data is a set of queries created by actual users using deployed spoken dialogue systems: thus it is directly transcribed from users' voice commands with automatic speech recognition (ASR). In general we found log data to be fairly noisy, containing many ASR and grammatical errors, whereas engineered data consisted of clean, well-formed text. Not surprisingly, synthetic queries in engineered data are not necessarily representative of real queries in log data since it is difficult to accurately simulate what users' queries will be before a fully functioning system is available and real user data can be gathered. Hence this setting can greatly benefit from weakly-supervised learning methods such as ours since it is critical to learn from new incoming log data. We use search engine log data to project lattice constraints for weakly supervised learning. In this setup, a user issues a natural language query to retrieve movies, music titles, games and/or information there of. For instance, a user could say \"play the latest batman movie\" or \"find beyonce's music\". Our slot sequence tagger is trained with variants of CRF using lexical features, gazetteers, Brown clusters and context words. The domains consist of 35 slot types for movies, 25 for music and 24 for games. Slot types correspond to both named entities (e.g., game name, music title, movie name) as well as more general categories (genre, media type, description). Discrepancy between Engineered Data and Log Data To empirically highlight the need for learning from real user queries, we first train a standard CRF on the (fully labeled) engineered data and test it on the log data. We have manually annotated some log data for evaluation purposes. For features in the CRF, we use n-grams, gazetteer, and clusters. The clusters were induced from a large body of unlabeled data which consist of log data and click log data. Table 2 shows the F1 scores in this experiment. They indicate that a model fully supervised with engineered data performs very poorly on log data. The difference between the scores within engineered data and the scores in log data is very large (29.05 absolute F1). Experiments with CRF Variants Our main contribution is to leverage search log data to improve slot tagging in spoken dialogue systems. In this section, we assume that we have no log data in training slot taggers. 78.93 46.81 76.46 67.40 POHCRF+ 79.28 47.35 78.33 68.32 Table 3 : The F1 performance of variants of CRF across three domains, test on log data average perceptron. We did not see a significant difference between perceptron and LBFGS in accuracy, but perceptron is faster and thus favorable for training complex HUCRF models. We used 100 as the maximum iteration count and 1.0 for the L2 regularization parameter. The number of hidden variables per token is set to 300. The same features described in the previous section are used here. We perform experiments with the following CRF variants (see Section 2): \u2022 CRF: A fully supervised linear-chain CRF trained with manually labeled engineered samples. \u2022 POCRF: A partially observed CRF of T\u00e4ckstr\u00f6m et al. (2013) trained with both manually labeled engineered samples and click logs. \u2022 POHUCRF: A partially observed hidden unit CRF (Figure 2 ) trained with both manually labeled engineered samples and click logs. \u2022 POHUCRF+: POHUCRF with pre-training. Weakly-Supervised Learning without Projected Annotations via Pre-Training We also present experiments within Cortana personal assistant domain where the click log data is not available. The amount of training data we used was from 50K to 100K across different domains and the test data was from 5k to 10k. In addition, the unlabeled log data were used and their amount was from 100k to 200k. In this scenario, we have access to both engineered and log data to train a model. However, we do not have access to web search click log data. The goal of these experiments is to show the effectiveness of the HUCRF and pre-training method in the absence of weakly supervised labels projected via click logs. Table 4 shows a series of experiments on eight domains. For all domains other than alarm, using non-linear CRF (HUCRF) improve performance from 90.12% to 90.75% on average. Initializing HUCRF with pretraining (HUCRF+) boosts the performance up to 91.08%, corresponding to a 10% decrease in error relative to a original CRF. Notably in the weather and reminder domains, we have relative error reduction of 23 and 16%, respectively. We speculate that pretraining is helpful because it provides better initialization for training HUCRF: initialization is important since the training objective of HUCRF is non-convex. In general, we find that HUCRF delivers better performance than standard CRF: when the training procedure is initialized with pretraining (HUCRF+), it improves further. Related Work Previous works have explored weakly supervised slot tagging using aligned labels from a database as constraints. Wu and Weld (2007) train a CRF on heuristically annotated Wikipedia articles with relations mentioned in their structured infobox data. Li et al. (2009) applied a similar strategy incorporating structured data projected through click-log data as both heuristic labels and additional features. Knowledge graphs and search logs have been also considered as extra resources (Liu et al., 2013; El-Kahky et al., 2014; Anastasakos et al., 2014; Sarikaya et al., 2014; Marin et al., 2014) . Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006) . T\u00e4ckstr\u00f6m et al. (2013) investigate weakly supervised POS tagging in low-resource languages, combining dictionary constraints and labels projected across languages via parallel corpora and automatic alignment. Our work can be seen as an extension of their approach to the structured-data projection setup presented by Li et al. (2009) . A notable component of our extension is that we introduce a training algorithm for learning a hidden unit CRF of Maaten et al. (2011) from partially labeled sequences. This model has a set of binary latent variables that introduce non-linearity by mediating between observations and labels. Conclusions In this paper, we applied weakly-supervised learning approach for slot tagging, projecting annotations from structured data to user queries by leveraging click log data. We extended the T\u00e4ckstr\u00f6m et al. (2013) model to nonlinear CRFs by introducing latent variables and applying a novel pre-training methodology. The proposed techniques provide an effective way to leverage incomplete and ambiguous annotations from large amounts of naturally occurring click log data. All of our improvements taken together result in a 21% error reduction over vanilla CRFs trained on engineered data used during system development.",
    "abstract": "In this paper, we apply a weakly-supervised learning approach for slot tagging using conditional random fields by exploiting web search click logs. We extend the constrained lattice training of T\u00e4ckstr\u00f6m et al. ( 2013 ) to non-linear conditional random fields in which latent variables mediate between observations and labels. When combined with a novel initialization scheme that leverages unlabeled data, we show that our method gives significant improvement over strong supervised and weakly-supervised baselines.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "28",
    "year": "2015",
    "month": "May{--}June",
    "title": "Weakly Supervised Slot Tagging with Partially Labeled Sequences from Web Search Click Logs"
}