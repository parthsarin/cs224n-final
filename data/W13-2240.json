{
    "article": "This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation (MT) output. Sentence-level ranking of alternative MT outputs is done with pairwise classifiers using Logistic Regression with blackbox features originating from PCFG Parsing, language models and various counts. Post-editing time prediction uses regression models, additionally fed with new elaborate features from the Statistical MT decoding process. These seem to be better indicators of post-editing time than blackbox features. Prior to training the models, feature scoring with ReliefF and Information Gain is used to choose feature sets of decent size and avoid computational complexity. Introduction During the recent years, Machine Translation (MT) has reached levels of performance which allow for its integration into real-world translation workflows. Despite the high speed and various advantages of this technology, the fact that the MT results are rarely perfect and often require manual corrections has raised a need to assess their quality, predict the required post-editing effort and compare outputs from various systems on application time. This has been the aim of current research on Quality Estimation, which investigates solutions for several variations of such problems. We describe possible solutions for two problems of MT Quality Estimation, as part of the 8th Shared Task on Machine Translation: (a) sentence-level quality ranking (1.2) of multiple translations of the same source sentence and (b) prediction of post-editing time (1.3). We present our approach on acquiring (section 2.1) and selecting features (section 2.2), we explain the generation of the statistical estimation systems (section 2.3) and we evaluate the developed solutions with some of the standard metrics (section 3). 2 Methods: Quality Estimation as machine learning These two Quality Estimation solutions have been seen as typical supervised machine learning problems. MT output has been given to humans, so that they perform either (a) ranking of the multiple MT system outputs in terms of meaning or (b) postediting of single MT system output, where time needed per sentence is measured. The output of these tasks has been provided by the shared task organizers as a training material, whereas a small keep-out set has been reserved for testing purposes. Our task is therefore to perform automatic quality analysis of the translation output and the translation process in order to provide features for the supervised machine learning mechanism, which is then trained over the corresponding to the respective human behaviour. The task is first optimized in a development phase in order to produce the two best shared task submissions for each task. These are finally tested on the keep-out set so that their performance is compared with the ones submitted by all other shared-task participants. Feature acquisition We acquire two types of sentence-level features, that are expected to provide hints about the quality of the generated translation, depending on whether they have access to internal details of the MT decoding process (glass-box) or they are only derived from characteristics of the processed and generated sentence text (black-box). Black-box features Features of this type are generated as a result of automatic analysis of both the source sentence and the MT output (when applicable), whereas many of them are already part of the baseline infrastructure. For all features we also calculate the ratios of the source to the target sentence. These features include: PCFG Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of VPs, NPs etc.), the parse log-likelihood and the number of the n-best parse trees generated (Avramidis et al., 2011) . Rule-based language correction is a result of hand-written controlled language rules, that indicate mistakes on several pre-defined error categories (Naber, 2003) . We include the number of errors of each category as a feature. Language model scores include the smoothed n-gram probability and the n-gram perplexity of the sentence. Count-based features include count and percentage of tokens, unknown words, punctuation marks, numbers, tokens which do or do not contain characters \"a-z\"; the absolute difference between number of tokens in source and target normalized by source length, number of occurrences of the target word within the target hypothesis averaged for all words in the hypothesis (type/token ratio). Source frequency: A set of eight features includes the percentage of uni-grams, bi-grams and tri-grams of the processed sentence in frequency quartiles 1 (lower frequency words) and 4 (higher frequency words) in the source side of a parallel corpus (Callison-Burch et al., 2012) . Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007) , using the other translations as references (Soricut et al., 2012) . Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. Counts from the best hypothesis: Count of phrases, tokens, average/minimum/maximum phrase length, position of longest and shortest phrase in the source sentence; count of words unknown to the phrase table, average number of unknown words first/last position of an unknown word in the sentence normalized to the number of tokens, variance and deviation of the position of the unknown words. Log probability (pC) and future cost estimate (c) of the phrases chosen as part of the best translation: minimum and maximum values and their position in the sentence averaged to the number of sentences, and also their average, variance, standard deviation; count of the phrases whose probability or future cost estimate is lower and higher than their standard deviation; the ratio of these phrases to the total number of phrases. Alternative translations from the search path of the decoder: average phrase length, average of the average/variance/standard deviation of phrase log probability and future cost estimate, count of alternative phrases whose log probability or future cost estimate is lower and higher than their standard deviation. Feature selection Feature acquisition results in a huge number of features. Although the machine learning mechanisms already include feature selection or regularization, huge feature sets may be unusable for training, due to the high processing needs and the sparsity or noise they may infer. For this purpose we first reduce the number of features by scoring them with two popular correlation measurement methods. Information gain Information gain (Hunt et al., 1966) estimates the difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif-ferent classes (Kononenko, 1994) . It picks up a number of instances in random and calculates a feature contribution based on the nearest hits and misses. It is a robust method which can deal with incomplete and noisy data (Robnik-\u0160ikonja and Kononenko, 2003) . Machine learning algorithms Machine learning is performed for the two subtasks using common pairwise classification and regression methods, respectively. Ranking with pairwise binary classifiers For the sub-task on sentence-ranking we used pairwise classification, so that we can take advantage of several powerful binary classification methods (Avramidis, 2012) . We used logistic regression, which optimizes a logistic function to predict values in the range between zero and one (Cameron, 1998) , given a feature set X: P (X) = 1 1 + e \u22121(a+bX) (1) The logistic function is fitted using the Newton-Raphson algorithm to iteratively minimize the least squares error computed from training data (Miller, 2002) . Experiments are repeated with two variations of Logistic Regression concerning internal features treatment: Stepwise Feature Set Selection (Hosmer, 1989 ) and L2-Regularization (Lin et al., 2007) . Regression For the sub-task on post-editing time prediction, we experimented with several regression methods, such as Linear Regression, Partial Least Squares (Stone and Brooks, 1990) , Multivariate Adaptive Regression Splines (Friedman, 1991) , LASSO (Tibshirani, 1996) , Support Vector Regression (Basak et al., 2007) and Tree-based regressors. Indicatively, Linear regression optimizes coefficient \u03b2 for predicting a value y, given a feature vector X: y = X\u03b2 + \u03b5 (2) Evaluation The ranking task is evaluated by measuring correlation between the predicted and the human ranking, with the use of Kendall tau (Kendall, 1938) including penalization of ties. We additionally consider two more metrics specialized in ranking tasks: Mean Reciprocal Rank -MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain -NDGC (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) , which give better scores to models when higher ranks (i.e. better translations) are ordered correctly, as these are more important than lower ranks. The regression task is evaluated in terms of Root Mean Square Error (RMSE) and Mean Average Error (MAE). Experiment and Results Implementation Relieff is implemented for k=5 nearest neighbours sampling m=100 reference instances. Information gain is calculated after discretizing features into n=100 values N-gram features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl (Koehn, 2005) and News Commentary (Callison-Burch et al., 2011) . PCFG parsing features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul\u00e9 et al., 2008) . The open source language tool 1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Dem\u0161ar et al., 2004) . Sentence-ranking The sentence-ranking sub-task has provided training data for two language pairs, German-English and English-Spanish. For both sentence pairs, we train the systems using the provided annotated data sets WMT2010, WMT2011 and WMT2012, while the data set WMT2009 is used for the evaluation during the development phase. Data sets are analyzed with black-box feature generation. For each language pair, the two systems with the highest correlation are submitted. We start the development with two feature sets that have shown to perform well in previous experiments: #24 (Avramidis, 2012) including features from PCFG parsing, and #31 which is the baseline feature set of the previous year's shared task (Callison-Burch et al., 2012) For German-English, we experiment with 14 feature sets, using both variations of Logistic Regression. The two highest tau scores are given by Stepwise Feature Set Selection using feature sets #33 and #24. We see that although baseline features #31 alone have very low correlation, when combined with previously successful #24, provide the best system in terms of tau. Feature set #431 (which combines the 15 features scored higher with ReliefF, the 15 features scored higher with Information Gain and the feature set #24) succeeds pretty well on the additional metrics MRR and NDGC, but it provides slightly lower tau correlation. For English-Spanish, the correlation of the produced systems is significantly lower and it appears that the L2-regularized logistic regression performs better as classification method. We experiment with 24 feature sets, after more scoring with ReliefF and Inf. Gain. Surprisingly enough, Kendall tau on counts of numbers and punctuation, combined with contrastive BLEU score. This seems to rather overfit a peculiarity of the particular development set and indeed performs much lower on the final test set of the shared task (tau=0.04). The second best feature set (#431) has been described above and luckily generalizes better on an unknown set. It is interesting to see that this issue would have been avoided, if the decision was taken based on the ranking metrics MRR and NDGC, which prioritize other feature sets. We assume that further work is needed to see whether these measures are more expressive and reliable than Kendall tau for similar tasks. The fitted \u03b2 coefficients (in tables 3 and 4) give an indication of the importance of each feature (see equation 1), for each language pair. In both language pairs, target-side features prevail upon other features. On the comparison of the models for the two language pairs (and the \u03b2 coefficients as well) we can see that the model settings and performance may vary from one language pair to another. This also requires further investigation, given that Kendall tau and the other two metrics indicate different models as the best ones. The fact that the German-English set is better fitted with Stepwise Feature Set Selec-set features #24 From previous work (Avramidis, 2012) English-Spanish by using L2-regularization tion, whereas the English-Spanish one with L2-Regularization (table 5 ) may be explained by the statistical theory about these two methods: The Stepwise method has has been proven to be too bound to particular characteristics of the development set (Flom and Cassell, 2007) . L2-Regularization has been suggested as an alternative, since it generalizes better on broader data sets, which is probably the case for English-Spanish. Our method also seems to perform well when compared to evaluation metrics which have access to reference translations, as shown in this year's Metrics Shared Task (Mach\u00e1\u010dek and Ond\u0159ej, 2013) . Post-editing time prediction The training for the model predicting post-editing time is performed over the entire given data set and the evaluation is done with 10-fold crossvalidation. We evaluated 8 feature sets with 6 regression methods each, ending up with 48 experiments. The evaluation of the most indicative regression models (two best performing ones per feature set) can be seen in Table 6 . We start with a glass- box feature set, scored with ReliefF and consequently add black-box features. We note the models that have the lowest Root Mean Square Error and Mean Average Error. Our best model seems to be the one built linear regression using feature set #6. This feature set is chosen by collecting the 17 best features as scored by ReliefF and includes both black-box and glassbox features. How well this model fits the development test is represented in Figure 1 . The second best feature set (#8) includes 29 glass-box features with the highest absolute Reli-efF, joined with the black-box features of the successful feature set #6. More details about the contribution of the most important features in the linear regression (equation 2) can be seen in table 7 , where the fitted \u03b2 coefficients of each feature are given. The vast majority of the best contributing features are glassbox features. Some draft conclusions out of the coefficients may be that post-editing time is lower when: \u2022 the longest of the source phrases used for producing the best hypothesis appears closer to the end of the sentence \u2022 the phrases with the highest and the lowest probability appear closer to the end of the translated sentence \u2022 there are more determiners in the source and/or less determiners in the translation \u2022 there are more verbs in the translation and/or less verbs in the source \u2022 there are fewer alternative phrases with very high probability Further conclusions can be drawn after examining these observations along with the exact operation of the statistical MT system, which is subject to further work. Conclusion We describe two approaches for two respective problems of quality estimation, namely sentencelevel ranking of alternative translations and prediction of time for post-editing MT output. We present efforts on compiling several feature sets and we examine the final contribution of the features after training Machine Learning models. Elaborate decoding features seem to be quite helpful for predicting post-editing time. Acknowledgments This work has been developed within the TaraX \u0170 project, financed by TSB Technologiestiftung Berlin -Zukunftsfonds Berlin, co-financed by the European Union -European fund for regional development. Many thanks to Prof. Hans Uszkoreit for the supervision, Dr. Aljoscha Burchardt, and Dr. David Vilar for their useful feedback and to Lukas Poustka for his technical help on feature acquisition."
}