{
    "article": "IWPT 2000, the Sixth International Workshop on Parsing Technologies, marks the existence of twelve years of parsing workshops, starting in 1989 with the workshop organized by Masaru Tomita in Pittsburgh and Hidden Valley, Pennsylvania. IWPT'89 was followed by four workshops in a biennial rhythm: IWPT'91 in Cancun (Mexico) IWPT'93 in Tilburg (The Netherlands) and Durbuy (Belgium) IWPT'95 in Prague and Karlovy Vary (Czech Republic) IWPT'97 in Boston/ Cambridge (Massachusetts). The series has achieved a number of successes, becoming the major forum for researchers in parsing technology to meet and discuss advances in the field, attracting a steady number of participants and submitted papers, and resulting in three books: Introduction This paper briefly surveys three research directions we hope will positively impact the field of parsing: parser output combination, automatic rule sequence learning, and manual rule sequence creation. Parser Combination Combining Off-the-Shelf Parsers There has been a great deal of recent research in the machine learning community on combining the outputs of multiple classifiers to improve accuracy. In [4] , we demonstrated that taking three off-the-shelf part of speech taggers and \ufffdombining their outputs results in a significant performance improvement over any single tagger. More recently [8] , we have demonstrated that the same is true for parsing. We took three statistical parsers that had been trained on a portion of the Penn Treebank [5, 6, 10] . We explored two methods of combination: constituent voting and parser switching. In constituent voting, each parser posits a set of constituents with weights (votes). We take the union of all posited constituents for a sentence, accumulating their weights, and then discard all constituents with a weight below a threshold. In the simplest version of constituent voting, each parser gives a weight of 1 to any constituent it posits, and then we retain all constuents posited by more than half of the parsers. We can also weight the vote according to the accuracy of the individual parser, or according to an estimate of the accuracy of an individual parser on a particular constituent type in a particular context. It is possible that, for instance, one parser will be very accurate at predicting noun phrases and very inaccurate at predicting prepositional phrases that occur before the main verb. In fact, in studying conditional weights, we were unable to achieve better performance than that achieved using the simplest weighting scheme. This could be either due to our lack of creativity, or an indication that the three parsers we used do not differ significantly in behavior across linguistic types or contexts. In parser switching, each parser outputs a parse and our algorithm chooses to keep one of these parses. We can define a distance measure over trees, and then given a set T of parser outputs for a particular sentence, we would like to output the centroid tree for that set. In other words, we want to output: argmin Ti L T i ET Dist(Ti, T i ). The motivation for this is that we can think of there being a true parse for a sentence, and then each parser makes random errors, independent of the errors made by the other parsers. Since finding the true centroid is intractible, we instead output: argmin TiET L T i ET Dist(Ti, T j )- In table 1 , we show the results obtained using both constituent voting and parser switching. We see that both combination techniques improve upon the best statistical parser, and indeed the re\ufffdults obtained using constuent voting are the highest accuracy numbers reported for the Wall Street Journal to date. Generating A Set of Parsers We have also explored how we can exploit the advantages of classifier combination when we have access to only one trainable parser [7, 9] . One technique for doing so is known as bagging [1] . In bagging, we take a single training set of size M and from it generate a new training set by sampling with replacement M times from the original set. We can do this multiple times to create multiple training sets. Then each derived training set is used to train a separate parser. We generated 18 bags from a Czech treebank [7] and then used each bag to train a Collins Parser [6] . We then used simple constituent voting to combine the outputs of these 18 parsers on test data. Figure 1 shows that the number of bagged parsers that posit a constituent correlates nicely with the accuracy of that constituent. When only one of the 18 parsers outputs a constituent, that constituent is correct less than 10% of the time. When all 18 parsers output a constituent, that constituent is \u25a1- ---------------------------------__ correct more than 90% of the time. Table 2 shows the results on Czech test data of a single Collins parser trained on the entire training corpus, compared to using bagging to generate 18 parsers and then combining the outputs of these parsers. Similar results have been obtained for English. Transformation-Based Parsing There have been great advances recently in the accuracy of parsers that are automatically trained from parsed corpora. One disadvantage of these grammar induction methods is that the derived linguistic knowledge is captured in opaque parameter files, typically many megabytes large. This makes it a challenge to capitalize on human intuitions to improve the machine-derived grammars. An alternative to these statistical induction methods is a method called Transformation-Based Parsing (TBP) [2, 11, 12] . In TBP, a grammar consists of an ordered sequence of tree transform rules. To learn the rules, we begin with some initial annotation of the training corpus (for instance, every sentence parsed as a flat structure under an S-node), and then we iteratively search for the transform rule whose application will bring the training set closest to the set of true parses, we append that rule to our rule list and apply it to the training corpus. While there is still much work to be done in improving TB P, we have found that in its current state we achieve significantly better performance than a corpus-derived PCFG, but worse performance than the best statistical methods. In table 3 we show results from training on a 20k-sentence subset of the Penn 'freebank WSJ corpus. While the transformation-based system underperforms the best statistical systems, we believe it is an exciting path worth pursuing for the following reason : the Collins parser needs a trained parameter file about two hundred megabytes in size, while the transformation-based system achieves its accuracy with just a few hundred (mostly) readily understood rules. A human expert can easily study the rule list, and edit or add rules as appropriate . Manual Rule Wr iting In parsing, machine-learned rule-based systems achieve accuracy near that obtained by statistical systems. In many other natural language tasks, such as part of speech tagging, word sense disam biguation, and noun phrase chunking, machine-learned rule-based systems achieve performance on par with the best statistical systems, always learning a relatively small sequence of simple rules. This raises the question of whether we are really gaining anything by using machine learning techniques in NLP. While clearly a 200MB file of parameters is not something a person could create manually, creating small rule lists certainly is something a person could do . In (3] , we addressed this question for noun phrase chunking . We built a system to allow people to easily write rule sequences by hand and provided tools for effective manual error analysis. The advantage of a rule sequence, compared to a CFG for instance, is that the human does not have to be concerned about how rules interact. In manually generating a weighted CFG, there are complex interactions between rules, which makes manual grammar adjustment difficult . With rule sequences, at any stage of development the list of rules currently in the sequence can be ignored . In composing the i+lst rule, the person need only study the corpus and try to derive a rule to improve the accuracy of the training corpus in its current state, having been processed by the previous i rules. We had students in a Natural Language Processing class write rule sequences for NP-bracketing . We found that the best students wrote rules that achieved test-set performance comparable to the best machine-learning system, and were able to achieve this performance after just a few hours of rule writing. We believe this raises a number of interesting issues, such as: Does machine learning really help? Can we effectively combine the (hopefully complementary) abilities of machine learning algorithms and human rule-writers? Summary We have briefly surveyed three lines of research for parsing: combining the outputs of multiple trained systems , reducing the problem to learning a small set of simple rules , and doing nothing automatically. wof (s, A, D} : wof (np, A, B) , wof (aux , B, C) , WO f ( vp , C , D) . There is a word or phrase (wof) of category s stretching from point A to point Din the string if, for some points B and C between A and D, there is a phrase of category np from A to B, of category aux from B to C, and of category vp from C to D. A terminal symbol, say dog, is recognized as belonging to category n by virtue of the clause: wo f (dog , [dog I X] , X) . This is based on the convention of using suffixes of the string as names of points in it. In particular, this \u2022 clause says that a string consists of a noun followed by a string X if it consists of the word dog followed by X. With these, and few more obvious definitions, the Prolog interpreter will be able to prove the proposition wof (s, [the , dog, will, chase, the cat] , []} The proof will be carried out in accordance with the so-called recursive-descent, or top-down backtracking strategy suggested by our initial definitions. In order to show that a string is a word or phrase of category s, the procedure is to first show that it begins with a phrase of category np, and then to show that the remainder of the string consists of the aux phrase followed by a vp . Each of these steps consists of a recursive applica tion of the same procedure. To get the structure of a string, one must arrange to capture the control structure of the recognition pro cess, and this can be done in a variety of ways. To capture all the possible structures of a string, it is neces sary to behave on success just as one does on failure, by backing up to the most recent choice point with hitherto unexplored branches. As an effective recognition or parsing algorithm, the flaws of DCG are well known. The two principal ones are ( 1) that the assymptotic time complexity is exponential in the length of the string, and (2) that the procedure does not terminate when a phrase of a given category can have an initial substring that must be analyzed as belonging to that same category. The information that the recognition procedure amasses about the a string can be summarized in the manner exemplified below: oracle (s, [the , dog , will, chase , the , cat] , [] ). wo f (aux , B, C) , guide (C, D) wo f (vp, C, D) . This chart shows where there are phrases in the string, but do es not give their grammatical category. It is suf ficient, however, to eliminate problems arising from left-recursive grammars. The scheme we will outline provides analyses of strings in linear time with a context-free grammar. There is good reason to believe that this is no t possible if all the structures allowed by the grammar are to be recovered, and our scheme will indeed ignore certain structures. Ho wever, there is also good reason to believe that the structures that we shall ignore are also no t accessible to humans and, if this is the case, then no thing but good can come from leaving them out of account. Abney and Johnson (1989) have shown that a left-comer parser with early co mpo sition uses stack sp ace in proportion to the amo unt of center embedding in the structure. Such a parser is clearly also equivalent to a finite-state automaton which can reco gnize a string in linear time. One problem with this is that a finite-state automaton can serve only as a recognizer, and no t as a parser. Ho wever, a reco gnizer and can serve as an ora cle for parser 1 . The idea is simply to scan the string to be parsed fro m right to left, using a finite-state autom aton that recognizes the reverse of the depth-limited version of the context-free language and to associate with the sp ace between each pair of wo rds the state of the machines. When read fro m left to right, this sequence of states serves as an oracle for the left-comer parser with the early co mpo sition and a finite stack. Unfortunately, even for mo dest sized grammars, and an early limit on stack size, the numbers states in the automato n is unmanageably large so that it cannot be represented explicitly and therefore cannot be made deterministic. But, while undoubtedly a setback, this do es not entirely up set the plan. Recognition with a no ndeterministic automaton is possible in linear time if an appropriate strategy is employed for caching the states reached following a given substring. This follows fro m the fact that the number of alternative states that the automaton can be in after a given sub-string is limited by properties of the automato n and no t by the length of string. Ho wever, even this is not enough to bring the cost of the computation within reasonable bo unds because the number of stacks configurations that are possible following even a fairly short string can also be unmanageably large. The idea of providing an oracle to contro l the construction of the sequence of state sets that will, in its tum, serve as an oracle in reading out the structures of the string suggests itself, but it is difficult to see ho w such an oracle wo uld differ fro m the structure it is intended to help assemble. The intuition is that a useful oracle must contain only a part of the information in the structure whose assembly it controls. However, the possibility of a guide may be more promising. The idea will be to construct a weaker version of the context free grammar, which assigns to any given string a superset to of the structures that are signed by the original grammar, but which gives rises to an automaton with a smaller set of states. These states map in a systematic way onto tho se of the original automaton and, when this is app lied to the string, the only states that will be considered at a given point will be tho se corresponding to states through which the smaller automato n has passed. A simple way to construct a weakend version of a grammar is to construct a partition that symbols and to map each class in the partition onto a single symbol. The rules of the weekend grammar are simply the images are under this mapp ing of the rules in the original grammar. The new grammar will be weeker to the extent that it derives from a smaller number of a larger classes . Since the to tal number of symbols in the weakened grammar is smaller than that in the original grammar, so is the number of possible stack configura tions. The picture we now have is of a parser that proceeds in three phases. First, it scans the string fro m left to right using a left comer recognizer based on the weakened grammar, annotating the spaces between the wo rds with the sets of states that the automaton is in at that point. Next, it scans the string fro m right to left using the left-comer recognizer based on the full grammar and allowing states to be entered only if they map onto members of the list of states associated with that point in the string in the preceding phase. The reaso n for the reversal of direction is simply to ensure that the states on each list that is encountered are reachable fro m the other end of the string, thus providing a guide for the present scan that is, to the extent possible, pre dictive. Fo llowing the practice in chart parsing, we declare these first two phases to constitute a parser and I. This is reminiscent of the way the first member of a bimachine ( Schi.itzenberger, 1961 ) is used to control the operation of the second member. declare its assymptotic in complexity to be linear . The third phase reads out structures using the original context free grammar and the left-corner parser with the early composition whose stack states must be chosen fro m tho se associated with the string in the second phase. One apparently minor matter remains, namely how to construct a weakened versio n of a particular grammar that will serve as an effective guide, in this pro cess. Surprisingly, this pro ves to be the sticking point. One possi bility wo uld be simply to construct the partition of the grammar symbols in a random fashion. Ano ther wo uld be to eliminate the distinctions made by X-bar theory, collapsing, for example, N, N-bar, and NP onto the same symbol. Yet another wo uld be to eliminate \"minor\" grammatical matters , such as agreement fro m the first scan. The disturbing fact is that no ne of these things can be counted on to give a weakened grammar with desirable properties. Either the grammar remains essentially unchanged, or it reduces to one that accepts almost every thing. Minor changes can easily cause it to mo ve, almost chaotically, fro m one of these conditions to the other. I offer this as a challenge to the parsing community. PARSING TECHNIQUES FOR LEXICALIZED CONTEXT-FREE GRAMMARS Giorgio Satta In recent years, much of the parsing literature has focused on so-called lexicalized grammars, that is grammars in which each individual rule is specialized for one or more lexical items. Fo rmalisms of this sort include dependency grammar [13] , lexicalized tree-adjoining grammar [17] , link gram mar [20] , head-driven phrase-structure grammar [14] , tree insertion grammar [18] , combinatorial cat egorial grammar [2 1] and bilexical grammar [7] . Probabilistic lexicalized grammars have also been exploited in state-of-the-art, real-world parsers, as reported in [11] , [1] , [6] , [2] , [4] , and [9] . Other parsers or language models for speech recognition that do not directly exploit a generative grammar, still are heavily based on lexicalization, as for instance the systems presented in [10] , [12] , [15] and [3] . The wide diffusion of lexicalized grammars is mainly due to the capability of these formalisms to control syntactic acceptability, when this is sensitive to individual words in the language, and word selection, accounting for genuinely lexical factors as well as semantic and world knowledge conditions. More precisely, lexicalized grammars can select the complements and modifiers that a constituent can take, on the basis of special words playing a particularly informative role within the given constituent and the given complements and modifiers. These special words are typically identified with the lexical head and co-heads of a constituent, where with the term co-head of a constituent A we denote a lexical head of any of the subconstituents of A. To give a simple example {from [8] ), the word convene requires an NP object to form a VP, but some NPs are more lexically or semantically appropriate than others, and the appropriateness depends largely on the NP 's head, e.g., the word meeting vs. the word party. In this way, the grammar is able to make stipulations on the acceptability of input sentences like Nora convened the meeting and Nora convened the party. This was not satisfactorily captured by earlier formalisms that did not make use of lexicalization mechanisms. See [5] for further discussion. Within the parsing community, and in the speech community as well, a considerable resea. rch effort has been devoted to the important problem of defining statistical parameters associated with lexi calized grammars, and to the problem of the specification of algorithms for the statistical estimation of these parameters. In contrast, not much has been done with respect to the sentence processing problem. Most of the above mentioned systems parse input strings using naive adaptations of existing algorithms developed for the unlexicalized version of the adopted formalism, possibly in combina-tion with heuristics specially tailored to cut down the \u2022 parsing search space. However, the internal structure and the large size of formalisms derived by means of some lexicalization mechanism render these systems unsuitable to be processed with traditional parsing techniques. In the talk we address these problems and present novel parsing algorithms for lexicalized grammars that overcome the com putational inefficiencies that arise when standard algorithms are used. We will focus on lexicalized context-free grammars (LCFGs), a class of grammars originally introduced in [8] . LCFGs are a con venient abstraction that allows us to study important computational and generative properties that are common to several of the above mentioned lexicalized grammars. A similar abstraction, called probabilistic feature grammars (PFG), has been presented in [9] , motivated by parameter estimation problems. In contrast to PFG, features with lexical values have a special status within LCFG: this facilitates analysis of several complexity measures. In what follows, we report a formal definition of LCFGs and give a brief outline of the results that will be more carefully presented in the talk. Lexicalized context-free grammars We can think of a lexicalized context-free grammar as a particular kind of CFG obtained by applying a lexicalization procedure to some underlying CFG . 1 Before presenting the formal definition of LCFG, we briefly discuss an example. Consider the sample phrase dumped sacks into a bin. In order to be able to capture the lexical and syntactic relations holding among the words in this phrase, we pair each of the standard nonterminals NP, VP, etc., with a tuple of words from the phrase itself. The nonterminals of the resulting grammar are therefore of the following kind : V[dump], NP [sack] , VP [dump] [sack], etc. Using these new lexicalized nonterminals we can write new context-free productions of the form VP [dump] [sack] \u2794 V[dump] NP [sack] . The main idea here is that lexical items appearing in the right hand side nonterminals can be inherited by the left-hand side nonterminal. A possible derivation of the above phrase is depicted in Figure 1 . VP[dump] [sack] VP[dump][sack] PP[into] [bin] \ufffd \ufffd V[dump] NP[sack] P[into] NP(bin] I \u2022 \u2022 [a1 ,r1 ] A2 [a2 ,1 ] \u2022 \u2022 \u2022 [a2 ,r2 ] \u2022 \u2022 \u2022 A q [a q ,1 ] \u2022 \u2022 \u2022 [a q ,r q ], where q 2:'.: 1 and multiset { ao ,1 , ... , ao ,r o} is included in multiset { a1 ,1 , ... , a 1 ,r u a2 ,1 , ... , a q ,r q }; (b) A[a] \u2794 a. The multiset condition in (ii)a states that the lexical items in the left-hand side nonterminal are always inherited from the nonterminals in the right-hand side . Note also that the start symbol S[$] is a lexicalized nonterminal. As a conve ntion, we assume that $ is a dummy lexical item that does not appear anywhere else in G, and disregard it in the definition of L(G). In current practice, the set of delexicalized no nterminals Vo is kept separate from set VT . Set V o usually encodes word sense information and other features as number, tense, etc., structural information as bar-level and subcategorization requirements, and any additional information that does not explicitly refer to individual lexical items, as for instance contextual constraints for parent node category [2] , or constraints on the constituent 's yield, expressed through finite information about distribution of some lexical category (4 ) . Let G be some LCFG and let p be some production in P. If p has the form in (ii)a above, let k p = Ej =1 r j ; otherwise, let k p = 0. The d egree of lexicalization of an LCFG G is defined as k c = max pE P k p . We also say that G is a kc-lexical CFG. Note that from condition (ii)a it directly follows that G has productions with right-hand side length bounded by k c . Wh en the set of delexicalized no nterminals is fixed, the degree of lexicalization induces a proper hierarchy on the class LCFG . More precisely, it can be shown that, for any no n-empty set V o and any k 2:'.: 3, there exists a k-lexical CFG G defined on V o such that L(G) cannot be generated by any k'-lexical CFG defined on Vo and with k' < k. The class of 2-lexical CFG, also called bilexical CFG, turns out to be of major importance in cur rent parsing practice. In fact, the probabilistic formalisms adopted in (1) , (6 ) , [2) and ( 4 ) are strongly equivalent to bilexical grammars, in the following sense . Fo r each of the above formalisms, we can effectively construct a corresponding probabilistic bilexical grammar with the following properties . There is a one-to-one mapping between derivations in the source formalism and derivations in the tar get bilexical grammar . This map preserves the associated probabilities and can be computed through a homomorphism (homomorphisms can be implemented as real-time, memoryless processes) . Most important here, when computational problems related to parsing must be investigated, bilexical gram mars are a useful abstraction of the above mentioned formalisms, and parsing algorithms developed for bilexical grammars can directly be adapted to these formalisms . In Section 3, we will mainly focus on bilexical grammars. 2 LCFG Parsing The cost of the expressiveness of an LCFG is a very large production set. In the simplest case of bilexical CFGs, the size of set P usually grows with the square of the size of V T , and thus can be very large. Standard context-free parsing algorithms, which run in time linear with the size of the input grammar, are inefficient in such cases. Therefore, a first goal in the design of a parsing algorithm for LCFGs is the one of achieving running time sublinear with respect to the grammar size. As a first result, we show that algorithms satisfying the so-called correct-prefix property [19] are unlikely to achieve this goal, even in case the grammar is precompiled in an amount of time polynomial in its size. In order to achieve the sublinear time goal, a usual practice is to use standard CFG parsing algo rithms and to select only those rules in the grammar that are lexically grounded in the input sentence. However, this in practice adds a fac tor of n 2 for an input string of length n, resulting in O( n 5 ) running time for bilexical CFGs and related formalisms. We show how dynamic programming can be exploited to achieve an O( n 4 ) result. Also, we specify an O(n 3 ) time algorithm for a restricted kind of bilexical CFGs. We argue that the proposed restriction leaves enough power to the formalism to capture most common natural language constructions. We discuss how these results generalize to LCFG with higher degree of lexicalization and to lexicalized tree-adjoining grammars [17] as well. The above algorithms exploit bottom-up strategies, which are the most common in parsing lexi calized grammars. We show how top-down strategies can be applied in parsing bilexical CFGs, still retaining the desired condition on sublinear running time with respect to the input grammar size. This is done by exploiting generalized look-ahead techniques. We argue that the resulting strategies are even more selective than standard top-down strategies satisfying the correct-prefix property. Introduction As NLP-based applications are growing, there is a stronger need for wide-coverage parsing systems. At the moment, comprehensive grammars are available for some languages, like English [ \u2022 Limited number of language users. This fact implies a reduced number of researchers/developers of computational linguistic tools. \u2022 Limited number of language resources, in the form of computational lexicons, grammars, corpora, annotated treebanks or dictionaries. Although there are current efforts for the development of parsing systems for other languages [Oflazer 1999, Hajic and Hladka 1998], there will always be the problem of reaching the complexity and perfo rmance of the parsers for the most studied languages. This is in spite of the efforts to make publicly available language resources (ELRA) that could at most alleviate the problem. Therefore, methods must be devised which obtain results automatically, minimizing development costs. This work presents both the development of a parsing system for unrestricted Basque texts and the first results obtained using it in the process of acquiring subcategorization information. The system is applied to Basque, which has as its main characteristics being agglutinative and having basically constituent-free order. These characteristics involve some complexities for syntactic analysis. As a first step, a basic parsing system has been developed. It consists of two modules, applied sequentially: an unification based chart-parser and a finite-state parser. This combined system covers the syntactic core of the language; however, although it is useful for several non-trivial applications like the detection of syntactic errors, is still incomplete, lacking important aspects like subcategorization information. For this reason, we have applied this basic parser to text corpora, with the aim of obtaining subcategorization information that will be used to enrich the lexical database. This way, we plan to develop a parsing system in a bootstrapping fashion, with incremental improvements. r -------------------, : Sentence : Moiphological analysis and disambiguation Unification based chart-parser r - -----------------, : chart (automaton) : - ------------------- Finite-state parser r----------------------, : Verb + subcategorized elements : \ufffd----------------------J Figure 1 . Overview of the system. The rest of the paper is organized as follows. Section 2 presents the basic parsing system we have implemented, detailing its main components and justifying its sequential architecture. It also examines the application of the system to the extraction of subcategorization information. Section 3 gives the results of its evaluation against a set of manually tagged 500 sentences, while section 4 reviews the literature on parsing systems and automatic acquisition of subcategorization information. The Parser We have developed a parsing system divided in two main modules: a unification based parser and a finite state parser (see figure 1 ). Prior to parsing, there is another step concerned with morphological analysis and disambiguation, using the basic tools for Basque (http://ixa.si.ehu.es) that have been developed in previous projects: \u2022 The lexical database. It is a large repository of lexical information, with about 70.000 entries (including lemmas and declension/derivational morphemes), each one with its associated linguistic features, like category, subcategory, case and number, contained in a commercial database management system. \u2022 Morphological segmentation. Inflectional morphology of Basque was completely described in [Alegria et al. 1996 ]. This system applies Two-Level Morphology for the morphological description and obtains for each word its segmentation(s) into component morphemes, where each morpheme is associated with its corresponding features in the lexicon. the segmentation module has full coverage of free-running texts in Basque, capable of treating unknown words and non-standard forms ( dialectal variants and typical errors). \u2022 Morphological disambiguation. A disambiguation system was implemented for the assignment of the correct lemma and part-of-speech to each token in a corpus [Ezeiza et al. 1998 ] taking the context into account, by means of statistical (Hidden Markov Models) and hand-crafted rules (Constraint Grammar (CG) formalism [Samuelsson and Voutilainen 1997] ). This tool reduces the high word-level ambiguity from 2.65 to 1.19 interpretations, still leaving a number of interpretations per word. The Unification Based Parser After morphological analysis and disambiguation, each word is assigned one or more readings, each of them as a list of its components (l emma and morphemes) with their associated morphosyntactic information. Some facts concerning syntactic analysis must be taken into account: \u2022 The morpheme is the basic unit of syntactic analysis, following the most extended syntactic descriptions for Basque [Abaitua 1988 ]. In figure 2 , dashed lines represent lemmas and morphemes, that is, units smaller than the word that will form the input to the syntactic analyzer. This kind of analysis has been adopted by other systems for agglutinative languages like Hungarian [Pr6szeky 1996] and Turkish [Oflazer 1999]. \u2022 Regarding the syntactic structure of Basque, it has been considered as a language with free order of constituents. However, this is only true for main sentence constituents with respect to the verb (such as noun phrases, prepositional phrases and sentential complements), because inside those constituents the order of elements is fixed or quite limited. Moreover, the syntactic rel ationships inside these fixed order components require the testing of complex agreement and the building of non-trivial syntactic structures, not definable by finite-state techniques [B eesley 1998] . These facts led us to describe the syntax of these components by means of feature structures, using a unification based formalism. \u2022 There are more probl ems if we want to go beyond the level of the main sentential constituents (verbs, NPs, PPs and sentential complements). As their rel ative order is al most free, their analysis would suppose the proliferation of a high number of unsolvable attachment ambi gu ities. Example 1 shows the presence of two elements (PP and NP) that could be attached to either of the two surrounding verbs (giving three different interpretations). Al though this kind of ambiguity can be resolved in some cases (the auxiliary verb, when present, can indicate information about the case, number and person of subject, object and indirect object), a general solution will need at least the use of subcategorization information, unavailable at the moment. As a result, the effort devoted to the design of such a grammar would be of littl e final value at the moment, due to unsolved ambi gu ity. Furthermore, its development would al so be a costly enterprise. For that reason, we decided to postpone the development of that part of the grammar until the relevant information is availabl e .   Hence, a partial unification grammar has been developed that gives a complete coverage of the main elements of the sentence (NPs, PPs and sentential complements). At the moment the grammar contains 120 rul es written in the PATR-II formalism. We chose this formal ism because there has not been a broad description for Basque using more elaborated theories like LFG [ Abaitua 1988 ] and HPSG, and also because the theories are based on information not available at the moment, such as verb subcategorization. PATR-II is more flexible at the cost of extra writing, as it is defined at a lower level. There is an average number of 15 equations per rule, some of them for testing cond\ufffdtions like agreement, and others for structure building. The main phenomena covered are: . . . it was seen necessary to create an institution at this side of the Py rennees . \u2022 Noun phrases and prepositional phrases. Agreement among the component elements is verified, added to the proper use of determiners. \u2022 Subordinate sentences, such as sentential complements (completive clauses, indirect questions, . .. ) and relative clauses. \u2022 Simple sentences using all the previous elements. The rich agreement between the verb and the main sentence constituents (subject, object and second object) in case, number and person is verified. As we explained before, sentence analysis is performed up to the level of phenomena that can be described using only syntactic information now included in the lexicon. Example 2 shows the (simplified) rule that combines a noun-group (noun + adjectives + determiners + noun modifiers) with a case mark (simple or composed), forming an indefinite NP or PP. Although we will not explain the rule in detail, the example shows the relative complexity of the rules as they must test for several kinds of agreement on number, definiteness and case. As a consequence, the linguistically relevant morphosyntactic information is very rich compared to most chunking systems. This will have the effect of increased flexibility, as different applications will typically use only a subset of the information. This system can be seen as a shallow parser [ Abney 1997, Giguet and Vergne 1997 ] that can be used for subsequent processing, following \" . . . the basic assumption that it is possible to define an interesting intermediate level between words and sentences\", as [Basili et al 1998] point out. The parser is applied bottom-up to each sentence, giving a chart as a result. The output for each sentence still contains both morphological and syntactic ambiguities, giving a huge number of different potential readings per sentence. Figure 2 shows an example where dashed lines are used to indicate lexical elements (lemmas or morphemes), while plain lines define syntactic ones. The bold circles represent word-boundaries, and the plain ones delimit morpheme-boundaries. Although the figure has been simplified, each arc is actually represented by its morphological and syntactic information, in the form of a sequence of feature-value pairs. The Finite-State Parser As we showed in the previous section, the unification based parser obtains the decomposition of a sentence into its main syntactic components. However, this result is not directly useful due to several reasons: \u2022 Ambiguity. There are multiple readings for each sentence, as a result of both morphological ambiguity (1. 19 interpretations per word-form) and syntactic ambiguities introduced by the unification based parser . \u2022 Different output profiles. Linguistic information is defined at different levels, each of which will be useful depending on a particular application. For example, in the acquisition of subcategorization information all NPs, PPs and sentential complements will be necessary, but for term identification only NPs and PPs are needed (this means that sentential complements, which may include NPs and PPs, must be eliminated from the output). As a consequence, a tool is needed that will allow the definition of complex linguistic patterns for disambiguation and filtering. In recent years, several parsing systems based on finite-state technology have been developed, baseci on automata and transducers [Roche and Schabes 1997 ]. We decided to treat the resulting chart (see figure 2 ) as an automaton to which finite-state disambiguation constraints and filters can be applied, encoded in the form of regular expressions and relations. This way, finite-state rules provide a modular, declarative and flexible workbench to deal with the resulting chart. Currently we use the Xerox Finite State Tool (XFST, http://www.rxrc. xerox .com/research/mltt/fst/home.html), which has as its main characteristic a rich set of operations, like the replacement operator [Karttunen et al. 1997 ], defined in terms of simpler regular expressions ( or relations) so that the combined expressions always belong to the finite-state calculus and can, therefore, be implemented using a finite-state automaton (transducer). Among the finite-state operators used we apply composition, intersection and union of automata and transducers. We use both ordinary composition and the recently defined lenient composition [Karttunen 1998 ]. This operator allows the application of different eliminating constraints to a sentence, always with the certainty that when some constraint eliminates all the interpretations, then the constraint is not applied at all, that is, the interpretations are 'rescued '. The operator was first proposed to formalize Optimality Theory constraints in phonology. As Karttunen points out, it also provides a flexible way to enforce linguistic or empirical constraints in syntactic disambiguation. The design of the finite-state rul es is a non-trivial task when dealing with real texts, including proper names, syntactic/spelling errors, unknown/foreign words and a wide variety of syntactic constructions. So we had to define 388 finite-state definitions and constraints for the acquisition of verb subcategorization information (actually most of them reflect linguistic facts that can be directly used in other applications). They range from simple local constraints (305 automata with less than 100 states) to most complex patterns (there are a few automata with more than 15,000 states and 300,000 arcs). As constraints and filters are defined by means of automata and transducers, they could theoretically be merged into a single final automaton, hence improving performance. However, as patterns are more complex, the size of the resulting automaton grows prohibitively large, so we had to arrange it by sequencing the automata. Although this slows down parsing time, it makes the compilation viable [Tapanainen 1997 ]. The interaction of different automata is a matter that requires further investigation. As a first evaluation of the system, we chose the problem of acquiring verbal subcategorization information, that is, given a sentence and a verb, extracting the verb's corresponding subcategorized elements. This application has the advantage of a well defined environment to test the performance of the parser, and al so that the resulting subcategorization frames may be fed back to the parser, to improve its coverage and precision [Briscoe and Carroll 1997 ]. These are the main operations performed by the finite-state parser (see figure 3 ): \u2022 Disambiguation. As whole syntactic units can be used, this process is similar to that of Constraint Grammar disambiguation, with the advantage of being abl e to reference syntactic units wider than the word, which must be defined in a roundabout manner in the word-based CG formalism. As figure 3 shows, the disambiguation constraints are applied using the lenient composition operator (.0.) , so that no constraint will discard all the readings of a sentence, making the system robust. \u2022 Extracting parts of a sentence. The gl obal ambiguity of a sentence is considerably reduced if only part of it is considered (see example 3). For instance, in the case of extracting verb subcategorization information, some rul es examine the context of the target verb and define the scope of the subsentence to which the disambiguation operations will be applied (these filters use the ordinary composition operator .o.). \u2022 Filtering. Sometimes not all the availabl e information is rel evant. For example, the noun/adjective ambiguity present in zuriekin ('with the whites' (zuri as a noun) / 'with the white ones' (adjective)) can be ignored when acquiring verb subcategorization information, as we are interested in the syntactic category and the grammatical case (prepositional phrase and commitative, respectively), the same in both al ternatives. Example 3 shows the application to a sentence containing the wordform doa (goes), in the context of analyzing the verb Joan (to go). The result is simplified, a& both the input and output are presented as text, rather than as an automaton containing feature-value pairs that represent syntactic components (the translation of the output subsentence is given later in example 4). We took a corpus consisting of 500 sentences corresponding to 5 verbs, that is, 100 sentences per verb. In order to test different corpora, half of the sentences were taken from a general corpus of Basque, while the other half came from newspaper texts. We manually marked for each sentence the occurrence of each target verb and its associated subcategorized elements, and then compared it with the output of the parser. 350 sentences were used for the refinement of both the unification based parser and the finite-state parser, while the remaining set of 150 sentences (30 for each verb) was only examined for the final test. Evaluation Eta lepo hom1atik Regarding the tested sentences, we did not select them by lexical or syntactic coverage of the parser, i. e., we took the first set of 500 sentences containing the target verbs from the two corpora, so that we could measure the actual performance of the parser with unrestricted texts. There are several extra difficulties added to the problem of ambiguity: \u2022 Sentence length. Each sentence contains an instance of the target verb together with other main or subordinate subsentences (the average sentence length is 22 words). Delimiting the exact boundaries of the subsentence corresponding to the target verb is a difficult task. \u2022 Multiword lexical units. Although we plan to include their treatment in the morphological analysis phase, it is not implemented yet. Its main consequence will be an increase in the number of errors (false positives), as some non-compositional elements will be interpreted compositionally by the general unification based grammar. \u2022 Unknown words, proper names and spelling errors. Although the morphological analyzer recognizes a subset of them, the rest is problematic because each of them will give a number of hypothetical interpretations, therefore increasing ambiguity and consequently the error rate. Increasing lexical coverage will have a positive impact in future developments. To evaluate the correct analysis of a sentence, we have developed a simple coding scheme inspired on [Carroll et al. 1999 ], who define a hierarchy of grammatical relations. Instead of marking syntactic functions, we annotate the declension case, lemma and number of NPs and PPs [Oesterle and Maier-Meyer 1998], and the subordination type for sentential complements (see example 4). We have postponed the assignment of syntactic functions until relevant data is available. For evaluation we measured precision (the number of correctly selected elements I all the elements returned by the parser) and recall (the number of correctly selected elements I all the elements present in the sentence). Table 1 shows the results as the mean over all sentences. Although there is always a balance between recall and precision, we tried to maximize the latter, sometimes at the cost of lowering recall. As we could inspect the development corpus during the refinement of the parser, the results in the second and third columns can be understood as an upper limit of the parser in its current state, approximately 92% precision and 71 % recall. As we will explain next, these results can be improved refining the lexicon and the grammars. We examined manually the causes of the errors ( 68 errors were identified in the test corpus causing problems in precision or recall), which can be classified into several types 1 : \u2022 Errors due to multiword units (5) , unknown words, !?roper names (9) and spelling errors (8) . Their treatment corresponds _ naturally to morphological analysis and is mainly linked to future extensions of the lexicon. \u2022 Errors due to incorrect disambiguation. They can be subdivided into two main types. When the morphological disambiguator chooses an incorrect reading, it has the effect of causing a false positive, i.e., decreasing precision (9 errors). On the other hand, sometimes more than one alternative is left (including the correct one). Its main effect will be increased ambiguity that will show as lower recall (5 errors). \u2022 Errors due to the lack of syntactic coverage of the grammars (3 2 errors). This kind of errors define the limits of the partial parsing approach. Although more than half of these errors are due to the incompleteness of the grammar, and they can be solved simply by extending it to cover the corresponding phenomena, there are other errors that \\\\:'Ould need qualitative changes, like the inclusion of subcategorization information. Finally, a third set of errors are due to the characteristics of unrestricted corpora, such as syntactically odd constructions, that we doubt a parser could analyze even after solving the two other problems. As the results show, more than half of the errors could be solved by improvements on the lexicon, the morphological analyzer and morphological disambiguation (totaling 36 errors). Although morphological disambiguation is relatively difficult to extend and modify, further careful work extending the treatment of proper names, spelling errors and multiword units would imply a noticeable increase in both recall and precision. In a similar way, work must be done extending the basic syntactic grammar, which we estimate could reduce the syntactic errors to about a half of the present ones. Consequently, we consider the results satisfactory, with 87% precision and 66% recall, as the results for new sentences are near the expected best results (those obtained for the development corpus, with 92% precision and 71 % recall), showing that the system behaves correctly with unseen sentences. After obtaining instances of putative subcategorization frames, there is still work to be done. In configurational languages like English, subcategorized elements appear at fixed places around the verb, while in nonconfigurational ones they can appear at several different positions (hypothetically all the permutations are possible). Basque being mainly a nonconfigurational and pro-drop language with respect to phrases in ergative, absolutive and dative cases, there is not a direct correspondence between subcategorization instances and frames, as one subcategorization frame may correspond to several kinds of instances. As an experiment, we applied the parser to 1,000 sentences (22,000 words) corresponding to the verb ikusi ('to see'), and classified the results according to the different sets of subcategorized elements, without taking their relative order in consideration. The results are presented in Table 2 . Results were obtained for 826 sentences, after discarding those having more than one interpretation. For example, the patterns 'instrumental' and 'absolutive instrumental' correspond to the same subcategorization frame, due to pro-drop phenomena with the phrase in the absolutive case. The possibility of automatically classifying subcategorization patterns into frames deserves further work. 4 Related Work [Abney 1997 , Giguet and Vergne 1997 , Basili et al. 1998 ] show the benefits of a stratified approach to parsing, where one or more intermediate levels can be defined between the basic level of words and the analysis of a full sentence. Our work differs from theirs in that we apply two different kinds of analyzers (unification based and finite-state), rather tha\ufffd defining the different levels using the same formalism. [Ritchie et al. 1992 ] present a system that performs morphological analysis, divided in a segmentation phase (using finite state networks) and the application of a unification grammar for the combination of morphemes. The results of the segmentation are interpreted as a chart that serves as input to a unification based chart parser. Our system shares the idea of dividing work between different kinds of formalism. However, our approach differs in that we first apply a unification grammar, indispensable for the treatment of complex syntactic phenomena, and then a finite state grammar is used for disambiguation and filtering. Regarding the problem of syntactic disambiguation, most grammar based systems [Briscoe and Carroll 1997 ] have adopted a statistical approach. For morphological disambiguation, however, there are both statistical and ru le based systems, with better results for the second approach [Samu elsson and Voutilainen 1997]. Our system adopts a ru le formalism based on regular expressions, using syntactic elements instead of words as the basic disambiguation unit. We justify this election on both the unavailability of syntactically annotated treebanks and the better performance of systems based on hand-coded rules. Concerning the acquisition of verb su bcategorization information, there are proposals ranging from manual examination of corpora [Grishman et al. 1994 ] to fully au tomatic approaches. [Briscoe and Carroll 1997 ] describe a grammar based experiment for the extraction of subcategorization frames with their associated relative frequencies, obtaining 76.6% precision and 43.4% recall. Our results are not directly comparable, as we only estimate precision and recall on subcategorization instances, not frames. [Kuhn et al. 1998 ] compare two approaches for the acquisition of su bcategorization information: a corpus qu ery pattern based approach (no grammar, using regular expressions on morphologically analyzed wordforms) and a grammar based approach (in a way similar to [Briscoe and Carroll 1997] ). Both are applied to the problem of acquiring su bcategorization instances of 3 subcategorization frames, showing that the grammar based approach improves results specially in recall, due mainly to the higher-level knowledge encoded in the grammar. Comparing with our work, we think that our system is situated between the two approaches, as we use patterns on partially parsed sentences. Ou r obj ective is more ambitious in the sense that we try to find all the subcategorization instances, rather than distinguishing among 3 previously selected frames. The above mentioned studies depend on a set of manually annotated analyses. [Carroll and Rooth 1998 ] present a learning technique for su bcategorization frames based on a probabilistic lexicalized grammar and the Expectation Maximization algorithm using unmarked corpora. The results are promising, although the method is still computationally expensive and requires big corpora (50M). Conclusion This work presents the development of a robust parser for unrestricted Basque texts. As the linguistic resources are limited, the lexicon lacks important aspects su ch as verbal su bcategorization information. We have implemented a basic syntactic parser using the information now available in the lexicon. The system has been divided in two sequential modules: \u2022 A unification based grammar that covers the main sentence components of the sentence. It gives a description of well-formed linguistic phenomena. Due to the agglutinative nature of the language, feature structures are necessary to treat the wealth of information contained in words/morphemes. \u2022 Finite-state rules that provide a modular, declarative and flexible workbench to deal with the resulting chart of syntactic elements. It establishes the application of empirical, corpus-oriented facts, versus the more general facts on linguistic well-formedness encoded in the unification grammar. The unification based gramm ar an d the fin ite-state one are complementary. The unification grammar is necessary to treat aspects like complex agreement an d constituent order variations , currently uns olvable using fin ite-state networks, due to the exponential growth in size of the resulting au tomata [Beesley 1998 ]. The limits of this grammar are mainly defined by the un availability of im portant information, like subcategorization frames . On the other hand, regular expressions an d relations , in the form of au tomata an d trans du cers , are in dispensable to cope with morphological/syntactic ambiguity (by means of hand-coded ru les or constraints) an d filtering of the information relevant to each application, thus adding to the flexibility of the resu lting tool. The parser is being used in the process of acquiring verb subcategorization instances, obtaining 87% precision an d 66% recall over a corpus of previous ly unseen 150 sentences. In future work, we plan to in tegrate the resulting subcategorization information into the gramm ar, so that it will be extended by successive bootstrapping cycles. Bibliography [Abaitua 1988] Abaitua, J. 1988 . Complex predicates in Basque: from lexical forms to fun ctional structures . PhD thesis, Un ivers ity of Manchester. Introduction Thee Adjoining Grammars (TAG) [8] and Linear Indexed Grammars (LIG) [7 ] are extensions of Con text Free Grammars (CFG). Thee adjoining grammars use trees instead of productions as primary representing structure and seems to be adequate to describe syntactic phenomena occurring in nat ural language, due to their extended domain of locality and to their ability for factoring recursion from the domain of dependencies. Linear indexed grammars associate a stack of indices with each non-terminal symbol, with the restriction that the indices stack of the head non-terminal of each pro duction (the fa ther) can be inherited by at most one body non-terminal (the dependent child) while the other stacks must have a bounded stack size . Several parsing algorithms have been proposed for TAG, ranging from simple bottom-up algorithms, like CYK (17] , to sophisticated extensions of the Earley 's algorithm [9] . In order to improve efficiency, it is usual to translate the source tree adjoining grammar into a linear indexed grammar [1 6, 12, 13, 17] . However, in some cases is not possible to translate the parsing strategy from TAG to LIG, as there are parsing strategies for TAG which are not incorporated in any parsing algorithm for LIG. To eliminate this drawback, we present in this paper several parsing algorithms for LIG which mimic the most popular parsing strategies for TAG [1]. Linear Indexed Grammars A linear indexed grammar is a tuple (Vr, V N , Vi, P, S), where Vr is a finite set of terminals, V N a finite set of non-terminals, Vi is a finite set of indices, SE V N is the start symbol and Pisa finite set of productions. Following [7] we consider productions in which at most one element can be pushed on or popped from a stack of indices : Ao [oo 1 ] \u2794 Ai [] ... Ai -i l ] Ai[oo,'] Ai-i l ] ... Am [] Ao [ ] \u2794 a where m is the length of the production, A i E VN for each O -\ufffd j \ufffd m, Ai is the dependent child, oo is the part of the indices stack transmitted from the father to the dependent child, , , ,' E Vi U { \u20ac} and for each production either , or ,' or both must be \u20ac and a E VT U { \u20ac}. The derivation relation => is defined for LIG as Y => Y' \u2022 if Y = Y 1 A[a,] Y 4 and there exists a production A[oo 1 ] \u2794 Y 2A'[oo,'] Y 3 such that Y' Y1 Y2A' [n,'] Y3 Y4 \u2022 or else if Y = Y 1 A[ ] Y 4 and there exists a production A[ ] \u2794 a such that Y' = Y 1 a Y 4 where A E V N , a E V/ and , , ,' E Vi U { \u20ac}. The reflexive and transitive closure of => is denoted by \u21d2. The language defined by a LIG is the set of strings w E v; such that S[ ] \u21d2 w. To parse this type of grammars, tabulation techniques with polynomial complexity can be designed based on a property defined in [17] , that we call context-freeness property of LIG, establishing that if A[,] \u21d2 uB[ ]w where u, w E v; , A, B E VN , , E Vi U {\u20ac} and B[] is a dependent descendant of A[,], then for each Yi, Y2 E_ (VN[Vt] U VT )*and /3 E V/ we have Y1 A[/3,]Y2 \u21d2 Y1uB[,B]wY2. Also, if B[,] is a dependent descendant of A[ ] and A[ ] \u21d2 uB[,]w then Y 1 A[/3]Y 2 \u21d2 Y 1 uB [/3,]wY 2. Parsing Schemata We will describe parsing algorithms using Parsing Schemata, a framework for high-level description of parsing algorithms (15] . An interesting application of this framework is the analysis of the relations between different parsing algorithms by studying the formal relations between their underlying parsing schemata. A parsing system for a grammar G and string a 1 ... a n is a triple (I, 1\u00a3, TJ), with I a set of items which represent intermediate parse results, 1l an initial set of items called hypothesis that encodes the sentence to be parsed, and 1J a set of deduction steps that allow new items to be derived from already known items. Deduction steps are of the form \ufffd cond, meaning that if all antecedents 1Ji of a deduction step are present and the conditions cond are satisfied, then the consequent \ufffd should be generated by the parser. A set :F \ufffd I of final items represent the recognition of a sentence. A parsing schema is a parsing system parameterized by a grammar and a sentence. Parsing schemata are closely related to grammatical deduction systems [14] , where items are called formula schemata, deduction steps are inference rules, hypothesis are axioms and final items are goal formulas. A CYK-like Algorithm We have chosen the CYK-like algorithm for LIG described in (16] as our starting point. Due to the intrinsic limitations of this pure bottom-up algorithm, the grammars it can deal with are restricted to those having two elements, or one element which must be a terminal, in the right-hand side of each production. This restriction could be considered as the transposition of the Chomsky normal form to linear indexed grammars. The algorithm works by recognizing in a bottom-up way the part of the input string spanned by each grammar element. The items used in the tabular interpretation of this algorithm are of the form [A, 'Y, i, j I B, p , q] and represent one of the following types of [ derivation: \u2022 A['Y] \u21d2 ai + I ... a p B[ ] a q +l ... aj if and only if (B, p, q) -:/ ( -, -, -), B[ ] is a dependent descendent of A['Y] and ( p, q) ::; ( i, j). \u2022 A[ ] \u21d2 ai +l ... aj if and only if 'Y = -and (B, p, q) = (-, -, -). wheremeans that the value of a component is not bound and (p, q) ::; (i, j) is used to represent that i ::; p ::; q ::; j when p and q are bound. These items are like those proposed for the tabulation of linear indexed automata [10] and for the tabulation of bottom-up 2-stack automata [6] .  LCYK = { [A, ,y,i,j I B, p,q] I A, B E VN, 'Y E Vi, 0 ::; i::; j, (p,q) :: [A, ,y,i,j I B, i, k]   [B,-, i, k 1 -, -, -] , 1)[00](](00] -[C, 17, k,j I D, p,q] CYK - ; (i, j) } 11.cYK = { [ a, i -1, i] I a = ai, lJ ::; i ::; n } v scan - [a,j, j + 1] A[ ] \u2794 a E p CYK -[A . . 1 1 ] , -, 1 , 1+ -, -, -I [B,-, i, k 1 -, -, -], [A, 17, i, j I D, p, q] 1)[00](00]( J _ CYK - 1)[00]( ](oo-y] _ CYK - 1)[00] (00-y]( J _ CYK - [B, 17,i,k I D, p,q], [C, -, k,j I -, -, -] [A , 17, i, j I D, p, q] [B, -,i, k I -, -, -], [C, 'Y, k, j I D, p, q], [D, 'TJ,P, q I E, r, s] [A, 77, i, j I E, r, s] [B, 'Y, i, k I D, p, q], [C, -, k,j I -, -, -], [D,17, p,q I E, r, s] [A, 77, i, j I E, r, s] I A[oo 1 ] \u2794 B[ ] C[oo] E P A[oo 1 ] \u2794 B[oo] C[ ] E P I A[oo] \u2794 B[ ] C[oo] E P A[oo] \u2794 B[oo] C[ ] E P I A[oo] \u2794 B[ ] C[oo 1 ] E P A[oo] \u2794 B[oo 1 ] C[ ] E P 1J _ v scan u v [oo.,,][ ](ooJ u v [oo-y](ool[ l u v [ool[ l[ooJ u v [ool[ool[ l U v [ool[ ][oo-yJ u v [ool[oo-y][ J CYK -CYK CYK CYK CYK CYK CYK CYK Fc YK \ufffd { [S, -, 0, n I-,-, -] } I The hypotheses defi?-ed for this parsing system are the standard ones and therefore they will be omitted in the remaining parsing systems described in this paper . Steps in the set 'D\ufffd\ufffdK are in charge of starting the parsing process . The other steps are in charge of combining the items corresponding to the elements in the right-hand side of a production in order to generate the item corresponding to the left-hand side element, propagating bottom-up ' the information about the indices stack . The space complexity of the algorithm with respect to the length n of the input string is O(n 4 ), as each item stores four positions of the input string . The time complexity is O(n 6 ) and it is given by the deduction steps in v \ufffd;; Yi gh t and v \ufffd;\ufffd Ieft . Although these steps involve 7 positions of the input string, by partial application each step can be decomposed in a set of deduction steps involving at most 6 positions. A CYK-like algorithm generalized for linear indexed grammar with productions manipulating more than one symbol at the top of the indices stacks is described in [17] . The same kind of generalization could be incorporated into the algorithm presented here. A Bottom-up Earley-like Algorithm The CYK-like algorithm has an important limitation: it can only be applied to linear indexed gram mars having at most two children in the right-hand side. To overcome this limitation we use dotted production into items instead single nodes . Thus, we can distinguish the part of a production already processed from the part not yet processed. With respect to notation, we use A to denote a grammar element having the non-terminal A wh en the associated indices stack is not relevant in that context . The items of the new parsing system 1P buE are of the form [ A \u2794 Y 1 \u2022 Y 2 , , , i, j I B, p, q] and can be obtained by refining the items in 1Pc YK \u2022 They represent one of the following two cases: \u2022 A[,]=} Y 1 Y 2 \u21d2 ai+l .:. a p B[ ] a q +l ... ai Y 2 if and only if (B,p, q) i= ( -, -, -), B [ ] is a dependent descendant of A[,] and (p, q) ::; (i, j). \u2022 Y 1 \u21d2 ai+l ... ai if and only if , = -and (B,p, q) = ( -, -, -). If the dependent child is in Y 1 then the indices stack associated to A and to the dependent child must be empty. _ { [A \u2794 Y1 \u2022 Y2 ,,,i,j I B,p,q] I .LbuE -A \u2794 Y1Y2 EP, BEV N, , E VJ, 0::; i::; j , (p, q) s_ (i, j) v init ---------- buE -[ A \u2794 \u2022Y, -,i,i 1 -, -, -] [A[ ] \u2794 \u2022a, -,j,j I-,-, -], [ . . 1] v scan -a , J , J + buE -[A[ ] \u2794 a\u2022, -,j,j + 11-, -, -] 32 } [A \u2794 T1 \u2022B[]'T2,,, i, k I C, p,q] , ,nComp[ ] _ [B \u2794 T3\u2022, -, k,j !'-,-,-] vbuE - [A \u2794 T1 B[ ] \u2022 T2, ,, i,j I C, p, q] [A[oo,] \u2794 Y 1 \u2022 B(oo] Y 2, -, i, k I -, -, -], ,nComp[oo-yl[oo] _ [B \u2794 T 3\u2022, T/, k, j I C, p, q] vbuE ------------------ [A[oo,] \u2794 T 1 B[oo] \u2022 T 2, 1', i,j I B, k,j] [A[oo] \u2794 Y 1 \u2022 B(oo] Y 2, -, i, k I -, -, -], , nComp[ool[oo] _ [B \u2794 T3\u2022 , ry, k,j I C, p,q] vbuE ------------------ [A[oo] \u2794 T1 B[oo] \u2022 T2,TJ, i,j I C, p, q] [ A[ 00] \u2794 Y 1 \u2022 B [ oo,] T 2 , -, i, k I -, -, -] , [B \u2794 Y3\u2022 , ,,k,j I C, p,q] , ,nComp[oo][oo-y] _ [C \u2794 Y 4\u2022, T/, P, q I D, r, s] vbuE ------------------- [A[oo] \u2794 Y 1 B[oo,] \u2022 Y 2, TJ, i,j I D, r, s] V -v init u v scan u v Comp[ l u V Comp[oo-y][oo] u V Comp[ool[oo] u v Comp[oo][oo-y] buE - buE buE buE buE buE buE F buE = { [ S \u2794 Y \u2022 , -, 0, n I -, -, -] } The space complexity with respect to the input string is O(n 4 ) because each item stores four positions . The time complexity with respect to the input string is O(n 6 ) and it is given by deduction t . ,nComp[oo][oo-y] s eps m vbuE 4 An Earley-like Algorithm The algorithm described by the parsing system P buE does not take into account whether the part of the input string recognized by each grammar element can be derived from S, the axiom of the grammar . Earley-like algorithms limit the number of deduction steps that can be applied in each moment by predicting productions which are candidates to be part of a derivation having as its starting point the axiom of the grammar. As a first approach, we consider prediction is performed taking into account the context-free skeleton only. The parsing system so obtained is denoted P E and can be derived from P buE by applying the following filters: \u2022 lnit deduction steps only consider productions with S as left-hand side. \u2022 Instead of generating items of the form [A \u2794 \u2022Y, -, i, i I -, -, -] for each possible production A \u2794 Y E P and positions i and j, a set of Pred deduction steps generate only those items involving productions with a relevant context-free skeleton . An Earley-like Algorithm Preserving the VPP Parsers satisfying the valid prefix property (VPP) guarantee that, as they read the input string from left to right, the substrings read so far are valid prefixes of the language defined by the grammar. More formally, a parser satisfies the valid prefix property if, for any substring a 1 ... ak read from the input string a 1 ... ] and (p, q) \ufffd (i, j). This type of derivation refers to a prediction of the non-terminal A with a non-empty indices stack. \u2022 S[ ] \u21d2 a1 ... aiA[ ]1' 4 \u21d2 a1 ... ai ... ai Y2 Y 4 if and only if (E, h) = (-, -), , = -and (B,p, q) = ( -, -, -) . If Y 1 includes the dependent child of A[ ] then the indices stack associated to that dependent child is empty. This type of derivation refers to the prediction or completer of a non terminal A with an empty indices stack. The new set of items so defined is a refinement of the items in the parsing system IP E : the element , is used to store the top of the predicted indices stacks (in the parsing system IP E , , =for items resulting of a prediction) and the pair ( E, h) allows us to track the item involved in the prediction. With respect to the deduction steps, the completer steps must be adapted to the new form of the items in order to manipulate the new components E and h and the predicted steps must be refined taking into account the different types of productions. Schema 4 The parsing system IP Earley1 corresponding to the Earley-like parsing algorithm preserving the valid-prefix property for a linear indexed grammar g and a input string a 1 ... a n is defined as follows: 7 -{ [E, h lA\u2794Y1 \u2022Y2, ,,i, jlB ,p,q] I .LEarleYI -A\u2794Y1 Y2 EP , B, CEV N, ,EVJ,} 0 \ufffd h \ufffd i \ufffd j , (p, q ) \ufffd (i, j) v init ------------- Earley1 -[--I s \u2794 \u2022 r -0 0 I ---] ' ' ' ' ' ' [-, -I A[ ] \u2794 \u2022a, -, j, j I -,-,-], v scan _ [a,j, j + l] Earley1 -[ I A[ ] \u2022 \u2022 l I ] -, - \u2794 a\u2022, -, J, J + -' -' - v Pred[ ) _ [E, h I A\u2794 Y1 \u2022 B[ ] Y2, ,,i, j I C,p, q] EarleYI - [-, -I B \u2794 \u2022 1'3 , -, j, j I-,-,-] [E, h I A[oo,] \u2794 Y1 \u2022B[oo] Y2, ,,i, j 1 -,-,-], v Pred[ oo-y][ oo) _ _ [ M _ , m __ l E _ -+ __ . _ r _ 3 _ , , _ ' _ , _h _ , h _l _-_, _-_ , _-_] ____ _ Earley1 - [M,m I B-+ \u2022l'4,,', j, j I-, -,-] v Pred[oo][oo] = [E,h I A[oo] -t Y1 \u2022B[oo] Y2, ,,i, j 1 -, -, -] Earley1 [E, h I B \u2794 eT 3 , ,,j, j 1 -, -, -] v Pred(oo][oo-y] = [E, h I A(oo] -+ 'f 1 . B[oo,] Y2, ,', i, j 1-, -, -] Earley1 [A, i I B-+ el'3, ,,j,j I -, -,-] [E, h I A-t Y1 \u2022B[ ] Y2, ,,i,j I C, p, q] , 1)Comp[ ] _ [ -, -I B -+ l'3\u2022, -, j,k I-, -, -] EarleYI -[E, h I A-+ r 1 B[ ]. r 2 , ,, i, k I C,p, q] [E, h I A[oo 1 ] \u2794 Y1 \u2022 B[oo] Y2 , ,, i, j 1-,-, -], [M, m I e \u2794 \u2022Y3,, 1 , h, h 1 -, -,-], vComp[OO\"y)[oo] _ [M, m I B \u2794 Y4\u2022, , 1 , j, k I C, p, q] Earley1 - [E, h I A[oo , ] \u2794 Y 1 B[oo] \u2022 Y 2 , ,, i, k I B, j, k] [E, h I A[oo] \u2794 Y1 \u2022 B[oo] Y2 , ,, i, j 1-, -, -], vComp[oo][oo] _ [E, h I B \u2794 Y3\u2022, ,, j, k I C, p, q] Earley1 - [E, h I A[oo] \u2794 Y 1 B[oo] \u2022 Y 2 , 1 , i, k I C, p, q] [E, h I A[oo] \u2794 Y1 \u2022 B[oo 1 ] Y2 ,,' , i,j I -,-,-], [A, i I B \u2794 Y3 \u2022 , ,, j, k I C, p, q], vComp[oo][oo-y] _ [E, h I C \u2794 Y4\u2022, , 1 , p, q I D , r, s] EarleYl - [E, h I A[oo] \u2794 Y1 B[oo 1 ] \u2022 Y2 , ,', i , k I D, r, s] V 'D lnit U v scan U 'DPred[ ] U 'DPred[oo-y][oo] U 'DPred[oo][oo] U 'DPred[oo][oo-y] U Earley1 = Earley1 Earley1 Earley1 Earley1 Earley1 Earley1 'DComp[ ] U 'DComp[oo-y][oo] U 'DComp[oo)[oo] U 'DComp[oo][oo-y] EarleYl EarleYl Earley1 EarleYl fE arley1 = { [-, -I S \u2794 Y\u2022 , -, 0 , n I -, -, -] } The space complexity of the algorithm with respect to the length n of the input string is O(n 5 ), due to the five positions of the input string stored in each item. The time complexity is O(n 7 ) due to deduction steps in the set v\ufffd::?:} ; 0 H 00 -Yl . To reduce the time complexity we will use a technique similar to that used in [5, 2] to reduce the complexity of the tabular interpretations of automata for tree adjoining languages. In this case , we split each deduction step in v\ufffd::?:} ; 0 H 00 -Yl into two different steps such that their final complexity is at most O(n 6 ) . The resulting parsing schema is defined by the following parsing system. 6 ) for a linear indexed grammar g and a input string a 1 ... an is defined as fo llows: ' The LIG so generated does not satisfy our definition of shared forest because single parse trees can not be extracted in linear time. Vijay-Shanker and Weir [18] try to solve this problem by defining a non-deterministic finite state automaton that determines if a given LIGed forest symbol (A, i, j)[a:] derives a string of terminals. A similar finite-state automata is also defined by Nederhof in [11] . Schema 5 The parsing system lP Earley corresponding to the Earley-like parsing algorithm preserving the valid-prefix property working with a time complexity O(n I _ { [E, h I A \u2794 Y1 \u2022 Y2 , ,, i, j I B,p, q] I EarleyC 1 ) - A \u2794Y1Y2 EP, B, C EV N, ,EVi,} 0 \ufffd h \ufffd i \ufffd j , (p , q ) \ufffd (i,j) L -{ [[A \u2794 Y\u2022, ,, i , j I B,p, q]] I Earley< 2 > - A \u2794YEP , BEV N, } , E VJ , i \ufffdj , ( p, q)\ufffd( i, j) LEarley = .'.IEarleyC 1 ) U .'.I E arley< 2 > [A, i I B \u2794 Y3 \u2022 ,-y, j, k I C, p, q], vComp[oo][oo-y] 0 _ [E, h I C\u2794 Y4\u2022,-y 1 , p, q I D, r, s] Earley - [(B \u2794 Y 3 \u2022 ' 'Y, j, k I D, r , s]] [[B \u2794 Y3\u2022 , ,, j, k I D, r, s]], [E, h I A(oo] \u2794 Y1 \u2022 B[oo 1 ] Y2 ,,' , i ,j 1-, -, -], vComp[oo][oo-y] 1 _ [E, h I C\u2794 Y4\u2022, , 1 , p,q I D, r, s] Earley - [E, h I A[oo] \u2794 Y1 B[oo 1 ] \u2022 Y2 ,'Y' , i , k I D , r, s] V v init u v scan u v Pred[ l u v Pred[oo-yl[oo] u v Pred[oo][oo] u v As an alternative approach, Boullier [4] defines the shared forest for a LIG g = (Vr, VN , Vi, P, S) and an input string w by means of a linear derivation grammar, a context-free grammar recognizing the language defined by the sequences of LIG productions of g that could be used to derive w. Previously to the construction of the linear derivation grammar, we must compute the transitive closure for a set of relations on V N x V N. To avoid the use of additional data structures, such as finite automata or precomputed relations, we have been inspired by the use of context-free grammars to represent the parse forest of tree adjoining grammars [18] in order to capture the context-freeness of production application in the case of LIG. Given a linear indexed grammar g = (Vr , VN , Vi, P, S) and an input string w = a1 ... a n , the shared forest for g and w is a context-free grammar g w = (Vr, VJ!j , p w , s w ). Elements in VJ!./ have the form \u21d2 a p+ I ... ar E[a] a s+ I ... a q (A, 17,i, j,E,r,s) \u2794 (B,\"Y,i, j,D, p, q) (D, 17, p, q, E, r, s) then In cases 4a, 4b and 7, derivations starting at (D, 17, p, q,E,r,s) allow us to retrieve the rest of the indices stack corresponding to A. Note that we are assuming a grammar with productions having at most two children. Any production A 0 [oo\"Y] \u2794 A1 [] ... Ad[oo\"Y'] ... A m [] can be translated into v'o[ ] \u2794 c v' 1 [00] \u2794 v'o[oo] Ai[] y' d -1 (00] \u2794 y' d-2 (00] Ad -d ] v' d[oo\"Y] \u2794 v' d-1 [] Ad[oo\"Y'] v' m [oo] \u2794 v' m-1 (oo] A m [] Ao[oo] \u2794 v' m [oo] where the v' i are fresh symbols that represent partial recognition of the original production. In fact, a v' i symbol is equivalent to a dotted production with the dot just before the non-terminal Ai + I or with the dot at the end of the right-hand side in the case of v' m \u2022 It is interesting to remark that the set of non-terminals is a subset of the set of items for CYK like and bottom-up Earley-like algorithms, and Earley-like algorithms without the VPP. The case of the Earley-like algorithm preserving the valid prefix property is slightly different, as a non-terminal (A, \"Y, i, j, B,p, q) represent the class of items [E, h I A, \"Y,i, j I D, p, q] for any value of E and h. Like context-free grammars used as shared forest in the case of TAG [18] , the derivations in g w encode derivations of the string w by g but the specific set of terminal strings that is generated by g w is not important. We do however have the language generated by g w is not empty if and only if w belongs to the language generated by g. We can prune g w by retaining only production with useful symbols to guarantee that every non-terminal can derive a terminal string. In this case, derivations of w in the original grammar can be read off by simple reading off of derivations in g w . The number of possible productions in g w is O(n Conclusion We have described a set of algorithms for LIG parsing, creating a continuum which has the CYK like parsing algorithm by Vijay-Shanker and Weir [16] as its starting point and a new Earley-like algorithm which preserves the valid prefix property as its goal. In the middle, a new bottom-up Earley-like algorithm and a new Earley-like algorithm have been described. The time complexity for all these algorithms with respect to the length of the input string is O(n 6 ). Other algorithms could also have been included in the continuum, but for reasons of space we have chosen to show only the algorithms we consider milestones in the development of parsing algorithms for LIG. Introduction NLP applications require efficient NLP core components both in terms of linguistic quality and throughput. Different NLP applications have different efficiency constraints and this reflects on each component. Several text processing applications include syntactic parsers as core components. Cus tomizing parsing processors enable the reuse of these components in different application scenarios. Let us consider as an example a real time application like a front-end question-answering for on-line services. Here fa st are preferred to accurate parsing processors. Target sentences are rather simple and structures are recurrent. For example, booking train tickets is often expressed by sentences like: At what time is the next train fro m Rome to Paris ?. A parsing processor able to produce partial structures like At what time and from Rome to Paris is sufficient to support a deductive machinery that answers the question. Complex analysis, e.g. clause boundary recognition, is not relevant as very short sentences (with high expectation about the discourse domain) are always used. On the other hand, an event recognition (ER) task in an Information Extraction (IE) [20, 21] scenario asks for accurate syntactic material over complex sentences. As an example, let us consider the Penn Tree-bank [19] sentence #1692(9) : ( wsj_l692(9 )) As part of the agreement, Mr. Gaubert contributed real estate valued at $ 25 million to the assets of Independent American. The focus here is on the extraction of the event mainly suggested by syntactic relations established by the verb to contribute. Clause embedding, i.e. valued at $ 25 million, plays here an important role. As a consequence , deeper parsing, relying on a more expressive grammar, is mandatory. It is also worth noticing that applications may differ in the type of necessary syntactic relations. In order to limit the time complexity, underlying grammars should thus be designed to efficiently cover specific phenomena of interest. All and only the information necessary to cover the specific target phenomena with the suitable quality should be used. A key issue is that limited coverage (i.e. low time complexity) and high confidence are conflicting requirements for the kind of grammatical competence available to the parser. Application developers search for technologies for the largest coverage and confidence of specific phenomena. Shallow parsing [1, 2, 9, 4] , introduced in the perspective of improving time performances, is, alone, inherently weak and often lexical sensitivity has been suggested as successful approach. In order to increase accuracy; syntactic parsing processors usually exploit lexical information; lexicalized grammar formalisms have been widely proposed (e.g. HPSG [22] , LTAG [16] , LFG [12] ) at this scope, although in frameworks (i.e. linguistic theories) targeted to full syntactic analysis [13] . On the contrary, applications require effective methods for specific phenomena. Flexibility is thus the crucial factor for the success of parsing technologies in applications. It is our opinion that the integration of lexicalized approaches in frameworks for shallow parsing [7] is a relevant area of research. Building upon the idea of grammar stratification [1] , we propose a method to push modularity and lexical sensitivity in parsing in view of customizable syntactic analysers. Supporting modularity within the parsing process requires: \u2022 a formal and homogeneous definition/representation for the partial parsing results able to support information sharing among subcomponents; \u2022 principles for coherent composition of parsing subcomponents able to ease the design of application specific parsers; \u2022 methods for the systematic control of ambiguity within as well as among the components(i.e. throughout a chain of interactions); \u2022 the detection of specific language phenomena where lexical information is relevant to the control of ambiguity, so that specific lexicalized components can be designed to reflect it. In this paper a framework for modular parsing is presented. The principles for the stratification of the grammatical analysis and their implications on modularity are defined in the next section. In Section 3 the notion of syntactic module is introduced and a classification according to basic grammatical properties of the different modules is given. In the same section an annotation scheme useful for information exchange among modules is defined as a combination of a dependency and constituency based formalism. Implications on grammatical properties and parsing architectures are then discussed. Finally, section 4 discusses the evaluation of some parsing architectures within a typical application scenario. Fitting parsing performance through stratification and rnod ularization The interest of NLP application developers is in customizing a parser in order to meet application quality and time constraints. Final performances depend on the adopted trade-off between the two. It \u2022 is widely accepted that computational lexicons increase the quality of the syntactic information produced by a parser [8] : this improvement is tightly dependent on the specific language level to which lexical information refers. The stratification of a grammar resulting from a modular decomposition of the parser should facilitates the use of lexical information specific to each level. Let us again consider the ( wsj _1692( 9 )) example and suppose that verb subcategorization informa tion is available. The verb to contribute would be associated to a direct obj ect and to a recipient (or benef iciary) argument as well. This would result in a frame like contribute-NP-PP (to) 1 . The other verb in the sentence, to value, would be associated to its obj ect (i.e. the evaluated entity) and to a prepositional phrase expressing the \"degree/amount\" (usually ruled by the preposition at), i.e. value-NP-PP (at) . A strategy using a combination of clause boundary recognition and a verb argument detection algorithms could decide that : (i) valued is linked to at $ 25 million; (ii) contributed is linked to to the assets. At the level of PP-attachment, most of the ambiguities in the sample sentence disappear since they are resolved by lexical information. Firstly, links derived on lexical basis (i.e. attachment of verb argumental modifiers) have important effects on the remaining ambiguities: other potential attachment sites of argumental PPs like at $ 25 million and to the assets are discarded. Secondly, persistent ambiguity is reduced. T . he ( of Independent American)pp structure is no longer allowed to attach to nouns like real estate or million as illegal bracket crossing of the clause related to contribute would be generated : as a result the only allowed attachments are those with the verb contribute itself or with the noun assets. The search space of the parser during the above lexicalized process depends on the number of sentence words. If an early parsing phase, i.e. chunking [l] , is applied, later parsing steps ( e.g. the detection of verb modifiers) deal with a much lower amount of ambiguity. Chunking is widely adopted to recognize sentence fragments whose boundaries are independent from the verb grammatical projections. In the example sentence ( wsj _1692(9)), noun phrases ( e.g. Mr. Gaubert, real estate) and modifiers ( e.g. to the assets, at $ 25 million) are simple examples of these segments. The detection of verb modifiers is disburdened since it has to deal only with the representative elements of the recognized structures. The above example is a simple instance of a phenomenon (i.e. verb subcategorization) that plays a relevant role in the control of the ambiguity propagation throughout the search space of the parser. The level (i.e. after chunking) in which this algorithm is applied and the used lexical knowledge are crucial for optimizing the derived advantage: \u2022 the use of chunks provide an optimal representation as the search for verb arguments is limited to chunk heads; \u2022 the adopted lexical knowledge (i.e. subcat frames) in this specific process is a well focused compo nents of a lexical KBs ; \u2022 the verb argument detection suggested by the example strongly interact with other parsing activities (e.g. detection of non-argumental and nominal modifiers), with positive side-effects on the reduction of ambiguity. The above properties are not specific to this kind of modular decomposition (i.e. chunking + verb_phrase_parsing) but can be generalized to a variety of other potential decompositions. The .:.. effects of lexical information within each component increase the accuracy with respect to each tar get specific (sub )problem. Modularity thus optimizes the lexical effects on the control of ambiguity throughout chains of specific parsing steps. The adoption of a modular view in parsing supports a more flexible design (via composition of simpler subcomponents in different parsing architectures) and the throughput control is explicit. First throughput constraints can be met via simplification (i.e. removing not crucial subcompo nents) of the overall architecture. If a modular design is adopted, functionalities of modules and functional dependencies are well-defined. Eliciting processing capabilities consists in removing mod ules from the parsing architecture. Moreover, modularity again helps in the control of losses in accuracy over the target phenomenon due to the removal of modules. As an example, let us consider a parser aiming to determine NP boundaries in order to detect candidate terms within a Terminology Extraction process. The removal of a verb argument recognition module would increase the parser throughput, by reducing also the resulting precision. In the example ( wsj _1692( 9 )), the lack of verb subcategorization information provides, as a potential NP, the wrong excerpt $ 25 million to the assets of Independent American. It is only by means of a well-defined notion of verb argument detection component that a systematic measure of the trade-off between accuracy and throughput can be controlled and employed as a design principle. In the next section, a method for designing modular parsing systems is introduced able to support principles of lexicalization and decomposition. A modular approach to parsing A syntactic processor SP, according to the classification given in [3] , is a linguistic processing module. It is a function SP(S, K) that, exploiting the syntactic knowledge K, produces a syntactic representation of the input sentence S. The stratification of the grammar induces modularization of the syntactic processor. The general module component \ufffd takes the sentence at given state of analysis Si and augments this information in Si +I exploiting the knowledge Ki. The parser SP is thus a cascade of this modules. It is crucial to define how the syntactic information produced and processed is represented. The stratification of the parsing activity requires that the representation scheme adopted satisfies some requirements. In fact, on the one hand, stratified parsing techniques require the handling of partially parsed structures (cf. Sec. 2). On the other, lexicalized approaches require that the heads of some types of phrases are accessible during the analysis. In sec. 3 .1, classical representations are discussed from the point of view of a modular perspective. Then, we propose, in sec. 3.2 , an annotation scheme that satisfies the two requirements, some properties of the annotation scheme are discussed and some restrictions, i.e. planarity constraints, proposed. Finally, a classification of the modules is given in Sec. 3.3 according to the kind of information K used and to the typical actions they perform in augmenting the syntactic knowledge gathered for the input sentence. Modularity vs. annotation scheme Modularization and lexicalization impose strict requirements on the annotation scheme used to de scribe the syntactic information that the processors gather for a target sentence. In a modularized approach, a stable representation of partially analyzed structures is crucial. In particular, it is required to handle the representation of long-distance dependencies. Fo r instance, considering the example (wsj_l692( 9 )), at a given state of the analysis could be necessary to express that contributed is linked to to the assets. In a constituency-based framework [11], it is quite hard to express the above relation without specifying the role of the excerpt real estate valued at $ 25 million. Furthermore, in the same framework, the relation between contiguous constituents can not be expressed if the constituent captured is not completely formed. In the excerpt of the example sentence ( wsj _1 692( 9 )) contributed real estate valued at $ 25 million, the relation between contributed and real estate can be expressed only if the constituent real estate valued at $ 25 million has been fully recognized. Extensions of constituency-based theories such as TAG [17] and D-'frees [23] allow to express discontinuous links and partial trees. From this point of view, a dependency-based syn tactic [24] representation is preferable, since constituency-based approaches in the annotation are not naturally conceived for the representation of distant dependencies without specifying the role of inner structures . On the other hand, a fully dependency-based syntactic approach generally considers the words of a sentence as basic constituents . Thus, each analyzing step has to deal with the same simple constituents: no packing of information is allowed . However, packing is important in a modular ap proach . A processor using verb subcategorization frames as suggested in section 2, would be enhanced by looking at the candidate complements as single structures . Fo r instance (wsj_l692), the analysis of the complements of the verb contribute -NP -PP (to) is disburdened if the candidate excerpt of the sentence were fac torized in its chunks [r eal estate}[v alued}[at $ 25 million}[to the assetsj[of Independent American]. In fact, the argument PP(to) can be easily filled with the chunk [to the assets]. In a lexicalized approach, it is crucial to determine the potential governor [14] of a given structure that is its semantic head [22] and activates lexicalized rules. For instance, given the structure [has widely contributed], the annotation scheme should allow to express that the lexical item governing its behavior is contribute. Extended dependency graph To satisfy the requirements imposed by the modularization and the lexicalization, the adopted annotation scheme is a combination of the constituency-based and the dependency-based formalisms . Basically, the syntactic information associated to a given sentence is gathered in a graph, i.e. g = (n, a). The typed nodes (i.e . elements of n) of the graph g are the basic constituents of the sen tence, while the typed and oriented arcs (i.e. elements of a) express dependencies between constituents (an head and a modifier). Since the order of constituents is important, the set n is an ordered set . Fo r the purposes of the syntactic parsing, nodes can represent sequences of words, i.e . constituents, that can degenerate in a single word. To satisfy the constraint arisen by the lexicalization, a function h that spot the head of each constituent has been introduced. The representation should allow to express the type of each constituent and each arc . The possible types, elements, respectively, of the sets NT AG and AT AG, depend on the underlying grammar model . In the following, we refer to those representation graphs as eXtended Dependency Graph (XDG) that is defined as follows: Def. 1 An XDG is a tuple XDG =< n, a, Ntag, Atag, h > where n are the nodes, a are the arcs, Ntag is the function that relates n with the set of NT AG, Atag is the function that relates a with the set of AT AG, and h is the function that elects for each node a representing head. For sake of simplicity, we introduce a compact version G = ( N, A) of the X DG. The compact version is a transcription of the X DG defined as follows: Def. 2 G = (N, A) related to XDG is such that N = {(node, tag, head) lnode E n, tag = Ntag(n), head = h(n)} and A= {(arc, tag) larc E a, tag = Atag(arc)}. The proposed XDG allows to model the grammatical information, i.e. the detected relation and persisting ambiguity, in an efficient way. In an XDG alternative interpretations coexist . In general, more than one interpretations projected by the same nodes are expressed by the same representation graph that, by itself, do not allow multiple interpretations of the nodes. The ambiguity at this level can be modeled with an inherent proliferation of the interpretation graphs . This limitation is an inheritance of the dependency-based theory. Generally, in these theories, words in an interpretation representation belongs to exactly a single word class ( cf. [10] ). The XDG represents a single syntactic interpretation only if it is a dependency tree ( defined in (10]). In term of constraints on the XDG, the requirement translates in the property that \u2022 forbids multi-headed nodes (24] : For instance, in the example (wsj_l692( 9 )), an interpretation willing to be a single unambiguous syntactic representation of the sentence can not include both the relations {[v aluedj,[at $ 25 million] ) and ([contributedj,[at $ 25 million] ). In order to preserve the compatibility in the proposed representation with the constituency based approach, the property (Prop. 1) is not enough . Not enabling crossing links may be required . Cro ssing links are defined as follows: Def. 3: Cro ssing links Two links, (wh ,wk ), (wm ,w n ) E A where min{h, k} < min{m, n}, are crossing iff min{m, n} < max{h, k} < max{m, n}. The planarity property [15] can, thus, be introduced: Prop. 2: Planarity Vii, l2 E A.l1, l2 are not crossing. The two properties, Prop. 1 and Prop. 2, are called planarity constraints and make a X DG that satisfies them a planar graph. An XDG satisfying planarity constraints is a single (partial) syntactic interpretation . Consequently, since a viable single interpretation of the sentence must be a planar graph, an interpre tation in which crossing links coexist is ambiguous . In the example, if both the relations ([v aluedj,[of Independent America]) and {[contributedj,[to the assets]) coexist, the interpretation is ambiguous. Parsing modules A component P of the modular syntactic parser is a processor that, using a specific set of rules R, adds syntactic information to the intermediate representation of the sentence . Formally, a processor P is a function P(R, G) where R the knowledge expressed in a specific set of rule, and G the input graph . The result P(R, G) = G' is still an XDG. The syntactic parser modules are classified according to the actions they perform on the sentence, and to the information they use to perform these actions . The actions that modules perform on the input XD G can be conservative or not-conservative. In the case of conservative modules, all the choices contained in the input graph are preserved in the output. The property is not true for the not conservative modules. A conservative module results in a monotonic function of the module. A not-conservative module is a not-monotonic function. A syntactic processor is a cascade of processing modules. Note that the composition of modules preserve, where it exists, the monotonicity. Furthermore, since the representation of the syntactic information is an XDG, the ability of the modules refers to: (i) constituent gathering ; (ii) dependency gathering. Under this distinction, processors are: \u2022 constituent processors, P c , that are purely constituent gatherer ; \u2022 dependency processors, P d , that are purely dependency gatherer ; \u2022 hybrid processors, P h , that perform both dependency and constituent gathering. Starting by a model of the process as previously described, i.e. P(R, G) = G', where G = (N, A) and G' = (N', A'), and by the distinctions introduced, a description of the typology of processing modules used in the whole parsing processor will be provided. The description is in term of the action they perform on the syntactic graph. The main characteristic of the processors of the typology P c is that, in the changing of the con stituents (i.e. nodes of the representation) the arcs between constituents are coherently translated, i.e. for each arc in A, there is the correspondent arc in A' if it connects different nodes. Fo r this typology of modules, a monotonic processor Pf preserves the property of not crossing-brackets between the input and the output, i.e. N' is a partition of N or vice-versa. A not-monotonic processor Pf M does not satisfy this property. In the monotonic processors, we distinguish : We now analyze how, according to this taxonomy, a tokenizer T and a chunker [1] can be classified. The aim of a tokenizer is to split a sentence S = c 1 c2 ... Cm represented by a stream of characters in its composing words S' = w 1 w 2 \u2022\u2022 \u2022 Wn. It is a P/{ module. In fact, the input is a graph whose set of node represents the stream of characters, i.e. G = ( { ( c 1 , char, c 1 ), ... , (C m , char, cm )}, 0), while the output G' models the words, i.e. G' = ( { ( w 1 , token, w 1 ) , . . . , ( Wn, token, wn)}, 0). The relation between A and A' satisfies the constraint of the module typology. A chunker [1] falls in the typology P c \ufffd. As, a Chunker is a rewriting device of input sentences, according to the available chunk prototype(CP) [6] . The objective of the chunker function is to build the chunk representation cs = ch 1 ... ch m corresponding to each input sentence ws = w 1 ... W n . Each chunk chi is the instance of a chunk prototype in GP and is a sequences of words that does not overlaps other chunk of the sentence. Then, in the proposed framework, the chunker transforms G = ( {(w 1 , m 1 , w 1 ), ... , (w n , mn, w n ), A)} in G' = ( { (ch 1 , cht 1 , h 1 ), ... , (ch n , chtn, hn), A)}, A'), where mi is the pos-tag of the word Wi , chti and hi are respectively the type and the head of the chunk. The main characteristic of the processor of the type P d is that in the processing the property N = N' is met. Fo r the not-monotonic processors Pf M of this type no additional property is required. Monotonic processor we adopt in the architecture are defined as follows: The module of unambiguous projection Prj(Fig. 1 .( 4 )) aims to project a given XDG on the unam biguous subgraph removing colliding arcs. This is a grammar-based dependency module of the type PNM d To meet the requirements of an application, different chains of analysis can be arranged. Note that in the present configuration, the \u2022 Verb Shallow Analyzer and Shallow Analyzer modules wo rk at the higher level of performance if the specific chunker is used. A chain Chunker-Verb Shallow Processor can be sufficient for an IE application devoted to extract events from sentences if the events prototypes are well described by the verb subcategorization frames . . On the other hand, for Lexical Acquisition applications such as verb subcategorization frames acqui sition that requires a high coverage of the phenomena [5] , a parser composed by the Chunker and the Shallow Analyzer is sufficient. Fo r an application as Terminology Extraction focussed on Noun Phrase boundary recognition, from the point of view of typology of the phenomena covered a chain composed by the Chunker and the Shallow Processor is enough, but the performance are not sufficient for the task. Thus, a chain Chunker-Verb Shallow Processor-Shallow Processor is required to augment the performance. Task oriented parser design We here analyze how to choose parsing chains for given application scenarios through the investi gation of their performances. The examined applications are event recognition in an IE context, and candidate term boundary detection in a Terminology Extraction framework. Performances in term of quality of the syntactic material are evaluated through the metrics of Recall, Precision and F-measure. Given a grammatical relation r (e.g. NP -PP) , metrics defined as follows: ( a) R T _ card((A:nA;)) (b) card(A\ufffd) p r _ card (( A:nA; )) (c) car d ( A; ) pr(a) -1 -(o: }r + (1-o:) Jir ) (1) A\ufffd are the correct syntactic relations of type r for the sentence, and A; are the syntactic relations of type r extracted by the system. The oracle used is obtained via a translation from the Penn Treebank [19] . The translation of the PTB constituency-based to the dependency-based annotation scheme, compliant with the evaluation r\ufffdquirements, is a crucial problem. Translation algorithms have been settled in previous works [18, 6) . In the present work the adopted translation algorithm left untranslated about 10% of the oracle trees(i.e. reference corpus trees). The resulting evaluation test-set consists of nearly 44,000 sentences. For the event recognition, three parsing chains have been tested: two light and one lexicalized. The first composes the chunker, the shallow analyzer and the disambiguator, i.e. Chunker-SP-Prj, the second remove the disambiguator, i.e. Chunker-SA, and the third introduces the lexicalized verb shallow analyzer, i.e. Chunker-VSP-SP-Prj. The interest here is in extracting relations whose verb is the head (V-Sub, V-Obj, and V-PP). Table 1: verb arguments Analyzing the table 1, from the point of view of the coverage of the phenomena, a better architecture appears to be Chunker-SP, but it guarantees a low level of precision compared to the other two. In case the interest of event extraction is in populating a database of facts, the most suitable process is the chain that guarantees the higher precision degree : the chain Chunker-VSP-SP-Prj. While, if the developer will feed an information retrieval system, the chain Chunker-SP-Prj is more appropriate. In the case of NP recognition that Terminology Extraction (TE) requires, the application is inter ested in the relation typed NP-PP. Experimental evidence shows that the coverage of the phenomena is assured by a chain Chunker-SP, but the quality of the syntactic material is improved through the use of triggers provided by verb subcategorization lexicon in the chain Chunker-VSP-SP. The trade off between the cost of the system in term of subcategorization lexicon production and the performance required is another factor to be considered. The table 2 shows experimental results. Parsing chain Link Type R p In a TE chain where the filtering is based upon statistical methods, the chain Chunker-SP is light and assures an higher coverage of the phenomena. While in a TE chain where the filtering is done manually, an high degree of precision disburden the work of the terminologists. The improvement with respect to the precision from Chunker-SP to the chain Chunker-VSP-SP, even if there is a loss in the recall, may justify the cost in term of time complexity of choosing the Chunker-VSP-SP instead of the Chunker-SP. F(o. = 0. Conclusions A framework for modularization of the parsing process that eases their customization to the applica tions has been here described . The notion of syntactic module has been introduced and a classification according to basic grammatical properties of the different modules has been provided . Particular at tention has been given to the syntactic annotation scheme. A useful syntactic information \"holder\" for the exchange among modules has been defined as a combination of a dependency and constituency based formalisms . An application of the given framework has been proposed. It has been shown and measured how different NLP applications may select an appropriate parsing chain according to their requirements . RANGE CONCATENATION GRAMMARS 1 Introduction The great number of syntactic formalisms upon which natural language (NL) processing is based may be interpreted in two ways: on one hand this shows that this research field is very active and on the other hand it shows that, at the evidence, there is no consensus for a single formalism and that the one with the righ t properties is still to be discovered . What properties should have such an ideal formalis m? Of course, it must allow the description of features that have been identified so far in various NLs, while staying computationally tractable. We know that, due to their lack of expressiveness, context-free grammars (CFGs) cannot play this role (See (S hieber, 1985]). Ye t, context sensitive grammars are powerful enough although they are too greedy in computer time. A first answer is given by the notion of mild context-sensitivity. This notion is an attempt to express the formal power needed to define NLs (see [Joshi, 1985) and [Weir, 1988) ). However, there exist some phenomena such as large Chinese numbers or word scrambling that are outside the power of mildly context-sensitive . (MCS) formalisms. In this paper, we present a convincing alternative: the range concatenation grammars (RCGs). RCG is a syntactic formalism which is a variant of the simple version of literal movement grammar (LMG), described in [Groenink, 1997 ] , and which is also related to the framework of LF P developed in [Rounds, 1988) . In fact it may be considered to lie halfway between their respective string and integer versions; RCGs retain from the string version of LMGs or LFPs the notion of concatenation, applying it to ranges ( couples of integers which denote occurrences of substrings in a source text) rather than strings, and fro m their integer version the ability to handle only (part of ) the source text (this later feature is the key to tractability). The rewriting rules of RCGs, called clauses, apply to composite objects named predicates which can be seen as nonterminal symbols with arguments. We have shown that the positive version of RCGs, as si mple LMGs or integer indexing LF Ps, exactly covers the class PTIME of languages recognizable in deterministic polynomial \u2022 time. Since the composition operations of RCGs are not restricted to be linear and non-erasing, its languages (RCLs) are not semi-linear. Therefore, RCGs are not MCS and they are more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker, Weir, and Joshi, 1987] , 1 while staying computationally tractable: its sentences can be parsed in polynomial time. However, this formalism shares with LCFRS the fact that its derivations are CF (i.e., the choice of the operation performed at each step only depends on the object to be derived from) . As in the CF case, its derived trees can be packed into polynomial sized parse forests. Besides its power and efficiency, this formalism possesses many other attractive properties. Let us emphasize in this introduction the fact that RCLs are both closed under intersection and complementation, and, like CFGs, RCGs can act as syntactic backbones upon which can be grafted decorations from other domains (probabilities, logical terms, feature structures) . This paper studies the full class of RCGs and presents a polynomial parse time algorithm . Positive Range Concatenation Grammars Ranges & Bindings If we consider a derivation in a CFG, headed at the start symbol and leading to some sentence, we know that each nonterminal occurrence is responsible for the generation of the substring laying between two indexes say i and j. For a given input string w = a 1 \u2022\u2022\u2022 a n , such a couple (i, j) is called a range. We know that in CF theory, ranges play a central role. For example the ( unbounded number of) parse trees, associated with some input string w can be represented by a CFG, called shared forest [Lang, 1994] . Its nonterminal symbols have the form (A, i, j) where A is a nonterminal of the initial grammar and (i,j) is a range in w. In an analogous way, ranges are the core of our formalism. In the sequel terminal symbol in T are denoted by early occurring lower case letters such as a, b, c, . We shall use several equivalent denotations for ranges in R w . If w = WI w 2 w3 with WI = aI ... ai, w2 = ai+I ... ai and w 3 = ai+I ... a n , the range (i, j) can be denoted either by an explicit dotted term WI \u2022 w 2 \u2022 w 3 , or by (i .. j) w , or even by (i .. j) when w is understood or of no importance. Given a range (i .. j), the integer i is its lower bound, j is its upper bound and j -i is its size. A range such that i = j is an empty range. The three substrings WI , w 2 and w 3 associated with (i .. j) are respectively denoted by w( o .. i ), w( i .. j ) and w( j .. n) . Therefore we have, wU--i ) = c, wU-1 In any PRCG, terminals, variables and arguments in a clause are supposed to be bound to ra nges by a substitution mechanism. Any couple (X, p) is called a variable binding denoted by X/ p, p is the range instantiation of X, and w P is its string instantiation. A set u = { Xi / PI , ... , X p / p p } of variable bindings is a variable substitution iff Xi / Pi =f:. Xi / Pi => Xi =/:-X j . A couple (a, p) is a terminal binding denoted by a/ p iff p = (j -l..j} and a= ai. The concatenation of ranges is a partial (associative) binary operation on R w defined by (iI--] l } w (i2 .. J2 ) w = (iI--J2 } w iff )1 = i2. If we consider a string w E T*, a string a= UI ... Ui ... u p E (T U V)*, a variable substitution u and a range p E R w , the couple ( a, p) is a string binding for u, denoted a/ p iff \u2022 for each Ui there exists a ra nge Pi s.t. -if Ui E V ' Ui I Pi E (J\"' -if Ui E T, ui/ Pi is such that Ui = w Pi , Derivation, Language, Derived Tree & Shared Forest G,w An input string w E T*, lwl = n is a sentence iff the empty string ( of positive instantiated predicates) can be derived from S( (O .. n) ), the positive instantiation of the start predicate on the whole source text. More generally, we define the string language of a nonterminal A by \u00a3,(A) = U wE T .. \u00a3,( A, w) where \u00a3( A, w) = {w P I pE Rt, h = arity(A), A( p) +\ufffd .s}. However, with this definition, we can note that G,w \u00a3,(S) and \u00a3,( G) are diffe rent. Consider the grammar G s.t. S(X) \ufffd A(Xa) A(X) \ufffd C We can see that we have \u00a3,( G) = 0 and \u00a3,(S) = T*. 3 In fac t, we have \u00a3,( G) C \u00a3,( S), and the equality is reached for non -increasing grammars. 4  As in the CF case, if we consider a derivation as a rewriting process, at each step, the choice of the (instantiated) predicate to be derived does not depend of its neighbors (the derivation process is context-free). All possible derivation strategies can be captured in a single canonical tree structure which abstracts all possible orders and which is called a derived tree (or parse tree). Fo r any given A( p)-derivation, we can associate a single derived tree whose root is labeled A( p). Conversely, if we consider a derived tree, there may be associated derivations which depend upon the way the tree is traversed (for example a top-down left-to-right traversal leads to a leftmost derivation). Note that from a deriva tion step (r, I''), it is not always possible to determine which predicate occurrence in r has been derived. Moreover, even if this occurrence is known, the clause used cannot be determined in the general case. This is due to the fact that A 0 (p 0 ) \ufffd A 1 (fi 1 ) ... A m ( P\ufffd ) may be an instantiation of diffe rent clauses. But, of course, each of these interpretations is a va lid one. Consider a k-PRCG G = (N, T, V, P, S ), a terminal string w and the set V w of all complete S( \u2022w\u2022)/.s-derivations. We define a terminal-free CFG G w = (N x Rt, 0, P w ,S (\u2022w\u2022)) whose set of rules P w is formed by all the instantiated clauses A o (p 0 ) \ufffd A 1 (p 1 ) ... A m (p\ufffd) used in the derivation steps in V w . This CFG is called the shared fo rest for w w.r.t. G. Note that, if V w is not empty, the language of a shared forest for w is not { w}, as in the CF case, but is { .s}. Moreover, this shared forest of polynomial size may be viewed as an exact packed representation of all the (unbounded number of) derived (parse) trees in G for w: the set of parse trees for G on the input w and the set of parse trees of its associated CF shared forest G w (on the input .s), are identical. + The arguments of a given predicate may denote discontinuous or even overlapping ranges. Fun damentally, a predicate name A defines a notion (property, structure, dependency, ... ) between its arguments, whose associated ranges can be arbitrarily scattered over the source text. PRCGs are therefore well suited to describe long distance dependencies. Overlapping ranges arise as a consequence of the non-linearity of the formalism. For example, the same variable (denoting the same range) may occur in different arguments in the RHS of some clause, expressing different views (properties) of the same portion of the source text . Note that the order of predicate calls in the RHS of a clause is of no importance (in fact, RHS of clauses are sets of predicate calls rather than lists). Example 1 As an example of a PRCG, the fo llowing set of clauses describes the three-copy language {www I w E {a, b}*} which is not a CFL and even lies beyond the fo rmal power of tree adjoining grammars (TAGs) . A(c, c,c ) S( XYZ) A( aX, aY, aZ) A( bX, bY, bZ) -t A( X, Y, Z) -t A( X, Y, Z) -t A( X, Y, Z) -t c Below, we check that the input string w = a1 b2a3b4a5b6 is a sentence. Of course, indices are not part of the input letters, they are used to improve readability of ranges: fo r each couple (l etter, index), we know where letter occurs in the source text. At each derivation step, we have made clear both the clause and the variable substitution used. S(XY Z)-A(X,Y,Z) +\u21d2 A( a1b2, a3b4, a5b6) S( XYZ) -t L( X) eq(X, Y) eq(X, Z) L(c) L( Xa) L( Xb) L( Xc) -t L( X) -t L( X) -t L( X) where the equality predicate eq is defined by eq(Xt, Yt) -t eq(X, Y) eq(c, c) -t c in which the first clause is a schema over all terminals t E T. The term PRCG (resp . NRCG) will be used to underline the absence (resp. presence) of negative predicate calls . In a NRCG, the intended meaning of a negative predicate call is to define the complement language (w.r.t. T*) of its positive counterpart: an instantiated negative predicate succeeds iff its positive _ counterpart (always) fails . This definition is based on a \"negation by failure\" rule . More formally, let G = (N, T, V, P, S) be a RCG, and let w be a string in T*. The set of negative instantiated predicate IP:; is defined by IP\ufffd = {A(p) I A( p) E I\ufffd }, 5 and the set of instantiated predicate IP w is defined by IP w = I\ufffd U IP\ufffd . For RCGs, we redefine the positive derive relation + => in the following way. If a;,/ Pi E w, 6 we have, either if \"Pi = Ai(ai) then <Pi = Ai(Pi ), or if \"Pi = Ai(a;,) then <Pi = Ai(Pi )-Note that negative instantiated predicates cannot be derived fu rther on by positive derive relations . We also define on strings of instantiated predicates a negative derive relation, denoted by -=>. If G,w r 1 and r 2 are strings in (IP w )*, we have iff A( p) is a negative instantiated predicate such that \u2022(A(p), .s-) i+\ufffd. G,w Note that this definition of -=> \"erases\" negative instantiated predicates whose positive counterpart G,w is not related to the empty string ( of instantiated predicates) by the transitive closure of + =>. As a G,w co nsequence of this definition, the structure (parse tree) associated with a negative derivation step is void, and, more generally, the structure of the (complement) language associated with a negative predicate call is void. In other words, within the RCG formalism, we cannot define any structure between a negative predicate call and the parts of an input string that it selects . Let --=> be any subset of -=>. We define a positive/negative derive relation \u00b1=> by G,w G,w G,w \u00b1=> = + => u --=> G,w G,w G,w We say that \u00b1=> is consistent ( otherwise inconsistent) iff for each A( p) E I\ufffd we have either G,w A( p) \u00b1\ufffd .s-or A( p) \u00b1=> .s-, but not both . Note that the existence of both such derivations would sho\ufffd G,w G,w that the tuple of strings w il simultaneously belongs to the language of A and to its complement! 5 The previous definition of IP;;, still holds for (N)RCGs. We say that a grammar G is consistent if for every w E T* , there exists a consistent relation \u00b1\u21d2 { (S(p ) , S(p )) , (S(p ) , c) } { ( 8 ( p) , 8 ( p)), ( 8 ( p) , c) , ( S ( p) , c ) } thus G is inconsistent. C \u00b1\u21d2 G,w + C \u00b1\u21d2 G,w In the sequel, we shall only consider consistent RCGs. Of co urse, PRCGs are always consistent. Definition 5 The (s tring) language of a RCG G = (N, T, V, P, S) is the set \u00a3( G) = {w I S(\u2022w\u2022 ) \ufffd c} G,w The language of inconsistent RCGs is undefined. In the sequel, without loss of generality, we will both prohibit clauses whose RHS contains arguments that are in T*, and assume that no argument has more than one instance of any variable. A Parsing Algorithm for RCGs In our prototype system, we have implemented a RCG parser which is based upon the algorithm depicted in Table 1 . Let G = (N, T, V, P, S) be a k-PRCG for which we also consider Pa s a sequence of clauses denoted by P. In this algorithm, the functions clause and prdct are both memoized : their returned values are kept in auxiliary l+k-dimensional matrixes r and II which are indexed by elements in P x R\ufffd and N x R\ufffd. We assume that their elements are all initialized to unset. The internal loop at lines #5, #6 and #7 is executed for each possible instantiation of the current clause, with the only constraint that its LHS arguments must always be bound to the parameter p0 . In function prdct at line #4, PA designates the set of A-clauses. It is not difficult to see that we have implemented a top-down recognizer 7 for any PRCG if the function prdct is called, for some input string w, with prdct(S, \u2022w\u2022 ). To turn this recognizer into a parser, we simply add the statement 7 Bottom-up algorithms can also be devised. (1) fu nction clause (i, p 0 ) return boolean (2) if r(i, Po ] # unset then return r(i, Po ] (3) let A o (o: o) --+ A1(0: i ) ... Ai (c(1) ... A m ( o: \ufffd ) = .P[i] (4) ret-val := r(i, p 0 ] := false ( 5) foreach A o (p o) --+ A1 (pi) ... Ai (p -; ) ... A m (p\ufffd) do (6) ret-val := ret-val V (prdct (A 1, pi ) /\\ .. . prdct (Ai ,P i) I\\ ... prdct (A m ,P \ufffd)) (7) end foreach (8) return r(i, p 0 ] := ret-val (9) end function (1) fu nction prdct (A, p) return boolean (2) if II[A, po ] # unset then return II[A, Po ] ( 3 ) ret-val : = false (4) foreach i such that .P[i] EP A do (5) ret-val := ret-val V clause (i, p) ( 6) end foreach (7) return II[A, Po ] := ret-val ( 8) end fu nction Ta ble 1: A Recognition Algorithm for PRCGs. after line #6 in clause, statement which must only be executed if the expression (prdct (A1 , pi ) /\\ ... /\\ prdct (A m , p\ufffd )) succeeds. In order to handle the full class of RCGs, we simply have to change line #6 in clause by something like (6) ret-val := ret-val V (prdct (A 1 ,p i) /\\ ... prdct(Ai ,P i) I\\ ... prdct (A m ,P \ufffd)) if .P[i] has the form A o (o: o) --+ A1 (o: i) .. . Ai (o: J ) ... A m (o:\ufffd). 8  We can also note that the assignment of r(i, p 0 ] to false at line #4 in clause allows this recognizer to handle cyclic grammars. 9 Its Parse Time Complexity For a given k-RCG, and an input string w E T*, lwl = n, the number of instantiations of a given clause .P[i], is less than or equal to n 2k ( l+ li) where l i is the number of predicate calls in the RHS of .P[i]. Thus, for a given clause, thanks to the memoization mechanism, the number of calls of the form prdct (Ai , p-; ) in line #6 of clause, is less than or equal to l i n 2 k ( I+ li ) ::; l i n 2k (Hl) wh ere l is the length of the longest clause. Thus, for all possible clauses this number is I:!\ufffd\ufffd lm 2k (Hl) = I Gln 2k ( Hl ) if IGI = I:!\ufffd\ufffd l i is the size of the grammar. Thus, if we assume that each use of the function prdct takes a constant time, the time complexity of this algorithm is at most O( IGln 2k ( Hl) ), 1 0 and its space complexity is O( IPln 2k ), the size of the \ufffdemoization matrixes r and II. We emphasize the fact that a linear dependency upon the grammar size is extremely important in NL processing where we handle huge grammars and small sentences. In the above evaluation, we have assumed that arguments in a clause are all independent; this is rarely the case. If we consider a predicate argument a = u 1 ... u p E (V U T)* and the string binding a/ p for some variable substitution u, each position 0, 1, . ... ,P in a is mapped onto a source index (a position in the source text) i 0 ,i 1 , ... ,i p s.t. i 0 ::; i 1 ::; \u2022 .\u2022 ::; i p and p = (i 0 .\u2022 i p )-These source indexes are not necessad.ly independent (free). In particular, if for example U j E T, we have i i = i j -1 + 1. Moreover, most of the time, in a clause, variables have multiple occurrences. This means that, in a clause instantiation, the lower source indexes and the upper source indexes associated with all the occurrences of the same variable are always the same, and thus are not free. In fac t, the degree d of the polynomial which expresses the maximum parse time complexity associated with a clause is equal to the number of free bounds in that clause. Fo r any RCG G, if d is its maximum number of free bounds, the parse time of an input string of length n takes at worst O( IGln d ). If we consider a bottom-up non-erasing 11 k-RCG G, by definition, there is no free bound in its RHS. Thus, in this case, the number of free bounds is less than or equal to d = max.P [ i] (ki + vi), where ki and Vi are respectively the arity and the num ber of (different) variables in the LHS predicate of .P[i]. In Example 2, we have defined a predicate named eq which may be useful in many grammars, thus, in our . prototype implementation, we decided to predefine it, together with some others, among which we quote here Zen and eqlen: len(l, X): checks that the size of the range denoted by the variable X is the integer l; eqlen(X, Y): checks that the sizes of X and Y are equal ; e q (X , Y) : checks that the substrings X and Y are equal . It must be noted that these predefined predicates do not increase the formal power of RCGs insofar as each of them can be defined by a pure RCG. Their introduction is justified by the fac t that they are more efficiently implemented than their RCG-defined counterpart and, more significantly, because they convey static information which can be used to decrease the number of free bounds and may thus lead to an improved parse time. Consider again Example 2. This grammar is bottom-up non-erasing, and the most complex clause is the first one. Its number of free bounds is four (one argument with three variables), and thus its parse time complexity is at worst O(n 4 ). In fac t, this complexity is at worst quadratic since neither the lower bound nor the upper bound of the argument XY Z is free since their associated source index values are always O and n respectively. Note that the lower bound of the definitions of the unary predicate L is not free either, si nce its value is always the source index zero . The parse time complexity of this grammar fu rther decreases to linear if eq is predefined, because in that case, we statically know that the sizes of X, Y and Z must be equal ( there is no more free bound within this first clause which is thus executed in constant tin:ie). The linear time comes from the upper bound of the LHS argument in the last three clauses. We can also check that the real parse time complexity of Example 3 is logarithmic in the length of the source text! Closure Properties & Modularity We shall show below that RCLs are closed under union, concatenation, Kleene iteration, intersection and complementation. Let G 1 = (N1,T1, Vi,Pi,S2) and G2 = (N2,T2, V2,P2,S2 ) be two RCGs defining the languages \u00a3 1 and \u00a3 2 respectively. Without loss of generality, we assume that N 1 n N 2 = 0 and that S is a unary predicate name not in N 1 U N2 . Consider two RCGs G' = ( N 1 U { S}, T 1 , Vi U { X}, Pi U P' , S) and G\" = (N 1 U N2 U {S}, T 1 U T2, V 1 U V2 U {X}, Pi U AU P\", S) defining the languages L' and L\" respectively. By careful definition of the additional sets of clauses P' and P\" , we can ge t L\" = \u00a31 UL 2 , L\" = L1L2 or L\" = L 1 n L2 and L' = Li or L' = L1 . Union: P\" = {S(X) \ufffd S 1 (X), S(X) \ufffd S2( X) } Concatenation: P\" = {S(XY) \ufffd S1(X) S2( Y) } Intersection: P\" = {S(X) \ufffd S 1 (X) S2( X) } Kleene iteration: P' = {S(.s) \ufffd .s, S(XY) \ufffd S 1 (X) S(Y)} Complementation: P' = { S(X) \ufffd S 1 (X)} In [ Boullier, 1999d] , we have shown that the emptiness problem for RCLs is undecidable and that RCLs are not closed under ho momorphism. In fact, we have shown that a polynomial parse time formalism that extends CFGs cannot be closed both under homomorphism and intersection and we advocate that, for a NL description f ormalism, it is worth being closed under intersection rather than under homomorphism. This is specially true when this closure property is reached without changing the component gra mmars. Let G 1 and G 2 be two grammars in some formalism F, their sets of rules are Pi and P 2 and they define the languages \u00a3 1 and \u00a3 2 respectively. We say that F is modular w.r .t. some closure operation f if the language L = J(L1, L2) can be defined by a grammar G in F whose set of rules P is such that Pi U P 2 C P. The idea behind this notion of sub-grammar is to preserve the structures (parse trees for G 1 and G 2 ) built by the component gra mmars. In that sense, we can say that CFGs are modular w.r.t. the union operation since CFGs have, on the one hand, the formal property to be closed under union and, on the other hand, this union is described without changing the component grammars G 1 and G 2 (we simply have to add the two rules S \ufffd S 1 and S \ufffd S 2 ). Conversely, CFGs are not modular w.r.t-. intersection or complementation since we know that CFLs are not closed under intersection or complementation. If we now consider regular languages, we know that they possess the formal property of being closed under intersection and complementation; however we cannot say that they are modular w.r.t. these properties, since the structure is not preserved in any sense. Fo r example, let us take a regular CFG G, defining the language L, we know that it is possible to construct a regular CFG whose language is L, but its parse trees are not related with the parse trees of G. Fo llowing our definition, we see that RCLs are modular w.r.t. union, concatenation, Kleene iteration, intersectio n and complementation. Of course it is of a considerable benefit for a formalism to be modular w.r.t. intersection and complementation. Modularity w.r .t. intersection allows one to directly define a language with the properties Pi /\\ P 2 , assumi ng that we have two grammars G1 and G2 describing Pi and P 2 , without changing neither G 1 nor G2 . Modularity w.r .t. complementation (or difference) allows for example to model the paradigm \"gen eral rule wi th exceptions\". Assume that we have a property P defined by a general rule R wi th some exceptions E to this general rule . Thus, formally we have P = R -E =Rn E. Within the RCG for malism, we simply have to add a clause of the form P(X) -\u25ba R(X) E(X) assuming that P, R and E are unary predicate names. If, moreover, these exceptions are described by some rules say D, we simply have to add the clause P(X) -\u25ba D (X) Conclusion In [ Boullier, 1999d] , we have shown that the 1-RCG su bclass of RCGs with a single argument, is already a powerful extension of CFGs which can be parsed in cu bic time and which contains both the intersection and the complement of CFLs . In [ B oullier, 1999b&c], we have shown that unrestricted TAGs and set-local multi-component TAGs can be translated into equivalent PRCGs. Moreover, these transformations do not induce any over-cost. Fo r example we have a linear parse time for regular CFGs, a cubic parse time for CFGs and a O(n 6 ) parse time for TAGs.  In this paper we present the full class of RCGs in which we can express several NL phenomena which are outside the formal power of MCS formalisms, while staying computationally tractable. The associated parsers work in time polynomial with the size of the input string and in time linear with the size of the grammar. Moreover, in a given grammar, only complicated (many arguments, many variables) clauses produce higher parse times whereas simpler clauses induce lower times. Fo r a given input string, the output of a RCG parser, that is an exponential or even unbounded set of derived trees, can be represented into a compact structure, the shared forest, which is a CFG of polynomial size and from which each individual derived tree can be extracted in time linear in its own size. As CFGs, RCGs may themselves be considered as a syntactic back bone upon which other formalisms such as Herbrand's domain or feature structures can be grafted. And lastly, we have seen that RCGs are modular This allows to imagine libraries of generic linguistic modules in which any language designer can pick up at will when he wants to specify such and such phenomena. All these properties seem to advocate that RCGs might well have the right level of formal power needed in NL processing. References Introduction Lexicalized grammars have been shown to be not only linguistically appealing but also desirable for parsing disambiguation . For example, among others, [Charniak, 1996] and [Collins, 1996] have found that lexicalizing a probabilistic model substantially increases parsing accuracy. As introduced in [Schabes et al ., 1988] , lexicalized tree adjoining grammar (LTAG) is a lexicalized grammar formalism in which lexical items are associated with sets of grammatical structures . [Resnik, 1992] shows that parsing disambiguation can be aided by statistical knowledge of cooccurrence relationships between LTA G structures . [Srinivas, 1997] and [Chen et al., 1999] show that considerable parsing disambigua tion is accomplished by assigning LTA G structures to words in the sentence using part of speech tagging techniques (supertagging). An LTA G grq,mmar G and a means of estimating parameters associated with G are prerequisites for probabilistic LTA G. [Schabes, 1992] shows how this may be done through grammar induction from an unbracketed corpus. The number of parameters that must be estimated, however, is prohibitively large for all but the most simple grammars. In contrast, [XTAG-Group, 1995] has developed XTAG, a complex, relatively broad coverage grammar for English . It is difficult, however, to estimate pa rameters with XTAG because it has been verified to accurately parse only relatively small corpora, such as the ATIS corpus . [Marcus et al., 1993) describes the Penn Treebank, a corpus of parsed sen tences that is large enough to estimate statistical parameters . From the treebank, [Srinivas, 1997] heuristically derives a corpus of sentences where each word is annotated with an XTAG tree, thus allowing statistical estimation of an LTA G. This method entails certain drawbacks: the heuristics make several mistakes, some unavoidable because of discrepancies between how XTAG and the Penn Treebank annotate the same grammatical constructions, or because XTAG does not cover all of the In this work, we explore extraction of an LTAG from the Penn Treebank. This allows us not only to obtain a wide coverage LTAG but also one for which statistical parameters can be reliably estimated. First, we develop various methods for extracting an LTAG from the treebank with the aim of being consistent with current principles for developing LTAG grammars such as XTAG. Second, we evaluate each grammar resulting from these methods in terms of its size, its coverage on unseen data, and its supertagging performance. Third, we introduce a preliminary method to extend an extracted grammar in order to improve coverage. Fourth, we situate our current work among other's approaches for tree extraction. Lastly, we present our conclusions and designs for future work. Tree Extraction Procedure In this section, we first describe the goals behind a tree extraction procedure and then describe the tree extraction procedure and its variations. An LTAG G is defined as a set of elementary trees T which are partitioned into a set I of initial trees and a set A of auxiliary trees. The frontier of each elementary tree is composed of a lexical anchor, the other nodes on the frontier are substitution nodes, and, in the case of an auxiliary tree, one node on the frontier will be a / oot node. The foot node of a tree {3 is labeled identically with the root node of /3. The sp ine of an auxiliary tree is the path from its root to its foot node. It is to be distinguished from the trunk of an elementary tree which is the path from its root node to the lexical anchor. Although the formalism of LTAG allows wide latitude in how trees in T may be defined, sev eral linguistic principles generally guide their formation. First, dependencies, including long distance dependencies, are typically localized in the same elementary tree by appropriate grouping of syntac tically or semantically related elements; \u2022 i. The genesis of a tree 'Y lexicalized by a word w E S, where S is a bracketed sentence in the Penn Treebank, using our tree extraction procedure proceeds as follows. First, a head percolation table is used to determine the trunk of 'Y \u2022 Introduced in [Magerman, 1995) , a head percolation table assigns to each node in S a headword using local structural information. The trunk of 'Y is defined to be that path through S whose nodes are labeled with the headword w, examples of which are shown in Figure 1 (b). Each node r/ that is immediately dominated by a node 1J on the trunk may either be itself on the trunk , a complement of the trunk 's headword-in which case it belongs to 1 , or an adjunct of the trunk 's headword-in which case it belongs to another (auxiliary) tree /3 which modifies ,. It is therefore necessary to determine a node's status as a complement or adjunct. [Collins , 1997 ] introduces a procedure which determines just this from the treebank according to the node 's label, its semantic tags , and local structural information. As described in [Marcus et al., 1994] , a node's se mantic tags provide useful information in determining the node's status , such as grammatical function and semantic role. Our procedure for identifying complements or adjuncts closely follows the method in [Collins, 1997] . The main differences lie in our attempt to treat those nodes as complements which are typically localized in LT AG trees. A critical departure from [Collins , 1997] is in the treatment of landing site of wh-movement. [Collins, 1997] 's procedure treats the NP landing site as the head and its sibling (typically labelled S) as a complement. In our procedure for extracting LT AG trees , we project from a lexical item up a path of heads. Then , by adopting [Collins, 1997] 's treatment , the landing site would be on the path of projection and from our extraction procedure , the wh-movement would not be localized. Hence , we treat the sibling (S node) of the landing site as the head child and the NP landing site as a complement. Figure 4 ( c) shows an example of a lexicalized tree we extract that localizes long-distance movement. We have conducted experiments on two procedures for determining a node's status as complement or adjunct. The first procedure that we consider , \"CAl,\" uses the label and semantic tags of node 1J and 11's parent in a two step procedure. In the first step , exactly this information is used as an index into a manually constructed table , which determines complement or adjunct status. \"IF current node is PP-DIR AND parent node is VP THEN assign adjunct to current node\" is an example of an entry in this table. The table is sparse ; should the index not be found in the table then the second step of the procedure is invoked: 1. Nonterminal PRN is an adjunct. 2 . Nonterminals with semantic tags NOM, DTV, LGS , PRD, PUT , SB J are complements. 3 . Nonterminals with semantic tags ADV, VOC, LOC, PRP are adjuncts. 4 . If none of the other conditions apply, the nonterminal is an adjunct. Whereas CAl uses the label and semantic tags of a node 1J and its parent 11', the procedure described in [Xia , 1999] , \"CA2 ,\" uses the label and semantic tags of a node 1J, its head sibling 1Jh , and distance between 1J and 1Jh in order to determine the complement or adjunct status of node 1J A recursive procedure is used to extract trees bottom up given a particular treebank bracketing. Figure 2 (a) shows one step in this proQ_ess. Among all of the children of node 11 2 , one child 171 is selected using the head percolation table so that the trunk </> associated with 11 1 is extended to Our tree extraction procedure also factors the recursion that is found in conjunction. Conjunction in the Treebank is represented as a flat structure such as Figure 3(a) . We define an instance of conjunction to be a sequence of siblings in the tree ((X)i (, )2 (X) 2 \u2022\u2022\u2022 (CC)k (X)k) where (, ) i , (X) i , and (CC)i are labels of the siblings, and there are k conjuncts. This follows from a basic linguistic notion which states that only like categories can be conjoined. When this configuration occurs in a Treebank bracketing, each pair ((, )i (X) i ) (or ((CC)k (X)k)) is factored into elementary trees as follows. The (, )ith (or (CC)kth) sibling anchors an auxiliary tree f3 representing the ith (kth) conjunct. The (X)ith (or (X)kth) sibling anchors an elementary tree that substitutes into {3. See Figure 3 This only considers conjunction of like categories. Although most conjunction is of this nature , it sometimes occurs that constituents with unlike categories are conjoined. In the Penn Treebank , these are annotated with the nonterminal label UCP. Although our current tree ext raction procedure does not treat these cases specially as conjunction , a similar strategy may be employed that does so , and in any case they are quite rare. The commas that were found in instances of conjunction were only one example of numerous cases of punctuation that are found in the treebank. In general , these are treated the same as adjuncts. On the other hand, it was found difficult to form a coherent strategy for dealing with quotes. Many times , an open quote would be found in one sentence and the closed quote would be found in an entirely different sentence. Therefore , we chose the simple strategy that quotes would anchor auxiliary trees that would adjoin to a neighboring sibling, namely, that sibling that was closer to the head sibling. The Penn Treebank has an extensive list of empty elements which are used to define phenomena\u2022 that are not usually expressed in LTAG. Among these are * U * , expressing a currency value , and * ICH * , indicating constituency relationships between discontinuous constituents. This observation led us to try two different strategies to cope with empty elements. The first strategy \"ALL\" is to include all empty elements in the grammar. The second strategy \"SOME\" is to only include empty elements demarcating empty subjects (0), empty PRO and passive NP trace ( * ), and traces ( * T * ) of syntactic movement ; these are usually found in LTA G grammars of English. The set of nonterminal and terminal labels in the Penn Treebank is quite extensive. A large set generally means that a greater number of trees are extracted from the Treebank ; these trees could miss some generalization and exacerbate the sparse data problem of any statistical model based on them. Also , some nonterminal labels are superfluous because they indicate structural configurations. Fo r example , NX is used to label nodes in the internal structure of multi-word NP conjuncts inside an encompassing NP. If NX were replaced by NP, the tree ext raction procedure can still determine . that an instance of conjunction exists and take appropriate action. On the other hand, distinctions that are made in a larger set of labels may aid the statistical model. Fo r these reasons , we evaluated two different strategies. One strategy, \"FULL,\" uses the original Penn Treebank label set. Another strategy, \"MERGED,\" uses a reduced set of labels. In the latter approach , the original set is mapped onto a label set similar to that used in the XTAG grammar ([XTAG-Group , 1995]). In our approach , headedness and status as complement or adjunct was first determined according to the full set of labels before the trees were relabeled to the reduced set of labels. Besides modifier auxiliary trees , there are predicative auxiliary trees which are generated as follows. During the bottom up extraction of trees , suppose trunk </> has a node 'T/ that shares the same label as another node 'f/ 1 , where 'f/ 1 is a complement , not on </>, but is immediately dominated by a node on </J. In this case, a predicative auxiliary tree is ext racted where 'T/ is its root , 'T/' is its foot and with </> serving as its trunk. Subsequently, the path </>' dominated by 'T/' becomes a candidate for being extended further. See Figure 4 (a). This mechanism wo rks in concert with other parts of our tree ext raction procedure (notably complement and adjunct identification , merging of nonterminal labels (from SB AR to S), and policy of handling empty elements) in order to p roduce trees that localize long distance movement as shown in Figure 4(c) . Evaluation Each variation of tree extraction procedure was used to extract a grammar from Sections 02-21 of the Penn 'Ireebank. These grammars were evaluated according to size, well formedness of trees, their coverage on Section 22, and their performance in supertagging Section 22. We subsequently evaluated truncated forms of these grammars which we term cutoff grammars. The grammars' sizes in terms of number of lexicalized trees and tree frames are shown in Table 1 . Removing the anchor from a lexicalized tree yields a tree frame. In terms of different tree extraction strategies, MERGED yields more compact grammars than FULL, SOME more than ALL, and CA2 more than CAL Perhaps the last dichotomy requires more of an explanation. Basically, CA2 factors more nodes into auxiliary trees, with the result being that there are fewer trees because each one is structurally simpler. We may also qualitatively judge grammars according to how well they satisfy our goal of extracting well formed trees in the sense of selecting the appropriate domain of locality and factoring recursion when necessary. There is not much difference between SOME and ALL because the empty elements that SOME ignores are the ones that are not usually captured by any LTAG grammar. Likewise, there is little difference between MERGED and FULL because most of MERGE's label simplification does not occur until after completion of tree extraction. The main difference lies between CAI and  CA2, strategies for labeling complements and adjuncts. Nodes detected as complements of a particular lexical item belong in the same ele_ mentary tree, thus satisfying the criterion of localizing dependencies. We believe that CAl labels nodes closer to what is traditionally found in LTAG grammars such as XTAG than does CA2, in that use of CA2 will generate less appropriate subcategorization frames because it tends to factor what might be considered as complements into separate auxiliary trees. It is difficult, however, to quantify the degree to which strategies CAl and CA2 are successful in distinguishing complements from adjuncts because there are no precise definitions of these terms. Here we resign ourselves to a qualitative comparison of an example of a lexicalized tree extracted from the same sentence by a CAl derived grammar G 1 (CAl SOME-MERGED) and a CA2 derived grammar G 2 (CA2-SOME-MERGED). First, a tree frame F is selected from G1 that is absent from G 2 \u2022 A bracketed sentence S out of which the CAl approach extracts Fis then culled from the training corpus at random . Figure 5 (a) shows S, (b) shows the tree corresponding to the main verb extracted to G 1 , (c) shows the tree corresponding to the main verb extracted to G 2 \u2022 It is typical of the examples of divergence between CAl and CA2 derived grammars: the CAl approach leads to a verb subcategorization that is more complicated, yet more appropriate. The various extracted grammars may also be evaluated according to breadth of coverage. In order to evaluate coverage of a particular grammar G, the strategy used to produce G was used to produce trees from held out data. We subsequently determined the degree of coverage of that strategy by the overlap in terms of tree frames and lexicalized trees as shown in Table 2 . For lexicalized trees t extracted from held-out data such that t ff: G, we also show the percentage of time the lexical anchors of such trees t were or were not found in the training corpus ( column in diet and not in diet respectively).  ' \\\ufffd :-.   We also measure the accuracies of supertagging models which are based on the various grammars that we are evaluating. Results are shown in Table 2 . Curiously, the grammars whose associated models achieved the highest accuracies did not also have the highest coverage. For example, CAl SOME-MERGED beat CA2-SOME-MERGED in terms of accuracy of supertagging model although the latter achieved higher coverage. This could possibly be caused by the fact that a high coverage grammar might have been obtained because it doesn't distinguish between contexts on which a sta tistical model can make distinctions. Alternatively, the cause may lie in the fact that a particular grammar makes better (linguistic) generalizations on which a statistical model can base more accurate predictions. CA1 -ALL-FULL -+ CA 1-ALL-MERGED -+-- CA 1-SOME-FULL \u2022B\u2022\u2022\u2022 CA 1-SOME-MERGED \u2022\u2022l><\u2022\u2022\u2022\u2022\u2022 CA2-ALL-FULL -b- CA2-ALL-MERGED -\u2022 - CA2-SOME-MERGED + -- A large grammar may lead to a statistical model that is prohibitively expensive to run in terms of space and time resources. Furthermore, it is difficult to obtain accurate estimates of statistical parameters of trees with low counts. And, in any case, trees that only infrequently appear in the training corpus are also unlikely to appear in the test corpus. For these reasons, we considered the effect on a grammar if we removed those tree frames that occurred less than k times, for some cutoff value k. We call these cutoff grammars. As shown in Figure 6 , even low values for k yield substantial Even though a cutoff grammar may be small in size, perhaps a statistical model based on such a grammar would degrade unacceptably in its accuracy. In order to see if this could indeed be the case, we trained and tested the supertagging model on various cutoff grammars. In the training data for these supertagging models, if a particular full grammar suggested a certain tree ti for word Wi, but the cutoff grammar did not include ti then word Wi was tagged as miss. The cutoff value of k = 3 was chosen in order to reduce the size of all of the original grammars at least in half. By the results shown in Table 2 , it can be seen that use of a cutoff grammar instead of a full extracted grammar makes essentially no difference in the accuracy of the resulting supertagging model. Extended Extracted Grammars The grammars that have been produced with the tree extraction procedure suffer from sparse data problems as shown in Table 2 by the less than perfect coverage that these grammars achieve on the test corpus. This is perhaps one reason for the relatively low accuracies that supertagging models based on these grammars achieve compared to, for example, [Srinivas, 1997 ) and [Chen et al., 1999) . Many approaches may be investigated in order to improve the coverage. Fo r example, although XTAG may be inadequate to entirely cover the Penn Treebank, it may be sufficient to ameliorate sparse data. Here we discuss how linguistic information as encoded in XTAG tree families may be used for this purpose and deliver some preliminary results. [XTAG-Group, 1995) explains that the tree frames anchored by verbs in the XTAG grammar are divided into tree families. Each tree family corresponds to a particular subcategorization frame. The trees in a given tree family correspond to various syntactic transformations as shown in Figure 7 . Hence, if a word Wi is seen in the training corpus with a particular tree frame ti, then it is likely for wo rd Wi to appear with other tree frames t ET where T is the tree family to which t i belongs. This observation forms the basis of our experiment. The extracted grammar G 0 derived from CAl SOME-MERGED was selected for this experiment. Call the extended grammar G 1 . Initially, all of the trees t E Go are inserted into G1. Subsequently, for each lexicalized tree t E G 0 , lexicalized trees t' are added to G1 such that t and t' share the same lexical anchor and the tree frames of t and t' belong to the same tree family. Out of approximately 60 XTAG tree families, those tree families that were considered in this experiment we re those corresponding to relatively common subcategorization frames including intransitive, NP, PP, S, NP-NP, NP-PP and NP -S. We achieved the following results. Recall that in Table 2 we divided the lapses in coverage of a particular extracted grammar into two categories : those cases where a word in the test corpus was not seen in the training corpus ( not in <J&ct), and those cases where the word in the test corpus was seen in training, but not with the appropriate tree frame ( in diet). Because our procedure deals only with reducing the latter kind of error, we report results from the latter 's fr&me of reference. Using grammar G 0 , the in diet lapse in coverage occurs 4.98% of the time whereas using grammar G1, such lapses occur 4.61% of the time, an improvement of about 7.4%. This improvement must be balanced against the increase in grammar size. Grammar Go has 4900 tree frames and 114850 lexicalized trees. In comparison, grammar G1 has 4999 tree frames and 372262 lexicalized trees. The results are somewhat encouraging and we believe that there are several avenues for improve ment. The number of lexicalized trees in the extended extracted grammar can be reduced if we account for morphological information. Fo r example, a verb \"drove\" cannot occur in passive forms. Instead of capitalizing on this distinction, our current procedure simply associates all of the tree frames in the transitive tree fam ily with \"drove.\" In related work, we are currently conducting an experiment to quantitatively extract a measure of similarity between pairs of supertags (tree frames) by taking into account the distribution of the supertags with words that anchor them. When a particular supertag word combination does not appear in the training corpus, instead of assigning it a zero probability, we assign a probability that is obtained by considering similar supertags and their probability of being assigned with this word. This method seems to give promising results, circumventing the need for manually designed heuristics such as those found in the supertagging work of [Srinivas, 1997] and [Chen et al., 1999] . We plan to apply this strategy to our extracted grammar and verify if similar improvements in supertagging accuracy can be obtained. [Neumann, 1998] presents a method for extracting an LTA G from a treebank. Like our work, [Neumann, 1998] determines the trunks of elementary trees by finding paths in the tree with the same headword, headwords being determined by a head percolation table. Unlike our work, [Neumann, 1998] factors neither adjuncts nor instances of conjunction into auxiliary trees. As a result, [Neumann, 1998] 's method generates many more trees than we do. Using only Sections 02 through 04 of the Penn 'Ireebank, [Neumann, 1998] produces about 12000 tree frames. Our approaches produces about 2000 to 9000 tree frames using Sections 02-21 of the Penn 'Ireebank . Related Work [Xia, 1999] also presents work in extracting an LTAG for a treebank, wo rk that was done in parallel with our own work. Like our work, [Xia, 1999] determines the trunk of elementary trees by finding paths in the tree with the same headword. Fu rthermore, [Xia, 1999] factors adjuncts (according to CA2 only) into separate auxiliary trees. Also, following our suggestion [Xia, 1999] factors instances of conjunction into separate auxiliary trees. [Xia, 1999] 's approach yields either 3000 or 6000 tree frames using Sections 02-21 of the Penn 'Ireebank, depending on the preterminal tagset used. Our work explores a wider variety of parameters in the extraction of trees, yielding grammars that have between about 2000 to 9000 tree frames on the same training data. Unlike our work in the extraction of an LTAG, [Xia, 1999] extracts a multi-component LTA G through coindexation of traces in the Penn 'Ireebank. Another difference is that [Xia, 1999] is more concerned with extraction of grammar for the purpose of grammar development (for which [Xia, 1999] for example makes the distinction between extracted tree frames that are \u2022 grammatically correct and those that are incorrect), whereas our current wo rk in extraction of grammar betrays our ultimate interest in developing statistical models for parsing (for which we perform an investigation of coverage, supertagging accuracy, effect of cutoff frequency, as well as explore the issue of extending extracted grammars using XTAG tree families for eventual use in statistical smoothing). This work raises the question as to how parsing with LTA G may compare to parsing where the model is based on a lexicalized context free formalism. Both recent work in parsing with lexicalized models and LTAG appear to manipulate basically the same kinds of information . Indeed, only a few trees in our extracted grammars from the Penn 'freebank have the form to cause the generative capacity of the grammars to exceed those of lexicalized context free grammars . The wo rk presented here makes it possible to see how a statistical parsing model based on an LTAG compares with models based on lexicalized context free grammars . Furthermore, supertagging as a preprocessing step may be used to improve the efficiency of a parsing using a statistical model based on an LTA G. We plan to explore these issues in future research. Conclusions Our wo rk presents some new directions in both the extraction of an LTAG from the Penn 'freebank as well as its application to statistical models . In the extraction of an LTAG from the Penn 'freebank, we have extended (Neumann, 1998 ]'s procedure to produce less unwieldy grammars by fac toring recursion that is found in adjuncts as well as in instances of conjunction . We have explored the effects that different definitions of complement and adjunct, whether or not to ignore empty elements, and extent of label sets have on the quality and size of the extracted grammar, as well as ability to cover an unseen test corpus . We have also evaluated those grammars according to supertagging accuracy. We have experimented with the notion of cutoff grammar, and seen that these grammars are more compact and yet yield little in the way of supertagging accuracy. We have introduced a preliminary technique for extending an extracted grammar using an external resource, namely, the tree families of XTAG. We have seen that this technique expands the coverage of an extracted grammar, and discussed how this technique may be developed in order to achieve better results. There are a number of ways to extend this work of extracting an LTA G from the Penn 'freebank. Because our goal is to develop a grammar around which to base statistical models of parsing, we are in particular interested in better procedures for extending the extracted grammars . Besides merely extending a grammar, it is also necessary to develop a method for estimating how often trees that are unseen in the training corpus, but are part of the e \ufffd tended grammar, are expected to occur in test data . After such issues are resolved, the extracted grammar could be used in a probabilistic model for LTAG, as de lineated by [ Schabes, 1992] and (Resnik, 1992 ] . This would provide not only another means of comparing different va rieties of extracted grammar, but would also allow comparison of LTAG parsing against the many other lexicalized parsing models mentioned in the introduction. [Chen et al., 1999 ] Chen, J., Bangalore, S., and Vijay-Shanker, K. (1999). New models for improving supertag disambiguation. In Proceedings of the 9th Conference of the European Chapter of the Association fo r Computational Linguistics, Bergen, Norway. [Collins, 1996] Introduction As the title indicates, this article describes two components rel ated to the parsing of un restricted En glish. Firstly, it discusses the auto matic generation of a large decl arative formal grammar fro m a collection of pre-anal ysed sentences of En glish. Secondly, it describes a parsin g methodology that employs bo th the automatically learned rul es and the database of cases to determine the syntactic structure of the input string. The discussions will be based on the Survey Parser that has been implemented by the author (Fan g 1996a), in the course of which some of the statistics will be presented to characterise the parsing appro ach to be reported here. Background In 1993-1996, the Survey of English Usage of Un iversity College London was en gaged in the machine-aided syntactic analysis of the mega-word British Co mponent of the In ternational Corpus of Engl ish (ICE-GB ; Greenbaum 1988 and 1996) . The co rpus comprises 600,000 wo rds of transcribed speech and 400,000 wo rds of writing. The anal ysis of the corpus included wo rdcl ass tagging and syntactic parsin g. Each wo rd in the corpus is assigned a contextuall y appropriate tag fro m a set of 270 grammaticall y possible tag-feature combinations. The parsin g scheme specifies the. an alysis for the category names (covering the clause and the canonical phrases) an d their syntactic fun ctions such as subject, verb, and direct object. Both the tagging and parsing schemes were based on wo rk by the TOSCA gro up of Nij megen Un iversity, the Netherl an ds (Oostdijk 1991) but substantiall y mo dified for the project. ICE-GB initially used a parser that required certain amo unt of manual pre-processing of the input text. Fo r instance, all cases of coordination had to be man ually marked and in dicated. The parser then pro duced all the po ssible anal yses for each sentence, one of which was to be selected and mo dified if necessary as the correct \u2022 representation of the constituent structure. Abo ut 70% of the corpus was parsed before it became clear in 1995 that the residue represented syntactic constructions beyond the capability of the parser. A parser that I had been developing since 1994 was used in stead. Together with a Windo ws-based graphic tree editor, the parser completed the analysis of the corpus in 1996. It is no w kno wn as the Survey Parser, whose further development was supported in 1995 for two years by the Engineering and Physical Sciences Research Council , UK. Design Requirements Because of the specific needs arising fro m the analyses of the ICE corpus, the design of the Survey Parser was co nditioned by the following requirements: \u2022 Sp eed -The proj ect required that input strings be batch parsed overnight so that researchers could supervise and mo dify the analyses the follo wing day. Since each researcher was assigned several texts a time and since there were abo ut six such people, the parser was typically required to batch parse about 20 texts at a time, which represented 40,000 wo rds. In addition, because of the correction of wro ng wo rdclass tags, researchers needed to frequently reparse individual sentences. \u2022 Robustness -Because of the unsupervised batch parsing overnight of unrestricted English, the parser was required to be capable of handling situations where the input string represents linguistic constructions beyond the descriptive power of the formal grammar. In practice, the parser should be ro bust enough to produce a partial analysis when a complete parse co ul d no t be achieved. \u2022 One analysis per input string -The experience was that it to ok shorter for the researcher to mo dify one incorrect analysis than to sel ect fro m a number of po ssible anal yses. This required the parser to produce one analysis that entails minimal manual intervention. To achieve this, the parser should either ensure that the first so lution is a good one or be able to co mpute the best analysis fro m the competing ones. \u2022 Ability to adapt to new grammatical constructions -The parser should be abl e to 'learn' and general ise abo ut new grammatical constructions no t described by the current grammar. In practice, this meant that the parser should be abl e to analyse a similar construction once the construction was manuall y anal ysed. Analogy-Based Parsing The parsing appro ach adopted in the Survey Parser seems to meet the abo ve requirements. Briefly speaking, the parsing methodology may be ro ughly described as analogy-based, variously discussed under case-, explanation-, and example-based learning (Mitchell et al 1986; Minton 1988 ; van Harmelen and Bundy 1988; Kno dner 1993). In the light of analo gy-based approach to pro blem solving, solutions to old problems can be used for new but similar pro blems. In terms of parsing, this appro ach is conceptually very simple: given a database of input strings that have already been syntactically analysed, the parsing of a new string is seen as identifying a same or similar case in the database. Once the similarity is established, the analysis stored in the database is then transferred onto the new input string. This appro ach to parsing has been explored by Samuelsson and Rayner (1991) , Neumann (1994), Samuel sson (1994), and Srinivas and Jo shi (1995). Such a parsing methodology requires a collection of syntactically anal ysed sentences as a case base and a mechanism to establish the similarity between the input string and one of the cases. Since a string may be represented as a sequence of wo rds, a sequence of wordclass tags, or a sequence of grammatical phrases, there are three obvious options to establish such similarity: Two sentences are judged as structurally identical or similar if there is an exact match in terms of lexical items, wo rdclass tags, or phrases. Intuitivel y, these three matching criteria have different levels of generalisability or coverage. The use of wo rdclass tags as a measurement of similarity, for instance, should have a higher chance of finding a match than the use of lexical items because of the data sparsity problem that is typically rel ated to wo rd sequences, a problem that has been extensivel y addressed within the speech recognition community. The use of phrase sequence will, in turn, \u2022represent a more generalised model to measure sentence similarities. A related problem, however, is that a more \u2022 general model tends to produce less reliable similarity indications. A match at the phrase level is less a guarantee than a match at the wordclass level that the two sentences are structurally similar or identical. The simple phrase sequence NP VP NP, for example, has at least three different syntactic structures according to the ICE-GB scheme. Thus a practical issue in analogy-based parsing is to increase the coverage of the case base while maintaining an acceptable degree of confidence that the retrieved syntactic structure is a good one.  The stochastically selected tags serve as indexes that allow for the retrieval of a sub-tree for any phrase identified by the phrasal rules in a left-to-right longest match manner. At the stage of clausal analysis, phrase types are used as indexes to identify a similar clause and retrieve the tree structure as the proposal analysis for the input string. When the process fails to find an identical clause from the database, the sequence of phrases with their internal structures analysed is treated as an intermediate or partial analysis. The architecture described above is illustrated by Figure 1 , where double arrows indicate system queries to components of the knowledge base. An Overview of the Survey Parser Generally put, the Survey Parser establishes analogy or similarity between the input string and a case in the knowledge base through a match at the phrase level. To ensure an acceptable degree of confidence, the phrase sequence is constrained by features inherited from the lexical properties of the head. The parser has two major components. The first is the syntactic knowledge base constructed on the basis of the syntactically analysed ICE-GB , from which we may automatically extract bi gram wordclass transitional probability, phrasal rules anchored to wordclass tags or The following discussions will be divided into three parts. I shall first of all describe the construction of the database, which is a process of automatic extraction of syntactic rules. I shall then describe the various analyses performed on the input string, including wordclass tagging and phrasal and clausal analyses. Finally, statistics will be presented to characterise the performance. Phrase Structure Rules PS rules determine the an alysis of wo rdclass sequences into phrases in cluding noun phrases (NP), verb phrases (VP), adjective phrases (AJP), adverb phrase (A VP), an d prepo sitional phrase (PP). The automatic generation of such rules is achieved by collecting all the tags as termin al symbols attributed to a particular phrase. Since the syntactic an alyses of ICE-GB explicitly specify the boundaries of constituent structures as well as their syntactic fun ctions, the extraction is a fairly straightforward matter. As a general rule, complementation and post-modification are not in cluded. Thus, for in stance, PS rules describing NPs all termin ate at the head of the phrase; similarly tho se describing VPs all terminate at the main verb. Differently, however, PS rules for PPs cover the complete span of the phrase. Here is an example illustratin g the extracting process. [1] And it's a very nice group to be working with because it's not too large The syntactic tree for [1] is graphically represented in Figure 2 , where each constituent has two elements of description, the first being the name of syntactic fun ction an d the second that of category type. Thus, su NP ( ) is in terpreted as \"noun phrase fun ctioning as clausal subject\". As ano ther example, AVHD CONNEC (ge) {And) is read as \"lexical item And is a general connective and fun ctions as the head of the adverb phrase\". -AVHD ADV (ge) {not} 0---CS AJP (prd) El---AJPR AVP ( inten) ! Fro m the analysis of [1] , PS rules for the five major phrases can be extracted an d stored in the syntactic kno wledge base. Each rule comprises a sequence of wo rdclass tags and is associated to a constituent structure that specifies the an alysis of the sequence. !.._ ___ AVHD ADV ( inten) { too} L_ __ AJHD ADJ (ge) { large} A Associated constituent structure B------AJP (prd) $--AJ \ufffd R AVP (inten) i L-AVHD ADV ( inten) { -} L---AJHD ADJ (ge) {-} B\u2022\u2022----\ufffdVP () Phrase Structure Cluster Rules The second component of the syntactic knowledge base deals with phrase structure clusters and superimposes the hierarchical structure of this cluster. In most of the cases, these clusters correspond to the conventional clause, but occasionally they represent co-ordinated or juxtaposed phrases. Again, such rules may be automatically extracted from a set of pre-analysed sentences. For example, the syntactic analysis of [ 1] yields one PSC rule as represented in Table 2 : Type VP :cop $-\u2022 --SU NP () AVP AJP [\u00b1)----VB VP (act, indic, cop, pres) III--A AVP (ge) tiJ----CS AJP (prd) Table 2 : A phrase cluster automatically extracted from [1] As mentioned at the beginning, analogy-based parsing has two practical issues: confidence and coverage. Confidence is the system assurance that the retrieved tree structure for the input string is a good one while coverage is the adaptation of the indexed cases so that they are useful to as many structural variations as possible. In the Survey Parser, confidence is maintained through the use of feature constraints and the increase of coverage is achieved through the identification and removal of non-obligatory syntactic elements. Feature Constraints To ensure the correct association between the PS cluster and the corresponding tree, some of the phrase types are normally described or restricted with features. This typically applies to VPs, whose sub-categorisation determines the analysis of their complements. For example, the phrase cluster NP VP PP may have at least two different analyses for the complementing PP: as subject complement if the VP is copula and as adverbial if the VP is intransitive. In the case of a non-finite VP, the very same phrase cluster needs to be analysed as a noun phrase post modified by a non-finite clause, e.g., countries pressurised by the decision and countries voting against the decision. Feature constraints inherited from the main verb help to dissolve such ambiguities. In Table 2 , the first and second VPs are described by cop, a feature name meaning copula. This feature ensures that the complementing NPs are correctly analysed as subject complement (cs). The other two constraint features for VPs are intr (intransitive) and tr (transitive). Non-finite VPs are described with additional features to indicate their forms, e.g., infin for infinitive, edp for past participle, and ingp for present participle. Non-obligatory Elements Non-obligatory elements include A VPs and PPs that do not complement any particular VPs. They are called non obligatory in the sense that their removal does not affect the overall syntactic structure of the sentence. In order to maximise the coverage of phrase cluster rules, such elements are removed. The example in Table 2 , for instance, is in fact written as NP VP : cop NP TO VP : intr : infin pp :ps CONJUNC : subord NP VP : cop AJP . Adverbial clauses may also be treated as non-obligatory and, indeed, they are probably the most . active constructions that contribute to the complexity of the clause. There is very good reason to expect a greatly increased coverage if such clauses could be treated separately and removed from the host clause. For the time being, however, this has not been implemented in the Survey Parser. Parsing with PS and PSC Rules The Survey Parser has three maj or modules that handle ( 1) assigning a wordclass tag to each item in the input string, (2) chunking the tags into a PSC, and (3) querying in the knowledge base for possible analyses for this PSC. The Analysis of Wordclasses The Survey Parser currently uses AUTASYS (Fang and Nelson 1994; Fang 1996b ) as a pre-processor that tokenises the input string into lexical items and then assigns one wordclass tag to each of the tokens. This tagger has a probabilistic backbone supported by a list of rules in order to achieve the informationally rich tagset designed for the ICE-GB project. The tagset features 22 general wordclasses with around 70 descriptive features, totalling about 270 grammatically possible tags (Fang 1994; Greenbaum and Ni 1994) . The descriptive features represent a detailed system of lexical sub categorisation critical for parsing with wide-coverage lexicalised grammars (Briscoe and Carroll 1997) . Consider [2] Th e search menu in the Circulation module may make additional search methods available to library staff. AUT ASYS assigns one ICE tag to each of the lexical items in [2] and the result is illustrated in Table 3 . It is worth noting that the tagging process provides the maximum grammatical information at this stage. For example, all compound nouns have already been marked up with 'ditto tags' that carry sequential numbers to indicate the boundary of the compound. The grammatical features related to the compound are selected according to the head. As a result, search in the co mpound noun search methods is tagged as a plural common no un, the first in the two-item sequence. Lexical verbs are also analysed for detai led su b-categori sations and, as Fi gu re 3 shows, there are 7 different types of verbs in the ICE tagging scheme. The verb make in [2] , for instance, is tagged as infini ti ve ditransiti ve thou gh it should be correctly analysed as complex transiti ve complemented by both a NP and an AJP. The Analysis of Phrases transi ti vity information for the verb is mainly Table 3 : Grammatical tagging of [2] for passivised VPs, which undergo the following valency shifts: mo no-transiti ve\u2794intransitive, complex-transitive\u2794copula, ditrans itive\u2794mono-transitive. The transi ti vi ty of a passivised complex transiti ve VP, for example, is shifted to that of copula in order to cater for the analysis of the complementi ser as subject complement, e.g., The home front was kept ignorant of the reality. Detailed verb transiti vity sub-categori sation is of great use when the system fai ls to find a global analysis for the input string and has to label syntactic functions fro m limited context. The input string, as a sequence of wo rdclass tags, is then pro cessed at the phrase level according to the PS ru les, whi ch are applied deterministically on a left-to-right longest match heuristic. When applied, these PS ru les chunk the input stri ng into a cluster of PSs, wi th feature information about the head. They also assign phrase types, boundaries, and internal structures to tag sequences that have a direct match in the PS ru le base. Analysed at thi s level, the input string is represented as a cluster of syntactic phrases, each of which is no w associated to a sub-tree. As demonstrated by Figure 4 , the su b-trees already present a neat representation of the constituent structure of the phrases. Note that the VP is described by a feature (tr, meani ng transiti ve) inherited fro m the corresponding wo rdclass tag except that there is no further distinction of su b-categorisation for transiti ve verbs (Figure 3 ). What still remains uncertain at thi s stage is the syntactic functio n to be determined by the PSC ru les. -----P PREP (ge) {to} 8----PC NP () L-NPHD N(com, s ing) { library staff} Fi gure 4: Input string as a PS cluster wi th associated sub-trees The Analysis of Clauses As a final step, the input string as a cluster of phrase structures is then queried in the database of PSC rules to see if an identical sequence can be located. With a positive feedback from the database, the associated tree structure for that sequence in the database is retrieved and used to specify the labelling and the attachment of the phrases of the input string. In our example, the parser determined that the PSC of the input string was the same as that of [3] , already analysed and stored in the database: [3] The cellular anatomy of the peripheral nervous system renders it vulnerable to injury. Accordingly, the parser retrieved the analysis for [3] and superimposed it on [2] . The final analysis is shown in Figure 5. In this particular example, the The final analysis of [2] parser successfully retrieved the correct tree structure for the input string. The incorrect sub-categorisation of the verb make as ditransitive at the stage of tagging did not prevent the parser from correctly labelling the sentence-final AJP as object complement (CO), suggesting the possibility of a post-parsing correction of wordclass tags. Partial Analysis If a global analysis cannot be achieved, the parser will enter a fa llback mode, where all the non-obligatory PPs are removed from the phrase cluster and a second attempt is made to find a match. When successful, the parser will paste the removed PPs back into the host clause. Failure in the fallback mode will then put the parser in the partial mode, where all the associated sub trees in the phrase cluster are written out as a partial analysis. The boundaries and the internal structures of the component phrases have already been analysed and labelled. The parser also naively assigns missing names guess-estimated from neighbouring phrases. 2 Evaluation In this section, I report empirical tests carried out to evaluate the performance of the Survey Parser. In particular, these tests were designed to indicate the coverage of the extracted grammar in terms of PS and PSC rules, labelling precision, the accuracy of analysis according to human judgements, and finally the processing speed. Evaluating the Coverage of Phrase Structure Rules The syntactic knowledge base currently makes use of about 50,000 syntactically analysed utterances from ICE-GB.  Corpus size (number of samples) 5 , such probability is 60% for test sets fro m WSJ and SEU. The ATIS set had a higher percentage mainly because of the relatively shorter sentence length i_ n this corpus than the other two . Input strings that cannot be described by the PSC rules were given partial analyses. Evaluating the Labelling Precision A set of 60 AMALGAM sentences 3 fro m a computer manual were used to measure the precision of labelling by the Survey parser. A scoring pro gram for evaluating the performance of speech recognition systems was used that measures not only the correct and wro ng labels but also insertion and deletion rates. As Table 6 indicates, 90. 1 % of the constituents for the 60 sentences were correctly labelled by the Survey Parser. With wro ng labels and deletion and insertion rates considered, the overall precision rate was 86.9%. Finally, the performance of the parser was subjected to the strictest human evaluatio n where an input string was judged to be wro ng with a single parsing error in terms of labelling, attachment, or to kenisation. Fo r this purpo se, Evaluating the Accuracy of Analysis Sent Defin ition No. 117 Full analysis 84 71.8% Correct analysis 77 65.8% the test used a set of 117 dictionary definitions extracted fro m the Lo ngman Dictionary of Contemporary English (see Briscoe and Carroll, 1991) . Table 7 summarises the results. Of these input strings, 84 were fully parsed, a coverage of 71.8%. Of the fully parsed strings, 77 were correctly labelled and attached, a precision rate of 65.8% of Table 7 : Accuracy of analysis the to tal number of input strings. It is significant that the majority of the fully analysed strings (91 .7%) were correct even according to the strictest requirements, indicating a high level of system confidence that the proposed analysis is a good one. Evaluating the Processing Speed Two sub-language corpora were used to measure the processing speech of the Survey Parser. One is a corpus of the English for science and technolo gy collected at the Shanghai Jiao To ng University (JDEST; Huang 1991) and the other a corpus of the English of computer science co llected at the Ho ng Ko ng University of Science and Technology (HKUST; Fang 1992). The statistics summarised in Table 8 were obtained with Dell OptiPlex Gxa. The tagging mo dule, according to the test, ran at a speed of 6,012 wo rds per second. The parsing mo dule was able to pro cess 177 wo rds per second. Corpus Source No. of words No. of sentences Concluding Remarks In this article, I have described the architecture of the Survey Parser as well as the construction of the syntactic kno wledge base that co mprises PS and PSC rules automatically extracted fro m a syntactically analysed corpus, ICE-GB. I then reported evaluation statistics that characterise the performance of analo gy-based parsing. They indicate that while the clause structure rules have a coverage of 60 %, the grammar has a high coverage in terms of phrase structures as the statistics for the verb phrase indicated. One co nclusion we can draw here is that it is indeed feasible to automatically generalise a co mprehensive formal grammar fro m a syntactically analysed corpus the size of ICE-GB and to apply it to large-scale practical parsing. A seco nd conclusion is that analogy-based parsing enjoys a high degree of analysis precision, with over 90 % of the co nstituents correctly labelled. When subjected to human inspection, 91.7 % of the complete parses were found to be correct. Other observed advantages include high parsing speed and the ability to produce an analysis for every input string. A true strength of analogy-based parsing is its intrinsic ability to learn over the acquisition of new phrase structure clusters. Since the formal grammar can be automatically learned, the parser can easily adapt itself to new constructions. Unlike probabilistic grammars, the automatically constructed grammar can be visually supervised and manipulated because of its declarative nature. The empirical tests suggested that the analogy based parsing reported here is capable of the design requirements outlined at the beginning of the article. As can be envisaged at this stage, the future wo rk on the analo gy-based parser will focus on metho ds to increase the coverage of the clause cluster rules. I have already mentioned the remo val of no n-obligatory elements such as A VP and PP in order to increase the co verage. Ano ther effective method, yet to be tested, is the segmentation of the input string into component finite clauses, which are then parsed individually and glued back into the ho st clause. The syntactic functions of the clauses can be reliably assessed independent of the ho st clause because of the conjunction markers. The key process to achieve this is an algorithm that automatically, and reliably, identifies the bo undaries of component clauses. Introduction Parsing procedures always have to be designed around a number of pre-specified requirements which arise from specific conditions of the individual application area in mind. Text retrieval tasks , for instance, can be accomplished already with a rather shallow analysis whereas speed and fail-soft behaviour are of utmost importance. Grammar checking and foreign language learning applications, on th e other hand, must provide for highly prec ise error detection ca pabilities but differ considerably in their coverage requirements. One of the most demanding combination of target spec ifications results from the development of sp oken language dialogue systems. Since this task is an attempt to model central capabilities of the human language fac ulty, rather strong criteria have to be met in order to achieve natural communicative behavior on a competitive level : \u2022 Robustness: A spoken language dialogue system is typically confronted with a rich va riety of linguistic constructs and will almost inevitably have to deal with extragrammatical input sooner or later. Also, repairs, hesitations, and other grammatical deviations will frequently produce ungrammatical utterances, while the recogn ition uncertainty inherent in spoken language input further increases the ambiguity. The parsing component must be able to cope with these problems in a robust way. Be sides being able to return (possibly partial) analyses even for unexpected and arbitrarily distorted input it is al so necessary to provide some kind of measure of how sure the parser is about its results. \u2022 C omplete d isambiguation: Na tural language utterances typically exh ibit ambiguity when treated in isolation. Nevertheless , a simple en umeration of different (structural) readings almost never can be considered a sensible contribution to a practical language processing task. Although interactive applications can en ga ge the speaker in a kind of clarification dialogue, usually this possibility brings many additional complications and should only be considered a measure of last resort. Instead, a well-designed system should make use of all the available information to obtain a single interpretation of the utterance , which is only abandoned if the user explicitly signals a communication failure. \u2022 Multiple-source d isambiguation: A va st va riety of knowledge sources can contribute to disambiguation : Syntactic constraints, semantic preferences, prosodic cues, domain knowledge , the dialogue history, etc. All the available knowledge should be put to use as soon as possible so that local ambiguity will not create a large space of useless hy potheses during processing. For rea sons of perspicuity and accessibility the integration of these knowledge sources should be organized in a way which maintains their modularity. Only then can the respective contributions of individual components be evaluated and properly balanced against each other. \u2022 Time-aware behavior: Three closely related aspects must be considered with respect to the temporal behavior of a language processing component: Efficiency, incremental processing and temporal adaptivity. Whereas efficiency always has been an issue of major \u2022 concern , explorations into incremental and time-adaptive parsing attracted more attention only recently [GKWS96, Amt99, Men94]. Since speech unfolds in time, speaking time is a va luable resource and an immediate response capability can be achieved only if incoming information is processed in an on-line fashion. Temporal adaptivity, on the other hand, is the capability of a component to dynamically control its processing regime depending on how much time is available to complete the task. In principle, such an anytime behaviour can be achieved by trading time against quality. Therefore a baseline performance will be required which allows the quality of available results to grow monotonically as more effort is made, and which is robust enough so that results of slightly reduced quality can still be considered being acceptable in a certain sense. Obviously, the most natural measure of external temporal pressure is given by the speaking rate of the dialogue partner. Thus temporal adaptivity makes sense first of all under an incremental processing scheme. Its basic mechanisms, however, can also be studied in the fa r simpler non-incremental case.  For this purpose, constraints are annotated with a weight or score between zero and one that determines how easily that constraint may be violated. Hard constraints have a weight of zero and must not be violated by any solution. Constraints wh ich reflect regularities of the grammar receive a small we ight greater than zero wh ile constraints that model mere preferences will be we ighted close to one . A solution candidate to the CSP can then be assigned a score by determining the product of the we ights of .all the constraints wh ich are violated somewhere in the structural description. Under these premises, parsing becomes a multidimensional optimization problem. This paper investigates a non-standard parsing approach, which attempts to reconcile two different kinds of robustness, namely robustness against unexpected and ill-formed input and robustness against external temporal pressure. It is based on the application of con straint satisfaction techniques to the problem of structural disambiguation and allows the parser to include a wide va riety of possibly contradicting informational contributions. Different solution procedures are presented and compared taking into account solution quality an d the observed temporal behavior. For reasons of efficiency at most binary constraints can be allowed, hence the universal ex pressive powe r of constraints is not available in practice . This serious shortcoming , however, can be neutralized to a certain degree by approximating higher order constraints using binary ones. Another disadvantage is the lack of a variable binding mechanism like the one which is provided by a unification operator together with the missing notion of a constituent (which, however, is share d with most dependency grammar approaches). Experience with grammar writing has confirmed that nevertheless nontrivial subsets of grammar can be encoded suc cessfully, although some phenomena such as long-distance dependencies can only be modeled approximatively [SMFS]. Constraint dependency grammar is a purely declarative formalism . This property makes it amenable to a variety of problem solving strategies that can be compare d, e. g. with respect to their temporal behaviour. The possibility to add further representational levels supports the integration of knowledge contributions from ve ry different sources into a single solution space without sacrificing the strict modularity of the grammar and of the structural representation. Of course, this possibility is limited to only those knowledge sources that can meaningfully attach information to single word forms. A single interpretation of the incoming utterance can be obtained by using all available evidence, including minor preference indicators like ordering , distance or default cases . A truly ambiguous sentence will usually allow several analyses with only small differences between their scores , which can be ig nored if desired. The approach exhibits a remarkable robustness ag ainst unexpected and ill-formed np ut [MS98b] , which obviously can be attributed to three important ch aracteristics: \u2022 the use of weighted constraints , which provides for the accommodation of conflicting evidence and therefore makes the analysis of deviating structures possible , \u2022 the redundancy between loosely coupled representational levels, wh ich allows conflicting information on one level to be overridden by sufficient evidence from a complementary one , and \u2022 the possibility to license arbitrary categories as an acceptable, but in most cases highly disfavored, top node of a dependency tree , thus introducing a partial parsing scheme as a natural extension of the normal mode of operation. Note that the resulting robust behavior follows immediately from the fundamental principles of the approach and no error rules or special operations become necessary. Two other ch aracteristics of the approach contribute to the rich potential for obtaining the desired anytime behaviour. In contrast to other parsing approaches the space of all possible analyses for constraint dependency parsing is always finite and very reg ularly structured. Pars ing therefore becomes a process of selection between different analyses with virtually identical formal properties , which considerably facilitates their mutual comparison. Solution Procedures Consistency-based Methods The canonical method for solving a constraint satisfaction problem is to establish a certain degree of consistency in it by deleting incompatible values from its domains, and then select an assignment to all constraint variables from the remaining values. This approach contrasts strongly with common parsing methods that are constructive in nature. Usually, grammatical structures are built up recursively from simpler structures, and ultimately from the information associated with the lexical items present in an utterance. Thus, the number of structures available increases over time. In contrast, the achievement of consistency is an eliminative process: The more progress is made, the fewer values remain in the problem. An attractive property of this kind of parsing is that it can be exactly determined, at any time, how much progress has already been made and how much work remains to be done until disambiguation is completed. This information will be of great use to a time-aware solution procedure. Various well-defined degrees of consistency can be achieved in a CSP, and general algorithms exist to establish any desired degree of consistency A suitable algorithm for consistency in a partial CSP should remove all those values that do not appear in the optimal solution-a property that is much more difficult to determine. The usual consistency algorithm will find a value that cannot appear in a solution by noting that it cannot appear in a valid n-ary assignment. A similar approach for partial CSP would be to select those values for deletion that only occur in n-ary assignments with low scores. The obvious method is to define a fixed limit and consider all scores below it unacceptable; this has much the same effect as employing the unmodified algorithms on a grammar in which more constraints are hard. Another method known as pruning [MS98b] goes one step further. While a consistency algorithm cannot guarantee how much progress it will achieve, a pruning method will invoke a selection function at regular intervals to select exactly one value for deletion. If this function uses a fixed amount of time, an exact appraisal can be given not only of the amount of work to be done, but also of the actual runtime left until termination. To guarantee that a value is selected for deletion within the allotted time, the selection function will usually have to be heuristic in nature. A simple selection function mimics the behaviour of a 2-consistency algorithm: Tables of mutual support are constructed for all pairs of domains in the problem. The support of a value v from another domain d can be defined as the maximal or the average compatibility of v with any value from d, or in a more elaborate way. The value whose maximal support by any other domain is smallest is selected for deletion. Since the globally optimal solution may consist of values that are locally suboptimal, in general this method of assessing values exclusively by local information may remove the wrong values from a problem. While the CDG formalism ensures that the remaining values form a complete assignment, in general it cannot be guaranteed that this assignment will be the optimal solution, or even a grammatically val id one. Thus, a heuristic consistency al gorithm may fail without result even though there is a val id solution, which defies the purpose of robust processing. Enumeration Most parsers use some kind of search al gorithm to enumerate all al ternatives for local or gl obal ambiguities arising in the analysis of their input. A great number of search variants has been invented for different parsing applications (top-down vs. bottom-up parsing, depth-first vs. breadth-first search , linear vs. island parsing) , and choosing the right method can have dram atic impact on th e efficiency of a parser. In general , considering every possible al ternative ensures that an al gorithm is complete as well as correct, but may require so many resources that it becomes impractical to apply. Since in a problem in CDG, consistency-based methods cannot gu arantee either complete or correct behaviour, a complete method of solution is desirable even if it has other disadvantages. For instance, a complete but inefficient al gorithm will still be of great use to the developer of a constraint grammar to check the val idity of their model, or to verify the results of an incomplete method. For the CSP, a complete search of all possible assignments can be conducted that is gu ar anteed to find the optimal solution. In the partial CSP, a normal best first search can be employed which finds the optimal solution without ever having to expand a partial solution with a lower score. The current implementation of the CDG parser provides a straightforward best-first search in which the variables of a problem are instantiated in a fixed order. This will usually be the order of the words corresponding to the constraint variables. Compared with other parsers, this would be classified as a heuristically driven left-to-right search . It resembles bottom-up parsing in that each word can immediately be integrated into a tree that forms part of the complete dependency structure. A top-down parsing method could be simulated by arranging the search so that every additional dependency edge must modify a word that has al ready been analyzed , starting with those words that can modify the root of the dependency tree. Since the CSP is NP-complete, probably any complete solution method will have an expo nential worst-case complexity. Although the ac tual runtime of a complete search al gorithm is usually far below the worst possible case, and heuristic re-ordering of both domains and values can greatly improve the efficiency, it is difficult to predict even approximately how long a par ticular instance of the problem will take to search . Therefore, a complete search is inadequate as a solution method when time-aware behaviour is required . However, in contrast to other methods a complete search can easily enumerate gl obally near-optimal structures such as those defined by syntactically ambiguous sentences. Transformation An obvious way to overcome the unacceptable temporal behaviour of complete al gorithms is to employ suboptimal methods. A strategy that works well in practice is that of heuristic repair. Rather than attempting to build the correct structure by selecting correct values step by step , this method first constructs an arbitrary dependency structure with errors in it and then tries to correct the errors. Suppose that the dependency structure shown in Figure 2 This method has several advantages as opposed to the previous ones: \u2022 Because a complete dependency analysis is maintained at all times, the algorithm may be interrupted at any time and still return a meaningful answer, though not always the optimal one. Thus, it automatically fulfills a strong anytime criterion. \u2022 The constraints that cause conflicts in a suboptimal assignment can suggest which value is inappropriate and what other value should be substituted. \u2022 Because all analyses of a given utterance comprise the same number of values, it is guaran teed that the optimal solution (if any exists) can be constructed from any other assignment by successively replacing one value at a time. A transformation step is usually defined as the exchange of one value of a constraint variable for an other. By this definition , the correct solution of a CSP of degree n can always be reached in not more than n transformation steps, if the correct replacement value is ch osen at any point. To accomplish this, however, every value in the problem has to be con sidered as an alternative in each step, whereas a backtracking search only has to try out ail values from one particular domain. Obviously the transformation alg orithm will encounter a much greater branching factor. However, the search space is now graph-shaped, an d so not every alternative must be pursued further because the order in wh ich several values are inserted into an an alysis does not matter. Instead, the number of alternatives that are tried out at all can be used as a parameter to speed up the computation. Obviously, the efficiency of such a repair algorithm depends on its ability to select the correct values for repair. Even a totally uninformed repair method can ultimately find the correct solution, since it is simply a random walk through the problem space. For better results, heuristic decision methods must be found to guide the selection. The simple hill climbing method will always ch oose the value that results in the best immediate improvement. The principal difficulty with th is method is that it can get stuck in local optima of the problem space where no immediate improvement is possible. Different methods exist to allow an algorithm to leave such misleading areas of the search space. \u2022 Occasional downhill steps may be allowed so that an algorithm may escape from a local maximum. For example, in the popular method of simulated annealing downhill steps are allowed but gradually discouraged as an alysis progresses. \u2022 In an other approach , hill climbing does not optimize the score of an an alysis itself, but an an cillary cost function that is adapted in each step, so that the local optimum can be turned into an ascent. \u2022 The definition of a transformation step can be ch anged so that several values may be replaced simultaneously. Assignments which differ in several variables will then become adjacent to the current assignment. Again , the number of values that may be ch anged in one step can be used as parameter to influence the speed of th e algorithm. A subsequent difficulty is that after such a repair alg orithm has converged to the optimal solution , it may not terminate. If the optimal solution still causes some minor conflicts, the alg orithm will continually try to repair these conflicts without success, since there is no simple way to distinguish a local optimum from the global one. In this case the individual constraints of the grammar can be used as a taboo criterion : If no repair step is allowed to re-introduce the conflict that prompted it in the first place, termination can be guaranteed. Although repair-based solution methods cannot guarantee to find the optimal solution in all cases, in practice they achieve results comparable to those of exh austive methods. This demonstrates that the values contained in a complete parse are helpful in selecting correct values even though some of them may themselves be incorrect [Minton92]. In this way, parsing by transformation can make use of global information without suffering the full combinatorial explosion of a complete search. 100% / , _____ ,..J r\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 : so% _j ______ _.r:\u2022 \u2022\u2022r./ \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 so% / . r=-\u2022\u2022\u2022\u2022: With the observed increase of the solution quality the parser has a performance profile typical for anytime procedures [BD89]. In 90% of all problems, the solutions found were either identical to those found by a complete search or had even better scores (since the search uses a finite agenda, it may actually become incomplete in large problems). In the final analyses, 99. 7% of all dependency links were established correctly. In general, a near-optimal structure will be constructed after a short time. Finding the exact optimal analysis may take considerably more time in medium-sized and large optimization problems. However, particularly in these cases the algorithm will be consistently faster than a complete, search. Figure 5 gives the average time that the transformation-based solution method takes to terminate, measured in terms of the runtime of a best-first search of the same problems. Clearly, the repair method is faster in most large problems except for the highest problem class (which only has two members in our corpus, however). Comparison of the time until the last successful repair with the total runtime shows that the algorithm will usually terminate not long after having reached the optimal analysis. : I j j \u2022--\u2022-\u2022-\u2022 -\u2022-\u2022---\u2022-\u2022-\u2022 i For applications that have to react to varying external time constraints, a parameter is provided that adjusts the number of alternatives to be tried out at each transformation step. In the cases investigated so far, decreasing this parameter can speed up the repair substantially, with an acceleration by a factor of about 3 reducing the accuracy to 86% of correct dependency links. Related work There is a striking analogy between constraint dependency parsing and customary approaches to the task of tagging natural language data: In both cases each word form in the utterance is annotated with a label from a finite set of alternatives and the approaches differ only in the information content of the labels. Although tagging usually. means to classify word forms into (syntactic and semantic) types, the idea can also be extended to the use of functional tags in a straightforward way. Karlsson et al. [KVHA95] use such functional descriptions in Constraint Grammar parsing. Their tagset contains elements like \"subject\" or \"a determiner modifying a noun to the right\". Usually these tags are underspecified, because the exact identity of the modified item is unknown. Constraint Dependency Grammar as discussed in this paper extends this approach to fully specified structural representations. Since the identity of the modified word is now included into the composite tags, the size of the tagset additionally depends on the number of word forms in the given utterance. Moreover, the extension to multi-level disambiguation allows to treat considerably richer representations as compared to what a usual tagger is taking into account. Another approach using complex tags is Supertagging [BJ99], where complex tree fragments are attached to word forms by means of a stochastic model. These tags represent considerably more structure than values in CDG, which correspond to single dependency edges. However, tags are treated in isolation and the compatibility between adjacent structures is modeled only probabilistically. In order to combine tags to complete parse trees an additional processing step has to be carried out after the tagging itself. The idea to obtain a structural description of natural language utterances by applying a sequence of transformations which successively modifies an intermediate representation has first been pursued within the framework of parsing as tree-to-tree transduction [BGQA82], although no explicit notion of scoring and quality improvement was involved at that time. The dynamics of the transformation process was fully under the control of the grammar writer, taking into account the precedence ordering implicit in a sequence of rules and some additional means to influence the degree of non-determinism. Transformation-based approaches have later been applied to the problem of syntactic tagging ( B ri95]. However, focus wa s on inducing an appropriate set of transformation rules from the information contained in an annotated corpus. Possibilities to model grammar by means of contradicting principles are inve stigated cur rently in the framework of optimality theory [PS91]. The grammatical principles postulated there are ranked rather than weighted , with higher ranked regularities completely overriding the influence of the lowe r ones. First applications have been identified in phonology and syntax. Conclusion A novel approach to parsing as constraint-based structural disambiguation has been presented. By combining techniques for robust parsing (graded constraints, multi-level-disambiguation and partial parsing) with the idea of a transformation-based problem solving mechanism, a parser can be created that shows the ty pical temporal behavior of an interruptible anytime algorithm. Further inve stigations will focus on 1. transferring these procedural characteristics to the case of incremental parsing, thus ad dressing particularly the problem of processing time for long utterances, The re sulting parsing method mimics human language processing in that it is time-adaptive and robust and therefore lends itself to the implementation of human-machine dialogue systems. References (Amt99] ( B J99] ( B D89J Jan W. Amtrup Introduction Parsing can be defined as the assignment of structure to an utterance according to a grammar, i.e., the mapping of a sequence of words (utterance) into a parse tree (structured representation). Because of the ambiguity of natural language , the same utterance can sometimes be mapped into more than one parse tree ; statistical parsing attempts to resolve ambiguities by preferring most likely parses. Also , spontaneous speech is intrinsically different from written text (see for example (Lavie 1996] ), \u2022 therefore when attempting to analyze spoken language, one must take a different parsing approach . Fo r instance one must allow for an utterance to be parsed as a sequence of parse trees ( which cover non-overlapping segments of the input utterance) , rather than expect a single tree to cover the entire utterance and fail otherwise. The Soup parser herein described, inspired by Ward's PHOENIX parser (Ward 1990 ] , incorporates a variety of techniques in order to achieve both flexibility and robustness in the analysis of spoken speech: flexibility is given by the lightweight formalism it supports , which allows for rapid grammar development, dynamic modification of the grammar at run-time , and fas t parsing speed ; robustness is achieved by its ability to find multiple-tree interpretations and to skip words at any point, thereby recovering in a graceful manner not only from false starts, hesitations, and other speech disfluencies but also from insertions unforeseen by the grammar. Soup is currently the . main parsing engine of the JANUS speech-to-speech translation system [Levin et al. 2000 , Woszczyna et al. 1998 ]. Section 2 briefly describes the grammar representation , section 3 sketches the parsing process , section 4 presents some performance results, section 5 emphasizes the key features of SOUP, and section 6 concludes this paper. Grammar Representation The grammar formalism supported by Soup is purely context-free. Each nonterminal (whose value is simply a label), has a set of alternative rewrite rules, which consist of possibly optional, pos sibly repeatable terminals and nonterminals . We have found over the years [Mayfield et al. 1995 , Woszczyna et al. 1998 ] that, at least for task-oriented semantic grammars used in speech transla tion systems, the advantages in parsing speed and ease of grammar construction of such a formalism outweight the lack of the more expressive power offered by richer formalisms (cf., for example, the [Verbmobil Semantic Specification 1994]) . . SOUP represents a context-free grammar (CFG) as a set of probabilistic recursive transition networks (P RTNs), where the nodes are marked as initial, regular or final, and the directed arcs are annotated with (i) an arc type (namely, specific-terminal (which matches and consumes a particular word), any terminal (which matches and consumes any out-of-vocabulary word or any word present in a given list), nonterminal (which recursively matches a subnet and, in the parsing process, spawns a subsearch episode) or lambda (the empty transition, which can always occur)), (ii) an ID to specify which terminal or nonterminal the arc has to match (if arc type is specific-terminal or nonterminal), and (iii) a probability (so that all outgoing arcs from the same node sum to unity). Fo r example, the right-hand side *good +bye ( where * indicates optionality and + repeatability and therefore matches good bye, bye, bye bye ( and also good bye bye, etc)) is represented as the PRTN in Figure 1 . SOUP also directly accepts grammars written in the Java Speech Grammar Fo rmat (see section 5.5 ). Grammar arc probabilities are initialized to the uniform distribution but can be perturbed by a training corpus of desired (but achievable) parses. Given the direct correspondence between parse trees and grammar arc paths, training the PRTNs is very fa st (see section 4). There are two main usages of this stochastic framework of probabilities at the grammar arc level: one is to incorporate the probabilities into the function that scores partial parse lattices, so that more likely ones are preferred; the other is to generate synthetic data, from which, for instance, a language model can be computed. The PRTNs are constructed dynamically as the grammar file is read; this allows for eventual on-line modifications of the grammar (see section 5.4 ). Also, strict grammar source file consistency is enforced, e.g., all referenced nonterminals must be defined, warnings for nonterminal redefinitions are issued, and a variety of grammar statistics are provided. Multiple grammar files representing different semantic domains as well as a library of shared rules are supported, as described in [Woszczyna et al. 1998 ] . The lexicon is also generated as the grammar file is being read, for it is simply a hash table of grammar terminals. s [time] +[point] [point] [time] ( [hour] ) ( [minute] ) \ufffd [hour] [point ] [point ] (n ine ) A A ( ten ) [hour] [minute] [hour] [minute] [minute] V V nine ten (n ine ) ( ten ) (a) (b) Sketch of the Parsing Algorithm Parsing is a particular case of search. In SOUP, parsing proceeds in the following steps: 1. Construction of the input vector : Given an utterance to be parsed, it is converted into a vector of terminal IDs. Special terminals <s> and <Is> are added at the beginning and end of an utterance, respectively, so that certain rules only match at those positions. Also, user-defined global search and-replace string pairs are applied, e.g., to expand contractions (as in I'd like \u2794 I would like) or to remove punctuation marks . Other settings allow to determine whether out-of-vocabulary words should be removed, or whether the input utterances are case-sensitive. Population of the chart: The first search populates the chart (a two-dimensional table indexed by input-word position and nonterminal ID) with parse lattices. (A parse lattice is a compact representation of a set of parse trees (similar to To mita 's shared-packed forest [Tomita 1987 ]); see Figure 2 for an example). This beam search involves top-down, recursive matching of PRTNs against the input vector. All top-level nonterminals starting at all input vector positions are attempted. The advantage of the chart is that it stores, in an efficient way, all subparse lattices found so far, so that subsequent subsearch episodes can reuse existing subparse lattices. To increase the efficiency of the top-down search, the set of allowed terminals with which a nonter minal can start is precomputed (i.e., the FIRST set), so that many attempts to match a particular nonterminal at a particular input vector position can be preempted by the lack of the correspond ing terminal in the FIRST set. This bottom-up filtering technique typically results in a threefold speedup. The beam serves to restrict the number of possible subparse lattices under a certain nonterminal and starting at a certain input position , e.g., by only ke eping those subparse lattices whose score is Performance Soup has been coded in C++ and Java and compiled for a variety of platforms including Windows (95, 98, NT) and Unix (HP-UX, OSF /1, Solaris, Linux). The upper portion of Table 1 lists some parameters that characterize the complexity of two grammars, one for a scheduling domain and the other for a scheduling plus travel domain; the lower portion lists performance results of parsing a subset of transcriptions from the English Spontaneous Speech Scheduling corpus (briefly described in [Waibel et al. 1996]). Parsing time increases substantially from a 600-nonterminal, 2,880-rule grammar to a 6,963nonterminal, 25,746-rule grammar but it is still well under real-time. Also, as depicted in Figure 4 , although worst-case complexity for chart parsing is cubic on the number of words, SouP's parse time appears to increase only linearly. Such behavior, similar to the findings reported in [Slocum 1981 ] , is due, in part, to SouP's ability to segment the input utterance in parsable chunks . (i.e., finding multiple-tree interpretations) during the search process. Therefore, even though comparisons of parsers using different grammar formalisms are not well defined, Soup appears to be faster than other \"fast parsers\" described in the literature (cf., for example, [Rayner and Carter 1996] or [Kiefer and Krieger 1998]). Key Features The following are some of the most interesting features of SouP. Skipping Given the nature of spoken speech it is not realistic to assume that the given grammar is complete (in the sense of covering all possible surface forms). In fact it turns out that a substantial portion of parse errors comes from unexpected insertions, e.g., adverbs that can appear almost anywhere. Soup is able to skip words both between top-level nonterminals (inter-concept skipping) and inside any nonterminal (intra-concept skipping). Inter-concept skipping is achieved by the second search step Character-level Parsing To facilitate the development of grammars for languages with a rich morphology, SOUP allows for nonterminals that operate at the character-level (see Figure 6 for an example) . Character-level parsing is achieved using the same functions that parse at the word-level. In fact it is during word-level parsing that character-level parses are spawned by exploding the current word into characters and recursively calling the parse functions. The only difference is that, in a character-level parse, the desired root nonterminal is already known and no skipping or multiple-tree interpretations are allowed. Multiple-tree Interpretations Dynamic Modifications Encoding the grammar as a set of PRTNs gives SOUP the flexibility to activate/deactivate nonterminals and right-hand sides at run-time. Fo r example, grammar nonterminals can be marked as belonging only to a specific speaker side (say, agent vs . client) ; then , at run-time and for each utterance, nonterminals not belonging to the current speaker are deactivated. Also, in the case of a multi domain grammar, one could have a topic-detector that deactivates all non-terminals not belonging to the current topic, or at least lowers their probability. More generally, nonterminals and right-hand sides can be created, modified and destroyed at run time, which allows for the kind of interactive grammar learning reported in [Gavalda and Waibel 1998 ]. Parsing JSGF Grammars Soup has been extended to natively support grammars written according to the specifications of the Java Speech Grammar Fo rmat (JSGF 1998 ]. The JSGF is part of the Java Speech Application Programming Interface [JSAPI 1998 ] and is likely to become a standard formalism for specifying semantic grammars, at least in the industrial environment.  Soup is able to represent a RuleGrammar as defined by JSGF with the same underlying PRTNs. This is accomplished by the usage of lambda arcs to encode the JSGF Rule source, so that, out of a parse tree, the corresponding RuleParse ( the result of a parse as specified by the JSAPI) , can be constructed. In more detail, for each JSGF RuleSequence, RuleAlternatives, RuleCount and RuleTag, a corresponding lambda-SEQ, lambda-ALT, lambda-CNT or lambda-TAG arc is built in the PRTNs, as well as a closing lambda arc (pictured upside-down) to indicate the end of scope of the current JSGF Rule. Figure 7 shows a toy grammar in the JSGF formalism. Figure 8 depicts the corresponding PRTNs as well as a schematic sample parse tree. Figure 9 shows the resulting RuleParse object constructed from such parse. Conclusion We have presented SouP, a parser designed to analyze spoken language under real-world conditions, in which analysis grammars are very large, input utterances contain disfluencies and never entirely match the expectations of the grammar, and yet backend processing must begin with minimal delay. Given its robustness to ill-formed utterances, general efficiency and support for emerging industry standards such as the JSAPI, Soup has the potential to become widely used. Introduction It seems to be a general feature of natural language that the el ements of a sentence are pronounced in one position, while at the same time serving a function in another part of the structure of the sentence. Linguistic theories in the transformational tradition have tried to capture this fact by proposing analyses that involve movement of constituents . Stabler ([8] ) presents a formalism for defining minimalist grammars that allow for movement of constituents. This formalism is based on Chomsky's Minimalist Program ( [2] ). Michaelis ([5] ) provides an argument showing that minimalist grammars as defined in ( [8] ) are weak ly eq uivalent to multiple context -free grammars as described in Seki et al. ([6] ) . 1 Multiple context-free grammars are non-concatenative in the sense that a non-terminal symbol in this grammar can domi nate a sequence of strings of terminal symbols, rather than just one string, as in the case of ordinary context-free grammars. Each of the strings dominated by a non-terminal symbol in a multiple context free grammar will be a substring of a sentence whose der ivation includes this non-terminal, but in the sentence these strings are not necessarily adjacent. The main insight contained in [5] is that minimal ist grammars are non-concatenative in a similar way. In minimalist grammars, non-concatenativity arises as the result of movement. Thus, in a minimalist grammar a constituent can dominate non adjacent substrings of a sentence. Seki et al. ([6] ) also describe an algorithm for recognizing multiple context-free grammars. Stabler ([10] ) sketches how this algorithm may be ext en ded to minimalist grammars. This paper contains a formal specification of a recognizer for minimalist grammars. Furthermore, it is shown that the recognizer is sound and complete, and that its time complexity is polynomial in the length of the input string. Besides this intro duction , this paper has six sections . Section 2 introduces the minimalist grammars as defined in (8] . Section 3 contains the specification of the recognizer. Sections 4 and 5 present the proofs of soundness and completeness. Section 6 describes some complexity results. The paper concludes with some directions for future work. Minimalist Grammars Minimalist grammars manipulate minimalist trees. Minimalist trees are finite, binary ordered trees whose leaves are labeled with sequences of syntactic and non-syntactic features. New trees are built by either merging two trees into one, or by moving a subtree in a tree. The application of merge and move operations is driven by the syntactic features labeling the leaves of the trees. The language defined by a minimalist grammar consists of the yields of a particular subset of the trees generated by the grammar. Formally, following (8] , a minimalist grammar G is defined to be quadruple (V, Cat, Lex, F), where V is a finite set of non-syntactic features, Cat is a finite set of syntactic features, Lex is a finite set of lexical expressions built from V and Cat, and F is a finite set of structure building functions. The set of non-syntactic features V is made up of a set of phonetic features P and a set of semantic features I: V =P U I. The set Cat comprises four kinds of syntactic features: Cat =base U select U licensors U licensees. The elements of base represent basic syntactic categories. The set base minimally contains the distinguished category feature c. For each category feature x E base, there will be a selection feature =x E select, although select does not necessarily contain a feature =c. The features in base and select play a role in merge operations. The features in licensors and licensees regulate movement operations. For each feature -y E licensees, there will be a feature +y E licensors. A minimalist tree T is given by a non-empty set of nodes Nr , a function Labelr, and three relations on Nr : dominance, precedence and projection. Immediate dominance and immediate precedence and their reflexive and transitive closures are defined as for trees of the usual kind. For any two sister nodes x, yin a minimalist tree r, either x projects over y, or y projects over x, notated x < y and y < x, respectively, or x > y and y > x. The function Labelr assigns to each leaf of T a string from (V U Cat)*, in particular, a string from select* licensors* select* base licensees*P *I*. The other nodes of T are not labeled. The elements of Lex are are assumed to be trees consisting of one node. This paragraph will introduce some arboreal notions that will be used in the remainder of the paper. For nodes x, yi n a tree r, x is .the head of y if and only if y is a leaf and x=y, or there is a node z in r such that y immediately dominates z, z projects over its sister and x is the head of z. The head of a tree T is the head of the root of r. A node yi n a tree Ti sa maximal projection of a head x in r if and only if x is the head of y and y's sister projects over y. A tree T is maximal if and only if its root is the maximal projection of some head. A tree T is complex if and only if it has more than one node, otherwise, T is simple. A tree T is said to have feature f E V U Cat if the first element of the sequence that labels the head of T is f. For T and v minimalist trees, [< r, v] denotes a tree with immediate subtrees T and v where the root of T projects over and precedes the root of v, and [> r, v] denotes a tree with immediate subtrees T and v where the root of T precedes the root of v and the root of v projects over the root of r. A tree T is complete if its head does not contain any syntactic features except for the distinguished category feature c, and no node in r other than the head has any syntactic features. The yield Y(r) of a tree T is the concatenation of the phonetic features in the labels of the leaves of T, ordered by precedence. There are two structure building functions: F={merge, move}. A pair of trees r, vis in the domain of merge if T has feature =x and v has feature x for some x E base. Then, merge(T, v)=[<T', v'] if T is simple, and merge(T, v)=bv', T 1 ] if T is complex, where T 1 is like T except that =x is deleted, and v' is like v except that x is deleted. A tree T is in the domain of move if T has feature +y E licensors and T has exactly one maximal subtree To that has featurey E licensees. 2 Then, move(T)=[>T\ufffd, T 1 ] , where T\ufffd is like To except thaty is deleted, and T 1 is like T except that +y is deleted and subtree To is replaced by a single node without features . 3 Let G=(V, Cat, Lex, F) be a minimalist grammar. Then CL(G)= UkEN CL k (G) is the closure of the lexicon under the structure building fun ctions, where CL k (G), k E N, are inductively defined by: 1. CL 0 (G)=Lex 2. CL k+ l (G)=CL k (G) U {merge(T, v)l(T, v) E Dom( merge) n CL k (G)xCL k (G)} U {move(T) IT E Dom( move) n CL k (G)}, where Dom( merge) and Dom( move) denote the domains of the functions merge and move. The language derivable by G consists of the yields of the complete trees in the closure of the lexicon under the structure building functions: L( G) ={Y(T) jT E CL(G) and T is complete}. \ufffd +qd3 < \ufffd -pl -q2 > \ufffd < 1 < /'--.... c4 > /'---... 2 < 3 > \ufffd 2 < \ufffd d3 < ,....... pl < ,r /'---... 1,p q 2 Since +y-=/:--y, r and ro have different head labels. Hence, ro is properly included in r. >,z < , y \ufffd c4,s > ,x \ufffd 2,t <,w /\"--.... 3,u V Tree r 5 is a complete tree. 4 Tree r 6 is the same as tree r 5 . The nodes in r 6 are named for illustrational purposes only; these names have no significance in the grammar. In tree r 6 , node u is the head of nodes u, w and x. Node s is the head of nodes s, y and z. Node t is the head of node t and no other node. Nodes z, r, q, x, t and v are maximal projections; nodes s, p, q, u, t, v are their respective heads. Node s has feature c. No other node has a syntactic feature. r w = [ < r u , r v ], and r x = l > rt, r w ], where Tn denotes the subtree of r 6 whose root is named n. \u25a1 3 Specification of the Recognizer This section contains a formal definition of an agenda-driven, chart-based recognizer for minimalist languages. Taking a logical perspective on parsing as presented in Shieber et al. ([7] ), the definition of the recognizer includes a specification of a grammatical deductive system and a specification of a deduction procedure. The formulae of the deductive system, which are commonly called items, express claims about grammatical properties of strings. Under a given interpretation, these claims are either true or false. For a given grammar and input string, there is a set of items that, without proof, are taken to represent true grammatical claims. These are the axioms of the deductive system. Goal items represent the claim that the input string is in the language defined by the grammar. Since our objective is to recognize a string, the truth of the goal items is of particular interest. The deductive system is completed with a set of inference rules, for deriving new items from old ones. The other component of the definition of the recognizer is the specification of a deduction procedure. This is a procedure for finding all items that are true for a given grammar and input string. Deduction Procedure The deduction procedure used in the recognizer presented in this paper is taken from Shieber et al. ([7] ). It uses a chart holding unique items in order to avoid applying a rule of inference to items to which the rule of inference has already applied before. Furthermore, there is an agenda for temporarily keeping items whose consequences under the inference rules have not been generated yet. The procedure is defined as follows: 1. Initialize the chart to the empty set of items and the agenda to the axioms of the deduction system. 2. Repeat the following steps until the agenda is exhausted: a) Select an item from the agenda, called the trigger item, and remove it. b) Add the trigger item to the chart, if the item is not already in the chart. c) If the trigger item was added to the chart, generate all items that can be derived from the trigger item and any items in the chart by one application of a rule of inference, 5 and add these generated items to the agenda. 3. If a goal item is in the chart, the goal is proved, i.e., the string is recognized, otherwise it is not. Shieber et al. ([7] ) prove that the deductive procedure is sound -it generates only items that are derivable from the axioms -and complete -it generates all the items that are derivable from the axioms. Deductive System Given an input string w=w 1 ... Wn and minimalist grammar G=(V, Cat, Lex, F), the items of the deductive system will be of the form [o: 0 , o:1, ... , o: m ]t, where m \ufffd llicenseesl, t E {s, c}. For O \ufffd i \ufffd m, ai is of the form (xi, Yi):1'i, where O \ufffd Xi \ufffd Yi \ufffd n, n=lwl, and ,'i E Cat *. The proper interpretation of the items requires the notion of narrow yield of a tree. The narrow yield Y n ( cp) of a minimalist tree cp is defined in the following way. If cp is a complex tree, then either cp= [ > r, v], or cp=[ < r, v] . If cp=[ > r, v], then: Yn(c/J)=Yn(r) \u2022Yn(v) if r does not have a feature -f E licensees 6 Y n(c/J)=Y n(v) otherwise. If cp= [ < r, v] , then: Yn(c/J)=Yn(r) \u2022Yn(v) if v does not have a feature -f E licensees Y n(c/J)=Y n(r) otherwise. If cp is not a complex tree, it must be a simple tree . In that case: Yn(c/J)=Y(cp) Informally, the narrow yield of a tree is that part of its yield that will not move out of the tree by some application of the function move. 1. If t=s, r is a simple tree; if t=c, r is a complex tree. 2 . The head of r is labeled by ')'o 7rt, 1r1, E P*I*. 3 . For every (xi, Yi):')'i, 1 \ufffd i \ufffd m, there is a leaf in r labeled 1'i7rL, 1r1, E P*I*. 4 . Besides the nodes labeled by 1'i7rL, 1r1, E P*I*, 0 \ufffd i \ufffd m, there are no other nodes with syntactic features in r. 5 . The narrow yield of the subtree whose root is the maximal projection of the node labeled by 1'i1r1, , 7rl E P*I*, is W xi + l ... Wy i , 0 \ufffd i \ufffd m. Axioms and Goals The set of axioms of the deductive system is specified in the following way. For each lexical item in Lex with syntactic features ')' E Cat* and whose phonetic features cover Wi+i ... W j of the input string, there will be an axiom [(i, j):1'] s in the deductive system . There will be two goal items: [( O, n):c] s and [(O, n):c] c , c E base being the distinguished category fe ature. These are appropriate goal items for a recognizer, since their truth under the interpretation provided above requires the existence of a complete tree r E CL(G), either simple or complex, with narrow yield Y n(r) = w 1 ... Wn = w. Since T is complete, Y n(r)=Y(r). Therefore, w E L(G) = {Y(r) lr E CL(G) and r is complete}. ) proved that the deductive procedure is sound and complete, establishing the correctness of the recognizer entails showing that the deductive system defined above is sound and complete relative to the intended interpretation of the items. This will be done in the next two sections. The deductive system has six rules of inference, grouped into Merge rules and Proof of Soundness The following two lemma's will be helpful for establishing soundness of the deductive system. Lemma la: if T, T 1 and v, v' are trees such that merge(T , v) = [ < T', v'] or merge(T, v) = bv', T'], then Y n (T') = Y n (T) and Y n (v') = Y n (v). Proof: inspection of the definition of merge shows that in both cases T 1 and T are identical except for the labels of their heads. According to the definition of narrow yield the label of the head of a tree is of no relevance for determining the narrow yield of that tree. Analogously, Y n (v') = Y n (v). \u25a1 \u25a1 Proving soundness of the recognizer amounts to showing that the axioms and the rules of inference of the deductive system are sound. Then it will follow that every derivable item in the deductive system will represent a true grammatical statement under the intended interpretation. Soundness of the Axioms According to the interpretation given in section 3.2, an axiom [(i, j):,] s asserts the existence of a tree T E CL(G) with the following properties: 1. Tree T is a simple tree. 2. The head of T is labeled by ,1r1,, for some 1r1, E P*I* . 3 . Besides the head of T there are no other nodes with syntactic features in T. 4 . The narrow yield of T is Wi+1 ... W j . It is easy to see that the lexical item in Lex which occasioned the axiom [(i, j):,] s has exactly these properties. By definition this lexical item is in CL 0 (G) \ufffd CL(G). Soundness of the Rules of Inference There are six rules of inference. Their soundness will be established below. 7  Merge-I: the items [(p, q) : = x,] s and [( q, v):x, a1 , . Proof of Completeness This section presents a completeness proof for the recognizer. A recognizer is complete if for every string that is in the language defined by the grammar, there is a derivation of a goal item from the axioms. First, the following two useful lemma's will be proved. Lemma 2: if the derivation of tree <pin a minimalist grammar G immediately includes tree T, then for every maximal subtree a contained in T, there is a maximal subtree a' in <p such that Y n (a) is a substring of Y n(a'). Proof: three distinct cases have to be considered: there is an v E CL(G) such that q>=merge(T, v), there is an v E CL(G) such that q>= merge(v, T), and q>=move(T). In the first case, either c/>= [ < T', v'] or c/>=bv', T'], depending on whether Tisa simple or a complex tree. Furthermore, either u is a proper subtree of T, or u=T. If u is a proper subtree of T, then T 1 will properly contain u, as T 1 is like T except that =x is deleted from the label of the head. Fo r the same reason, u is maximal in T 1 \u2022 Since q> contains T 1 , u will be a subtree of q> and, trivially, Y n (u) is a substring of Y n (u). If u=T, then Y n (u)=Y n (T)=Y n (T') by lemma la. Also, Y n (T') is a substring of Y n ( </>) by the definition of narrow yield. Trivially, q> is a maximal subtree of q>. In the second case, either c/>=[ < v', T'] or c/>=[ > T', v'], depending on whether vi s a simple or a complex tree. Again, either u is a proper subtree of T, or u=T. If u is a proper subtree of T, then u will be a proper, maximal subtree of q>, as argued for the similar situation in the case above. If u=T, then Y n (u)=Y n (T)=Y n (T') by lemma la. Tree T 1 is a maximal subtree contained in q>, and, trivially, Y n (u) is a substring of Yn(T'). In the third case, q>= [> T\ufffd , T'], with To specified as in the definition of move. Either u is a proper subtree of To, or u=To, or u is a proper subtree of T and To is a proper subtree of u, or u=T. If u is a proper subtree of To, then u will be a proper, maximal subtree of q>, as in the similar situations above. If u=To, then Y n (u)=Y n (To)=Y n (T\ufffd) by lemma lb. Now, T\ufffd is a maximal subtree contained in q>, and, trivially, Y n ( u) is a substring of Y n ( T\ufffd ). If u is a proper subtree of T_ and To is a proper subtree of u, then, by the definition of move, T 1 will contain a tree u' which is like u, except that subtree To is replaced by a single node without features. Y n (u)= Y n (u'), since the yield of To is excluded from Y n ( u) because of itsf feature, and the yield of To is excluded from Y n ( u') because To is not a subtree of u'. Hence, trivially, Y n (u) is a substring of Y n (u'). Moreover, u' is a maximal subtree of T 1 since u is a proper, maximal subtree of T. Finally, if u=T, then Y n ( u) = Y n ( T) = Y n ( T 1 ) by lemma lb. Furthermore, Y n (T') is a substring of Y n (c/>) by the definition of narrow yield. Trivially, q> is a maximal subtree contained in q>. \u25a1 Lemma 3: if for a minimalist grammar G, the derivation of a complete tree , includes T, then Y n ( T) is a substring of Y (,). Proof: by repeated application of lemma 2, , will contain a maximal subtree u' such that Y n (T) is a substring of Y n (u'). Since , is a complete tree, it does not have any labels with a feature -f E licensees. Hence, by the definition of narrow yield, Y n (u') is a substring of of Y n(,)=Y(,), and, consequently, Y n (T) is a substring of Y( ,). \u25a1 Completeness of the parser will follow as a corollary of lemma 4 below. If w E L(G), for some minimalist grammar G, then there is a complete tree q> E CL(G) such that Y( c/>) =w. Since q> E CL( G), there must be a k E N such that q> E CL k ( G). Since q> is a complete tree, lemma 4 guarantees that an item corresponding to q> will be generated. Obviously, the item will be a goal item, since q> is complete and Y( c/>) =w. Lemma 4: for a given minimalist grammar G, if c/> E CL k ( G), k E N, and q> is included in the derivation of a complete tree or q> is a complete tree itself, then an item (o 0 , o 1 ... , om]t corresponding to q> will be generated, (o 0 , o 1 ... , om]t as defined in section 3. 2 . Proof: 9 it will be shown by induction over k that for arbitrary k E N and q> E CL k ( G) such that q> is included in the derivation of a complete tree or q> is a complete tree itself, an item corresponding to q> will be generated. \u25a1 6 Complexity Results Fo r a given minimalist grammar G=(V, Lex, Cat, F) and input string of length n, the number of items is polynomially bounded by the length of the input string. All items are of the form [(xo, Yo):,o, (x 1 , y 1 ) :, 1 , ... , (xm, Ym):, m ]t, as defined in section 3. 2 . Each part (xi, Yi):,i of an item, 0 \ufffd i \ufffd m, has O(n 2 ) possible instantiations, as both Xi and Yi range between O and n, 0 and n included. The possible choices of 1'i do not depend on the length of the input string. The number of choices is bounded, however, because the labels occurring in any tree in CL(G) are substrings of the labels of the expressions in Lex. This follows immediately from the the definition of merge and move. Moreover, Lex is a finite set and the labels assigned by Label are finite sequences. Thus, the set of possible labels is bounded by the grammar, and, since the recognizer is sound, the nu mber of possible 1'i 's is bounded by the grammar, too. Since an item has at most k+l parts (xi, Yi):,i, where k=llicenseesl, the number of items in the chart is bounded by O(n 2k+2 ). As regards the time complexity of the recognizer, step 2.b) of the deduction procedure specified in section 3.1 requires every item on the agenda to be compared with the items already in the chart. Since the number of items in the chart is O(n 2k+2 ), this step will take O(n 2k+2 ) per item on the agenda. Fo r any item on the agenda but not already in the chart, step 2.c) of the deduction procedure checks whether any rule of inference will apply. Checking applicability of the Merge rules involves looking through all the items in the chart, since all Merge rules have two antecedents. Given an item on the agenda and an item from the chart, actually verifying whether any of the Merge rules applies to these items takes constant time. Thus, the time cost is O(n 2k+2 ) per item on the agenda. In order to determine whether one of the two Move rules will apply, the label ,o in an item has to be inspected and compared to the other labels 1'i, 1 \ufffd i \ufffd m in the same item. Since m is bounded by k=l licenseesl, there is no dependency on the length of the input string. Since steps 2.b) and 2.c) are performed in sequence, the time cost of both steps is bounded by O(n 2 k +2 ) per item on the agenda, ignoring without loss of generality the fact that step 2.c) is not performed for all items on the agenda. Steps 1. and 3. of the deduction procedure do not exceed this bound. The nu mber of items that will be put on the agenda while recognizing a string is O(n 2k+2 ). This is the upperbound on the number of possible items. There will be duplicates in the agenda, but their number is finite and does not depend on n, essentially because the number of axioms and the number of inference rules is finite and all items in the chart are unique. Thus, the overall time complexity of the recognizer is O(n 4k+4 ). Conclusions and Future Wo rk In this paper we have provided a formal definition of a recognizer for minimalist grammars, together with proofs of completeness and soundness and an analysis of its space and time complexity. There are several issues that deserve further investigation. First of all, the recognizer has to be developed into a parser. This can be done by either extending the items with a field for recording the derivational history of an item, or by devising a method for retrieving derivation trees from the chart. Secondly, we conjecture that the efficiency of the parser can be greatly improved by imposing some order on the chart. In the current recognizer, the entire chart is searched in order to determine whether any one of the Merge rules will apply. The definitions of the Merge rules suggest that grouping the items in the chart according to the first feature of their 'head labels' will allow for a more efficient search. The current recognizer operates in a bottom-up manner: no constituent is recognized until all substrings that make up its yield have been encountered. So, thirdly, it would be interesting to investigate other recognition strategies. Finally, there are still some open questions with regard to the formal power of minimalist grammars in comparison with other grammar formalisms. Careful examination of the complexity of the algorithms for recognizing and parsing these various grammar formalisms might answer some of the questions. Introduction In many domains neural networks are an effective alternative to statistical methods. This has not been the case for syntactic parsing, but recent work has identified a viable neural network architecture for this problem (SSN) [6) , [7) . This alternative parsing technology is potentially of interest because neural networks have different strengths from statistical methods, and thus may be more applicable to some tasks. Like statistical methods, neural networks are robust against noise in the input and errors in the data. But unlike statistical methods, neural networks are particularly good at handling sparse data. In order to compensate for the necessarily limited amount of data available, statistical methods must make strong independence assumptions. These assumptions lead to undesirable biases in the model generated, and may still not guarantee coverage of less frequent cases. Neural networks also require independence assumptions in order to define their input-output format, but these assumptions can be much weaker. Because neural networks learn their own internal representations, neural networks can decide automatically what features to count and how reliable they are for predicting the output. In this paper we empirically investigate the ability of SSNs to handle sparse data, relative to the statistical method Probabilistic Context Free Grammars (PCFGs) and a statistical version of the SSN parser. To test this ability we use a small dataset relative to those typically used with statistical methods. We train all the models on the same dataset and compare their results, both in terms of coverage and performance on the covered portion. The statistical version of the SSN parser has good coverage, but its performance is worse than both the other two models. The PCFG covers under half of the test sentences. The SSN produces a parse for all of the sentences, while still achieving better performance than that of the PCFG on the PCFG's parsed sentences, as measure by recall and precision on both constituents and a dependency-like measure. 2 Simple Synchrony Networks The neural network architecture used in this paper has previously been shown to be a viable parsing technology based on both initial empirical results [6) and the linguistic characteristics of the basic computational model [5) . Their appropriateness for application to syntactic parsing is a result of their ability to learn generalisations over structural constituents. This generalisation ability is a result of using a neural network method for representing entities, called Temporal Synchrony Variable Binding (TSVB) [12) . In Simple Synchrony Networks (SSNs) this method is used to extend a standard neural network architecture for processing sequences, Simple Recurrent Networks (SRNs) [3] . SRNs can learn generalisations over positions in an input sequence ( and thus can handle unbounded input sequences), which has made them of interest in natural language processing. However, their output at any given time in the input sequence is an unstructured vector, thus making them inappropriate for recovering the syntactic structure of an input sentence. By using TSVB to represent the constituents in \u2022 a syntactic structure , SRNs can be extended to also learn ge neralisations over structural constituents. The linguistic relevance of this class of generalisations is what accounts for the fact that SSNs generalise from training set to testing set in an appropriate way, as demonstrated in section 4. In this section we briefly outline the SSN architecture. Representing Constituents \u2022The problem of representing constituents in a neural network is an instance of a broader problem, representing entities in general. The difficulty is that each entity can have multiple features and the network needs to represent which fe atures go with which entity. One proposal is to use pulsing units and represent the binding between the features of each entity using the synchrony of the units ' activation pulses. This has been proposed on both biological grounds [13] and computational grounds [12] . Following the computational work we will call this method Temporal Synchrony Var iable Binding. We use TSVB's pulsing units to represent information about syntactic constituents. TSVB can be applied to a learning task by combining it with a standard neural network architecture. Because we need the network to process sequences as well as constituents, we use an architecture that combines TSVB with Simple Recurrent Networks. In addition to a layer of input units and a layer of output units, SRNs have a layer of memory units which store the internal state of the network from the previous position in the sequence. This allows SRNs to learn their own internal representation of the previous input history. To extend SRNs with TSVB, pulsing units need to be added. These units represent inputs, outputs, and internal state about individual constituents. In order to retain the SRN's ability to represent information about the situation as a whole, the architecture also has normal nonpulsing units. These units also provide a means for information about one constituent to be passed to other constituents. Pulsing units can be added to SRNs in a variety of ways. One proposed class of such networks is called Simple Synchrony Networks (SSNs) [7) . The architectural restriction which defines SSNs is appropriate for the parser used here because information about only one constituent is input at any one time, namely the constituent headed by the current input word. This input simply specifies the current word. To accommodate SSN's restrictions we simply need to provide the same information through nonpulsing input units as we provide to this one constituent through the pulsing input units [6] . In this way each constituent gets information about its head word from the pulsing input units and gets information about every other word from the nonpulsing input units. In this paper we use the simplest form of SSN, called type A in [7] . This architecture only has a single internal state layer and a single memory layer, both consisting of pulsing units. We use 100 units in each of these layers, which is a moderately large size, resulting in a ability to approximate a wide variety of functions. Learning Generalisations over Constituents The most important feature of any learning architecture is how it generalises from training data to testing data. SRNs are popular for sequence processing because they inherently generalise over sequence positions. Because inputs are fed to an SRN one at a time, and the same trained parameters (the link weights) apply at every time, information learned at one sequence position will inherently be generalised to other sequence positions. This generalisation ability manifests itself in the fact that SRNs can handle arbitrarily long sequences. The same argument applies to TSVB and constituents. Using synchronous pulses to represent constituents amounts to cycling through the set of constituents and processing them one at a time. Because the same trained parameters are applied at every time, information learned for one constituent will inherently be generalised to other constituents. This generalisation ability manifests itself in the fact that these networks can handle arbitrarily many constituents. A SSN Parser The Simple Synchrony Network architecture gives us the basic generalisation abilities that we need for syntactic parsing, but the specific way that a SSN parser generalises from training set to testing set also depends on the specific input-output format that we ask it to learn. The main challenge in defining an input-output format is that a syntactic structure consists of a set of relationships between constituents, whereas SSNs only provide us with outputs about individual constituents. More precisely, there are O(n 2 ) possible relationships between constituents and only O(n) outputs at any given time. The solution is simply to output the syntactic structure incrementally. By making use of the 0( n) positions in the input sentence 1 to produce output, the network can produce the necessary O(n 2 ) outputs over the course of the parse. In this section we will specify the nature of this incremental output in more detail, and give specifics about the format of the input to the network. In each case, the importance of these choices is how they effect the way that the network will generalise. The Model of Constituency Before discussing the specific input-output pattern, it is necessary to specify what exactly we mean by \"constituent\" . This is a crucial decision, since the constituent is the unit over which generalisations are learned. One answer would be to simply use the definition of constituency that is implicit in whatever corpus you have available. It would be possible for us to do this, but at the cost of a more complicated output format. Also, the import of constituents as the unit of generalisations may not be the one intended by the corpus writer. Instead we choose to take a lexicalised approach. Each constituent is associated with a word in the input, which is intended to be the head of that constituent. The constituent structure can then be thought of as a set of relationships between each constituent's head word and the other words and constituents in the structure. This perspective is closely related to Dependency Grammar (8] . It is also closely related to Lexicalized 'free Adjoining Grammar (11] and the head-driven statistical parsing model by Collins [2] , but in these cases our constituent needs to be mapped to the entire projection of a lexical head. Unfortunately the corpus we will use in the below experiments does not label heads for constituents. As an approximation, because we know we are using a head-initial language, we will use the first word which is a direct child of a constituent as the constituent's head. If a constituent in the original corpus does not have any words as direct children, then that constituent is collapsed into one of its child constituents, as discussed further in section 4. 1 . Thus the corpus used in the below experiments is slightly flatter than most corpora. In particular it often does not distinguish between an S and its VP, the two constituents having been collapsed. Using a lexicalised definition of constituency not only makes the unit of generalisation more lin guistically appropriate, it has the added advantage that it provides a fixed mapping between the constituents in the network's output and the constituents in the corpus. Two constituents are the same if they have the same word as their head. Using this mapping it is easy to define a fixed input output pattern for the network to learn during training, which is necessary for supervised learning algorithms such as the one used here. However, we should emphasise that it would be possible to define a different input-output format for . an SSN parser which could use any corpus's definition of constituency. The Independence Assumptions As mentioned above, the independence assumptions made by a neural network model are embodied in the definition of its input-output format. If there is no way for information to flow from a given input to a given output, then the model is assuming that that output is independent of that input. For the SSN parser used here these assumptions are due to the incremental nature of the network's output. As discussed above, the SSN parser solves the problem of outputting relationships between con stituents by outputting them incrementally. In particular, this SSN parser outputs structural rela tionships maximally incrementally. Wo rds are input to the parser one at a time, and with each word a new constituent is introduced to act as the constituent which the word heads, if needed. 2 A struc tural relationship is output as soon as both things in the relationship have been introduced. This incremental output is illustrated in figure 1 . When the parse is complete the accumulation of all the outputs fully specifies the parse, as illustrated in figure 1 . The syntactic structure of a sentence can be fully specified by specifying the set of parent-child relationships in it. These come in two types, one where the parent is a constituent but the child is a word, and one where both the parent and the child are constituent. When the child is a word, then because we assume that the first word child of a constituent is its head, the parent constituent must have been introduced before or at the same time as the word is input. Thus as soon as a word is input, the network can output which constituent is the parent of that word. This is done with a single output unit, called the parent output, which pulses in synchrony with the parent of the current input word. For example when 'John' is input in figure 1 the new constituent, cl, is specified as the parent, while when 'joke' is input the previous constituent, c3, is specified as the parent. When both the parent and child are constituents, then the relationships is output as soon as the second of the two constituents is input. If the second constituent is the child, then this output is specified using the grandparent output unit, which pulses in synchrony with the parent of the constituent which is headed by the current input word. For example when 'a' is input in figure 1 the previous constituent, c2, is specified as the grandparent. If the second constituent is the parent, then this output is specified using the sibling output unit, which pulses in synchrony with the child of the constituent which is headed by the current input word. For example when 'loves' is input in figure 1 the previous constituent, cl, is specified as a sibling. Given the previous assumptions, these three output units are all that is necessary to specify the syntactic structure of the sentence. So far we have only discussed the output pattern which we would like the network to produce (the target output), but a network will actually output real values, not zeros and ones. To interpret this output we look at all the possible parents for a given child, and pick the one whose output activation is the highest. First we incrementally choose the parents for each word, based on the parent output unit's activations. This not only determines all parent-child relationships involving words, it also determines which of the introduced constituents have head words and thus are included in the output structure. For example in figure 1 constituent c4 is not included in the output structure. Only these headed constituents candidates to be in relationships with other constituents or later words. Second we choose the best parent for these headed constituents, based on both the grandparent output unit's activations at the time of the constituent's head (for leftward attachments) and the sibling output unit's activations in synchrony with the constituent (for rightward attachments). This determines all the parent-child relationships between two constituents. 3 While the resulting set of parent-child relationships may not form a tree, this representation of the structure is sufficient to determine which words are included in each constituent, and thus to calculate recall and precision on constituents. This maximally incremental output format implies some fairly strong independence assumptions. This is particularly true for the parents of words. No output after a word can influence which con stituent is interpreted as its parent, and thereby this decision is assumed to be independent of later input words. For the parents of constituents there is a slight complication in that a later sibling output can override a grandparent output that is produced at the time of the constituent's head word. For example, in the sentence \"John knows Mary left\", the network may produce a fairly high grandparent output value for attaching 'Mary' to 'knows', but when 'left' is input an even higher sibling output value can override this and result in 'Mary' being attached to 'left '. However it is not possible for later outputs to affect a decision which does not involve a later word's constituent. For example, in the sentence \"John saw the policeman with Mary\", the decision of whether 'with' should be attached to 'saw' or 'the policeman' is not effected by any output at the time when 'Mary' is introduced, and thus the unsuitability of 'Mary' as an instrument of the seeing cannot be used. Thereby decisions between two possible parents for a constituent are assumed to be independent of any input words af ter the last head word of the three constituents involved. Finally, in addition to outputs about structural relationships, the parser also outputs the labels of the constituents. This output is produced at the time when the constituent's head word is input, using a bank of output units, one for each label. Thus the decision of what label to give a constituent is assumed to be independent of all words after the constituent's head word. This assumption is mostly for convenience, since any output about an individual constituent could be delayed until the end of the sentence. 4 The Soft Biases Because all the words are input to the representation of each constituent, in theory any earlier input can effect a later output, and thus they are not being assumed to be independent. However, in practice there are ways to make the network pay more attention to some inputs than to others. In particular, a recurrent network such as an SSN will learn more about the dependencies between an input and an output if they are close together in time. The immediate effect of this is a form of recency bias; the most recent input words will effect an output more than earlier input words. To make use of this we provide a new constituent input pattern, which is correlated with being an output for a short time afterwards. We also provide a last parent input unit, which is the disambiguated parent output from the previous input word. This is also correlated with being an output for a short time afterwards. Finally, we bias the network to pay particular attention to the head word for each constituent by providing the head word as an input at every time during the life of a constituent. Thus an output at a given input word for a given constituent pays particular attention to the input word and the head of the constituent, with previous input words providing an influence proportional to their recency. This input format has been devised in part on the basis of the author 's linguistic knowledge and in part on the basis of experimental results. 5 Generalising fr om Sparse Data To test the ability of Simple Synchrony Networks to handle sparse data we train the SSN parser described in the previous section on a relatively small set of sentences and then test how well it generalises to a set of previously unseen sentences. Because the training set is small, the testing sentences will contain many constituents and constituent contexts which the network has never seen before. Thus the network cannot simply choose the constituents which it has seen _the most times in the training set, because in most cases it will have never seen any of the constituents which it needs to choose between. To handle this sparseness of training data the network must learn which of the inputs about a constituent are important, as well as how they correlate with the outputs. The advantage of SSNs is that they automatically learn the relative importance of different inputs as part of their training process. To provide a comparison with the SSN parser we also apply a standard statistical method to the same data sets. We estimate the probabilities for two Probabilistic Context Free Grammars, one using just the network's tra ining sentences and another using both the training sentences and the cross va lidation sentences. These PCFGs are then both tested on the network's testing sentences. PCFGs deal with the problem of sparse data by ignoring everything about a constituent other than its label. The strength of this independence assumption depends on how many constituent labels the corpus has, and thus how much information the labels convey. Because we are dealing with small training sets, we only use a small number of labels. Even so, the PCFGs have only seen enough CFG rules in the training sets to produce parses for about half of the test sentences. The problem with such statistical approaches is that information is either counted (i.e. increasing the number of labels) or ignored (i.e. decreasing the number of labels), and there is no middle ground. 6  In addition to the PCFGs, we train a statistical model based on the output format of the SSN parser. This test is to control for the possibility that it is the linguistic assumptions discussed in the previous section which are responsible for the SSN parser 's performance, and not the SSN architecture. Rather than estimating the probabilities for CFG rules like in PCFGs, this statistical model estimates the probabilities of the same structural relationships used in the output of the SSN parser (parent, grandparent, and sibling, plus label-head relationships). Thus we will call this model the Probabilistic Structural Relationships (PSR) model. The PSR model also makes all the independence assumptions made by the SSN parser, but in order to deal with the sparse data it must also make additional independence assumptions. Every structural relationship is dependent on the word involved in the relationship and the head word of the constituent involved in the relationship ( and whether they are the same), but they are independent of all other words. This assumption imposes a hard bias that parallels the main soft biases provided by the SSN parser 's input format. This independence assumption is strong enough to provide us with sufficient statistics given our training data, but still captures to the extent possible the relevant information for estimating the structural relationship probabilities. The PSR model is closely related to dependency-based statistical models, such as that in (2 ] . A Small Corpus Work on statistical parsing typically uses a very large corpora of preparsed sentences (for example more than a million words in [1 ] , [2) , and [9] ). Such corpora are very expensive to produce, and are currently only available for English. In addition, using a very large training corpus helps hide inadequacies in the parser's ability to generalise, because the larger the training corpus the better results can be obtained by simply memorising the common cases . 7 Here we use a training set of only 26,480 words, plus a cross validation set of 4365 words (30 ,845 words total). By using a small training set we are placing greater emphasis on the ability of the parser to generalise to novel cases in a linguistically appropriate way, and to do so robustly. In other words, we are testing the parser on its ability to deal with sparse data . We use the Susanne 8 corpus as our source of preparsed sentences. The Susanne corpus consists of a subset of the Brown corpus, preparsed according to the Susanne classification scheme described in [10] , and we make use of the \"press reportage\" subset of this. These parses have been converted to a format appropriate for this investigation , as described in the rest of this section. 9  As is commonly done for PCFGs, we do not use words as the input to the parser , but instead use part-of-speech tags . The tags in the Susanne scheme are a detailed extension of the tags used in the Lancaster-Leeds Treebank (see [41), but we use the simpler Lancaster-Leeds scheme . Each tag is a two or three letter sequence , for example 'John' would be encoded 'NP', the articles 'a ' and 'the' are encoded 'AT', and verbs such as 'is ' encoded 'VBZ'. There are 105 tags in total. The parse structure in the Susanne scheme is also more complicated than is needed for our purposes. Firstly, the meta-sentence level structure has been discarded, leaving only the structures of individual sentences. Secondly, the 'ghost' markers have been removed . These elements are used to represent long distance dependencies, but they are not needed here because they do not effect the boundaries of the constituents . Third, as was discussed above, we simplify the nonterminal labels so as to help the PCFG deal with the small training set. We only use the first letter of each label, resulting in 15 nonterminal labels (including a new start symbol). Finally, as was discussed in section 3.1, some constituents need to be collapsed with one of their child constituents so that every constituent has at least one terminal child. There are very few constructions in the Susanne corpus that violate this constraint, but one of them is very common , namely the S-VP division. The head word of the S ( the verb) is within the VP, and thus the S often occurs without any terminals as immediate children. In these cases , we collapse the S and VP into a single constituent, giving it the label S. The same is done for other such constructions. As discussed above , this change is not linguistically unmotivated. The total set of converted sentences was divided into three disjoint subsets, one for training , one for cross validation , and one for testing. The division was done randomly, with the objective of producing cross validation and testing sets which are each about an eighth of the total set. No restrictions were placed on sentence length in any set. The training set has 26480 words, 15411 constituents, and 1079 sentences, the cross validation set has 4365 words, 2523 constituents, and 186 sentences, and the testing set has 4304 words, 2500 constituents, and 181 sentences. Training the Models The SSN parser was trained using standard training techniques extended for pulsing units [7 ] . Neural network training is an iterative process, in which the network is run on training examples and then modified so as to make less error the next time it sees those examples. This process can be continued until no more changes are made , but to avoid over-fitting it is better to check the performance of the network on a cross validation set and stop training when this error reaches a minimum. This is why we have split the corpus into three datasets, one for training, one for cross validation, and one for testing. This technique also allows multiple versions of the network to be trained and then evaluated using the cross validation set, without ever using the testing set until a single network has been chosen. This is the technique which we have used in developing the specific network design reported in this paper. The resulting network trained for 325 passes through the training set before reaching a maximum of the average between its recall and precision on constituents in the cross validation set. Estimating the parameters of a PCFG is straightforward. All the sequences of child labels that occur in the corpus for each parent label need to be extracted, counted, and normalised in accordance with the conditional probabilities required by the model. Because this process does not require a cross validation set, we create two PCFGs, one using only the network's training set and the other using both the training set and the cross validation set. The first provides a more direct comparison with the SSN parser's ability to deal with small training sets, but the second provides a better indication of how the two methods compare in practice. Estimating the PSR model is also straightforward. All the head tag bigrams associated with each structural relationship ( or label-head tag bigrams) are extracted, counted, and normalised in accor dance with the probability model. 1 0 As with the second PCFG, we use the network's training set plus its cross validation set to estimate the probabilities. In addition to embodying the same linguistic assumptions as the SSN parser, the PSR model has the advantage that it has a finite space of possible parameters (namely one probability per tag bigram for each relationship). Because a PCFG places no bound on the number of children that a parent constituent can have, a PCFG has an infinite space of possible parameters (namely one probability for each of the infinite number of possible rules). This makes it difficult to apply smoothing to a PCFG, to avoid the problem of assigning zero probability to rules that did not occur in the training set. Thus we have not applied smoothing to the PCFGs, contributing to the bad test set coverage discussed in the next section. However the PSR's finite space of parameters makes it simple to apply smoothing to the PSR model. Thus we apply a smoothing method to estimate the parameters for a second PSR model; before normalising we add half to all the counts so that none of them are zero. Testing Results Once all development and training had been completed, the SSN parser, the two PCFGs, and the two PSRs were tested on the data in the testing set. For the PCFGs and the PSRs the most probable parse according to the model was taken as the output of the parser . 11 The results of this testing are shown in table 1 , where PCFG 1 is the PCFG that was produced using only the training set, PCFG 2 is the PCFG produced using both the training set and the cross validation set, PSR is the unsmoothed PSR, and PSR Sm is the smoothed PSR. The first thing to notice about the testing results is that both PCFGs only found parses for about half of the sentences. For the unparsed sentences the PCFGs had not found enough rules in their training sets to construct a tree that spans the entire sentence, and thus there is no straightforward way to choose the most probable parse. 12 In contrast the SSN parser is able to make a guess for every sentence. This is a result of the definition of the SSN parser, as with the smoothed PSR, but unlike both PSR models the SSN parser produces good guesses . The first evidence of the quality of the SSN parser 's guesses is that they ar e exactly correct more that 4 times as often as for any of the PCFGs or PSRs. The remaining four columns in table 1 give the performance of each parser in terms of recall (percentage of desired which are output) and precision (percentage of output which are desired) on both constituents and parent-child relationships. An output constituent is the same as a desired constituent if they contain the same words and have the same label. This measure is a common one for comparing parsers. Parent-child relationships are the result of interpreting the parse as a form of dependency structure. Each parent-child relationship in the parse is interpreted as a dependency from the head word of the child to the head word of the parent . Two such relationships ar e the same if their words ar e the same. This measure is more closely related to the output of the SSN parser and the PSR model, and may be more appropriate for some applic at io ns . Given that both PCFGs produce no parse for about half of the sentences, it is no surprise that the SSN parser achieves about twice the recall of both PCFGs on both constituents and parent-child relationships (although it is interesting that the SRN actually performs more than twice as well). Thus we also compute these performance figures for the subset of sentences which ar e parsed by each PCFG, as discussed below . However restricting attention to the parsed subset will not change the PCFGs' precision figures . Just as the recall figures ar e particularly low, the precision figures are improved due to the fact that the PCFGs are not outputting anything for those sentences which are particularly hard for them (i.e. the sentences they cannot parse); Even so, the SSN does about 10% better than either PCFG on both constituent precision and parent-child precision. Table 2 shows the performance of the PCFGs and SSN on the subset of test sentences parsed by each PCFG. Note that these figures are biased in favour of the PCFGs, since we are excluding only those sentences which are difficult for each PCFG, as determined by the results on the testing set itself . Even so, the SSN outperforms each PCFG under every measure. In fact, under every measure the SSN's performance on the entire testing set is better than the PCFGs' performance on the subset of sentences which they parse. Given the large difference between the linguistic assumptions embodied in the PCFGs and the SSN parser, we need to address the possibility that the better performance of the SRN parser on this corpus is due to its linguistic assumptions and not due to the SSN architecture. The poor performance of both PSR models clearly demonstrates this. The PSR model was designed to follow the linguistic assumptions embodied in the SSN parser as closely as possible, only imposing additional independence assumptions to the extent that they were required to get sufficient counts for estimating the model's probabilities. Nonetheless, both PSR models do much worse than the PCFGs, even on the parent-child relationships, which are directly related to the parameters of the PSR model. The only exception is comparing against the recall results of the PCF G models including their unparsed sentences, but the recall results of the PCFG models on the parsed subsets indicates that this is simple an artifact of the low coverage of the PCFGs. Thus the better performance of the SSN parser cannot be due to its linguistic assumptions alone. It must be due to the SSN architecture's ability to handle sparse data without the need to impose strong independence assumptions. One final thing to notice about these results is that there is not a big difference between the results of the SRN parser on the full testing set and .the results on the subsets which are parsed by each PCFG. There is a small improvement, reflecting the fact that the PCFGs fail on the more difficult sentences. However the lack of any big improvement is a further demonstration that the SRN is not simply returning parses for every sentence because it is defined in such a way that it must do so . The SSN is making good guesses, even on the difficult sentences . This demonstrates the robustness of the SSN parser in the face of sparse training data and the resulting novelty of testing cases. Conclusion The good performance of the Simple Synchrony Network parser despite being trained on a small training set demonstrates that SSN parsers are good at handling sparse data . In comparison with Probabilistic Context Free Grammars trained on the same data, the SSN parser not only returns parses for twice as many sentences, its performance on the full testing set is even better than the performance of the PCFGs on the subset of sentences which they parse. By demonstrating SSNs' ability to handle sparse data we have in fact shown that SSNs generalise from training data to testing data in a linguistically appropriate way. The poor performance of the PSR model shows that this is not simply due to clever linguistic assumptions embodied in the particular parser used . This generalisation performance is due to SSNs' ability to generalise across constituents as well as across sequence positions, plus the ability of neural networks in general to learn what input features are important as well as what they imply about the output . The resulting robustness makes SSNs appropriate for a wide variety of parsing applications, particularly when a small amount of data is available or there is a large amount of variability in the input . The structure of the paper is as follows. In the next section, we recapitulate some basic HPSG terminology. We argue why it is hard to extract a restrictive CFG from a given HPSG due to lexicalization. We also introduce other concepts which we will need later: restrictors and root nodes. After that, section 3 present the theoretical foundation of our method: approximation as fixpoint construction of a certain monotonic function T. Section 4 presents the basic al gorithm which exactly implements T. In order to overcome some deficiencies of the basic al gorithm, both in terms of the running time and the resulting CFG, we describe useful optimizations in section 5. In section 6, we apply our method to four grammars and discuss the outcome of the approximation. We conclude this article by indicating our next steps. Basic Inventory Unification-based theories of grammar allow for an elegant integration of different levels of linguistic descriptions in the common framework of typed feature structures (see Carpenter 1992 for an intro duction). In Head-Driven Phrase Structure Grammar this assumption is embodied in the fundamental concept of a sign; see Pollard and Sag 1994 . A sign is a structure incorporating information from all levels of linguistic analysis , such as phonology, syntax, and semantics . This structure specifies interactions between these levels by means of coreferences, indicating the sharing of information. It also describes how the levels constrain each other mutually. Such a concept of linguistic description is attractive for several reasons : 1. it supports the use of common formalisms and data structures on all linguistic levels, 2. it provides declarative and reversible interface specifications between these levels , 3. all information is simultaneously available, and 4. no procedural interaction between linguistic modules needs to be set up. Similar approaches , especially for the syntax-semantics interface, have been suggested for all major kinds of unification-based theories , such as LFG or CUG. A crucial property of HPSG is that it is lexicalized, meaning that the rules ( called rule schemata) are very general ( and only a few of them exist) and that the lexicon entries ( actually the lexical ty pes) are relatively specific. HPSG shares this feature with other approaches, such as TAG and CG. Let us now clarify some terminology which is used throughout this paper. In this paper, we inter change the terms rules, rule schemata, and ID schemata in HPSG. The set of all rules is depicted by 'R. An example of a rule is the head-complement schema. When we say lexicon entry, we mean a feature structure that is subsumed by word. The collection of all lexicon entries constitutes the lexicon \u00a3. ar( r) denotes the number of daughters of a given rule r E 'R and is called the arity of r. We also need the notion of a restrictor , as originally introduced by Shieber 198 5. A restrictor is an automaton describing the paths in a feature structure that will remain after restriction ( the deletion operation). Since HPSG requires all relevant information to be contained in the SYNSEM feature of the mother structure, the unnecessary daughters only increase the size of the overall structure without constraining the search space. Due to the Locality Principle of HPSG, they can therefore be legally removed in fully instantiated items ( see Kiefer et al. 1999 ). Other portions of a feature structure will also be deleted during the iteration process. In section 4, we will describe why this is necessary. We employ two different restrictors in our approach: the lexicon restrictor L and the rule restrictor R. Finally, we introduce the term root node. These are constraints specifying well-formedness conditions for other feature structures to be legal utterances/phrases (e.g., empty SUBCAT list). Root nodes are also feature structures and checking well-formedness is achieved via unification. The set of all root nodes is depicted by N. As we said in the introduction, we not merely produce CF rules employing flat (non-)terminal symbols, but instead enrich them with other information as is known from GPSG. This information directly comes from the feature structure and a user is requested to state this information in terms of an ordered set of paths 'P, the so-called relevant paths. The values under these paths are exactly the annotations associated with the (non-)terminal symbols. Usually, one select those paths which restrict HPSG derivations the most, hence a resulting CF grammar becomes more restrictive. Kiefer et al. 1999 describe how such paths can be obtained with respect to a training corpus. This set of relevant paths is used both for the mother as well as for every daughter of an instantiated rule. In the following, we make some minor simplifications to the original HPSG framework. (1) We do not distinguish between rules and principles. Instead, we assume that each rule has already inherited the principles which can be applied to it via unification. Such a strategy can often be found in implemented systems. (2) We do not pay attention to LP constraints as they are used in HPSG. The idea here is to hard-wire them in the rules, using the specialized attribute ARGS (see section 6 for a description). This technique is commonly used in many systems. (3) We do not take into account relational or set constraints, such as set membership or set difference. This information is clearly missing in the CFG. We note here that crucial relational constraints such as append as used in the subcategorization principle of HPSG must be expressed differently; in case of append either by employing the well-known difference list representation (good!) or by using a recursive type constraint a la A:it-Kaci. Again, many systems such as TD\u00a3 or LKB use exactly the cliff list representation. For instance , the English Verbmobi/ grammar (see section 6.4) as well as CSLI's LINGO grammar circumvent append in favor of cliff lists. Approximation as Fixpoint Construction The basic inventory we have introduced in the last section now comes into play in order to formalize our idea as stated in the introduction. Let \u00a3 be the set of all lexicon entries, n the set of all rules (rule schemata), L the lexicon restrictor, and R the rule restrictor. We can view a rule r E 'R, with ar(r) = n as an n-ary function, mapping n-tuples of feature structures into a single feature structure: (3) corresponds to the generalization process of the lexicon, whereas ( 4) exactly describes the i + 1 step of the iteration. More formally in terms of the (upward) ordinal power of a monotonic mapping T (see Lloyd 1987), we define T(S) := SU LJ LJ R(r(s)) (5) rE'R. s E S<> r (r) Mathematically speaking, T itself operates on the power set of our domain of feature structures :FS , mapping approximations into finer approximations, i.e., T : p( :FS ) f-4 p( :FS ) (6) The idea here is that in the limit, T will exactly enumerate those feature structures which are instantiations of underspecified rule schemata and which have undergone the application of the rule restrictor R. Thus we are interested in sets of feature structures SE p(:FS) , where T(S) = S (7) since in this case, S has been saturated, and so we can stop our iteration process via T. In this case, we call S a fixp oint of T. Clearly, T is monotonic, since for all S, S' E p(FS) with S \ufffd S', we have T(S) \ufffd T(S'). In order to show T(S) \ufffd T(S'), we use the definition of T: SU LJ LJ R(r(s)) \ufffd S' U LJ LJ R(r(s')) rE'R. s E S<> r (r) Since S \ufffd S' (assumption), we have LJ LJ R(r(s)) \ufffd LJ LJ R(r(s')) r E'R. sE S<> r ( r ) r E 'R. s ' E S ' ar ( r ) Because S \ufffd S', there must exist an S\" s.t. SU S\" = S', hence we finally have for every r E \"R., LJ R(r(s)) \ufffd LJ R(r(s)) U LJ R(r(s\")) \ufffd LJ R(r(t)) = LJ R(r(s')) The approximated context-free grammar g then is defined as the least fixpoint of T: g := lfp(T) (8) Alternatively, one can define a fixpoint over the sets of annotated CF productions (Pi\\ \ufffd0 which are created in parallel with the construction of (Ti\\ ;?: o \u2022 Since elements from Ti are much more specific than from Pi, (7) should of course guarantee a much more specific CF grammar. From a practical point of view, however, terminating the iteration process when Pi = Pi +1 might suffice in most cases, due to the following observation. When we chose a wrong restrictor R during our experiments, we often experimented a dramatic increase of Ti , but a standstill of Pi . The reason for this were growing feature values in elements from \ufffd which however do not occur as annotations in rules from Pi . It should now be clear that a finite fixpoint can only be reached if such growing values are eliminated. Furthermore, the more features are deleted by the restrictor, the more general the CFG will be and the sooner the fixpoint will be reached. Since the approximated CFG is defined as the least fixpoint of T, a pre-specified finite iteration depth usually does neither lead to a subset nor a superset of the original HPSG. But even a pre-specified finite iteration depth together with a restrictor that only d\ufffdletes the daughters makes perfect sense, since in this case the CFG should generate the same sentences as the HPSG up to a certain number of embeddings. The Basic Algorithm We will now describe the naive approximation algorithm which can be seen as a direct implementation of the definitional equations ( 3 ) and ( 4 ), together with the fixpoint termination criterion given by (7) . The construction of (new) annotated context-free rules from successful instantiations is done in parallel with the computation of \ufffd during iteration step i. Several optimizations are discussed later in section 5 that improve the efficiency of the algorithm, but are uninteresting at this stage of presentation. We start with the description of the top-level function HPSG2CFG which initiates the approxima tion process. In addition to the parameters already presented, we also use a global variable n, the iteration depth, and a local variable P that accumulates annotated context-free rules. We begin the approximation by first abstracting from the lexicon entries .C with the help of the lexicon restrictor L (line 6 of the algorithm). This constitutes our initial set To (line 7) . We note here that an abstracted lexicon entry is not merely added using set union but instead through a more complex operation, depicted by U b . We will describe this operation in section 5. In case that an abstracted entry l is compatible with one of the root nodes from N (line 8), a start production is generated from l ( employing the relevant paths P) and added to the set of productions P (line 9). Again, adding a production does not reduce to a simple set union, but to a more sophisticated operation. We will describe U \u2794 in section 5. Finally, we start the true iteration calling Iterate with the necessary parameters. 1 HPSG2CFG(R, .C,N,P,R,L,n Iterate first checks whether we have reached a pre-defined iteration depth n specified originally in HPSG2CFG (line 13 of the algorithm). IT this is the case, we immediately return the annotated CF rules computed so far (line 14 ). Otherwise, we decrease the depth, since we are going to start a new iteration (line 15). Usually the iteration depth is oo, hence line 13 will never become true. After that the instantiation of the rule schemata with rule/lexicon-restricted elements from the previous iteration Ti begins (line 17-23). The instantiation is performed by Fill-Daughters which takes into account a single rule r and T i , returning successful instantiations (line 18). It is explained in section 5. For ea ch successful instantiation t which represents a feature structure tree of depth 1, we create an annotated CF rule using Make-Production. As was the case for Make-Symbol, Make-Production uses the relevant paths P for extracting the right annotations (line 19). It also inspects the daughters of t to compute the proper right-hand side of the new CF rule. Again, we are using the special set union operation U \u2794 here. After this, we apply the rule restrictor to t, since at this point we are allowed to delete the daughters (line 20). Furthermore, the rule restrictor should be carefully tuned (if possible) to get rid of paths whose va lues woul d otherwise grow infinitely through the approximation. Critical areas include the semantics ( roughly speaking: information under feature CONTENT in HPSG) which is collected through a derivation but does not shrink as is usually the case for va lence information. Now to guarantee a finite fixpoint ( and we are interested in this), such information must be deleted in order to enforce a moderate growth of T i+1 . We then add the restricted t to T i+1 (line 21) and check whether it is saturated as specified by one of the root nodes from N; if so, we generate a start production for t (lines 22-23). We finally come to the point where we compare the (restricted) feature structures from the previous iteration Ti with the new ones from 11+1 . If both sets contain the sa me elements, we clearly have reached a fixpoint. In this case we immediately terminate with the set of annotated CF rules P; otherwise, we proceed with the iteration (lines 24-26). Implementation Issues and Optimizations In this section we describe several techniques that speed up the iteration and that help to reduce the size of the resulting annotated context-free grammar. Speeding Up the Algorithm In order to make the basic algorithm work for large-size grammars, we modified it in several ways. The rule filter precomputes the applicability of rules into each other and thus is able to predict a failing unification using simple and fast table lookup. The quick-check method exploits the fact that unification fails more often at certain points in feature structures than at others. In an off-line stage, we parse a test corpus using a special unifier that records all failures instead of bailing out after the first in order to determine the most prominent failure points. These points constitute the quick-check vector. When executing a unification during approximation, those points are efficiently accessed and checked prior to the rest of the structure. As it was mentioned in section 4, instead of using set union we use the more elaborate operation U !; when adding new feature structure nodes to Ti +l\u2022 In fact , we add a new node only if it is not subsumed by some node already in the set. To do this efficiently, the quick-check vectors described above are employed here: before performing full feature structure subsumption, we pairwise check the elements of the vectors using type subsumption and only if this succeeds do a full subsumption test. Extending feature structure subsumption by quick-check subsumption definitely pays of: between 95-99% of all failing subsumptions can be detected early. If we add a new node, we also remove all those nodes in \u2022 T i+1 that are subsumed by the new node in order to keep the set small. This does not change the language of the resulting CF grammar because a more general node can be put into at least those daughter positions which can be filled by the more specific one. Consequently, for each production that employs the more specific node, there will be a (possibly) more general production employing the more general node in the same daughter positions. A further, although orthogonal method to speed up the subsumption check is to compute a hash key from a quick-check vector that preselects sets of feature structures which are likely to be subsumed. I.e., the quick-check vector here serves as a means to partition feature structures from T i +l\u2022 The collection of all vectors form a hierarchy (DAG) which is incrementally build up during the iteration process. The last two methods accelerate different aspects of the subsumption check: the former technique quickly rejects failing subsumptions, whereas the latter determines likely-subsuming structures, i.e., it is worth to have them both. Reducing CF Grammar Size Subsumption can be extended to annotated symbols and CF productions in an obvious way, namely as componentwise subsumption on types and (non-)terminal symbols. We can then remove all those productions which are subsumed by another one from the set of productions P. We indicated this in the algorithm in section 4 by using the special operation U \u2794\u2022 Since applicability of the more general production implies the applicability of the specialized one, the language induced by the reduced grammar remains the same. Rule folding is another method t\ufffdat can drastically decrease the number of CF productions. Assume that we find a set of annotated CF rules that only differ in one slot. Assume further that the set of values for the slot is equal to the set of all possible values for this slot. Hence we can replace these rules by a single rule in which the slot is assigned the most general value. Rule folding also leads to a grammar inducing the same language. Clearly, the analogue to rule folding can also be implemented in the feature structure domain, but is much more complicated . Since the annotated CF grammar is still a CFG, we can furthermore remove those symbols which will never contribute to a reading. These symbols and the productions using them are called useless and there exist fast decidable algorithms that given a grammar produce a weakly equivalent one which does not contain useless productions (e.g., Hopcroft and Ullman 1979, pp. 87). Computing the Productions Afterwards Although the alg orithm from section 4 produces annotated CF productions in parallel with instanti ated rule schemata (lines 9, 19, and 23), it might be more efficient to generate them afterwards when reaching the fixpoint ( or a predefined iteration depth). Since (5) tells us that (more general) elements from iteration step i are always present in step j, j 2'.: i (remember, T is monotonic), it suffices to generate CF productions simply by instantiating the rule schemata a final time with elements from the last iteration step . Such a strategy can be more efficient than the parallel generation, due to the fact that we do not have to ch eck for rule subsumption and rule folding ( recall that elements in \ufffd are pairwise incomparable). Examples and Results In A Grammar With Coreferences Our second example employs coreferences to show the variation of the CAT feature in the annotated CF rules between the mother and the two daughters. After three iterations, the algorithm terminates with six feature structure nodes and 18 context-free productions (see figure 2 ). Rule Lexicon Entries [CAT 0 ] ARGs( [cA@] , [cA10] ) [cAT A] Rule [a] --> lex-entry [a] lex-entry [a] Rule [b] --> lex-entry [b] lex-entry [b] Rule [c] --> lex-entry [c] lex-entry [c] Rule [a] --> lex-entry [a] Rule [a] Rule [b] --> lex-entry [b] Rule [b] Rule [c] --> lex-entry [c] Rule [c] Rule [a] --> Rule[a] lex-entry [a] Rule [b] --> Rule [b] lex-entry [b] Rule [c] --> Rule [c] lex-entry [c] [cAT a] [cAT c] S[_] --> Rule [a] S[_] --> Rule [b] SL] --> Rule [c] SL] --> lex-entry [a] SL] --> lex-entry [b] SL] --> lex-entry [c] Rule [a] --> Rule [a] Ru le [a] Rule [b] --> Rule [b] Rule [b] Rule [c] --> Rule [c] Rule [c] Figure 2 : Coreference variation in grammar 2. Shieber's Toy Grammar The CSLl's English Verbmobil Grammar Our final example employs the English Verbmobi/ grammar, developed at CSLI, Stanford. The gram mar consists of 42 rule schemata, 7,473 types, and a lexicon of 4,919 stems from which 10,967 full forms are derived. The lexicon restrictor for the English grammar as shown in fi gur e 4 maps these entries onto 422 lexical abstractions. This restrictor tells us which parts of a feature structure have to be deleted-this is the kind of restrictor which we are usually going to use. We call this a negative restrictor, contrary to the positive restrictors used in the PATR-II system that specify those parts of a feature structure which will survive after restricting it. Since a restrictor could have reentrance points, one can even define a recursive ( or cyclic) restrictor to foresee recursive embeddings as is the -----a------e-------0------- a \u2022 \u2022\u2022 ,c \u2022 \u2022 \u2022 \u2022 . \u2022 IC\u2022 \u2022\u2022\u2022\u2022\u2022 -IC\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022IC Summary We have presented a context-free approximation of unification-based grammars, such as HPSG or PATR-II. The application of our method to the large-scale English Verbmobi/ grammar has been finished with promising results. We plan to evaluate our method on the German and Japanese HP SGs used in Verbmobi/. The idea also seems to work with minor modifications for approximating HP SG by Tree Substitution Grammars, TAGs, or other kinds of lexicalized grammars. The substantial difference here comes from the interpretation of the restrictor and the function Fill-Daughters. This is currently under evaluation. We also plan to compare the outcome of our approach with the results reported in Torisawa et al. 2000 who also achieved a conversion of the LINGO grammar into a CFG. In section 5. 1, we outlined two orthogonal techniques that speed up different aspects of feature structure subsumption, viz., quickly rejecting failing subsumptions and quickly determining likely subsuming feature structures. These techniques might be worthwhile in other cases where subsump tion is a time-critical operation (e.g., during packing of feature structures). Introduction Efficient parsing algorithms for purely context-free grammars have long been known. Most natural language applications, however, require a far more linguistically detailed level of analysis that cannot be supported in a natural way by pure context-free grammars. Unification-based grammar formalisms (such as HPSG), on the other hand, are linguistically well founded, but are difficult to parse efficiently. Unification-augmented context-free grammar formalisms have thus become popular approaches where parsing can be based on an efficient context-free algorithm enhanced to produce linguistically rich representations. Whereas in some formalisms, such as the ANLT Grammar Formalism [2] [3], a context-free \"backbone\" is compiled from a pure unification grammar, in other formalisms [19] the grammar itself is written as a collection of context-free phrase structure rules augmented with unifi cation constraints that apply to feature structures that are associated with the grammar constituents. When parsing with such grammar formalisms', the goal of the parser is to produce both a c-structurea constituent phrase structure that satisfies the context-free grammar -and an !-structurea feature structure that satisfies the functional unifi cation constraints associated with phrasal constituents. Regardless of the specific context-free parsing algorithm used, the most common computational approach to performing the dual task of deriving both c-structure and f-structure is an interleaved method, where the unification functional constraints associated with a context-free rule are applied whenever the parser completes a constituent according to the rule. If a \"bottom-up\" parsing approach is used, this process usually involves applying the unification constraints that augment a grammar rule at the point where the right-hand side (RHS) constituents of the rule have all been identified and are then reduced to the left-hand side (LHS) constituent in the rule. The functional unification constraints are applied to the already existing feature structures of the RHS constituents, resulting in a new feature structure that is associated with the LHS constituent. In the case that any of the specified functional constraints cannot be satisfied, unifica tion fails, and the completed left-hand side (LHS) constituent of the rule is pruned out from further consideration. The computational cost involved in the construction and manipulation of f-s tructures during parsing can be substantial. In fac t, in some unification-augmented formalisms, parsing is no longer guaranteed to be polynomial time, and in certain cases may even not terminate (when infinitely many f-struc tures can be associated by the grammar with a given substring of the input) [ 1 7 ]. Additionally, as pointed out by Maxwell and Kaplan [1 2), interleaved pruning is not the only possible computational strategy for applying the two types of constraints. However, not only is interleaved pruning the most common approach used , but certain grammars, in which the context-free backbone in itself is cyclic, actually rely on interleaved f-structure computation to \"break\" the cycle and ensure termination. Thus, in this paper, we only consider parsers that use the interleaved method of computation. Ambiguity packing is a we ll known technique for enhancing the efficiency of context-free parsers . However, as noted by Briscoe and Carroll [2] [3], when parsing with unification-augmented CFGs, the propagation of feature structures imposes difficulties on the ability of the parser to effectively perform ambiguity packing . In this paper, we demonstrate that a clever heuristic for prioritizing the execution order of grammar rules and parser actions can significantly reduce the problem and achieve high levels of ambiguity packing. We prove that our ordering strategy is in fac t optimal in the sense that no other ordering strategy that is based only -on the context-free backbone of the grammar can achieve better ambiguity packing. We then present empirical evaluations of the proposed technique, implemented for both a chart parser and a generalized LR parser, that demonstrate that rule prioritization using our heuristic can achieve very significant gains in parser efficiency. We start out (Section 2) by describing the general tech nique of ambiguity packing, its utility in parsing, and the problem of applying ambiguity packing in context-free parsers with interleaved uni fica tion. Section 3 then describes our heuristic for prioritizing the order in which rules are applied during parsing, in order to ensure effective ambiguity packing. We then analyze the proposed heuristic and prove that it is indeed optimal. In Section 4 we present evaluations performed with both a GLR parser and a chart parser, wh ich demonstrate the effectiveness of our rule prioritization tech nique. Finally, our conclusions and future directions are discussed in Section 5. Ambiguity Packing in Context Free Parsing Ambiguity Packing Many grammars developed for parsing natural language are highly ambiguous . It is often the case that the number of diffe rent parses allowed by a grammar increases rapidly as a function of the length (number of words) of the input, in some cases even exponentially. In order to maintain feasible runtime complexity (usually cubic in the length of the input), many context-free parsing algorithms use a shared parse node representation and rely on performing Local Ambiguity Packing'. A local ambiguity is a case in which a portion of the input sentence can be parsed and reduced to a particular grammar category (non-term inal) in multiple ways . Local ambiguity packing allows storing these multiple sub-parses in a single common data structure, indexed by a single pointer. Any constituents further up in the parse tree can then refer to the set of sub-parses via this single pointer, instead of referring to each of the sub-analyses individually. As a result , an exponential number of parse trees can be succinctly represented, and parsing can still be performed in polynomial time. Obviously, in order to achieve optimal parsing efficiency, the parsing algorithm must identify all pos sible local ambiguities and pack them together. Certain context-free parsing algorithms are inherently better at this task than others. Tabular parsing algorithms such as CKY [21] by design synchronize processing in a way that supports easy identification of local ambiguities. On the other hand, as has been pointed out by Billot and Lang [1] , the Generalized LR Parsing algorithm (GLR) [18] is not capable of performing full ambiguity packing, due to the fact that stacks that end in different states must be kept distinct. There has been some debate in the parsing community regarding the relative efficiency of GLR and chart parsers, with conflicting evidence in both directions [16] , [20} , [2] , [14] . The relative effectiveness of performing ambiguity packing has not received full attention in this debate, and may in fact account for some of the conflicting evidence 1 . The Problem: Ambiguity Packing in CFG Parsing with Interleaved Unification Most context-free parsing algorithms, including GLR and chart parsing, are under-specified with respect to the order in which various parsing actions must be performed. In particular, when pursuing multiple ambiguous analyses, the parsing actions for different analyses may be arbitrarily interleaved. In a chart parser, for example, this non-determinism is manifested via the choice of which key (or inactive edge) among the set of keys waiting in the agenda should next be used for extending active edges. In the case of a GLR parser the non-determinism appears in the choice of which among a set of possible rule reductions should be picked next. The particular order in which parsing actions are performed determines when exactly alternative analyses of local ambiguities are created and detected. With certain orderings, a new local ambiguity for a constituent may be detected after the constituent has already been further processed ( and incorporated into constituents higher up in the parse tree) . When parsing with a pure CFG, this does not affect the ability of the parser to perform local ambiguity packing. When a new local ambiguity is detected, it can simply be packed into the previously created node. Any pointers to the packed node will now also point to the new sub-analysis packed with the node. However, when parsing with unification-augmented CFGs, this is not the case. Since feature structures corresponding to the various analyses are propagated up the parse tree to parent nodes, a new local ambiguity requires the creation of a new feature structure that must then be propagated up to any existing parent nodes in the parse tree. In effect, this requires re-executing any parsing actions in which the packed node was involved. In many implementations (for example, the Generalized LR parser/Compiler [19] ), to avoid this rather complex task, ambiguity packing is not performed if it is determined that the previous parse node has already been further processed. Instead, the parser creates a new parse node for the newly discovered local ambiguity and processes the new node separately. As a result, the parser's overall local ambiguity packing is less than optimal, with a significant effect on the performance of the parser. [6] . While the two issues are related, we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. While there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-structure level is a necessary pre-condition for efficient parsing, regardless of how well f-structures are packed. The Rule Priorit ization Heuristic In order to ensure effective ambiguity packing in unification-augmented context-free parsers, the sit uation in which a new local ambiguity is detected after the previous ambiguity has already been further processed must be avoided as much as possible. Ideally, we would like to further process a constituent only after all local ambiguities for the constituent have been detected and packed together into the appropriate node. Our goal was thus to find a computationally inexpensive heuristic that can determine the optimal ordering, or at least closely approximate it. The heuristic that we describe in this section uses only the context-free backbone of the grammar and the spans of constituents in determining what grammar rule ( or parsing action) should be \"fired\" next. Later in the section we prove that this heuristic does in fact achieve the best amount of packing possible, given the informa tion available. In practice, it is easy and fa st to compute, and it results in very substantial parser efficiency improvements, as demonstrated by the evaluations presented in the following section. The heuristic was initially developed for the GLR parser, parsing with a unification-augmented CFG. It was then modified to handle the corresponding ordering problem in a chart parser. The GLR Ordering Heuristic In the context of the GLR parser, the heuristic is activated at the point where a decision has to be made between multiple applicable grammar rule reductions. The following example demonstrates the problem and how we would like to address it. Let us assume we have just executed a rule reduction by [ruleO : A --> B CJ that created a parse node for a constituent A that spans words 4 -7 of the input. Assume we now have to choose between the following possible rule reductions: 1. Reduce by [rule1 : D --> A] that would create a constituent D spanning words 4 -7 (using the previously just created constituent A). Reduce by [rule2 : A --> E F ] that would create a constituent A spanning words 4 -7. Reduce by [rule3 : G --> B A] that would create a constituent G spanning words 3 -7 (using the previously just created constituent A) . Which rule reduction should be picked next? Since the last rule reduction created a constituent A spanning 4 -7, by picking [rule2] we can create another constituent A spanning 4 -7, that can then potentially be packed with the previous A. If we pick [rule1] on the other hand, we would further process the previously created constituent A. When [rule2] is then executed later, the new A can no longer be packed with the previous A. Thus, it is best to choose to perform the [rule2] reduction first. The first criterion we can use in a heuristic aimed at selecting the desired rule is the span of the constituent that would result from applying the rule. Obviously, we wish to delay applying rules such Input : a set of applicable grammar rule reductions . Output : A selected grammar rule reduction to be performed next . Select ion Heuristic: (1) For each potential grammar rule reduct ion, determine the span (start and end positions) and category of the constituent that would result from the rule reduction . (2) Select the rule reduction that is rightmost -has the largest start position . (3) If there is more than one rightmost rule reduction , pick a rule reduction that results in a category that is ''least'' according to the ''>* '' partial-order . Ignoring for the moment the complications arising from unary rules (such as [rule1 : D --> A] above) and epsilon rules (which consume no input), selecting a rule that is \"rightmost\" -creating a constituent with the greatest possible starting position, will in fact achieve this goal. This is due to the fact that in the absence of unary and epsilon rules, every constituent must cover some substring of the input, and thus every RHS constituent in a grammar rule must start at a different input position. Thus, the \"rightmost\" criterion supports the goal of delaying the construction of constituents that extend \"further to the left\" until all ambiguities of the previous span have been processed, and all potential local ambiguities have been packed. In the presence of unary grammar rules, however, the \"rightmost\" criterion is insufficient. Looking again at our example above, both [rule1] and [rule2] are \"rightmost\" , since both create a con stituent that starts in position 4. However, [rule1] would further process the existing A constituent before [rule2] detects a local ambiguity. The problem here is that the application of unary rules does not consume an additional input token. We therefore require a more sophisticated measure that can model the dependencies between constituents in the presence of unary rules. To do this, we use the context-free backbone of the grammar, and define the following partial-order relation \">\" between constituent non-terminals: \u2022 For every unary rule A --> B in the grammar G, A \ufffd B. \u2022 We compute \"\ufffd*\" -the transitive closure of \"\ufffd\" We can now extend our heuristic to use the partial-order information . The idea is not to perform a unary reduction resulting in a constituent B when there is an alternative \"rightmost\" reduction that produces a constituent A where B \ufffd* A. The resulting GLR parser heuristic can be seen in Figure 1 . The partial-order \"\ufffd*\" can easily be pre-compiled from the grammar. Note that it is theoretically possible that for particular non-terminals A and B, both A \ufffd* B and B \ufffd* A. This implies that the context-free backbone of the grammar contains a cycle. With unification-augmented CFGs, this is indeed possible, since the unification equations augmenting the grammar rules may resolve the cycle. In such cases, the above heuristic may encounter situations where there is no unique rule that is minimal according to the partial-order. If this occurs, we pick one of the \"least\" categories randomly. This may in fact result in sub-optimal packing, but only full execution of the unification operations can in fact correctly decide which rule should be picked in such cases. In practice, the computational cost of such a test would most likely far exceed its benefits. Handling Epsilon Rules The case that the grammar contains epsilon rules requires some additional attention. In this case, the premise that every RHS constituent of a grammar rule starts at a different position (i.e. consumes input) no longer holds. Consequently, the \"rightmost\" criterion no longer ensures that a rule that includes a constituent A on the RHS will not fire until all possible local ambiguities of A have been processed and packed. It is useful to note, however, that the problem is in fact similar to that of unary rules, and can be treated quite similarly as well. We first find all \"nullable\" non-terminals in the grammar G -the set of all non-terminals that can derive the empty string. A well known algorithm for detecting nullable non-terminals can be found in Hopcroft and Ullman [5] . We den\ufffdte by A E EP(G) that A is nullable. We now modify the partial-order relation defined above to handle the case of nullable categories. We define \"2::;\" as follows: 1. if A 2::* B then A 2::; B 2. for every rule A \u2794 B1 B2 \u2022\u2022\u2022Bk in G, check if Bi E EP(G) (for 1 \ufffd i \ufffd k). If the rule is such that at most one Bi /E EP (G), then for all 1 \ufffd i \ufffd k, Ai Bi 3. we compute the transitive closure of 2::; We then replace the \"2::*\" relation in our ordering heuristic with the extended \"2::;\" relation. In tuitively, the idea is to identify grammar rules that are not unary, but which when \"fired\" will not construct a constituent that extends further than the RHS constituents. This is the case whenever the LHS constituent derives the empty string, or else all but one of the RHS constituents derive the empty string. We extend the partial-order so that the RHS constituents in such rules are \"lesser\" in the order than the LHS constituent. This ensures that such rules will not be selected until all local ambiguities of the RHS constituents have been processed and packed. Proof of Optimality We now argue that the complete ordering heuristic that uses the \"rightmost\" and \"least\" criteria as defined above is in fact optimal in the sense that it achieves the maximal ambiguity packing possible given only the context-free backbone of the grammar as available information. Let us assume to the contrary that the heuristic does not result in optimal packing. This implies that a constituent A of span ( i, j) was created, that a rule that contains A on the RHS was selected by the heuristic, executed, and created a LHS constituent B, and that subsequently, another rule with a LHS of A was executed, creating a new A of span ( i, j) that can no longer be packed with the previous A. B must also be of span ( i, j), since otherwise it could not have been chosen due to the \"rightmost\" criterion. At the time the rule creating B was selected by the heuristic, B must have satisfied the \"least\" criterion. If the second A is created as a result of processing the first A via a series of rule applications, then the grammar is in fact cyclic. As mentioned earlier, while our heuristic is not guaranteed to be optimal in such cases, any better heuristic would require using f-structure unification information. So we assume that the second A constituent was created via a series of rule applications that did not involve the first A. We look at the sequence of constituents X1 , X 2 , \u2022\u2022\u2022A created in the series of rule applications that resulted in the second A after the first A had already been created. All must have span (i, j) , since otherwise the second A cannot have a span of ( i, j). At least one of these rule applications must have been a possible candidate at the point in time that the rule using the first A and creating B was chosen and executed. However, by the definition of \ufffd; , for each of the Xi , A \ufffd; Xi , and since B is created by applying a rule that uses the first A, we also have that B \ufffd= A. By the transitive closure property of \ufffd; we thus have that for all of the Xi , B \ufffd; Xi . Thus, at the point in time where the rule creating B was fired, B was not minimal according to the \"least\" criterion, and we derive a contradiction. We therefore conclude that the heuristic is in fact optimal. The Chart Parser Ordering Heuristic In order to achieve effective ambiguity packing in the case of the chart parser, we require that the parser process the input strictly left-to-right, in order to synchronize the creation and processing of constituents with similar spans. While chart parsers in general do not require such ordered processing, many chart-based parsers are in fact left-to-right, and this is usually not a burdensome restriction. Our rule reordering heuristic can then be added to the parsing algorithm in a straightforward way. In the case of the chart parser, rather than directly reordering reductions, the reordering heuristic modifies the order in which active edges are extended. The idea is that the most efficient way to extend active edges is to ensure that active edges like [A \u2794 .. . \u2022C ... ] are not extended over inactive edges of category C until all possible inactive edges of category C starting in the appropriate position in the chart have been inserted and packed. The same criteria of \"rightmost\" and \"least\" are applied in deciding which active edge should be selected next for extension. Empirical Evaluations and Discussion To evaluate the effectiveness of our rule selection heuristic we conducted empirical test runs with both a GLR parser and a chart parser. Both parsers are designed to support the same LFG style unifica tion grammar framework. The two parsers and the grammar formalism are briefly described below. Both parsers also have robust versions that are designed to overcome input that is not completely grammatical due to disfluencies or lack of grammar coverage. The robust parsers can skip over words or segments of the input that cannot be incorporated into a grammatical structure. When operated in robust mode, the skipping of words and segments introduces a significantly greater level of local ambiguity. In many cases, a portion of the input sentence may be reduced to a non-terminal symbol in many different ways, when considering different subsets of the input that may be skipped. Thus, efficient runtime performance of the robust versions of the parsers is even more dependent on effective local ambiguity packing. We therefore conducted evaluations with the two parsers in both robust and non-robust modes, in order to quantify the effect of the rule selection heuristic under both scenarios. All of the described experiments were conducted on a common test set of 520 sentences from the JANUS English Scheduling Task [10] , using a general English syntactic grammar developed at Carnegie Mellon University. The grammar has 412 rules and 71 non-terminals, and produces a full predicate-argument structure analysis in the form of a feature structure. For the GLR parser, the The GLR Parser The Evaluation Results We first ran both the GLR and the LC parsers in non-robust mode (with no word skipping allowed) , once without the rule reordering heuristic and once with the heuristic. Figure 3 (left ) shows a plot of the average number of created parse nodes as a function of sentence length. For both parsers, the total number of nodes created when rule reordering is applied significantly decreases, especially with longer sentences. This is a direct result of the increased level of ambiguity packing performed by the parsers when rule reordering is applied. For the LC parser, the savings are quite dramatic. For example, for sentences of length 12, the average relative reduction in number of parse nodes with reordering was about 12% for the GLR parser and about 40% for the LC parser. As can be expected, this relative reduction rate appears to grow as a function of sentence length, due to increasing levels of ambiguity. was more significant than the non-robust case, while for LCFLEX it appears to be about the same. For example, for sentences of length 12, the average relative reduction in number of parse nodes with reordering was about 19% for the GLR* parser and about 39% for the LCFLEX parser. Figure 4 (right) shows a plot of the average parse times as a function of sentence length. As can be seen, in robust mode, the LCFLEX parser is much faster than GLR* . As expected, runtimes when rule reordering is applied significantly decreased. For G LR *, the decrease is more pronounced than in the none-robust experiment, while for LCFLEX, it was about the same. For sentences of length 12, the time savings were about 44% for GLR* and about 21 % for LCFLEX . Conclusions and Fut ure Directions The efficiency of context-free parsers for parsing natural languages crucially depends on effective ambiguity packing. As demonstrated by our experimental results presented in this paper, when parsing with CFGs with interleaved unification, it is vital to execute parsing actions in an order that allows detecting local ambiguities in a synchronous way, before their constituents are further processed. Our grammar rule prioritization heuristic orders the parsing actions in a way that achieves the best possible ambiguity packing using only the context-free backbone of the grammar and the constituent spans as input to the heuristic. Our evaluations demonstrated that this indeed results in a very substantial improvement in parser efficiency, particularly so when incorporated into robust versions of the parsers, where local ambiguities abound. The focus of most of our current and planned future research is on efficient parsing within the context of the robust LCFLEX parser. In particular, we are interested in investigating the interdependence between the parser's robustness capabilities, its search strategy, and its disambiguation and parse selection heuristics. Effective ambiguity packing will continue to be an important factor in this investigation. We plan to further explore strategies other than interleaved pruning for efficiently applying both phrasal and functional constraints, while using the existing grammar framework and broad-coverage grammars that are at our disposal. This paper focuses on robust chart parsing with Lexicalized Tree Grammars. We do not con sider here probabilistic approximation but only complete or partial structures obtained with valid rule applications. The classical parsing algorithms for LTAG, for . instance CKY-like algorithm [Vijay-Shanker, 1987], Head-Corner [van Noord, 1994] or Earley-like algorithm [Schabes, 1994] focus on the parsing of complete grammatical utterances. We argue that an algorithm dedicated to the parsing of a Lexicalized Tree Grammar can take into account more efficiently EDL and lexicalization in order to (1) obtain beneficial extended partial results if the global parsing fails, (2) decrease the average complexity of the analysis obtained with bottum-up parsers. The constraints expressed with lexicalized elementary trees are richer than the constraints captured with a set of rewriting context free rules. The extended domain of locality is the fact that an elementary tree encodes directly a syntactical substructure view as a partial parsed tree. It allows to define constraints in more than one level of the parsing tree as compared to context free rules and permits to use atomic features. One way to exploit this property during parsing is to consider for instance co-anchors which are often neglected in existing parsing algorithm. Co-anchors encode cooccurrence information which are significant for parsing. Co-anchors are fr equently used in the French LTAG grammar for prepositional complements and idioms [Abeille et al., 1999) . They could also be massively used in elementary trees obtained with Explanation Based Learning (EBL) methods used to speed up parsing . The lexicalization imposes that each elementary tree contains at least one lexical terminal symbol called the anchor. This constraint permits to represent in each elementary tree one of the syntactical contexts of a lexical entry. This property is usually exploited during a pre-parsing process which consists in the selection of the sub-set of elementary trees that can anchor at least one of the words of the input sentence . But such a property results also in a lot of duplication of the same sub-structures in the grammar which does not exist in a CFG. Existing parsers usually ignore this drawback and result in a lot of redundant computations . Considering the syntactic level, the two main differences between parsing a sentence using a formal grammar and a grammar dedicated to natural language are the necessity to take into account the ambiguity of the language and the potential \"out of grammar\" words and structures. Addressing the problem of ambiguities, efficient results preserving all valid rule applications has been obtained using tabular parsing technics: The result stored in a chart is a shared parse forest [Lang, 1991] . From the point of view of robustness, it is important to be able to implement local analyses, i.e. to try to extract a maximum of information from the utterance ( at least all potential constituents) even if the complete parsing failed. Prediction usually speeds up a parser. Howeve r as explained for example in [Magerman and Weir, 1992] , Earley-style prediction can improve the average case performance but it has serious impacts on ro bust processing of incomplete and ungrammatical sentences which are very common in natural language systems. _The same limit can be observed for LR-style parsing results as [Nederhof, 1998 ]. As a consequence, we present here a new tabular parsing algorithm for LTAG called connection driven parsing. This incremental bottom-up algorithm could be applied to other kind of LT G. Classical top-down predictions could be used to improve t_ he parsing but they would decrease ro bustness. Connection driven parsing technics Lexicalized Tree Grammars A Thee Adjoining Grammar is specified as a quintuplet G = (E, NT , I , A , S), where E is a set of terminal symbols, NT the disjoint set of nonterminals including the start symbol S, and where I and A are two finite sets of trees called respectively initial trees and auxiliary trees. The set I U A gathers the elementary trees. We consider the standard definition of completely Lexicalized TAG in which each tree contains at least a leaf node, called the anchor, which corresponds to a word. Fo r the complexity results we note N the maximum number of nodes of an elementary tree, G the size of the set I U A and n the lenght of the input string to parse. We note a substitution node with its category and the mark .!,., internal nodes without mark, root nodes with D.. and anchors with the lexical mark O. Finally, we note * l (resp . *r ) the foot node of a left (resp. right) auxiliary tree and * the foot node of a wrapping auxiliary tree. In order to make explanations easier, we will not consider non-adjunction constraints on nodes. A Thee Inserted Grammar (TIG) is a LT G which does not include any wrapping auxiliary tree and supposes that this kind of tree never appears dynamically during the parsing . Using a TIG the worst case complexity of the parsing is decreased to (n 3 ) as all derivations are equivalent to Context Free deriva tions [Schabes and Waters, 1995] . Limits of CF algorithms invariant One of the most important caracteristic of a parser for natural language is its ability to deliver the richest possible partial parsing results when the complete parsing fails. Diagnostic, repairing and interpretation are much easier when we can exploit informative partial derivation trees. When we use a chart parsing algorithm, the partial results are given by the items which represent a chunk of well recognized words. The nature of a partial result is given by the invariant of the algorithm which caracterizes the properties of each produced item. In CFG algorithms, the invariants are based on subtree recognition: Each item represents one position in a elementary tree and a portion of the string to parse. The classical invariants im pose that the sub-tree dominated by the dotted node associated to an item is completely parsed [Shieber et al ., 1995] . Non predictive rules combine complete subtrees into larger ones preserving this invariant. In figure 1 , we indicate different subtrees which have to be completely recognized to allow the processing of the parsing of the elementary tree. Now we suppose that the utterance to be parsed is agrammatical because of an unexpected component on the right ( at position marked with a ?) . s Not {,V , N ,\ufffd / \ufffd ' , , , The subtree a 1 on the right is not adjacent to the anchor of the elementary and so the attachment on substitution node N 1 is impossible. The subtree dominated by the node SV can not be complete and the parsing process of this elementary tree must stop, even if the subtree a0 can be combined by adjacency to the susbtitution node N0\u2022 Considering a bidirectional parsing as a CKY type or Head Driven type [Lavelli and Satta, 1991] [van Noord, 1994 ] when an unexpected phenomena occurs the classical invariant stops the parsing process on both sides of the current recognized subtree even when it is possible to continue the parsing on one side, resulting in poorer partial results. Our first proposition is to focus the parsing on recognized islands and not on recognized subtree and to allow real bidirectional extensions. Since tabular technics impose adjacency of items to combine, we will see that we can take into account the topology of the trees to decrease the average complexity with a new level of granularity for a linearized tree called connected route. Finite States Automata representation of an elementary tree The existing representations of the parsing process of a elementary tree are dotted tree [Vijay-Shanker, 1987] and dotted rules [Nederhof, 1997] . In both case this representation is con strained by their locality. We propose an alternative representation that allows to express contraints of significant parts of the tree . The linearization of a tree can be represented with a Finite State Automaton (FSA) as in figure 2 . Every tree traversal (left-to-right, bidirectional from an anchor, ... ) can be performed on this au tomaton . The dotted trees used for example in [Schabes , 1994] or [Shieber et al ., 1995] are equivalent to the states of these automata. (1/ s ft) '1 1) \u25ca S N .!, V \u25ca V S \u00a9-0-0-0-0-0-\u00a9 Figure 2 : Simple FSA representing an elementary tree for the normal form of an intransive verb. We consider the following definitions and notations \u2022 Each automaton transition is annotated with a category of node, each non-leaf node appears twice in the list of transition framing the nodes which it dominates . In order to simplify our explanation the transition is shown by the annotated category. \u2022 Transitions can be bidirectionnal in order to be able to start a bidirectionnal tree walk of a tree starting from any state. \u2022 Considering a direction of transition (left-to-right, right-to-left) the FSA becomes acyclic . The two first indices are the limits on the input string of the island ( an anchor or consecutive anchors) corresponding to the item . During the initialization, we build an item for each anchor ( or co-anchor) present in the input string . An item also stores two states of the same FSA corresponding to the maximal extension of the island on the left and on the right, and only if necessary two additional indices for the position of the foot node of a wrapping auxiliary tree and the state star corresponding to the node where the current wrapping adjunction have been predicted. This representation maintains the following invariant: An item of the form (p, q, (7 \u00a3, UR) specifies the fac t that the linearized tree represented by a FSA . 6 . is completely parsed between the states l7\u00a3 and UR of . 6 . and between the indices p and q. No other attachment on the tree can happen on the nodes located between the anchors p and q-1. Parsing invariant and island representation Connected routes Considering an automaton re presenting the linearization of an elementary tree, we can define a con nected route as a part of this automaton corresponding to the list of nodes crossed successively until reaching a substitution, a foot node or a root node (included transition) or an anchor (excluded tran sition). Connected route is an intermediate level of granularity when representing a linearized tree : each elementary ( or a derived tree) can be represented as a list of connected routes . Since they can represent frontiers of the constituants, considering connected routes during the parsing permits to take into account the topology of the elementary trees and to locate significative nodes for an attachment . The main advantage of considering connected routes during the parsing is the following: The connected routes located between two c\ufffdnsecutive anchors become useless, because there is no place for any operation, and they can be directly eliminated without considering the states they contain . Such elimination will be performed during the parsing each time anchors get close to each other, so each time that an attachment is performed because, by using classical tabular techniques and linearized trees, the parsing is driven by the connection of anchors . This elimination operation is called co-anchor reduction and permits to consider less states (so nodes) during the parsing and so less hypotheses to test . We use the following additional simplified notations and primitives \u2022 The connected route passing through the state ad is noted r d. \u2022 next(r) (resp. previous(r)) gives the first state of the connected route after (resp . before) r according to a left-to-right automaton walk. \u2022 next(N) (resp . previous(N)) gives the state after (resp . before) the transition N. \u2022 head(f) (resp. tail(f)) gives the first right (resp . left) transition of the leftmost (resp . rightmost) state of the connected route r. Inference rules system The derivation process can be viewed as inference rules which use and introduce items . The inference rules (Schabes, 1994] have the following meaning : If item1 and item2 are present in the chart and if the conditions are fulfilled then add item3 in the chart if necessary: item1 add item3 ( conditions ) We present in tables 1 to 5 the different rules of the parser. We illustrate each rule with an abstract tree combination diagram and the state positions on the automata involved in the rule . The first item can be considered as the functor of the rule, for each items we examine successively the nodes on the left and right connected routes . We just have to test then if another adjacent item fulfills the requirements to be the operand of one of the rules. Fo r the bidirectional parsing we just implement the symetric rules . Each rule extends the position on the linearized elementary tree instead of climbing up the spine 1 of the elementary tree as in (Lavelli and Satta, 199 1] or [van Noord, 1994] . Even if the parsing is stopped on one side of the spine, our algorithm allows to reach the root node on the other side ( of course if no agrammaticality occurs on this second side) when the classical algorithms would completely stop the parsing on the both sides . (p, q,uL,UR) (q, r,UL 1 ,UR (p, q,uL,uR) (q, r,uL',uR') (p, r, UL, next(N) Left adjunction: ( 3( N 6. V N) E r L ' head(r L ) = N 6. Right adjunction: ( 3(N 6. V N) E r R head(r L ') = N*r (uL I\\ UR) E \ufffd tail(r R ) = N*t ) (uL' I\\ UR 1 ) E \ufffd tail(r R ') = N 6. Chart parsing Items are stored in a chart type data structure, indexed by their indices and combined according to their adjacency, which allows the factorization of items of the chart (tabular techniques) . No item can be added that already exists in the chart. We use classical item history that prevents to add in the chart already existing items. The same technique is used to select possible operand items for a rule exploiting mutual exclusion between items corresponding to concurrent lexical hypothesis (same anchors or co-anchors). It allows to obtain a global coherence of an item considering its corresponding tree. We also have used a subsomption test, as described in [Satta and Stock, 1989 ) and suggested for LTAG in [Lavelli and Satta , 199 1) , in order to limit redundant items due to the bidirectional expansion . Parsing is complete when all possible rules have been executed. Coanchors reduction rule \u2022 \u2022 . I ()+-... -+6--0->----\ufffd . q. -----o aR cr L p q (p, q,uL,UR) (q, r,UL 1 ,UR  ementary trees for any kind of tree walk, this FSA does not impose a specific strategy during the parsing and makes it possible to exploit the granularity of connected route . V s s Figure 3 : FSA representing 28 elementary trees corresponding to some intransive and transitive contexts . Figure 3 gives an example of a minimalized FSA which allows to share 28 elementary trees . The number in a state indicates how many trees pass through that state . Table 6 gives the resulting compaction statistics. automaton II no. of trees I no . of states no. of transitions trees per state 245 1 23 21.84 Ta ble 6: An example of Lexicalized Thee Grammar compaction When FS A are shared in a single one, each state contains identifiers of the elementary trees which pass through it and each item the list of elementary tree identifiers valid for the item 's positions. To test conditions of a rule we must consider every possible transitions paths according to connected routes and the shared FSA. The resulting item of a rule is valid for a subset of identifiers of the elementary trees passing throught the both position states . The \"uncompaction\" can be done when we enumerate the derivations . Implementation and results The algorithm has been implemented in Java and is integrated in a graphical environment system dedicated to grammar design and parsing tests . The parsing workbench allows to test a grammar on corpus and to compare different parsing algorithms for LTA G. The connection driven parsing algorithm and the graphical workbench will be freely avalaible with an open-source licence by the end of 1999. The connection driven parser includes derivation enumeration from the shared parse forest, features unification but not yet sharing of automata. We present in table 7 statistics for the chart parsing of a corpora of 86 1 utterances in French of transcripted spontaneous spoken language with a Sun Ultra 1. We compared two sets of rules: One for the bottum-up connected driven parsing and one for a generalized CKY algorithm for LTAG with predictions as suggested by [Vijay-Shanker and Weir, 1993] , both on the same data, with the same 2.06 Table 7: Global results of the complete parsing A partial result corresponds here to the maximal extension of an island, so to an item which is not the origin of any other item. Table 8 shows that the average length of partial recognized substrings is higher with our techniques than with this other bottum-up strategy, preserving a running time better than the predictive CKY. This result means that our algorithm has checked more possible attachments and has delivered more extented partial results. Table 8: Comparaison between bottum-up parsers for partial parses The difference between these two running times can be explained by the consideration of connected routes which results in less items to tabulate and by the initialization of foot node. In a predictive CKY-like algorithm for LTAG, the foot nodes are initialized each time that an internal node (with the same category label) is recognized independently to other positions in the auxiliary trees they belong to. This initialization is more determinist in our algorithm since a foot node is initialized when a left or right extension has reached this node. Conclusion We have presented a new tabular algorithm dedicated to LTG which really takes into account the fact that we manipulate partial parsing trees and not CF rules. Lexicalized partial parsing trees are richer structures than CF rules and allow to obtain more extended partial results which are relevant for natural language robust parsing. A complementary compaction of the LTG allows the factorization of sub-structures parsing. This algorithm tries to compromise a chunk parsing and the complete parsing of an utterance, it satisfies the following properties at any moment: \u2022 The maximal extension of islands according to adjacency and local constraints located on the left and right connected route of this island. \u2022 The global correction of each partial results. The theorical worst case complexity is in O(N 2 G 2 n 6 ) , but we argue that in practice, when dealing with natural language, decreasing the average complexity is more important than considering a worst case complexity which conditions almost never happen. When no connected parse can span the whole sentence , our algorithm results not only in more extended partial derivation trees but also consists in representations of islands and both their right and left connected routes. We can then exploit these results for parsing repairs: The adjacency of two islands which can not be combined according to a given LTAG often lo calizes an agrammaticality. The corresponding chart items can trigger additional repairing ru les and flexible controls in a two-step strategy as shown in [Lopez , 1999b] . Introduction Parsing algorithms for contex-free grammars (CFGs) are generally recognized as the backbone of virtually all approaches to parsing natural-language. Even in systems that use a grammar formalism more complex than CFGs (e.g., unification grammar), the parsing method is usually an extension of one of the well-known CFG parsing algorithms. Moreover, recent developments have once again made direct parsing of CFGs more relevant to natural-language processing, including the recent explosion of interest in parsing with stochastic CFGs or related formalisms, and the fact that commercial speech recognition systems are now available (from Nuance Communications and Microsoft) that accept CFGs as language models for constraining recognition. These applications of context-free parsing share the common trait that the grammars involved can be expected to be very large. A \"tree bank grammar\" extracted from the sections of the Penn Thee bank commonly used for training stochastic parsers contains over 15,000 rules, and we also have a CFG containing over 24,000 rules, compiled from a task-specific unification grammar for use as a speech recognition language model. Grammars such as these stress established approaches to context-free parsing in ways and to an extent not encountered with smaller grammars. In this work we develop an improved form of left-corner chart parsing for large context-free gram mars. We introduce improvements that result in speed-ups averaging 38% or more compared to previously-known variants of left-corner parsing. We also compare our method to several other major parsing approaches: Cocke-Kasami-Younger (CKY), Earley/Graham-Harrison-Ruzzo (E/GHR), and generalized LR (GLR) parsing. Our improved left-corner parsing method outperforms each of these by an average of at least 50%. Finally, we also describe a new technique for minimizing the extra information needed to efficiently recover parses from the data structures built in the course of parsing. Evaluating Parsing Algorithms In this work we are interested in algorithms for finding all possible parses for a given input. We measure the efficiency of the algorithms in building a complete chart ( or comparable structure) for the input, where the chart includes information sufficient to recover every parse without additional searching . 1  We take CPU time to be the primary measure of performance . Implementation-independent measures, such as number of chart edges generated, are sometimes preferred in order to factor out the effects of different platforms and implementation methods, but only time measurement provides a practical way of evaluating some algorithmic details. Fo r example, one of our major improvements to left-corner parsing simply transposes the order of performing two independent filtering checks, resulting in speed ups of up to 67%, while producing exactly the same chart edges as the previous method. To ensure comparability of time measurements, we have re-implemented all the algorithms considered, in Perl 5, 2 on as similar a basis as possible . One caveat about this evaluation should be noted. None of the algorithms were implemented with general support for empty categories, due to the fact that none of the large, independently motivated grammars we had access to coritained empty categories. We did, however make use of a grammar transformation (left factoring) that can produce empty categories, but only as the right-most daughter of a rule with at least two daughters. Fo r the algorithms we wanted to test with this form of grammar, we added limited support for empty categories specifically in this position . Terminology and Notation Nonterminals, which we will sometimes refer to as categories, will be designated by \"low order\" upper-case letters (A, B, etc .) ; and terminals will be designated by lower-case letters. We will use the notation ai to indicate the ith terminal symbol in the input string . We will use \"high order\" upper case letters (X , Y , Z) to denote single symbols that could be either terminals or nonterminals, and Greek letters to denote (possibly empty) sequences of terminals and/or nonterminals. Fo r a grammar rule A \u2794 B1 ... Bn we will refer to A as the mother of the rule and to B1 ... B n as the daughters of the rule. We will assume that there is a single nonterminal category S that subsumes all sentences allowed by the grammar. All the algorithms considered here build a collection of data structures representing segments of the input partially or\u2022 completely analyzed as a phrase of some category in the grammar, which we will refer to as a \"chart\". We will use the term \"item\" to mean an instance of a grammar rule with an indication of how many of the daughters have been recognized in the input. Items will be represented as dotted rules, such as A \u2794 B1 .B 2 \u2022 An \"incomplete item\" will be an item with at least one daughter to the right of the dot, indicating that at least one more daughter remains to be recognized before the entire rule is matched ; and a \"complete item\" will be an item with no daughters to the right of the dot, indicating that the entire rule has been matched . We will use the terms \"incomplete edge\" or \"complete edge\" to mean an incomplete item or complete item, plus two input positions indicating the segment of the input covered by the daughters that have already been recognized. These will be written as (e.g.) (A \u2794 B1 B2 .B3 , i,j), which would mean that the sequence B1 B2 has been recognized starting at position i and ending at position j, and has been hypothesized as part of a longer sequence ending in B3 , which is classified a phrase of category A. Positions in the input will be numbered starting at 0, so the ith terminal of an input string spans position i-1 to i. We will refer to items and edges none of whose daughters have yet been recognized as \"initial\" . Test Grammars For testing context-free parsing algorithms, we have selected three CFGs that are independently mo tivated by analyses of natural-language corpora or actual applications of natural language processing. The CT grammar 3 was compiled into a CFG from a task-specific unification grammar written for CommandTalk (Moore et al., 1997) , a spoken-language interface to a military simulation system. The ATIS grammar was extracted from an internally generated treebank of the DARPA ATIS3 training sentences. The PT grammar was extracted from the Penn 'Ireebank. 4 We employ a standard test set for each of the three grammars. The test set for the CT grammar is a set of sentences made up by the system designers to test the functionality of the system, and the test set for the ATIS grammar is a randomly selected subset of the DARPA ATIS3 development test set. The test set for the PT grammar is a set of sentences randomly generated from a probabilistic version of the grammar, with the probabilities based on the frequency of the bracketings occuring in the training data, and then filtered for length to make it possible to conduct experiments in a reasonable amount of time, given the high degree of ambiguity of the grammar. The terminals of the grammars are preterminal lexical categories rather than words. Preterminals were generated automatically, by grouping together all the words that could occur in exactly the same contexts in all grammar rules, to eliminate lexical ambiguity. 1 . Note that for the CT and ATIS sets, not all sentences are within the corresponding grammars. The most striking difference among the three grammars is the degree of ambiguity. The CT grammar has relatively low ambiguity, the ATIS grammar may be considered highly ambiguous, and the PT grammar can only be called massively ambiguous. Left-Corner Parsing Algorithms and Refinements Left-corner (LC) parsing-more specifically, left-corner parsing with top-down filtering-originated as a method for deterministically parsing a restricted class of CFGs. It is often attributed to Rosenkrantz and Lewis (1970) , who may have first used the term \"left-corner parsing\" in print . Griffiths and Petrick (1965) , however, previously described an LC parsing algorithm under the name \"selective bottom-to top\" (SBT) parsing, which they assert to be an abstraction of previously described algorithms. The origins of LC parsing for general CFGs (other than by naive backtracking) are even murkier. Pratt's (1Q75) algorithm is sometimes considered to be a generalized LC method, but it is perhaps better described as CKY parsing with top-down filtering added. Kay's (1980) method for undirected bottom-up chart parsing is clearly left-corner parsing without top-down filtering, but in adding top down filtering to obtain directed bottom-up chart parsing , he changed the method significantly. The BUP parser of Matsumoto et al. (1983) appears to be the first clearly described LC parser capable of parsing general CFGs in polynomial time. 5  LC parsing depends on the left-corner relation for the grammar, where X is recursively defined to be a left corner of A if X = A, or the grammar contains a rule of the form B --+ X a, where B is a left corner of A. This relation is normally precompiled and indexed so that any pair of symbols can be checked in essentially constant time. Although LC parsing was originally defined as a stack-based method, implementing it _in terms of a chart enables polynomial time complexity to be achieved by the use of dynamic programming; which simply means that if the same chart edge is derived in more than one way, only one copy is retained for further processing. A chart-based LC parsing algorithm can be defined by the following set of rules for populating the chart: 1. Fo r every grammar rule with S as its mother, S --+ a, add (S --+ .a, 0, 0) to the chart. 2 . Fo r every pair of edges of the form (A --+ a.X/3, i , k) and (X \u2794 ,., k,j) in the chart , add (A \u2794 aX./3, i, j) to the chart. 3. Fo r every edge of the form (A \u2794 a.ai/ 3, i, k) in the chart, where ai is the ith terminal in the input , add (A \u2794 o.ai-(3, i, j) to the chart. 4 . Fo r every pair of edges of the form (A \u2794 a.C/3, i, k) and (X \u2794 ,., k,j) in the chart and every grammar rule with X as its left-most daughter, of the form B \u2794 X8, if B is a left corner of C, add (B \u2794 X.8, k,j) to the chart. Fo r every edge of the form (A --+ a.C/3, i, k), where ai is the ith terminal in the input , and every grammar rule with ai as its left-most daughter, of the form B \u2794 ai8, if Bi s a left corner of C, add (B --+ X.8, k,j) to the chart. An input string is successfully parsed as a sentence if the chart contains an edge of the form (S \u2794 a., 0, n) when the algorithm terminates. Rules 1-3 are shared with other parsing algorithms, notably E/GHR, but rules 4 and 5 are distinctive to LC parsing. If naively implemented , however, they can lead to unnecessary duplication of work. Rules 4 and 5 state that for every triple consisting of an incomplete edge, a complete edge or input terminal, and a grammar rule, meeting certain conditions, a new edge should be added to the chart . Inspection reveals, however, that the form of the edge to be added depends on only the complete edge or input terminal and the grammar rule, not the incomplete edge. Thus if this parsing rule is applied separately for each triple, the same new edge may be proposed repeatedly if several incomplete edges combine with a given complete edge or input terminal and grammar rule to form triples satisfying the required conditions. A number of implementations of generalized LC parsing have suffered from this problem, including the BUP parser, the left-corner parser of the SRI Core Language Engine (Moore and Alshawi, 199 1), and Schabes's (199 1) table-driven predictive shift-reduce parser. However, if parsing is performed strictly left-to-right, so that every incomplete edge ending at k has already been computed before any left-corner checks are performed for new edges proposed from complete edges or input terminals starting at k, there is a solution that can be seen by rephrasing rules 4 and 5 follows: 4a. Fo r every edge of the form (X \u2794 ,., k, j) in the chart and every grammar rule with X as its left-most daughter, of the form B \u2794 X 8 , if there is an incomplete edge in the chart ending at k, (A \u2794 a. This formulation suggests driving the parser by proposing a new edge from every grammar rule exactly once for each complete edge or input terminal corresponding to the rule's left-most daughter, and then checking whether some previous incomplete edge licenses it via left-corner filtering. If implemented by nested iteration, this still requires as many nested loops as the naive method; but the inner-most loop does much less work, and it can be aborted as soon as one previous incomplete edge has been found to satisfy the left-corner check . Wiren (1987) seems to have been the first to explicitly propose performing left-corner filtering in an LC parser in this way. Nederhof (1993) proposes essentially the same solution, but formulated in terms of a graph-structured stack of the sort generally associated with GLR parsing. Several additional optimizations can be added to this basic schema. Wiren adds bottom-up filtering (Wiren uses the term \"selectivity\", following Griffiths and Petrick (1965) ) of incomplete edges based on the next terminal in the input . That is, no incomplete edge of the form (A \u2794 a..X/3, i, k) is added to the chart unless a k is a left corner of X. Nederhof proposes that, rather than iterate over all the incomplete edges ending at k each time a left-corner check is performed, compute just once for each input position a set of nonterminal predictions, consisting of the symbols immediately to the right of the dot in the incomplete edges ending at that position, and iterate over that set for each left-corner check at the position. 6 With this optimization, it is no longer necessary to add initial edges to the chart at position O for rules of the form S \u2794 a.. If Pi denotes the set of predictions for position i, we simply let Po = { S} . Another optimization from the recent literature is due to Leermakers (1992) , who observes that in Earley's algorithm the daughters to the left of the dot in an item play no role in the parsing algorithm ; thus the representation of items can ignore the daughters to the left of the dot, resulting in fewer distinct edges to be considered . This observation is equally true for LC parsing. Thus, instead of A \u2794 B 1 B 2 .B 3 , we will write simply A \u2794 .B 3 \u2022 Note that with this optimization, A \u2794 . becomes the notation for an item all of whose daughters have been recognized; the only information it contains being just the mother of the rule. We will therefore write complete edges simply as (A, i, j), rather than (A \u2794 ., i,j). We can also unify the treatment of terminal symbols in the input with complete edges in the chart by adding a complete edge (ai, i -1, i) to the chart for every input terminal ai. 7   Taking all these optimizations together, we can define an optimized LC parsing algorithm by the following set of parsing rules: 1. Let Po = {S}. 2. For every input position j > 0, let Pj = {B I there is an incomplete edge in the chart ending at j, of the form (A \u2794 .Ba, i,j)}. 3 . For every input terminal ai , add (ai, i -1, i) to the chart. 4 . For every pair of edges (A \u2794 .XYa, i, k) and (X, k, j) in the chart, if ai is a left corner of Y, add (A \u2794 .Ya, i, j) to the chart. 5 . For every pair of edges (A \u2794 .X, i, k) and (X, k, j) in the chart, add (A, i, j) to the chart. 6 . For every edge (X, k,j) in the chart and every grammar rule with X as its left-most daughter, of the form A \u2794 XY a, if there is a B E Pk such that A is a left corner of B, and ai is a left corner of Y, add (A \u2794 .Ya, k, j) to the chart. 7. For every edge (X, k, j) in the chart and every grammar rule with X as its only daughter, of the form A \u2794 X, if there is a BE Pk such that A is a left corner of B, add (A, k, j) to the chart. Note that in Rule 6, the top-down left-corner check on the mother of the proposed incomplete edge and the bottom-up left-corner check on the symbol immediately to the right of the dot in the proposed incomplete edge are independent of each other, and therefore could be performed in either order. Wiren, the only author we have found who includes both, is vague on the ordering of these checks. For each proposed edge, however, the bottom-up check requires examining an entry in the left-corner table for each of the elements of the prediction list, until a check succeeds or the list is exhausted; while the bottom up check requires examining only a single entry in the left-corner table for the next terminal of the input. It therefore seems likely to be more efficient to do the bottom-up check before the top-down check, since the top-down check need not be performed if the bottom-up check fails. To test this hypothesis, we have done two implementations of the algorithm: LC1 , which performs the top-down check first, and LC 2 , which performs the bottom-up check first. Shann (1991) uses a different method of top-down filtering in an LC parser. Shann expands the list of predictions created by rules 1 and 2 to include all the left-corners of the predictions. He does this by precomputing the proper left corners of all nonterminal categories and adding to the list of predictions all the left-corners of the original members of the list. Then top-down filtering consists of simply checking whether the mother of a proposed incomplete edge is on the corresponding prediction list. Graham, Harrison, and Ruzzo (1980) attribute this type of top-down filtering to Cocke and Schwartz, so we will refer to it as \"Cocke-Schwartz filtering\" . Since our original form of filtering uses the left-corner relation directly, we will call it \"left-corner filtering\" . We have implemented Cocke-Schwartz filtering as described by Shann, except that for efficiency in both forming and checking the sets of predictions, we use hash tables rather than lists. The resulting algorithm, which we will call LC 3 , can be stated as follows: 1. Let Po = { all left corners of S}. 8   2. For every input position j > 0, let P i = { all left corners of B I there is an incomplete edge in the chart ending at j , of the form (A \u2794 .Bo:,i, j)}. , i -1, i ) to the chart. For every input terminal ai , add (ai For every pair of edges (A \u2794 .XYa, i, k) and (X, k,j) in the chart, if ai is a left corner of Y, add (A \u2794 .Ya, i, j) to the chart. A, i,j ) to the chart. 6 . For every edge (X, k,j) in the chart and every grammar rule with X as its left-most daughter, of the form A \u2794 XYa, if A E Pk , and ai is a left corner of Y, add (A \u2794 .Ya,k, j) to the chart. 7 . For every edge (X, k,j) in the chart and every grammar rule with X as its only daughter, of the form A \u2794 X, if A E Pk , add (A, k,j) to the chart. For every pair of edges (A \u2794 .X, i, k) and (X, k,j) in the chart, add ( There is one simple refinement, not mentioned by Shann, that we can add to this algorithm. Since we already have the information needed to perform bottom-up filtering, we can apply bottom-up filtering to building the prediction sets, omitting any left-corner of an existing prediction that is incompatible with the next terminal of the input. This will certainly save space, and may save time as well, depending on the relative costs of adding a nonterminal to the prediction set compared to performing the bottom-up left-corner check. Our modification of LC parsing with Cocke-Schwartz filtering to include this refinement is implemented as LC 4 . The results of running algorithms LC 1 -LC 4 appear in Table 2 . The numbers are CPU time in seconds required by the parser to completely process the standard test set associated with each grammar. 9 LC2 , which performs the bottom-up left-corner check on proposed incomplete edges before top-down left-corner check, is faster on all three grammars than LC1 , which performs the checks in the reverse order-substantially so on the CT and ATIS grammars. Comparing LC 3 with LC 4which both use Cocke-Schwartz filtering, but differ as to whether the prediction sets are bottom-up filtered-the results are less clear. LC 4 , which does filter the predictions, is noticably faster on the CT grammar, while LC 3 which does not filter predictions is slightly faster, but not significanly so, on the ATIS grammar and PT grammar. Finally, both parsers that use Cocke-Schwartz filtering are faster on all grammars than either of the parsers that use left-corner filtering. CT Grammar ATIS Grammar PT Grammar Transformations One other issue remains to be addressed in our examination of LC parsing. It is a common observation about left-to-right parsing, that if two grammar rules share a common left prefix, e.g., A \u2794 BC and A \u2794 BD, many parsing algorithms will duplicate work for the two rules until reaching the point where they differ. A simple solution often proposed to address the problem is to \"left factor\" the grammar. Left factoring applies the following grammar transformation repeatedly, until it is no longer applicable: For each nonterminal A, let a be the longest nonempty sequence such that there is more than one grammar rule of the form A \u2794 af3. Replace the set of rules A \u2794 a /3 1 , ... , A \u2794 af3 n with A \u2794 aA', A' \u2794 /3 1 , ... , A' \u2794 f3 n , where A' is a new nonterminal symbol. Left factoring has been explored in the context of generalized LC parsing by Nederhof (1994), who refers to LC parsing with left factoring as PLR parsing. Shann (1991) also applies left factoring directly in the representation of the rules he rises in his LC parser, e.g. A \u2794 B(C, D) . One complication associated with left factoring is that if the daughters of one rule are a proper prefix of the daughters of another rule, then empty rules will be introduced into the grammar, even if there were none originally. For example A \u2794 BC and A \u2794 BCD will be replaced by A \u2794 BC A' , A' \u2794  D, A' \u2794 f The requirement that (3 always be nonempty blocks the introduction of empty productions, so with this transformation A \u2794 BC and A \u2794 BCD will be replaced by A \u2794 BA',A' \u2794 CD,A' \u2794 C. Left factoring is not the only transformation that can be used to address the problem of common rule prefixes. Left factoring applies only to sets of rules with a common mother category, but as an essentially bottom-up method, generalized LC parsing does most of its work before the mother of a rule is determined. There is another grammar transformation that seems better suited to LC parsing, introduced by Griffiths and Petrick (1965), but apparently neglected since: For each grammar symbol X, let a be the longest nonempty sequence such that there is more than one grammar rule of the form A \u2794 X af3. Replace the set of rules A 1 \u2794 X a/31 , ... , A n \u2794 X af3n with A' \u2794 X a, A1 \u2794 A' /31 , ... , A n \u2794 A' f3 n , where A' is a new nonterminal symbol. Like left factoring, this transformation is repeated until it is no longer applicable. Griffiths and Petrick do not give this transformation a name, so we will call it \"bottom-up prefix merging\" . It should be noted that all of these grammar transformations simply add additional levels of non terminals to the grammar, without otherwise disturbing the structure of the analyses produced by the grammar. Thus, when parsing with a grammar produced by one of these transformations, the original analyses can be recovered simply by ignoring the newly introduced nonterminals, and treating their subconstituents as subconstituents of the next higher original nonterminal of the grammar. Before we apply our LC parsers to our test grammars transformed in these three ways, we make a few small adjustments to the implementations. First, as \u2022 noted above, full left factoring requires the ability to handle empty categories, at least as the right-most daughter of a rule. We have created modified versions of LC 1 -LC 4 specifically to use with the fully left-factored grammar. Second we note that with a left-factored grammar, 1 0 the non-unary rules have the property that, given the mother and the left-most daughter, there is only one possibility for the rest of the rule. With a bottom-up prefix merged grammar, the non-unary rules have the property that, given the two left-most daughters, there is only one possibility for the rest of the rule. We take advantage of these facts to store the indexed forms of the rules more compactly and simplify the logic of the implementations of variants of our parsers specialized to these grammar forms. The results of applying our four LC parsing algorithms with these three grammar transformations are displayed in Table 3 , along with results for the untransformed grammars presented previously. The grammar transformations are desginated by the symbols UTF (untransformed), FLF (fully left factored), PLF (partially left-factored), and BUPM (bottom-up prefix-merged). We set a time-out of 10 minutes on some experiments, since that was already an order of magnitude longer than any of the other parse times. Several observations stand out from these results. First, in every case but one, partial left factoring out-performed full left factoring. Much more surprising is that, in every case but one, either form of left factoring degraded parsing performance relative to the untransformed grammar. For LC 1 and LC2 , the algorithms that use left-corner filtering, the degradation is dramatic, while for LC3 and LC 4 , which use Cocke-Schwartz filtering, the degradation is very slight in the case of the ATIS and PT grammars, but more pronounced in the case of the CT grammar. On the other hand, bottom-up prefix-merging significantly-in some cases dramatically-speeds up parsing for LC 1 and LC2 , while significantly degrading the performance of LC 3 and LC 4 . CT Grammar ATIS Grammar PT Looking at the overall results of these experiments, we see that bottom-up prefix merging reverses the previous advantage of Cocke-Schwartz filtering over left-corner filtering. With bottom-up prefix merging, LC2 is at least 66% faster on the ATIS grammar and 55% faster on the PT grammar than either LC 3 or LC4 ; and it is only 15% slower than LC 4 on the CT grammar, and the same speed as LC3 . Averaging over the three test grammars, LC 2 is 40% faster than LC 3 and 38% faster than LC 4 . Extracting Parses from the Chart The Leermakers optimization of omitting recognized daughters from items raises the question of how parses are to be extracted from the chart. The daughters to the left of the dot in an item are often used for this purpose in item-based methods, including Earley's original algorithm. Graham, Harrison, and Ruzzo (1980), however, suggest storing with each noninitial edge in the chart a list that includes, for each derivation of the edge, a pair of pointers to the preceding edges that caused it to be derived. This provides sufficient information to extract the parses without additional searching, even without the daughters to the left of the dot. In fact, we can do even better than this. For each derivation of a noninitial edge, even in the Leermakers representation, it is sufficient to attach to the edge only the mother category and starting position of the complete edge that was used in the last step of the derivation. Every noninitial edge is derived by combining a complete edge with an incomplete edge. Suppose (A \u2794 ./3, k, j) is a derived edge, and we know that the complete edge used in the derivation had category X and start position i. We then know that the complete edge must have been (X, i, j), since the complete edge and the derived edge must have the same end position. We further know that the incomplete edge used in the derivation must have been (A \u2794 .X/3, k, i), since that is the only incomplete edge that could have combined with the complete edge to produce the derived edge. In this way, for any complete edge, we can trace back through the chart until we have found all the complete edges for the daughters that derived it. The back-trace terminates when we reach an incomplete edge that has the same start point as the complete edge it was derived from. Comparison to Other Algorithms We have compared our LC parsers to efficient implementations of three other important approaches to context-free parsing : Cocke-Kasami-Younger (CKY), Earley/Graham-Harrison-Ruzzo (E/GHR), and generalized LR. (GLR) parsing. We include CKY, not because we think it may be the fastest parsing algorithm, but because it provides a baseline of how well one can do with no top-down filtering. Our implementation of E/GHR includes many optimizations not found in the original descriptions of this approach, including the techniques used to optimize our LC parsers, where applicable. In our GLR parser we used the same reduction method as Tomita's (1985) original parser, which results in greater-than-cubic worst-case time complexity, after verifying that a cubic-time version was, in fact, slower in practice, as Tomita has asserted. 4 shows the comparison between these three algorithms, and our best overall LC algorithm. CT Grammar ATIS Grammar 180 As the table shows, LC2+B UP M outperforms all of the other algorithms with all three grammars . While each of the other algorithms approaches our LC parser in at least one of the tests, the LC parser outperforms each of the others by at least a fac tor of 2 with at least one of the grammars . The comparison between LC 2 +B UPM and GLR is instructive in view of the claims that have been made for GLR. While GLR(O) was essentially equal in performance to LC2+B UP M on the least ambiguous grammar, it appears to scale very badly wi th increasing ambiguity. Moreover, the parsing tables required by the GLR parser are far larger than for LC2+B UP M. Fo r the CT grammar, LC 2 +B UP M requires 27,783 rules in the transformed grammar, plus 210,701 entries in the left-corner table. Fo r the (original) CT grammar, GLR requires 1,455,918 entries in the LR(O) parsing tables . The second part of Table 4 shows comparisons of LC 2 +B UP M and two versions of GLR with look ahead . The \"LC+follow\" line is for LC2+B UP M plus an additional filter on complete edges using a \"follow check\" equivalent to the look ahead used by SLR(l) parsing . The \"GLR(O)+follow\" line adds the same follow check to the GLR(O) parser. This builds exactly the same edges as a GSLR(l) parser would, but allows smaller parsing tables at the expense of more table look ups .11 With the follow check, the parse times for the CT grammar are substantially reduced, but LC 2 +B UP M and GLR remain essentially equivalent, while only small changes are produced for the AT IS and PT grammars . The final line gives results for GLALR( l) parsing with the CT and AT IS grammars . 12 These results are not directly comparable to the others because the LALR( l) reduce tables for the CT and AT IS grammars contained more than 6.1 million and 1.8 million entries, respectively, and they would not fit in the memory of the test machine along with the other LR tables . Various methods were investigated to obtain timing results by loading only a subset of the reduce tables sufficient to handle the test set. These gave inconsistent results, but in all cases times were longer than for either GLR(O) or GLR(O)+follow, presumably due to additional overhead caused by the large tables, with relatively little additional filtering (5-6% fewer edges) . The numbers in the table represent the best results obtained for each grammar. Conclusions Probably the two most significant results of this investigation are the discoveries that: \u2022 LC chart parsing incorporating both a top-down left-corner check on the mother of a proposed incomplete edge and a bottom-up left-corner check on the symbol immediately to the right of the dot in the proposed incomplete edge is substantially fas ter if the bottom-up check is performed before the top-down check . \u2022 Bottom-up prefix merging is a particularly good match to LC chart parsing based on left-corner filtering, and in fac t substantially out performs left fac toring combined with LC chart parsing in most circumstances. Moreover we have shown that wi th these enhancements, LC parsing outperforms several other major approaches to context-free parsing, including some previously claimed to be the best general context free parsing method. We conclude that our improved form of LC parsing may now be the leading contender for that title. Measure For Abstract Over the past few years significant progress was accomplished in efficient processing with wide-coverage ( HPSG grammars. HPSG-based parsing systems are now available that can process medium-complexity sentences (of ten to twenty words, say) in average parse times equivalent to real (i.e. human reading) time. A large number of engineering improvements in current HPSG systems were achieved through collaboration of multiple research centers and mutual exchange of experience, encoding techniques, algorithms, and even pieces of software. This article presents an approach to grammar and system engineering, termed competence & performance profiling, that makes systematic experimentation and the precise empirical study of system properties a focal point in development. Adapting the profiling metaphor familiar from software engineering to constraint-based grammars and parsers, enables developers to maintain an accurate record of system evolution, identify grammar and system deficiencies quickly, and compare to earlier versions or between different systems. We discuss a number of exemplary problems that motivate the experimental approach, and apply the empirical methodology in a fairly detailed discussion of what was achieved during a development period of three years. Given the collaborative nature in setup, the empirical results we present involve research and achievements of a large group of people. Dramatis Personre *This play builds heavily on results obtained through collaboration between a largish group of people at several sites; Act 5 presents a list of individuals who have contributed to the development providing the background for our current discussion. Technical details and empirical assessments of individual techniques will be documented in a forthcoming Special Issue of the Journal of Natural Language Engineering (Flickinger, Oepen, Uszkoreit, & Tsujii, 2000). 1 See 'http ://wvv .dfki .de/lt/' and 'http ://WVll.coli .uni-sb .de/' for information on the DFKI Language Tech nology Laboratory and the Computational Linguistics Department at Saarland University, respectively. Although the individual systems often supply extra functionality, the groups have converged on a common descriptive formalism -a conservative blend of Carpenter (1992) , Copestake (1992), and Krieger and Schafer (1994) -that allows grammars 4 a to be processed by five different platforms. The LinGO grammar, a multi-purpose, broad-coverage grammar of English developed at CSLI and among the largest HPSG implementations currently available, serves as a common reference for all three groups ( while of course the sites continue development of additional grammars for English, German, and Japanese) . With one hundred thousand lines of source, roughly eight thousand types, an average feature structure size of some three hundred nodes, twenty seven lexical and thirty seven phrase structure rules, and some six thousand lexical (stem) entries, the LinGO grammar presents a fine challenge for processing systems. While scaling the systems to this rich set of constraints and improving processing and constraint resolution algorithms, the groups have regularly exchanged benchmarking results, in particular at the level of individual components, and discussed benefits and disadvantages of particular encodings and algorithms. Precise comparison was found essential in this process and has facilitated a degree of cross-fertilization that proved beneficial for all participants. Act 1 below introduces the profiling methodology, supporting tools, and the sets of common reference data and benchmarking metrics that were used among the groups. By way of example, the profiling metaphor is then applied in Act 2 to a choice of engineering issues that (currently) can only be approached empirically. Act 3 introduces the PET platform (Callmeier, 2000) as another actor in the experimental setup; PET synthesizes a variety of techniques from the individual systems in a fresh, modular, and highly parameterizable reimplementation. Finally, on the basis of empirical data obtained with PET , Act 4 provides a detailed comparison of competence and performance profiles obtained in October 1996 with the current development status (as of October 1999). Competence & Performance Profiling In system development and optimization, subtle algorithmic and impleme:ntational decisions often have a significant impact on system performance, so monitoring system evolution very closely is cru cial. Developers should be enabled to obtain a precise record of the status of the system at any given point; also, comparison with earlier results, between various parameter settings, and across platforms should be automated and integrated with the regular development cycle. System performance, how ever, cannot be adequately characterized merely by measurements of overall processing time ( and perhaps memory usage). Properties of (i) individual modules (in a classical setup, especially the unifier, type system, and parser) , (ii) the grammar being used, and (iii) the input presented to the system all interact in complex ways. In order to obtain an analytical understanding of strengths and weaknesses of a particular configuration, finer-grained records are required. By the same token, developer intuition and isolated case studies are often insufficient, since in practice, people who have worked on a particular system or grammar for years still find that an intuitive prediction of system behaviour can be incomplete or plainly wrong. Although most grammar development environments supply facilities to batch-process a test corpus and record the results produced by the system, these are typically restricted to processing a flat, un structured input file (listing test sentences, one per line) and outputting a small number of processing results into a log file. 5 In total, we note a striking methodological and technological deficit in the area readings number of complete analyses obtained ( when applicable, after unpacking) filter percentage of parser actions predicted to fail (rule filter plus 'quick check ') etasks number of attempts to instantiate an argument position in a rule stasks number of successful instantiations of argument positions in rules aedges number of active edges built by the parser ( where appropriate) pedges number of passive edges built by the parser (typically in all-paths search) unifications number of top-level calls into the fea ture structure unification routine copies number of top-level feature structure copies made tcpu amount of cpu time (in milliseconds) spent in processing space amount of dynamic memory allocated during processing (in bytes) Ta ble 1: Some of the parameters making up a competence & performance profile. of precise and systematic, let alone comparable, assessment of grammar and system behaviour . Oepen and Flickinger (1998) propose a methodology, termed grammar profi ling, that builds on structured and annotated collections of test and reference data ( traditionally known as test suites) . The competence & performance profiling approach we advocate in this play can be viewed as a gener alization of this methodology -in line with the experimental paradigm suggested by, among others, Erbach (199 1a) and Carroll (1994) . A competence & perfor mance profile is defined as a rich, pre cise, and structured snapshot of system behaviour at a given development point. The production, maintenance, and inspection of profiles is supported by a specialized software package ( called [incr tsdb(}] 6 ) that supplies a uniform data model, an application program interface to the grammar-based processing components, and graphical facilities for profile analysis and comparison . Profiles are stored in a relational database which accumulates a precise record of system evolution, and which serves as the basis for flexible report generation, visualization, and data analysis via basic descriptive statistics . All tables and figures used in this play were generated using [incr tsdb(}]. The profiling environment defines a common set of descriptive metrics which aim both for in-depth precision and also for sufficient generality across a va riety of processing systems. Most parameters are optional, though analysis potential may be restricted for partial profiles. Roughly, profile contents can be classified into information on (i) the processing environment (grammar, platform, versions, parameter settings and others), (ii) grammatical coverage (number of analyses, derivation and parse trees per reading, corresponding semantic formulae), (iii) ambiguity measures (lexical items retrieved, number of active and passive edges, where applicable, both globally and per result), (iv) resource con sumption (various timings, memory allocation), and indicators of (v) parser and unifier throughput. Excluding relations and attributes that encode annotations on the input data, the current compe tence & performance database schema includes some one hundred attributes in five relations. Table 1 summarizes some of the profiling parameters as they are relevant to the drama to come . While the current [incr tsdb(}] data model has already been successfully adapted to six different parsing systems (with another two pending; see Act 5), it remains to be seen how well it scales to the description of a larger va riety of processing regimes. And although absolute numbers must be viewed cum grano salis, the common metric has greatly increased comparability and data exchange among the groups mentioned above, and has in some cases also helped to identify unexpected sources of performance va riation . For example, we have found that two Sun UltraSparc servers ( at different sites) with identical hardware configuration ( down to the level of cpu revision) and OS release reproducibly accounting of system results to a small number of parameters ( e.g. number of analyses, overall processing time, memory consumption, possibly the total number of chart edges), and only offer a limited, predefined choice of analysis views. exhibit a performance difference of around ten per cent. This appears to be caused by different installed sets of vendor-supplied operating system patches. Also, average cpu load and availability of main memory have been observed to have a noticeable effect on cpu time measurements; therefore, all data reported in this play, was collected in an ( artificial, in some sense) environment in which sufficient cpu and memory resources were guaranteed throughout each complete test run. Set The [incr tsdb{)] package includes a number of test suites and development corpora for English, German, and French (and has facilities for user-level import of additional test data). For benchmarking purposes with the LinGO grammar four test sets were chosen: (i) the English TSNLP test suite (Oepen, Netter, & Klein, 1997), (ii) the CSLI test suite derived from the original Hewlett-Packard data {Flickinger, Nerbonne, Sag, & Wasow, 1987), (iii) a small collection of transcribed scheduling dialogue utterances collected in the VerbMobil context, and (iv) a larger extract from recent VerbMobil corpora that was selected pseudo-randomly to achieve a balanced distribution of one hundred samples for each input length below twenty words. Some salient properties of these test sets are summarized in Table 2 . 7  Looking at the degrees of lexical (i.e. the ratio between columns five and four), global (column seven), and local ( approximated in column eight by the number of passive edges created in pure bottom up parsing) ambiguity, the three test sets range from very short and unambiguous to mildly long and highly ambiguous. The 'blend' test set is a good indicator of maximal input complexity that the available parsers can currently process (in plausible amounts of time and memory). Contrasting columns six and three (i.e. items accepted by the grammar vs. total numbers of well-or ill-formed items) provides a measure of grammatical coverage and overgeneration, respectively. Strong Empiricism: A Few Examples A fundamental measure in comparing two different versions or configurations of one system as well as for contrasting two distinct systems is correctness and equivalence of results. No matter what unification algorithm or parsing strategy is chosen, parameters like the numbers of lexical items retrieved per input word, total analyses found, passive edges derived (in non-predictive bottom-up parsing, at least) and others should only vary when the grammar itself is changed. Therefore, regular regression testing is required. In debugging and experimentation practice, we have found that minor divergences in results are often hard to identify; using an experimental parsing strategy, for example, over-and undergeneration can even out for the number of readings and even the accounting of passive edges. Hence, assuring an exact match in results {for a given test set) is a non-trivial task. The [incr tsdb()] package eases comparison of results on a per-item basis, using an approach similar to Un * x diff (1), but generalized for structured data sets . By selection of a set of parameters for intersection ( and optionally a comparison predicate), the user interface allows to browse the subset of items that fail to match in the selected properties. One dimension that we found especially useful in intersecting profiles is on the derivation trees ( a bracketed structure labeled with rule names and identifiers of lexical items) associated with each parser analysis . Once a set of missing or extra derivations (representing under-or overgeneration, respectively) between two profiles is identified, they can be fed back into the defective parser as a request to try and reconstruct each derivation. Reconstruction of derivation trees, in a sense, amounts to fully deterministic parsing, and enables the processor to record where the failure occurs that caused undergeneration in the first place ; conversely, when dealing with overgeneration, reconstruction in the correct parser can be requested to identify the missing constraint(s). While these techniques illustrate basic debugging facilities that the profiling and experimentation environment provides, the following two scenes discuss algorithmic issues in parser design and tuning that can only be addressed empirically. Hyper-Active Parsing The two established development platforms, the LKB (CSLI Stanford) and PAGE (DFKI Saarbriicken) systems, have undergone homogenization of approaches and even individual modules ( the conjunctive PAGE unifier, for instance, was developed by Rob Malouf at CSLI Stanford) for quite a while . 8 Until recently, however, the parsing regimes deployed in the two systems were significantly different. Both parsers use quasi-destructive unification, are purely bottom-up, chart-based, perform no ambiguity packing, and can be operated in exhaustive (all paths) or agenda-driven best-first search modes; before any unification is attempted, both parsers apply the same set of pre-unification filters, viz . a test against a rule compatibility table (Kiefer et al., 1999) , and the 'quick check' partial unification test (Malouf, Carroll, & Copestake, 2000). The LKB passive chart parser (in exhaustive mode) uses a breadth-first CKY-like algorithm ; it processes the input string strictly from left to right, constructing all admissible complete constituents whose right vertex is at the current input position before moving on to the next lexical item . Attempts at rule application are made from right to left. All and only complete constituents found (passive edges) are entered in the chart. The active PAGE parser, on the other hand, uses a va riant of the algorithm described by Erbach (1991b). It operates bidirectionally, both in processing the input string and instantiating rules; crucially, the key daughter (see Scene 2.2 below) of each rule is analyzed first, before the other daughter(s) are instantiated . But while the LKB and the PAGE developers both assumed the strategy chosen in their own system was-the best-suited for parsing with large fea ture structures (as exemplified by the LinGO grammar), the choices are motivated by conflicting desiderata . Not storing active edges (as in the passive LKB parser) reduces the amount of feature structure copying but requires frequent recomputation of par tially instantiated rules, in that the unification of a daughter constituent with the rightmost argument position of a rule is performed as many times as the rule is applied to left-adjacent sequences of candidate chart edges . Creating active edges that add partial results to the chart, on the other hand, requires that more feature structure copies are made, which in turn avoids the necessity of redoing unifications . Given the effectiveness of the pre-unification filters it is likely that for some active edges no attempts to extend them with adjacent inactive edges will ever be executed, so that the copy associated with the active edge was wasted effort. Profiling the two parsers individually showed that overall performance is roughly equivalent ( with a minimal lead for the passive LKB parser in both time and space) . While the passive parser executes far more parser tasks (i.e. unifications), it creates significantly fewer copies -as should be expected from what is known about the differences in pars ing strategy. Hence, from a superficial comparison of parser throughput one could conclude that the passive parser successfully trades unifications for copies, and that both basic parsing regimes perform equally well with respect to the LinGO grammar. To obtain fully comparable results, the algorithm used in PAGE was imported into the LKB, which serves as the (single) experimentation environment for the remainder of this scene. The direct com parison is shown in Table 3 for three of the standard test sets. The re-implementation of the active parser in the LKB, in fact, performs slightly better than the passive version and does not allocate very much more space. On the 'aged' test set, the active parser even achieves a modest reduction in memory consumption which most likely reflects the larger proportion of extra unifications compared to the savings in copies ( columns five and six) for this test set. Having profiled the two traditional parsing strategies and dissected each empirically, it now seems natural to synthesize a new algorithm that combines the advantages of both strategies (i.e. reduced unification and reduced copying) . The following algorithm, termed 'hyper-active' by Oepen and Carroll (2000), achieves this goal: \u2022 use the bottom-up, bidirectional, key-driven control strategy of the active parser; \u2022 when an 'active' edge is derived, store this partial analysis in the chart but do not copy the associated feature structure; 9 \u2022 when an 'active' edge is extended (combined with a passive edge), recompute the intermediate feature structure from the original rule and already-instantiated daughter(s); \u2022 only copy feature structures for complete passive edges; partial analyses are represented in the chart but the unification(s) that derived each partial analysis are redone on-demand. Essentially, storing 'active' (or, in a sense, hyper-active) edges without creating expensive feature structure copies enables the parser to perform a key-driven search effectively, and at the same time avoids over-copying for partial analyses; additional unifications are traded for the copies that were avoided only where hyper-active edges are actually extended in later processing.10  Ta ble 3 confirms that hy per-active parsing combines the desirable properties of both basic algo rithms: the number of copies made is exactly the same as for the passive parser, while the number of unifications is only moderately higher than for the active parser ( due to on-demand recomputation of intermediate structures) . Accordingly, average parse times are reduced by twenty six ('csli') and thirty seven ('aged') per cent , while memory consumption drops by twenty seven and thirty two per cent, respectively. Applying the three parsers to the much more challenging 'blend' test set , reveals that the greater search space poses a severe problem for the passive parser, and limits the relative advantages of the hyper-active over the plain active strategy somewhat: while in the latter compari son the amount of copying is reduced by one third in hyper-active parsing , the number of unifications increases by thirty per cent at the same time (but see the discussion of rule instantiation below) . Still , the hyper-active algorithm greatly reduces memory consumption, which by virtue of lower garbage collection times (not included in tcpu va lues) results in a significant overall speed-up. Compared to the original LKB passive parser, hyper-active parsing achieves a time and space reduction of forty three and thirty eight per cent, respectively. Thorough profiling and eclectic engineering have resulted in an im proved parsing algorithm that is now used standardly in both the LKB and PAGE; for the Ger man and Japanese VerbMobil grammars in PAGE, the observed benefits of hyper-active parsing were broadly confirmed. Rule Instantiation Strategies Head-driven approaches to parsing have been explored successfully with lexicalized grammars like HPSG (see va n Noord , 1997, for a recent overview) because, basically, they can avoid proliferation of partial rule instantiations (i.e . active edges in a chart parser) with rules that contain very unspecific argument positions. Many authors either implicitly (Kay, 1989) or explicitly (Bouma & va n Noord, 1993) assume the linguistic head to be the argument position that the parser should instantiate first . However, the right choice of argument position in each rule , such that it best constrains rule applicability ( with respect to all categories derived by the grammar) cannot be determined analytically. Though the selection is likely to be related to the amount and specificity of information encoded for each argument , for some rules a single feature value ( e.g. the [WH +] constraint on the non-head daughter in one of the instantiations of the filler-head schema used in LinGO) can be most im portant . Fo r terminological clarity, PAGE introduces the term key daughter to refer to the argument position in each rule that is the best discriminator with respect to other categories that the grammar derives; at the same . time , the notion of key-driven parsing emphasizes the observation that for individual rules in a particular grammar a non-(linguistic )head daughter may be a better candidate. Figure 1 compares parser performance (using the PET parser; see below) for a rule instantiation strategy that always fills the (linguistic) head daughter first (labelled 'head-driven') with a variant that uses an idiosyncratically chosen key daughter for each rule (termed 'key-driven'; see below for key selection) . The data shows that the number of executed ( et asks) as well as the number of successful (stasks) parser actions increase far more drastically with respect to input length in the head-driven setup (on the 'blend' test suite, truncated above 20 words due to sparse data) . Since parser tasks are directly correlated to overall parser performance, the key-driven strategy on average reduces parsing time by more than a factor of two. Clearly, for the LinGO grammar at least, linguistic headedness is not a good indicator for rule instantiation. Thus, the choice of good parsing keys for a particular grammar is an entirely empirical issu_e. Key daughters, in the current setup, are stipulated by the grammar engineer(s) as annotations to grammar rules; in choosing the key positions, the grammarian builds on knowledge about the grammar and observations from parsing test data. The [incr tsdb()] performance profiling tools can help in this choice since they allow the accounting of active and passive edges to be broken down by individual grammar rules ( as they were instantiated in building edges) . Inspecting the ratio of edges built per rule, for any given choice of parsing keys, can then help to identify rules that generate an unnecessary number of active edges. Thus, in the experimental approach to grammar and system optimization the effects of different key selections can be analyzed precisely and compared to earlier results. 11 Table 4 shows the head and key positions together with the differences in the number of active edges derived (in strict left to right vs. right to left rule instantiation modes) for a subset of binary grammar rules in LinGO. For the majority of head -argument structures (with the notable exception of the subject -head rule) the linguistic head corresponds to the key daughter, in adjunction and (most) filler -head constructions we see the reverse_ image; for some rules, choosing the head daughter as the key can result in an increase of active edges close to two orders of magnitude. Inspecting edge proliferation by individual rules reveals another property of the particular grammar: the ratio of passive to active edges ( column seven in Table 4 , using the key-driven values for aedges) varies drastically. The specifier -head rule, for example, licenses a large number of active edges but, on average, only one out of seven active edges can be completed to yield a passive edge. The head marker rule, on the other hand, on average generates seventy one passive edges from just one active edge. While the former should certainly benefit from hyper-active parsing, this seems very unlikely for the latter; Scene 2.1 above suggests that no more than three unifications should be traded for one copy in the LKB. Therefore, it seems plausible to apply the hyper-active parsing regime selectively to rules with a pedges to aedges ratio below a certain threshold t. We plan to explore this technique in a series of experiments, evaluating parser performance in relation to varied values for t. PET -Synthesizing Current Best Practice PET is a platform to build processing systems based on the descriptive formalism represented by the LinGO grammar. It aims to make experimentation with constraint-based parsers easy, including comparison of existing techniques and evaluating new approaches. Thus, flexibility and extendibility are primary design objectives. Both desiderata are achieved by a tool box approach -PET provides an extendible set of configurable building blocks, that can be combined and configured in different ways to instantiate a concrete processing system. The set of building blocks includes objects like chart, agenda, grammar, type hierarchy, and typed fe ature structure. Using the available objects, a simple bottom-up chart parser, for instance, can be realized in a few lines of code. Alternative implementations of a certain object may be available to allow comparison of different approaches to one aspect of processing in a common context. Fo r example, the current PET en vironment provides a choice of destructive, semi-destructive, and quasi-destructive implementations of the typed fe ature structure object (viz. the algorithms proposed by Wr oblewski (198 7), Ciortuz (2000), To mabechi (1991), and Malouf et al. (2000) ; experimentation with additional algorithms and fixed-arity encodings is under development). In this setup properties of various graph unification al gorithms and feature structure representations can be compared among each other and in interaction with different processing regimes. In a parser called cheap, PET implements all relevant techniques from Kiefer et al. (1999) (i.e. conjunctive-only unification, rule filters, quick-check, restrictors), as well as techniques originally de veloped in other systems (e.g. key-driven parsing from PAGE, caching type unification and hyper-active parsing from the LKB, and partial expansion from CHIC). Re-implementation and strict modularization often resulted in improved representations and algorithmic refinement ; since individual modules can be specialized for a particular task, the overhead often found in monolithic implementations (like slots in internal data structures, say, that are only required in a certain configuration) could be reduced. Efficient memory management and minimizing memory consumption was another important con sideration in the development of PET. Experience with Lisp-based systems suggests that memory throughput is one of the main bottlenecks when processing large grammars. In fac t, one observes a close correlation between the amount of dynamically allocated memory and processing time, indicat ing much time is spent moving data, rather than in actual computation. Using builtin C++ memory management, allocation and release of feature structure nodes can account for up to forty per cent _ of total run time. Like in the WAM (AYt-Kaci, 1991), a general memory allocation scheme allowing arbi trary order of allocation and release of structures is not necessary in this context. Within a larger unit of computation, the application of a rule, say, the parser typically builds up structure monotonically ; memory is only released in the case of a top-level unification failure when all partial structure built during this unification is freed. Therefore, PET employs a simple stack-based memory management strategy, acquiring memory from the operating system in large chunks which are then sub-allocated. A mark -release mechanism allows saving the current allocation state (the current stack position) and returning to that saved state at a later point. Thus, releasing a chunk of objects amounts to a single pointer assignment. Also, feature structure representations are maximally compact. (generated by (incr tsdb{)) at 5-nov-1999 {17:11 h)) Table 5 : Development of grammatical coverage and overgeneration over three years. parsing) this results in very attractive memory consumption characteristics for the cheap parser, al lowing to process the 'blend' test set with a process size of around one hundred megabytes (where Lisp-or Prolog-based implementations easily grow beyond half a gigabyte). To maximize compactness and efficiency, PET is implemented in ANSI C++, but uses traditional C representations (rather than C++ objects) for some central objects where minimal overhead is required (e.g. the basic features structure elements). Quantifying Progress The preceding acts have exemplified the benefits of competence and performance profiling in the application to isolated properties of various parsing algorithms. In this final act we take a wider perspective and use the profiling approach to give an impression of overall progress made in processing the LinGO grammar over a development period of three years . The oldest available profiles (for the 'tsnlp' and 'aged' test sets) were obtained with PAGE (version 2\u20220 released in May 1997) and the October 1996 version of the grammar; the current best parsing performance, to our best knowledge, is achieved in the cheap parser of PET. All data was sampled on the same Sun UltraSparc server (dual 300 Mhz; 1.2 gbytes memory; mildly patched Solaris 2.6) at Saarbriicken. The evolution of grammatical coverage is depicted in Table 5 , contrasting salient properties from the individual competence profiles (see Table 2 ) side by side; to illustrate the use of annotations on the test data, the table is further broken down by selected syntactic phenomena for the TSNLP data (Oepen et al., 1997, give details of the phenomenon classification). Comparison of the lexical and parser averages shows a modest increase in lexical but a dramatic increase in global ambiguity (by close to a factor of three for 'aged'). Columns labeled in and out indicate coverage of items marked wellformed and overgeneration for ill-formed items, respectively. While the 'aged' test set . does not include negative test items, it confirms that coverage within the Ve rbMobil domain has improved. However, the TSNLP test suite is far better suited to gauge development of grammatical coverage, since it was designed to systematically exercise different modules of the grammar. In fact, a net increase in coverage (from sixty five to seventy seven per cent) in conjunction with slightly reduced overgeneration confirms that the LinGO grammar engineers have steadily improved the overall quality of the linguistic resource. The assessment of parser performance shows a more dramatic development. Average parsing times per test item have dropped by more than two orders of magnitude ( a factor of one hundred and fifty on the 'aged' data), while memory consumption was reduced to about two per cent of the original values.  Because in the early PAGE data the 'quick check ' pre-unification filter was not available, current filter rates for PET ( and the other systems alike) are much better and result in a reduction of parser tasks that are actually executed. At the same time, comparing the number of passive edges licensed by the two versions of the grammar, provides a good estimate on the size of the search space processed by the two parsers. Although for the (nearly) ambiguity-free TS N LP test suite the pedges averages are almost stable, the 'aged' data shows an increase by a factor of three. Asserting that the average number of passive edges is a direct measure for input complexity ( with respect to a particular grammar), we extrapolate the overall speed-up in processing the Lin GO grammar as a factor of roughly five hundred (again, tcpu values in Table 6 do not\u2022 include garbage collection for PAGE which in turn is avoided in PET). Finally, Table 6 includes PET results on the currently most challenging 'blend' test set (see above). Despite of greatly increased search space and ambiguity, the cheap parser achieves an average parse time of 650 milliseconds and processes almost ninety per cent of the test items in less than one second. 13 Conclusion, Outlook, and Acknowledgments Precise, in-depth comparison has enabled a large, multi-national group of developers to quantify and exchange algorithmic knowledge and benefit from each others experience. The [incr tsdb()] profiling package has been integrated with six processing environments for ( HPSG-type) unification grammars so far; developers of other systems are encouraged to seek assistance in interfacing to the common metric (connections to the Alvey Tools GDE and Xerox XLE are currently under consideration) -scaling up and generalizing the set of competence and performance parameters is, once more, an empirical challenge. In parallel, the range of experimental choices in PET will be increased, aiming for import (and adaptation) of recent results, especially in the areas of fixed-arity feature structure encodings (inspired by the Tokyo LiLFeS implementation) and ambiguity packing (from the LKB). The cathartic effect achieved in this period of close collaboration between sites over several years would not have been possible without the main characters, researchers, engineers, grammarians, and managers at Saarbriicken, Stanford, Tokyo, and Sussex University. To name a few, John Carroll, Liviu Ciortuz, Ann Copestake, Dan Flickinger, Bernd Kiefer, Hans-Ulrich Krieger, Takaki Makino, Rob Malouf, Yus uke Miyao, Stefan Miiller, Mark-Jan Nederhof, Giinter Neumann, Takashi Ninomiya, Kenji Nishida, Ivan Sag, Kentaro Torisawa, Jun'ichi Tsujii, and Hans Uszkoreit have all greatly contributed to the development of efficient HPSG processors as described above. Many of the individual achievements and results are reflected in the bibliographic references given throughout the play. Introduction Natural languages exhibit a good deal of discontinuous constituency phenomena, especially with re gard to languages with a relatively free word order like German or Dutch, that cannot be described adequately by context-free phrase structure trees alone. Arguments from linguistics motivate the representation of structures containing discontinuous constituents by means of trees with crossing branches (McCawley, 1982; Blevins, 1990; Bunt, 1996). Bunt (1991 Bunt ( , 1996) ) proposed a formalism called Discontinuous Phrase Structure Grammar (DPSG) for use in generation of and parsing with discontinuous trees and outlined an active chart parsing algorithm for DPSGs. DPSG is quite a natural and straightforward extension of common Context-Free Grammar (CFG). Hence, it is reasonable to assume that probabilistic approaches to parsing with CFGs might also be extended with similar ease to deal with crossing branches. In order to test this hypothesis, we adapt the algorithm for computing the Most Probable Parse (MPP) for a sentence given a probabilistic CFG (PCFG) to perform the same task for a DPSG enhanced with rule probabilities (PDPSG) and report on experiments conducted with this algorithm based on grammars extracted from the NEGRA corpus (Skut, Krenn, Brants, & Uszkoreit, 1997) . It turns out that accuracy results from PDPSG experiments are comparable to those obtained in PCFG experiments on the same corpus. In Sections 2 and 3, we define precisely what we mean by a discontinuous tree and a DPSG and present our chart parser for DPSGs. The theoretical foundations of our MPP algorithm for DPSG are discussed in Section 4, in which we also explain how the MPP is computed by a slightly modified version of our parser. Section 5 reports on experiments conducted with grammars extracted from a corpus of discontinuous trees and discusses the results we obtained. The paper 1 concludes with a brief summary and an outline of possible future lines of research. Discontinuous Phrase Structure Grammar The formalism of Discontinuous Phrase Structure Grammar (DPSG) is originally due to Bunt (1991 Bunt ( , 1996) ) . Below, we slightly deviate from Bunt's account in order to have suitable formal definitions on which we can base our algorithm for computing the MPP. However, the motivation and ideas underlying DPSG are preserved. We start with a recursive definition C?f discontinuous trees ( or discotrees for short) . Discotrees consist of substructures which themselves are not necessarily valid discotrees. Consider, for instance, the discotree (a) in Figure 1 We thus need a definition of these substructures first, which we provide below. Definition 1 (Subdiscotree) Let N be a non-empty set of nodes. A subdiscotree is recursively defined as follows. If x E N, then the pair (x, []} is a subdiscotree ; such a subdiscotree is called atomic. 2. If x E N and X 1 , ... , X n (n 2:: 1) are subdiscotrees that do not share any subdiscotrees2 , then the pair (x, [X1 , X\ufffd 2 , ... , X\ufffd:._ 1 1 , X n ] ) is a subdiscotree, where ci E {O, 1} for 1 < i < n. No other structures than those defined by (1) and (2) are subdiscotrees. \u25a0 In a subdiscotree (x, [X 1 , X? , ... , X\ufffd:._ 1 Next, we provide a suitable definition of linear precedence in discotrees. To this end, we first need two auxiliary definitions. The leftmost daughter of a subdiscotree T=x(X1, \u2022\u2022\u2022 ,X n ), Lm(T), is the top node of X1 ; if T = (x, []) is atomic, then Lm(T) = x. The leaf sequence of a discotree T, LeafSeq(T) , is the sequence of terminal nodes in T's formal representation in left to right order; if a leaf appears more than once, only its leftmost appearance is included in the sequence . The leaf sequence of S(P(a, b, [c], d), Q(c, [d], e), f) is thus (a, b, c, d, e, f) . Definition 2 (Linear precedence ( <)) Linear precedence ( <) in a discotree A with LeafSeq(A) = (x1, ... , x n ) is defined as follows. 1. For two leaves Xi and Xj in LeafSeq(A) , Xi < Xj if and only if i < j. In wo rds, the leaves of a discotree are totally ordered according to <. For two arbitrary nodes x and y, x precedes y if and only if the leftmost daughter of x precedes the leftmost daughter of y and there exists a node z in between Lm(x) and Lm(y) or identical to the latter, such that x does not dominate z. In the discotree in Figure la , we thus have a<b, a<c, a<d, a<e, a<f, b<c, b<d, b<e, b<f, c<d, c<e, c<f, d<e, d<f, e<f, P<c, P<d, P<e, P<f, a<Q, b<Q, Q<d, Q<e, Q<f, P<Q. Note that both P<d and P/d holds; that \ufffds, there is no \"Exclusivity Condition\". This is necessary because we want a no de n to precede its first context daughter ( and < to be a strict partial order) since the latter might be the leftmost daughter of a different constituent that is preceded by n. We say that two no des x and y are adjacent ( denoted by x + y) if x precedes y and there is no node z such that x < z < y. In our example discotree, the following adjacency relations hold: a+b, b+c, c+d, d+e , e+f, P+c, b+Q, P+Q, Q+d . For two arbitrary nodes x and y in A, x < y if and only if A CFG rule is used to re write its left-hand side category as a sequence of pairwise adjacent con stituents . If we directly apply this concept to the generation of discotrees, we encounter a problem . To see this, consider again the discotree in Figure la which we would like to be generated by the rules in (1). (1) S \u2794 P Q f P \u2794 a b [c] d Q\u2794 c [d] e (2) S \u2794 P Q [d] [e ] f P \u2794 a b [c] d Q\u2794 c [d] e But the first rule is not applicable, since Q and fare not adjacent . We would thus be forced to use the rules in (2) which are quite awkward and counterintuitive . To eliminate this problem, we relax the condition that the symbols on the right-hand side of a rule have to be pairwise adjacent and instead require them to form an adjacency sequence, as defined below. Definition 3 ( Adjacency sequence) A sequence of nodes (x1, x2, ... , X n ) in a sub disco tree A is an adjacency sequence if and only if l. 'v'l\ufffdi<n: (xi + Xi + I V( 3nodesyt, \u2022\u2022 \u2022 , Ym in A: xi +Yt I\\ Y1 + Y2 I\\ ... I\\ Ym-t+Ym /\\ y m + Xi +t I\\ 'v'l \ufffd j \ufffd m: 31 \ufffd k \ufffd i: XkD Y j)) and 2. 'v'l \ufffd i <j \ufffd n: ,(3 node zin A: xiDz I\\ xi Dz) . \u25a0 In words, we require in clause (1) that every pair (xi, xi +1 ) in the sequence is either an adjacency pair or is connected by a sequence of adjacency pairs of which all members are dominated by some element in the subsequence (x 1 , ... , Xi) and in clause (2) that the elements of the sequence do not share any constituents. Thus, (P, Q, f) constitutes an adjacency sequence since P and Q are adjacent, Q and fare connected by the sequence of adjacency pairs Q+d, d+e, e+f, and d and e are dominated by P and Q, respectively. Furthermore, P, Q and f do not share any constituents. We can therefore use the rules in (1) to generate the discotree in Figure la . A Discontinuous Phrase Structure Grammar (DPSG) then essentially consists of a set of rules which can be used in generating a discot ree such that the nodes corresponding to the symbols on the rules' right-hand sides form adjacency sequences . Definition 4 (Discontinuous Phrase Structure Grammar) A Discontinuous Phrase Structure Grammar (DPSG) is a quadruple (VN, Vr ,S, R), where \u2022 VN is a fi.nite set of nonterminal symbols; \u2022 Vr is a fi.nite set of terminal symbols; Jet V denote V N U Vr ; \u2022 S E V N is the distinguished start symbol of the grammar; \u2022 Ri sa fi.nite set of rules X \u2794 y l ,c i y 2 , c 2 \u2022\u2022\u2022 y n,cn ' where X E VN , Y 1 ' ... ' y n E V, and Ci E {O, 1} (1 \ufffd i \ufffd n) indicates whether y i is a context daughter in the rule. Sin ce neither the fi.rst nor the last daughter may be marked as internal context, c1 = Cn = 0. \u25a0 This definition makes apparent the close similarity between CFG and DP SG. In fact, a DPSG not containing rules with context daughters degenerates to a CFG. 3 The Parsing Algorithm Bunt (1991) outlined an active ch art parsing algorithm for DPSG that constructs discotrees bottom up (see also (van der Sloot, 1990)). Here, we present an agenda-based chart parser that provides us with greater flexibility with respect to the order in which edges are to be processed . The algorithm makes use of three data structures. Edges correspond to (partial) parse trees (sub discotrees, that is) and an agenda stores edges considered for combination with other edges already residing on the chart. An edge consists of the starting and ending position of its corresponding sub parse, a pointer to the DPSG rule which has been used to construct the edge, and the number of its right-hand side symbols for which constituents have already been found (fields start, end, rule and dot_pos, resp .) . This information alone, though, is not sufficient to uniquely describe a pos sibly discontinuous constituent . We also need to know which terminal symbols are dominated by direct daughters of the edge and which by context daughters . To this end, each edge is additionally asso ciated with two bit-strings (fields covers and ctxt_covers ) Later on, it also builds the edge [O, 4, S \u2794 P \u2022 Q f, 110100, 000000] , which is called active since it still needs to be combined with consd.tuents for Q and f. Notice that the input symbol c is not yet dominated by a direct or a context daughter of the edge . Therefore, the third bit in both covers (110100) and ctxt_covers (000000) is unset (equals 0) . The core of the parsing algorithm works as follows. for i = 0 to nl do During initialization , edges corresponding to the terminal symbols t 0 , ... , tn-l of the input sentence are added to the agenda . Edges are then popped off the agenda and added to the chart , one after the other . In the process of adding an active edge to the chart , it is combined with all matching inactive edges already on the chart , thus giving rise to new edges , which are added to the agenda. Likewise, an inactive edge popped from the agenda is combined with suitable active ones on the chart and, in addition, gives rise to new edges based on matching DPSG rules. This is repeated until the agenda is empty. If the chart then contains an inactive edge that is headed by the goal category and spans the entire input (that is, if edge.covers = 1 n ), it corresponds to one or more complete parse trees for the input sentence . edge t-new edge [i, i + 1, t i \u2794 \u2022, o i 1 o n -i-l , Let us make the pecularities due to dealing with discotrees instead of ordinary context-free trees more precise . Firstly, what conditions must hold so that an active edge ae and an inactive edge ie can be combined? The left-hand side category of ie (ie.ihs) must match the symbol to the right of the dot on ae's right-hand side (ae.next_cat) . Furthermore , in order to ensure that the constituents corresponding to the right-hand side symbols of ae form an adjacency sequence, the next edge to be combined with ae has to start at the position of the first terminal symbol within ae's span that is neither dominated by a direct nor by a context daughter of ae. We therefore compute the bit-wise 'or ' (\\1) of ae.covers and ae.ctxt_covers restricted to the interval [ae.start, ae.end) , set ae.next_pos to the position of the first unset bit in this bit-string , and require that ie.start = ae.next_pos holds . Additionally, we need to check that the two edges do not share any constituents. This is the case , if the bit-wise 'and' of ae.covers and ie.covers and of ae.ctxt_covers and ie.covers is zero. To summarise,  The parser would construct the discotree shown in Figure 2b , even though the P constituent assumes a context daughter Y dominating the input symbols c and d, whereas the direct daughter corresponding to Yi n span is headed by an X. To put it differently, the X constituent is not licensed as a context daughter of P according to rule P \u2794 b [Y] e. In order to prevent the construction of such ill formed structures, our algorithm additionally checks that context daughters and corresponding direct daughters match in category 5 \u2022 In order to be able to reconstruct all (partial) parse trees corresponding to an edge e, we associate with e a list of pairs of edge pointers (field children), each pair representing one possible way in which e has been constructed from an active and an inactive edge. When a new edge n is to be added to the chart, we check whether an edge e corresponding to an equivalent subparse already exists. If yes, the children list of n is appended to e's list, and n is discarded. Two edges correspond to equivalent subparses if they would behave in the same way would parsing proceed without them being merged. It turns out that e and n can only be merged safely, if they both correspond to a continuous constituent 6 \u2022 Additionally, if e and n are inactive, they have to agree in the values\u2022 of their fields start , end and lhs. If the two edges are active, their fields start, end, rule and dot_pos have to contain the same values. Note, finally, that the parsing algorithm described above constructs context-free trees when pre sented with a CFG as input . The additional mechanisms for dealing with discotrees come into play only if at least some of the input rules contain context daughters . Computing the Most Probable Parse As in the CFG /P CFG case, we can extend a DP SG with a probability distribution on its rule set, yielding a Probabilistic Discontinuous Phrase Structure Grammar (PDPSG) . Definition 5 (Probabilistic Discontinuous Phrase Structure Grammar) A Probabilistic Discontinuous Phrase Structure Grammar (PDPSG) is a quintuple (V N , V r ,S ,R,P), where (VN, V r , S, R) is a DPSG and Pi sa function R i--+ [O, 1] that assigns a probability to each rule such that VX E VN : E P(X \u2794 \ufffd) = 1. \u25a0 .\u00a3l In words, we assign a probability to each rule such that the probabilities of all rules with the same left-hand side category sum to 1. Note that \ufffd denotes right-hand sides of DP SG rules and as such contains information about which symbols are marked as context daughters. We define the probability of a string of terminal symbols as the sum of the probabilities of all parse trees that yield this string . The probability of a parse tree is the product of the probabilities of all rule instances used in constructing this tree . We adapted the algorithm for computing the MPP given a PCFG (see e.g. (B rants, 1999) for a nice presentation) to perform the same task for PDPSGs. The algorithm maintains a set of accumulators c> n (Y) for each symbol YE V and each node n in the parse tree . Contrary to the context-free case, a node in a discontinuous tree cannot uniquely be described by the starting and ending position of its corresponding subparse . We instead use two bit-strings for this purpose, as explained in the previous Initialization: { 1 if Z = Wt-1 (5)  ) J\u2022 J 1, 1 k, k in the recursion formula (6) . Given the two bit-strings of each subconstituent, we can perform the checks described in the previous section to ensure that the subconstituents form an adjacency sequence . The probability of each alternative is computed as the product of the probability of the rule used to construct it and the accumulators for all right-hand side symbols corresponding to its direct daughters, and & b, b(X i ) is set to the maximum of all alternatives ' probabilities. After the values for all & b, b(X i ) have been computed, the probability of the MPP is that of the accumulator for parse trees headed by the start symbol of the input grammar (X 1 ) that dominate all terminal symbols in the input (b = 1 T , b = o T ). The computation of these accumulators can easily be incorporated within a slightly modified version of our DP SG parser. To this end, we associate each edge with an additional field prob that stores the accumulator value for the subparse the edge corresponds to. Edges for the terminal symbols in the input added to the agenda during initialization are assigned a prob va lue of 1, mirroring Equation (5) . A new active edge that results from a DP SG rule matching an inactive edge that has been added to the chart inherits its probability from the underlying rule . When an active edge ae and an inactive edge ie are combined, the probability of the new edge is computed as the product of ae's and ie's probabilities, if ie is a direct daughter in the new edge, and is set to ae's probability otherwise (cf. Equation ( 6 )). Furthermore, we do not store more than one subparse with each edge, but only the most probable one found so far. In other words, if we encounter a new edge that wo uld be merged with an already existing one, we instead discard the edge that has a lower probability value . This implies that the discarded edge must not already be contained in a higher subparse because, if this were the case, the probability of the higher subparse would have been computed incorrectly. To prevent this, we organise the agenda in such a . way that parsing proceeds strictly bottom-up. Finally, when the agenda is empty, the edge that is headed by the start symbol of the grammar and that spans the entire input represents the Most Probable Parse for the given input sentence. The probability of-the MPP is contained in the prob field of this edge (cf. Equation ( 7 )). Again, this algorithm can be used to find the MPP for either a PCFG or a PDPSG, depending on whether the input grammar contains rules with context daughters . Experiments All experiments we conducted were based on grammars extracted from the NEGRA corpus (Skut et al., 1997) , which consists of German newspaper text. The version we used contains 20 571 sentences. All sentences are part-of-speech tagged, and their syntactic structures are represented as discontinuous trees. In a preprocessing step, we removed sentences without syntactic structure, attached punctuation marks to suitable nodes in the discotree, and removed unbound tokens. The corpus of discotrees thus obtained consists of 19 445 sentences with an average length of 17 tokens. In order to have some sort of baseline against which we could compare our results from PDPSG parsing, we additionally transformed the discotrees in this corpus to context-free trees by re-attaching all continuous parts of discontinuous constituents to higher nodes. We kept 1 005 sentences from each corpus to be used in case of unforeseen events and split the remaining corpus into a training set of 16 596 sentences (90%) and a test set of 1844 sentences (10%), such that both test sets (and both training sets) contained the same sentences, albeit with different structures. We extracted rule instances from both training sets 8 and computed each rule's probability as the ratio of its frequency to the frequency of all rules with the same left-hand side category, thus obtaining a PCFG and a PDPSG 9 \u2022  Due to limited computational resources, we restricted the two test sets to sentences with a maximum length of 15 tokens. We ran our parser on the part-of-speech tag sequences of these 959 sentences, once with the PCFG as input, and once using the PDPSG. The parse trees from the PCFG (PDPSG) experiment were then compared against the correct context-free trees (discotrees) in the test set. We determined average CPU time 10 per sentence and various accuracy measures for both experiments, which are summarised in Figure 3 .  Precision is defined as the percentage of constituents proposed by our parser which are actually correct according to the tree in the corpus. \"Correct\" means that the two constituents dominate the same terminals; for the different \"labeled\" measures, the node labels must match in addition. Recall is the percentage of constituents in the test set trees which are found by our parser. We define F-score as the harmonic mean of recall R and precision P, that is, as F = i\ufffd\ufffd . Coverage denotes the percentage of sentences for which a complete parse has been found by our parser. A proposed parse tree with an F-score of 100% is a structural match; if the labeled F-score is 100%, the tree is called an exact match. In comparing the results from the PDPSG experiment against the PCFG results, it is important to keep in mind that parsing with discotrees is a much harder task than parsing with context-free trees. The complexity of the latter is cubic in the sentence length, whereas our parsing algorithm for PDPSG takes, in the worst case, exponential time (see also (Reape, 1991) ). Therefore, unsurprisingly, the average CPU time per sentence in the PDPSG experiment is almost 50 times larger than in the PCFG case. To make things worse, the difference in running time would be even larger for longer sentences. 90% 80% \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . \u2022 .\u2022 \u2022 . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 . \u2022 .. . \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 70% 60% --+--PDPSG F-score \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022 PCFG F-score 50% The good news, on the other hand, is that accuracy drops only slightly (see Figure 4 ) when we accommodate within our parser the possibility of constituents to be discontinuous. Conclusion and Future Work We have presented a probabilistic extension of Discontinuous Phrase Structure Grammar, a formalism suitable for describing restricted discontinuities in a perspicuous single-level representation. Further more, we have developed a parser that can be used with probabilistic and non-probabilistic versions of both Context-Free Grammar and Discontinuous Phrase Structure Grammar, constructing context-free or discontinuous trees, respectively. Although the probabilistic method applied is rather simplistic, the accuracy results obtained in the PDPSG experiment are comparable to the PCFG results. A severe drawback of our approach is its worst-case exponential running time. Future work will thus be primarily aimed at reducing this complexity ( and additionally at increasing accuracy) . There are at least two avenues towards this goal. Firstly, we could try to restrict the formalism of DPSG further with respect to the kinds of dis continuities it is capable of representing. Vogel and Erjavec (1994) presented a restricted version of DPSG, called DPSG R , and claimed that this formalism is properly contained in the class of mildly context-sensitive languages and that consequently a polynomial time recognition procedure exists for it. Note, however, that DPSG R does not allow for the description of cross-serial dependencies; it is therefore too restricted to represent the discontinuity phenomena occuring in the NEGRA corpus 11 \u2022 A similar account, although within a different grammatical framework, was provided by Miiller (1999) who presented an HPSG system in which linguistically motivated constraints on (dis)continuity were imposed, which led to a notable decrease in running time. An alternative, orthogonal approach towards faster mechanisms for probabilistic parsing with dis continuous constituents is to stick to DPSG and try to find suitable approximation algorithms that reduce the observed running time. Several such approaches exist in the literature on probabilistic parsing, mostly based on (P)CFGs. Since DPSG is a straightforward extension of CFG, we expect that at least some of these can be extended with reasonable effort to also deal with discontinuous trees. In fact, the work presented in this paper serves as a first indication confirming this intuition. A more sophisticated approach could, for instance, be based on the statistical parser devised by Ratna parkhi (1997). He utilized a set of procedures implementing certain actions to incrementally construct (context-free) parse trees. The probabilities of these actions were computed by Maximum Entropy models based on certain syntactic characteristics (features) of the current context, and effectively rank different parse trees. A beam search heuristic was used that attempts to find the highest scoring parse tree. In order to extend this approach to DPSG, we would need a different set of procedures capable of constructing discontinuous trees and a suitable set of features. Edge-based best-first chart parsing (Charniak, Goldwater, & Johnson, 1998; Charniak & Caraballo, 1998) is another very promising approach. Charniak et al. (1998) proposed to judge edges according to some probabilistic figure of merit that is meant to approximate the likelihood that an edge will ultimately appear in a correct parse. Edges are processed in decreasing order of this value until a complete parse has been found ( or perhaps several ones), leaving edges on the agenda. They reported results equivalent to the best prior ones using only one twentieth the number of edges. The edge-based best-first parsing approach could easily be applied to DPSG and our chart parsing algorithm as well. To this end, we would need to organise the agenda as a priority queue, so that edges are processed in decreasing order of their respective figure of merit values. We are confident that suitable figures of merits can be found for DPSG chart parsing that will lead to a significant decrease in running time. is realized by a stack that stores the sequence of states traversed during recognition . At each final state, the recognized production is unwound from the stack, in an operation called a reduction , leaving exposed the state q1 and making a transition over the non-terminal B to q2 (a goto transition) . A reduction means a commitment to a certain substructure in the attempt to find a parse for the sentence. Whenever the parser reaches a state that has a reduction ( a final state), and there are also additional reduce operations or a shift, it has decide whether to do that reduction or try another alternative . That is certainly a limitation of the method, but at least, the decision has to be taken only after witnessing that the input has a complete yield for the production. That is . exactly what will be hard to guarantee in the TAG case. Q al v\ufffd The problem with LR parsing for TAGs is adjunction, as illustrated in Figure 2 .  Qo => a : l(B) => Q1 => /3: l(foot) => q4 => (q 1) => a : b(B) => q2 => (q4) => /3: r(foot) => => q5 => (q2) => a : r(B) => q3 Although we marked only q3 and q5 as final, as we would like it to be, since they mark the end of the recognition of each tree, there are two other points of disruption at states q4 and q2 \u2022 In a normal LR model of computation these points would correspond to reduction-like operations with goto's to jump to the resuming state. Two problems then immediately arise: one that we have called \"early reduction\" ; the other is the unsuitability of the normal stack model. Figure 3 shows an intuitive but rather naive first attempt to solve the problem. At the State q4 a reduction would be triggered sending the machine via a \"goto-beloul' to state q6\u2022 But notice that this operation would pop out all the states after q1 , including q4 \u2022 Later, at q2, when a new reduction would be required, we do not know anymore how to get back to the state that contains the \"goto-fo of' , the popped out q4 , unless we change the underlying storing model to something different from a stack. We are not finished. Even if we could get to q8 , we have to keep q2 alive somehow, because when reaching q5 we need it to access the \"goto-righf' to state q7 \u2022 It is important to note that we do not know in advance (i.e., at compilation time) which states those are. They are dependent on the actual computation. Although we could look for another storing model, the graver problem is the early reduction: the need to commit to tree (3, before seeing its right side in the input. The reduction in q2 does not cause any trouble since the state from where we want to take the goto, q4 , will be in the stack at that moment and is easy to recover. At q5 the reduction takes its goto-right to q7 from q1 , which is recoverable from the stack, instead of q 2 as in the previous approach, which has been popped out. Alas, things are not that simple. The problem of \"early-reduction\" is still significant, although not as much as in the first model. When the parser is in a state where (as one of the alternatives) it has just finished recognizing the subtree below a node where something has adjoined, as in state q 2 in Figure 4 , somehow it has to get back to state q 4 , and with a goto (goto-fo ot) to resume at state q 8 the recognition of the adjoined tree. In Nederhof's approach this is done by performing a reduction, that is, choosing one of the items in q2 that signifies that a subtree below an adjunction node has been recognized (e.g., the subtree corresponding to a : b(B)), and \u2022promoting a reduction of that subtree. This actually means committing to tree a too early, before seeing whether the rest of the input matches a : r(B). In the example in Figure 5 , immediately after seeing a prefix \"N\" , as in (1), that could be anchoring any of the innumerably many ai trees generating continuations such as in (2), the parser would have to commit in advance to one of them, due to the need to turn back to the right side of the relative clause tree /3. 1 The most visible consequence of this problem in the table is the huge rate of conflicts involving this kind of reduction that we drastically reduce with our approach as shown in Section 4. A ----+ ---\ufffd 1 6. NP\u2022 Rei Cl. N\u25ca subcat . I NP The second problem is an immediate consequence of the first. goto's are traditionally made on a symbol basis. In Nederhof's approach, however, because we\ufffdhave already committed to a (certain node of a) tree at state q 2 , a strategy of having one goto per node is adopted. Besides increasing the number of states, it simultaneously causes an explosion in the number of goto transitions per state, making the size of the table unmanageable. What we had in mind when looking for a new algorithm was precisely to solve those two problems. 6. NP l {L There is still a third problem, related to the lack of the valid prefix property. Predicting the path a : b(B) below a node where adjunction is supposed to take place after the left part of the adjoined tree is hard. Actually, we can show it to be impossible with Nederhof's approach as well as our own (although it would not be a problem for the \"naive\" solution). 2 An attempt to do that in the algorithm for table generation would lead to non-termination for some grammars. Hence, at state q4 all items corresponding to adjunction nodes labeled B in some tree are inserted, with the dot past the node, even if many of those nodes were not possible at that state, for a particular input prefix. Although in our approach the effects of misprediction and overgeneration are less harmful, it decisively affects the design of the algorithm as we see below. Figure 6 sketches our proposed solution . The goto now \ufffdepends on both q1 and q2 . At state q2, the items that correspond to the end of the recognition of bottom subtrees labeled B, like a:b(B) , are grouped into subsets of the same size, size being the number of leaves of the subtree . Hence goto-adj( q1, q2, B, l) has the set of possibilities of continuation like a:r( B) that we re predicted at q1 and confirmed at q2 that have l leaves under B. The dependency on q1 has the sole purpose of fixing the overprediction we mentioned above, originating at q4 , and propagated to q2 \u2022 When q2 is reached, an action bpack(B,l) extracts from the stack the l nodes under B (a:b(B) , uncovering q4 and the goto fo ot) , and puts them back in the stack as a single embedded stack element . The material _ is unpacked during f3 reduction before moving to q7 \u2022 3 The details of the algorithm are in the next section . The Table Generation for the NA/OA Case We describe, in this subsection, an algorithm for a TAG grammar in which all nodes are marked either NA(for Null Adjunction) or OA(Obligatory Adjunction). Later we deal with the general case . We do not consider selective adjunction (SA). To refer to the symbol (label) of a node n, we use symb(n). f is used to label anchors to represent the absence of a terminal symbol (the \"empty\" label). A dotted node is a pair (n, pos) where n is a tree node and pas, as in [Schabes, 1990) , can be la (at the left and above the node), lb (left and below), rb (right and below), or ra (right and above). We also represent the dotted nodes pictorially as \u2022n, .n, n., and n\u2022. A dotted item (item, for short) is a pair (t, dn), where t is an elementary tree, and dn a dotted node whose node component belongs\u2022 to t. We define a congruence relation \ufffd as the least symmetric and transitive relation such that, for any pair of items i 1 = (t, dn 1 ) and i 2 = (t, dn 2 ) of the same tree t, i 1 \ufffd i 2 if any of the following conditions apply: (1.] dn1 = \u2022n, dn2 = .n, and n is marked for null adjunction (NA). ( ] dn 1 = .n, dn2 = n., and n is an anchor labeled \u20ac. (  Congruent items are indistinguishable for the purpose of our algorithm and its underlying theory. Hence instead of dealing with separate items, we use equivalence classes under \ufffd-H i is a term, [i] is the congruence class to which i belongs Given a set S of dotted items (under \ufffd) of a TAG grammar, we define closure(S) as the minimal set that satisfies the following conditions: 1. if [i] E S, then [i] E closure(S). if [(t, .n )] E closure(S), and n is a substitution node, then, for every initial tree t1 with root node r labeled symb(n), [(t1 , \u2022r)] E closure(S). if [(t, \u2022n)] E closure(S) and n is a node marked OA, then, for every auxiliary tree t 1 with root node r labeled symb(n), [(t1 , \u2022r)] E closure(S). We point out that it is at this closure operation that the loss of the valid prefix property occurs. To define a parsing table for a grammar G with goal symbol S, we first extend G by adding one new tree called start, with two nodes: the root, labeled with a fresh symbol ST, marked NA; and ST 's single child, a substitution node labeled S. Then, let I be the set of all equivalence classes of items of G under \ufffd-Let N be the set of symbols, T \ufffd N be the set of symbols that appear in some anchor, and N the set of non-negative integers. We define the \"parsing table\" as a set Q \ufffd 2 1 of states with initial state q0 = closure( {[( start, \u2022ST)]}) E Q, together with. the functions GOT O s ub s t : Q x N \u2794 Q, GOT Otoot : QxN \u2794 Q, GOT O a di :QxQ'>!-NxN \u2794 Q, and ACTIONS : Q xT \u2794 2 A , where A = { shift q lq E Q} U { a-reduce tit is an alpha tree } U {,B-reduce tit is a beta tree } U {bpack (A, l) I A E N, l E N} U {accept}. Q, GOT O su b s t, GOTOtoot, GOTO a di, and ACT IONS are the minimal set and functions that satisfies the following inductive definition. 1 . qo E Q. 2. H q E Q and p = {[(t, n.)] I [(t, .n)] E q and n is an anchor }, then p' = closure(p) E Q, and (shift p') E ACTIONS(q, symb(n)). If q E Q and [(t, n\u2022 )] E Q, where n is the root of t, then, for every a ET: if t is an initial tree, then (a-reduce t) E ACTIONS(q,a), else (,B-reduce t) E ACT IONS(q,a). The General Case The obvious way to handle the general case is to transform the arbitrary input grammar into an equivalent one with all nodes marked NA/OA prior to the application of the algorithm of Subsection One last word: the approach we took is not totally equivalent to the bare OA/NA decomposition version. Keeping track of the history of adjunction for nodes that appear before the dotted node would generate tables with slightly more fine grain distinctions. Whether this could have some practical importance is still to be investigated. (shifting part of it to full tree reductions). The size of the table, roughly evaluated by the sum of the number of transitions and the number of action entries, is reduced by a factor of about 75 from unmanageable 75M to IM. We also reported in the second half of the Conclusion and Future Research We aim at building a parser that, parsing sentences from left to right using LR techniques, is able to take sound decisions towards reaching their correct analysis. The algorithm we presented here is suited for a non-lexicalized TAG, and in fact the results we presented here are all for the non-lexicalized component (set of template trees) of LTAG grammars where the terminals are parts of speech. We were able to produce a parse table of small size, with reasonably low degree of conflicts, considering the degree of ambiguity inherent to the sets of trees. The misprediction of bottom trees, due to lack of the valid prefix property, remains a concern. It is not clear whether a rescue of that property would have some significant impact in reducing conflicts in our current algorithm. Of course, as we said before, the property does not hold for the general case. But we are studying the subclass of TAG grammars for which our algorithm would never mispredict to see whether they are adequate for modeling Natural Language. That would mean, among other things, Introduction Numerous formalisms and systems have been designed for representing the grammar of free word order languages [10] , [18) , [19] , [9] , [15) , [7] , [14) . Each formalism has tried to capture some examples of local scrambling or long distance scrambling. Some of the formalisms considered the role of discourse in scrambling and the fact that under a specific intonation, one word order may be more accept able. Another issue which has not been thoroughly investigated is the notion of acceptability itself and implementing this imperfect notion for scrambling cases. Re cently, notions such as probability, optimality, possibility, plausibility, acceptability and graded grammaticality have been incorporated into the linguistic theories. Despite the fact that scrambling and word order introduce a degree of ac ceptability and graded grammaticality, nevertheless the necessary acceptability or plausibility notions have not been added to the scrambling rules . Modelling graded grammaticality has been neglected in many of the past works, and [2] is one of the few that tried to incorporate them into the linguistic competence . Is graded grammaticality part of competence or performance or part of both? Graded grammaticality and its interaction with word order constraints have also been studied from another perspective in performance models for languages [6] . The main problem is that not many significant theoretical works have been done to incorporate graded grammaticality in a unified model of competence and syntax. The lack of methods for gathering data and formal models of graded grammaticality are also complicating the problem . For a flexible word order language such as Persian, an Indo-European language spoken in the Middle East, accounting for graded grammaticality is essential because there are different levels of ambiguity in the grammar that interact together. In Persian the subject and object of a sentence can be missing (i.e. Pro-drop property) and subject and object marking is ambiguous in some cases. The notion of specificity which is a graded notion in Persian plays an important role in the disambiguation between subject and object. So modelling graded grammaticality becomes essential and it interacts with the word order rules. In a computational framework, one can model graded grammaticality as a form of competition among a set of alternatives with different degrees of grammaticality. In a competition framework, the result depends on the entities that are taking part. The violation of the principles of th.e grammar reduces the graded amount of grammaticality (i.e. acceptability) for each alternative. Competition in a grammar can arise for acquiring the highest degree of grammaticality among a set of plausible interpretations, but competition can also arise for limited linguistic resources. What are these resources and are there specific principles in languages that put further restrictions for acquiring these resources? We will answer these questions in the specific domain of modelling Persian and the scrambling in its word order. In this paper we will look at some of these issues and by introducing competition and parallelism at the same time. We avoid some of the problems of backtracking and the inefficiency that it causes. We will further investigate linguistic limitations which _ one can impose on the processing architecture to restrict some of the possible alternatives. For this purpose we turn to recent proposals for adding resource limitation strategies to the processing [8] . The structure of the paper is the following: in Section 2 we discuss the details of the parser. Section 3 illustrates the parser. In Section 4 we discuss some major aspects of the parser.  The first level of the parser, which is a variant to the PATR-II system, groups the words of the sentence into chunks: NP, PP, V, and Comp using context-free phrase structure rules. As soon as a chunk is found it is passed to the second level of the parser. The two stages are run in parallel in contrast to [12] (which the parser is based on) . Abney [1] uses a similar notion of pipeline parsing. He refers to the first stage as chunk level and to the second stage as the level of simplex clauses. Abney uses a finite-state cascade and his system uses finite-state models for grammatical representation at both stages. Instead of finite-state models we have used an extension to CFG rules in the first stage and regular grammars for the second stage. CFGs are more flexible and powerful in representing constituents with levels of recursion. We have also introduced a look ahead for these rules at the first stage. A Pipeline Parser Input CHUNKING Fo r representing scrambling we have extended the regular grammar rules for clauses with a special path set that keeps record of possible interpretations for the arguments of the clause. This path set is used to represent competition for grammatical functions and backtracking is avoided. It is updated incrementally. Fo r example if the first constituent can be attached to the clause as SUBJect and OB Ject, and if the next constituent can be attached as both SUBJect and OB Ject, then the path set will include all possible combinations of: [SUBJ.SUBJ, OB J.SUBJ, SUBJ.OB J, OB J.OB J]. Some of these possibilities are restricted by the use of word order constraints. In this example SUBJ.OB J is referred to as a path. Each path in the path set has an activation or possibility value attached to it which shows the plausibility of that particular path relative to the others. The value corresponding to each path in the path set is calculated based on word order constraints and the numeric values considered for each word order constraint. In other words, in our framework the word order constraints are defined locally to a clause (and not for rules), and they specify the precedence relations between two grammatical functions. The precedence relations are probabilistic and each possible word order has a probability measure attached to it. The word order constraints are of two types: hard and soft . The hard constraints cannot be violated, while the soft ones can be violated. The violation of a hard constraint makes the corresponding path inactive, while the violation of a soft constraint reduces the level of activity of that specific path. Fo r simplicity we assume that the activity level is the same as a probability number. In the following we will explain the details of the system and will elaborate on hard and soft constraints that put restrictions on these alternatives (paths). First Stage Fo r parsing the phrase structure rules of the grammar we have used a Prolog implementation of the standard version of PATR-II. We will first review a simple example of parsing . The input sentence is ali seab xord. (  At this stage we specify for each marked NP the possible grammatical functions that it can accept. The numbers after the grammatical functions correspond to the possibility of that alternative. These numbers are derived from the specificity value of a noun and the presence or absence of ra after the constituent. For example in the above Ali is a proper noun and it is specific. Since it is not marked by ra (specificity object marker) , its object value is low (20%) and its subject value is high (80%). For NPs which are not marked with ra we have considered subjecthood equal to the specificity value and objecthood = 100 -specificity-value. We have used a numeric value for specificity because specificity of a phrase varies over a non-discrete range. In the absence of a corpus for deriving the probabilities of words and their co-occurrence we have used this notion to initialize the activation value, because we mainly use it for subject-object disambiguation which relies on specificity. In contrast seab 'apple' is not a proper noun; as it is not marked by ra, it can be either subject or object. For objects like seab the subjecthood value of 20% and objecthood of 80% have been considered. This is because the corresponding specificity value for seab is 20. Note that one can consider different numbers, but the choice of numbers and their relation with specificity and object marking by ra should be taken into account. Our goal in designing the phrase structure (PS) component of the parser was to parse the input string into chunks and pass these chunks to the next level of parsing. By using the parallelism concept of Linda, the interface between the two stages is implemented. In our model the chunks are transmitted as Linda tuples between the two stages.  For representing local scrambling, we have used the :\u00b5otion of the path set. This notion allows one to have competing alternatives of plausible word orders and rank them according to some constraints. The word order constraints that we have considered are listed in Table 1 . The word order constraints are designed to reduce the activity of those alternatives which are not marked and which deviate from the canonical word order. A zero in the precedence constraint imposes a hard constraint to filter out illegal word orders 1 . -A non-zero value imposes a soft constraint to reduce the activation value for non-canonical word orders. J20 X 100 = 44. 27 The second chunk is seab and as a result of multiplication, we will have 4 grammatical-function pairs as potential candidates in the path set : subj.obj, obj.subj, obj.obj, subj.subj. When the verb is added with the activation of 100, those subjects which don't agree with verb will be deleted. Since both of the subjects agree with the verb, both alternatives will survive 2 \u2022 Finally, for the attachment of the arguments to the verb the path with the highest activity (acceptability) will be chosen and the arguments are bound to the verb . Since no wo rd order constraint has been violated the activation value will be 91 .97 = J84 .58 x 100 . Note that with the same constituents and a different order, the constraints will interact to yield a different measure of acceptability. Fo r (6) the acceptability measure is 89.58. This is because the example with canonical word order is considered more correct . (6) seab ali xord. apple Ali ate 'Ali ate an apple.' In this example, the object precedes the subject and hence violates the canonical wo rd order. As a result the activation will be multiplied by 0.90 (see Table 1 for precedence rules). A simple matrix for deriving the acceptability measure can be calculated by multiplying the values for the constraints which were violated . One can calculate all violations and yield a final violation measure and multiply the end result with this number. Instead we have multiplied each violation as soon as it is found. This incremental approach ensures that alternatives which are reduced to zero are not further extended . Finally in Persian, PPs can scramble freely and in parsing them we do not add them to the com petition (unmarked) set, because they contribute the same to all the competing paths . Instead of adding them to all paths we factor them out and store them in another marked structure because their contribution to all parallel paths are similar. The subcat( egorisation) expectations of the verb are also added to a separate structure in our model and when the end of the clause is reached the resources and the expectations are matched with each other and the dependency links are generated. In our model we choose the path with the highest activation and discard the other ones. Note the difference in order of items for subcat resources and the normal resources. In subcat we have subj: [4, 5] where [4, 5] corresponds to the location of the verb in the sentence, while in the unmarked resources we have [0,1] :subj (note the difference in the order of grammatical relation and the brackets in the two). When the parser reaches the end of a clause, the highest active path in unmarked will be selected and the matching /bf subj resource and subcat /bf subj resource 3  \u2022 Word order restrictions. These were illustrated in 1 and are used to penalize possible alternatives which deviate from the canonical word order. They also block alternatives which violate obligatory word order rules. \u2022 Verb subject agreement. In Persian a subject must agree with the verb of the clause. \u2022 One example of each resource in the sentence or Resource Limitation Principle (RLP). (7) Resource Limitation Principle No two NPs can exist in a clause with the same grammatical function. In the rest of this section we will elaborate on RLP. Consider example (8). This performance constraint that we call Resource Limitation Principle (RLP) is not restricted in Persian to datives, and no clause can exist in which two phrases (resources) have the same grammatical function. RLP has been implemented in our system as a general constraint that a resource cannot precede another with the same case or grammatical function (as is implemented in our system). We have used an extension to blocking word order restrictions. For example the constraint that 'no subject can precede another subject' implements the existence of at most one subject-marked resource in a clause. Discussion The path set implements a mechanism for modelling competition among a set of paths. Corresponding to each path we had an activation value. By assigning a value to each path, we implemented a numerical notion for competition. Such competition notion is robust enough to capture soft and hard constraints for word order rules. Each word order rule had numeric value attached to it. In modelling competition numerically, one can easily represent blocking as reduction of the activation value to zero. Another advantage is that such a mechanism can be extended to allow robustness and degrees of ungrammaticality. In the following we will discuss major aspects of our work in comparison to other approaches. In our approach we haven't employed ID rules and instead have used regular rules which allow different word orders. The possible word orders are restricted by a separate notion of word order binary constraints which restrict the possible order of grammatical relations in the paths. Comparison With Classical Reape introduces word order domains to deal with word order in Germanic [15] . In his approach, the word order domains of the constituents that join with each other are merged. Unlike the word order domain in Reape's notation, in our approach we only allow one instance of a grammatical resource to be present in a word order path in each domain and each word order domain can consist of a set of parallel competing word order paths. Corresponding to each possible word order path in the word order domain, we have an activation measure. The activation measure for a specific word order path is reduced if a word order constraint is violated. The reduction corresponds to the strength of that constraint and the stronger the constraint, the bigger the reduction. In the case of hard constraints, violation of a constraint makes the word order illegal and blocks that word order path. In this way relaxation of word order constraints can be achieved. A general restriction of classical LP constraints is that they cannot be relaxed and they must always be satisfied. [18] proposes to extend these by use of complex LP constraints. In complex LP constraints, as long as at least one of the LP rules is satisfied the other LP rules can be violated. Our approach is a numerical extension to LP rules which allows the possibility of relaxing the LP rules. Introducing a complex LP extension is also feasible in the model to have non-binary word order constraints. Similar to fuzzy logic sets, one can consider linguistic measures for referring to different relaxation possibilities for each word order rule and a linguist can use these for encoding the strength of the word order rules. Ideally, the relaxation of word order relations and their strengths should be derived from a corpus of texts, so that the most dominant word order gets the highest activation. In other words, these word order rules are statistically prevalent and are designed in such a way that the less plausible word orders get penalized and their activation gets reduced. In our framework we have used the unmarked order as the most optimal path and deviations from this unmarked order are penalized. This approach can be considered as an extension to a notion of optimal parsing introduced in [6] based on typological research. Comparison With Other Approaches Another alternative to modelling the competition and word order constraints is to use Optmality Theory (OT). OT [13] uses a notion of constraint ranking. For parsing free word order languages the cumulative sum of constraints from Syntax, Semantics, Discourse and world knowledge determines the grammaticality of an utterance and the preference of one alternative over another. It is not clear whether one can come up with a constraint ranking of these separate modules. [11] has also shown that for implementation of OT in a finite-state framework, one should restrict the OT model and a subset of it be considered. Another alternative is to use . a cumulative and weighted approach to ranking the constraints 5 that we have adopted. A general criticism to Optimality Theory ( OT) is that the ranking of constraints does not allow any cumulative effect in which a number of lower ranked constraints can compete against a higher ranking constraint. This so-called 'ganging up' effect can be represented by using a numerical representation. OT is a limited case of such a numerical representation with no 'ganging up' effect. A further criticism to OT parsing model has been raised in the literature [5] . In OT model of parsing , only the most harmonic alternative will be selected and the algorithm does not allow for a number of alternatives with a lower degree of harmony to compete in parallel with the most harmonic alternative, so that if the most harmonic alternative fails, one of the alternatives with lower degree of harmony be selected for the parse to continue. In our work , we have adopted a parallel competitive model that allows a number of alternatives to be run in parallel. This also allows a degree of robustness to be incorported into the parser in the future. Ta ble 2: Comparison and Evaluation In Ta ble 2 we have contrasted the implemented system with the previous systems for parsing Persian. The system is developed with the goal of complementing the capabilities of the previous systems and it has its limitations. Fo r further details of the above systems see [17] . Conclusion The framework that we discussed in this paper provides a method for adding the notion of graded grammaticality to the principles of the grammar. In traditional approaches to principle based ap proaches a principle can be satisfied or violated. In our view some of the principles of the grammar can be violated , but the overall relaxation of the principles ( when added together) should not reduce the acceptability of the solution below a certain level. The acceptability of a particular solution is reduced by a factor whenever a principle of the grammar is violated . This fa ctor depends on the contribution and importance of that specific principle. By adding features to the word order rules, we can introduce more complex word order rules to take into account features such as animacy. In our work we have introduced performance constraints for scrambling and we discussed the Re source Limitation Principle (RLP) for local scrambling. There is another counterpart to RLP for long distance scrambling -the Resource Barrier Principle (RBP) -that we have also implemented. These performance constraints restrict the scrambling in some free word order languages. They can be used to classify free word order languages. Introduction to Statistical Machine Translation In this paper, we describe some methods of how to use more structured inform at ion in language modelling to improve statistical mach ine translation (SMT). The organisation of the paper is as follows: In this section a short introduction to SMT is given. The following section gives an overview of the different langu age model ap proaches to SMT, then our definition of perform ance measures an d experimental results follow. The go al of mach ine transl at ion is the transl at ion of a text given in some source langu age into a text in the target language. We are given a source string If = Ji ... Ji ... /J, wh ich is to be translated into a targ et string e{ = e1 ... ei ... e1. Among all possible target strings, . we will ch oose the string with the highest probability : arg max {Pr( e{ 1 ft ) } The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target languag e. Pr( e{) is the language model (LM) of the target language, which will be investigated in this paper, whereas Pr (f f le{) is the translation model that is the main topic in [1, 9, 11, 12, 13, 14] . In this wo rk the al ignment templ ate ap proach as described in [11] is used. Language Models fo r SMT Especially for the task of translation, the need of more restriction in the LM prove to be necessary. Restriction of the LM of the target language intuitively should improve translation quality in that way that the translation model should allow many alignment possibilities that are then restricted by the language model. Some heuristics help SMT systems to perform better, for example by re-ordering the source sentence as in (13] or by producing permutations in the search that are scored by a LM as in (11, 17, 18) . The advant\ufffdge of the latter approach is that the reordering is integrated into the search and needs not be done as a preprocessing step. For this approach a more restricting LM, in terms of linguistic constraints and the ability to model long range dependencies is helpful. In the work of [1] , where the translation direction is French to English, the robust m-gram models have been used, as they cover the need for the analysed task well. For French-English the word order difference is mostly of a local nature, LMs that model embedded long range structures seem not to be necessary. In this paper we show experiments using following types of LMs: word based m-gram models, class based m-gram models and context free grammar based models. Standard Wo rd based m-Grams In equation (3) the LM is divided into two submodels: the classification model p ( e i le i , h i ) and the class sequence model p ( Ci lh i), where ei denotes the ith word of the hy pothesised sentence, Ci a classification of this word and h i is the sequence of preceding word classes h i = ci-1 or, using an m-gram, hi = i-1 c i-m+l As shown in formula (3) we use the simplification that we perform the maximisation before calculating the probability of the whole word sequence. This means that the found maximum is only a local maximum. Instead the maximisation should have taken place after the calculation for the whole sequence, which would require a search similar to the monotone search (12] . Using non-unique class membership adds the problem of smoothing the class probability p ( e i lei , h i ) in formula (3). It shows that absolute discounting for disambiguation of the class here performs best. Formally the absolute discounting of the class probability is as follows: - { N(e i , Ci , h i ) -be o} b . w -no(ci , h i) . r-1 ( \u2022 I . -h -) p ( ed Ci , h i ) -max N ( Ci , h i ) , + c N ( Ci , h i ) fJ e i c , , z , (4) where N(.) denotes the count of an event. W is the size of the vocabulary and n 0 (c i , h i) denotes the number of words not seen with the class .c i and class history h i . /3( e i lei , h i ) describes a less specific distribution with the generalised class history h i . The probability distributions for the class probabilities used in the present study have the same order as the class sequence probabilities, for example if the class sequence model is a trigram, the class probability is Pc,o = max { N ( e i , Ci , Ci -1 , Ci -2 ) -b c 3 0} N( Ci , C i-1 , C i-2) ' b W -no (c i , Ci -1, Ci -2) ( I ) + c,3 \u2022 N( ) \u2022P ei C i , C i-1 Ci , Ci -1 , Ci -2 max { N(e i , Ci , Ci-1) -b c,2 ' o} N(ci , C i-1 ) b W -no(c i , C i-1 ) ( I ) + c,2 \u2022 N( ) \u2022 p e i Ci Ci , C i-1 { N(ei , Ci ) -b c,1 o} max ( ) , N Ci 1 W -no (ci) + b c,1 \u2022 N(ci) \u2022 Pc,O w (5) (6) (7) (8) As with absolute discounting in standard language modelling the calculation of the probability p(ei lei , C i-1 , C i-2) consists of a trigram (5), a bigram (6), a unigram (7) and a zerogram (8) portion, which is given by the inverse of the number of vocabulary entries. Stochastic Context Free Parser When using m-gram LMs, some linguistic phenomena are not captured very well, because they do not model long range dependencies and embedded structures. A possible solution to this problem the use of context free grammars (CFG) as LMs (2, 18] . Formally a CFG is a quadruple g = (V N, Vr , R, S), where VN is the set of all nonterminals, Vr the set of terminals, R is the set of rules and S denominates the starting symbol that has to cover the analysed sentence. To use CFG we implemented a stochastic parser using the stochastic version of the algorithm introduced by Cocke, Younger and Kasami (4, 19] , by assigning a probability p(r :A n \ufffd A 0 A ,0 IAn) to ea ch rule r : A n --+ A 0 A ,a . The best hypothesis is the sequence of rule applications that produces the whole sentence from the nonterminal S an d has the highest probability. The probabilities are trained using the Viterbi approximation so that only the best parse is used to calculate the rule probabilities . If we assume the rule probabilities to be independent of ea ch other an d the class probabilities of the Part-of-Speech tags distributed uniformly, the LM probabilities can then be written as: In order to include the tagging pro cess into the sto chastic parser, we ch ange the format of the standard Chomsky Normal form (CNF) in the following way. We distinguish three types of rules an d asso ciated probabilities : \u2022 tagging rule with probability p( Ci --+ edci), \u2022 the so-called lexical rule with probability p(A a --+ ci lA a ), an d \u2022 the so-called structure rule with probability p(A 0 --+ A ,a A \")' IA a ). Fo r wo rds that are not observed in the training corpus, a simple backing-off smoothing technique is used for tagging to be able to tag these unknown wo rds. Note that, according to equation ( 9 ), the optimal POS tags are determined only aft er the whole senten ce has been parsed. In that sense the tagger uses a global sentence level criterion rather than a lo cal decision criterion . The search for the best pa rse is done using dynamic programming over the positions 1 < i, j < I. The recursion formula for the stochastic Cocke-Younger-Kasami-style parser (SCYK) using the Viterbi approximation is based on partial hypotheses Q derived from (9) with Q(j, ilA n --+ A a A 13 ) = p(A n --+ A a A 13 IAn) . m \ufffd ( Q(j, l lA a ) Q( l + l , ilA 13 ) ) J <l<i-1 We use two speed-up metho ds for the SCYK: top-down filtering: The parsing pro cee ds mainly bottom up , apart from a top-down filtering metho d that does not affect the parsing accuracy. The filtering limits the number of nontermi nals that can produce the hypothesised senten ce fragment, allowing only nonterminals that can be reached from a special position within the sentence. bounding: Another implemented feature to improve speed is that a calculation is cancelled, if the scores of partial hypotheses do not reach a lower bound. The main idea is that a product of two probabilities cannot be higher than the smaller of the two probabilities. To be able to use grammars that are not in CNF , we implemented an algorithm that converts any CFG into CNF . The standard algorithm is sketched in table 1 [3] .  The problem of this algorithm is that the number of newly added rules increases rapidly so that the memory requirement for the CNF grammar is very high and also time complexity increases. The reason of this increase is that the complexity of the SCYK is O(IVNI \u2022 IRI \u2022 1 3 ) . According to this formula we would prefer a CNF grammar with minimal numbers IRI of rules and I VN I nonterminals. Here, we introduce an algorithm that uses bigram frequencies to determine which pair of nontermi nals to choose when generating a new rule (see algorithm in table 2 ). Thus we can reduce the number of rules significantly.  There must be some considerations as to what to do regarding the rule probabilities when trans forming the grammar. The easiest way to handle the probabilities is as follows: when applying a rule, the rule probabilities are multiplied as usual. When adding a rule to the productions R, the new rule r' has probability p( r' : A' ---+ A 0 A p IA') = l. The implemented parser is constructed \u2022in such a way so that the hypothesised sentence is processed left-to-right that means the chart for the SCYK is constructed along the covered positions of the sentence instead of the standard way, where the chart is constructed along the depth of the parse subtrees. In such a way, we can use the parser as a left-to-right LM that can be embedded into the translation approaches, e.g. described in [9, 11, 12] or in speech recognition systems that build up the recognised sentence incrementally. Linear Interpolation of Language Models Experiments show that every LM type has some advantages compared to the other LMs, but also bears some weakness so that it would be best to use all LMs at the same time. An m-gram LM for instance has its strength in robustness and we expect the grammar based LM to model long range dependencies better. A very easy method to combine two LMs p 1 ( \u2022) and p 2 ( \u2022) is to use linear interpolation [8] at the full-sentence level: Pr( e {) = (1 -a) \u2022 P1 (e{ ) + a \u2022 p2 (e{) with O < a < 1 Parsing Performance The \"VERBMOBIL Task\" [15] is a speech translation task in the domain of appointment scheduling, travel planning and hotel reservation. The translation direction is from German to English which poses special problems due to the big difference in the word order of the two languages. To perform experiments with the SCYK parser, we used the English part of the VERBMOBIL tree bank [5] . Table 3 shows some statistics about the investigated corpora. For the performance tests of the parser, we used the standard training and test set that consist of about 9000 and 500 trees, respec tively, where every tree corresponds to one sentence. To train the parser for rescoring the translation results, we used all available trees in the VERBMOBIL treebank, i.e. about 21,000 sentences. 6 shows the parsing performance on the VERBMOBIL treebank test corpus. When looking at the results shown in table 6, we should keep in mind that there was a certain number of unseen words (see table 3 ). Due to this number of unseen words the tagging accuracy decreases from 96.73% to 95.87%, the complete match from 51.6% to 49.6% and bracketing recall and precision drop by about one percent absolute respectively. Translation Results The VERBMOBIL Task For translation experiments we used the bilingual text corpus of the VERBMOBIL task. The text input was obtained by manually transcribing the spontaneously spoken sentences. There was no constraint on the length of the sentences, and some of the sentences in the test corpus contain more than 50 words. Therefore, for text input, each sentence was split into shorter units using the punctuation marks. The segments thus obtained were translated separately, and the final translation was obtained by concatenation. Table 7 shows a summary of the corpus used for the experiments. Here the term word refers to full-form word as there is no morphological processing involved. Performance Measures In the translation experiments, we use the following performance measures [10] : \u2022 mWER (multi-reference word error rate): A known weakness of the word error rate that is widely used for speech recognition is that a translation of a given sentence is not unique so that there can be more than one translation for a source sentence. A possible solution for this is to use a set of several possible translations for each source sentence. The m WER is the Levenshtein distance to the most similar sentence of this set. \u2022 SSER (subjective sentence error rate): For a more detailed analysis, subjective judgements by test persons are necessary. Each translated sentence is judged by a human examiner according to an error scale from 0.0 to 1.0 in eleven steps. A rating of 0.0 means that the translation is semantically and syntactically correct and a rating of 1.0 means that the sentence is semantically wrong, i.e. either the produced sentence has no sense at all or the produced sense does not convey the sense of the source sentence. The human examiner was offered the translated sentences for the different LMs at the same time. Translation Results for VERBMOBIL In Table 8 Using long history length seems to perform better for m-gram LMs both word and POS-based. The SCFG alone does not improve translation results compared to the word based LM. The interpolation of both POS-based LM and SCFG achieves the best results. Linear interpolation of word and POS based LMs also achieves better results than POS-based or word based LMs alone but does not reach the performance of the combination of POS-based LM and SCFG. Analysing the sentences chosen from the different LM types we can observe that the SCFG is superior to the m-gram LMs in modelling nested structures and long range dependencies as can be seen in table 9. In each of the two cases for linear interpolation shown in table 9, the interpolation factors were 0.5, which produced the best results. The sentences show that the syntactic quality increases . when using more linguistic information for SMT. The third sentence in table 9 shows the advantage of using grammar based LMs. The corresponding sentence of the word based model contains the problem that the verb group for this sentence in the source language is composed of two parts: am produced from the first part habe and make produced from ausgemacht, are positioned relatively far from each other within the sentence. The coherence of these two words is thus not detected by the m-gram LMs. The SCFG, however, can detect this and constructs a better verb phrase. For the third translation example in table 9 it should be noted that the list of 100 sentences does not include the correct sentence. After adding the correct sentence to the list, the system produced the correct translation. Conclusion In this paper we discussed the use of linguistically motivated language models for statistical machine translation, namely Part-of-Speech based m-gram models and stochastic context fr\ufffde grammars. The results of the different language model types and the interpolation of the language models are then presented on the VERBMOBIL task. It shows to be a promising approach to use a stochastic context free parser as syntax-restricting language model and to interpolate it with a Part-of-Speech based language model. In the near fu ture the language models introduced here will be integrated into the translation process itself instead of using rescoring. In (7] different interpolation methods are compared and the linear interpolation was found to be worse than log-linear interpolation methods. Therefore these interpolation methods should be examined for language models for statistical machine translation in the future. Also refined grammar based models should be investigated, for instance usage of lexicalized grammars or stochastic attribute grammars. The usage of morphological analysis should achieve a gain in syntactic quality for the produced sentences. Introduction Linear indexed grammars (LIG) (2] and tree adjoining grammars (TAG) [4] are weakly equivalent grammar formalisms that generate an important subclass of the so-called mildly context-sensitive languages (MCSL). In recent publications (see for example [1, 5] and the papers cited there) the design of parsing algorithms for LIG and TAG is based on an operational model of (formal) language recognition. It consists of the construction of some nondeterministic push-down automaton from a grammar, depending on the parsing strategy, and a tabular interpretation of that automaton. This approach is modular because the tabulation of the automaton is independent of the parsing strategy. Besides its obvious advantages over a direct construction of parsing algorithms (as in [9] ), this approach is still dissatisfying in two respects: First, the tabulation of a LIG automaton is motivated only informally, in terms of a certain non-contextuality of LIG derivations (i.e., parts of LIG derivations do not depend on the bottom parts of the dependent stacks) or in terms. of an efficient representation of unbounded LIG stacks. Second, because the usual push-down automata read their input sequentially from left to right, this technique cannot be applied straightforwardly to head-corner strategies, which start the recognition of an input string in the middle. In this paper we present an algebraic approach to the design of parsing algorithms. By this we mean that a parsing algorithm is derived from an algebraic specification of a parsing strategy by means of algebraic operations such as homomorphic images, direct products, subalgebras and quotient algebras. A parsing strategy is expressed through the operations in an algebra where the objects are partial parse trees ( called a tree algebra). A second algebra ( called yield algebra) describes how the input string is processed. Fo llowing [7] we do not construct parsing algorithms but rather parsing schemata, i.e. , high-level descriptions of tabular parsing algorithms that can be implemented as tabular algorithms in a canonical way [8] . A parsing schema describes the items in the table and the steps that the algorithm performs in order to find all valid items , but leaves the order in which parsing steps are performed unspecified. Our approach picks up an idea originally proposed but not fully developed 1 by Sikkel [7 ] whereupon a parsing schema could be regarded as the quotient (with respect to some congruence relation) of a tree-based parsing schema. A parse item is seen as a congruence class of a partial parse tree for some part of the input string. The problem is that items that do not denote a valid parse tree for some part of the input string cannot be described in this way because they would denote empty congruence classes. In our approach a parse item is seen as a pair (a\ufffd,\ufffd) where a\ufffd is a congruence c}ass of trees and \ufffd denotes a substring of the input string. In this way all items can be characterized algebraically. This allows us to lift the correctness proof from the level of items to the level of trees. In this paper we construct a new bottom-up head-corner (buHC) parsing schema for LIG to demon strate the algebraic approach. The construction proceeds in two steps: In the first step we construct a buHC parsing schema for context-free grammars (CFG) algebraically and give a correctness proof. In the second step an algebraic , correctness preserving transformation is applied to the tree algebra of this parsing schema to obtain a buHC parsing schema for LIG. The transformed tree algebra imple ments the non-contextuality of LIG derivations into the tree operations and thus makes this notion more precise. Our approach has a series of advantages over the automaton-based construction of parsing algo rithms: It is not limited to parsing strategies that process the input string from left to right ; it provides a precise characterization of an item in terms of congruence classes ; it allows simpler and more elegant correctness proofs by means of general algebraic theorems; it allows to derive parsing schemata for LIG from parsing schemata for CFG by means of algebraic transformations; and finally, it provides a precise explanation for certain characteristics of LIG parsing algorithms. The paper may be outlined as follows: In Sect. 2 we define the basic algebraic concepts used in this paper. Sect. 3 presents a short introduction to parsing schemata and describes the general method of constructing parsing schemata algebraically. In Sect. 4 we show the algebraic construction of the buHC parsing schema for CFG, and in Sect. 5 we define an algebraic transformation that yields a buHC parsing schema for LIG. Sect. 6 presents final conclusions. Nondeterministic Algebras In this section we present generalized versions of standard concepts of Universal Algebra [3] for algebras with nondeterministic operations, called nondeterministic algebras, which provide the basis for the algebraic description of parsing schemata. The theorems in this section are given without proof, the proofs can be found in [6] . Although nondeterministic variants of algebras have been \u2022 defined previously, for example relational systems [3] , most concepts of Universal Algebra have been fully developed only for algebras with deterministic operations. An algebra A is a pair (A, F) where A is a nonvoid set (the carrier of A) and Fis a fam ily of finitary operations f: A n \u2794 A. An n-ary nondeterministic operation is a set-valued function f: A n \u2794 P(A) , where P(A) denotes the powerset of A. We use the notation f(a1 (N, \"E, Q, P, S, h, o) where (N, \"E, P, S, h ) is a headed CFG, Q is a finite stack alphabet and o is a function that assigns each production p E P a stack operation push q or pop q (where q E Q) if the head of p is a nonterminal symbol, and nop otherwise. Let q E Q* be a finite stack. The stack operations are defined as follows: push q (w) = wq, pop q (w) = w' if w = w'q, else undefined, nop(w) = w. A headed tree is a tree such that for each node v that is not a leaf, exactly one child of v is marked and the others are unmarked. The marked child is called the dependent descendant of v. A buHC-LIG tree is a tuple (T, k, l) such that Ti sa headed tree labeled with pairs (X, w) E (N U \"E) x Q* , written The buHC-LIG tree operations are obtained from the buHC tree operations by incorporating the stack operations associated with the productions. Fig. 3 shows the buHCa and buHCA operations. In the resulting trees the nodes labeled with a resp. B[w] are marked. The other operations do not mark or unmark nodes. The Scan and Co mpl operations do not perform any operations on stacks, however, the Co mpl operations are only defined if the stack on the root of the side tree is empty. The buHC-LIG tree algebra is denoted with Ab u HC-LIG \u2022 Prop. I remains valid if Ab u HC is replaced with Ab u HC-LIG and \"buHC trees\" is replaced with \"buHC-LIG trees\" , if we consider a buHC-LIG tree as a buHC tree over the infinite label domain (N U \"E) x Q* and a LIG production as an abbreviation for an infinite set of context-free productions over this infinite domain. Note that the proof of Prop. I -, -, -, i -1, i, -, -] (A \u2794 /3ai8 E P) [ B \u2794 \u2022 , \u2022 , q, C, q\", i, j, r, s], [ C \u2794 \u2022 ,' . , q1, X, q\ufffd, r, s, u, v] I-[ A \u2794 ,B \u2022 B. 8, q1, X, q\ufffd, i, j, u , v] (p = A \u2794 ,BB8 E P, o(p) = push q ) [B \u2794 . ,., q, C, q\", i, j, r, s] , [C \u2794 . ,'., -, -, -, r, s, -, -] I-[A \u2794 ,B . B. 8, -, -, -, i, j, -, -] (p =A\u2794 ,BB8 E P, o(p) = push q ) [B \u2794 . ,., q1, C, q\ufffd,i,j,r ,s] I- [A \u2794 ,B. B. 8, q, B, q1,i,j,i,j] (p =A\u2794 ,BB8 E P, o(p) = pop q ) [A \u2794 /3ai\u2022r\u2022 8,q,B, q',i,j, r,s] I-[A \u2794 /3_\u2022 an-8, q,B,q', i -1, j, r,s] [A \u2794 ,B.,. ai +1 8, q,B, q',i,j,r,s] I-[A \u2794 .B\u2022, ai + 1 . 8, q,B, q 1 ,i, j + 1,r,s] [A \u2794 ,BB. ,. 8, q, C , q', i, j, r, s], [B \u2794 . , ' ., -, -, -, k, i, -, -] I-[A \u2794 ,B. B,. 8, q, C, q', k, j, r, s] [A \u2794 ,B.,. B8, q, C,q ',i,j, r,s], [B \u2794 .,'. , -, -, -,j, k, -, -] I-[A \u2794 ,B .,B. 8,q, C, q',i,k, r, s] of r 1 then B[w'q'] is the dependent stack descendant of r 2 , for some w, w',w,w'. If (r,k,l) is as in Fig. 4 (left) then let 9h u HC-LIG(r,k, l) = (u1 ,u2 ,u3), and if r = (AO \u2794 /3(r \ufffd u)8) then let 9b uHC -Lia(r,k,l) = ( -,u, -). Then the buHC-LIG operations can be defined on the buHC LIG yields in a straightforward way (for example, see Fig. 5 for buHCA p us h), such that 9h u HC-LIG is a homomorphism. Using the construction described in Sect. 3 (analogously to Sect. 4) we obtain a (cor rect!) buHC-LIG parsing schema. The buHC-LIG items are of the form [A \u2794 !3 . ,. 8, q,B, q', i, j,r , s] where A \u2794 /3,8 is a production, q, q' are stack symbols, 0 ::; i ::; r < s ::; j ::; lwl and q, B, q', r, s areif a buHC-LIG tree has no dependent stack descendant (then O ::; i < j ::; lwl) -The item homomorphism 'Ph u HC-LIG maps a tuple of positions (i, j, r, s) to (ai+1 ... a r ,a r+ l ... a s ,a s+ l ... aj), resp. to (-, ai + l ... aj, -) if r = s = -, where w = a1 ... an is the input string. The deduction steps are shown iri Fig. 6 . The transformation defined in Theorem 4 may also be used to account for the form of the steps in the CYK-LIG algorithm in [9, 10] . This algorithm may be seen as the result of a transformation of a CYK tree algebra for CFG in Chomsky normal form using a similar substitution of subtrees as in Fig. 4 . Conclusion We have proposed an algebraic method for the construction of tabular parsing algorithms. A parsing algebra for a grammar G and input string w is a relative subalgebra of a quotient algebra of the direct product of a tree algebra A ( that reflects the parsing strategy) and a yield algebra B ( that describes how the input string is processed) which is homomorphic to A. A parsing schema is the inverse image of a parsing algebra under a strong homomorphism. Correctness of a parsing schema is defined at the level of tree operations. We have demonstrated the construction using a buHC parsing strategy for CFG. Furthermore, we have derived a buHC parsing schema for LIG from the buHC parsing schema for CFG by means of a correctness preserving algebraic transformation. The most widely-used methods for POS tagging are stochastic methods based on fixed order Markov chains models and Hidden Markov models. However, the complexity of Markov chains grows exponen tially with its order L, and hence only low order Markov chains could be considered in practical applica tions. Moreover, it has been observed that, in many natural sequences, memory length depends on the context and is not fixed. In order to solve this problem, in 1996 Dana Ron [Ron, 1996 , Ron et al ., 1996 ] described a learning model of a subclass of PFAs (Probabilistic Finite Automatas) called PSAs (Prob abilistic Suffix A utomatas). A PSA is like a Markov chain with variable memory length . So, this model avoids the exponential complexity relation with the order L and thus can be applied to better effect than Markov chains. In this paper a Spanish POS tagger based on variable memory Markov chains is descr\ufffdbed ( which will be called VMM throughout). This tagger is based on a modified version of Singer's tagger [Schutze and Singer, 1994 ]. The VMM training algorithm used by Singer's tagger is more efficient than the Markov chains training algorithm. Fu rthermore, it allows higher order chains than fixed order Markov models . This implies better accuracy and a lower error rate . On the other hand, the described . tagger makes use of a feature structure set of tags like the model described by Kempe [Kempe, 1994}. Usually, the contextual probability of a tag is estimated as the relative frequency in a given training corpus. With a large tag set (resulting from the fact that the tags contain -besides the POS -a lot of morphological information ) and with only a small training corpus available, most of these probabilities are too low for an exact estimation of contextual probabilities . Moreover, a large tag set increases the complexity of the Markov chains and it implies low training efficiency. An example of this problem is the tagger developed by Sanchez (Sanchez Leon and Nieto Serrano, 1995]. Finally, most taggers deal with the problem of determining the syntactic part of speech of an occurrence of a word in context, but they can not determine the set of correct tags for a word without any such context . Instead, most taggers use a lexicon that lists possible tags . In morphologically rich languages, this lexicon must be very large in order to represent the language correctly. In order to solve this problem, a single word tagger has been added to the system described. This single word tagger (Trivino, 1995] is based on a modified version of the ID3 algorithm described by Quinlan (Quinlan, 1986 ]. This paper is organised as follows: first the single word tagger is described . Next, the VMM is shown and how it is applied to POS tagging. Finally, the results obtained from this model are analysed. Single word tagging The single word tagger is the algorithm that computes all valid tags for a word if the word is considered out of context . The information used to carry out this task is the morphology of the word. Therefore ; this kind of algorithm must be applied to morphologically rich languages where a word may have many derived words and where the lexicon must be very large in order to hold all these words. Most single word taggers like Padro 's tagger [Padro, 1996] are based on the two-level morphology described by Kimmo Koskenniemi [Koskenniemi, 1985) . Koskenniemi's model of two-level morphology was based on the traditional distinction that linguists make between morphotactics, which enumerates the inventory of morphemes and specifies in what order they can occur, and morphophonemics, which accounts for alternate forms or \"spellings \" of morphemes according to the phonological context in which they occur. Fo r example, the word chased is analyzed morphotactically as the stem chase (0.8, 0.2) (0.3, 0.7) An interesting feature of the PST model is that the transition probabilities are smoothed along the learning task. This avoids transition probabilities with value 0. For this reason, the model must not be interpolated after the learning task unlike the Markov model. The PST learning algorithm generates a PST hypothesis that holds the \u20ac-good hypothesis property with respect to a given PSA. \u2022 L is the maximum length of the string labeling the states. Definition 3.2 ( \u20ac-good hypothesis) Let M be a PSA and let T be a PST and let PM and PT be the two probability distributions they generate respectively. We say that T is an \u20ac-good hypothesis with respect to M if 'v N > 0, it is held that: \u2022 'n is an upper bound on the number of states in M. \u2022 8 is the confidence parameter, 0 < 8 < 1. \u2022 \u20ac is the approximation parameter, 0 < \u20ac < 1 \u2022 The training sample is compounded by m' strings generated by M each of length L + 1 at least. Definition 3. 4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample). This is computed by the fo llowing equations: Pr(s) = Pr(uls) = m' l-1 (LL) \ufffd\ufffdx; ( s ) m' l-1 i Ei =l E i =L Xj+i (su) \u00b0'\"'m' \u00b0'\"'l-1 i ( ) L.., i =l L.,j=L Xj s Where m 1 is the number of strings in the sample and f is the length of the strings, such that f 2'. \u20ac \u20ac2 = 48 * \u00a3 \u20ac2 'Y min , \ufffd, \u20ac f o = 2 * n * L * log ( -1 -. ) 'Ynun \u20ac2 * 'Y mi n \u20ac1 8 * n * <:o The learning algorithm generates a PST from a training sample following a top-down approach. It starts with a tree consisting of a single root node labeled with the empty string. Progressively, new no des and edges are added to the tree. Each node is added depending on its observation \u2022probability. In this way, a new node is added when it has a significant probability of being in the sample and the transition probabilities from this node differ substantially from the transition probability of its parent . An example of the learning task can be seen in figure 2 . Tagging with VMM (Variable Memory Markov Models) Part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context . Therefore, give\ufffd a sequence of words w 1 , . = argmax Pr(t1, ... ,t n ,W1, ... ,w n ) t1 , ... ,tn The equation 1 could be laid out as follows: Pr(t1, . ( ) 5 This tagger is based on a feature structure set of tags. Therefore, the term ti in equation 5 is a vector of a symbols 2 (a symbol for each attribute). However, the Viterbi algorithm [Viterbi, 1967] can only compute the probability of a sequence of a single attribute. In order to solve this problem, a set of a trees ( a tree for each attribute) has been co mputed independently. On the other hand, most of the errors arise because the tagger does not take into account relations between out attributes. Thus, the main development line of this work is to change the variable memory model for a model that takes relations between attributes into account. Introduction Efficiency, memory, ambiguity, robustness, and scalability are the central issues in natural language parsing (5, 21, 7, 19, 12, 24, 14] . Among others, Earley 's algorithm is often used for its superior performance due to a good combination of top-down prediction and bottom-up bookkeeping. Schabes' algorithm precompiles all the prediction steps and some completion steps in Earley 's algorithm, and has resulted in improved average performance (19] while maintaining the cubic time complexity and square space complexity of Earley 's algorithm. Tomita [21] generalized Knuth's LR(k) parsers to allow nondeterminism, and reported faster performance than Earley 's parser. However , in the worst case, as different researchers [23, 7] have pointed out, (G)LR parsers have exponential time and space complexity. By utilizing a lookup table for reduce steps in a GLR parser, Kipps [9] was able to limit the time complexity to O(n 3 ), but this approach led to a less practical system. Nederhof and Satta [14] further improved the GLR parser through splitting a reduce step into more primitive operations that could remove *The author is currently with Intel China Research Center, No.l GuangHua Road, Beijing 100020, China. His new e-mail address is fu liang.weng@intel.com. redundancy in parsing. To deal with real-world natural language parsing, others [18, 12, 24] proposed ways to handle extra-grammatical cases for G LR parsers. As reported in this paper, we have continued our work in grammar partition and parser composition [25] that explores advantages of different parsing algorithms. We have also inves tigated guidelines for grammar partitioning, the relationship between partitioning and state space selection, and practical ways for composing different parsing algorithms, in the context of robust parsing of Chinese and English queries from an ATIS corpus. In Section 2, we briefly review some concepts of grammar partitioning, and discuss a few guidelines for the task and their relationship with state space selection for the parsers mentioned above. Section 3 presents an improved parser composition algorithm. Section 4 describes some initial but encouraging results on parsing Chinese and English queries. The final section ptesents our conclusion and describes our future research directions. Grammar Partitioning To facilitate our discussion, we first briefly review the motivation and concept for grammar partition [25) . Then, we give some intuitive guidelines for the partitioning and their relationship with state space selection for the parsers analyzed by [13] . Motivation and Concept Earley [4] and Ukkonen [23] proved that the following artificial grammar G n is exponential for LR(k) parsers, and even for any right parsers, respectively : S -+ Ai ( 1 \ufffd i \ufffd n) Ai -+ a i Ai ( 1 \ufffd i f:. j \ufffd n) Ai -+ ai Bi I bi ( 1 \ufffd i \ufffd n) Bi -+ ajBi lb i (l \ufffdi\ufffdn) These results present a potentially severe problem for LR(k) parsers and their applications to natural language processing. However, as illustrated in [25] , if we partition the grammar into n sub grammars, parse each sub grammar separately, and combine subparsers for the sub grammars, we can obtain a satisfactory compromise in terms of speed and memory. From a practical point of view, in addition to the possibility of causing an exponential num ber of compiled states, sublanguages in a large parser may be described by pre-existing sub grammars using specialized parsing algorithms. Implementation and maintenance constraints may prevent integration of the sublanguage modules into a monolithic system. A framework for partitioning grammars and composing parsers of different types would be useful for the scalability of a parsing system. In the rest of this paper, we use the definitions in [25] for grammar partition. Intuitively, we partition the production set into subsets, and create subgrammars for the subsets, accordingly. The interaction among different subgrammars is through nonterminal sets, i.e., INPUT and OUTPUT, and a virtual terminal technique. The INPUT to a subgrammar is a set of nonter minals that were previously parsed by other subgrammars. The OUTPUT of a subgrammar is those nonterminals that were parsed based on this subgrammar and used by other subgrammars as their INPUT symbols. For every nonterminal A in the INPUT set of a subgrammar, there is an augmented rule A-+ vtA for that grammar, where vtA is called a virtual terminal acting as if it were a terminal [10, 24, 25] . A directed calling graph for the sub grammar set of G is defined as (V, E), where V is the subgrammar set and E = {(A, B)}, with the overlap of the OUTPUT of B and the INPUT of A being nonempty. It can be proved that the partitioned grammar will lead to the same recognizable language. It is straightforward to see that this grammar partition definition is more general than the one defined with respect to nonterminal symbol set [10] . Guidelines for Grammar Partitioning Despite the fact that grammar partitioning is desirable, it is not clear how it should be done in any general case. We describe properties that may help in understanding this subject. First, let us look at the two extreme cases: the coarsest and finest partitions. The coarsest partition is the grammar itself, and all the production rules are included in the set. The finest grammar partitioning is the set of all singletons with one exact grammar rule in each set. The INPUT/OUTPUT set members are the nonterminal symbols on the right-hand side/left-hand side (RHS /LHS) of the rules in these singletons. So, an inverse problem of grammar partitioning is the clustering of smaller subgrammars into bigger subgrammars based on calling relations among different subgrammars. Intuitively, we may want to create clusters that have frequent interaction within cluster members, but fewer interactions between members of different clusters. This will reduce the possibility of combining common prefix computations for highly ambiguous grammars. How ever, it may increase the chance of computing a common prefix repetitively. Our criterion is to form as big a cluster as possible within an affordable size limit. First we create a weighted graph describing the connections among different clusters. Its node set consists of all the clusters in the grammar. When no training data is available, its edge set and weights are obtained through the following procedure. For each pair of clusters, intersect one cluster's INPUT and the other cluster's OUTPUT, take the total size of the two symmetric intersections as the weight on the connecting edge, and create an edge between the two clusters only when the weight is above a certain threshold. When training data is available, probability weights on the edges can be computed based on the actual calls between the corresponding clusters, in a way, related to the grammar chunking approach [16] . For better cluster quality, we present a set of simple heuristics for the refinement of grammar clusters. There are two types of refinements, merge to form bigger clusters, and split to form smaller clusters. The merge operations are:  The split operations can be through removing the edges with low weights or the ones that lead to minimum changes with respect to its original graph using K ullback-Leibler distance. the current word wordi is adj acent to any word on the top of the graph-structured stack. This same idea can be used for the Earley parser, left-corner parser, and shift-reduce parser. That is, instead of moving from i to i + 1,: the new algorithm will look for the next adjacent symbol to the current node in the lattice. However, direct application of this idea would force topological sorting to be performed each time a virtual terminal is added to the lattice. In the new composition algorithm, we use a stack to store the initially sorted lattice input, and dynamically add newly found virtual terminals to the stack in such a way that the topological order is always preserved for the changing LMG. By doing this, we avoid performing a constant topological sorting required by the simplistic approach. In the original definition [25] , we duplicated multiple sub grammars for a partition element with multiple output nonterminals. However, it is not necessary to make such duplication for the purpose of parsing because we can easily overcome the difficulty in handling multiple output symbols. To do so, we let {O} }j= 1 be the output of a partition element Gi. If we add rule set { S i --+ OJ }j =l to G i , use Si as its start symbol for partition element G i , and label the output node in the lattice with corresponding O} during parsing, the correctness of derivation is still guaranteed. In the rest of this paper, without loss of generality, we can safely adopt the more general definition. We have developed a new composition algorithm for parsers with different subgrammars, given a word lattice as its input. Without loss of generality, we use GLR parsers. Other parsers, such as Earley 's parser or the shift-reduce parser, can be used in combination. Let the word lattice be L = (N, E) , where N is a node set, indicating spatial positions, and Ei s a set of directed transitions, indicating the ranges the (virtual) terminals cover . Functions start(dt), end(dt), and word(dt) return the start, the end, and the word label of directed transition dt, respectively. Let u be the stack that stores the topologically sorted transition sequence from L. Suppose that psri is the parser for sub grammar Gi , psr0 is the parser for the top grammar G o , and the largest subgrammar index is J{. Each psri takes a (virtual) terminal stack u as input (call by value), and returns a list of pairs with form (v, d), where di s the end node for virtual terminal v of subgrammar G i if parsed successfully. If no parse is found, it returns 0. Parser Composition Al g orithm: This algorithm replaces PARSE() and PARSEWORD() in Farshi's version of Tomita's recognition algorithm [15] . In this algorithm, ACT OR(A, Q, w), COMPLETER( R, f), and SHIFTER( Q, f) are similar to that version, with a few minor changes: w, the to-be-processed word, replaces ai+ 1 as the next word for processing in ACT OR; U j , the state set at node j in L, replaces Ui+1 as a state set next to Ui when w corresponds to the transition from U i to U j . Ui 's and L are shared by all the parsers. However, graph-structured stack r and shared-packed forest are private to each parser. SP is a list of triples (pid, dt , u) , where pid is a parser id, dt is the transition being processed, and u is the sorted transition stack. SP records all the parsers started at a particular location in L to avoid infinite recursion. Notice that function PREDICTIVE-SUB-PARSING only adds virtual terminals to the stack and lattice, and these virtual terminals have the same start node as the transition being pro cessed. Therefore, the updated stack still maintains the topological order for the updated lattice. This leads to a straightforward correctness proof of the algorithm. In PREDICTIVE-SUB-PARSING, function PVT(Ui , w), serving as a subparser filter, has not been defined. The occurrence of PVT(Ui , w) in the algorithm is not essential for its cor rectness. However, without PVT it would start multiple parsers at every node in the lattice, that is, a big efficiency concern. Our technique for addressing this problem resembles predic tive calling of sub parsers in the strict top-down composition algorithm [25] , or FIRST in the conventional parsing table generator. Let VT(i) = { vt li is a state, vt is a virtual terminal, 3s, shift s E ACTION(i, vt)}. The predictive set of virtual terminals for a given state i and a (virtual) terminal w is defined as PVT(i, w) = {vt jvt E VT(i) ,w is a (virtual) terminal, VT(i),vt \u21d2 w ... }. To overload the notation a little bit, we define the predictive set of virtual terminals for a set of given states Q and a (virtual) terminal w as: PVT(Q, w) = LJie Q PVT(i, w). The algorithm to realize the definition is straightforward, and therefore omitted. Intuitively, for any state in Ui , if there is a shift s step under a virtual terminal for that state in the ACT ION table of the caller parser, we add the virtual terminal to PVT(Ui , w). Those virtual terminals that do not have w as their left corner can be pruned. For other types of parsers, we can also compute a set of virtual terminals, given a nonterminal ( or a set of non terminals) and a left corner, similar to the way PVT or FIRST is computed. Thus, we can avoid invoking subparsers excessively. In addition to predictive pruning, parallelization is an approach to the efficiency problem. Notice that all legitimate subparsers in PVT for a particular transition in the lattice can be parallelized without affecting other parts of the algorithm. To address both parsing accuracy and efficiency simultaneously, we can incorporate prob abilistic models into lattice. Estimation of ngram word transition models is standard. In our lattice framework, head words can be passed to virtual terminals for more detailed probabilistic estimation to better capture certain . long distance dependency without lexicalizing the gram mar. The latter approach leads to a huge grammar in any practical system [6] . However, a potential sparse data problem for the introduction of lexicalized virtual terminals can be partially addressed by the relatively easy-to-control granularity of grammar partitioning. To address the robustness for real-world applications, the techniques used in the EGLR parser [24] can be adapted in the lattice framework, because, in the EGLR parser, the three editing operations (insertion, deletion, and substitution) can be applied to both terminals and virtual terminals. Modifying input is equivalent to adding transitions in the lattice [11, 26] . The only additional issue is that the cost function for the optimal path selection may not be a uniform unit cost, but should be associated with the number of words changed when virtual terminals are in the editing operations. Experiments Ideally, we want to test grammar partitioning and parser composition in its full scale. However, before the full implementation of the algorithms described above, we took a shortcut to evaluate the feasibility in a real-world natural language application. We describe a concrete partition of Chinese and English grammars in an AT IS domain, and a robust cascading parsing and composition. We then report the parsing and understanding results for the ATIS domain. Cascading Parsing for the ATIS Grammars English ATIS tasks contain queries regarding air travel information, including flights leaving from or arriving at locations at various dates and times, services of certain flights, and airline routes. The Chinese queries used in the experiments are translated from an English AT IS corpus. Based on the guidelines for grammar partitioning, we manually partitioned the rule set into subgrammars, such as STATE-NAMES, CITY-NAMES, AIRPORT-CODE, with corresponding virtual terminals prefixed with vt. In this particular case, the calling graphs of the subgrammars are acyclic and, therefore, they can be assigned with level indices (ids). Their corresponding subparsers are GLR parsers, compiled separately, and assigned with the same ids. The top level sentence sub grammar is not yet completed and deployed, because of the flexible word order problem in Chinese and many variations of sentence structures in English. However, the LMG representation allows a robust parser to compose all the legal sequences of virtual terminals and choose the best path that covers the most words in the input string. The whole parsing process proceeds as follows. The control mechanism takes a query sen tence and converts it into an LMG. Then, the subparsers at the lowest level are activated to parse the LMG, and leave their corresponding virtual terminals on the LMG when parsing succeeds. If no more virtual terminals can be added to the LMG, the subparsers at one level higher are activated and start to parse the same LMG 3 \u2022 This process continues until the highest level is reached, when the robust sentence-level parser finishes its job of determining the best virtual terminal sequence. For the semantic evaluation described in subsection 4.2, a semantic interpreter converts syntactic virtual terminals into semantic frames of key-value pairs. The main difference between the parsing composition algorithm described in Section 3 and the one used here is the adoption of a viterbi-style robust master parser instead of a GLR parser. This gives us a flexible way to deal with extra-grammatical queries even without any sentence-level grammar rules. As a consequence, it also derives a preliminary set of sentence level grammar rules for future refinements as we will see in Section 4.2. On the other hand, when both predictive pruning and robust handling are integrated and tuned in a GLR parser, it can replace the current one without affecting the other components, which is the desired feature of the approach described in this paper. Experimental Results Our experiments use the English ATIS-3 corpus and its translated Chinese version, specifically the Class A queries. It contains 1564 queries from the training set, 448 queries from the 1993 test set and 444 queries from the 1994 test set. Each utterance is labeled with a corresponding SQL query for information retrieval. For the Chinese experiment, the non-sentence level grammar rules used for parsing were developed based on 375 queries from the MIT subset of the training set, and the remaining 1189 queries of the training set were held for tuning. When parse errors occur in any of the held-out sentences, we modify the grammar rules to incorporate the changes. Evaluations were conducted on 1993 and 1994 test sets. Table 1 shows the general statistics of the hand-craft ed grammar. As we can see from the table, the majority of the rules (825 out of 1013) are directly related to the lexicon, and only about 178 rules are related to phrase structures. Applying the parsing algorithm described in Section 4.1 and the above grammar on the training set and the two test sets, we obtained the results shown in Table 2 , where the error rates are measured against the Chinese translation of the standard semantic key-value pairs supplied in the AT IS-3 corpus by Linguistic Data Consortium. Fu ll understanding refers to utterances with full matches between the semantic categories in our frame and , those in the standard answer with no error. Part. understanding refers to partial matches with an error rate between zero and one. No understanding refers to no matches. In addition to the incomplete grammar coverage, parsing errors can have sources in trans lation errors or implicit information. The former are mainly caused by wrong or imprecise translation from English to Chinese. The latter are due to a discrepancy in the semantic meaning translation between the standard answers and our semantic interpreter. For example, tonight has semantic meaning of time 2: 1800&time \ufffd 2359, while our interpreter has only a symbolic value. We are developing a correction for this discrepancy. We also summarized the parsing statistics in Table 3 . From the table, we observe that the speed of our parser is very fast for this application. In addition, the number of rules reduced and states visited in the best paths and in the successful sub-parses are less than those in the overall parsing process, which means that further pruning is possible. Because the current master parser does not use explicit sentence-level grammar rules, we do not have direct parsing table size comparison between partitioned and unpartitioned grammars. However, as an approximation , we can take as the sentence level rules those best paths from all the training data that occur more than once. In the Chinese experiment, we obtained 205 such rules, in addition to the original 1,013 lower level rules. For the unpartitioned grammar, the total number of GLR states is 10,395, while for the partitioned grammars, the total number of GLR states is only 2,825, out of which 852 states are used by the sentence-level subgrammar. To speed up the English experiment, we constructed English lower level rules based on the Chinese rules and repeated the same processes for parsing and for obtaining English sen tence level rules as we did for Chinese. English parsing results are listed next to its Chinese counterpart in Tables 1 to 3 . There are additional 198 sentence level English rules. For the unpartitioned grammar, the total number of GLR states is 10,233, while for the partitioned grammars, the total number of GLR states is only 2,128, out of which 671 states are used by the sentence-level subgrammar. Both Chinese and English experiments show that, at least in the ATIS domain, grammar partitioning leads to the desirable effect of reducing parser sizes. Conclusions and Future Wo rk We have presented guidelines for grammar partitioning through a set of heuristics, and proposed a new mix strategy parser composition algorithm based on a flexible LMG representation, branches of a computation. This approach is attractive but fairly complex to implement, and does not solve one of the main problems, which is the debugging of a single monolithic grammar. In this pa per, we present a modular parsing architecture that attempts to address some of these problems. The architecture is derived from ideas developed in the Tipster project [ Using a centralized data structure has several adva ntages: a chart gives a precise idea on the state of the system at any point in the computation , and integrating components to build larger systems become easy since all components implement the same interface. In this architecture , all parsers operate in a bottom-up fa shion on the same data structure: the input of a parser is a chart, maybe reduced to a linear sequence of tokens, and its output is also a chart, where active edges have been removed ( and possibly edges representing sub-constituents). To operate, the first parser obviously needs a component that can read the input text and convert it to a chart. It is also possible to specify that when a rule successfully applies, its constituents can be erased from the graph. This is used in the Persian system for example when parsing complex predicates with auxiliaries. This grammar, although written using an extended context-free formalism, is actually a regular deterministic grammar. Therefore, when an auxiliary can be integrated in a larger constituent, we can be sure that this auxiliary will never be needed to form another constituent, and we can delete the corresponding edge from the graph. This does not influence the correctness of the result but results in a reduced number of edges that have to be examined by the parsing algorithm, and reduces the number of useless constituents that will never be integrated in the final result (spurious intermediary constituents). Reducing the number of edges has also a direct benefit in terms of debugging, since the chart becomes less cluttered with useless information. The system also includes a component that removes all edges that do not belong to a shortest path in the graph, thereby implementing a maximum coverage heuristics. This component can be used instead of the erase facility that is local to a rule, in particular when all intermediary results are needed to ensure completeness. Each parser is created as an instance of the component Parser such as in the following definition: A parser's output is not an edge or a set of edges covering the whole input sentence but a chart containing all edges built by the parser that belong to a particular type. The measure of parsing success is therefore not the recognition of a single edge spanning the input, but the production of a graph, preferably connected, where edges belong to a success type. Thus, a parse is not a boolean answer anymore, but a graded result where graph connectivity, graph topology and edge type play a role. This approach proves extremely useful in building robust NLP systems which performance degrades gracefully on 'ungrammatical' input [Zajac 99b ]. Since input and output are of the same kind, parsers can be chained together, the output edges of one parser becoming the input to the next parser. In the Persian system for example, there is one 2 sub grammar used to analyze complex verbal predicates ( conjugated forms with auxiliaries). For the next level of parsing, each complex verbal predicate is then viewed as a single word and sub-constituents of a complex predicate are erased from the chart. A rule is specified in the feature structure notation and each syntactic element follows the general feature structure syntax. Although this makes it sometimes a little bit awkward, it allows to compile rules as feature structures which are themselves compiled as compact arrays of integers 3 and enable It can be used to order the application of sub-grammars. For example, adjectival phrases must be built before noun phrases , adverb phrases before adjectival phrases, and so on. At the higher level, level 3 constituents include about any other constituent, but at lowe r levels, for example adverb phrases , level 3 involve mostly or solely coordination of constituents of the same category. In a classification of constituents into adve rb phrases, adjective phrases , determiner phrases, noun phrases, ve rb predicates, ve rb phrases , and clauses, a complex monolithic grammar could be ideally be decomposed into 28 sub-grammars, each of them describing a we ll defined set of constructions. The grammar writer could then define a parser for each of these grammars , define classes of input/output using a set of corresponding tags, and test separately each sub-grammar in turn. Additionally, after most of grammars from level 0, 1 and 2, it is possible to remove sub-constituent edges from the chart since level O and level 1 constituents are typically not ambiguous { contrary to level 3, where we find the traditional PP attachment problems and the likes ), thereby reducing the number of hypotheses that the parsing algorithm needs to consider. Edge reduction has also the advantage of fac ilitating the inspection of the chart for debugging purposes. Conclusion The MEAT parsing architecture has been developed within the Corelli project at CRL. The system has been implemente d in C++ and is used to deliver applications running on Unix and Windows platforms. Strings are internally represented using Unicode, and a set of Unicode converters is also included in the system. The system has been used for developing the Shiraz Persian-English machine translation system (see http://crl.nmsu.edu/shiraz/) and for the Turkish-English MT system developed within the Expedition project. Situation: Guidelines for an Actual Constraint-Based Approach Modern linguistic theories often make use of the notion of constraint to represent information. This notion allows a fine-grained representation of information, a clear distinction between linguistic objects and their properties, and a better declarativity. Several works try to take advantage of a constraint-based implementation (see for example [Maruyama90] , [Carpenter95] , [Duchier99] ). However, the parsing process cannot be interpreted as an actual constraint satisfaction one. This problem mainly comes from the generative conception of grammars on the linguistic analysis. Indeed, in constraint based theories, constraints can appear at different levels: lexical entries, grammar rules, universal principles. However, during a parse, one has first to select a local tree and then to verify that this tree satisfies the different contraints. This problem comes from the generative interpretation of the relation between grammars and languages . .In this case, the notion of derivation is central and parsing consists in finding a derivation generating an input. We propose then a new formalism called Property Grammars representing the linguistic information by means of constraints. These constraints constituting an actual system, it becomes possible to consider parsing as a satisfaction process. An optimal use of constraints should follow some requirements. In particular, all linguistic information has to be represented by means of constraints. This information constitutes a system of constraints, then all the constraints are at the same level and the order of verification of the constraints is not relevant. Encapsulation is another important characteristics which stipulates that a constraint must represent homogeneous information. The last important point concerns the notion of grammaticality. In the particular problem of parsing, finding an exact solution consists in associating a syntactic structure to a given input. In the case of generative approaches, this amounts to finding a derivation from a distinguished symbol to this input. However, the question when parsing real natural language inputs should not be the grammaticality, but the possibility of providing some information about the input. We propose then to replace the notion of grammaticality with that of characterization which is much more general: a characterization is the state of the constraint system for a given input. Property Grammars Property Grammars ( cf. [Blache99] ) provide a framework implementing these requirements: declarativity, encapsu lation, satisfiability. We use for the representation of syntactic information 7 properties defined as follows : \u2022 Constituency (noted const) Set of categories constituting a phrase. \u2022 Obligation (noted oblig) Set of compulsory, unique categories (heads) . \u2022 Unicity (noted uniq) Set of categories which cannot be repeated in a phrase. \u2022 Requirement (noted \u21d2) Cooccurrency between sets of categories. \u2022 Exclusion (noted \u00a5>) Restriction of cooccurrence between sets of categories. \u2022 Linearity (noted -<) Linear precedence constraints. \u2022 Dependency ( noted \"-+) Dependency relations between categories. It 1s interesting to notice that properties can be expressed over sets of categories, allowing then to represent contextual information. Parsing as Constraint Satisfaction The parsing process (1) takes as input the set of elementary trees (in fact unary quai trees, as described in [Blache98]) that can be associated to the sentence and (2) builds all the characterizations of this input. A characterization is then defined over a set of categories by p + (the set of satisfied properties) and p-( the set of unsatisfied properties). The following example describes a rammar of the NP in french. Properties of the NP: (1) Const = {Det, N, AP, Sup} (2) Oblig = {N, AP} (3) N{com} \u21d2 Det (4) Det --< N (5) Det --< AP (6) Det --< Sup (8) N--< Sup (9) AP \ufffd Sup (10) Det ._ N (11) (3) Adj \u21d2 Det (4) Adj \u21d2 Adv (5) Det --< Adv (6) Det --< Adv (7) Adv --< Adj (8) Det ._ Adj (9) Adv ._ Adj The following example presents the elementary trees associated to the words of the noun phrase le livre le plus rare {the rarest book). The general approach consists in characterizing all subsets of categories belonging to the set of elementary trees, whatever their level. Con sidering all the possible subsets of categories has an exponential cost and some simple controls can be applied. In the following, we will only take into account the sets of juxtaposed categories. Building the characterizations consists for each subset of categories of verifying the satisfiability of the properties. Recently there has been an increasing interest in finite-state techniques for NLP. Finite-state trans ducers have been used for a number of NLP tasks, from morphological analysis and shallow parsing to semantic processing. In [3] we have proposed a parsing approach for Information Extraction (IE) based on finite-state cascades. The parser aims at building a syntactic representation equivalent (from an IE point of view) to the ones built by systems based on full parsing. The approach is inspired by the work of Abney on finite-state cascades for parsing unrestricted text [1] and provides: (i) a method for efficiently and effectively performing full parsing on texts for IE purposes; (ii) a way of organizing generic grammars that simplifies changes, insertion of new rules and integration of domain-oriented rules. Parsing is based on the application of a fixed sequence of cascades of rules. It is performed deterministically in three steps: (1) chunking; (2) clause recognition and nesting; (3) modifier attach ment. NP NP Sup NP Sup Grammar Organization fo r Cascade-Based Parsing in Information Extraction The approach has been developed as part of LE-FACILE, a successfully completed EU project for multilingual text classification and IE [4] . The approach is currently used in Pinocchio, an environment for developing and running IE applications [2] . The proposed approach has been tested mainly for Italian, but proved to work also for English and partially for Russian. Due to space limitations, in this paper we concentrate exclusively on the issues connected with grammar organization. For further details on the adopted formalism, the parsing approach, and some experimental results, see [3] . Rules are grouped into cascades that are finite, ordered sequences of rules. Cascades represent elementary logical units, in the sense that all the rules in a cascade deal with some specific construction (e.g., subcategorization of verbs) . From a functional point of view a cascade is composed of three segments: s1 contains rules that deal with idiosyncratic cases for the construction at hand; s2 contains rules dealing with the regular cases; s3 contains default rules that fire only when no other rule can be successfully applied. The initial generic grammars for chunking and clause recognition are designed to cover the most frequent phenomena in a restrictive sense. Additional rules can be added to the grammar (when necessary) for coping with the uncovered phenomena, especially domain-specific idiosyncratic forms. The limited size of the grammar makes modifications simple (e.g., our clause recognition grammar for Italian is composed of 66 rules). The deterministic approach combined with the use of cascades and segments makes grammar modi fications simple, as changes in a cascade (e.g., rule addition/modification) influence only the following A finite-state parser with dependency output Finite-state parsing and dependency grammars both offer advantages to implementers of practical NLP systems. Finite-state parsers have good computational properties, typically O(n 2 ) in the num ber of wo rds if the finite-state machines are determinised. Finite-state grammars can also be partly modularised by using cascading (Abney, 1996) , and can be implemented so as to tolerate ungrammat ical input. Dependency grammars, on the other hand, allow a pseudo-semantic analysis, by extracting relationships such as modification between words. In general, dependency grammars can be imple mented with O(n 3 ) complexity, although ungrammaticality and certain constructs can lead to NP complexity (Neuhaus and Broker, 1997) . We have developed a simple technique for making a finite-state parser produce dependency struc tures. As well as outputting the phrase bracketing, the parser also stores words in \"variables\", in response to annotations in the grammar rules. Variables may be unindexed, storing lists of words, or indexed, providing a mapping from words to lists of words. Indexed variables stand for dependency relations ; unindexed ones contain non-dependent elements such as the overall head. Each variable has a name, corresponding to the type of dependency relationship . Thus the dependencies for a big brown dog barked at the man could be represented as /3* . 0 1'}, iff a= f3*.')'; {0/3./3*.1'}, iff a= 13 + .')'. Here also NEXT is recursively computed as long as f3 ( and 1' for /3*. 0 1') starts with an opening bracket in order to reach the Gorn addresses to be finally returned. Traversing the regular expressions replaces the analysis of branches in the six procedures of the TA G parser by Schabes in a straight-forward manner. Fo r more details of the extension of the individual procedures of the Schabes parser see (3] and (8] . With the close relation to ordinary TAGs and the claim that direct parsing is advantageous, the question arises how can existing TAGs made available to the direct parser? Obviously, an arbitrary TA G G can trivially be transformed into an S-TAG G' by annotating the concatenation of all daugh ters from left to right at each inner node of each elementary tree. This transformation involves no compression and accordingly, no benefit of a direct S-TAG parser arises. Therefore, we -have devel oped a transformation component which produces a S-TAG where each label at the root node occurs only once in the set of initial and auxiliary trees. The following steps lead to this goal: Firstly, in all elementary trees all subtrees which do not contain the foot node are rewritten by substitution in order to find shared structures. Some reformulations become necessary \u2022 to prevent the grammar from overgeneration because existing substitution trees with the same label would become applicable unintentionally. Accordingly, the corresponding trees are renamed. Now, all alternatives with the same root node are collected and described by a regular expression denoting the set of all these trees . Finally, the resulting RXs are condensed by applying the distributivity law (a 'Y (l lk ) 'Y f3 = O'.')' (l lkH ) {3, a(1'8 1 )/3 + ... + a(1'8m)f3 = 0'.')'(8 1 + ... + Om)/3 and 0'.')'/3 + a/3 = O'.')' (O l l ) /3 where a, /3, 1', 81, ... , O m are arbitrary complex RXs). This procedure has been applied to the XTAG system (1] and the trans formed grammar serves a syntax grammar for analysis and generation as we run our direct parser as a reversible uniform generation modul ( cf. (3] and [4] ) . Currently we transform existing semantic and pragmatic knowledge sources into the S-TAG formalism .  The problem of finding a least cost parse is similar to that of fi nding the most probable parse. Our implementation is based upon Stolcke's extension of Earley's algorithm to probabilistic parsing [3] . It is necessary to restrict our cost expressions to be non-negative monotonically non-decreasing functions of their inputs; this permits a prun ing strategy that removes items from the parser's states. The essence of the parsing method is to extend each item in an Earley parser state with two extra compo nents.An item, therefore, has this structure: [ A \u2794 a \u2022 (3 ; @n ; C ; @r ] The n component refers back to the anc\ufffdstor state where the parser started matching the rule A\u2794a[3. The C com ponent is a vector of costs, one per symbol in a. The r component is called the least cost predecessor. It records the identity of a predecessor item in the same state which generated this particular item. The completer step of the parser is responsible for computing the cost of the completed rule based on the input values in C. It must then consider whether to add a new item to the current state where the marker has been ad vanced past the non-terminal A. If a similar item ( one with the same rule, same dot position and same ancestor state) already exists in the current state, the monotonicity property of the ancestor rule's cost expression can fre quently be used to choose only one of the two similar items to retain. The least cost predecessor in the new item is linked to the item containing the completed rule, and may be used to construct the least cost parse after all the input has been processed. The worst-case time complexity of the parsing method is exponential. We note, however, that if the grammar can be transformed so that each righthand side contains no more than two non-terminals, the pruning strategy would be totally effective and the parsing efficiency will be the same as Earley's method. Such a transformation would come at the cost of reducing the expressivity of the cost formulae however. As mentioned before, synchronization is also required to store results in the chart . Letting threads wait unconditionally for exclusive access incurs too much contention. We solve this problem by splitting all tasks in distributed and sequential parts. Instead of contending for chart access, a thread simply queues the sequential work 4 and proceeds with other work. The scheduler [9] ensures that at most one processor is processing sequential work, automatically providing synchronization . A final optimization improves on the locality. Our parser allows synchronization of the chart per grammar rule. By associating each grammar rule-and its data-with a single processor, we can improve caching behavior. This does not compromise on the ability to balance load, because assigned work may still be stolen . In general, the scheduler has been designed to move most overhead to the stealing side, leaving minimal overhead for normal operation . Experiments confirmed that work stealing indeed yields a well-balanced distribution of work. On a 4-thread setup, 6% of the total time was consumed by idling and 6% could be attributed to scheduling overhead. 5 Work stealing proved to be necessary, as omitting it yielded 20% idling for a 2-thread setup (rapidly increasing for more processors) . The solution to reduce synchronization proved effective as well . Only 1/2000 to 1/5000 synchronizations per unification were required on a 2-processor run . The ratio of running time of the sequential version and the I-processor parallel version Ti / //flwas 1.03, yielding minimal overhead. Running a 2-processor version on a dual Pentium-II yielded a speedup of 1.7 (1.4 for a Pentium-Pro). Experiments verified that the gap in speedup between this result and the load balancing simulation was caused by communication of the (hardware) cache coherency protocol . This indicates that the bottleneck is caused by ineffective cache usage. Currently our research is focusing on improving cache utilization to mitigate the usage of memory bandwidth . In conclusion, the speedup that can be obtained depends on the specific grammar being used. Nevertheless, the presented scheduling method allows for the finest possible grain of parallelism, with out resorting to parallel unification . In addition, although originally designed for the parallelization of chart parsers, the presented technique can be applied to any tabulation programming technique. The Partial Parser for Greek The model of FSTs with constraints has been employed in the creation of a partial parser for Modern Greek as a three level FST cascade. The first level locates and groups consecutive adjectives, as well as other auxiliary word groups, in order to make the task of subsequent levels easier. The second level performs the major part of the noun phrase recognition task. It resolves all types of noun phrases, without incorporating clitic forms of pro nouns. This is due to the curious role of ditics in the noun phrase of Modern Greek. This level incorporates \"stray\" clitics (i.e., ones that are after a noun phrase and do not constitute a noun phrase by themselves) into the preceding noun phrases. In the input text stream there are two types of ambiguities to be resolved: at the tag level and at the fe ature level. Resolution of a tag ambiguity takes place when a pattern matches only one of the tags assi gn ed to the word. Ambiguities at the feature level happen when a symbol is accompanied by more than one fe ature tuples. These ambiguities are resolved during the transitions, since the FST actually calculates the intersection of tuple sets for consecutive symbols inside the sentence. The result of the intersection contains the disambi gu ated values of the fe atures for both the word group recognized by the FST and the words themselves. Each transducer of the parser has been implemented as a lex [8] program, and translated with flex, the imple mentation of the tool available for many platforms. The partial parser has been incorporated and used in a machine learning application for the identification of named entities in Greek business texts [9] . gether as a single set, the accepting capacity of the systems equals the accepting power of deterministic pushdown automata. In other words, the systems collapse to RC-uniquely parsable grammars. When local level is considered, where conditions apply tQ all rules of each component independently, for some more restrictive classes, one gets more computational power keeping the parsability without backtracking. We give a simple recognition algorithm for these systems. Our algorithm has two distinct phases: one in which we find the unique component which is to become active and the other one in which a t-leftmost reduction is performed by this component. The former phase is based on the algorithm proposed in [1] for solving the multiple pattern matching problem. The only difference is that the searching process is stopped as soon as a matching pattern has been found in the text. The text is the sentential form while the dictionary of matching patterns is formed by the words in the lefthand side of all rules of the given uniquely parsable grammar system. For the latter phase we define a procedure which provides the word obtained by a t-leftmost re duction of the current sentential form in the component found in the first phase. Then, this process is resumed. A criterion to detect infinite loops in the derivation process is also presented. When no reduction is possible anymore, we check the sentential form and decide whether or not the input word is accepted. The exact complexity of this algorithm is briefly discussed. This technique was evaluated in a chart parser with unification, left-corner and look-ahead con straints, among other features. We used a large-scale English grammar for machine-translation of heavy equipment manuals [7, 51 , and a test-set of 2524 sentences (22,558 words) . Without any spe cial restrictions, when compared to the na'ive implementation, the tree-structured grammar reduced the number of active arcs created by 23%, and when employing full left-corner and look-ahead con straints [11, 8, 4] on the parser, the tree-grammar gave a 40% reduction. not only unification, but also any other operation that can be defined in terms of the underlying logic programming language can be performed on linguistic descriptions. The advantage of this approach is twofold: (i) procedures of any complexity can be encoded, while keeping a single control structure driven by a declarative grammar; (ii) any number of additional knowledge sources can be separately encoded, while being all accessible at parsing time. Modularity. One of our design guideline was to develop a multi-functional grammar: we wanted to be able to use the same grammar for both ordinary parsing and error detection. This can be achieved by defining attachments in a consistent way. Whenever a feature is relevant to error detection, any check on its value is done in some suitable attachment to the relevant rule, instead of the rule body. Moreover, each predicate is systematically defined as a disjunction: one clause performs whatever checks and actions are needed for error detection, the other one performs whatever standard unification is needed for ordinary parsing. The two clauses of each predicate are mutually exclusive: the former is only used when error detection is needed, the latter when only ordinary parsing is. To this end we define environment variables, again in the form of definite clause, which we use as system parameters. All violation-related attachments are uniformly handled in three stages: (i) A test on the relevant features, where violations are detected; (ii) A selection of the action to be taken, where detected violation are interpreted, in terms of some grammatical configuration; (iii) The execution of the se lected action, where a grammatical configuration is mapped onto an appropriate label to be added to the list of violation or non-violation, respectively. This processing scheme allows one to decouple constraint relaxation from error ( or non-error) flagging, which is based on a number of other factors, independently handled. If the system were ported to a different domain, the use of a different in ventory of violations in the error flagging phase would not necessarily involve changes in the error detection phase. Different error inventories can be alternatively used by setting an appropriate system parameter. Efficiency. We implemented a mechanism that allows user-defined control strategies on parsing. A control strategy is defined via a definite clause, which takes as arguments a linguistic description and a priority value. Given a linguistic description, the clause assigns a numeric value, representing the description's priority in the parsing agenda. Edges in the parsing chart are retrieved from the agenda and asserted according to the priority value associated to them by the priority clause. In this way, parses are obtained in the order defined by a user, according to some relevant criterion. Different parsing strategies can be defined and the system can be switched between them, again by means of system parameters. For example, we use different parsing strategies depending on whether error detection or ordinary parsing is performed. In the proposed architecture, a single control structure represented by a declarative unification grammar can be integrated with any number of further knowledge sources and procedures of any complexity, and provided with means to define a parsing strategy based on any information available. The result is a modular integrated system whose behavior can be changed between different modes by simply setting some general system parameters. Proceedings of Third Conference on Empirical Methods in Natural Language Acknowledgements This work was partly funded by NSF grant IRI-95023 12. Acknowledgements This abstract presents ongoing research in collaboration with Jason Eisner, University of Rochester, and Mark-Jan Nederhof, DFKI. The author is supported by MU RST ' under project PRIN: Bioln fo rmatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sistemi ad Addestramento Automatico per l'Analisi del Linguaggio Naturale. Acknowledgements This research is supported by the Basque Goverment, the University of the Basque Country an d the In terministerial Commision for Science an d Technology (CICYT). Thanks to Gorka Elordieta for his help writing the fin al version of the paper. Acknowledgements We would like to thank Pierre Boullier, Patrice Lopez and Mark-Jan Nederhof for fr uitful discussions. This work was partially supported by the FEDER of EU (project 1FD97-004 7-C04-02) and Xunta de Acknowledgements This wo rk was supported by NSF grants #SBR-97 104 11 and #GER-93 54 869. (Charniak, 1996] References Acknowledgement The author wo uld like to thank the three anonymous reviewers for their valuable comments on an earlier version. This article was supported in part by Engineering and Physical Sciences Research Council, UK, Grant No GR/L81406. Acknowledgements Laura Mayfield Tomokiyo wrote the Scheduling grammar, Donna Gates and Dorcas Wallace wrote the Travel grammar; Detlef Koll programmed the methods that read in a JSGF RuleGrammar. Klaus Zechner, Matthias Denecke and three anonymous reviewers gave helpful comments to earlier versions of this paper. Acknowledgements Thanks to Ed Stabler and Crit Cremers for inspiring discussions. Acknowledgments We Acknowledgement This work has been partly supported as part of the VERBMOBIL project ( contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology. The authors would like to thank Franz Josef Och for his fruitful discussions and Richard Zens for his support in performing the tests with the m-gram LMs. We have proposed the algebraic construction of tabular parsing algorithms for LIG as an alterna tive to the automaton-based approach proposed in recent papers (1, 5]  because it allows to derive LIG algorithms from CFG algorithms by means of algebraic transformations , allows sim. pler and more ele gant correctness proofs by using general theorems, and is no t restricted to left-right parsing strategies. Furthermore , it makes the no tion of parse items more precise and thus adds to a better understanding of parsing schemata. Acknowledgments Part of this work was done while the first author was visiting the Chinese University of Hong Kong. This work is mostly supported by the Direct Grant from the Chinese University of Hong Kong, and, in part, supported by SRI International, under Mediated Space project. Acknowledgments The Acknowledgements Financial support was received from the Natural Sciences and Engineering Research Council of Canada. Acknowledgements The work done by the second author has been supported by the Direcci6n General de Ensefianza Superior e Investigaci6n Cientifica, SB 97-00110508. Acknowledgements This work was supported by the CMU Language Technologies (p, q, (n , O'R ) (p, q, head(aL), tail(aR)) Ta ble 4: Inference rule for end of connected route. Wr apping adjunction case 1 -1111111 p q r Foot node initialization: (p, q, O'L , O'R ) (q, r, O'L Capting CF factorizations: LT G sharing Lexicalization raises the problem of multiplication of the same substructures which can be serious. In CFG the same rule can be used for all possible parsing trees which contain the corresponding substructure , but in LT G this substructure is duplicated. Considering classical linguistics choices , polystructures (for example to speak to ... , speak about ... , to speak to ... about ... ) are very common. The corresponding elementary trees must share common substructures and therefore do not cost as much as an independant elementary tree for each. As chart parsing can reduce the complexity in n exploiting factorisation of chart items by their positions on the string and on the elementary trees , we can reduce the average complexity in G using compaction of common substructures because such com mon structures of elementery trees imply common actions of the parser. Fo r example [Nederhof, 1998] adresses this problem for LR parsing algorithm . [Evans and Weir, 1998 ] shares different substructures of elementary trees using FSA and classical minimalization techniques . As presented in [Evans and Weir, 1997] , the authors use automata corre sponding to one particular traversal of the trees . We use similar techniques to share here linearized structures between different elementary trees . The main differences are that , since it represents el- References Ait-Kaci, H. (1991). Warren's Abstract Calder, J. (1993). Graphical interaction with constraint-based grammars. In Proceedings of the 3rd Pacific Rim Computational Linguistics Con fe rence (pp. 160 -169). Vancouver, BC. Callmeier, U. (2000). PET -A platform for exper imentation with efficient HPSG processing tech niques. In {Flickinger et al., 2000). Carpenter, B. (1992). The logic of typed feature structures. Cambridge, UK: Cambridge Univer sity Press. Carroll, J. (1994). Relating complexity to practical performance in parsing with wide-coverage unifi cation grammars. In Proceedings of the 31st meet ing of the AGL (pp. 287-294) . Las Cruces, NM. Carroll, J., Copestake, A., Flickinger, D., & Poz nanski, V. (1999). An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of the7th European Workshop on Natural Language Generation. Toulouse, France. Ciortuz, L. (2000). Compiling HPS G into C. In prepa ration, DFKI, Saarbriicken, Germany. IJCA I 1991 (pp. 931 -937) . San Mateo, CA: Morgan Kaufmann Publishers. Copestake, A. (1992). The ACQUILEX LKB. Rep resentation issues in semi-automatic acquisition of large lexicons. In Proceedings of ANLP 1992 (pp. 88 -96). Trento, Italy. Erbach, G. (1991a) . An environment for experiment ing with parsing strategies. In J. Mylopoulos & R. Reiter (Eds.), Proceedings of Erbach, G. (1991b Wroblewski, D. A. (1987). Nondestructive graph uni fication. In Proceedings of the 6th National Con ference on Artificial Intelligence (pp. 582 -587). Seattle, WA. AN EFFICIENT LR PARSER GENERATOR FOR TREE ADJOINING GRAMMARS Carlos A. Prolo* Dept. of Computer and Information Science University of Pennsylvania Philadelphia, PA, 19104, U.S.A. prolo@linc.cis.upenn.edu Abstract The first published LR algorithm for Tree Adjoining Grammars {TAGs [Joshi and Schabes, 1996)) was due to Schabes and Vij ay-Shanker [1990]. Nederhof [1998] showed that it was incorrect {after [Kinyon, 1997)), and proposed a new one. Experimenting with his new algorithm over the XTAG En glish Grammar[XTAG Research Group, 1998) he concluded that LR parsing was inadequate for use with reasonably sized grammars because the size of the generated table was unmanageable. Also the degree of conflicts is too high. In this paper we discuss issues involved with LR parsing for TA Gs and propose a new version of the algorithm that, by maintaining the degree of prediction while deferring the \"subtree reduction\" , dramatically reduces both the average number of conflicts per state and the size of the parser. Introduction For Context Free Grammars, LR parsing [Knuth, 1965, Aho et al., 1986 ] can be viewed as follows. If at a certain state q 0 of the LR automaton, during the parsing of a sentence, we expect to see the expansion of a certain. non-terminal A, and there is a production A \u2794 X 1 X 2 X3 \u2022.. X n in the grammar, then the automaton must have a path labeled X 1 X 2 X 3 ... X n starting at q 0 \u2022 This is usually represented by saying that each state in the path contains a \"dotted item\" for the proq.uction, starting with A \u2794 \u2022 X 1 X 2 X 3 ... X n at q0 , with the dot moving one symbol ahead at each state in the path. We will refer to the last state of such paths as final. The dot being in front of a symbol Xi represents the fact that we expect to see the expansion of Xi in the string. If Xi is a non-terminal, then, before crossing to the next state, we first have to check that some expansion of Xi is actually next in the input. The situation is depicted in Figure 1 , where paths are represented as winding lines and single arcs as straight lines. At a certain state q1 , where some possible yield of the prefix a1 of a production A \u2794 a1 Ba2 has just been scanned, the corresponding dotted item is at a non-terminal B. This state turns out to be itself the beginning of other paths, like {3 in the picture, that lead to the recognition of some match for B through some of its rules. The machine, guided by the input string, could traverse this sub-path until getting to q4 \u2022 At this final state, some sort of memory is needed to get back to the previous path for A, to then cross from q1 to q2 (i.e. B has just been seen) . In LR parsing this GOT O a d j (q1, Q2,symb(n), k) = p' . The state corresponding to the empty set is called error. The domain of k, in practice, is bounded by the grammar: the maximum number of leaves of any node. As usual, if an entry contains more than one action, we say that there is a conflict in the entry. As expected, two shift actions are never in conflict. Although reduce and bpack actions do not actually depend on a terminal symbol for the current algorithm, in practice the table could be so constrained, either by having the algorithm extended to an LR ( l) version or by using empirical evidence to reduce/resolve conflicts. In our inductive definition, each state was associated with the closure of a (usually much smaller) set of items . This set prior to the closure is generally knqwn as its kernel. It is a property of our framework that an alternative definition whose states are associated with ke rnels would produce an automaton isomorphic to the one we defined above. .The Driver The algorithm for the driver presented below uses a stack. Let st, st 1 be stacks and el be a stack element . PUSH (el,st), TOP (st) and POP (st) have the usual meanings . POP (st, k) pops out the top k elements from st. EXTRA C T (st, k) pops out the top k elements returning another stack with the k elements in the same order they were in st. Its counterpart, INSERT (st1, st), pushes onto st all elements in st1, again preserving the order. null is the empty stack. size( st) is the number of elements in the stack st. The stack is a sort of higher order structure, in the sense that an element of the stack may contain an embedded stack. More precisely, an element of the stack is a pair (X, q), where q is a state of the parsing table, and X is either a grammar symbol or another stack. The algorithm for the driver also uses input, the sequence of symbols to be recognized . Two operations are defined over it : look, which returns the leftmost symbol of input or $ if input is the null sequence; and advance, which removes the leftmost symbol from input. Let ACTIONS, GOTO su b st , GOT OJoot, and GOTO adj be the four tables/functions for a grammar G. Let Qo be the initial state of the corresponding machine. Let input and the stacks stack, emb-stack be as defined above. The algorithm for the driver is then as follows. Evaluation of the Algorithm We show in Table 11 the results of the application of our algorithm as well as Nederhof's 5 to a recent version of the XTAG grammar with 1009 trees and 11490 nodes. conflicts is the average number of actions per pair (state,terminal) . reductions and bpacks are respectively the average number of reduce and bottom pack actions in a state. 6 transitions is the total number of transition entries, including shift's and goto's. The number of conflicts is drastically reduced with our algorithm. In particular the nasty early reduction( bpack) conflicts are decreased, improving the general quality of the conflicts that function GOTO adj would depend only on one state (q2 in Figure 6 ), a significant improvement in size. We are currently working on an extension that takes into account the lexical items of the lexicalized LTAG grammars while keeping the size of the parsing table manageable. The effort we made in reducing the size of the generated table is essential due to the expected scaling up with the inclusion of lexicalization (which has already been confirmed in preliminary experiments). The algorithm could be extended to have reductions dependent on lookaheads (e.g., an LALR(l) version), what is reasonably easy to do, that would not increase the number of states or transitions and would likely reduce substantially the degree of conflicts. The reason we did not do it is because we intend to use empirical evidence from annotated corpora to rank the conflict alternatives, for any given pair (state, lookahead), in the lexicalized version. Of course this subsumes the effect of the LR(l) version. Parsing Scrambling with Path Parsing Local Scrambling The constituent rules in this stage are simple CFG rules. A clause can be generated as a result of the combination of a clause and a constituent, or it can introduce a new embedded clause, or by reaching the end of sentence, a clause can be terminated. These automata do not specify the precedence relations between the constituents, and a separate Linear Precedence component imposes the precedence constraints. This is done incrementally and as a constituent is added to a clause, all the possible word order constraints are applied between it and the constituents which are already part of the clause. Note that we haven't considered any immediate dominance (ID) component and the binary precedence relations are not imposed on sisters of an ID rule. The system parses a sentence by initialising a clause and attaches the incoming chunks to this clause. For (2), repeated in (5), the first chunk is Ali. As a result of incremental attachment at this stage we will have: The candidates also show the competing paths in the path set for each clause. At the beginning when the clause is initiated this path set is empty and after parsing the first constituent, the candi dates ( or path set) will be initiated. It is at this stage that the activation values for each path will be calculated. We have assumed 1 00 as the initial number for an empty clause and when it is combined separately with 20 and 80, the results will be 44.72 and 89.44. 100 is the maximum value of activation and the activation value can range from O to 100. We have used the square root function because it implements blocking constraints as reduction by multiplication to zero. f is nullary we write f I-a instead of J() Ia. A nondeterministic algebra A is a pair (A, F) where A is a nonvoid set and F is a family of finitary nondeterministic operations. A partial algebra is a nonvoid set A together with a family of finitary partial operations on A, i.e., partial functions f : A n \ufffd A. The type of a (partial, nondeterministic) algebra is the function that assigns each operation its arity.    A homomorphism of A into B is a function h : A \u2794 B satisfying the condition: For all a 1 , ... , a n E A, if J A (a 1 , ... , a n ) Ia then J 8 (ha 1 , ... , ha n ) Iha. A homomorphism h of A into Bi s called strong if for all a 1 , ... , a n E A, for all b E B: whenever J 8 (ha 1 , ... , ha n ) Ib, then there is some element a EA such that J A (a 1 , ... , a n ) Ia and ha = b. We write A strong congruence relation of a A is an equivalence relation -:::. on A satisfying the condition: if f (a 1 , ... , a n ) Ia and ai -:::. a\ufffd, for i = 1, ... , n, then there is some a' E A such that a -:::. a' and f( a\ufffd, ... , a\ufffd) Ia'. The set of all strong congruence relations of A is denoted with Cgr(A). Strong congruence relations and strong homomorphisms of nondeterministic algebras are related as follows [6] : The kernel of a strong homomorphism is a strong congruence relation, and every strong congruence relation is the kernel of some strong homomorphism. Let -:::. be a strong congruence relation of A. The quotient algebra A/-:::. is the nondeterministic algebra (A/-:::., F) where the operations are defined through J A fr::=( a 1 r::=, ... , a n r::=) Ia r::= iff for some elements a\ufffd, ... , a\ufffd, a' E A: a\ufffd -:::. ai (for all i) and a' -:::. a and J A (a\ufffd, ... , a\ufffd) I-a '.  The following theorem describes the connection between homomorphisms and generated subalgebras of direct products: Algebraic Construction of Parsing Schemata In this section we present a formal definition of parsing schemata and describe the general scheme of the algebraic construction of parsing schemata. This scheme is completely independent of a particular grammar formalism or parsing strategy. Parsing schemata were proposed by Sikkel as a well-defined level of abstraction for the description and comparison of tabular parsing algorithms [7] . A parsing schema 2 is a deduction system (I, D) consisting of a finite set of (parse) items I and a finite set D of deduction steps written in the form x 1 , ... , Xn Ix (meaning x is deducible from X1, . .. , Xn) where n \ufffd 0 and x 1 , . (I, D ) is correct then a string w is in the language of Giff an item representing a G_ -derivation of w from the start symbol of G is valid. Let G be a grammar. An (augmented) tree algebra for G is a nondeterministic algebra Aa = (A, F) where A is a set of partial derivation trees augmented with some state information (that depends on the parsing strategy) and F is a family of (possibly nondeterministic) tree operations that depend on the grammar G as well as the parsing strategy. We assume that F contains at least one nu\ufffdlary operation. In the sequel we will assume that G is understood and write A instead of Aa . Let :Ebe an input alphabet. An (augmented) yield algebra is a partial algebra B = (B, F) where Bi s a set of strings from :E* augmented with some state information. A homomorphism g : A \u2794 B ( where A is an augmented tree algebra and B is an augmented yield algebra) that assigns each augmented tree the augmented string of terminal symbols at its leaves is called a yield homomorphism. Let A be an augmented tree algebra and B an augmented yield algebra and g : A \u2794 Ba yield homomorphism. Let '.::::'. be a strong congruence relation of A. Fo r any string w E :E* let B ( w) \ufffd B be the set of all augmented substrings of w ( the exact definition of substring depends on the parsing strategy). Let B(w) = (B(w), F) be the relative subalgebra of B with carrier B(w). The nondeterministic algebra A/:::'. xB(w), that is, the direct product of the quotient algebra of A and the relative subalgebra of B with augmented substrings of w, is called a parsing algebra for G, w. The elements of a parsing algebra are pairs (a\ufffd, b) where a\ufffd is a congruence class of an augmented derivation tree and bi s an augmented substring of w. By Theorems 1 and 2, (a\ufffd, b ) is generated in the parsing algebra iff there is some generated derivation tree a' in A such that a\ufffd = a\ufffd and ga' = b. Let w E :E* be an input string. An augmented substring of w may be given by a tuple l E N71 of positions in w. Two different tuples l, ( may determine the same augmented substring of w. A parse item is a pair (a\ufffd, l) where a\ufffd is an equivalence class of augmented derivation trees and \ufffd is a tuple of positions. A parse item algebra is a nondeterministic algebra I, = (I, F) where J is a set of parse items . A parse item homomorphism is a strong homomorphism cp from a parse item algebra into a parsing algebra, such that cp(a\ufffd , \ufffd) = (a\ufffd, b), i. A parsing schema (I, D) is obtained from a finite parse item algebra (I, F) by defining D = {x1 , ... ,X n r X 1 3/ E F : f(x1, ... ,x n ) r x}. Then by the previous equivalences, (a\ufffd,e) is de ducible in the parsing schema iff for some a' EA, a' is generated in A and a\ufffd = a\ufffd and ga' E B(w) and cp( a\ufffd , e) = (a\ufffd, ga'). Note that if cp is a parse item homomorphism of a finite parse item algebra into a parsing algebra, then there are only finitely many congruence classes of generated augmented derivation trees. A nondeterministic algebra A is called sound (resp. complete) w.r. (I, D ) for G, w is constructed as above and the tree algebra is correct w.r.t. admissible derivation trees of G, then a parse item (a\ufffd, e) is deducible in (I, D) iff a\ufffd is the congruence class of some admissible derivation tree a' and ga' is the augmented substring of w denoted by e; that is, (I, D) is correct. By definition, w E L( G) iff there is a derivation of w from some start symbol of G. An element in A that represents a derivation of a string w E :E* from a start symbol is called a complete ( augmented} derivation tree. An equivalence relation ::: on A is called regular if there are no mixed equivalence classes; that is , if the condition holds: whenever a is complete and a::: a' then a' is complete, too. If ::: is regular then a\ufffd is called complete iff a is complete. A parse item (a\ufffd, e) where a\ufffd is complete is called a final item. If (I, D) is correct for G, w then w E L(G) iff there is some final item (a\ufffd, e) such that (a\ufffd, e) is deducible in (J, D) and e denotes w. Context-Free Bottom-Up Head-Corner Parsing In this section we present an algebraic description of the bottom-up head-corner (buHC) parsing schema for CFG [7, Schema 11.13 ] , according to the construction scheme described in the previous section. A buHC parser starts the recognition of the right-hand side of a production at a predefined position (the head of the production) rather than at the left edge, and proceeds in both directions. In the sequel we will denote terminal symbols with a, a1 , a2 , ... , nonterminal symbols with A, B, ... ,  strings of terminal symbols with u, w and strings of terminal and nonterminal symbols with /3, ,, 8. l /31 denotes the length of (3. We borrow a practical notation for trees from [7] : (A \"\"4 /3) denotes an arbitrary tree with root symbol A and yield (i.e., sequence of labels on the leaves, from left to right) f3 (possibly of height 0, in which case f3 = A). (A \u2794 /3) denotes the unique tree of height 1 with root symbol A and yield /3. A tree of height 1 is called a local tree. Expressions of this form can be nested, thus specifying subtrees of larger trees. We also write (/3 -v-+ ,) for a sequence of trees with root symbols f3 (from left to right) and concatenated yields ,. A headed context-free grammar G is a tuple (N, \ufffd, P, S, h) such that (N, :E, P, S) is a CFG without \u20ac productions, where N, \ufffd,P are finite sets of nonterminal symbols, terminal symbols and productions, respectively, S is a start symbol and h : P \u2794 N is a function that assigns each production p = A \u2794 f3 a position 1 \ufffd h(p) \ufffd 1 /3 1-The h(p)-th symbol in f3 is called the head of p (for simplicity it is assumed that the same production cannot occur twice with different heads). The pair ( p, h(p)) is called a r, k, l ) is an admissible buHC tree then ( r, k, l) is computed by buHGaA-+f3g_\"f or we find admissible buHC trees (r i ,ki, l i ) for 1 \ufffd i \ufffd j such that (r, k, l) is computed from (ri ,ki , l i ) by some j-ary buHC tree oper ation and A(r i , ki , li) <r A(r, k, l) for all i. Then completeness follows by induction on the values of A. \u25a1 does not rely on the finiteness of N and P. However, in Ab u HC-LIG we do not find a strong congruence relation with finitely many congruence classes of admissible buHC-LIG trees: Proposition 2. Let \ufffd be a strong congruence relation of Ab u HC-LIG\u2022 If the length of stacks in the derivation trees of G is unbounded, then [0]A buH cu al\ufffd is infinite. Proof. We give an informal proof. Consider the buHCA operation and a production with a push q operation. First, observe that if two admissible buHC-LIG trees are congruent then they must have the same symbol Q on top of the stacks at their root nodes, because of the buHCA operation. Let w = Qi ... Q m and w' = Q\ufffd ... Q\ufffd be the stacks at the root nodes, where Q m = Q\ufffd . By the same operation we can conclude that Q m -i = q\ufffd -i -By induction it follows that any two congruent, admissible buHC LIG trees must have the same stack at their root nodes. Thus, if the length of stacks is unbounded, there are infinitely many noncongruent buHC-LIG trees. \u25a1 Below we will define an algebraic transformation that preserves the correctness of the transformed nondeterministic algebra under certain conditions. Then we proceed as follows: First, we replace the buHCA operation in Fig. 3 with two operations buHCA 0 p for op E {push, pop}, with the additional condition that o(p ) = op q (for some Q) in Fig. 3 . Obviously, this does not affect the correctness of the tree algebra. Next, we use the transformation to modify the buH_CA p us h operation. The transformed buHC-LIG tree algebra will have new congruence relations with only finitely many congruence classes of admissible buHC-LIG trees. We first define some additional algebraic concepts. Let A= (A, F) be a nondeterministic algebra and R : A 2 \u2794 P(A) a binary operation on Cgr(A[R]) = Cgr(A) n Cgr(( A, R)) \ufffd Cgr(A') . Proof. ( . 1 Introduction Many words in Spanish function as different parts of speech (POS) . Part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context. Because most of the high-frequency Spanish words function as several parts of speech, an automatic system for POS tagging is very important for most other high level natural language text processing. There are many approaches to the performance of this task, but they could be classified into two main ones: \u2022 The linguistic approach, in which the language_ model is written by a linguist, generally in the form of rules or constraints. An example of this approach is the tagger developed by Voutilainen [Voutilainen and Jarvinen, 1995]. \u2022 The automatic approach, in which the language model is automatically obtained from corpora. This corpora can be either raw or annotated. In this way, the automatic taggers are classified into supervised and unsupervised learning: -Supervised learning is applied when the model is obtained from annotated corpora. -Unsupervised learning is applied when the model is obtained from raw corpora training. followed by the suffix -ed. However, the addition of the suffix -ed apparently causes the loss of the final e of chase; thus chase and chas are allomorphs or alternate forms of the same morpheme. Koskenniemi's model is \"two-level\" in the sense that a word is represented as a direct letter-for-letter correspondence between its lexical or underlying form and its surface form. Instead of this, the single word tagger that we have developed is based on a modified version of the ID3 algorithm. This algorithm generates a decision tree for a single out attribute ( called class in TDIDT terminology). However, the feature structure set of tags of the tagger needs a decision tree for several out attributes. In order to solve this problem the ID3 algorithm has been modified. So, the modified version of the algorithm builds up the tree recursively as follows: first, the algorithm chooses the best out attribute in order to generate the tree; next, the tree is built up recursively f9llowing the traditional ID3 algorithm until the selected out attribute is classified; finally, the algorithm goes back and chooses another out attribute for each leaf to build up the tree from that leaf. The most noticeable examples of automatic tagging approaches are The single word tagger has been trained with an 87830 words corpus. From this corpus, the learning algorithm has computed a decision tree with 48317 rules. A word matches in this rule if the word ends with the symbol 'b' followed by any three symbols followed by the string 'dad'. This rule has been computed from the words: accesibilidad, aceptabilidad, adaptabilidad, admisibilidad, a/ abilidad, agregabilidad, amabilidad, amigabilidad, antiobesidad, ap lica  bilidad, asociabilidad, autoestabilidad, barbaridad, combustibilidad, comensurabilidad, compatibilidad,  comunicabilidad, conductibilidad, confortabilidad, conmutabilidad, contabilidad, contrastabilidad, con  vertibilidad, corresponsabilidad, corruptibilidad, credibilidad, cuestionabilidad, culpabilidad, debilidad,  deseabilidad, disponibilidad, elegibilidad, estabilidad, excitabilidad, f actibilidad, f alibilidad, fiabilidad,  fiexibilidad, fiotabilidad, globalidad, gobernabilidad, habilidad, habitabilidad, heredabilidad, honorabi   lidad, impasibilidad, impenetrabilidad, . impermeabilidad, imposibilidad, imprevisibilidad, improbabili   dad, imputabilidad, inaccesibilidad, incompatibilidad, incomunicabilidad, incorruptibilidad, inestabili   dad, in/ alibilidad, ingobernabilidad, ininteligibilidad, insaciabilidad, insensibilidad, inteligibilidad, in  viabilidad, invisibilidad, irresponsabilidad, irreversibilidad, irritabilidad, maniobrabilidad, morbilidad,  morbosidad, mutabilidad, nubosidad, obesidad, obviedad, perdurabilidad, permeabilidad, posibilidad,  probabilidad, rentabilidad, respetabilidad, responsabilidad, reversibilidad, sensibilidad, sobriedad, so  ciabilidad, solubilidad, subunidad, susceptibilidad, urbanidad, verbosidad, viabilidad, volubilidad, Analysis of results The order L of the VMM tagger has been set to 4. The tagger has been trained on a Spanish 45000 words corpus tagged with the set of tags shown in table 1 . An example of tagged text 3 can be seen in figure 3 . The accuracy obtained is 98.58%. This accuracy is better than that achieved by any other tagger we know of. In table 2 a comparison between the accuracy of several taggers is shown. Our results (98.58%) are better than Singer's tagger (95.81%) based on VMM because the single word tagger has added lexical information to our tagger. This can decrease the error rate when errors due to bad tags for rare words are avoided by the single word tagger. However, it is difficult to compare these results to other works, since the accuracy varies greatly depending on the corpus, tag set, etc. More exact performance could be measured by increasing the size of training a testing text. Thus, the performance could decrease or increase if different corpus styles are used. Conclusions and future work In this paper, a high accuracy Spanish tagger has been presented. This tagger has three main features: a single word tagger based on decision trees, a feature structure set of tags, and a variable memory Markov model. The results obtained show that joining an accurate single word tagger with a feature structure set of tags and a high order VMM produces an improvement in the performance of the tagger. Parsing with Multiple Grammars Weng and Stokke (25] presented a top-down style algorithm that combines several parsers with partitioned subgrammars, with a restriction that the calling graph must not have left recursion, a somewhat stronger constraint than Tomita's original parser [15] . Here, we will present a flexible parsing composition algorithm that does not have such restriction on the grammar. The intuition behind this new algorithm is that each subparser with a subgrammar will try to parse from the current input position and place its output nonterminal (virtual terminal) onto the same lattice, if successful. A subparser is invoked by a caller subparser only when certain predictive pruning conditions are satisfied to avoid excessive parser invoking. The representation used for interfacing different parsers is a special chart or lattice. The nodes of the lattice are spatial positions, and its transitions represent terminals/virtual terminals that cover positions indicated by their corresponding start and end nodes. Usually, a terminal covers one position, while a virtual terminal can cover multiple positions. During parsing, both terminals and virtual terminals are represented in the same lattice, but, partial constituents and nonterminals that are not the output of any subgrammar cannot be placed there. This implies that only the granularity represented by subgrammars is present in the lattice, and therefore, is termed as a lattice with multiple granularity (LMG). In addition, this representation gives us a good control over the lattice density for parsing. These features differentiate LMG from the traditional chart used in other parsing algorithms [8, 2, 17, 27] . Dealing with word lattice input for a single GLR parser was solved by Tomita (22] . Two steps in Tomita's algorithm are different from the standard GLR parsing algorithm: the lattice is topologically sorted for synchronization; for shift action, an additional procedure tests whether 3 . Create in r a vertex v 0 labeled with state id so, U o = { vo }. 4 . SP = 0 (initializing the started parser list). dt PARSER psr m (dt, u) , m = 0, ... , I<: ii. else if R -f= 0 then do COMPLETER(R, f). until A== 0 n R == 0 (done with reductions). (d) PREDICTIVE-SUB-PARSING(dt, u). (e) SHIFTER(Q, f). (f) dt = pop( u). Predictive Sub-parser: The parameters for procedure PREDICTIVE-SUB-PARSING are call-by-name. In the procedure, OUT PUTk is the OUTPUT set of sub grammar Gk, and PVT( Ui, w) is a function that takes a set of states and a (virtual) terminal, and returns a set of legitimate virtual terminals to be parsed at that point. iii. For (m, d) E lvt, m E PVT( Ui, w) and m is not in L to cover from i to d then 2 A. add m to L: create a transition mdt with label m that connects from node i to node d. so that we can alleviate size, speed, and robustness problems in real-world natural language applications. We also reported our initial but encouraging parsing results in the AT IS domain. We plan to further integrate the predictive robust parsing algorithm [24] into the current system, and to use probabilistic modeling for better pruning and selecting of correct parses. To alleviate the current grammar writing bottleneck in our parsing experiments, we will also work with semi-automatically induced grammars [20] . In turn, each parser operating on the chart is a component parameterized with a unification-based grammar. Modular Unification-Based Parsers Remi A working system can easily be assembled from a set of components by writing a resource file, called an application definition file, which describes instantiations of components, the calling sequence of components and various global variables and parameters. Assembling components together is done using a composition operator which behaves much like the Unix pipe. When, in a Unix command, data between programs is transmitted using files (stdin/stdout) and programs are combined using the pipe 'I' command, in MEAT, the data transmitted between components is a chart, and syntactically the sequence of component is combined using the ':' composition operator. In effect, the MEAT system is a specialized shell for building NLP systems. The implementation language is C++ but external components can be integrated in the system by writing wrappers ( as done for several morphological analyzers previously built or used at CRL). The system would save the chart in some file that can they be browsed using the chart viewer ( also a component of the system). Each component is an instance of a C++ Modular Unification-Based Parsers The parser component of the MEAT system implements a bottom-up island-driven parsing algorithm [Stock et al. 88], with a few modifications geared towards efficiency. In each rule, it is possible to identify one element of the right-hand side as the island which will be searched before attempting to apply the rule. The default behavior when no island is specified is a left-to-right regular chart-parsing algorithm. Islands can be used to avoid triggering rules on the first element of the left-hand side when this el ement can appear much more frequently than the island. For example, the left-hand side of a rule describing the coordination of noun phrases would start with a noun phrase, and be triggered for each noun phrase in the default left-to-right parsing strategy. Specifying the coordinator as the island allows the rule to be triggered only in the presence of a coordination. Elements of the right-hand side can also be specified as optional, reducing the number of disjunctive rules in the grammar. In this rule, unlike grammars written in the LFG or HPSG-style, a feature structure is created anew for the left-hand side: the left-hand side is not simply the head of the construction percolated up. Therefore, all head features that need to be transmitted need to be specified in the rule. This style makes each rule heavier than its LFG counterpart 4 but allows to categorize each constituent as belonging to a different class, e.g. a different bar-level. For example, the following rule builds an N' from a noun. Rules for noun phrases will then consider only N' and not Nouns, and so on. Recategorization of heads is not possible if the head itself becomes the left-hand-side constituent. Note that this style also make grammars inherently more reversible (assuming that generation would start with a syntactic structure) since we would only need to specify the head in this rule (specified here as the island). The structural analysis of hy perg raphs constructed incrementally while the parsing processes are already running poses some interesting questions with regard to the consistency of the overall operation of the system. Within such a system , the quality (score) of a piece of information may change over time. This is partly due to further incoming recognition results, and partly due to penalties inflicted upon certain hypotheses by individual parsing processes. file very efficiently, a facility which is used to pre-compile grammars as arrays of integers as well. 4 It is of course possible to write LFG-style rules in this formalism. The structural analysis within MILC consists of three components with different purposes and descriptive power. First, idioms are recognized. In our case, idioms are fixed expressions without any inflectional or order va riation . _ This stage performs a fa st incremental graph search by intersecting the input hypergraph with a graph describing the known idioms and compounds. The second component performs an in crem\ufffdntal left-to-right parse of an utterance, with an emphasis on the syntactic structure of limited size constituents (Abney, 1991 ). The following integration component is implemented as a bidirectional island-parser, capable of extending hypotheses to the left. This is useful, e.g., if the subcategorization frame of a verb is known late due to a verb-last position in an utterance. In our experiments, employing a second parser for incorporating complements to the left resulted in a speed up of ten as compared to a strict left-to-right approach. The principle question with this method, however, is how much of the incrementality is lost using islands due to delayed attachment of elements to the left. In practice, this delay involves only a few calls to the fun damental chart parsing rule, mainly to incorporate complements into verb phrases. In theory, the worst case would arise if the island wa s the rightmost element of every rule. Suppose that a rule ha s n elements on the right hand side, then nl more elements would have to . be incorporated than with the left-to-right approach (the standard approach has to incorporate these as well, but does that before the last element is reached). Thus, at any given point in the cha rt, the additional effort is bound by the number of elements on the right hand side of a rule, which in turn is bound by the number of wor ds to the left. The combination of three methods for structurally analyzing an input utterance yields a reasonably high performance, in our tests six-fold real time for a machine translation of spontaneous speech input. PARSING MILDLY CONTEXT-SENSITIVE RMS Tilman Becker Dominik Heckmann German Research Center for Artificial Intelligence (DFKI GmbH) {becker ,heckmann }@dfki.de Abstract We introduce Recursive Matrix Systems (RMS) which encompass mildly context-sensitive for malisms and present efficient parsing algorithms for linear and context-free variants of RMS. The time complexities are O(n 2h+ I ), and O(n 3h ) respectively, where his the height of the matrix. It is possible to represent Tree Adjoining Grammars (TAG [1], MC-TAG [2], and R-TAG [3] ) as RMS uniformly. Recursive Matrix Systems (RMS) RMS = (G, I) is a two-step formalism. In a first step, a grammar G = (N,T, S, P) generates a set of recursive matrices. In a second step, a yield function maps, according to the interpretation I, the recursive matrices to a set of strings L(G, I) . The recursive matrix generating grammars are ordinary phrase structure grammars, where the terminal symbols are replaced by vertical vectors. These vertical vectors are filled with terminal symbols, nonterminal symbols, or they are left empty. derive ield The two-step formalism: S E N ==} 0 m E M H I w E T*. Example: G1 = (N, T, S, P) = ( { S}, {a, b, c}, S, { S \u2794 ,\ufffd I S, S--+ I \ufffd I}). Every time the first rule is applied, a new column with terminals is added to the matrix. The last rule terminates the process. A possible derivation: S =? h , I : I S =?h, I : II: is =?h , I ! II: I I H The product of the derivation process is a matrix with terminals as elements. Finally, these terminal symbols are combined into one string. There are many possible interpretation functions. However, it seems reasonable to read the terminal symbols within the matrix row by row from top to bottom, and each row alternating from left to right and from right to left. This interpretation condition for height   Several controls can be added in order (1) to reduce the number of sets to analyze and (2) to control the satisfaction process. We can for example choose to build only sets of juxtaposed categories or consider only sets actually characterized by properties. The satisfaction process itself can be controlled using several heuristics: it is possible to filter the satisfiability according to a threshold given by the cardinality of the set of unsatisfied properties p-: it is possible to build only characterizations with less than a certain amount of unsatisfied constraints. At the extreme, we can reduce the satisfiability to positive characterizations (reducing characterization to grammaticality). Conclusion The representation of linguistic information by means of constraints is interesting both for knowledge representation (no implicit information, encapsulation) and for implementation point of view (parsing is seen as a constraint satisfaction process). Property Grammars offer a framework taking advantage of these characteristics and present several interests for natural language processing. The first one concerns the generality and the robustness of the approach: constraints allow the introduction of the notion of characterization to replace that of grammaticality. The parsing process consists in verifying constraints for the possible set of categories associated to the input instead of trying to build a structure according to the grammar. One other interesting point concerns the integration of different information sources: the properties can concern all kind of information (prosodic, synta\ufffdtic, pragmati<; etc.). part of the cascade or the following cascades. This makes the writing and debugging of grammars easier than in approaches based on context-free grammars, where changes to a rule can in principle influence the application of any rule in the grammar. The grammar organization in cascades and segments allows a clean definition of the grammar parts. Each cascade copes with a specific phenomenon (modularity of the grammar) . All the rules for the specific phenomenon are grouped together and are easy to check. The segment structure of cascades is suitable for coping with the idiosyncratic phenomena of re stricted corpora. As a matter of fact domain-oriented corpora can differ from the standard use of language (such as those found in generic corpora) in two ways: (i) in the frequency of the construc tions for a specific phenomenon; (ii) in presenting different (idiosyncratic) constructions. Coping with different frequency distributions is conceptually easy by using deterministic parsing and cascades of rules, as it is sufficient to change the rule order within the cascade coping with the specific phe nomenon, so that more frequently applied rules are first in the cascade. Coping with idiosyncratic constructions requires the addition of new rules. Finally from the point of view of grammar organization, defining segments brings some added value to ordered cascades. Generic rules (in s2) are separated from domain specific ones (in sl); rules covering standard situations (in s2) are separated from recovery rules (in s3) . In s2, rules are generic and deal with unmarked cases. In principle s2 and s3 are units portable across the applications without changes. Domain-dependent rules are grouped together in sl and are the resources the application developer works on for adapting the grammar to the specific corpus needs (e.g., coping with idiosyncratic cases) . Such rules generally use contexts and/or introduce domain-dependent (semantic) constraints in order to limit their application to well defined cases. S 1 rules are applied before the standard rules and then idiosyncratic constructions have precedence with respect to standard forms. Segments also help in parsing robustly. S3 deals with unexpected situations, i.e. cases that could prevent the parser from continuing. For example the presence of unknown words is coped with after chunking by a cascade trying to guess the word's lexical class. If every strategy fails, a recovery rule includes the unknown word in the immediately preceding chunk so to let the parser continue. Recovery rules are applied only when rules in sl and s2 do not fire. The deduction steps of the parser are the following: j, -, -] 1) \u20ac =- ------ [N-Y \u2794 .. , j, j, -, -] i, j, p, q]  [N-Y \u2794 v\u2022M-Y \u2022w,i, j,p, q Ad\" The steps v init and 1) \u20ac are the initializer deduction steps in charge of starting the parsing process. The steps v ine and v con traverse elementary trees when no adjoining is performed. Finally, given /3 E adj (M 7 ), the steps v Foot and 1J A<l j are used to recognize the adjoining operation, where p U q is equal to p if q is undefined and is equal to q if p is undefined, being undefined in other case. Given a E I, the input string w belongs to the language defined by the grammar when an item [T \u2794 \u2022R 0 \u2022, 0, n, -, -] is deduced. From the set of derived items (the chart), a parser of the input can be constructed retracing the recognition steps in reverse order or annotating the items computed by the recognition algorithm with information about how they were obtained. The time complexity of the algorithm with respect to the length n of the input is O(n 6 ). This complexity is due to adjoining step since presents the maximum number of relevant input variables (j, m, k, l, p , q ). With respect to the size of the grammar, the time-complexity is O(IGl 2 ), since at most two elementary trees are simultaneously considered in a single step. The space-complexity of the parser is O(n 4 ) since every item is composed of four input positions ranging on the length of the input string. = NP :(agent{head}=nhead ,-) VP : head=vhead- The rules can be glossed as follows. In the first rule, for noun phrases , the final noun is stored in the nhead va riable, the determiner in the specifier (spec) of the head, and the adjectives and nouns as modifiers of the head ( nomM otl). The verb phrase rule stores the verb in the vhead va riable of the VP. If a noun phrase object is present, its head is copied to patient, indexed on the VP 's head , thus making a dependency link between the verb and its object. /\\ is shorthand for copying all other va riables (spec, mod) up to the VP. The sentence rule links the subject NP as the agent of the verb , and copies the other dependencies and the head of the VP to the sentence's va riables 2 \u2022 Implementation Most aspects of the implementation are straightforward. We wait until a phrase has been . recognised , and then set the unindexed va riables followed by the indexed ones . This is implemented by maintaining an agenda of what va riables need to be set, during execution of the finite-state machines . In order to obtain O(n 2 ) complexity, the finite-state machines must be deterministic. Standard algo rithms for determinising finite-state machines will not work here, since the transitions are not uniqely determined by the input symbols. Thus, in Det : spec (N : nomMod{spec}) * N: head, the N input can co rrespond to different va riable outputs . The algorithm described by Roche and Schabes (1995) will solve this in most cases , by deferring output symbols are deferred until enough right-hand context is accumulated to decide which output to produce. Ambiguity and packing Determinising the finite-state machines only allows a single output, even if the input is ambiguous. Lexical ambiguity can be controlled by a pre-processing step such as tagging, but structural ambiguity is more of a problem . However, the use of va riables to represent dependency allows a scheme in which structural ambiguities are packed into a single structure. This will not solve all problems to do with structural ambiguity, but two of the more common ones in English -PP attachment and noun compoundingcan easily be handled this way. To construct packed dependency structures, we place a word in the range (value) of more than one indexed va riable, so that it can be accessed via multiple dependencies. The grammar below shows how this is done for prepositional phrase attachment ambiguity. NP (nhead) = Det N:nhead PP (prep , pcomp[] , phead) = Prep : prep NP :( pcomp{prep}=nhead , phead=nhead) PP (prep, pcomp [] , phead) = PP : -PP :(-,pmod{phead}=prep) NP (nhead , pmod [] , pcomp[]) = NP :-(PP : pmod{nhead}=prep ,pcomp=pcomp ,pmod=pmod) * The first PP rule builds simple prepositional phrases , in which phead provides access to the head of embedded NP, while pcomp provides access to it via the preposition. In the second PP rule, a pmod link from this phead is then made to the preposition of the second PP. When the phrase the man in the park on a hill is parsed , on a hill is attached in the final structure to both man and park. A similar approach can be used for noun compounds. DISCRIMINANT REVERSE LR PARSING OF CONTEXT-FREE GRAMMARS Jacques Farre Laboratoire 13S -CNRS and Universite de Nice -Sophia Antipolis jf@essi.fr Introduction In Discriminant Reverse LR{k) parsing [l] , actions are determined by scanning a stack suffix, thanks to a small deterministic finite automaton (dfa) . Thus, opposite to (direct) LR parsers, DRLR parsers do not need the whole content of the stack in order to determine actions. DRLR parsers can, in few cases, deeply explore the stack, but it has been proven they are linear in time. When the grammar is not of the required class, this feature of DRLR parsers allows to defer actions in conflict and to push a conflict mark onto the stack. The DRLR parser can keep on parsing the input right of the conflict, that is to say it can perform shifts and reductions as long as these actions are determined by a stack suffix above the mark. Otherwise, the action cannot be chosen and an induced confl ict occurs; a new mark is pushed, and parsing of the remaining input can resume. For most grammars, the stack configurations in which marks resolve the LR conflict, and how to resolve it, are computed at construction. For other grammars, the conflict is resolved at parsing time by comparing the stack content with the candidate derivations (which are given by the marks), but the stack configurations in which marks trigger these comparisons are computed at construction. One can argue that conflict marks are an implementation of multiple stacks. But the stack is not explored below a conflict mark, and Extended DRLR needs a single stack and a single parser. Thus, it is not comparable to GLR. In particular, Extended DRLR parsers for which conflicts resolution can be computed at construction are linear in time and size. Principles of (Extended) DRLR construction This presentation will rely on a DRLR(O) parser. In a DRLR(O) item [i, A \u2794 a.,B] , i is a parsing action (shift if i =3D 0, reduce according tH' iproduction if i > O) ; the core A \u2794 a.,B means that a stack suffix u has been scanned, and ,B =;{ ux. The states of the DRLR automaton are the states of the dfa which recognizes these u's by reading them from right to left. If [i, A \u2794 aX .,B] belongs to a state q and if no [i', A' \u2794 a' X .,B'], i' =p3D i, belongs to q, the suffix Xu determines action i; else more stack must be read, and a transition by X to q' , which will contain [i, A \u2794 a.X ,B] , is done; [i, A \u2794 .a] infers ( closure computation) [i, B \u2794 a' .A,B'], that is a step back in a rightmost derivation chain. A LR(O) conflict occurs when distinct rigthmost derivations produce a same prefix <p. These deriva-tions (or proto-derivations when <p belongs to a language a/3*,) can be computed from the states of the DRLR{0) automaton. Each step of these derivations defines mark positions. For example, let S =} 1r' A' x' => 1r' a' B' /3' x ' =} 1r' a' B' y ' x' => 1r' a',' C' 8' y ' x' =} 1r' a',' C' z' y ' x' => <p z ' y' x' rm rm rm rm rm rm be two derivations in conflict: a mark is pushed onto stack after </;, and it will be found instead of or C' , but only the C and C' of B \u2794 ,CJ and B' \u2794 ,'C'8'. The cores B \u2794 ,C.8 and B' \u2794 ,'C'.8' will be the mark positions. If the languages of 8 and 8' are disjoint and not prefix, z and z' can be reduced respectively to 8 and 8' without scanning the stack until the mark. As, in this case, 8 #30, 4:i , B \u2794 ,C .8 ] and [i', B' \u2794 ,'C'. 8 '] cannot be in the same state, and the mark can determine action i or action i' . Choosing i or i' means the LR{0) conflict must be resolved by reducing respectively to C or to C'. But, if for example 8 =3f>, qi, B \u2794 ,C .8 ] and [i' , B' \u2794 ,'C' .8'] will be in a same state. The mark cannot decide between i and i' . The new mark for this induced conflict can be found in stack instead of B or B', but only in positions A \u2794 aB./3 for reduction to C and A' \u2794 a' B' ./3 1 for reduction to C'. The LR conflict can be resolved if the languages of /3 and /3' are discriminant. Conflicts cannot be resolved at construction if a mark has the positions A \u2794 aB./3 and A \u2794 a' B' ./3, which mean that 1raB \u21d2 w, 1r'a' B' \u21d2 w. This does not mean that the grammar is ambiguous, as shown by the example below. The mark m2 has positions S' \u2794I-S. -I for actions 2 and 3, S \u2794 aA.b and A \u2794 aA.b for 4, S \u2794 aB.bb and B --+ aB.bb for 6. When the stack suffix is m2 -1, the conflict cannot be resolved at construction. Otherwise (suffix =3D2b), m2 has the same positions as m 0 , and pushes m1 . A working example The mark m2 means that an even number of b has been read, while m1 means that an odd number of b has been read. The suffix m2 -I decides a conflict resolution at parsing time, while m1 -I can decide at construction to resolve the conflict in favor of 5. But if we have S \u2794 aAba instead of S \u2794 aAb, the conflict resolution can be decided at construction in any case : the position S' \u2794I-S. -I of m2 is no more compatible with the reduction in conflict 5, and it can decide the reduction 7. Direct Parsing of Schema-TAGs* K. Harbusch & J. Wo ch University of Koblenz-Landau, Computer Science Department E-mail: {harbuschlwoch }@uni -koblenz . de Schema-Tree Adjoining Grammars {S-TAGs) are an extension of Tree Adjoining Grammars {TA Gs) (for an introduction see, e.g., [5] ), initially introduced by [7] . In a schematic elementary tree, a regular expression (RX) 1 , is annotated at each inner node of an elementary tree. This means, that the elementary schemata enumerate a possibly infinite set of elementary trees. For instance, see the tree t 1 of Fig. 1 , which performs NP-coordination as , e.g., the zero-coordination Peter (121) or Bob, Bill, Mary, Sue, and the dog (l2l 4 -l3l.lll.l21). Obviously, the schemata of S-TAGs provide a condensed In order to analyse S-TAGs, an ordinary TAG parser, e.g., the Earley-based parser by Yves Schabes ([6] ), can only be applied if the length of the input string is considered in order to compute a finite subset of applicable elementary trees. This is disadvantegeous because the computation must be performed at runtime and the property of condensed representations is lost. Therefore we introduce a direct parser of S-TAGs which works directly on the schemata. Accordingly, common prefixes (i.e. substructures) of the same scheme are represented only once instead of enumerating all explicit orderings in the individual elementary trees. This leads to a better average runtime. However, the worst case time complexity remains the same as for TAGs (O(n 6 )). The direct parser adapts the basic idea of an Earley parser [2] to the exploration of regular expressions. This means, that a new dot position ( 0) is defined which indicates whether a prefix of an alternative in a regular expression is analysed. In order to move the 0, two operations SHIFT and NEXT are defined. SHIFT(</>) moves 0 by one token to the right, i.e. SHIFT(a 0 /3) = bi, = a.NEXT(/3) 0 (/3 \\ NEXT(/3))} where NEXT(</>) returns a set of all alternatives for the next symbol (Gorn address) . NEXT(a) = {0a} iff a is a Gorn address; NEXT(a) = { 0/3 1 , ... , 0/3 n }, iff a= (/3 1 + ... + f3 n ), n 2::: 2 and NEXT is applied recursively as long as /3i starts with an opening bracket , finally a Gorn address is reached 2 ; { 013 + .,y, *This work is partially funded by the DFG -German Research Foundation -under grant HA 2716/1-1 and 1-2. 1 RXs are inductively defined. Let o., /3 and /3 1 , ... , f3 n (n 2: 2) be Gorn addresses uniquely referring to daughters or arbirarily complex RXs, then o../3 (concatenation of branches), ( /3 1 + ... + f3n ) (enumeration of alternatives), o.* (Kleene Star) and o. + (o.* without the empty repetition) are RXs. Finally, \"-\" allows to cut off a subtree at the end of a path. As an abbreviation we write o. < n l m ) which enumerates L 7:o o. n+i (n, m 2: 0). Notice, \".\" binds stronger than \"+\" . 2 Notice sums are always bracketed even if no binding conflict occurs as on the top level (cf. footnote 1). { nigelh,aycock} @csr.uvic.c a Abstract Mathematical equations in LaTeX are composed with tags that express formatting as opposed to structure. For conversion from LaTeX to other word-processing systems, the structure of each equation must be inferred. We show how a form of least cost parsing used with a very general and ambiguous grammar may be used to select an appropriate structure for a LaTeX equation. MathML provides another application for the same technology; it has two alternative tagging schemespresentation tags to specify formatting and content tags to specify structure. While conversion from content tagging to presentation tagging is straightforward, the converse is not. Our implementation of least cost parsing is based on Earley's algorithm. ANALYSIS OF EQUATION _Introduction LaTeX [l] is a widely used system for typesetting where the user embeds markup tags in a text document to con trol the formatting. Some tags are structure tags because they denote the logical structure of some part of the doc ument (e.g. \\section or \\footnote are structure tags). Others are presentation tags because they specify formatting without necessarily imparting any information about the document structure. For example, \\it and \\tiny are pres entation tags. HTML is another example of a markup language where structure tags (e.g. <ul> and <h2>) can be mixed with presentation tags ( e.g. <font> and <center>) in the same document. For reformatting a document according to new style guidelines or for importing into some word processing systems, it is necessary to know the full structure of the document. Conversion of a LaTeX document into an Ado be FrameMaker document is an example where full structural information is needed. We have devel oped such a conversion tool , a FrameMaker import filter named laimport. However, conversion of equations in LaTeX docu ments has been problematic and motivated the research reported here. The research al so has applications to MathML [2] where both presentation and content tagging schemes are provided. A 'Cost-Based' Grammar fo r LaTeX Equation Notation The grammar must be abl e to accept any sequence of mathematical symbols, even nonsensical ones. But when a conventional structure exists, it should be inferred. We accomplish this goal by attaching cost expressions to grammar rul es, where high costs are associated with rul es when parentheses do not balance ,operator symbols lack operands, and so on. The grammar al so takes account of hints in the form of spacing -operators surrounded by white spaceare assumed to have lower precedence than operators with less surrounding white space. A gram mar for a subset of LaTeX equations is shown in Figure 1 . In this grammar, '-' represents an inserted space. Each non-terminal has a synthesized cost attribute, computed by the cost expression attached to the grammar rule. These costs increase if parentheses do not balance or if '*' is used with lower precedence than '+'. With the example grammar, the LaTeX equation $a+b-*-c$ has parses equivalent to ${a+{b-*-c} }$ with cost 14780 and ${ {a+b }-*-c }$ with cost 9873. 5 . The white space has overridden the usual operator precedences, as is desirable when handling operators whose meanings are unknown to the translator. Similarl y, the grammar pro vides a parse for $((a$ while rejecting a parse of $(a+b)$ as equivalent to ${ (a}+{b) }$. Parallel parsing of natural language has been researched extensively. In [6] we can find an overview of parallel chart parsing. Most attempts, however, were not very successful [8, 3] . Only recently two NLP applications were successfully parallelized [7, 5] . However, the former focussed on Prolog and the latter exploits coarse-grained parallelism of the kind that proved unusable for our Deltra system 1 or other systems [3] . We present a more widely applicable approach, not limited to Prolog. EXPLOITING PARALLELISM IN Most unification-based parsers have characteristics that make them particularly hard to parallelize. Typically, unifications account for the bulk of the pro cessing time in unification-based parsing. How ever, parallelizing this operation is difficult [1] and does not speed up the CF part. Therefore, most research has focussed on exploiting parallelism at the CF level, where the unification operations are atomic and distributed amongst processors. This approach has several problems as well. First, each item has a different impact on the derivation of new items. In addition, the computational cost of unifying the items in one parse can vary wildly. These irregularities make it hard to find a good distribution of work ( load balancing). Another ch aracteristic is the lack of locality, causing excessive communication and ineffective cache utilization. Finally, because all intermediate results need to be recorded in a chart, there is a lot of synchronization between processors. This aspect is aggravated by the need of dynamic load balancing, which requires additional synchronization. The implementation is aimed at shared-memory architectures, mainly driven by the difficulties discussed before. The increasing availability of these systems further justifies this choice. Let us first consider the problem of load balancing. Experiments in [10] indicate that only dynamic load balancing of single unification operations can yield a scalable solution to parallel parsing. However ; putting each individual unification in a central queue will result in too much overhead. We solve this dilemma by taking a work stealing approach: 2 each pro cessor-or thread 3 -h as its own work queue. However, whenever a processor runs out of work, it may steal work from other pro cessors. This considerably decreases the amount of synchronization because now synchronization is only required when a pro cessor is stealing work. We further limit the number of steals by allowing a thread to steal an amount of work proportional to the amount of work available at the victim. Grammatical Features in Free Word Order Languages The noun phrase in Modern Greek consists of various parts of speech; it can be elementary or composite, containing recursively other noun phrases or secondary clauses. In this paper we will concentrate on simple, non recursive noun phrases. Modern Greek is a language with free order of the sentence constituents; this also holds partially for the words inside the noun phrase. The possible part of speech combinations are numerous, although there are some general rules that apply to any noun phrase [ Finite State Transducers with Constraints Extended Abstract Cooperating distributed grammar systems (CDGS, for short) have been introduced independently in [ 3] as a grammatical approach of the so-called \"blackboard model\" in the problem solving theory [8] and [2] , with motivations coming from regulated rewriting area. Most of the results known in this field until the middle of 1992 can be found in [4] , while more recent results are surveyed in [5] . Constructing parsers in the grammar systems set-up is a topic, which is not only of theoretical interest, but it will make grammar systems more appealing to researchers in applied computer science. This will clearly bring to the user all the advantages of having a model which can cope with such phenomena as cooperation and distribution of the work carried out by several processors. In [6] , Mihalache and Mitrana study the effect on CD grammar systems of some syntactical con straints similar to those considered for strict deterministic context-free grammars. They obtained a promising result in this respect, namely the unambiguity of the derivations holds for some classes of grammar systems as well. We belive that a more involved study of the derivations in a CD grammar system would be very useful to a possible parser constructor for the languages generated by grammar systems. This is the aim of the present paper: a new class of accepting devices called uniquely parsable accepting grammars systems (UPAGS, for short) is introduced. These mechanisms have a restricted type of accepting rules such that parsing can be done without backtracking. Each component of a UPAGS is a so-called RC-uniquely parsable grammar [7] viewed as an accepting grammar. In [7] one provided a hierarchy of uniquely parsable grammars that gave a simple grammatical characterization of the deterministic counterpart of the classical Chomsky hierarchy. When extending the restrictions for unique parsability to accepting grammar systems, two vari ants should be taken into consideration, depending on the level, local/global, to which the restrictions address. In the global level case, where the restrictions apply to all rules of the system consiered alto- CHART PARSING AS CONSTRAINT PROPAGATION Frank Morawietz U niversitat Tiibingen, Wilhelmstr. 113\ufffd 7207 4 Tiibingen frank@sfs.n phil .uni-tuebingen .de The parsing-as-deduction approach proposed in Pereira and Warren [6] and extended in Shieb er et al . [7] and the parsing schemata defined as special deduction systems in Sikkel [8] ar e well established parsing paradigms. They allow for a uniform presentation and comparison of a va riety of parsing algorithms. Furthermore, they are extensible to more complex formalisms, e.g., augmented phrase structure rules or the ID /LP format . Constraint Programming has been used in computational lin guistics in several areas, for example in (typed) feature-based systems based on Oz [9] , or conditional constraints [5 ] , or advanced compilation techniques [3] or specialized constraint solvers [4] . But to my knowledge, none of these approaches uses constraint programming techniques to implement standard chart parsing algorithms directly in a constraint system. The core idea of the proposal is that the items of a conventional chart parser are constraints on labeled links between the words and positions of an input string . The inference rules allow for the deduction of new constraints, again labeled and spanning parts of the input string, via constraint propagation. The resulting constraint store represents the chart which can be accessed to determine whether the parse was successful or to reco\ufffdstruct a parse tree. While this may seem a trivial observation it allows for a rapid and flexible method of implementation. The goal is not necessarily to build the fa stest parser, but rather to build -for an arbitrary algorithm -a parser fast and perspicuously. The advantage of our approach compared to the one proposed in Shieber et al. [7] is that we do not have to design a special deduction engine, we do not have to handle chart and agenda explicitly. and that it can be seamlessly integrated into existing applications (e.g., within the Constraint Handling Rules (CHR) framework [2] ). Assuming familiarity with parsing-as-deduction, I will presuppose the definitions as given in Shieber et al. [7] . Their algorithms are implemented by specifying the inference rules as constraint propagation rules, the axioms and the goal items as constraints . As an example, Earley 's algorithm is presented in Tab. 1 . We cannot go into the details of the definitions, but it is easily observable that the given constraint propagation rules are nothing but literal realizations of the inference rules. And the only other ingredient we need for the specification of a parser is a predicate which traverses the input and posts the corresponding initial edge constraints. This predicate is universal to all algorithms, but can be va ried with respect to the order the constraints are posted, thereby achieving for example a left-to-right or right-to-left traversal of the string . So, the similarity between parsing-as-deduction and constraint propagation is used to propose a flexible and simple system which is easy to implement and offers itself as a testbed for different Abstract We investigate a method of improving the memory efficiency of a chart parser. Specifically, we propose a technique to reduce the number of active arcs created in the process of parsing. We sketch the differences in the chart algorithm, and provide empirical results that demonstrate the effectiveness of this technique. One basic shortcoming of a classic chart parser (6, 1, 10] is that it does not make efficient use of its grammar. In grammars used to parse natural languages, there is quite often a substantial amount of redundancy in the prefixes of the rule right-hand-sides. A nai\"ve implementation of a chart parser will not take advantage of this redundancy. In contrast t a shift-reduce parser (4, 9, 2, 10] will often use a grammar that has been optimized to eliminate this redundancy (4] . Since chart parsing and shift-reduce parsing are substantially similar (10] , many techniques used in_ shift-reduce parsing can be applied to a chart parser, including this particular optimization. Tree-Structured Grammar Consider a context-free grammar represented as follows: we will refer to a sequence of children (the \"right-hand-side\" of a rule) as a sequence of shifts, and the parent (or \"left-hand-side\" ) as the reduce operation. We write the rules with the children on the left leading \ufffdo the parent reduction on the right. Finally, a child symbol can have multiple shifts and multiple reductions to its right. standard representation S -<= NP VP NP -<= NP PP tree representation NP VP \u21d2 s \ufffd PP \u21d2 NP The tree grammar is then constructed in the straight-forward way, compressing the left prefixes of the right-hand-sides as much as possible. Using the Tree-Structured Grammar Parsing with the tree grammar is quite straightforward. The principle difference between this algo rithm and the classic chart algorithm [1] is that in the classic implementation, extending an active arc results in one new arc, whereas when using the tree-grammar, extending an arc may result in several new arcs. Finally, since one active arc could spawn multiple arcs, if we must keep track of of children used to create an arc (e.g. to resolve unifications), we must do so using an up-tree [3] We discuss the design of grammars for syntactic error detection. The topic is equally relevant for gram mar checkers and Intelligent Language Tutoring Systems (henceforth ILTS). The proposed methodol ogy \u2022addresses three interrelated issues recurrent in the error detection literature: Efficiency. In error detection the search space is larger than in ordinary parsing. Therefore there is a need to keep the search space manageable and to introduce some control mechanism over parsing. Modularity. Different knowledge sources need to be used, some of which are domain-dependent. In order to combine efficiency with modularity, different knowledge sources should be amenable of being stored separately but accessed in parallel at parsing time. A related issue is also the ability to reuse the same grammar for both error detection and ordinary parsing. Expressive power. There is a tendency to use unification-based frameworks of the same sort used for ordinary parsing. However, a more powerful machinery is needed for error detection than for ordinary parsing. Although the different requirements of error detection have been separately addressed in various ways, their contemporaneous satisfaction can be problematic, due to the drawbacks that solutions to one requirements can imply for other requirements. In the following, we show how an extended unification-based formalism can be used to satisfactorily address all the different requirements listed above at the same time. The described ideas have been implemented in a grammar for error detection, embedded in an ILTS for Modern Greek . Methodology description The task of our ILTS grammar module is to analyze an input sentence and return a list of violations, along with a list of 'non-violations', i.e. correct instances of grammatical phenomena occurring in a sentence, used to update an individual student model. Expressive power. We take an approach to error detection based on a unification grammar extended via procedural attachments. We defined a typed feature structure formalism with definite clause logic programming attachments, akin to existing formalisms like ALE [l] Abstract We describe a new model for dependency structure analysis. This model learns the relationship between two phrasal units called bunsetsus as three categories; 'between', 'dependent', and 'beyond', and estimates the dependency likelihood by considering not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and a:}l of the bunsetsus to its right. We implemented this model based on the maximum entropy model. When using the Kyoto University corpus, the dependency accuracy of our model was 88%, which is about 1 % higher than that of the conventional model using exactly the same features. 1 Introduction Dependency structure analysis is one of the basic techniques used in Japanese sentence analysis. The Japanese dependency structure is usually represented by the relationships between phrasal units called 'bunsetsu.' The analysis is done in two steps. In the first step , a dependency matrix is prepared where each element represents the likelihood of one bunsetsu being dependent on another in a sentence. In the second step , an optimal set of dependencies for the entire sentence is found. In this paper we only discuss the first step , a model for estimating the likelihood of dependency. In our approach , the value for each element in the dependency matrix is estimated as a proba bility. We previously developed a statistical model that conside..rs only the relationship between two bunsetsus when estimating the dependency likelihood [ l] . We call this model the old model in this work. Here we describe a model that considers not only the relationship between two bunsetsus, but also the relationship between the left bunsetsu and all of the bunsetsus to its right in a sentence. The probability of whole sentence dependencies is cakulated as the product of all the dependency probabilities in a sentence. By searching for the dependencies that maximize the probability, we can identify the optimal dependencies in a sentence. The dependencies in a sentence are identified by analyzing it from right to left[2J. Dependency Model Using Posterior Context Given a tokenization of a test corpus, the problem of dependency structure analysis in Japanese can be reduced to the problem of assigning one of two tags to each relationship between two bunsetsus. A relationship can be tagged with a 'l' or a '0 ' to indicate whether or not there is a dependency between the bunsetsus, respectively. Assigning these tags is the usual way to describe a dependency relationship (3 , 4, 1]. However, there are two other possibilities when there is not a dependency between two bunsetsus. One is the case where an anterior bunsetsu depends on one between it and the posterior bunsetsu. The other case is where an anterior bunsetsu depends on a bunsetsu beyond the posterior one. We believe there is a big diffe rence between the two cases. We developed a dependency model to identify this diffe rence. A dependency relationship between two bunsetsus is tagged with a 'O,' '1,' or '2 ' to indicate the three cases , respectively. The anterior bunsetsu can depend on (1) a bunsetsu between the two, (2) the posterior bunsetsu, or (3) a bunsetsu beyond the posterior one. Our new model uses these three categories while the old model uses only two. The dependency probability of two bunsetsus is estimated by using the product of the probabilities of the relationship between the left bunsetsu and those to its right in a sentence. We show how to estimate dependency probability with this model using the example in Fig. 1 . Figure. 1 shows a simulated calculation of the dependency probabilities of a bunsetsu that has five bunsetsus to its right and is represented by the left most circle. The probabilities of the relationship between this bunsetsu and each modifiee candidate are shown in the table in Fig. 1 . The dependencies of the five bunsetsus on the right are assumed to have been identified. Each dependency is represented by an arrow with a dotted line. In this example, bunsetsu 3 and 4 cannot be modified by the current bunsetsu because we assume that \"dependencies do not cross.\" For example, the dependency probability between the current bunsetsu and bunsetsu 5 is calculated as shown in the bottom example in Fig. 1 . It has a normalized probability of 52.2%. Experimental Result We implemented our model based on the maximum entropy model. We used in our experiments the same features as in Ref. (1] . Those features \u2022were basically some attributes of a bunsetsu itself or those between bunsetsus. We used the Kyoto University text corpus (Version 2) [5] , a tagged corpus of the Mainichi newspaper. For training we used 7,958 sentences from newspaper articles appearing from January 1st to January 8th in 1995, and for testing we used 1,246 sentences from articles appearing on January 9th. We assumed that the input sentences were morphologically analyzed and their bunsetsus were identified correctly. The results of our experiment are shown in Table 1 . The dependency accuracy means the percentage of correct dependencies out of the total analyzed. The sentence accuracy means the percentage of sentences in which all the dependendes were analyzed correctly. The first and the second lines in Table 1 compare the accuracy of our new model and the old model. The bottom line in Table 1 shows the accuracy when we assumed that every bunsetsu depended on the next one. The dependency accuracy of the new model was about 1 % better than that of the old model and there was a 3% improvement in sentence accuracy. Our Investigation of the relationships between sentence length (number of bunsetsus) and dependency accuracy, and between the amount of training data (number of sentences) and the accuracy of the model found that the accuracy of the new model was almost always better than that of the old one for any sentence length. A\ufffdd we found that the accuracy of the new model was about 1 % higher than that of the old model for any size of training data used. Abstract In an information system indexing can be accomplished by creating a citation based on context-free parses, and matching becomes a natural mechanism to extract patterns. However, the language intended to represent the document can often only be approximately defined, and indices can become shared forests. Queries could also vary from indices and an approximate matching strategy becomes also necessary. We present a proposal intended to prove the applicability of tabulation techniques in this context. A dynamic fr ame for parsing We use ICE [2] as parsing frame. The formalism is an extended,LALR( 1), push-down transducer (PDT). It proceeds by building items, compact representations of the PDT stacks, which are produced by applying transitions to existing ones, until no new application is possible. These items provide an optimal sharing of computations for non-deterministic inputs. A merit ordering guarantees fairness and completeness, and redundant items are ignored by using a simple subsumption relation. We represent a parse as the chain of the context-free rules used in a leftmost reduction of the input sentence, whose non-terminals are items. The output grammar is then represented in finite shared form by an AND-OR graph that in our case is precisely the shared-forest. The time complexity (resp. space complexity) for this bottom-up parser is O(n 3 ) (resp. O(n 2 )), for inputs of length n. This complexity is lineal for deterministic inputs, which favourices the performance in practice. A dynamic fr ame fo r approximate pattern matching",
    "abstract": "IWPT 2000, the Sixth International Workshop on Parsing Technologies, marks the existence of twelve years of parsing workshops, starting in 1989 with the workshop organized by Masaru Tomita in Pittsburgh and Hidden Valley, Pennsylvania. IWPT'89 was followed by four workshops in a biennial rhythm: IWPT'91 in Cancun (Mexico) IWPT'93 in Tilburg (The Netherlands) and Durbuy (Belgium) IWPT'95 in Prague and Karlovy Vary (Czech Republic) IWPT'97 in Boston/ Cambridge (Massachusetts). The series has achieved a number of successes, becoming the major forum for researchers in parsing technology to meet and discuss advances in the field, attracting a steady number of participants and submitted papers, and resulting in three books:",
    "countries": [
        "Spain",
        "United States",
        "France",
        "United Kingdom",
        "Hong Kong",
        "Germany"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2000",
    "month": "February 23-25",
    "title": "Proceedings of the Sixth Internatonal Workshop on Parsing Technologies"
}