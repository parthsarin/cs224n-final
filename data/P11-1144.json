{
    "article": "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F 1 score on a blind test set, over a state-of-the-art supervised baseline. Introduction Frame-semantic parsing aims to extract a shallow semantic structure from text, as shown in Figure 1 . The FrameNet lexicon (Fillmore et al., 2003) is a rich linguistic resource containing expert knowledge about lexical and predicate-argument semantics. The lexicon suggests an analysis based on the theory of frame semantics (Fillmore, 1982) . Recent approaches to frame-semantic parsing have broadly focused on the use of two statistical classifiers corresponding to the aforementioned subtasks: the first one to identify the most suitable semantic frame for a marked lexical predicate (target, henceforth) in a sentence, and the second for performing semantic role labeling (SRL) given the frame. The FrameNet lexicon, its exemplar sentences containing instantiations of semantic frames, and full-text annotations provide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a) . 1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we address the problem of idenfifying the semantic frames for targets unseen either in FrameNet (including the exemplar sentences) or the collection of full-text annotations released along with the lexicon. Using a standard model for the argument identification stage (Das et al., 2010a) , our proposed method improves overall frame-semantic parsing, especially for unseen targets. To better handle these unseen targets, we adopt a graph-based semi-supervised learning stategy ( \u00a74). We construct a large graph over potential targets, most of which are drawn from unannotated data, and a fraction of which come from seen FrameNet annotations. Next, we perform label propagation on the graph, which is initialized by frame distributions over the seen targets. The resulting smoothed graph consists of posterior distributions over semantic frames for each target in the graph, thus increasing coverage. These distributions are then evaluated within a frame-semantic parser ( \u00a75). Considering unseen targets in test data (although few because the test data is also drawn from the training domain), significant absolute improvements of 15.7% and 13.7% are observed for frame identification and full framesemantic parsing, respectively, indicating improved coverage for hitherto unobserved predicates ( \u00a76). Background Before going into the details of our model, we provide some background on two topics relevant to this paper: frame-semantic parsing and graph-based learning applied to natural language tasks. Frame-semantic Parsing Gildea and Jurafsky (2002) pioneered SRL, and since then there has been much applied research on predicate-argument semantics. Early work on frame-semantic role labeling made use of the exemplar sentences in the FrameNet corpus, each of which is annotated for a single frame and its arguments (Thompson et al., 2003; Fleischman et al., 2003; Shi and Mihalcea, 2004; Erk and Pad\u00f3, 2006, inter alia) . Most of this work was done on an older, smaller version of FrameNet. Recently, since the release of full-text annotations in SemEval'07 (Baker et al., 2007) , there has been work on identifying multiple frames and their corresponding sets of ar-guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval'07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a) . The current paper builds on those probabilistic models to improve coverage on unseen predicates. 2  Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia) . In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009) , and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad\u00f3 and Lapata, 2005) . Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F\u00fcrstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to improve the coverage of a frame-semantic parser on several syntactic categories, in a novel framework that makes use of graph-based semi-supervised learning. Graph-based Semi-Supervised Learning In graph-based semi-supervised learning, one constructs a graph whose vertices are labeled and unlabeled examples. Weighted edges in the graph, connecting pairs of examples/vertices, encode the degree to which they are expected to have the same label (Zhu et al., 2003) . Variants of label propagation are used to transfer labels from the labeled to the unlabeled examples. There are several instances of the use of graph-based methods for natural language tasks. Most relevant to our work an approach to word-sense disambiguation due to Niu et al. (2005) . Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.'s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. Our semi-supervised learning setting is similar to these two lines of work and, like them, we use the graph to arrive at better final structures, in an inductive setting (i.e., where a parametric model is learned and then separately applied to test data, following most NLP research). Approach Overview Our overall approach to handling unobserved targets consists of four distinct stages. Before going into the details of each stage individually, we provide their overview here: Graph Construction: A graph consisting of vertices corresponding to targets is constructed using a combination of frame similarity (for observed targets) and distributional similarity as edge weights. This stage also determines a fixed set of nearest neighbors for each vertex in the graph. Label Propagation: The observed targets (a small subset of the vertices) are initialized with empirical frame distributions extracted from FrameNet annotations. Label propagation results in a distribution of frames for each vertex in the graph. Supervised Learning: Frame identification and argument identification models are trained following Das et al. (2010a) . The graph is used to define the set of candidate frames for unseen targets. Parsing: The frame identification model of Das et al. disambiguated among only those frames associated with a seen target in the annotated data. For an unseen target, all frames in the FrameNet lexicon were considered (a large number). The current work replaces that strategy, considering only the top M frames in the distribution produced by label propagation. This strategy results in large improvements in frame identification for the unseen targets and makes inference much faster. Argument identification is done exactly like Das et al. (2010a) . Semi-Supervised Learning We perform semi-supervised learning by constructing a graph of vertices representing a large number of targets, and learn frame distributions for those which were not observed in FrameNet annotations. Graph Construction We construct a graph with targets as vertices. For us, each target corresponds to a lemmatized word or phrase appended with a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998) . 3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994) , and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs and adjectives/adverbs and the thesaurus contains three parts for each of these categories. For each item in the thesaurus, 200 nearest neighbors are listed with a symmetric similarity score between 0 and 1. We processed this thesaurus in two ways: first, we lowercased and lemmatized each word/phrase and merged entries which shared the same lemma; second, we separated the adjectives and adverbs into two lists from Lin's original list by scanning a POS-tagged version of the Gigaword corpus (Graff, 2003) and categorizing each item into an adjective or an adverb depending on which category the item associated with more often in the data. The second step was necessary because FrameNet treats adjectives and adverbs separately. At the end of this processing step, we were left with 61,702 units-approximately six times more than the targets found in FrameNet annotations-each labeled with one of 4 coarse tags. We considered only the top 20 most similar targets for each target, and noted Lin's similarity between two targets t and u, which we call sim DL (t, u). The second component of graph construction comes from FrameNet itself. We scanned the exemplar sentences in FrameNet 1.5 4 and the training section of the full-text annotations that we use to train the probabilistic frame parser (see \u00a76.1), and gathered a distribution over frames for each target. For a pair of targets t and u, we measured the Euclidean distance 5 between their frame distributions. This distance was next converted to a similarity score, namely, sim F N (t, u) between 0 and 1 by subtracting each one from the maximum distance found in the whole data, followed by normalization. Like sim DL (t, u), this score is symmetric. This resulted in 9,263 targets, and again for each, we considered the 20 most similar targets. Finally, the overall similarity between two given targets t and u was computed as: sim(t, u) = \u03b1 \u2022 sim F N (t, u) + (1 \u2212 \u03b1) \u2022 sim DL (t, u) Note that this score is symmetric because its two components are symmetric. The intuition behind taking a linear combination of the two types of similarity functions is as follows. We hope that distributionally similar targets would have the same semantic frames because ideally, lexical units evoking the same set of frames appear in similar syntactic contexts. We would also like to involve the annotated data in graph construction so that it can eliminate some noise in the automatically constructed thesaurus. 6 Let K(t) denote the K most similar targets to target t, under the score sim. We link vertices t and u in the graph with edge weight w tu , defined as: w tu = sim(t, u) if t \u2208 K(u) or u \u2208 K(t) 0 otherwise (1) The hyperparameters \u03b1 and K are tuned by crossvalidation ( \u00a76.3). Label Propagation First, we softly label those vertices of the constructed graph for which frame distributions are available from the FrameNet data (the same distributions that are used to compute sim F N ). Thus, initially, a small fraction of the vertices in the graph have soft frame labels on them. Figure 2 shows an excerpt from a constructed graph. For simplicity, only the most probable frames under the empirical distribution for the observed targets are shown; we actually label each vertex with the full empirical distribution over frames for the corresponding observed target in the data. The dotted lines demarcate parts of the graph that associate with different frames. Label propagation helps propagate the initial soft labels throughout the graph. To this end, we use a variant of the quadratic cost criterion of Bengio et al. (2006) , also used by Subramanya et al. (2010) and Das and Petrov (2011) . 7 Let V denote the set of all vertices in the graph, V l \u2282 V be the set of known targets and F denote the set of all frames. Let N (t) denote the set of neighbors of vertex t \u2208 V . Let q = {q 1 , q 2 , . . . , q |V | } be the set of frame distributions, one per vertex. For each known target t V l , we have an initial frame distribution r t . For every edge in the graph, weights are defined as in Eq. 1. We find q by solving: arg min q t\u2208V l r t \u2212 q t 2 + \u00b5 t\u2208V,u\u2208N (t) w tu q t \u2212 q u 2 + \u03bd t\u2208V q t \u2212 1 |F | 2 s.t. \u2200t \u2208 V, f \u2208F q t (f ) = 1 \u2200t \u2208 V, f \u2208 F, q t (f ) \u2265 0 (2) We use a squared loss to penalize various pairs of distributions over frames: a\u2212b 2 = f \u2208F (a(f )\u2212 b(f )) 2 . The first term in Eq. 2 requires that, for known targets, we stay close to the initial frame distributions. The second term is the graph smoothness regularizer, which encourages the distributions of similar nodes (large w tu ) to be similar. The final term is a regularizer encouraging all distributions to be uniform to the extent allowed by the first two terms. (If an unlabeled vertex does not have a path to any labeled vertex, this term ensures that its converged marginal will be uniform over all frames.) \u00b5 and \u03bd are hyperparameters whose choice we discuss in \u00a76.3. Note that Eq. 2 is convex in q. While it is possible to derive a closed form solution for this objective function, it would require the inversion of a |V |\u00d7|V | matrix. Hence, like Subramanya et al. (2010) , we employ an iterative method with updates defined as: \u03b3 t (f ) \u2190 r t (f )1{t \u2208 V l } (3) + \u00b5 u\u2208N (t) w tu q (m\u22121) u (f ) + \u03bd |F| \u03ba t \u2190 1{t \u2208 V l } + \u03bd + \u00b5 u\u2208N (t) w tu (4) q (m) t (f ) \u2190 \u03b3 t (f )/\u03ba t (5) Here, 1{\u2022} is an indicator function. The iterative procedure starts with a uniform distribution for each q (0) t . For all our experiments, we run 10 iterations of the updates. The final distribution of frames for a target t is denoted by q * t . 5 Learning and Inference for Frame-Semantic Parsing In this section, we briefly review learning and inference techniques used in the frame-semantic parser, which are largely similar to Das et al. (2010a) , except the handling of unknown targets. Note that in all our experiments, we assume that the targets are marked in a given sentence of which we want to extract a frame-semantic analysis. Therefore, unlike the systems presented in SemEval'07, we do not define a target identification module. Frame Identification For a given sentence x with frame-evoking targets t, let t i denote the ith target (a word sequence). We seek a list f = f 1 , . . . , f m of frames, one per target. Let L be the set of targets found in the FrameNet annotations. Let L f \u2286 L be the subset of these targets annotated as evoking a particular frame f . The set of candidate frames F i for t i is defined to include every frame f such that t i \u2208 L f . If t i \u2208 L (in other words, t i is unseen), then Das et al. (2010a) considered all frames F in FrameNet as candidates. Instead, in our work, we check whether t i \u2208 V , where V are the vertices of the constructed graph, and set: F i = {f : f \u2208 M -best frames under q * t i } (6) The integer M is set using cross-validation ( \u00a76.3). If t i \u2208 V , then all frames F are considered as F i . The frame prediction rule uses a probabilistic model over frames for a target: f i \u2190 arg max f \u2208F i \u2208L f p(f, | t i , x) (7) Note that a latent variable \u2208 L f is used, which is marginalized out. Broadly, lexical semantic relationships between the \"prototype\" variable (belonging to the set of seen targets for a frame f ) and the target t i are used as features for frame identification, but since is unobserved, it is summed out both during inference and training. A conditional log-linear model is used to model this probability: for f \u2208 F i and \u2208 L f , p \u03b8 (f, | t i , x) = exp \u03b8 g(f, , t i , x) f \u2208F i \u2208L f exp \u03b8 g(f , , t i , x) (8) where \u03b8 are the model weights, and g is a vectorvalued feature function. This discriminative formulation is very flexible, allowing for a variety of (possibly overlapping) features; e.g., a feature might relate a frame f to a prototype , represent a lexicalsemantic relationship between and t i , or encode part of the syntax of the sentence (Das et al., 2010b) . Given some training data, which is of the form x (j) , (j) , f (j) , A (j) N j=1 (where N is the number of sentences in the data and A is the set of argument in a sentence), we discriminatively train the frame identification model by maximizing the following log-likelihood: 8 max \u03b8 N j=1 m j i=1 log \u2208L f (j) i p \u03b8 (f (j) i , | t (j) i , x (j) ) (9) This non-convex objective function is locally optimized using a distributed implementation of L-BFGS (Liu and Nocedal, 1989 ). 9 Argument Identification Given a sentence x = x 1 , . . . , x n , the set of targets t = t 1 , . . . , t m , and a list of evoked frames f = f 1 , . . . , f m corresponding to each target, argument identification or SRL is the task of choosing which of each f i 's roles are filled, and by which parts of x. We directly adopt the model of Das et al. (2010a) for the argument identification stage and briefly describe it here. Let R f i = {r 1 , . . . , r |R f i | } denote frame f i 's roles observed in FrameNet annotations. A set S of spans that are candidates for filling any role r \u2208 R f i are identified in the sentence. In principle, S could contain any subsequence of x, but we consider only the set of contiguous spans that (a) contain a single word or (b) comprise a valid subtree of a word and all its descendants in a dependency parse. The empty span is also included in S, since some roles are not explicitly filled. During training, if an argument is not a valid subtree of the dependency parse (this happens due to parse errors), we add its span to S. Let A i denote the mapping of roles in R f i to spans in S. The model makes a prediction for each A i (r k ) (for all roles r k \u2208 R f i ): A i (r k ) \u2190 arg max s\u2208S p(s | r k , f i , t i , x) (10) A conditional log-linear model over spans for each role of each evoked frame is defined as: p \u03c8 (A i (r k ) = s | f i , t i , x) = (11) exp \u03c8 h(s, r k , f i , t i , x) s \u2208S exp \u03c8 h(s , r k , f i , t i , x) This model is trained by optimizing: max \u03c8 N j=1 m j i=1 |R f (j) i | k=1 log p \u03c8 (A (j) i (r k ) | f (j) i , t (j) i , x (j) ) This objective function is convex, and we globally optimize it using the distributed implementation of L-BFGS. We regularize by including \u2212 1 10 \u03c8 2 2 in the objective (the strength is not tuned). Na\u00efve prediction of roles using Equation 10 may result in overlap among arguments filling different roles of a frame, since the argument identification model fills each role independently of the others. We want to enforce the constraint that two roles of a single frame cannot be filled by overlapping spans. Hence, illegal overlap is disallowed using a 10,000hypothesis beam search. Experiments and Results Before presenting our experiments and results, we will describe the datasets used in our experiments, and the various baseline models considered. Data We make use of the FrameNet 1.5 lexicon released in 2010. This lexicon is a superset of previous versions of FrameNet. It contains 154,607 exemplar sentences with one marked target and frame-role annotations. 78 documents with full-text annotations with multiple frames per sentence were also released (a superset of the SemEval'07 dataset). We randomly selected 55 of these documents for training and treated the 23 remaining ones as our test set. After scanning the exemplar sentences and the training data, we arrived at a set of 877 frames, 1,068 roles, 10 and 9,263 targets. Our training split of the full-text annotations contained 3,256 sentences with 19,582 frame annotatations with corresponding roles, while the test set contained 2,420 sentences with 4,458 annotations (the test set contained fewer annotated targets per sentence). We also divide the 55 training documents into 5 parts for crossvalidation (see \u00a76.3). The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005) following Das et al. (2010a) . In this work we assume the frame-evoking targets have been correctly identified in training and test data. Baselines We compare our model with three baselines. The first baseline is the purely supervised model of Das et al. (2010a) trained on the training split of 55 documents. Note that this is the strongest baseline available for this task; 11 we refer to this model as \"SEMAFOR.\" The second baseline is a semi-supervised selftrained system, where we used SEMAFOR to label 70,000 sentences from the Gigaword corpus with frame-semantic parses. For finding targets in a raw sentence, we used a relaxed target identification scheme, where we marked every target seen in the lexicon and all other words which were not prepositions, particles, proper nouns, foreign words and Wh-words as potential frame evoking units. This was done so as to find unseen targets and get frame annotations with SEMAFOR on them. We appended these automatic annotations to the training data, resulting in 711,401 frame annotations, more than 36 times the supervised data. These data were next used to train a frame identification model ( \u00a75.1). 12 This setup is very similar to Bejan (2009) who used selftraining to improve frame identification. We refer to this model as \"Self-training.\" The third baseline uses a graph constructed only with Lin's thesaurus, without using supervised data. In other words, we followed the same scheme as in \u00a74.1 but with the hyperparameter \u03b1 = 0. Next, label propagation was run on this graph (and hyperparameters tuned using cross validation). The posterior distribution of frames over targets was next used for frame identification (Eq. 6-7), with SEMAFOR as the trained model. This model, which is very similar to our full model, is referred to as \"LinGraph.\" \"FullGraph\" refers to our full system. Experimental Setup We used five-fold cross-validation to tune the hyperparameters \u03b1, K, \u00b5, and M in our model. The   uniform regularization hyperparameter \u03bd for graph construction was set to 10 \u22126 and not tuned. For each cross-validation split, four folds were used to train a frame identification model, construct a graph, run label propagation and then the model was tested on the fifth fold. This was done for all hyperparameter settings, which were \u03b1 \u2208 {0.2, 0.5, 0.8}, K \u2208 {5, 10, 15, 20}, \u00b5 \u2208 {0.01, 0.1, 0.3, 0.5, 1.0} and M \u2208 {2, 3, 5, 10}. The joint setting which performed the best across five-folds was \u03b1 = 0.2, K = 10, \u00b5 = 1.0, M = 2. Similar tuning was also done for the baseline LinGraph, where \u03b1 was set to 0, and rest of the hyperparameters were tuned (the selected hyperparameters were K = 10, \u00b5 = 0.1 and M = 2). With the chosen set of hyperparameters, the test set was used to measure final performance. The standard evaluation script from the Se-mEval'07 task calculates precision, recall, and F 1score for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one in the FrameNet lexicon. We present precision, recall, and F 1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels. This evaluation scheme follows Das et al. (2010a) . Statistical significance is measured using a reimplementation of Dan Bikel's parsing evaluation comparator. 13 Model Exact Match Partial Match Exact Match Partial Match P R F 1 P R F 1 P R F 1 P R F 1 SEMAFOR Results Tables 1 and 2 present results for frame identification and full frame-semantic parsing respectively. They also separately tabulate the results achieved for unknown targets. Our full model, denoted by \"FullGraph,\" outperforms all the baselines for both tasks. Note that the Self-training model even falls 13 http://www.cis.upenn.edu/ \u02dcdbikel/ software.html#comparator short of the supervised baseline SEMAFOR, unlike what was observed by Bejan (2009) for the frame identification task. The model using a graph constructed solely from the thesaurus (LinGraph) outperforms both the supervised and the self-training baselines for all tasks, but falls short of the graph constructed using the similarity metric that is a linear combination of distributional similarity and supervised frame similarity. This indicates that a graph constructed with some knowledge of the supervised data is more powerful. For unknown targets, the gains of our approach are impressive: 15.7% absolute accuracy improvement over SEMAFOR for frame identification, and 13.7% absolute F 1 improvement over SEMAFOR for full frame-semantic parsing (both significant). When all the test targets are considered, the gains are still significant, resulting in 5.4% relative error reduction over SEMAFOR for frame identification, and 1.3% relative error reduction over SEMAFOR for full-frame semantic parsing. Although these improvements may seem modest, this is because only 3.2% of the test set targets are unseen in training. We expect that further gains would be realized in different text domains, where FrameNet coverage is presumably weaker than in news data. A semi-supervised strategy like ours is attractive in such a setting, and future work might explore such an application. Our approach also makes decoding much faster. For the unknown component of the test set, SE-MAFOR takes a total 111 seconds to find the best set of frames, while the FullGraph model takes only 19 seconds to do so, thus bringing disambiguation time down by a factor of nearly 6. This is because our model now disambiguates between only M = 2 frames instead of the full set of 877 frames in FrameNet. For the full test set too, the speedup 3 : Top 5 frames according to the graph posterior distribution q * t (f ) for four targets: discrepancy.N, contribution.N, print.V and mislead.V. None of these targets were present in the supervised FrameNet data. * marks the correct frame, according to the test data. EXPERIENCER OBJ is described in FrameNet as \"Some phenomenon (the Stimulus) provokes a particular emotion in an Experiencer.\" is noticeable, as SEMAFOR takes 131 seconds for frame identification, while the FullGraph model only takes 39 seconds. t = discrepancy.N t = contribution.N t = print.V t = mislead.V f q * t (f ) f q * t (f ) f q * t (f ) f q * t ( Discussion The following is an example from our test set showing SEMAFOR's output (for one target): Note that the model identifies an incorrect frame REASON for the target discrepancy.N, in turn identifying the wrong semantic role Action for the underlined argument. On the other hand, the FullGraph model exactly identifies the right semantic frame, SIMILARITY, as well as the correct role, Entities. This improvement can be easily explained. The excerpt from our constructed graph in Figure 2 shows the same target discrepancy.N in black, conveying that it did not belong to the supervised data. However, it is connected to the target difference.N drawn from annotated data, which evokes the frame SIMILARITY. Thus, after label propagation, we expect the frame SIMILARITY to receive high probability for the target discrepancy.N. Table 3 shows the top 5 frames that are assigned the highest posterior probabilities in the distribution q * t for four hand-selected test targets absent in supervised data, including discrepancy.N. For all of them, the FullGraph model identifies the correct frames for all four words in the test data by ranking these frames in the top M = 2. LinGraph also gets all four correct, Self-training only gets print.V/TEXT CREATION, and SEMAFOR gets none. Across unknown targets, on average the M = 2 most common frames in the posterior distribution q * t found by FullGraph have q ( * ) t (f ) = 7 877 , or seven times the average across all frames. This suggests that the graph propagation method is confident only in predicting the top few frames out of the whole possible set. Moreover, the automatically selected number of frames to extract per unknown target, M = 2, suggests that only a few meaningful frames were assigned to unknown predicates. This matches the nature of FrameNet data, where the average frame ambiguity for a target type is 1.20. Conclusion We have presented a semi-supervised strategy to improve the coverage of a frame-semantic parsing model. We showed that graph-based label propagation and resulting smoothed frame distributions over unseen targets significantly improved the coverage of a state-of-the-art semantic frame disambiguation model to previously unseen predicates, also improving the quality of full framesemantic parses. The improved parser is available at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We are grateful to Amarnag Subramanya for helpful discussions. We also thank Slav Petrov, Nathan Schneider, and the three anonymous reviewers for valuable comments. This research was supported by NSF grants IIS-0844507, IIS-0915187 and TeraGrid resources provided by the Pittsburgh Supercomputing Center under NSF grant number TG-DBS110003.",
    "abstract": "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F 1 score on a blind test set, over a state-of-the-art supervised baseline.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "95",
    "year": "2011",
    "month": "June",
    "title": "Semi-Supervised Frame-Semantic Parsing for Unknown Predicates"
}