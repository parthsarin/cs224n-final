{
    "article": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models. Introduction Latent Dirichlet allocation (LDA) models provide useful insights into themes and trends in a large text collection through the unsupervised inference of topics, or probability distributions over unigram word types in the corpus (Blei et al., 2003) . Topics from these models are often interpreted based on their highest-probability words, with documents expressed as vectors of proportions of each topic. Unfortunately, the context in which these tokens arise can be obscured in the bag-of-words rendering of text as unigram counts in documents. For instance, a topic with high probabilities of both \"coffee\" and \"table\" is tempting to interpret as focusing on the furniture item \"coffee table\", but both words could be frequent in a discussion of cafes containing no coffee tables. This problem is amplified in languages without marked word boundaries, such as Chinese and Thai: while existing tokenizers in these languages can segment characters into * Corresponding author words, there is always a question about to what extent the tokenizers should group words together. Words that have been segmented by tokenizers may not express the concept of the original text if they were found as parts of collocations. Meaningful interpretation of topics can be lost without careful recombination of these words. We hypothesize that the morphology of the language should play an important role in determining the suitable pre-processing steps that would improve the results of topic models. The main morphological types we consider are synthetic language and analytic language. Synthetic languages use many morphemes to compose a word and can be further divided into fusional and agglutinative languages. Fusional languages such as German differ from agglutinative languages such as Korean and Japanese: a single morpheme in fusional languages can code for many morphosyntactic features. On the other hand, analytic languages such as Thai and Chinese convey meanings by relating many words together, and morphological devices are more rarely used. Under our hypothesis, analytic languages should benefit from token merging, but synthetic languages might not because the meaning is conveyed by inflection (through bound morphemes) and agglutination (through free morphemes). In this project, we investigate the effects of token merging as a pre-processing step, and study how those effects vary based on the writing systems and the morphological features of the languages. We evaluate three measures to determine when to merge multiple adjacent words into conceptuallyunified phrasal tokens prior to LDA model training: chi-squared statistics, t-statistics, and raw frequency counts of phrases. We test these merging strategies on English, German, Chinese, Japanese, Korean, Thai, and Arabic. This set of languages is drawn from various writing systems and different morphological typology to see which type of language favors which type of merging strategy. The main contributions of this paper are as follows: \u2022 We determine through empirical studies that a t-statistic and raw-frequency approach to token merging improves the topic modeling results across all language types and writing systems for the corpora that do not differ much from the collocation training data. \u2022 We also show the positive consequences of token merging: the percentage of merged tokens in the LDA training data is correlated with the quality of the topic modeling results. \u2022 Finally, we provide evidence that the popular approach of applying a \u03c7 2 measure to token merging tends to overfit to the collocation training data and result in a low percentage of merged tokens in a number of languages, making it a less suitable general-purpose approach than t-statistics. Related Work Pre-processing steps can substantially alter the results of the LDA models even in languages with good tokenization heuristics such as English (Schofield and Mimno, 2016; May et al., 2016) . We believe that languages that do not have clear tokenization standards deserve investigation into what kind of processing is appropriate. Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012; Lau et al., 2013; Yu et al., 2013; El-Kishky et al., 2014; Wang et al., 2016; Bin et al., 2018; Li et al., 2018) . We consider it valuable to specifically assess approaches to determining these phrases. Despite their popularity in analyzing large amounts of text data, LDA models are notoriously complex to evaluate. One must evaluate both the statistical fit of a model and the human-registered thematic coherence of the words found to arise in the high-probability words, or keys, of a topic, which may not correlate (Chang et al., 2009) . Analyses often combine evaluations of fit (Wallach et al., 2009) and automated approximations of human judgments of coherence (Bouma, 2009; Mimno et al., 2011) based on mutual information, even with the expectation these may only somewhat correlate with true human judgments (Lau et al., 2014) . A limitation of these existing approaches, however, is that they expect the vocabulary and tokenization to remain constant between the two models. For our evaluation, we use a normalized log-likelihood approach to capture fit while accounting for changes in vocabulary (Schofield and Mimno, 2016) . Collocations as LDA Token Collocations consist of two or more words that express conventional meaning, which can convey information about multi-word entities, context, and word usage. We hypothesize that the introduction of multi-word tokens, which capture collocations as bigrams or trigrams by way of concatenation of adjacent tokens, can help achieve more useful and coherent topic models. For languages without clear word boundaries, there is a possible additional benefit to multi-word tokens: it can be hard to intuit whether inferred word boundaries will have a large impact on the final results. Merging adjacent words into 'multi-word' tokens may help remedy the potential problem of a segmentation that is not optimal for topic modeling purposes. Many methods are possible to select collocations to merge from tokenized text (Manning and Schutze, 1999) . In this paper, we evaluate the chisquared statistics (\u03c7 2 ), the t-statistic and raw frequency as approaches to develop a threshold for merging collocations into multi-word tokens prior to topic model training. The chi-squared measure \u03c7 2 (w 1 , w 2 ) and t(w 1 , w 2 ) t-statistic for two adjacent tokens w 1 and w 2 are defined as: \u03c7 2 (w 1 , w 2 ) = (P (w 1 , w 2 ) \u2212 P (w 1 )P (w 2 )) 2 P (w 1 )P (w 2 ) (1) t(w 1 , w 2 ) = x \u2212 \u00b5 s 2 N \u2248 P (w 1 , w 2 ) \u2212 P (w 1 )P (w 2 ) P (w 1 ,w 2 ) N (2) We first compute the collocation measures for all bigrams on a large collocation training corpus. Then we select the top bigrams that score the highest on the collocation measures and add those to our lexicon. After we tokenize and pre-process the collection of documents on which we would like to train LDA, we retokenize the data based on the collocation training corpus. We find all of the bigrams in the LDA training data that are also found in the top bigram lexicons that we obtain from the collocation training corpus. Then, the LDA training process proceeds as usual but with some of the original tokens merged into multi-word tokens as defined from the collocation training data. Evaluation Metrics We consider two primary evaluation metrics for exploring the effect of merging tokens: one based on log-likelihood, and one based on silhouette coefficients. Held-Out Likelihood. When multi-word phrases are converted to individual tokens, the number of tokens in the document decreases while the size of the corpus vocabulary increases. It is therefore illogical to compare the likelihoods of the word-token model and collocation-token model directly. In order to normalize the scores between the two models that do not have the exact same vocabulary and tokens, we use the log-likelihood ratio between the LDA model likelihood and the null (unigram) likelihood for each model. In other words, we normalize the LDA model likelihood (L model ) by dividing it with the unigram likelihood (L unigram ) as introduced by Schofield and Mimno (2016) . Therefore, the normalized loglikelihood per token (PTLL norm ) is PTLL norm = log L model \u2212 log L unigram N ( 3 ) where N is the number of tokens. Since likelihood per token has been normalized by the unigram likelihood per token, the higher the PTLL, the better the model. Concatenation-based Embedding Silhouette (CBES) Previous measures of topic coherence rely on statistics from the training data and assume that the vocabularies are identical for both models, which is not the case for our settings. To address this, we propose a new application of the silhouette coefficients (Rousseeuw, 1987), a common clustering evaluation metric to measure topic coherence. A good topic should have all of its topic keys close to each other and away from other words that do not belong in the same topic. Therefore, the word embeddings of these topic keys should have shorter cosine distances within the same topic, and longer distances to the topic keys in other topics. When words are represented as a vector, this is exactly what the silhouette coefficients measure. To compute them, we first compute the a(i), which is the mean cosine distance between topic-key i and other topic-keys in the same topic. a(i) = 1 | C i | \u22121 j\u2208C i ,i =j d(i, j) (4) where d(i, j) is the distance between ith and jth topic-key and | C i | is the number of topic-keys in topic i. Then for each other topic, we compute the mean of the distance of topic-key i to topic-keys in that other topic. And b(i) is the smallest of such mean among other topics. b(i) = min k =i 1 | C k | j\u2208C k d(i, j) (5) After obtaining a(i) and b(i), the silhouette coefficient for topic-key i is defined as: s(i) = b(i) \u2212 a(i) max(a(i), b(i)) , if | C i |> 1 (6) and s(i) = 0, if | C i |= 1 (7) The silhouette coefficient for the entire model is the average s(i) over all i. The larger silhouette coefficient means that topic-keys are relatively similar within their topic and different from other topics. In order to compare the distances among words merged by different criteria, all compared word embeddings must be in the same space. Since merged tokens will modify the vocabulary of the corpus, we create four versions of the word embedding training corpus: the original version and the three other versions where tokens are merged based on \u03c7 2 , t and frequency collocation measures. We train the word embeddings on these four versions of the corpus so we can then compare word embeddings on a consistent vocabulary in each retokenization scheme. Experiments We hypothesize that morphology should play an important role in determining the suitable preprocessing steps. We test our methods on one fusional language (German), two agglutinative languages (Japanese and Korean), three analytic languages (Chinese, Thai, and Arabic), and English, which can be thought of as either analytic or fusional. These languages also represent languages drawn from all writing systems: logograms (Chinese), syllabic system (Japanese), featural system (Korean), abugida (Thai), abjad (Arabic), and true alphabets (English and German). The English corpora are drawn from The New York Times (Sandhaus, 2008) , the Yelp Dataset 1 , and United States State of the Union addresses (1790 to 2018) divided into paragraphs 2 . The German data come from Ten Thousand German News Articles Dataset 3 . The Chinese data come from three corpora: the news articles from Chinanews 4 , restaurant reviews from Dianping 5 , and the movie reviews from Douban 6 . The Japanese data is from the Webhose's Free Datasets 7 . The Korean data come from the KAIST Corpus 8 . The Thai data come from the news articles in Prachathai 9 , the restaurant reviews from Wongnai 10 , the BEST corpus 11 , and the Thai National Corpus (Aroonmanakun, 2007) . The Arabic data come from the Antcorpus (Chouigui et al., 2017) . Each corpus is separated into 75% training documents and 25% test documents (Table 1 ). We train the \u03c7 2 , t, and frequency-based tokenizers for each language on Wikipedia articles for that language. For all languages, we use the reduced version of Wikipedia database, except for English we use the filtered Wiki103 dataset (Merity et al., 2016) . English, German, Chinese, Japanese, Korean, Thai and Arabic documents are tokenized with NLTK (Bird, 2006) To train word embeddings, we use the gensim ( \u0158eh\u016f\u0159ek and Sojka, 2010) implementation with the Continuous Bag-of-Word (CBOW) algorithm (Mikolov et al., 2013) to obtain word embeddings. The training corpora and their collocation versions are prepared based on the tokenizers that we discuss above. We preprocess the word embedding training data and the LDA training data the same way. For English, we lemmatize and lowercase the data. For Korean, Japanese, and Arabic, we lemmatize the data. For German, Chinese, and Thai, we do not do any normalization. We use MALLET (McCallum, 2002) implementation of LDA with the default hyperparameters to train and evaluate topic models in both word and multi-word (collocation) documents with 10, 50, 100 topics. We run the experiment 3 times for each combination of corpus, type of retokenization (no retokenization, \u03c7 2 , t or frequency) and number of topics to compute the means of the normalized held-out likelihood and CBES, discussed in section 4. Results and Discussion The normalized log-likelihood per token of the t and frequency-based retokenization is significantly higher than the baseline for English, German, Chinese, Japanese, Korean, and Arabic for all text collections and the number of topics except EN-Yelp, TH-BEST, and TH-TNC (Table 3 ). Frequency- (bottom) for between the baseline and retokenization models: \u03c7 2 , textitt, and raw frequency. Shaded cells mean that the results are inferior to the baseline, while bolded cells show the best results for each corpus and number of topics. based retokenization gives the best results for most settings but not significantly higher than t retokenization. However, we observe mixed results from \u03c7 2 retokenization for some languages. This is quite surprising because raw frequency was previously found to be an inferior measure of collocation. This suggests that t and frequency-based retokenization might be a more reliable method for improving the goodness of fit of the LDA model. This also suggests that Japanese and Korean might have some specific quality that interacts well with all three types of retokenization. Similarly, we observe a general improvement in coherence for the t and frequency retokenization (Table 3 ). The higher CBES score indicates that topic-keys are more semantically coherent and topics are more distinct. The coherence improves after t and frequency-based retokenization for English, Japanese, Korean, and Arabic corpora regardless of the number of topics. The improvement for Thai is spotty, and Chinanews is the only Chinese corpus in which we see improvement. This suggests that the choice of retokenization strategy might depend on the language types or the content of corpora itself. Consistent with the normalized log-likelihood results, Japanese and Korean corpora interact well with all three types of retokenization, suggesting that the morphology or typology of these two languages consistently benefit from collocation before training LDA models. What could account for this discrepancy across languages and corpora? First, we observe a large variation of percentages of merged tokens across corpora. Because we fix the number of bigrams types to merge during the tokenizer training process to 50,000 for all three criteria (Table 1 ), we can use this analysis to find trends in the relative frequency of merged tokens. We see that \u03c7 2 retokenizer only merges barely 1% of all the tokens before training the LDA models for English, Chinese, \u03c7 2 : dvenadsat apostolov, jormp jomp, malwae tweep, aboul gheit, achduth vesholom, adavari matalaku, adeste fideles, afforementionede oughtt, agoraf drws, aht urhgan, akanu ibiam, aksak maboul, alberthiene endah, alfava metraxis, alfonsas eidintas, allasani peddana, alteram partem, amantes clandestinos, amarin winitchai, amel oluna t: united states, new york, world war, km h, take place, miles km, los angeles, united kingdom, first time, high school, tropical storm, new zealand, war ii, video game, mph km, h mph, north america, air force, two years, peak number frequency: united states, new york, world war, km h, take place, miles km, first time, los angeles, united kingdom, high school, tropical storm, new zealand, video game, war ii, mph km, two years, h mph, north america, air force, peak number German, Arabic, and Thai corpora, possibly introducing noise in the data that yield the results similar to or worse than the baseline. In contrast, the t and frequency-based retokenizers merge around 8%-15% of all the tokens for English, German, and Chinese. Arabic has seen the highest merging percentage of 26%-27%. Notably, around 20 % of tokens are retokenized by all three retokenizers in Japanese and Korean. The truncation of the top \u03c7 2 bigrams list might cause this different behavior. The number of \u03c7 2 collocations that pass the hypothesis testing is significantly larger than that of t collocations. For example, there are 3.73 million \u03c7 2 collocations versus 231 thousand t collocations in Thai for the same significance level \u03b1 = 0.005. This full list of \u03c7 2 collocations includes all the top collocations from the t score and frequency treatments, implying that were we to use this significance threshold, the percentage of merged word would be at least as high as the two methods. However, the large vocabulary that the \u03c7 2 approach induces is impractical in many applications, suggesting it is an inefficient approach if the goal is primarily to merge frequent ngrams. \u03c7 2 : \u3046\u305d\u5bd2\u3044 \u808c\u5bd2, \u304e \u304e \u304e\u3063 \u3063 \u3063\u3053 \u3053 \u3053\u3093 \u3093 \u3093 \u3070 \u3070 \u3070\u3063 \u3063 \u3063\u305f \u305f \u305f\u3093 \u3093 \u3093, \u3056\u3089\u308a \u3050\u3089\u308a, \u3078\u3078\u3078\u3078 \u3078\u3078\u3078, \u30a2 \u30a2 \u30a2\u30a6 \u30a6 \u30a6\u30ec \u30ec \u30ec\u30aa \u30aa \u30aa\u30eb \u30eb \u30eb\u30b9 \u30b9 \u30b9 \u30dc \u30dc \u30dc\u30f3 \u30f3 \u30f3\u30d0 \u30d0 \u30d0\u30b9 \u30b9 \u30b9\u30c8 \u30c8 \u30c8\u30a5 \u30a5 \u30a5\u30b9 \u30b9 \u30b9, \u30a2 \u30a2 \u30a2\u30b8 \u30b8 \u30b8 \u30bf \u30bf \u30bf \u30b1 \u30b1 \u30b1\u30b5 \u30b5 \u30b5\u30ab \u30ab \u30ab\u30f3 \u30f3 \u30f3\u30d0 \u30d0 \u30d0\u30ea \u30ea \u30ea\u30f3 \u30f3 \u30f3, \u30a2 \u30a2 \u30a2\u30c3 \u30c3 \u30c3\u30b7 \u30b7 \u30b7\u30e3 \u30e3 \u30e3\u30eb \u30eb \u30eb\u30af \u30af \u30af \u30a2 \u30a2 \u30a2\u30eb \u30eb \u30eb\u30a2 \u30a2 \u30a2\u30a6 \u30a6 \u30a6\u30b5 \u30b5 \u30b5\u30c8 \u30c8 \u30c8, \u30a2 \u30a2 \u30a2\u30c8 \u30c8 \u30c8\u30df \u30df \u30df\u30ba \u30ba \u30ba\u30e0 \u30e0 \u30e0 \u30a2 \u30a2 \u30a2\u30c9 \u30c9 \u30c9\u30ea \u30ea \u30ea\u30a2 \u30a2 \u30a2\u30b7 \u30b7 \u30b7\u30f3 \u30f3 \u30f3, \u30a2 \u30a2 \u30a2\u30c9 \u30c9 \u30c9\u30ea \u30ea \u30ea\u30a2 \u30a2 \u30a2\u30b7 \u30b7 \u30b7\u30f3 \u30f3 \u30f3 \u30a2 \u30a2 \u30a2\u30c9 \u30c9 \u30c9\u30ea \u30ea \u30ea\u30a2 \u30a2 \u30a2\u30de \u30de \u30de\u30a4 \u30a4 \u30a4\u30b7 \u30b7 \u30b7 \u30f3 \u30f3 \u30f3, \u30a2 \u30a2 \u30a2\u30eb \u30eb \u30eb\u30d1 \u30d1 \u30d1\u30a4 \u30a4 \u30a4 \u30aa \u30aa \u30aa\u30b6 \u30b6 \u30b6\u30e9 \u30e9 \u30e9\u30f3 \u30f3 \u30f3, \u30a2 \u30a2 \u30a2\u30ef \u30ef \u30ef\u30b5 \u30b5 \u30b5\u30ab \u30ab \u30ab \u30c4 \u30c4 \u30c4\u30de \u30de \u30de\u30aa \u30aa \u30aa, \u30a4 \u30a4 \u30a4\u30d6 \u30d6 \u30d6\u30ea \u30ea \u30ea\u30c4 \u30c4 \u30c4\u30e2 \u30e2 \u30e2\u30de \u30de \u30de\u30d6 \u30d6 \u30d6 \u30c1 \u30c1 \u30c1\u30a6 \u30a6 \u30a6\u30ad \u30ad \u30ad\u30bb \u30bb \u30bb\u30bf \u30bf \u30bf\u30f3 \u30f3 \u30f3, \u30a6 \u30a6 \u30a6\u30c0 \u30c0 \u30c0\u30e4 \u30e4 \u30e4\u30f3 \u30f3 \u30f3 \u30d7 \u30d7 \u30d7\u30e9 \u30e9 \u30e9\u30b5 \u30b5 \u30b5\u30c3 \u30c3 \u30c3\u30c9 \u30c9 \u30c9, \u30a6 \u30a6 \u30a6\u30e9 \u30e9 \u30e9\u30de \u30de \u30de\u30c4 \u30c4 \u30c4 \u30b5 \u30b5 \u30b5 \u30df \u30df \u30df\u30bf \u30bf \u30bf\u30ed \u30ed \u30ed\u30a6 \u30a6 \u30a6, \u30a8 \u30a8 \u30a8\u30a6 \u30a6 \u30a6\u30b0 \u30b0 \u30b0\u30e9 \u30e9 \u30e9\u30f3 \u30f3 \u30f3\u30c7 \u30c7 \u30c7\u30a3 \u30a3 \u30a3\u30ca \u30ca \u30ca \u30ed \u30ed \u30ed\u30bb \u30bb \u30bb\u30a2 \u30a2 \u30a2, \u30a8 \u30a8 \u30a8\u30b9 \u30b9 \u30b9\u30c8 \u30c8 \u30c8\u30e9 \u30e9 \u30e9\u30e0 \u30e0 \u30e0\u30b9 \u30b9 \u30b9\u30c1 \u30c1 \u30c1\u30f3 \u30f3 \u30f3 \u30a8 \u30a8 \u30a8\u30b9 \u30b9 \u30b9\u30c8 \u30c8 \u30c8\u30e9 \u30e9 \u30e9\u30b5 \u30b5 \u30b5\u30a4 \u30a4 \u30a4\u30c8 \u30c8 \u30c8, \u30aa \u30aa \u30aa\u30af \u30af \u30af\u30bf \u30bf \u30bf\u30af \u30af \u30af\u30ed \u30ed \u30ed\u30eb \u30eb \u30eb\u30c6 \u30c6 \u30c6\u30c8 \u30c8 \u30c8\u30e9 \u30e9 \u30e9\u30d2 \u30d2 \u30d2\u30c9 \u30c9 \u30c9\u30ed \u30ed \u30ed \u30e1 \u30e1 \u30e1\u30bf \u30bf \u30bf\u30ce \u30ce \u30ce \u30d5 \u30d5 \u30d5\u30bf \u30bf \u30bf\u30e9 \u30e9 \u30e9\u30f3 \u30f3 \u30f3, \u30aa \u30aa \u30aa\u30c9 \u30c9 \u30c9\u30cd \u30cd \u30cd \u30bb \u30bb \u30bb\u30f3 \u30f3 \u30f3\u30c7 \u30c7 \u30c7\u30ed \u30ed \u30ed\u30eb \u30eb \u30eb, \u30aa \u30aa \u30aa\u30e9 \u30e9 \u30e9\u30f3 \u30f3 \u30f3\u30d0 \u30d0 \u30d0\u30e4 \u30e4 \u30e4\u30eb \u30eb \u30eb \u30d3 \u30d3 \u30d3\u30e3 \u30e3 \u30e3\u30f3 \u30f3 \u30f3\u30d0 \u30d0 \u30d0\u30b8 \u30b8 \u30b8\u30e3 \u30e3 \u30e3\u30d6 \u30d6 \u30d6, \u30af \u30af \u30af\u30c4 \u30c4 \u30c4\u30df \u30df \u30df \u30bd \u30bd \u30bd\u30af \u30af \u30af\u30c1 \u30c1 \u30c1\u30e5 \u30e5 \u30e5\u30a6 \u30a6 \u30a6 t: \uf98e \u6708, \u308b \u5c45\u308b, \u6708 \u65e5, \u308b \u4e8b, \u5176\u306e \u5f8c, \u6210\u308b \u5c45\u308b, \u662d\u548c \uf98e, \u4e8b \u51fa\u308b, \uf98e \u662d\u548c, \u65bc\u304f \u308a, \uf98e \uf98e, \u6210\u308b, \u4e8b \u6709\u308b, \u4e8b \u6210 \u308b, \u4f7f\u7528 \u308b, \u7269 \u6709\u308b, \u5b58\u5728 \u308b, \u5e73\u6210 \uf98e, \u7b2c \u56de, \u308b \uf98e frequency: \u308b \u5c45\u308b, \uf98e \u6708, \u6708 \u65e5, \u308b \u4e8b, \u308b \uf98e, \uf98e \uf98e, \u6210\u308b \u5c45\u308b, \u5c45\u308b \uf98e, \u5176\u306e \u5f8c, \u4e8b \u6709\u308b, \u662d\u548c \uf98e, \u308b , \u308b \u5176\u306e, \u4e8b \u6210\u308b, \u4e8b \u51fa\u308b, \uf98e \u662d\u548c, \u6709\u308b \uf98e, \u6210\u308b, \u4f7f\u7528 \u308b, \u65bc\u304f \u308a \u03c7 2 : \u1100 \u1161 \u1100 \u1161 \u1100 \u1161\u1102 \u1175 \u11ba \u1102 \u1175 \u11ba \u1102 \u1175 \u11ba \u110b \u1161 \u11af \u110b \u1161 \u11af \u110b \u1161 \u11af\u1112 \u116f \u11ab \u1112 \u116f \u11ab \u1112 \u116f \u11ab\u1109 \u1169 \u1109 \u1169 \u1109 \u1169, \u1100 \u1161 \u1100 \u1161 \u1100 \u1161\u110b \u116c \u11ba \u110b \u116c \u11ba \u110b \u116c \u11ba\u110b \u1175 \u11af \u110b \u1175 \u11af \u110b \u1175 \u11af \u1107 \u1169 \u11ba \u1107 \u1169 \u11ba \u1107 \u1169 \u11ba\u110b \u1175 \u11af \u110b \u1175 \u11af \u110b \u1175 \u11af, \u1100 \u1161 \u1100 \u1161 \u1100 \u1161\u110e \u1173 \u110e \u1173 \u110e \u1173\u1110 \u1166 \u1110 \u1166 \u1110 \u1166\u1105 \u116e \u1105 \u116e \u1105 \u116e \u110b \u116e \u110b \u116e \u110b \u116e\u1105 \u116e \u1105 \u116e \u1105 \u116e\u1109 \u1163 \u1109 \u1163 \u1109 \u1163, \u1100 \u1161 \u1100 \u1161 \u1100 \u1161\u1110 \u1169 \u11af \u1110 \u1169 \u11af \u1110 \u1169 \u11af\u1105 \u1175 \u1105 \u1175 \u1105 \u1175\u110f \u1169 \u11ab \u110f \u1169 \u11ab \u110f \u1169 \u11ab \u110b \u1162 \u11b7 \u110b \u1162 \u11b7 \u110b \u1162 \u11b7\u1107 \u1172 \u11af \u1107 \u1172 \u11af \u1107 \u1172 \u11af, \u1100 \u1161 \u11af \u1100 \u1161 \u11af \u1100 \u1161 \u11af\u1101 \u1172 \u1101 \u1172 \u1101 \u1172 \u1100 \u1161 \u1100 \u1161 \u1100 \u1161\u1109 \u1175 \u11af \u1109 \u1175 \u11af \u1109 \u1175 \u11af\u1101 \u1172 \u1101 \u1172 \u1101 \u1172, \u1100 \u1161 \u11af \u1100 \u1161 \u11af \u1100 \u1161 \u11af\u1105 \u1161 \u1105 \u1161 \u1105 \u1161\u1105 \u1161 \u11b7 \u1105 \u1161 \u11b7 \u1105 \u1161 \u11b7 \u110b \u1161 \u11af \u110b \u1161 \u11af \u110b \u1161 \u11af\u1107 \u116e \u1107 \u116e \u1107 \u116e\u1103 \u1161 \u11b7 \u1103 \u1161 \u11b7 \u1103 \u1161 \u11b7, \u1100 \u1161 \u11b7 \u1100 \u1161 \u11b7 \u1100 \u1161 \u11b7\u1106 \u1175 \u11ab \u1106 \u1175 \u11ab \u1106 \u1175 \u11ab \u110b \u116f \u11af \u110b \u116f \u11af \u110b \u116f \u11af\u1106 \u1175 \u11ab \u1106 \u1175 \u11ab \u1106 \u1175 \u11ab, \u1100 \u1161 \u11b7 \u1100 \u1161 \u11b7 \u1100 \u1161 \u11b7\u1109 \u1165 \u11bc \u1109 \u1165 \u11bc \u1109 \u1165 \u11bc \u110e \u1162 \u110e \u1162 \u110e \u1162 \u1102 \u1165 \u11af \u1102 \u1165 \u11af \u1102 \u1165 \u11af@21, \u1100 \u1161 \u11b8 \u1100 \u1161 \u11b8 \u1100 \u1161 \u11b8\u1107 \u1169 \u11a8 \u1107 \u1169 \u11a8 \u1107 \u1169 \u11a8 \u1100 \u1161 \u11b8 \u1100 \u1161 \u11b8 \u1100 \u1161 \u11b8\u1100 \u1172 \u1100 \u1172 \u1100 \u1172, \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u110e \u1166 \u11ab \u110e \u1166 \u11ab \u110e \u1166 \u11ab \u110f \u1175 \u110f \u1175 \u110f \u1175\u1109 \u116d \u11bc \u1109 \u116d \u11bc \u1109 \u116d \u11bc, \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u110e \u1171 \u110e \u1171 \u110e \u1171\u110b \u116a \u11ab \u110b \u116a \u11ab \u110b \u116a \u11ab \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u110e \u1171 \u110e \u1171 \u110e \u1171\u110b \u1175 \u11af \u110b \u1175 \u11af \u110b \u1175 \u11af, \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u1112 \u1169 \u11bc \u1112 \u1169 \u11bc \u1112 \u1169 \u11bc\u110b \u1165 \u11b8 \u110b \u1165 \u11b8 \u110b \u1165 \u11b8 \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u1112 \u116d \u1112 \u116d \u1112 \u116d\u110b \u1165 \u11b8 \u110b \u1165 \u11b8 \u110b \u1165 \u11b8, \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u1112 \u1173 \u11bc \u1112 \u1173 \u11bc \u1112 \u1173 \u11bc\u1109 \u1165 \u11ab \u1109 \u1165 \u11ab \u1109 \u1165 \u11ab \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc \u1100 \u1161 \u11bc\u1112 \u1173 \u11bc \u1112 \u1173 \u11bc \u1112 \u1173 \u11bc\u110b \u1175 \u11a8 \u110b \u1175 \u11a8 \u110b \u1175 \u11a8, \u1100 \u1162 \u1100 \u1162 \u1100 \u1162\u110b \u1167 \u11bc \u110b \u1167 \u11bc \u110b \u1167 \u11bc \u1100 \u1170 \u1100 \u1170 \u1100 \u1170\u110b \u1167 \u11bc \u110b \u1167 \u11bc \u110b \u1167 \u11bc, \u1100 \u1162 \u1100 \u1162 \u1100 \u1162\u110e \u1169 \u110e \u1169 \u110e \u1169\u1112 \u1161 \u11bc \u1112 \u1161 \u11bc \u1112 \u1161 \u11bc \u1100 \u1165 \u1100 \u1165 \u1100 \u1165\u1105 \u1172 \u11ab \u1105 \u1172 \u11ab \u1105 \u1172 \u11ab\u1112 \u1161 \u11bc \u1112 \u1161 \u11bc \u1112 \u1161 \u11bc, \u1100 \u1162 \u1100 \u1162 \u1100 \u1162 \u1110 \u1171 \u1110 \u1171 \u1110 \u1171\u110b \u1174 \u110b \u1174 \u110b \u1174\u110b \u1163 \u11af \u110b \u1163 \u11af \u110b \u1163 \u11af \u1104 \u1169 \u11bc \u1104 \u1169 \u11bc \u1104 \u1169 \u11bc\u1111 \u1165 \u1111 \u1165 \u1111 \u1165\u1106 \u1165 \u11a8 \u1106 \u1165 \u11a8 \u1106 \u1165 \u11a8\u1102 \u1173 \u11ab \u1102 \u1173 \u11ab \u1102 \u1173 \u11ab, \u1100 \u1162 \u11a8 \u1100 \u1162 \u11a8 \u1100 \u1162 \u11a8\u1105 \u1167 \u11af \u1105 \u1167 \u11af \u1105 \u1167 \u11af\u110b \u1162 \u11a8 \u110b \u1162 \u11a8 \u110b \u1162 \u11a8 \u1100 \u1165 \u11b8 \u1100 \u1165 \u11b8 \u1100 \u1165 \u11b8\u1105 \u1167 \u11af \u1105 \u1167 \u11af \u1105 \u1167 \u11af\u110b \u1162 \u11a8 \u110b \u1162 \u11a8 \u110b \u1162 \u11a8, \u1100 \u1162 \u11af \u1100 \u1162 \u11af \u1100 \u1162 \u11af\u1105 \u1165 \u1105 \u1165 \u1105 \u1165 \u1105 \u1175 \u1105 \u1175 \u1105 \u1175@KCUA, \u1100 \u1162 \u11af\u1105 \u1165 \u11ab\u110b \u1166\u1109 \u1165 \u1100 \u1162 \u11af\u1105 \u1165 \u11ab\u110b \u1173\u1105 \u1169, \u1100 \u1165 \u1100 \u1165 \u1100 \u1165\u1103 \u1162 \u1103 \u1162 \u1103 \u1162\u110b \u1172 \u110b \u1172 \u110b \u1172\u1107 \u1161 \u11bc \u1107 \u1161 \u11bc \u1107 \u1161 \u11bc\u110c \u1173 \u11bc \u110c \u1173 \u11bc \u110c \u1173 \u11bc \u1103 \u1162 \u1103 \u1162 \u1103 \u1162\u110b \u1172 \u110b \u1172 \u110b \u1172\u1107 \u1161 \u11bc \u1107 \u1161 \u11bc \u1107 \u1161 \u11bc t: \u110c \u1165 \u11a8 \u110b \u1175 \u11ab, \u1112 \u1161\u1103 \u1161 \u1109 \u116e, \u1112 \u1161 \u11ab \u1103 \u1161, \u110b \u1171 \u1112 \u1161 \u11ab, \u1106 \u1161 \u11af \u1112 \u1161\u1103 \u1161, \u1109 \u1175\u110c \u1161 \u11a8 \u1112 \u1161\u1103 \u1161, \u1109 \u1161\u110b \u116d \u11bc \u1112 \u1161\u1103 \u1161, \u1106 \u1169 \u11ba \u1112 \u1161\u1103 \u1161, \u1109 \u116e \u110b \u1165 \u11b9\u1103 \u1161, \u110b \u1171\u110e \u1175 \u1112 \u1161 \u11ab, \u1112 \u1161\u1103 \u1161 \u110b \u1161 \u11ad\u1103 \u1161, \u1109 \u1161\u110b \u116d \u11bc \u1103 \u116c\u1103 \u1161, \u1112 \u1161 \u1103 \u1161 \u110b \u1171\u1112 \u1162, \u1100 \u1161\u110c \u1175 \u1100 \u1169, \u1100 \u1175\u1103 \u1169 \u1112 \u1161\u1103 \u1161, \u110b \u1175 \u11af\u1107 \u1161 \u11ab \u110c \u1165 \u11a8, \u1103 \u116c\u1103 \u1161 \u110b \u1161 \u11ad\u1103 \u1161, \u110c \u1169 \u11ab\u110c \u1162 \u1112 \u1161\u1103 \u1161, \u1100 \u1175\u1105 \u1169 \u11a8 \u1112 \u1161\u1103 \u1161, \u110b \u1173 \u11ab \u1103 \u1162\u1112 \u1161 \u11ab\u1106 \u1175 \u11ab\u1100 \u116e \u11a8 frequency: \u110c \u1165 \u11a8 \u110b \u1175 \u11ab, \u1112 \u1161\u1103 \u1161 \u1109 \u116e, \u1112 \u1161\u1103 \u1161 \u1112 \u1161\u1103 \u1161, \u1112 \u1161 \u11ab \u1103 \u1161, \u1109 \u1161\u110b \u116d \u11bc \u1112 \u1161\u1103 \u1161, \u1106 \u1161 \u11af \u1112 \u1161\u1103 \u1161, \u1109 \u1175\u110c \u1161 \u11a8 \u1112 \u1161\u1103 \u1161, \u1112 \u1161\u1103 \u1161 \u110b \u1161 \u11ad\u1103 \u1161, \u110b \u1171 \u1112 \u1161 \u11ab, \u1106 \u1169 \u11ba \u1112 \u1161\u1103 \u1161, \u1109 \u116e \u110b \u1165 \u11b9\u1103 \u1161, \u110b \u1171 \u110e \u1175 \u1112 \u1161 \u11ab, \u1112 \u1161\u1103 \u1161 \u110b \u1171\u1112 \u1162, \u1112 \u1161\u1103 \u1161 \u1102 \u1173 \u11ab, \u1109 \u1161\u110b \u116d \u11bc \u1103 \u116c\u1103 \u1161, \u1100 \u1175\u1105 \u1169 \u11a8 \u1112 \u1161\u1103 \u1161, \u1103 \u116c\u1103 \u1161 \u110b \u1161 \u11ad\u1103 \u1161, \u1112 \u1161\u1103 \u1161 \u1103 \u116c\u1103 \u1161, \u1100 \u1175\u1103 \u1169 \u1112 \u1161\u1103 \u1161, \u1112 \u116a \u11af\u1103 \u1169 \u11bc \u1112 \u1161\u1103 \u1161 Another possible effect these results may show is that the writing system or the morphology could account for this notable discrepancy in retokenization percentage across languages. For English, the top 20 \u03c7 2 collocations are primarily specific named entities, but the t and frequency-based retokenizers yield more general compound nouns and common phrases (Figure 1 ). As the top 50,000 \u03c7 2 collocations contain primarily rare words, these are expected to co-occur rarely enough that even a few co-occurrences can trigger significance. Therefore, when we use this truncated list of rarelyoccurring \u03c7 2 collocations, we generally see a very low merged token percentage. The quality of retokenization impacts both the goodness of fit the model, as indicated by the normalized log-likelihood score, and the coherence of the model, as indicated by the CBES score. Within the same language, news corpora have higher percentages of merged words when merged with t and frequency collocations, while corpora containing restaurant and movie reviews tend to see lower percentages (Table 1 ). This could be because the news corpora are in a similar domain to that of the Wikipedia which we use to build the list of co-occurring words. A good retokenizer (in our cases, trained on Wikipedia data) should generalize well and recognize many collocations in a new corpus, which differs somewhat from the retokenizer training data. We found a significant positive correlation between merge percentage and the margin of improvement over the baseline (the difference between the PTLL of the model without retokenization and the PTLL or CBES of the model  Figure 4 : CBES improvement vs. merged percentage. with retokenization). Pooling across all languages and corpora, we found the correlation coefficients of 0.41, 0.77, and 0.68 for the models with 10, 50, and 100 topics respectively for PTLL. As for the coherence metric, we found the correlation coefficients of 0.73, 0.76, and 0.79 for the models with 10, 50, and 100 topics respectively for CBES. This means the models with higher merge percentages are better than their corresponding word models in reproducing the statistics of the held-out data. This suggests that the quality of the LDA models depends on the generalizability of the retokenizers. The LDA model results become more understandable when certain tokens are retokenized. We see merged tokens in the topic key sets of almost all topics in all corpora when retokenized based on t or raw frequency. Many of these represent non-compositional meanings that might have been lost without retokenization: for example, the collocation \"social security\" is not fully represented by the individual tokens \"social\" or \"security\" separately. More strikingly, the collocation 'k\u014dn s\u01d4a d\u0101ng' refers to a political movement group in Thailand. When it is separated into k\u014dn (people) s\u01d4a (shirt) d\u0101ng (red), the key meaning is totally lost. When we compare by looking at the topic-keys of the word and multi-word models, we can come up with similar topics because we as a human who understands English and has general knowledge of the world can make the connection based on surrounding topic-keys even though they are not explicitly merged. However, if we want to use these topic keys as input to other downstream tasks such as information retrieval or text classification, the merged tokens help retain the specificity of the \"red shirt people\" as a meaningful entity distinct from the phrase's constituting parts. Conclusion In this work, we improve the quality of LDA models by better processing the input text before training the model. We found that the retokenizers trained based on t statistics and raw frequency yield an improvement across all languages considered in this study, while the \u03c7 2 approach was a less efficient approach that focuses more on rare named entities than common noun phrases. Using retokenizers ensures that LDA models can fit better to the data, the topic keys are more coherent, and the topics are more distinct. Outputs from retokenization with t statistics and frequency approaches yield common noun phrases in the most frequent terms of topics that represent a significant aid to both direct topic interpretation and expected utility of these topics in downstream tasks. Acknowledgments This project is partially supported by Grants for Development of New Faculty Staff, Ratchadaphiseksomphot Endowment Fund. The authors would like to thank Vincent Ng, who provided us with very insightful comments as a Student Research Workshop mentor. We are also grateful for the suggestions from the anonymous reviewers from the previous submission.",
    "abstract": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.",
    "countries": [
        "Thailand"
    ],
    "languages": [
        "Chinese",
        "English",
        "Thai",
        "Japanese",
        "Korean",
        "German"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "More Than Words: Collocation Retokenization for {L}atent {D}irichlet {A}llocation Models"
}