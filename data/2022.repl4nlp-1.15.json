{
    "article": "We propose the Video Language Co-Attention Network (VLCN) -a novel memory-enhanced model for Video Question Answering (VideoQA). Our model combines two original contributions: A multimodal fast-learning feature fusion (FLF) block and a mechanism that uses selfattended language features to separately guide neural attention on both static and dynamic visual features extracted from individual video frames and short video clips. When trained from scratch, VLCN achieves competitive results with the state of the art on both MSVD-QA and MSRVTT-QA with 38.06% and 36.01% test accuracies, respectively. Through an ablation study, we further show that FLF improves generalization across different VideoQA datasets and performance for question types that are notoriously challenging in current datasets, such as long questions that require deeper reasoning as well as questions with rare answers 1 . Introduction Video Question Answering (VideoQA) has emerged as a challenging task at the intersection of natural language processing and computer vision. In contrast to image-based visual question answering (Lu et al., 2016; Anderson et al., 2018; Yu et al., 2019) , VideoQA takes dynamic visual content (a video) as input (Xu et al., 2017; Gao et al., 2018; Li et al., 2019) . This poses new challenges given that generating correct answers requires models to analyze spatial, appearance-based features of individual video frames jointly with the temporal, motion-based dynamics across multiple frames (Zhu et al., 2017) . However, there still is a semantic gap between the visual and language channels (Lei et al., 2018; Sun et al., 2021; Song et al., 2018) that prior work has * *Equal contribution. 1 Our code is publicly available at the project website https://www.perceptualui.org/publications/ abdessaied22_repl4NLP/ tried to close by leveraging external memory (Kim et al., 2018 (Kim et al., , 2019;; Fan et al., 2019) . While external memory allows models to cache sequential information and retrieve relevant multimodal content (Patel et al., 2021) , latest models still suffer from decreased performance, for example on ambiguous questions that require deeper reasoning abilities. Moreover, current deep neural models for VideoQA are limited in that they only gradually learn during training. In contrast, human cognition leverages two different learning systems: a gradual and a fast-learning system (McClelland et al., 1995) .The interplay between the fast and gradual duel learning systems is essential for humans to learn new representations, hence to generalizing (McClelland et al., 2020b) . Current networks lack a similar fast-learning mechanism, which impedes their ability to efficiently reason and generalize to unseen data since the fast learning system acts as an encoder of new information which is then transferred to the gradual learning system for referencing and consolidating (Arani et al., 2021) . To address these limitations, we propose the Video Language Co-Attention Network (VLCN) -a novel memory-enhanced model for VideoQA. VLCN implements a video language co-attention module that uses self-and guided-attention to align language features of the question with static and dynamic visual features extracted from videos. As such, the module offers complementary information that our network attends to, independently of each other, when visually grounding a question. Furthermore, VLCN features a novel multimodal fast-learning fusion (FLF) block that helps the model to deal with challenging questions that need deeper reasoning and understanding. Inspired by the cognitive fast-learning system (McClelland et al., 2019) , we leverage the differentiable neural computer (DNC) to incorporate an external memory which the network learns how to use by freeing and reusing its memory slots. We seamlessly integrate our novel video language co-attention module as well as the fast-learning feature fusion approach in the recent transformerbased MCAN network (Yu et al., 2019) . We show that our model achieves competitive results with the state of the art on two challenging datasets -MSVD-QA and MSRVTT-QA. Our results further show that our model performs better on ambiguous questions and can better reason not only about questions with rare answers but also longer questions that require a deeper understanding of both the question and the visual input. In addition, we show that FLF facilitates generalization across different VideoQA datasets via transfer learning. Related Work Our work is related to previous works on 1) attention mechanisms in VideoQA, and 2) memoryenhanced networks. Attention Mechanisms in VideoQA. Neural attention mechanisms have become the de-facto standard in machine comprehension tasks (Sood et al., 2020; Yu et al., 2019; Li et al., 2019) . In VideoQA, attention mechanisms are particularly important given that the information necessary to generate correct answers is scattered across frames -many of which are redundant or even irrelevant to the question at hand (Patel et al., 2021) . Ye et al. ( 2017 ) introduced the attributeaugmented attention network that learned temporally attended video representations according to semantic attributes. Xu et al. (2017) reported new state-of-the-art performance by applying questionguided attention over both the appearance and motion features of individual as well as multiple video frames. Motivated by the challenge to capture long-range dependencies, Li et al. (2019) used a transformer-based co-attention network to exploit the global dependencies of the text and the temporal dynamics of the videos. Yang et al. (2020) leveraged BERT (Devlin et al., 2018) to obtain richer contextual feature representations over the question. More recently, Seo et al. (2021) proposed a two-stream multimodal video transformer based architecture (CoMVT) that jointly attends over words in text and visual objects and scenes to learn visualdialogue context. Although CoMVT achieves stateof-the-art results on multiple downstream VideoQA datasets, it requires a computationally-demanding pretraning stage on 1.2M instructional videos. These previous methods have used question features to guide attention over either frame or cliplevel visual features, and some applied self and co-attention to individual frames. Our work, however, is the first to use self-attention on the question which then separately guides the attention over both individual video frames and clips. Memory-enhanced Networks. In parallel, other works have focused on augmenting models with external memory components to improve their reasoning capabilities particularly over long-range data that are common in many visiolinguistic tasks, e.g. images with many objects or videos with a large number of frames. One of the first methods introduced a memory component over simple facts for question answering (Weston and Bordes, 2015) . The introduction of end-to-end trainable models popularized the use of external memory components (Sukhbaatar et al., 2015) . Driven by the insight that memory access is similar to neural attention (Collier and Beel, 2019) , other works integrated attention mechanisms to allow networks to better interact with their external memory through read and write operations, such as the Neural Turing Machine (NTM) (Graves et al., 2014) or the Differential Neural Computer (DNC) (Graves et al., 2016) . The latter includes a dynamic memory allocation scheme that enables it to learn how to effectively free and reuse memory slots. Several works aimed to leverage the potential of memory-enhanced networks for VideoQA. Na et al. (2017) applied memory over the video frames using multi-layered CNNs read and write networks to capture richer temporal dynamics of frame-level sequence information. Xue et al. ( 2018 ) obtained syntax parse trees over questions and then stored these into memory, allowing their model to perform better on more complex questions. Fan et al. (2019) used one memory component to effectively learn global context information from appearance and motion features in combination with another question-memory to help understand the complex semantics of questions and highlight queried subjects. Gao et al. (2018) used a co-memory attention mechanism to generate attention from motion and appearance cues. More recently, Yin et al. (2020) achieved new state-of-the-art results on MSVD-QA (Xu et al., 2017) by using a DNC (Graves et al., 2016) to encode the textual information of the question and the visual information of the video. While previous works used memory-enhanced networks to extract linguistic and visual features, we propose a memory-augmented block adapted from the DNC to potentially emulate the human-like fast-learning capabilites (McClelland et al., 2020a) and use it to fuse multimodal features previously attended by an encoder-decoder transformer-based co-attention module instead. Method We propose the Video Language Co-Attention Network (VLCN) that integrates two original contributions (see Figure 1 ): First, we propose to use self-and guided-attention to separately align the language features with the static and dynamic visual features extracted from single video frames and frame sequences (clips). Second, we introduce Fast-Learning Fusion (FLF) -a novel memory-enhanced multimodal block that learns a single fused repre- sentation of all features (i.e. language, static and dynamic visual features). Feature Representation In contrast to images, videos consist of multiple frames that capture temporal object dynamics and motion features. Combinations of static and dynamic visual features have therefore become the de-facto standard for video representations (Xu et al., 2017; Le et al., 2020a) in VideoQA. We adopt the same approach in our Video Language Co-Attention Network (VLCN). Visual Features. For each video, we first sample n v evenly-distributed frames and clips where a clip is a sequence of 16 consecutive video frames. Then, we apply a VGG network (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Russakovsky et al., 2015) and a C3D network 2 (Ji et al., 2012) pre-trained on Sports1M (Karpathy et al., 2014) on these sampled frames and clips, respectively. The activations of their last d v -dimensional fullyconnected layers are our static and dynamic visual features. This results in a set of static frame features F = [f 1 , . . . , f nv ] \u2208 R nv\u00d7dv and a set of dynamic clip features C = [c 1 , . . . , c nv ] \u2208 R nv\u00d7dv . Language Features. Question tokens are represented using 300-D GloVe embeddings (Pennington (Hochreiter and Schmidhuber, 1997) with d l hidden dimensions. Thus, each question is represented as a matrix L \u2208 R n l \u00d7d l , where n l is the number of question tokens. Video Language Co-Attention The intuition behind our overall approach is the way humans typically answer questions about videos: first, we read the question. Then, we consider the visual input, i.e. its static (colours, objects and shapes) and dynamic (movements and actions) visual features, to answer it. VLCN uses stacked video language co-attention layers in an encoderdecoder fashion (see Figure 1 ). Given a set of features, each layer simultaneously computes the self-attention of the question, frames and clips features. Then, the self-attended question features of the last layer, i.e. L (K) , are used to separately guide the attention over the frames and clip features in a bottom up manner (Anderson et al., 2018) . At the core of self-and guided-attention sits a multihead attention block (Vaswani et al., 2017) that computes a scaled dot-product of a query q \u2208 R 1\u00d7d and a set of n keys K \u2208 R n\u00d7d , where d is a common hidden dimension. A softmax function is then applied to obtain the attention weights A on the values V \u2208 R n\u00d7d following: A = softmax( qK T \u221a d )V. ( 1 ) Similar to (Vaswani et al., 2017) , attention weights A are computed for multiple queries Q \u2208 Q n\u00d7d at the same time using Equation (1). The outputs of the final video language co-attention layers L (K) \u2208 R n l \u00d7d , F (K) \u2208 R nv\u00d7d and C (K) \u2208 R nv\u00d7d encode information about the attention weights over the question tokens and visual semantics. We reduce them to get the final attended features l, f, v \u2208 R d by linearly combining the rows of L (K) , F (K) and C (K) , respectively (see Figure 1 ). Taking the language features as an example, we first process L (K) by a multi-layer Feed-Forward Network (FFN) followed by a softmax to obtain the attention weights that we use to linearly combine the rows of L (K) as: a = softmax(FFN(L (K) )) \u2208 [0, 1] n l , (2) l = n l i=1 a i L (K) [i, :] \u2208 R d . (3) Fast-Learning Feature Fusion We opted to use the DNC as a basis for this approach given that it is, to our knowledge, the most capable memory-augmented model to date that can be trained in an end-to-end fashion (Graves et al., 2016) . In previous works (Graves et al., 2016; Yin et al., 2020) , the DNC was heavily used to process long input sequences. However, its capability to treat shorter input sequences remains unexplored even though it has been argued for by cognitive science to be capable of emulating the human fast-learning system proposed in the complementary learning systems theory (McClelland et al., 1986 (McClelland et al., , 2019)) . In our work, we leverage it -for the first time -to fuse our three multi-modal inputs (i.e. language, static, and dynamic visual features) within the VideoQA task. Differential Neural Computer (DNC). The DNC consists of two major components: A neural network controller and an external memory. At each time-step t, the controller receives an input vector x t and emits an output vector y t . In addition, it receives a set of R read vectors {r i t\u22121 } R i=1 from the N \u00d7 W memory matrix M t\u22121 of the previous time-step t \u2212 1. Both controller inputs and the read vectors are concatenated to form the final input vector \u03c7 t = [x t ; r 1 t\u22121 ; . . . ; r R t\u22121 ]. Theoretically, the controller can be a network of any type. However, it is common to use an LSTM network with L hidden layers. The output vector y t is computed via y t = W h [h 1 t ; . . . ; h L t ] + W r [r 1 t ; . . . ; r R t ], (4) where W h and W r are learnable weights and h t = {h i t } L i=1 are the hidden states of the LSTM controller. These hidden states are used to parameterize one write and R read heads to interact with the N \u00d7 W external memory matrix through the so-called fast-learning connections. Further details on the DNC can be found in (Graves et al., 2016) . Feature Fusion. First, the reduced language and visual features l, f and c (see Figure 1 ) are projected and summed to get the first intermediate output z 1 . Then, they are duplicated and stacked one after another to form the input sequence X = [l, f, c] = [x 1 , x 2 , x 3 ] \u2208 R 3\u00d7d to the DNC. The output sequence Y = [y 1 , y 2 , y 3 ] \u2208 R 3\u00d7d is summed along the first dimension to obtain the final output o and the last R read vectors are concatenated to form the global read vector r. Finally, the second intermediate output z 2 is obtained by projecting [o; r] onto the same space as z 1 and the final output z is computed by summing z 1 and z 2 . We concatenated the last read vectors to the DNC's output to preserve the memory information from the last step of processing the input sequences. Answer Prediction. Given that VideoQA is formulated as a classification task, the fused features z are projected onto the answer space using a fullyconnected layer. A sigmoid function is applied to train the network with binary cross-entropy (BCE) loss (see Figure 1 ). The training, validation, and test sets account for 65%, 5%, and 30% of the total number of videos, respectively. Further details on the datasets can be found in Appendix A.1. Experiments Implementation Details. For each video, we sampled n v = 20 frames and clips and used them to generate the static and dynamic visual features. We set the dimensionality of the input question features d l and input visual features d v (static and dynamic) to 512 and 4, 096, respectively. The fused features z 1 , z 2 and z had a dimension d z = 1, 024. Following (Vaswani et al., 2017) , we set the latent dimension d of the multi-head attention block to 512 and the number of heads to eight, i.e. each had a dimensionality of 64. Since VideoQA is formulated as a classification task, similar to (Xu et al., 2017) , we used the most frequent 1, 000 groundtruth answers of the training and validation splits as our answer candidates. The number of video language co-attention layers K was fixed to six. Finally, for the DNC 3 in the FLF block we used a two-layer bidirectional LSTM network (Hochreiter and Schmidhuber, 1997) with 512 hidden dimensions as a controller as well as four read and one write heads to interact with the 512 \u00d7 64 external memory matrix. We used Adam (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 to optimize the weights of our model over a maximum of 30 epochs. We set the base learning rate to 10 \u22124 . The batch-size was fixed to 64 and 32 during training and evaluation, respectively. We implemented our model in PyTorch (Paszke et al., 2019) . It is based on a Visual Question Answering (VQA) opensource implementation 4 and will be made publicly available together with our pre-trained models. All experiments were conducted on one Nvidia Tesla V100 GPU with 32GB VRAM. Ablated Models. In all experiments that follow we denote with VLCN our full model that uses a DNC inside the FLF block and whose architecture is illustrated in Figure 1 . Although we experimented with training the DNC with different permutations of its inputs, we did not obtain any improvements in terms of performance when we changed the order of the input features. Therefore, we kept the same order that we used to encode the features (i.e. [l, f, c]). We additionally implemented different ablated versions of our model to study the impact of the proposed video language co-attention and fast-learning fusion: \u2022 MCAN : This is the original MCAN model as proposed in (Yu et al., 2019) but adapted for VideoQA. We trained it using the concatenated static and dynamic visual features as they share the same dimensionality d v . This model was not equipped with our novel video-language co-attention and fast-learning feature fusion. \u2022 VLCN\u2212FLF : For this model we used a simple multimodal fusion by summing the reduced features l, f and c, i.e. only the first intermediate output z 1 was passed through to the subsequent parts of the network (see Figure 1 ). \u2022 VLCN+LSTM : For this model we only used the controller of the DNC, i.e. a two-layer bidirectional LSTM with 512 hidden dimensions, to compute the second intermediate output z 2 by summing the outputs of the LSTM and projecting them onto the same space as z 1 . This model did not have the external long-term memory matrix and the fastlearning connections. Model Training. We evaluated the robustness of our model and its ablated versions by training each five times with five different seeds. We report the performance as \u00b5\u00b1\u03c3, where \u00b5 and \u03c3 are the average and standard deviation of the ensemble-accuracy on MSVD-QA and MSRVTT-QA test. Question Length and Answer Frequency. Complementing analyses according to common question type categories (what, who, how, when and where), we propose two other question-binning strategies: In the first strategy, questions are put into three bins based on question length. The first bin contains questions with up to three words, the second bin between four and eight, and the last bin with more than nine words. The longer the question, the harder it should be for the model to answer as it requires deeper reasoning and understanding. In the second strategy, questions are binned according to the frequency rank of their ground-truth answers in the training and validation splits. The first bin contains questions whose ground-truths are the 100 most frequent answers. The second contains questions whose ground-truths are the next 200 most frequent answers. The last bin contains the rest of the questions, i.e. questions with the scarcest 700 answers. The rarer answers to a question are, the more difficult it should be for the model to answer correctly. Transfer Learning of the FLF Weights. MSRVTT-QA includes more questions and longer videos compared to MSVD-QA: The average video lengths of MSRVTT-QA and MSVD-QA are 20 and 10 seconds, respectively (Aafaq et al., 2019) . Performance on MSRVTT-QA should thus benefit from the knowledge acquired while training on MSVD-QA (Pan and Yang, 2010). Through transferlearning of the fast-learning connections and the DNC controller weights learned from MSVD-QA, FLF should be able to better interact with its external memory when dealing with questions from MSRVTT-QA. To study this hypothesis, we conducted the following experiment: We trained an ensemble of five VLCNs using five different seeds on MSVD-QA. Then, we trained two further ensembles of five VLCNs on MSRVTT-QA using the same seeds: For one ensemble, we initialised the FLF weights of each model with those learned from MSVD-QA and fine-tuned them on MSRVTT-QA. We call these models VLCN+FT. For models in the second ensemble we trained these weights from scratch. Additionally, we experimented with finetuning the entire architecture of the FLF block instead. These experiments did not yield any performance improvements and we decided not to include them in this work. Results Comparison with the State of the Art. VLCN achieves competitive performance with the state of the art on both MSVD-QA and MSRVTT-QA. On MSVD-QA, our best model reaches an overall accuracy of 38.06% compared to 35.70%, 36.10%, and 36.20% achieved by CoMVT (scratch) (Seo et al., 2021) , HCRN (Le et al., 2020b), and MA-DRNN (Yin et al., 2020) , respectively. This corresponds to a relative improvement of 1.86% over the state of the art when the latter is trained from scratch (see Table 1 ). Although CoMVT can reach an overall accuracy of 42.60%, this was only possible after a computationally-demanding pretraining stage on HowToFUP (Miech et al., 2019 ) -a dataset consisting of 1.2M instructional videos for the task of Future Utterance Prediction (FUP). On the most diverse question types our model achieves a higher accuracy on what (\u223c 4% increase) and performs slightly worse on who compared to MA-DRNN. On the other types how, when and where, our model performs on par with the state of the art methods. As depicted in Table 2 , our best VLCN model achieves an overall accuracy of 36.01% on MSRVTT-QA -the second best performance after CoMVT which achieves 37.30% accuracy when trained from scratch and 39.50% after pretaining on HowToFUP. Ablation Study. Our analysis of the question length shows that VLCN achieves the best performance across all question length bins on MSVD-QA and on long questions, i.e. questions with length bigger than three, on MSRVTT-QA (see Tables 3  and 4 ). By comparing the first two rows of Table 3 and Table 4 , we can see that VLCN\u2212FLF outperforms MCAN across all of the question length bins of MSVD-QA and on very long questions (\u2265 9) of MSRVTT-QA. This suggests that our co-attention approach helps the model make reliable predictions when the question becomes more complex compared to the simple question-guided attention over the stacked visual features. We hypothesize that the static and dynamic visual features offer complementary information that our network needs to attend to, independently of each other, while trying to visually ground the question. By removing the external memory of the FLF block and using a plain LSTM network, VLCN+LSTM falls behind on all question length bins resulting in an overall accuracy decrease of 0.84% and 0.8% on MSVD-QA and MSRVTT-QA, respectively, compared to VLCN (see Tables 3  and 4 ). We hypothesize that the proposed external memory is indispensable when answering questions that exceed the working memory capacity of the model, i.e. in this case of the LSTM network. We then analyzed the performance of our ablated versions with respect to the answer frequency bins (see Table 5 ). On MSVD-QA, VLCN achieves the best results on the most challenging questions, i.e. questions whose answers are not amongst the 100 most frequent, and performs on par with VLCN\u2212FLF on questions with the 100 most frequent answers. Although VLCN+LSTM performs on par with VLCN and improves on the performance of MCAN and VLCN\u2212FLF on the most challenging questions, it falls behind VLCN when it comes to the easier questions with the most frequent answers. This results in an overall accuracy decrease of 0.5% compared to VLCN. Similarly, VLCN outperforms all of its ablated versions on the most challenging questions of MSRVTT-QA (see Table 6 ). In contrast to MSVD-QA, VLCN+LSTM does not reach superior results on the most challenging questions compared to MCAN and VLCN\u2212FLF. Performance on such questions only improves when using the external memory. In fact, VLCN achieves 20.97% and 5.84% on questions with the second 100 most frequent answers and questions with the scarcest 700 answers, respectively. This translates into a relative improvement of 2.49% and 3.89% compared to the second best models on such answer frequency bins, i.e. MCAN and VLCN+LSTM, respectively (see Table 6 ). It is interesting to see the difficulty of answering questions with rare ground truth answers as highlighted by the severe drop in performance for the last answer frequency bin of Table 5 and  Table 6 . We do not think that this is related to a language understanding problem as suggested by the error analysis we conducted on the ablated versions. Please refer to Appendix A.2 for more details. Transfer Learning. The last two rows of Tables 4 and 6 show the importance of curriculum learning (Bengio et al., 2009) . By fine-tuning the converged weights of FLF from MSVD-QA on MSRVTT-QA, VLCN+FT reaches new state of the art result on MSRVTT-QA by improving the accuracy on all question length and answer frequency bins compared to VLCN. This indicates that transfer learning of the fast-learning connections of FLF is possible and improves performance across different datasets. Further details about the effect of fine-tuning on the performance on individual question types can be found in Appendix A.3. Qualitative Analysis. Figure 2 shows sample attention maps learned by the last video language co-attention layer together with the predictions of our model and its ablated versions. These predictions are depicted in the orange box, where other denotes the ablated versions of our full VLCN model. Further examples can be found in Appendix A.4. The language self-attention SA(L) and the guidedattention over the clips G(C, L) show that VLCN Conclusion In this work, we proposed the Video Language Co-Attention Network (VLCN) for VideoQA. At its core are two distinct novel contributions: Stacked co-attention layers in an encoder-decoder framework to separately guide self-attended language features over both static video frame and dynamic clip features; and Fast-Learning Fusion (FLF) -a memory-enhanced multimodal block to efficiently fuse the reduced features. We demonstrated that the combination of both results in significant improvements and competitive performance with state-of-the-art models on the challenging MSVD-QA and MSRVTT-QA datasets. We also demonstrated the particular advantage of our model in dealing with long questions that require deeper reasoning or questions with rare answers. Finally, further experiments showed that our FLF block allows our model to generalize better across different datasets via transfer learning. DRNN (Yin et al., 2020) and QueST (Jiang et al., 2020) . A.2 Ablation Study Tables 9 and 10 show the ensemble performance of our VLCN model and its ablated versions with respect to individual question types. On MSVD-QA, our full model achieves the best accuracy on the most diverse question type what and performs on par with its ablated versions on the remaining question types, i.e. who, how, when, and where. Similar results can be observed on MSRVTT-QA: Our full VLCN model achieves the best accuracy on the most diverse question type what as well as question type when and performs on par with the rest of its ablated versions on the remaining question types who, how, and where. A.3 Transfer Learning By observing the last two rows of Table 10 , we can see the effect of transfer learning on the performance of our full VLCN model with respect to individual question types. In fact, by fine-tuning the fastlearning connections and the DNC weights inside the FLF block on MSRVTT-QA, we improved the performance on three different questions types, i.e. the most and second most diverse types what and who as well as question type when. This results in a new state-of-the-art overall accuracy of 36.01%. Acknowledgments A. Bulling was funded by the European Research Council (ERC; grant agreement 801708). E. Sood was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2075 -390740016. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. A.4 Qualitative Analysis We further show a qualitative example to highlight the supremacy of our full VLCN model over its ablated versions. In Figure 3 , we can see how both the language self-attention SA(L) and the guided-attention over the frames GA(L, F ) are both flat indicating that the model is having difficulties aligning the multi-modal features. However, the guided-attention over the clips GA(L, C) shows high attention values to the word who which is, in this case, the keyword to answer the question who sat in his chair? depicted in the orange box. While all of the ablated versions predict the wrong answer lady, our VLCN model answers the question correctly by predicting man.",
    "abstract": "We propose the Video Language Co-Attention Network (VLCN) -a novel memory-enhanced model for Video Question Answering (VideoQA). Our model combines two original contributions: A multimodal fast-learning feature fusion (FLF) block and a mechanism that uses selfattended language features to separately guide neural attention on both static and dynamic visual features extracted from individual video frames and short video clips. When trained from scratch, VLCN achieves competitive results with the state of the art on both MSVD-QA and MSRVTT-QA with 38.06% and 36.01% test accuracies, respectively. Through an ablation study, we further show that FLF improves generalization across different VideoQA datasets and performance for question types that are notoriously challenging in current datasets, such as long questions that require deeper reasoning as well as questions with rare answers 1 .",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Video Language Co-Attention with Multimodal Fast-Learning Feature Fusion for {V}ideo{QA}"
}