{
    "article": "Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match (EM ) and F 1 . However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of EM / F 1 on MRC tasks. Introduction A competency assessment is used to measure someone's capabilities against the requirements of their job (Cheryl Lasse, 2020) . In other words, it measures how (behaviors) someone does the what (task or skill). By showing what it looks like to be good in a job, a competency assessment can effectively empower and engage people who want to understand and improve their unique skill profile and tell them what action to take to close any gaps so they can own their development. A natural question that arises here is: can we develop competency assessments for machine learning models to help better understand their capabilities and improve their performance on a given task? In this paper, we focus on competency assessments for machine reading comprehension (MRC). MRC is a core task in natural language processing (NLP) that aims to teach machines to understand human languages and answer questions (Zeng et al., 2020; Chen et al., 2019) . Recently, pretrained language models (LMs) (Mikolov et al., 2013; Peters et al., 2018; Pennington et al., 2014; Devlin et al., 2018) have demonstrated superior performance on MRC tasks by pre-training on large amounts of unlabeled corpus and fine-tuning on MRC datasets. The performance is usually evaluated by metrics such as Exact Match (EM ) and F 1 score, lacking interpretability to the capabilities of a model. That is to say, such metrics only tell how good a model performs overall on a specific dataset, but uncovers little about what specific skills a model has gained and the level of each skill. We argue that the value of each data sample varies during the training process of a model, depending on the model's current capabilities. A deep understanding of the model's intrinsic capabilities can help us estimate each data sample's learning value and better manage the training process to improve the training efficiency. Take student learning as an example. There is no doubt that a college student can do well in solving primary school level exercises, but such exercises do not help improve his/her ability. On the contrary, a primary school student can not acquire knowledge efficiently from college-level exercises due to the big gap between his/her current knowledge or skills and the require- Capability c i Subclasses Metrics m j i Reading words Recognize vocabulary Intra-n (Gu et al., 2018) and Ent-n (Serban et al., 2017) . Recognize function words Frequency of function words. Reading sentences Recognize grammaticality Height and width of a question's constituency parsing tree. Readability Readability metrics. Understanding words Arithmetic operation Frequency of numerical expressions (CD tag). Logical operation Frequency of logically qualified words such as any, all and every. Understanding sentences Syntactic and semantic overlap BLEU-n (Papineni et al., 2002) , BERTScore (Zhang* et al., 2020) and MoverScore (Zhao et al., 2019) between the context and question. Coreference resolution Frequency of personal and possessive pronouns, such as PRP and PRP$ tags. Linguistic reasoning Con/Dis-junction, negation Frequency of coordinating junctions, such as and and or. Causality Frequency of causal clauses, such as because and the reason for. Spatial/Temporal Frequency of spatial/temporal expressions, such as before, after. and in front of. Factual reasoning Multi-hop reasoning Number of supporting evidences. ment to solve the exercises. We need to measure the ability of a student and then choose the appropriate exercises accordingly. Existing works on interpreting MRC model capabilities concentrate on analyzing a model's behavior with adversarial data (Jia and Liang, 2017) , or defining the prerequisite skills to solve a specific dataset (Sugawara et al., 2017) . However, these works require costly human annotation efforts or ignore the fact that model capabilities change during the training progresses. In this paper, we design a competency assessment framework for MRC model capabilities. Specifically, we define four major capability dimensions for understanding text and solving MRC tasks: reading words, reading sentences, understanding words and understanding sentences, which are inspired by the computational models of human text comprehension in psychology (Kintsch, 1988) . Based on the proposed framework, we can obtain a more appropriate assessment of model capabilities than the regular EM or F 1 metrics. Furthermore, we analyze a variety of data properties to estimate how good a model has to be to solve a specific data sample and identify the relationships between data properties and model performance. This greatly helps us estimate the learning value of each training sample. Based on this analysis, we discover a very common situation: if a sample is scored as a high value in one capability dimension, the other dimensions have the same tendency as well, and vice versa. To alleviate these inevitable correlations, we utilize data whitening to quantify each sample as four capability-specific scores in a decorrelated fashion. Finally, to reveal the potential usefulness of our proposed competency assessment framework and evaluate its efficiency, we employ it in a curriculum learning pipeline and design a Capability Boundary Breakthrough Curriculum (CBBC) strategy. This strategy gradually enlarges the model capability boundary by picking samples around the boundary and breaking through it. Based on the analysis of model capabilities and data properties, we feed the model with training samples that are neither too simple nor too hard for it to solve. Extensive experiments on four benchmark datasets demonstrate that our approach significantly improves the performance of existing MRC models, achieving up to an 11.22% / 8.71% improvement of EM / F 1 on MRC tasks. These results show the reasonability and effectiveness of our proposed assessment framework and provide a widely applicable measurement for dealing with the relationship between the model capability and data quality. Competency Assessment of MRC Capabilities In this section, we first formulate our competency assessment framework of 4-dimensional MRC capabilities. Based on this framework, the data properties related to each capability dimension are described as corresponding heuristic metrics. We then uncover the relationship between various data properties and model performance in a decorrelated manner, quantifying each sample as 4-dimensional capability-specific scores with little correlation. Assessment Framework Formulation Human text comprehension has been studied in psychology for a long time. Constructionist, landscape model, and computational architectures have been proposed for such comprehension (McNamara and Magliano, 2009) . Among them, the construction- integration (CI) model (Kintsch, 1988) Reading words. To formulate the surface structure of the CI model in our framework, we first highlight the text representation at the verbal or linguistic level. Theoretically, the units at the linguistic level are the words that make up the text and the hierarchical sentence constituents to which these words belong. Empirically, Sugawara et al. (2018) has shown that some questions are answered correctly by just reading the first k tokens. Similarly, the perturbation-based experiments of Nema and Khapra (2018) have demonstrated the significant influence of four types of words (i.e. content words, named entities, question types, and function words) on an MRC question. Therefore, the dimension of reading words is defined as recognizing the observed vocabulary and the special words' appearance (i.e. function words). In this study, The former is implemented as Intra-n (Gu et al., 2018) and Ent-n (Serban et al., 2017) to measure vocabulary distribution, while the latter is computed as the frequency of corresponding words. Reading sentences. The rules that are used to form a sentence using the aforementioned linguistic units are conventional phrase-structure grammars. Consequently, before understanding the information contained in a text, an MRC system inevitably requires capturing the sentence structure and handling the possible obscure words. We define the dimension of reading sentences as recognizing grammaticality and readability, and they are implemented by constituency parsing tree statistics and readability metrics 1 , respectively. Understanding words. The semantic representation of text is usually established by local and global links according to the linguistic units at word-level and sentence-level, respectively. To reflect the local semantic structure, we design the dimension of understanding words to assess how well an MRC model understands the relationships between words. In this work, we exemplify two relations (i.e. the arithmetic operations and logical items) that usually have salient patterns in the text. The former directly focuses on statistical and operational reasoning from the text, while the latter deals with the reasoning of predicate logic, e.g. conditionals and qualifiers. Inspired by the human annotation process (Boratko et al., 2018; Schlegel et al., 2020) , where the annotators are asked to label as many reasoning skills as possible by paying more attention to corresponding indicative words, the sub-capabilities of this dimension is quantified as the frequency of those words. Understanding sentences. Integrating the local structures into a global representation requires not only the text itself but also specific knowledge. To simplify the forms of knowledge, we divide the dimension of understanding sentences into two subclasses, linguistic and factual reasoning. They respectively mean understanding the relationship between sentences based on linguistics and the events (i.e. five dimensions including time, space, causation, intentionality, and objects). Among metrics of this dimension, BERTScore (Zhang* et al., 2020) , MoverScore (Zhao et al., 2019) and LS_score (Wu et al., 2020) are used to measure semantic overlap between the context and question and multihop reasoning is an extra particular subclass on the HotpotQA (Yang et al., 2018; Cheng et al., 2021) dataset. For the other sub-capabilities of this dimension, we consider lessons of the ablation operations performed by Sugawara et al. (2020) to observe the performance change of the MRC model and quantify them using the corresponding indicative structures. Consider the two examples questions shown in Figure 1 . To solve Q1, an MRC system just needs to match the words between the question and context. However, Q2 requires understanding temporal relations among the events (went to the grocery store \u2192 walked to the fast-food restaurant) and the verb semantics (walk to means go to). Therefore, Q2 is more challenging to the MRC system than Q1. Please refer to Appendix B for more detailed examples and descriptions of our employed metrics. Relationship Between Data Properties and Model Performance Based on our assessment framework, the learning value of each sample is also decomposed into four dimensions, namely capability-specific values. In this section, we first uncover the connection between the capability-specific values and model performance from four dimensions and then recalibrate the connection by removing the interdimension correlations. Capability-specific value. Given a sample x, we represent it by four capability-specific value (denoted as {v i (x)} 4 i=1 ) to reflect its learning value for 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 each capability dimension. According to our assessment framework, v i (x) can be computed by merging the corresponding metrics {m j i (x)} n(i) j=1 . Specifically, considering the sensitivity of capabilityspecific value to different ranges of the metric score, we normalize each raw metric m j i (x) from its original scale to range [0, 1] by the cumulative density function (CDF) as Platanios et al. ( 2019 ), which is denoted as m j i (x). In this work, the normalization computes the cumulative density from a higher model performance to ensure that the normalized metric and model performance are negatively correlated. The capability-specific score v i (x) is for- mulated as: v i (x) = 1 n(i) n(i) j=1 m j i (x). Analysis between capability-specific values and model performance. For each sample x, we obtain a 4-dimensional score {v i (x)} 4 i=1 . It is necessary to explore the relationship between samples' v i (x) and model performance for knowing about what specific capabilities a model has gained and the level of each capability. In this work, we employ BERT-base (Devlin et al., 2018) as the MRC model and train it respectively on training split of datasets SQuADv1 (Rajpurkar et al., 2016) , SQuADv2 (Rajpurkar et al., 2018) , Hot-potQA (Yang et al., 2018) and RACE (Lai et al., 2017) . We then analyze the correlations between four capability-specific scores and the model's overall performance on the corresponding dev split. In addition to F 1 , we also report the results of scaled F 1 (denoted as F logits ) by taking the model's confidence to an answer span or candidate into account.  F logits is computed as: v 1 v 2 v 3 v 4 v 1 v 2 v 3 v 4 (a) Before apply- ing inter-dimension decorrelation. v 1 v 2 v 3 v 4 v 1 v 2 v 3 v F logits = F 1 * ln(slog) * ln(elog) or F 1 * ln(candlog) (1) where slog and elog mean the model output logits for start and end token in answer extraction style questions, and candlog represents the largest logits among all candidate answers. Table 2 quantitatively shows the Pearson's correlation coefficients (r) between capability-specific values and model performance. From the results, we have the following observations: First, each capability-specific score has a relatively strong correlation with the model performance under a statistically significant guarantee, showing the reasonability of our capability-based assessment framework. Second, F logits shows better relevancy than F 1 , which indicates that F logits is a more appropriate performance measurement in our framework. We further explore the distribution of model performance over different ranges of v i . The distribution diagrams of v 1 and v 4 are shown in Figure 2 . There are two inspiring characteristics in this diagram: First, among all the bins of v i , the frequency of prediction results within the intermediate range (0.4 \u223c 0.6) are similar (\u2248 50%). Second, as the v i increases, the frequency of prediction results within a low range (0.0 \u223c 0.2) also increases, while the one of a high range (0.8 \u223c 1.0) decreases. These observations reveal that the samples with high v i can be used in indicative measurements to the corresponding model capability c i . Please refer to Appendix C for more diagrams illustrating this relationship. 3a in a heatmap fashion. The results show a common situation where if a sample is difficult (scored as high capability-specific value) in a dimension, the other dimensions have the same tendency and vice versa. To alleviate the inevitable correlations and construct a clear value representation for our following specific application scenario (i.e. CBBC), we eliminate the 4-dimensional capabilities by decorrelation. Specifically, we employ zero-phase component analysis (ZCA) whitening (Bell and Sejnowski, 1996) to diagonalize the covariance matrix while keeping the local information of the samples as much as possible. Inter-dimension decorrelation. Let V = {v i |i = 1, \u2022 \u2022 \u2022 , 4}. Pairwise correlations of V are illustrated in Figure As shown in Figure 3b , the 4-dimensional capabilities are not highly correlated after interdimension decorrelation, which can be in favor of constructing clear indicators for our following data sampling in CBBC. Improve Learning Efficiency with Competency Assessment In this section, our (Mihalcea, 2004 ) (Appendix E). Figure 4 shows an illustration of the pipeline of our CBBC. Following the original formulation of curriculum learning (Bengio et al., 2009) , our CBBC organizes all samples by a sequence of ordered training stages {s} S s=1 and corresponding training sets {D s } S s=1 with an easy-to-difficult fashion. The classic curriculum learning works (Soviany et al., 2021) usually consist of two essential components: the performance measurer and the curriculum scheduler. In general, the measurer is used to determine the learning status of a model by evaluating performance, while the scheduler is responsible for deciding when and how to update the curriculum by selecting the input samples. In our work, the measurer and scheduler are implemented by analyzing the multi-dimensional capability levels of the model interpretably and measuring the capability-specific values of the data in a decorrelated way, respectively. That is to say, the only difference between our CBBC and the original curriculum learning design is incorporating MRC capability assessment into the curriculum learning. Without significantly increasing the complexity of the pipeline, our proposed assessment framework can generally empower the MRC training pipeline in a plug-and-play manner. Performance measurer. Recall what we have discussed in Section 2.2 that the samples with high v i can be used in indicative measurements to the corresponding model capability c i . In this work, we use samples scored in the top-k of each capabilityspecific value to assess the corresponding model capability. More precisely, we first evaluate the model on the dev set and obtain an average F logits for each capability on the corresponding top-k subset. Then partial correlation (Baba et al., 2004) (denoted as \u03c1 i ) between dimension v i and F logits is computed to mask the contributions of the other dimensions V\\{v i }. After that, each model capability on stage s is quantified as: c s i = \u03c1 i 4 j=1 \u03c1 j F logits . Empirically, we set k in top-k as 32. Curriculum scheduler. Following the most works (Xu et al., 2020; Platanios et al., 2019) , we schedule the curriculum at a linear pace (every 1,000 training iterations). During each curriculum schedule, we enlarge the training set two times until it includes all the samples. The capability upper bound c s+1 i for s + 1 stage by exponential growth: c s+1 i = max{\u03b3c s i , 1.0}. After that, we use criterion v i (x) < c s+1 i to construct candidate set D s+1 i for the i-th capability on the state s + 1, and use absolute contribution of v i to F logits as sampling ratio (i.e. \u03c1 1 : \u03c1 2 : \u03c1 3 : \u03c1 4 ) to construct D s+1 . Experiments Datasets. We employ two question styles to evaluate our CBBC: answer span extraction and multiple choice. The former consists of SQuADv1 (Rajpurkar et al., 2016) , SQuADv2 (Rajpurkar et al., 2018) and HotpotQA (Yang et al., 2018) , while the latter adopts RACE (Lai et al., 2017) . For each dataset, we train and evaluate the model on official training and dev split, respectively. Implementation details. The source code and hyperparameters are included in the supplementary material. We use BERT-base (Devlin et al., 2018) as our backbone model, which is initialized by pre-trained parameters from cased BERT. AdamW (Loshchilov and Hutter, 2017) optimizer with weight decay 5e \u2212 4 and epsilon 8 is used to finetune the model with max sequence length 384, document stride 128. The learning rate warms up over the first 10% steps and then decays linearly to 0 for all experiments with training batch size 16 and maximum iteration 40, 000. Baseline models. In addition to the BERTbase model, we also consider the following ten baselines. The first two baselines are trained through a pre-defined curriculum learning strategy, which sorts the samples, then feeds them to the model stage-by-stage. \"B+CL+V (M 2 )\" sorts the samples by four capability-specific scores in an easy-to-difficult order. \"B+antiCL+V (M 3 )\" does like \"M 2 \", but in a reverse difficult-to-easy order. The following five baselines are trained using our CBBC strategy to maximize the data value in each dimension, respectively. \"B+C+v 1 (M 4 )\", \"B+C+v 2 (M 5 )\", \"B+C+v 3 (M 6 )\" and \"B+C+v 4 (M 7 )\" use the corresponding v 1 , v 2 , v 3 and v 4 respectively to perform the competency test and filter samples. \"B+C+V corr (M 8 )\" is trained using four correlated scores through CBBC. The following three baselines are devised by embedding other instance scoring methods into our CBBC pipeline. \"B+C+DatasetMap (M 9 )\", \"B+C+Forgetting (M 10 )\" and \"B+C+Predictability (M 11 )\" substitute the capability-specific scores with the confidence score of true answer span (Swayamdipta et al., 2020) , number of \"forgotten\" events (Toneva et al., 2018) and predictability score (Le Bras et al., 2020) , respectively. The last two baselines (denoted as M 12 and M 13 ) are start-of-the-art curriculum learning pipelines consisting of DRCA (Xu et al., 2020) and CBCL (Platanios et al., 2019) . Finally, our full model is trained using four decorrelated scores through CBBC instead. The critical difference between the full model and M 8 is the decorrelation operation. Experimental Results Quantitative Results. We present a summary of our quantitative results in Table 3 . As shown in the table, we have the following key observations. On the one hand, our proposed competency framework does benefit the MRC learning efficiency in either a single or multiple dimensions. For example, when using a pre-defined curriculum strategy, M 2 achieves EM and F 1 far beyond M 1 , highlighting that our quantification to data properties properly estimates the learning value contained in the data. M 3 degrades performance w.r.t. M 1 , demonstrating that the learning strategy from easy to difficult samples is more reasonable than the reverse. When equipped with our CBBC, all models of M 4 , M 5 , M 6 and M 7 achieve improvements w.r.t. M 1 on four datasets, which indicates the significant contribution of each capability dimension on gradually increasing the model capability. In particular, among the four different dimensions, M 7 has the best result, indicating that understanding sentences is a relatively more important capability for MRC. M 8 outperforms all the models except for ours. This demonstrates that our CBBC can maximize the learning value of the data sample to increase an MRC model's capability. On the other hand, our framework wins other scoring methods and curriculum learning pipelines by a considerable margin. Although M 9 , M 10 , M 11 , M 12 and M 13 achieve substantial improvements on four datasets w.r.t. M 1 , their perfor-   mances are still worse than our full model. These results verify that our proposed framework can assess the model capability more correctly and make better use of the learning value within data. Finally, our full model achieves significantly higher EM , F 1 and Acc. compared to all other baselines, demonstrating the necessity of the decorrelation between capability-specific scores. Its superior performance roots from constructing a decorrelated value representation of each dimension for our CBBC learning strategy. Overall, compared to M 1 , our full model achieves tremendous improvement of EM / F 1 up to 11.22% / 8.71% on the average of three answer extraction style datasets. Qualitative Results. Figure 5 shows the performance of baselines on the HotpotQA dev set. There are two observations worth noting here. First, the performance of our full model lies consistently on top of the other baseline models during the whole training stage. This result shows that CBBC can make the model more prepared for complex samples by enlarging its capability boundary step by step. Second, the performance plot of the baseline model with v 4 sits on top of other baselines with v 1 , v 2 , and v 3 from the beginning of training to the end. This result highlights the main contribution of v 4 (understanding sentences) to the final performance. Furthermore, the capability map after maxmin normalization of the model is shown in Fig- ure 6. First, among 4-dimensional capability, the c 3 (i.e. understanding words) has the largest initial value. A possible explanation is that pre-trained BERT has a fair amount of prior knowledge obtained from unlabeled corpus, which concentrates more on semantic understanding of words. Second, the capability c 1 increases at the fastest speed as the training progresses. Interestingly, the model M 4 based on v 1 does not seem improving accordingly as the capability c 1 increases. The possible reason could be that the superficial structure is easy to learn from samples but makes a limited contribution to the final performance. Please refer to Appendix D for the results of other MRC models. Human Annotation Annotation specification. we ask three annotators to answer (100 \u00d7 4 = 400) questions randomly sampled from four datasets, consisting of SQuADv1, SQuADv2, HotpotQA, and RACE. Using only our proposed four capabilities, they first read the context, question, and gold standard answer (the correct candidate answer under multiplechoice situation), and then choose the evidence sentences in context. After that, they respectively label the subclasses of four major capabilities as 1 (required) or 0 (not required). Please refer to Appendix A for more details about human annotation. Annotation results. In the annotation of required capabilities, the inter-annotator agreement is 75.33% for all 400 samples. We use the average of three corresponding annotator labels as the final human judgments for a specific sub-capability required by the question. Finally, a sample will be annotated (2 + 2 + 2 + 6 = 12) human ratings. Table 4 summarizes the correlations between human judgments and capability-specific scores of samples. The relatively strong correlations on all four dimensions indicate that our employed heuristic metrics can reasonably approximate the learning value contained in the samples. Related Work Analytic approaches to MRC capability. Some works performed skill-based analyses for the MRC model. In the scientific question domain, Clark et al. (2018) constituted the ARC benchmark, which requires far more powerful knowledge and reasoning than previous benchmarks. In a generalizable definition, Sugawara et al. (2017) proposed a set of 10 skills for MCTest (Richardson et al., 2013) . The others focused more on the analysis of the MRC dataset itself. For example, Sugawara et al. ( 2020 ) proposed a semi-automated, ablation-based methodology to assess the capacities of datasets. Rajpurkar et al. (2016) analyzed their proposed datasets using several types of reasoning, e.g. lexical and syntactic variation, and multiple sentence reasoning. Nevertheless, they require too costly human efforts and ignore that the model capability changes as training progresses. Data selection for debiased representations. Some works proposed different criteria to score instances according to the model response to input. Swayamdipta et al. (2020) built data maps using training dynamics measures for scoring data samples. Toneva et al. (2018) also employed the number of \"forgotten\" events to measure a sample, which was misclassified during a later epoch of training, despite being classified correctly earlier. The others (Le Bras et al., 2020) used adversarial filtering algorithms to rank instances based on their \"predictability\". However, these approaches require training a model once in advance on the dataset to obtain the corresponding training dynamics, which is computationally expensive, especially when using a large model. Conclusion We design a competency assessment framework for MRC capabilities, which describes model skills in an explainable and multi-dimensional manner. By leveraging the framework, we further uncover and disentangle the connections between various data properties and model performance on a specific task, as well as propose a capability boundary breakthrough curriculum (CBBC) strategy to maxi-mize the data value and improve training efficiency. The experiments performed on four benchmark datasets verified that our approach can significantly improve the performance of existing MRC models. Our work shows a deep understanding of model capabilities and data properties helps monitor the model skills during training and improves learning efficiency. Our framework and learning strategy are also generally applicable to other NLP tasks. A Annotation Details We ask three annotators to answer (100 \u00d7 4 = 400) questions randomly sampled from four datasets, consisting of SQuADv1, SQuADv2, HotpotQA, and RACE. They are graduate students majoring in Computer Science or Electronic Engineering and competent in English. They voluntarily offer to help without being compensated in any form. Before annotation, they are informed of the detailed annotation instruction with the following three steps. \u2022 Step 1. Make a hypothesis using a question statement and gold standard answer or the correct candidate under the multiplechoice situation. Example 1: Q: Why did Tom look angry? A: His sister ate his cake. \u2192 Hypothesis: Tom looked angry because his sister ate his cake. Example 2: Q: When did French Revolution occur? A: In 1789 \u2192 Hypothesis: French Revolution occurred in 1789. \u2022 Step 2. Select sentences (from the context) required to provide the hypothesis. Example 1: Context: (C1) Tom is a student. (C2) Tom looks annoyed because his sister ate his cake. (C3) His sister's name is Sylvia. Hypothesis: Tom looks angry because his sister ate his cake. \u2192 Select: C2 \u2022 Step 3. Select capabilities required for understanding an entailment from selected context sentences to hypothesis and label the corresponding capability as 1 (required). Example 1: C2: Tom looks annoyed because his sister ate his cake. Hypothesis: Tom looks angry because his sister ate his cake. \u2192 Capability: causal relation (\"because\"), semantic overlap (lexical knowledge of \"annoyed = angry\") Then, we describe our annotation schema in greater detail. We present the respective phenomenon, give a short description, and present an example illustrating the corresponding feature. \u2022 Reading words Recognize vocabulary. We annotate this as \"1\" if repetition of some word rarely occurs in a sentence (less than two times in every ten words). Question with label 0: The creek of which Paradise Creek is a tributary of what river? Context: Paradise Creek is a 9.6 mi tributary of Brodhead Creek in the Poconos of eastern Pennsylvania in the United States. Brodhead Creek is a 21.9 mi tributary of the Delaware River in the Poconos of eastern Pennsylvania in the United States. Question with label 1: Of these two publications-B\u00e1iki and Sick-what type of publication is the one that was published most frequently? Context: B\u00e1iki: The International S\u00e1mi Journal (\"B\u00e1iki\" means a place in Sami) is a biannual English-language publication that covers Sami culture, history, and current affairs. The coverage also includes the community affairs of the Sami in North America, estimated at some 30,000 people. Sick was a satiricalhumor magazine published from 1960 to 1980, lasting 134 issues. Recognize function words. We annotate this as \"1\" if a sentence consists of lots of the structural relationships between words signaled by function words (more than five times in every ten words). Question: What drug is among the list of illegal drugs in the Philippines and can be taken by mouth or by injection? Context: [. . .] Ephedrine and methylenedioxy methamphetamine are also among the list of illegal drugs that are of great concern to the authorities. Ephedrine is a medication and stimulant. [. . .] \u2022 Reading sentences Recognize grammaticality. We annotate this as \"1\" the sentence pattern and grammar involved are relatively complex, such as multiple nested subordinate clause structures. Question: Sudha Kheterpal, who is a musician best known as the percussionist in Faithless, has played with what singer who is recognized as the highest-selling Australian artist of all time by the Australian Recording Industry Association? Readability. We annotate this as\"1\" if there are a lot of obscure words in the question or context (more than five times in every ten words). Question: The creature HNoMS Draug is named after comes from what kind of mythology? Context: Two ships of the Royal Norwegian Navy have borne the name HNoMS \"Draug\", after the sea revenant Draugr: The draugr or draug (Old Norse: \"draugr\", plural draugar ; modern Icelandic: \"draugur\", Faroese: \"dreygur\" and Danish, Swedish, and Norwegian: \"draug\" ), also called aptrganga or aptrgangr , literally \"again-walker\" (Icelandic: \"afturganga\" ) is an undead creature from Norse mythology. \u2022 Understanding words Arithmetic operation. We annotate this as \"1\" if an arithmetic operation needs to be performed to answer the question, such as addition, subtraction, ordering, and counting. Question: How many points were the Giants behind the Dolphins at the start of the 4th quarter? Context: New York was down 17-10 behind two rushing touchdowns. Logical operation. We annotate this as \"1\" if it is required to understand the concept of quantification (existential and universal) in order to determine the correct answer. Question: How many presents did Susan receive? Context: On the day of the party, all five friends showed up. Each friend Quantification had a present for Susan. \u2022 Understanding sentences Syntactic and semantic overlap. We annotate this as \"1\" if some part of the context and the question overlap semantically. Question: Is it freezing today? Context: The weather is cold today. Coreference resolution. We annotate this as \"1\" if inter-sentence coreference and anaphora need to be resolved in order to retrieve the expected answer. Question: What is the name of the psychologist who is known as the originator of social learning theory? Context B Examples of Our Employed Metrics We first present a brief overview of employed metrics, especially those that are adapted from other studies. In the following descriptions, the question Q and corresponding context C are denoted as the sequence of n-grams Q n = {q n i } and C n = {c n j }, respectively. Intra-n and Ent-n. Intra-n (Gu et al., 2018) and Ent-n (Serban et al., 2017) are originally designed to evaluate the diversity of neural dialogue responses. The former calculates the ratio of distinct unigrams (Intra-1) and bigrams (Intra-2) in generated responses, while the latter measures the information entropy of n-grams. Specifically, in our work, they are formulated as: Intra-n = U nique(Q n ) |Q n | (2) Ent-n = i=1 \u2212 Count(q n i ) |Q n | log Count(q n i ) |Q n | (3) Tree statistics. Empirically, most of the complicated sentences have a relatively high and wide constituency parsing tree. In this paper, we calculate the height and width of the constituency parsing tree by Standford CoreNLP API (Manning et al., 2014) . Readability metrics. Readability is the ease with which a reader can understand a written text. Readability metrics produce an approximate representation of the US grade level needed to comprehend the text and are widely used in the field of education to assess the English proficiency of non-native English speakers. We employ the py-readability package (https://pypi.org/ 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 . 9 project/py-readability-metrics/) to calculate the readability of a question based on two metrics, including Flesch Kincaid Grade Level and Automated Readability Index (ARI). BERTScore and MoverScore. To measure the semantic overlap between the question and corresponding context, we slightly modify the contextualized embedding-based similarity metrics of text generation task, comprising BERTScore (Zhang* et al., 2020) and MoverScore (Zhao et al., 2019) . Unlike the original implementation, the question and the context rather than the gold standard reference are fed into the metrics computation. We exemplify two samples from HotpotQA dev set to show the difference of specific metrics in Table 5 and Table 6 . C Additional Diagrams of Samples' Capability-specific Value The distribution diagrams of v 2 and v 3 are shown in Figure 7 . They present a conclusion consistent with v 1 and v 4 discussed in Section 2.2. That is, among all the bins of v i , the frequency of prediction results within the intermediate range (0.4 \u223c 0.6) are similar (\u2248 50%). Furthermore, as the v i increases, the frequency of prediction results within a low range (0.0 \u223c 0.2) also increases, while the one of a high range (0.8 \u223c 1.0) decreases. In addition to the distribution of model performance over different ranges of v i , the mean value and standard deviation of model performance (in F logits ) over them and their subclasses are illustrated in Figure 8 . As shown in the figure, it qualitatively shows the relatively strong correlation be-   tween the model performance and each capabilityspecific score. D Additional Experiments Using Other Models In addition to the Transformer-based MRC model, we also perform ablation analysis using the following systems to further verify the effectiveness of our proposed assessment framework, whose training setting is consistent with that of our BERT-based MRC model. \u2022 R-Net (Wang et al., 2017) matches the question and passage with gated recurrent neural networks (RNNs) to obtain the questionaware passage representation and employs the pointer networks to locate the positions of answer span from the passages. \u2022 QANet (Yu et al., 2018) encodes the local and global interactions with the convolution and self-attention, respectively. It achieves higher training efficiency while obtaining the equivalent accuracy to the recurrent models. The quantitative results of R-Net and QANet are summarized in Table 7 and Table 8 , respectively. Our assessment framework also provides a significant performance improvement to these weaker backbones than BERT, such as RNN-based R-Net and convolution-based QANet. E Additional Experiments Using Other Pipelines To further verify the effectiveness of our proposed MRC competency assessment framework and reveal more available application scenarios for it, we embed it into the active learning pipeline besides curriculum learning. CBBC in active learning. Given the training state of a model, active learning aims to select the most valuable samples from the unlabeled dataset and hand it over to the oracle (e.g. human annotator) for labeling so as to reduce the cost of labeling as much as possible while still maintaining performance. Take the most common pool-based active learning (Lewis and Gale, 1994; Gal and Ghahramani, 2016) as an example, which queries the best sample based on the confidence evaluation and ranking of the entire dataset. This query strategy is usually implemented by the uncertaintybased sampling (Ebrahimi et al., 2019; Gal et al., 2017; Houlsby et al., 2011; Kirsch et al., 2019) and distribution-based sampling (Pinsler et al., 2019; Wei et al., 2015) . Compared to the original poolbased active learning (shown in Figure 9a ), CBBCguided active learning (shown in Figure 9b ) pro- Acknowledgements 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 . 9 4 v1 0.0 0.2 0.4 0.6 F1 * ln(logit) 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 . 9 4 v 1 1 F1 * ln(logit) 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 . 9 4 v 2 1 F1 * ln(logit) 0 . 0 6 0 . 1 9 0 . 3 1 0 . 4 4 0 . 5 6 0 . 6 9 0 . 8 1 0 . 9 4 v2 vides a novel and interpretable query strategy by assessing the model in a 4-dimensional capability. In addition to the pool-based baseline, we employ a more recent active learning pipeline ICAL (Gao et al., 2020) as a comparison, which selects samples with the high inconsistency of predictions over a set of data augmentations. We employ BERT as our MRC backbone. In each active learning cycle, we continue to train the MRC model by adding 5% labeled data points by simulating the oracle annotating process. The initial training set is randomly sampled from the HotpotQA train split and follows Gao et al. (2020) for the setting of initial training set size and active learning budget. Figure 10 illustrates the results of different meth-  ods at each active learning cycle qualitatively. Our CBBC-guided active learning (denoted as \"CBBC-AL\") achieve a higher MRC performance than the pool-based active learning (denoted as \"Pool-AL\") and ICAL from the start of training, demonstrating that the assessment of MRC model capability derived by our CBBC is reasonable and can also make a substantial difference to active learning beside curriculum learning. When using only 35% labeled samples, our CBBC-guided active learning outperforms the baseline model normally trained on the entire dataset by a considerable margin.",
    "abstract": "Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match (EM ) and F 1 . However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of EM / F 1 on MRC tasks.",
    "countries": [
        "Canada",
        "China"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Feeding What You Need by Understanding What You Learned"
}