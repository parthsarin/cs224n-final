{
    "article": "Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT 1 , a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties). Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 PORT: Precision-Order-Recall Tunable metric. roles: to allow rapid (though sometimes inaccurate) comparisons between different systems or between different versions of the same system, and to perform tuning of parameter values during system training. The latter has become important since the invention of minimum error rate training (MERT) (Och, 2003) and related tuning methods. These methods perform repeated decoding runs with different system parameter values, which are tuned to optimize the value of the evaluation metric over a development set with reference translations. MT evaluation metrics fall into three groups: \u2022 BLEU (Papineni et al., 2002) , NIST (Doddington, 2002) , WER, PER, TER (Snover et al., 2006) , and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). \u2022 METEOR (Banerjee and Lavie, 2005) , METEOR-NEXT (Denkowski and Lavie 2010) , TER-Plus (Snover et al., 2009) , MaxSim (Chan and Ng, 2008) , TESLA (Liu et al., 2010) , AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. \u2022 More sophisticated metrics such as RTE (Pado et al., 2009) , DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011 ) use higher level syntactic or semantic analysis to score translations. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2010; Callison-Burch et al., 2011) . However, BLEU remains the de facto standard tuning metric, for two reasons. First, there is no evidence that any other tuning metric yields better MT systems. Cer et al. (2010) showed that BLEU tuning is more robust than tuning with other metrics (METEOR, TER, etc.), as gauged by both automatic and human evaluation. Second, though a tuning metric should correlate strongly with human judgment, MERT (and similar algorithms) invoke the chosen metric so often that it must be computed quickly. Liu et al. (2011) claimed that TESLA tuning performed better than BLEU tuning according to human judgment. However, in the WMT 2011 \"tunable metrics\" shared pilot task, this did not hold (Callison-Burch et al., 2011) . In (Birch and Osborne, 2011) , humans preferred the output from LRscore-tuned systems 52.5% of the time, versus BLEU-tuned system outputs 43.9% of the time. In this work, our goal is to devise a metric that, like BLEU, is computationally cheap and language-independent, but that yields better MT systems than BLEU when used for tuning. We tried out different combinations of statistics before settling on the final definition of our metric. The final version, PORT, combines precision, recall, strict brevity penalty (Chiang et al., 2008) and strict redundancy penalty (Chen and Kuhn, 2011) in a quadratic mean expression. This expression is then further combined with a new measure of word ordering, v, designed to reflect long-distance as well as short-distance word reordering (BLEU only reflects short-distance reordering). In a later section, 3.3, we describe experiments that vary parts of the definition of PORT. Results given below show that PORT correlates better with human judgments of translation quality than BLEU does, and sometimes outperforms METEOR in this respect, based on data from WMT (2008 WMT ( -2010)) . However, since PORT is designed for tuning, the most important results are those showing that PORT tuning yields systems with better translations than those produced by BLEU tuning -both as determined by automatic metrics (including BLEU), and according to human judgment, as applied to five data conditions involving four language pairs. BLEU and PORT First, define n-gram precision p(n) and recall r(n): ) ( grams - n # ) ( grams - n # ) ( T R T n p \u2229 = (1) ) ( grams - n # ) ( grams - n # ) ( R R T n r \u2229 = (2) where T = translation, R = reference. Both BLEU and PORT are defined on the document-level, i.e. T and R are whole texts. If there are multiple references, we use closest reference length for each translation hypothesis to compute the numbers of the reference n-grams. BLEU BLEU is composed of precision P g (N) and brevity penalty BP: BP N P BLEU g \u00d7 = ) ( (3) where P g (N) is the geometric average of n-gram precisions N N n g n p N P 1 1 ) ( ) ( \uf8f7 \uf8f7 \uf8f8 \uf8f6 \uf8ec \uf8ec \uf8ed \uf8eb = \u220f = (4) The BLEU brevity penalty punishes the score if the translation length len(T) is shorter than the reference length len(R); it is: ( ) ) ( / ) ( 1 , 0 . 1 min T len R len e BP \u2212 = (5) PORT PORT has five components: precision, recall, strict brevity penalty (Chiang et al., 2008) , strict redundancy penalty (Chen and Kuhn, 2011) and an ordering measure v. The design of PORT is based on exhaustive experiments on a development data set. We do not have room here to give a rationale for all the choices we made when we designed PORT. However, a later section (3.3) reconsiders some of these design decisions. Precision and Recall The average precision and average recall used in PORT (unlike those used in BLEU) are the arithmetic average of n-gram precisions P a (N) and recalls R a (N): \u2211 = = N n a n p N N P 1 ) ( 1 ) ( (6) \u2211 = = N n a n r N N R 1 ) ( 1 ) ( (7) We use two penalties to avoid too long or too short MT outputs. The first, the strict brevity penalty (SBP), is proposed in (Chiang et al., 2008) . Let t i be the translation of input sentence i, and let r i be its reference. Set \uf8f7 \uf8f7 \uf8f8 \uf8f6 \uf8ec \uf8ec \uf8ed \uf8eb \u2212 = \u2211 \u2211 i i i i i r t r SBP |} | |, min{| | | 1 exp (8) The second is the strict redundancy penalty (SRP), proposed in (Chen and Kuhn, 2011) : \uf8f7 \uf8f7 \uf8f8 \uf8f6 \uf8ec \uf8ec \uf8ed \uf8eb \u2212 = \u2211 \u2211 i i i i i r r t SRP | | |} | |, max{| 1 exp (9) To combine precision and recall, we tried four averaging methods: arithmetic (A), geometric (G), harmonic (H), and quadratic (Q) mean. If all of the values to be averaged are positive, the order is max Q A G H min \u2264 \u2264 \u2264 \u2264 \u2264 , with equality holding if and only if all the values being averaged are equal. We chose the quadratic mean to combine precision and recall, as follows: 2 ) ) ( ( ) ) ( ( ) ( 2 2 SRP N R SBP N P N Qmean a a \u00d7 + \u00d7 = (10) Ordering Measure Word ordering measures for MT compare two permutations of the original source-language word sequence: the permutation represented by the sequence of corresponding words in the MT output, and the permutation in the reference. Several ordering measures have been integrated into MT evaluation metrics recently. Birch and Osborne (2011) use either Hamming Distance or Kendall's \u03c4 Distance (Kendall, 1938) in their metric LRscore, thus obtaining two versions of LRscore. Similarly, Isozaki et al. (2011) adopt either Kendall's \u03c4 Distance or Spearman's \u03c1 (Spearman, 1904) distance in their metrics. Our measure, v, is different from all of these. We use word alignment to compute the two permutations (LRscore also uses word alignment). The word alignment between the source input and reference is computed using GIZA++ (Och and Ney, 2003) beforehand with the default settings, then is refined with the heuristic grow-diag-finaland; the word alignment between the source input and the translation is generated by the decoder with the help of word alignment inside each phrase pair. PORT uses permutations. These encode one-toone relations but not one-to-many, many-to-one, many-to-many or null relations, all of which can occur in word alignments. We constrain the forbidden types of relation to become one-to-one, as in (Birch and Osborne, 2011) . Thus, in a one-tomany alignment, the single source word is forced to align with the first target word; in a many-to-one alignment, monotone order is assumed for the target words; and source words originally aligned to null are aligned to the target word position just after the previous source word's target position. After the normalization above, suppose we have two permutations for the same source n-word input. E.g., let P 1 = reference, P 2 = hypothesis: P 1 : 1 1 p 2 1 p 3 1 p 4 1 p \u2026 i p 1 \u2026 n p 1 P 2 : 1 2 p 2 2 p 3 2 p 4 2 p \u2026 i p 2 \u2026 n p 2 Here, each j i p is an integer denoting position in the original source (e.g., 1 1 p = 7 means that the first word in P 1 is the 7 th source word). The ordering metric v is computed from two distance measures. The first is absolute permutation distance: \u2211 = \u2212 = n i i i p p P P DIST 1 2 1 2 1 1 | | ) , ( (11) Let 2 / ) 1 ( ) , ( 1 2 1 1 1 + \u2212 = n n P P DIST \u03bd (12) v 1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman's \u03c1 (Spearman, 1904) . However, we have found that \u03c1 punishes long-distance reorderings too heavily. For instance, 1 \u03bd is more tolerant than \u03c1 of the movement of \"recently\" in this example: Ref: Recently, I visited Paris Hyp: I visited Paris recently Inspired by HMM word alignment (Vogel et al., 1996) , our second distance measure is based on jump width. This punishes a sequence of words that moves a long distance with its internal order conserved, only once rather than on every word. In the following, only two groups of words have moved, so the jump width punishment is light: = \u2212 \u2212 \u2212 \u2212 \u2212 = n i i i i i p p p p P P DIST 1 1 2 2 1 1 1 2 1 2 | ) ( ) ( | ) , ( (13) where we set 0 0 1 = p and 0 0 2 = p . Let 1 ) , ( 1 2 2 1 2 2 \u2212 \u2212 = n P P DIST v (14) As with v 1 , v 2 is also from 0 to 1, and larger values indicate more similar permutations. The ordering measure v s is the harmonic mean of v 1 and v 2 : ) / 1 / 1 /( 2 2 1 v v v s + = . (15) v s in ( 15 ) is computed at segment level. For multiple references, we compute v s for each, and then choose the biggest one as the segment level ordering similarity. We compute document level ordering with a weighted arithmetic mean: \u2211 \u2211 = = \u00d7 = l s s l s s s R len R len v v 1 1 ) ( ) ( ( 16 ) where l is the number of segments of the document, and len(R) is the length of the reference. Combined Metric Finally, Qmean(N) (Eq. ( 10 ) and the word ordering measure v are combined in a harmonic mean: \u03b1 v N Qmean PORT / 1 ) ( / 1 2 + = ( 17 ) Here \u03b1 is a free parameter that is tuned on heldout data. As it increases, the importance of the ordering measure v goes up. For our experiments, we tuned \u03b1 on Chinese-English data, setting it to 0.25 and keeping this value for the other language pairs. The use of v means that unlike BLEU, PORT requires word alignment information. Experiments PORT as an Evaluation Metric We studied PORT as an evaluation metric on WMT data; test sets include WMT 2008 , WMT 2009 , and WMT 2010 all-to-English, plus 2009 , 2010 English-to-all submissions. The languages \"all\" (\"xx\" in Table 1 ) include French, Spanish, German and Czech. Table 1 summarizes the test set statistics. In order to compute the v part of PORT, we require source-target word alignments for the references and MT outputs. These aren't included in WMT data, so we compute them with GIZA++. We used Spearman's rank correlation coefficient \u03c1 to measure correlation of the metric with systemlevel human judgments of translation. The human judgment score is based on the \"Rank\" only, i.e., how often the translations of the system were rated as better than those from other systems (Callison-Burch et al., 2008) . Thus, BLEU, METEOR, and PORT were evaluated on how well their rankings correlated with the human ones. For the segment level, we follow (Callison-Burch et al., 2010) in using Kendall's rank correlation coefficient \u03c4. As shown in PORT achieved the best segment level correlation with human judgment on both the \"into English\" and \"out of English\" tasks. At the system level, PORT is better than BLEU, but not as good as METEOR. This is because we designed PORT to carry out tuning; we did not optimize its performance as an evaluation metric, but rather, to optimize system tuning performance. There are some other possible reasons why PORT did not outperform METEOR v1.0 at system level. Most WMT submissions involve language pairs with similar word order, so the ordering factor v in PORT won't play a big role. Also, v depends on source-target word alignments for reference and test sets. These alignments were performed by GIZA++ models trained on the test data only. PORT as a Metric for Tuning Experimental details The first set of experiments to study PORT as a tuning metric involved Chinese-to-English (zh-en); there were two data conditions. The first is the small data condition where FBIS 2 is used to train the translation and reordering models. It contains 10.5M target word tokens. We trained two language models (LMs), which were combined loglinearly. The first is a 4-gram LM which is estimated on the target side of the texts used in the large data condition (below). The second is a 5gram LM estimated on English Gigaword. The large data condition uses training data from NIST 3 2009 (Chinese-English track). All allowed bilingual corpora except UN, Hong Kong Laws and Hong Kong Hansard were used to train the translation model and reordering models. There are about 62.6M target word tokens. The same two LMs are used for large data as for small data, and the same development (\"dev\") and test sets are also used. The dev set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST. Evaluation was performed on NIST 2006 and 2008. Four references were provided for all dev and test sets. The third data condition is a French-to-English (fr-en). The parallel training data is from Canadian Hansard data, containing 59.3M word tokens. We used two LMs in loglinear combination: a 4-gram LM trained on the target side of the parallel training data, and the English Gigaword 5-gram LM. The dev set has 1992 sentences; the two test sets have 2140 and 2164 sentences respectively. There is one reference for all dev and test sets. The fourth and fifth conditions involve German--English Europarl data. This parallel corpus contains 48.5M German tokens and 50.8M English tokens. We translate both German-to-English (deen) and English-to-German (en-de). The two conditions both use an LM trained on the target side of the parallel training data, and de-en also uses the English Gigaword 5-gram LM. News test 2008 set is used as dev set; News test 2009, 2010, 2011 are used as test sets. One reference is provided for all dev and test sets. LDC2003E14 3 http://www.nist.gov/speech/tests/mt All experiments were carried out with \u03b1 in Eq. ( 17 ) set to 0.25, and involved only lowercase European-language text. They were performed with MOSES (Koehn et al., 2007) , whose decoder includes lexicalized reordering, translation models, language models, and word and phrase penalties. Tuning was done with n-best MERT, which is available in MOSES. In all tuning experiments, both BLEU and PORT performed lower case matching of n-grams up to n = 4. We also conducted experiments with tuning on a version of BLEU that incorporates SBP (Chiang et al., 2008) as a baseline. The results of original IBM BLEU and BLEU with SBP were tied; to save space, we only report results for original IBM BLEU here. Comparisons with automatic metrics First, let us see if BLEU-tuning and PORT-tuning yield systems with different translations for the same input. The first row of Table 3 shows the percentage of identical sentence outputs for the two tuning types on test data. The second row shows the similarity of the two outputs at wordlevel (as measured by 1-TER): e.g., for the two zhen tasks, the two tuning types give systems whose outputs are about 25-30% different at the word level. By contrast, only about 10% of output words for fr-en differ for BLEU vs. PORT tuning. Table 4 shows translation quality for BLEU-and PORT-tuned systems, as assessed by automatic metrics. We employed BLEU4, METEOR (v1.0), TER (v0.7.25) , and the new metric PORT. In the table, TER scores are presented as 1-TER to ensure that for all metrics, higher scores mean higher quality. All scores are averages over the relevant test sets. There are twenty comparisons in the table. Among these, there is one case (French-English assessed with METEOR) where BLEU outperforms PORT, there are seven ties, and there are twelve cases where PORT is better. Table 3 shows that fr-en outputs are very similar for both tuning types, so the fr-en results are perhaps less informative than the others. Overall, PORT tuning has a striking advantage over BLEU tuning. Both (Liu et al., 2011) and (Cer et al., 2011) showed that with MERT, if you want the best possible score for a system's translations according to metric M, then you should tune with M. This doesn't appear to be true when PORT and BLEU tuning are compared in Table 4 . For the two Chinese-to-English tasks in the table, PORT tuning yields a better BLEU score than BLEU tuning, with significance at p < 0.05. We are currently investigating why PORT tuning gives higher BLEU scores than BLEU tuning for Chinese-English and German-English. In internal tests we have found no systematic difference in BLEUs, so we speculate that PORT's emphasis on reordering yields models that generalize better for these two language pairs. Human Evaluation We conducted a human evaluation on outputs from BLEU-and PORT-tuned systems. The examples are randomly picked from all \"to-English\" conditions shown in Tables 3 & 4 (i.e., all conditions except English-to-German). We performed pairwise comparison of the translations produced by the system types as in (Callison-Burch et al., 2010; Callison-Burch et al., 2011) . First, we eliminated examples where the reference had fewer than 10 words or more than 50 words, or where outputs of the BLEU-tuned and PORT-tuned systems were identical. The evaluators (colleagues not involved with this paper) objected to comparing two bad translations, so we then selected for human evaluation only translations that had high sentence-level (1-TER) scores. To be fair to both metrics, for each condition, we took the union of examples whose BLEU-tuned output was in the top n% of BLEU outputs and those whose PORT-tuned output was in the top n% of PORT outputs (based on (1-TER)). The value of n varied by condition: we chose the top 20% of zh-en small, top 20% of ende, top 50% of fr-en and top 40% of zh-en large. We then randomly picked 450 of these examples to form the manual evaluation set. This set was split into 15 subsets, each containing 30 sentences. The first subset was used as a common set; each of the other 14 subsets was put in a separate file, to which the common set is added. Each of the 14 evaluators received one of these files, containing 60 examples (30 unique examples and 30 examples shared with the other evaluators). Within each example, BLEU-tuned and PORT-tuned outputs were presented in random order. After receiving the 14 annotated files, we computed Fleiss's Kappa (Fleiss, 1971) on the common set to measure inter-annotator agreement, all \u03ba . Then, we excluded annotators one at a time to compute i \u03ba (Kappa score without i-th annotator, i.e., from the other 13). Finally, we filtered out the files from the 4 annotators whose answers were most different from everybody else's: i.e., annotators with the biggest i all \u03ba \u03ba \u2212 values. This left 10 files from 10 evaluators. We threw away the common set in each file, leaving 300 pairwise comparisons. Table 5 shows that the evaluators preferred the output from the PORTtuned system 136 times, the output from the BLEU-tuned one 98 times, and had no preference the other 66 times. This indicates that there is a human preference for outputs from the PORTtuned system over those from the BLEU-tuned system at the p<0.01 significance level (in cases where people prefer one of them). PORT tuning seems to have a bigger advantage over BLEU tuning when the translation task is hard. Of the Table 5 language pairs, the one where PORT tuning helps most has the lowest BLEU in Table 4 (German-English); the one where it helps least in Table 5 has the highest BLEU in Table 4 (French-English). (Table 5 does not prove BLEU is superior to PORT for French-English tuning: statistically, the difference between 14 and 17 here is a tie). Maybe by picking examples for each condition that were the easiest for the system to translate (to make human evaluation easier), we mildly biased the results in Table 5 against PORT tuning. Another possible factor is reordering. PORT differs from BLEU partly in modeling longdistance reordering more accurately; English and French have similar word order, but the other two language pairs don't. The results in section 3.3 (below) for Qmean, a version of PORT without word ordering factor v, suggest v may be defined suboptimally for French-English. Computation time A good tuning metric should run very fast; this is one of the advantages of BLEU. Table 6 shows the time required to score the 100-best hypotheses for the dev set for each data condition during MERT for BLEU and PORT in similar implementations. The average time of each iteration, including model loading, decoding, scoring and running MERT 4 , is in brackets. PORT takes roughly 1.5 -2.5 as long to compute as BLEU, which is reasonable for a tuning metric. zh-en small zh-en large fr-en Hans de-en WMT en-de WMT BLEU 3 (13) 3 ( 17 ) 2 ( 19 ) 2 ( 20 ) 2 (11) PORT 5 (21) 5 ( 24 ) 4 ( 28 ) 5 ( 28 ) 4 (15) Analysis Now, we look at the details of PORT to see which of them are the most important. We do not have space here to describe all the details we studied, but we can describe some of them. E.g., does the ordering measure v help tuning performance? To answer this, we introduce an intermediate metric. This is Qmean as in Eq. ( 10 ): PORT without the ordering measure. Table 8 compares tuning with BLEU, PORT, and Qmean. PORT outperforms Qmean on seven of the eight automatic scores shown for small and large Chinese-English. However, for the European language pairs, PORT and Qmean seem to be tied. This may be because we optimized \u03b1 in Eq. ( 18 ) for Chinese-English, making the influence of word ordering measure v in PORT too strong for the European pairs, which have similar word order. Measure v seems to help Chinese-English tuning. What would results be on that language pair if we were to replace v in PORT with another ordering measure? Table 9 gives a partial answer, with Spearman's \u03c1 and Kendall's \u03c4 replacing v with \u03c1 or \u03c4 in PORT for the zh-en small condition (CTB with human word alignment is the dev set). The original definition of PORT seems preferable. A related question is how much word ordering improvement we obtained from tuning with PORT. We evaluate Chinese-English word ordering with three measures: Spearman's \u03c1, Kendall's \u03c4 distance as applied to two permutations (see section 2.2.2) and our own measure v. Table 10 shows the effects of BLEU and PORT tuning on these three measures, for three test sets in the zh-en large condition. Reference alignments for CTB were created by humans, while the NIST06 and NIST08 reference alignments were produced with GIZA++. A large value of \u03c1, \u03c4, or v implies outputs have ordering similar to that in the reference. From the table, we see that the PORT-tuned system yielded better word order than the BLEU-tuned system in all nine combinations of test sets and ordering measures. The advantage of PORT tuning is particularly noticeable on the most reliable test set: the hand-aligned CTB data. What is the impact of the strict redundancy penalty on PORT? Note that in Table 8 , even though Qmean has no ordering measure, it outperforms BLEU. Table 11 shows the BLEU brevity penalty (BP) and (number of matching 1-& 4-grams)/(number of total 1-& 4-grams) for the translations. The BLEU-tuned and Qmeantuned systems generate similar numbers of matching n-grams, but Qmean-tuned systems produce fewer n-grams (thus, shorter translations). E.g., for zh-en small, the BLEU-tuned system produced 44,677 1-grams (words), while the Qmean-trained system one produced 43,555 1grams; both have about 32,000 1-grams matching the references. Thus, the Qmean translations have higher precision. We believe this is because of the strict redundancy penalty in Qmean. As usual, French-English is the outlier: the two outputs here are typically so similar that BLEU and Qmean tuning yield very similar n-gram statistics. Conclusions In this paper, we have proposed a new tuning metric for SMT systems. PORT incorporates precision, recall, strict brevity penalty and strict redundancy penalty, plus a new word ordering measure v. As an evaluation metric, PORT performed better than BLEU at the system level and the segment level, and it was competitive with or slightly superior to METEOR at the segment level. Most important, our results show that PORTtuned MT systems yield better translations than BLEU-tuned systems on several language pairs, according both to automatic metrics and human evaluations. In future work, we plan to tune the free parameter \u03b1 for each language pair."
}