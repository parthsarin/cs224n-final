{
    "article": "This research aims to incorporate argument status-based modelling within an otherwise selectional constraint-based system of verb sense disambiguation, to capture effects such as underspecification, surface case alternation and semantic backingoff. The proposed implementation hinges around a description of the general behavioural characteristics of integral complements, complements, middles and adjuncts through a pre-determined weighting schema. On limited evaluation, the resultant system returned an accuracy of over 83%, and was further shown to significantly outperform baseline methods. Introduction The purpose of this paper is to devise a robust verb sense disambiguation (VSD) technique based around selectional constraints which, for a given clausal input, both determines the correct sense of the main verb and aligns input case slots from the input, with target case slots within the valency frame described for the chosen verb sense. The driving mechanism behind the proposed system is argument status, that is an expanded model of complementhood/adjuncthood as laid out by Somers (1984) incorporating 'integral complements', 'complements', 'middles' and 'adjuncts'. Our particular interest in argument status stems from the fact that the properties of each category of argument status can be generalised to provide a surprisingly accurate model of such effects as surface case alternation, scrambling, and propensity for semantic backing-off. Note that although this paper is centered around a VSD system for Japanese, the system mechanism is transferable to other languages for which the notion of argument status is equally well defined. In performing any research in VSD, there is clearly a requirement for some means of sense distinction/demarkation, and in our case this is based around the Goi-Taikei pattern-based valency dictionary (Ikehara et al. 1997; Shirai et al. 1997) , as developed by NTT for their ALT-J/E machine translation system (Ikehara et al. 1991) . A valency frame entry in the Goi-Taikei valency dictionary is represented as a list of case slots, each of which is provided with a set of class-based selectional constraints and/or lexical filler candidates. By nature, selectional constraints commonly overlap between individual case slots for a given valency frame, and also between valency frames; the co-existence of lexical filler candidates for many verb senses further complicates things. This brings about the need for an interface to the dictionary which is able to select between the potentially sense-compatible candidates, and at the same time have recourse to relax the selectional constraints appropriately in the case of over-restriction. A robust VSD system must be able to cope with non-canonical input. This correlates to being able to model semantic effects such as metonymy (see (Mahesh et al. 1997b )), Table 1 : Argument status matrix sense extension (see (Mahesh et al. 1997a )) and occurrences of unknown words, and lexical effects such as (in the case of Japanese) surface case alternation, zero anaphora, and scrambling. While we make no attempt to individually analyse the various types of semantic effects, there is a clear need to be able to back-off from the level of selectional constraint described in Goi-Taikei (semantic backing-off). This need is met through the application of argument status in balancing the trade-off between over-relaxation of selectional constraints bringing incompatible senses into play, and over-restriction discounting the correct analysis. To summarise, the main intent of this research is to apply argument status in scoring the various semantic and lexical idiosyncrasies faced in robustly disambiguating verb sense. Section 2 defines argument status and sets the scene for its later system application; Section 3 then goes on to describe the overall system architecture and various scoring schema utilised therein, followed by a basic evaluation of the system on Japanese inputs in Section 4. Argument status The argument status of a case slot is defined as the degree of boundedness of that case slot within the valency frame, or a measure of the intrinsic association between an argument and the predicate. The system of argument types utilised herein is a simplified version of the sixfold scale proposed by Somers (1984 Somers ( , 1987)) , and consists of integral complements, complements, middles and adjuncts. Integral complements (i.e. fixed arguments) are highly restricted as to scope for surface case alternation and case slot order alternation, are fixed as to lexical content, and are lexically obligatory (must have a lexical spell-out); complements, as major verb constituents, generally display high scope for surface case alternation, produce marked semantics upon case slot permutation, are generally not heavily demarcated semantically, and are argument obligatory (zero instances must be co-indexed to discourse entities or otherwise instantiated through deixis -see (Kameyama 1985) ); middles form an 'in-between' category between complements and adjuncts, and are generally reasonably restricted in terms of surface case alternation, relatively free in case slot positioning, generally well-demarcated semantically, and argument optional (zero instances indicate the noninstantiation of that case-role); and adjuncts are the most peripheral argument type, being highly restricted in terms of surface case alternation, freely insertable into almost any position within the case frame, strongly demarcated semantically, and argument optional. A summary of the characteristics of each argument type is given in Table 1 . 1 Argument status is not indicated within the basic Goi-Taikei framework, and was thus predicted from the case-role mark-up of each case slot and post-checked manually. The compatibility with different lexical effects for complements, middles and adjuncts is translated into the arg_weight function, with integral complements conditionally equivalent to complements in weight. Basic system framework The basic means of analysis is to produce a lattice of all possible mappings between input and valency frame case slots (see Fig. 1 ). Each edge is then scored through evaluation of the relative satisfaction by the input case filler of the selectional constraints for the target case slot. These preliminary mappings are subsequently verified for surface case marker compatibility, the full set of 1-to-l mappings from input to target case slots is extracted for each verb sense (including null mappings where permissible), and each candidate mapping is scored accordingly. Case slot restrictiveness All non-fixed case slots are encoded with a set of selectional constraints indexed to the Goi-Taikei thesaurus (Ikehara et al. 1997 ) and/or a list of lexical fillers. The Goi-Taikei thesaurus is set up in a conventional tree structure, of non-uniform depth and incorporating lexical items at all levels (average depth from the root \u2248 8.26). So as to establish a measure of restrictiveness of each subtree (node) in the thesaurus structure, the naive notion of case slot restrictiveness is introduced, which draws on the intuition that, in cases of selectional overlap between verb senses such as between the \"read\" and \"look at\" senses of miru with sinbun-o miru \"newspaper-ACC read/look at\", 2 the more highly specialised (restrictive) sense of \"read\" is inherently preferred in the absence of other distinguishing information. The degree of case slot restrictiveness (CSR) of a given node x in the thesaurus is estimated as the inverse of the mean subtree depth of all leaf nodes l 1 .. n subsumed by x, with the tree_depth function defined as the number of nodes between the subtree root x and leaf l i , inclusive. Hence, CSR for a leaf is one. while CSR for a case slot of unrestricted selectional scope (i.e. with selectional constraint set to the root node) is . Lexical fillers (e.g. fixed arguments) are treated as leaf nodes, and hence have CSR of one. We are now in a position to posit a metric for the degree of satisfaction of selectional constraint (SS) of target case slot t with selectional constraint c, by sense f s of input case filler f. In the formulation presented in (2), is the subsumption operator (a b\u21d2 'a subsumes b'), sub(a,b} returns the least common hypernym node subsuming both a and b, and rdepth(a) is a statement of the inclusive path length from the thesaurus root to a. SS essentially allows for semantic back-tracking up the thesaurus structure until c has been relaxed sufficiently to subsume f s . As we ascend the thesaurus structure, the degree of SS is diminished through a deflated CSR value, as well as the rdepth ratio of the relaxed selectional constraint to the original selectional constraint being accentuated. 3 Clearly in the case that c subsumes f s , the rdepth ratio becomes 1, and the CSR remains unchanged. On occurrence of semantic backtracking, SS is additionally weighted by way of the arg_weight of target case slot t, in line with the observations on semantic demarkation presented above. This acts so as to uphold adjunct constraints (arg_weight = 0) and penalise the relaxation of middle constraints (arg_weight = 0.5), but permit the relaxation of complement constraints without additional penalisation (arg_weight = 1). Note that integral complements are never represented by way of selectional constraints. Due to the potential for multiple senses of the input case filler, and multiple sets of selectional constraints/candidate fillers for the target case slot, we score each edge in the alignment lattice as the maximum value of SS (i.e. SS max ) for the given combination of filler f, with senses f 1 .. m , and target case slot t, with selectional constraints c 1 .. n : Table 3 : A fragment of the case marker alternation matrix Note that in the current formulation, we give consideration only to verb sense disambiguation and choose not to independently disambiguate case fillers. Clearly, there is scope to augment this unidirectional verb-driven approach and potentially improve the precision of VSD through intra-case slot local and intra-clausal topical disambiguation techniques (Yarowsky 1994; Ng & Lee 1996; Leacock et al. 1998) , as well as through consideration of domain (Wilks & Stevenson 1998) , which are left as topics for future research. Case marker alternation Surface case alternates derive from the canonical case marker type(s), and argument status of the target case slot. In the current implementation, this is represented simply as a matrix of possible case marker alternations for each canonical case marker, for a given argument type. This matrix has been developed based around the relative degrees of freedom of case marker alternation outlined in Table 1 , in that whereas a complement-type nominative case marker (ga) can commonly alternate with any of a range of case markers including the topic (wa), and genitive (no) markers, an integral complement type nominative case marker can generally alternate only with the genitive marker. A fragment of the connection matrix which documents such potential case marker alternations is provided in Table 3 . From the case marker alternation matrix, we define the binary function calt for input case marker i CM and the canonical case marker set T CM for target case slot t, such that calt(i CM ,T CM ) is 1 if connect(i CM ,j) = 1 for some case marker j \u2208 T CM , and 0 otherwise. Scoring individual case slot alignments The score align for each case slot alignment (i,t), incorporating input case slot i (filler i f and case marker i CM ) and target case slot t (selectional constraint set t c and case marker set T CM ), is determined via the product of SS max and calt. Penalising non-alignment Equally important as scoring case slot alignments is the enforcement of penalties on unaligned case slots. In this, we treat input and target case slots distinctly. Potential adjuncthood of unaligned input case slots When mapping case slots onto the valency frame, we are invariably left with a residue of adjunct case slots. We thus devised a classification of nuclear adjunct types (tempo-ral=TEMP, locative=LOC and adverbial=ADV 4 ) and, under the assumption that all adjuncts of same basic type coincide in semantic demarkation, we developed a fixed set of lexical and semantic filters for each. These filters and the associated set of canonical case markers for each adjunct type, are employed as meta-case slots to match nonaligning input case slots against, so as to distinguish between 'dangling' input case slots not aligning at any level of processing, and adjunct case slots which are simply not described within the valency frame proper. The scoring of adjunct case slot alignment is performed identically to that for other case slots, with each adjunct type being described by a pre-defined region within the Goi-Taikei thesaurus. The final score for the potential adjuncthood PA of a given input case slot i is given as the maximum score for the three given adjunct types. Non-alignment of target case slots Non-alignment of target case slots is penalised by way of the mean CSR measure for the case slot in question. In this, however, we have to be careful not to over-penalise unaligned adjunct and middle case slots, which we would expect to be readily omissible. This is achieved by multiplying the mean CSR value for each unaligned target case slot t with the arg_weight value corresponding to the argument status of t, and taking the maximum such value as the combined penalty for non-alignment of target case slots (PEN targ ). Our motivation for taking the maximum here is that we want to identify that unaligned target case slot which is most characteristic (i.e. highly constrained) of the given verb sense, while respecting the potential for that case slot to be genuinely uninstantiated in the case of middles and adjuncts (hence the arg_weight factoring), and avoiding over-penalisation of valency frames with higher numbers of case slots. Scoring and ranking valency frame mappings The scores and penalties detailed above are added to return a single combine score for each mapping M. The various mapping candidates are then ranked in descending order according to their respective scores. Evaluation We evaluated our system on all verbs found in the Goi-Taikei valency dictionary which derive from the verb miru \"to see\". By this is meant that, in addition to all dictionary entries for the base verb miru. all verbs containing the kanji prefix in their stem were considered. Examples of verbs included in this set are mieru \"can see/to be visible\" and minaosu \"to reconsider/re-evaluate\". The motivation for this seemingly arbitrary choice of verbs is the high degree of lexical ambiguity that exists between them, in the form of full and partial verb homophony (Baldwin 1998) . The dictionary composition is as follows: Total number verb senses in dictionary: 148 Number distinct verb stem types: 54 Average entries per verb stem: 2.72 The 148 verb senses were used in a verb sense evaluation task on a set of 289 simplex clauses extracted from the EDR corpus (EDR 1995), with each extracted clause having a main verb lexically matching one or more verbs in the dictionary. Unfortunately, the verb sense indices used in the EDR corpus do not align well with those designated in the Goi-Taikei valency dictionary, such that all 289 clauses had to be manually annotated for both verb sense and case slot alignment. Because the Goi-Taikei verb senses are linked to unique English verb translations, appropriateness of the English translation was used as the sole criterion in sense annotation. Hence, verb sense annotation was construed as an interlingual lexical selection task. The labour overhead involved in this annotation severely restricted the size of the test corpus, and results given below should be interpreted in light of the limited scope of the evaluation task. The reader is additionally cautioned that, in this evaluation task, all inputs were both automatically clustered into case-marked phrase units (i.e. input case slots) and segmented (for automatic head determination purposes) according to the original EDR mark-up. Total number clauses in corpus: 289 Total number fixed sense clauses: 23 Coverage of distinct verb senses: 58 Average number case slots per clause: 1.46 In terms of determining the semantic head of each phrase for calculation of SS, we identified the maximum segment-preserving suffix of the overall case filler which matches with a thesaurus entry; for unknown words, SS was set to 0.1. In evaluation, solutions were ranked based upon the overall score for that mapping, with solutions of equivalent score down-ranked. An output was adjudged to be correct if and only if it coincided both in verb sense and case slot alignment, with those annotated for the input in question. Evaluation of the test corpus in the manner described above, with PA and PEN targ variously activated and deactivated (\u00b1PA and \u00b1TARG, respectively), produced the results given in Fig. 2 . Note that the arg_weight schema given in Table 2 was applied consistently for all given system configurations. BEST-n in Fig. 2 indicates the percentage of clauses for which the correct solution was found in the top n ranked outputs. As a point of reference, the native ALT-J/E parser was run over the same corpus and sense-level accuracy (ignoring correctness of case slot alignment) calculated on the same BEST-n scale. The ALT-J/E verb sense scoring mechanism can be likened to a coarse-grained version of our CSR, weighted according to the part-morphological, part-semantic \"case-role\" of the target case slot (Bond & Shirai 1997) . Unfortunately, we were unable to get ALT-J/E to work on the pre-parsed input format required by our system, such that inputs had to be given as unformatted raw text. This tainted performance in that any internal errors in morphological and syntactic analysis led to noise during verb sense scoring. Having said this, the restricted length of the simplex clause inputs is thought to have minimised the scope for parsing errors. The correct solution (over both verb sense and case slot alignment) was located at a mean rank of 1.26 out of an average of 6.84 outputs, with the correct verb sense (irrespective of case slot alignment) identified at a mean rank of 1.22. The results show that we are able to identify the correct case slot alignment for the correct verb sense in over 83% of cases, in the instance that both PA and PEN targ are activated. Further, in nearly 98% of cases, the correct alignment and verb sense were contained in the top three ranking system solutions. Close inspection of the relative accuracy with component scoring mechanisms deactivated, reveals that the use of PEN targ avails a performance gain around 10% for n \u2264 5. Similarly, PA produces significant gains, particularly when employed in tandem with PENtarg. Despite the overall high accuracy rates attained, there were two clauses for which the correct analysis could not be produced due to lexical obligatoriness constraints on target case slots not being met. This represents an over-constraint on the part of the Goi-Taikei dictionary, and is not perceived as a threat to the integrity of our system. Looking again to Fig. 2 , we notice that our system outperformed the ALT-J/E parser despite the more stringent criterion of both alignment and sense accuracy imposed on it (vs. simple sense accuracy for ALT-J/E). Indeed, the relative performance of the two systems is perhaps better reflected in the figures of 86.51% vs. 81.22%the simple sense BEST-1 accuracy for our system against that for ALT-J/E. As an additional measure of true performance gain, we additionally calculated the accuracy of a naive \"first-sense\" technique on the test corpus. Here, we simply return the most common/prominent verb sense lexically matching with the input, as defined within Goi-Taikei. This produces a baseline accuracy of 81.66%, underlining the significance of the 86.51% sense accuracy for our system. While these figures are promising, they do not directly verify the validity of our claim that argument status aids VSD. We thus additionally tested the system (+PA + TARG) with various arg_weight parameter settings, indicated in Fig. 3 as the triple (complement_weight, middle_weight, adjunct_weight) , with the inequality 1 \u2265 complement_weight \u2265 middle_weight \u2265 adjunct_weight \u2265 0 upheld in all cases. The most striking result in Fig. 3 is the degradation in performance for the parameter triple (0.0,0.0,0.0), that is where all case slots are treated as adjuncts. The principle reason for this fall-off in performance is the blocking of semantic backingoff for complements and middles. There is little separating the remaining parameter combinations, although the proposed (1.0,0.5,0.0) setting did marginally outperform other settings at all positions other than BEST-1. This suggests that we get better performance when we treat middles as an in-between category than when conflating them with either complements or adjuncts, and provisionally backs up Somers' claim as to a need for this extra category. Part of the reason for the minimal differentiation between the different middle_weight scores is that the particular verb types targeted in evaluation tended not to collocate with middle case slots, suggesting the need for further evaluation on 'middle-heavy' verbs. One effect not apparent above is the relation between the number of input case slots and accuracy in analysis. We thus further evaluated the influence the number of input case slots had on the average rank, with results given below. Interestingly enough, we get a 100% success rate for input case frames devoid of case slots, pointing to an ability to cope admirably with underspecification. There is then a drop in accuracy for a single case slot in the input, with gradually recovers with increasing numbers of input case slots. No. input No An additional item worthy of verification is the ability of the system to correctly discriminate between fixed and general sense. Analysis of the 23 clauses containing a verb of fixed sense revealed that the system returned the correct analysis with the highest rank in all cases, and that there were no instances of a fixed sense solution being returned for a general-sense verb. While this certainly bodes well, these results should perhaps be played down, as all clauses of fixed sense were very clearly so. The performance of the system on more borderline examples of fixed expressions thus remains to be determined. Discussion In this research, we distance ourselves from much of conventional VSD research in that we specifically set out to handle cases of underspecification and non-canonical input. This is particularly salient in the case of Japanese due to the high levels of zero anaphora and commonality of case marker alternation, as are not found in English. It is thus difficult to identify comparable results in the VSD literature, although Kurohashi & Nagao (1994) and Fujii (1998) cite average sense-level accuracies of 76.5% and 82.3%, respectively, the former on underspecified and the latter on fully instantiated inputs. In this respect, our performance levels would appear to improve on previous research. One other important distinction between our work and much VSD research is that we do not explicitly employ the case-role, case marking or grammatical relation of a case slot to weight it. While we do utilise argument status to weight occurrences of semantic backing-off and penalise non-alignment of target case slots, we do not differentiate between aligned case slots other than implicitly through the CSR score. In this, we distance ourselves from the heuristical, static formulation of ALT-J/E and also the observations of Yarowsky (1993) that certain syntactic relations derive more disambiguating evidence than others (e.g. objects are better disambiguators of verbs than subjects). We do claim, however, that CSR is able to dynamically capture the types of phenomena targeted in these handlings. To sum up, the verb sense disambiguation method presented here was devised around the Goi-Taikei valency dictionary, as a means of overcoming problems related to underspecification, case marker alternation, word order, and fixed expressions, primarily through the application of argument status-based weighting and analysis of the selectional constraints encoded within the original dictionary. Argument status was successfully applied in semantic backing-off techniques, penalisation of input and target case slot non-alignment, and prediction of case marker alternation. The system returned an accuracy of over 83% on a limited test set of 289 simplex clauses. Areas of further research include the need to expand evaluation of the system, possible integration with disambiguation of case fillers, expansion of the handling of fixed expressions, and consideration/handling of the effects of scrambling. Acknowledgements The authors would like to thank the NTT Machine Translation Research Group, and in particular Francis Bond, for providing access to their peerless system resources, without which this research would not have been possible. We are also indebted to Christoph Neumann (TITECH) for support with the ALT-J/E system, and Assoc. Prof. Noguchi (TITECH) and two anonymous reviewers for valuable comments on earlier versions of this paper.",
    "abstract": "This research aims to incorporate argument status-based modelling within an otherwise selectional constraint-based system of verb sense disambiguation, to capture effects such as underspecification, surface case alternation and semantic backingoff. The proposed implementation hinges around a description of the general behavioural characteristics of integral complements, complements, middles and adjuncts through a pre-determined weighting schema. On limited evaluation, the resultant system returned an accuracy of over 83%, and was further shown to significantly outperform baseline methods.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese",
        "English"
    ],
    "numcitedby": "7",
    "year": "1999",
    "month": "August 23-25",
    "title": "Argument status in {J}apanese verb sense disambiguation"
}