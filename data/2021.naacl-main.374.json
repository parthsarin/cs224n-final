{
    "article": "Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semisupervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly. Introduction Event scripts are a classic way of summarizing events, participants, and other relevant information as a way of analyzing complex situations (Schank and Abelson, 1977) . To learn these scripts we must be able to group similar-events together, learn common patterns/sequences of events, and learn to represent an event's arguments (Minsky, 1974) . While continuous embeddings can be learned for events and their arguments (Ferraro et al., 2017; Weber et al., 2018a) , the direct inclusion of more structured, discrete knowledge is helpful in learning event representations (Ferraro and Van Durme, 2016) . Obtaining fully accurate structured knowledge can be difficult, so when the external knowledge is neither sufficiently reliable nor present, a natural question arises: how can our models use the knowledge that is present? Generative probabilistic models provide a framework for doing so: external knowledge is a random variable, which can be observed or latent, and the data/observations are generated (explained) from it. Knowledge that is discrete, sequential, or both-  Figure 1 : An overview of event modeling, where the observed events (black text) are generated via a sequence of semi-observed random variables. In this case, the random variables are directed to take on the meaning of semantic frames that can be helpful to explain the events. Unobserved frames are in orange and observed frames are in blue. Some connections are more important than others, indicated by the weighted arrows. K A + K J W z X 6 7 h S q S J s V 7 H r u j V D c N m t 1 R 3 k 2 H i B E q z Q H B T f + 8 O Y p i G L N B V E q Z 6 D E + 1 l R G p O B Z s V + q l i C a E T M m I 9 Q y M S M u V l i 1 N n 6 M w o Q x T E 0 l S k 0 U L 9 P p G R U K l p 6 J v O k O i x + u 3 N x b + 8 X q q D m p f x K E k 1 i + h y U Z A K p G M 0 / x s N u W R U i 6 k h h E p u b k V 0 T C S h 2 q R T M C F 8 f Y r + J 2 3 X d q o 2 v q m U G p e such as for script learning-complicates the development of neural generative models. In this paper, we provide a successful approach for incorporating partially-observed, discrete, sequential external knowledge in a neural generative model. We specifically examine event sequence modeling augmented by semantic frames. Frames (Minsky, 1974, i.a.) are a semantic representation designed to capture the common and general knowledge we have about events, situations, and things. They have been effective in providing crucial information for modeling and understanding the meaning of events (Peng and Roth, 2016; Ferraro et al., 2017; Padia et al., 2018; Zhang et al., 2020) . Though we focus on semantic frames as our source of external knowledge, we argue this work is applicable to other similar types of knowledge. We examine the problem of modeling observed event tuples as a partially observed sequence of semantic frames. Consider the following three events, preceded by their corresponding bracketed frames, from Fig. 1: [EVENT] crash occurred at station. [IMPACT] train collided with train. [KILLING] killed passengers and injured. We can see that even without knowing the [KILLING] frame, the [EVENT] and [IMPACT] frames can help predict the word killed in the third event; the frames summarize the events and can be used as guidance for the latent variables to represent the data. On the other hand, words like crash, station and killed from the first and third events play a role in predicting [IMPACT] in the second event. Overall, to successfully represent events, beyond capturing the event to event connections, we propose to consider all the information from the frames to events and frames to frames. In this work, we study the effect of tying discrete, sequential latent variables to partially-observable, noisy (imperfect) semantic frames. Like Weber et al. (2018b) , our semi-supervised model is a bidirectional auto-encoder, with a structured collection of latent variables separating the encoder and decoder, and attention mechanisms on both the encoder and decoder. Rather than applying vector quantization, we adopt a Gumbel-Softmax (Jang et al., 2017) ancestral sampling method to easily switch between the observed frames and latent ones, where we inject the observed frame information on the Gumbel-Softmax parameters before sampling. Overall, our contributions are: \u2022 We demonstrate how to learn a VAE that contains sequential, discrete, and partiallyobserved latent variables. \u2022 We show that adding partially-observed, external, semantic frame knowledge to our structured, neural generative model leads to improvements over the current state-of-the-art on recent core event modeling tasks. Our approach leads to faster training convergence. \u2022 We show that our semi-supervised model, though developed as a generative model, can effectively predict the labels that it may not observe. Additionally, we find that our model outperforms a discriminatively trained model with full supervision. Related Work Our work builds on both event modeling and latent generative models. In this section, we outline relevant background and related work. Latent Generative Modeling Generative latent variable models learn a mapping from the low-dimensional hidden variables f to the observed data points x, where the hidden representation captures the high-level information to explain the data. Mathematically, the joint probability p(x, f ; \u03b8) factorizes as follows p(x, f ; \u03b8) = p(f )p(x|f ; \u03b8), (1) where \u03b8 represents the model parameters. Since in practice maximizing the log-likelihood is intractable, we approximate the posterior by defining q(f |x; \u03c6) and maximize the ELBO (Kingma and Welling, 2013) as a surrogate objective: L \u03b8,\u03c6 = E q(f |x;\u03c6) log p(x, f ; \u03b8) q(f |x; \u03c6) . (2) In this paper, we are interested in studying a specific case; the input x is a sequence of T tokens, we have M sequential discrete latent variables z = {z m } M m=1 , where each z m takes F discrete values. While there have been effective proposals for unsupervised optimization of \u03b8 and \u03c6, we focus on learning partially observed sequences of these variables. That is, we assume that in the training phase some values are observed while others are latent. We incorporate this partially observed, external knowledge to the \u03c6 parameters to guide the inference. The inferred latent variables later will be used to reconstruct to the tokens. Kingma et al. (2014) generalized VAEs to the semi-supervised setup, but they assume that the dataset can be split into observed and unobserved samples and they have defined separate loss functions for each case; in our work, we allow portions of a sequence to be latent. Teng et al. (2020) characterized the semi-supervised VAEs via sparse latent variables; see Mousavi et al. (2019) for an in-depth study of additional sparse models. Of the approaches that have been developed for handling discrete latent variables in a neural model (Vahdat et al., 2018a,b; Lorberbom et al., 2019, i.a.) , we use the Gumbel-Softmax reparametrization (Jang et al., 2017; Maddison et al., 2016) . This approximates a discrete draw with logits \u03c0 as softmax( \u03c0+g \u03c4 ), where g is a vector of Gumbel(0, 1) draws and \u03c4 is an annealing temperature that allows targeted behavior; its ease, customizability, and efficacy are big advantages. Event Modeling Sequential event modeling, as in this paper, can be viewed as a type of script or schema induction (Schank and Abelson, 1977) via language modeling techniques. Mooney and DeJong (1985) provided an early analysis of explanatory schema 4703 < l a t e x i t s h a 1 _ b a s e 6 4 = \" O A X I u d v J 7 t f 0 N U 6 8 I K H u i N r 2 L T 0 = \" > A A A B 8 X i c d V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B 0 5 J d W 2 x v R S 8 9 K l g r t k v J p t k 2 N J t d k q x Q l v 4 L L x 4 U 8 e q / 8 e a / M a 0 V V P T B w O O 9 G W b m h a n g 2 m D 8 7 h S W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 7 e t U 4 y R V m b J i J R N y H R T H D J 2 o Y b w W 5 S x U g c C t Y J x + c z v 3 P H l O a J v D K T l A U x G U o e c U q M l W 5 7 r W z I 8 q j v T f v l C n Y b D V y t 1 h B 2 a 9 j 3 / b o l + M S v N z z k u X i O C i x w 0 S + / 9 Q Y J z W I m D R V E 6 6 6 H U x P k R B l O B Z u W e p l m K a F j M m R d S y W J m Q 7 y + c V T d G S V A Y o S Z U s a N F e / T + Q k 1 n o S h 7 Y z J m a k f 3 s z 8 S + v m 5 m o H u R c p p l h k n 4 u i j K B T I J m 7 6 M B V 4 w a M b G E U M X t r Y i O i C L U 2 J B K N o S v T 9 H / 5 N p 3 v Z q L L 6 u V 5 t k i j i I c w C E c g w e n 0 I Q W X E A b K E i 4 h 0 d 4 c r T z 4 D w 7 L 5 + t B W c x s w 8 / 4 L x + A M U w k P 4 = < / l a t e x i t > f 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" d 8 T r u l W Q x 4 q q n M 6 M h I h c g G B M T h 0 = \" > A A A B 8 X i c d V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l L S 9 N b 0 U u P F e w H t q F s t p N 2 6 W Y T d j d C C f 0 X X j w o 4 t V / 4 8 1 / 4 6 a N o K I P B h 7 v z T A z z 4 s 4 U 9 q 2 P 6 z c x u b W 9 k 5 + t 7 C 3 f 3 B 4 V D w + 6 a k w l h S 6 N O S h H H h E A W c C u p p p D o N I A g k 8 D n 1 v f p 3 6 / X u Q i o X i V i 8 i c A M y F c x n l G g j 3 Y 3 a 8 R Q S f 1 x d j o s l u 2 z b j l N v 4 J S k M K T Z r D W d G q 5 k S g l l 6 I y L 7 6 N J S O M A h K a c K D W s 2 J F 2 E y I 1 o x y W h V G s I C J 0 T q Y w N F S Q A J S b r C 5 e 4 g u j T L A f S l N C 4 5 X 6 f S I h g V K L w D O d A d E z 9 d t L x b + 8 Y a x 9 x 0 2 Y i G I N g q 4 X + T H H O s T p + 3 j C J F D N F 4 Y Q K p m 5 F d M Z k Y R q E 1 L B h P D 1 K f 6 f 9 K r l S r 1 s 3 9 R K r a s s j j w 6 Q + f o E l V Q A 7 V Q G 3 V Q F 1 E k 0 A N 6 Q s + W s h 6 t F + t 1 3 Z q z s p l T 9 A P W 2 y f C / Z D 9 < / l a t e x i t > f 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" a i p U t q X O Y J J N U Z B D t i G m 3 Y k 5 J o M = \" > A A A B 6 n i c d V D J S g N B E K 2 J W 4 x b 1 K O X x i B 4 G n r G B J N b 0 I s X I a J Z I B l C T 6 c n a d K z 0 N 0 j h C G f 4 M W D I l 7 9 I m / + j Z 1 F U N E H B Y / 3 q q i q 5 y e C K 4 3 x h 5 V b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H L R W n k r I m j U U s O z 5 R T P C I N T X X g n U S y U j o C 9 b 2 x 5 c z v 3 3 P p O J x d K c n C f N C M o x 4 w C n R R r o N + t f 9 Y g n b t R o u l y s I 2 x X s u m 7 V E H z m V m s O c m w 8 R w m W a P S L 7 7 1 B T N O Q R Z o K o l T X w Y n 2 M i I 1 p 4 J N C 7 1 U s Y T Q M R m y r q E R C Z n y s v m p U 3 R i l A E K Y m k q 0 m i u f p / I S K j U J P R N Z 0 j 0 S P 3 2 Z u J f X j f V Q d X L e J S k m k V 0 s S h I B d I x m v 2 N B l w y q s X E E E I l N 7 c i O i K S U G 3 S K Z g Q v j 5 F / 5 O W a z s V G 9 + U S / W L Z R x 5 O I J j O A U H z q E O V 9 C A J l A Y w g M 8 w b M l r E f C R V U 3 h P W u a n N 9 Q n G o g Y o x A = \" > A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B i y U R R Y 9 F L x 4 r W F t I Q 9 h s J + 3 S 3 W z Y 3 Q g l 5 G d 4 8 a C I V 3 + N N / + N 2 z Y H b X 0 w 8 H h v h p l 5 U c q Z N q 7 7 7 V R W V t f W N 6 q b t a 3 t n d 2 9 + v 7 B o 5 a Z o t C h k k v V i 4 g G z h L o G G Y 4 9 F I F R E Q c u t H 4 d u p 3 n 0 B p J p M H M 0 k h E G S Y s J h R Y q z k Q 5 j H Y S 7 O v K I I 6 w 2 3 6 c 6 A l 4 l X k g Y q 0 Q 7 r X / 2 B p J m A x F B O t P Y 9 N z V B T p R h l E N R 6 2 c a U k L H Z A i + p Q k R o I N 8 d n K B T 6 w y w L F U t h K D Z + r v i Z w I r S c i s p 2 C m J F e 9 K b i f 5 6 f m f g 6 y F m S Z g Y S O l 8 U Z x w b i a f / 4 w F T Q A 2 f W E K o Y v Z W T E d E E W p s S j U b g r f 4 8 j J 5 P G 9 6 l 0 3 3 / q L R u i n j q K I j generating system to process narratives, and Pichotta and Mooney ( 2016 ) applied an LSTM-based model to predict the event arguments. Modi (2016) proposed a neural network model to predict randomly missing events, while Rudinger et al. (2015) showed how neural language modeling can be used for sequential event prediction. 2019 ), similar to our model structure, provided an analysis of hierarchical relaxed categorical sampling but for the unsupervised settings. Method Our focus in this paper is modeling sequential event structure. In this section, we describe our variational autoencoder model, and demonstrate how partially-observed external knowledge can be injected into the learning process. We provide an overview of our joint model in \u00a73.1 and Fig. 2 . Our model operates on sequences of events: it consists of an encoder ( \u00a73.2) that encodes the sequence of events as a new sequence of frames (higher-level, more abstract representations), and a decoder ( \u00a73.3) that learns how to reconstruct the original sequence of events from the representation provided by the encoder. During training ( \u00a73.4), the model can make use of partially-observed sequential knowledge to enrich the representations produced by the encoder. In \u00a73.5 we summarize the novel aspects of our model. Model Setup We define each document as a sequence of M events. In keeping with previous work on event representation, each event is represented as a lexicalized 4-tuple: the core event predicate (verb), two main arguments (subject and object), and event modifier (if applicable) (Pichotta and Mooney, 2016; Weber et al., 2018b) . For simplicity, we can write each document as a sequence of T words w = {w t } T t=1 , where T = 4M and each w t is from a vocabulary of size V . 1 Fig. 1 gives an example of 3 events: during learning (but not testing) our model would have access to some, but not all, frames to lightly guide training (in this case, the first two frames but not the third). While lexically rich, this 4-tuple representation is limited in the knowledge that can be directly encoded. Therefore, our model assumes that each document w can be explained jointly with a collection of M random variables f m : f = {f m } M m=1 . The joint probability for our model factorizes as p(w, f ) = T t=1 p(w t |f , w <t ) M m=1 p(f m |f m\u22121 ). (3) For event modeling, each f m represents a semantic frame. We assume there are F unique frames and let f m be a discrete variable indicating which frame, if any, was triggered by event m. 2  In the general case, f is completely unobserved, and so inference for this model requires marginalizing over f : when F 1, optimizing the likelihood is intractable. We follow amortized variational inference (Kingma and Welling, 2013) as an alternative approach and use an ancestral sampling technique to compute it. We define q(f |w) as the variational distribution over f , which can be thought of as stochastically encoding w as f . Our method is semi-supervised, so we follow Kingma et al. ( 2014 ), Chen et al. (2018) and Ye et al. ( 2020 ) and optimize a weighted variant of the evidence lower bound (ELBO), L = Reconstruction term E q(f |w) log p(w|f ) + KL term \u03b1 q E q(f |w) log p(f ) q(f |w) , + Supervised classification term \u03b1 c L c (q(f |w)), (4 ) where L c (q(f |w)) is a classification objective that encourages q to predict the frames that actually were observed, and \u03b1 q and \u03b1 c are empirically-set to give different weight to the KL vs. classification terms. We define L c in \u00a73.4. The reconstruc-tion term learns to generate the observed events w across all valid encodings f , while the KL term uses the prior p(f ) to regularize q. Optimizing Eq. ( 4 ) is in general intractable, so we sample S chains of variables f (1) , . . . , f (S) from q(f |w) and approximate Eq. (4) as L \u2248 1 S s log p(w|f (s) ) + \u03b1 q log p(f (s) ) q(f (s) |w) + \u03b1 c L c (q(f (s) |w)) . (5) As our model is designed to allow the injection of external knowledge I, we define the variational distribution as q(f |w; I). In our experiments, I m is a binary vector encoding which (if any) frame is observed for event m. 3 For example in Fig. 1 , we have I 1 = 1 and I 3 = 0. We define q(f |w; I) = M m=1 q(f m |f m\u22121 , I m , w). ( 6 ) We broadly refer to Eq. ( 6 ) as our encoder; we detail this in \u00a73.2. In \u00a73.3 we describe how we compute the reconstruction term, and in \u00a73.4 we provide our semi-supervised training procedure. Encoder The reconstruction term relies on the frame samples given by the encoder. As discussed above though, we must be able to draw chains of variables f , by iteratively sampling f m \u223c q(\u2022|f m\u22121 , w; I), in a way that allows the external knowledge I to guide, but not restrict, f . This is a deceptively difficult task, as the encoder must be able to take the external knowledge into account in a way that neither prevents nor harms back-propagation and learning. We solve this problem by learning to compute a good representation \u03b3 m for each event, and sampling the current frame f m from a Gumbel-Softmax distribution (Jang et al., 2017) parametrized by \u03b3 m . Alg. 1 gives a detailed description of our encoder. We first run our event sequence through a recurrent network (like an RNN or bi-LSTM); if w is T tokens long, this produces T hidden representations, each of size d h . Let this collection be H \u2208 R T \u00d7d h . Our encoder proceeds iteratively over each of the M events as follows: given the previous sampled frame f m\u22121 , the encoder first computes a weighted embedding e f m\u22121 of this previous frame (line 1). Algorithm 1 Encoder: The following algorithm shows how we compute the next frame f m given the previous frame f m\u22121 . We compute and return a hidden frame representation \u03b3 m , and f m via a continuous Gumbel-Softmax reparametrization. Input: previous frame f m\u22121 , f m\u22121 \u2208 R F current frame observation (I m ), encoder GRU hidden states H \u2208 R T \u00d7d h . Parameters: W in \u2208 R d h \u00d7de , W out \u2208 R F \u00d7d h , frames embeddings E F \u2208 R F \u00d7de Output: f m , \u03b3 m 1: e f m\u22121 = f m\u22121 E F 2: \u03b1 \u2190 Softmax(HW in e f m\u22121 ) Attn. Scores 3: c m \u2190 H \u03b1 Context Vector 4: \u03b3 m \u2190 W out ( tanh(W in e f m\u22121 ) + tanh(c m )) 5: \u03b3 m \u2190 \u03b3 m + \u03b3 m I m Observation 6: q(f m |f m\u22121 ) \u2190 GumbelSoftmax(\u03b3 m ) 7: f m \u223c q(f m |f m\u22121 ) f m \u2208 R F Next, it calculates the similarity between e f m\u22121 and RNN hidden representations and all recurrent hidden states H (line 2). After deriving the attention scores, the weighted average of hidden states (c m ) summarizes the role of tokens in influencing the frame f m for the m th event (line 3). We then combine the previous frame embedding e f m\u22121 and the current context vector c m to obtain a representation \u03b3 m for the m th event (line 4). While \u03b3 m may be an appropriate representation if no external knowledge is provided, our encoder needs to be able to inject any provided external knowledge I m . Our model defines a chain of variables-some of which may be observed and some of which may be latent-so care must be taken to preserve the gradient flow within the network. We note that an initial strategy of solely using I m instead of f m (whenever I m is provided) is not sufficient to ensure gradient flow. Instead, we incorporate the observed information given by I m by adding this information to the output of the encoder logits before drawing f m samples (line 5). This remedy motivates the encoder to softly increase the importance of the observed frames during the training. Finally, we draw f m from the Gumbel-Softmax distribution (line 7). For example, in Fig. 1 , when the model knows that [IMPACT] is triggered, it increases the value of [IMPACT] in \u03b3 m to encourage [IMPACT] to be sampled, but it does not prevent other frames from being sampled. On the other hand, when a frame is not observed in training, such as for the third event ([KILLING]), \u03b3 m is not adjusted. Since each draw f m from a Gumbel-Softmax is a simplex vector, given learnable frame embeddings E F , we can obtain an aggregate frame representation e m by calculating e m = f m E F . This can be thought of as roughly extracting row m from E F for low entropy draws f m , and using many frames in the representation for high entropy draws. Via the temperature hyperparameter, the Gumbel-Softmax allows us to control the entropy. Decoder Our decoder (Alg. 2) must be able to reconstruct the input event token sequence from the frame representations f = (f 1 , . . . , f M ) computed by the encoder. In contrast to the encoder, the decoder is relatively simple: we use an auto-regressive (leftto-right) GRU to produce hidden representations z t for each token we need to reconstruct, but we enrich that representation via an attention mechanism over f . Specifically, we use both f and the same learned frame embeddings E F from the encoder to compute inferred, contextualized frame embeddings as E M = f E F \u2208 R M \u00d7de . For each output token (time step t), we align the decoder GRU hidden state h t with the rows of E M (line 1). After calculating the scores for each frame embedding, we obtain the output context vector c t (line 2), which is used in conjunction with the hidden state of the decoder z t to generate the w t token (line 5). In Fig. 1 , the collection of all the three frames and the tokens from the first event will be used to predict the Train token from the second event. Training Process We now analyze the different terms in Eq. ( 5 ). In our experiments, we have set the number of samples S to be 1. From Eq. ( 6 ), and using the sampled sequence of frames f 1 , f 2 , . . . f M from our encoder, we approximate the reconstruction term as L w = t log p(w t |f 1 , f 2 , . . . f M ; z t ), where z t is the decoder GRU's hidden representation after having reconstructed the previous t \u2212 1 words in the event sequence. Looking at the KL-term, we define the prior frame-to-frame distribution as p(f m |f m\u22121 ) = 1/F , and let the variational distribution capture the dependency between the frames. A similar type of strategy has been exploited successfully by Chen et al. (2018) to make computation simpler. We see the computational benefits Algorithm 2 Decoder: To (re)generate each token in the event sequence, we compute an attention \u03b1 over the sequence of frame random variables f (from Alg. 1). This attention weights each frame's contribution to generating the current word. Input: E M \u2208 R M \u00d7de (computed as f E F ) decoder's current hidden state z t \u2208 R d h Parameters: \u0174in \u2208 R de\u00d7d h , \u0174out \u2208 R V \u00d7de , E F Output: w t 1: \u03b1 \u2190 Softmax(E M \u0174in z t ) Attn. Scores 2: c t \u2190 E M \u03b1 Context Vector 3: g \u2190 \u0174out ( tanh( \u0174in z t ) + tanh(c t )) 4: p(w t |f ; z t ) \u221d exp(g) 5: w t \u223c p(w t |f ; z t ) of a uniform prior: splitting the KL term into E q(f |w) log p(f ) \u2212 E q(f |w) log q(f |w) allows us to neglect the first term. For the second term, we normalize the Gumbel-Softmax logits \u03b3 m , i.e., \u03b3 m = Softmax(\u03b3 m ), and compute L q = \u2212E q(f |w) log q(f |w) \u2248 \u2212 m \u03b3 m log \u03b3 m . L q encourages the entropy of the variational distribution to be high which makes it hard for the encoder to distinguish the true frames from the wrong ones. We add a fixed and constant regularization coefficient \u03b1 q to decrease the effect of this term (Bowman et al., 2015) . We define the classification loss as L c = \u2212 Im>0 I m log \u03b3 m , to encourage q to be good at predicting any frames that were actually observed. We weight L c by a fixed coefficient \u03b1 c . Summing these losses together, we arrive at our objective function: L = L w + \u03b1 q L q + \u03b1 c L c . (7) Relation to prior event modeling A number of efforts have leveraged frame induction for event modeling (Cheung et al., 2013; Chambers, 2013; Ferraro and Van Durme, 2016; Kallmeyer et al., 2018; Ribeiro et al., 2019) . These methods are restricted to explicit connections between events and their corresponding frames; they do not capture all the possible connections between the observed events and frames. Weber et al. (2018b) proposed a hierarchical unsupervised attention structure (HAQAE) that corrects for this. HAQAE uses vector quantization (Van Den Oord et al., 2017) to capture sequential event structure via tree-based latent variables. Our model is related to HAQAE (Weber et al., 2018b) , though with important differences. While HAQAE relies on unsupervised deterministic inference, we aim to incorporate the frame information in a softer, guided fashion. The core differences are: our model supports partially-observed frame sequences (i.e., semi-supervised learning); the linearchain connection among the event variables in our model is simpler than the tree-based structure in HAQAE; while both works use attention mechanisms in the encoder and decoder, our attention mechanism is based on addition rather than concatenation; and our handling of latent discrete variables is based on the Gumbel-Softmax reparametrization, rather than vector quantization. We discuss these differences further in Appendix C.1. Experimental Results We test the performance of our model on a portion of the Concretely Annotated Wikipedia dataset (Ferraro et al., 2014) , which is a dump of English Wikipedia that has been annotated with the outputs of more than a dozen NLP analytics; we use this as it has readily-available FrameNet annotations provided via SemaFor (Das et al., 2014) . Our training data has 457k documents, our validation set has 16k documents, and our test set has 21k documents. More than 99% of the frames are concentrated in the first 500 most common frames, so we set F = 500. Nearly 15% of the events did not have any frame, many of which were due to auxiliary/modal verb structures; as a result, we did not include them. For all the experiments, the vocabulary size (V ) is set as 40k and the number of events (M ) is 5; this is to maintain comparability with HAQAE. For the documents that had more than 5 events, we extracted the first 5 events that had frames. For both the validation and test datasets, we have set I m = 0 for all the events; frames are only observed during training. Documents are fed to the model as a sequence of events with verb, subj, object and modifier elements. The events are separated with a special separating <TUP> token and the missing elements are represented with a special NONE token. In order to facilitate semi-supervised training and examine the impact of frame knowledge, we introduce a userset value : in each document, for event m, the true value of the frame is preserved in I m with probability , while with probability 1 \u2212 we set I m = 0. This is set and fixed prior to each experiment. For all the experiments we set \u03b1 q and \u03b1 c as 0.1, found empirically on the validation data. Setup We represent words by their pretrained Glove 300 embeddings and used gradient clipping at 5.0 to prevent exploding gradients. We use a two layer of bi-directional GRU for the encoder, and a two layer uni-directional GRU for the decoder (with a hidden dimension of 512 for both). See Appendix B for additional computational details. 4   Baselines In our experiments, we compare our proposed methods against the following methods: \u2022 RNNLM: We report the performance of a sequence to sequence language model with the same structure used in our own model. A Bidirectional GRU cell with two layers, hidden dimension of 512, gradient clipping at 5 and Glove 300 embeddings to represent words. \u2022 RNNLM+ROLE (Pichotta and Mooney, 2016) : This model has the same structure as RNNLM, but the role for each token (verb, subject, object, modifier) as a learnable embedding vector is concatenated to the token embeddings and then it is fed to the model. The embedding dimension for roles is 300. \u2022 HAQAE (Weber et al., 2018b) This work is the most similar to ours. For fairness, we seed HAQAE with the same dimension GRUs and pretrained embeddings. 4 https://github.com/mmrezaee/SSDVAE Evaluations To measure the effectiveness of our proposed model for event representation, we first report the perplexity and Inverse Narrative Cloze metrics. Perplexity We summarize our per-word perplexity results in Table 1a , which compares our event chain model, with varying values, to the three baselines. 5 Recall that refers to the (average) percent of frames observed during training. During evaluation no frames are observed; this ensures a fair comparison to our baselines. As clearly seen, our model outperforms other baselines across both the validation and test datasets. We find that increasing the observation probability consistently yields performance improvement. For any value of we outperform RNNLM and RNNLM+ROLE. HAQAE outperforms our model for \u2264 0.5, while we outperform HAQAE for \u2208 {0.7, 0.8, 0.9}. This suggests that while the tree-based latent structure can be helpful when external, semantic knowledge is not sufficiently available during training, a simpler linear structure can be successfully guided by that knowledge when it is available. Finally, recall that the external frame annotations are automatically provided, without human curation: this suggests that our model does not require perfectly, curated annotations. These observations support the hypothesis that frame observations, in conjunction with latent variables, provide a benefit to event modeling. Inverse Narrative Cloze This task has been proposed by Weber et al. (2018b) to evaluate the ability of models to classify the legitimate sequence of events over detractor sequences. For this task, we have created two Wiki-based datasets from our validation and test datasets, each with 2k samples. Each sample has 6 options in which the first events are the same and only one of the options represents the actual sequence of events. All the options have a fixed size of 6 events and the one that has the lowest perplexity is selected as the correct one. We also consider two NYT inverse narrative cloze datasets that are publicly available. 6 All the models are trained on the Wiki dataset and then classifications are done on the NYT dataset (no NYT training data was publicly available). Table 1b presents the results for this task. Our method tends to achieve a superior classification score over all the baselines, even for small . Our model also yields performance improvements on the NYT validation and test datasets. We observe that the inverse narrative cloze scores for the NYT datasets is almost independent from the . We suspect this due to the different domains between training (Wikipedia) and testing (newswire). Note that while our model's perplexity improved monotonically as increased, we do not see monotonic changes, with respect to , for this task. By examining computed quantities from our model, we observed both that a high resulted in very low entropy attention and that frames very often attended to the verb of the event-it learned this association despite never being explicitly directly to. While this is a benefit to localized next word prediction (i.e., perplexity), it is detrimental to inverse narrative cloze. On the other hand, lower resulted in slightly higher attention entropy, suggesting that less peaky attention allows the model to capture more of the entire event sequence and improve global coherence. 6 https://git.io/Jkm46 Qualitative Analysis of Attention To illustrate the effectiveness of our proposed attention mechanism, in Table 2 we show the most likely frames given tokens (\u03b2 enc ) , and tokens given frames (\u03b2 dec ). We define \u03b2 enc = W out tanh(H ) and \u03b2 dec = \u0174out tanh(E M ) where \u03b2 enc \u2208 R F \u00d7T provides a contextual token-to-frame soft clustering matrix for each document and analogously \u03b2 dec \u2208 R V \u00d7M provides a frame-to-word softclustering contextualized in part based on the inferred frames. We argue that these clusters are useful for analyzing and interpreting the model and its predictions. Our experiments demonstrate that the frames in the encoder (Table 2 , top) mostly attend to the verbs and similarly the decoder utilizes expected and reasonable frames to predict the next verb. Note that we have not restricted the frames and tokens connection: the attention mechanism makes the ultimate decision for these connections. We note that these clusters are a result of our attention mechanisms. Recall that in both the encoder and decoder algorithms, after computing the context vectors, we use the addition of two tanh(\u2022) functions with the goal of separating the GRU hidden states and frame embeddings (line 3). This is a different computation from the bi-linear attention mechanism (Luong et al., 2015) that applies the tanh(\u2022) function over concatenation. Our additive approach was inspired by the neural topic modeling method from Dieng et al. ( 2016 ), which similarly uses additive factors to learn an expressive and predictive neural component and the classic \"topics\" (distributions/clusters over words) that traditional topic models excel at finding. While theoretical guarantees are beyond our scope, qualitative analyses suggests that our additive attention lets the model learn reasonable soft clusters of tokens into frame-based \"topics.\" See Table 6 in the Appendix for an empirical comparison and validation of our use of addition rather than concatenation in the attention mechanisms. Table 3 : Accuracy and macro precision and F1-score, averaged across three different runs. We present standard deviations in Table 5 . How Discriminative Is A Latent Node? Though we develop a generative model, we want to make sure the latent nodes are capable of leveraging the frame information in the decoder. We examine this assessing the ability of one single latent node to classify the frame for an event. We repurpose the Wikipedia language modeling dataset into a new training data set with 1,643,656 samples, validation with 57,261 samples and test with 75903 samples. We used 500 frame labels. Each sample is a single event. We fixed the number of latent nodes to be one. We use RNNLM and RNNLM+ROLE as baselines, adding a linear classifier layer followed by the softplus function on top of the bidirectional GRU last hidden vectors and a dropout of 0.15 on the logits. We trained all the models with the aforementioned training dataset, and tuned the hyper parameters on the validation dataset. We trained the RNNLM and RNNLM+ROLE baselines in a purely supervised way, whereas our model mixed supervised (discriminative) and unsupervised (generative) training. The baselines observed all of the frame labels in the training set; our model only observed frame values in training with probability , which it predicted from \u03b3 . The parameters leading to the highest accuracy were chosen to evaluate the classification on the test dataset. The results for this task are summarized in Table 3 . Our method is an attention based model which captures all the dependencies in each event to construct the latent representation, but the baselines are autoregressive models. Our encoder acts like a discriminative classifier to predict the frames, where they will later be used in the decoder to construct the events. We expect the model performance to be comparable to RNNLM and RNNLM+ROLE in terms of classification when is high. Our model with larger tends to achieve better performance in terms of macro precision and macro F1-score. Epochs are displayed with a square root transform. Training Speed Our experiments show that our proposed approach converges faster than the existing HAQAE model. For fairness, we have used the same data-loader, batch size as 100, learning rate as 10 \u22123 and Adam optimizer. In Fig. 3 , on average each iteration takes 0.2951 seconds for HAQAE and 0.2958 seconds for our model. From Fig. 3 we can see that for sufficiently high values of our model is converging both better, in terms of negative log-likelihood (NLL), and faster-though for small , our model still converges much faster. The reasons for this can be boiled down to utilizing Gumbel-Softmax rather than VQ-VAE, and also injection information in the form of frames jointly. Conclusion We showed how to learn a semi-supervised VAE with partially observed, sequential, discrete latent variables. We used Gumbel-Softmax and a modified attention to learn a highly effective event language model (low perplexity), predictor of how an initial event may progress (improved inverse narrative cloze), and a task-based classifier (outperforming fully supervised systems). We believe that future work could extend our method by incorporating other sources or types of knowledge (such as entity or \"commonsense\" knowledge), and by using other forms of a prior distribution, such as \"plug-and-play\" priors (Guo et al., 2019; Mohammadi et al., 2021; Laumont et al., 2021) . A Table of Notation B Computing Infrastructure We used the Adam optimizer with initial learning rate 10 \u22123 and early stopping (lack of validation performance improvement for 10 iterations). We represent events by their pretrained Glove 300 embeddings and utilized gradient clipping at 5.0 to prevent exploding gradients. The Gumbel-Softmax temperature is fixed to \u03c4 = 0.5. We have not used dropout or batch norm on any layer. We have used two layers of Bi-directional GRU cells with a hidden dimension of 512 for the encoder module and Unidirectional GRU with the same configuration for the decoder. Each model was trained using a single GPU (a TITAN RTX RTX 2080 TI, or a Quadro 8000), though we note that neither our models nor the baselines required the full memory of any GPU (e.g., our models used roughly 6GB of GPU memory for a batch of 100 documents). C Additional Insights into Novelty We have previously mentioned how our work is most similar to HAQAE (Weber et al., 2018b) . In this section, we provide a brief overview of HAQAE (Appendix C.1) and then highlight differences (Fig. 4 ), with examples (Appendix C.2). C.1 Overview of HAQAE HAQAE provides an unsupervised tree structure based on the vector quantization variational autoencoder (VQVAE) over M latent variables. Each latent variable z i is defined in the embedding space denoted as e. The varaitional distribution to approximate z = {z 1 , z 2 , . . . , z M } M i=1 given tokens x is defined as follows: q(z|x) = q 0 (z 0 |x) M \u22121 i=1 q i (z i |parent_of(z i ), x). The encoder calculates the attention over the input RNN hidden states h x and the parent of z i to define q i (z i = k|z i\u22121 , x): 1 k = argmin j g i (x, parent_of(z i )) \u2212 e ij 2 0 elsewise, where g i (x, parent_of(z i )) computes bilinear attention between h x and parent_of(z i )). In this setting, the variational distribution is deterministic; right after deriving the latent query vector it will be compared with a lookup embedding table and the row with minimum distance is selected, see Fig. 4a . The decoder reconstructs the tokens as p(x i |z) that calculates the attention over latent variables and the RNN hidden states. A reconstruction loss L R j and a \"commit\" loss (Weber et al., 2018b) loss L C j force g j (x, parent_of(z i )) to be close to the embedding referred to by q i (z i ). Both L R j and L C j terms rely on a deterministic mapping that is based on a nearest neighbor computation that makes it difficult to inject guiding information to the latent variable. C.2 Frame Vector Norm Like HAQAE, we use embeddings E F instead of directly using the Gumbel-Softmax frame sam-  ples. In our model definition, each frame f m = [f m,1 , f m,2 , . . . , f m,F ] sampled from the Gumbel- Softmax distribution is a simplex vector: 0 \u2264 f m,i \u2264 1, F i=1 f m,i = 1. (8) So f m p = ( F i=1 f p m,i ) 1/p \u2264 F 1/p . After sampling a frame simplex vector, we multiply it to the frame embeddings matrix E F . With an appropriate temperature for Gumbel-Softmax, the simplex would be approximately a one-hot vector and the multiplication maps the simplex vector to the embeddings space without any limitation on the norm. D More Results In this section, we provide additional quantitative and qualitative results, to supplement what was presented in \u00a74. D.1 Standard Deviation for Frame Prediction ( \u00a74.3) In Table 5 , we see the average results of classification metrics with their corresponding standard deviations. D.2 Importance of Using Addition rather than Concatenation in Attention To demonstrate the effectiveness of our proposed attention mechanism, we compare the addition against the concatenation (regular bilinear attention) method. We report the results on the Wikipedia dataset in Table 6 . Experiments indicate that the regular bilinear attention structure with larger obtains worse performance. These results confirm the claim that the proposed approach benefits from the addition structure. D.3 Generated Sentences Recall that the reconstruction loss is L w \u2248 1 S s log p(w|f (s) 1 , f (s) 2 , . . . f (s) M ; z), where f (s) m \u223c q(f m |f (s) m\u22121 , I m , w). Based on these formulations, we provide some examples of generated scripts. Given the seed, the model first predicts the first frame f 1 , then it predicts the next verb v 1 and similarly samples the tokens one-by-one. During the event generations, if the sampled token is unknown, the decoder samples again. As we see in Table 7 , the generated events and frames samples are consistent which shows the ability of model in event representation. D.4 Inferred Frames Using Alg. 1, we can see the frames sampled during the training and validation. In Table 8 , we provide some examples of frame inferring for both training and validation examples. We observe that for training examples when > 0.5, almost all the observed frames and inferred frames are the same. In other words, the model prediction is almost the same as the ground truth. Interestingly, the model is more flexible in sampling the latent frames (orange ones). In Table 7 : Generated scripts and the inferred frames (in brackets) from the seed event in boldface ( = 0.7) ITY_START and finally in Table 8d we have TAK-ING_SIDES rather than SUPPORTING. Some wrong predictions like CATASTROPHE instead of CAUSATION in Table 8c can be considered as the effect of other words like \"pressure\" in \"resulted pressure revolution in\". In some cases like Table 8b the predicted frame BEING_NAMED is a better choice than the ground truth APPOINTING. In the same vein FINISH_COMPETITION is a better option rather than GETTING in Table 8f . D.5 Clustering Here we provide more examples for \u03b2 enc and \u03b2 dec in Table 9 . Our experiments show that by sorting the tokens in each frame, the first 20 words are mostly verbs. And among different token types, verbs are better classifiers for frames. 9 : Results for the outputs of the attention layer, the upper table shows the \u03b2 enc and the bottom table shows the \u03b2 dec , when = 0.7. Each row shows the top 5 words for each clustering. Acknowledgements and Funding Disclosure We would also like to thank the anonymous reviewers for their comments, questions, and sug- gestions. Some experiments were conducted on the UMBC HPCF, supported by the National Science Foundation under Grant No. CNS-1920079. We'd also like to thank the reviewers for their comments and suggestions. This material is based in part upon work supported by the National Science Foundation under Grant Nos. IIS-1940931 and IIS-2024878. This material is also based on research that is in part supported by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003. The U.S.Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either express or implied, of the Air Force Research Laboratory (AFRL), DARPA, or the U.S. Government.",
    "abstract": "Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semisupervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "7",
    "year": "2021",
    "month": "June",
    "title": "Event Representation with Sequential, Semi-Supervised Discrete Variables"
}