{
    "article": "Readability aims to assess the difficulty of texts based on various linguistic predictors (the lexicon used, the complexity of sentences, the coherence of the text, etc.). It is an active field that has applications in a large number of NLP domains, among which machine translation, text simplification, text summarisation, or CALL (Computer-Assisted Language Learning). For CALL, readability tools could be used to help the retrieval of educational materials or to make CALL platforms more adaptive. However, developing a readability formula is a costly process that requires a large amount of texts annotated in terms of difficulty. The current mainstream method to gather such a large corpus of annotated texts is to get them from educational resources such as textbooks or simplified readers. In this paper, we describe the collection process of an annotated corpus of French as a foreign language texts with the purpose of training a readability model. We follow the mainstream approach, getting the texts from textbooks, but we are concerned with the limitations of such \"annotation\" approach, in particular, as regards the homogeneity of the difficulty annotations across textbook series. Their reliability is assessed using both a qualitative and a quantitative analysis. It appears that, for some educational levels, the hypothesis of the annotation homogeneity must be rejected. Various reasons for such findings are discussed and the paper concludes with recommandations for future similar attempts. Introduction Today, the market for foreign language learning is actively growing as a result of various factors, such as the E.U. enlargement and the increase in the number of languages represented in the Union, but also a greater mobility of its citizens. Faced with this increased interest in foreign language learning, teaching institutions are struggling to keep up with demand. In this context, the domains of CALL (Computer-Assisted Language Learning) and iCALL (Intelligent CALL) have a role to play (Nerbonne, 2003, 673) . Various CALL and iCALL applications have been designed to enhance classroom practices or replace it, but they still lack some flexibility as regards the input and the feedback offered to the user (Klenner and Visser, 2003) . For instance, some adaptive programs are able to select, in an exercise database, an item tailored to the learner's level (Desmet, 2006) . However, it requires the pre-annotation of all the items in terms of difficulty, which restricts the versatility of the user module. Being able to generate suitable exercises on the fly from a corpus appears as a better way to adapt to specific learner difficulties. The automatic generation of exercises has already been researched, mostly for English (Coniam, 1997; Brown et al., 2005; Smith et al., 2009; Chen et al., 2006; Heilman, 2011; Meurers et al., 2010) , but also for French (Antoniadis et al., 2005; Selva, 2002) . However, the majority of these systems either use excerpts whose difficulty has been manually annotated or excerpts extracted from a large corpus and thus lacking any difficulty annotations. In the first case, the system is able to adapt to the user's needs only within the limits of the available materials. In the second case, any type of exercise can be generated on the fly, but because there is no control of the difficulty of excerpts, the contextual complexity is likely to hinder the user's comprehension and his/her ability to perform the exercise. Faced with this challenge, one solution is to use readability metrics in order to pre-select a subset of excerpts matching the user's proficiency level, as it is done in the L\u00e4rka platform (Pil\u00e1n et al., 2013) . Readability is a field that aims to assess the difficulty of texts in a reproducible way -which can therefore be automatized -based on various linguistic dimensions of the texts (e.g. lexicon, syntax, text structure, etc.). The first studies in the field date back to the 1920's (Lively and Pressey, 1923) and have traditionally been carried out by psychologists. However, readability has undergone recent developments. They result from the contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005) , but also on other languages such as Swedish (Pil\u00e1n et al., 2014) , French (Fran\u00e7ois and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; Fran\u00e7ois et al., 2014) , or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce. In our view, this can be explained by the high cost needed to create a readability model, especially in terms of the corpus collection process. Moreover, a convenient readability model should be able to output predictions that are useful for users. In Europe, this means to be able to assess text complexity in terms of the Common European Framework of Reference for Languages (CEFR) (Council of Europe, 2001) . This scale has now become the reference for foreign language education within Europe. To our knowledge, only two research teams have currently designed a readability model compliant with the CEFR scale (Fran\u00e7ois and Fairon, 2012; Pil\u00e1n et al., 2014) . We suspect that this is partly due to the efforts needed to collect the training corpus required to develop such readability formula. In this paper, we detail the collection process of a readability-intended corpus that has been carried out for French as a foreign language (FFL), using FFL textbooks as a source for the labelled texts. We describe the various issues encountered during this collection, focusing mostly on the issue of the reliability of the difficulty annotations. In section 2, we first expose the various type of criteria that have been used in readability studies to get data annotated in terms of difficulty and we discuss the advantages and shortcomings of each of them. The section 3 then details our collection process and describes the resulting corpus. Finally, Section 4 investigates the quality of the collected data, using both a qualitative analysis and a quantitative analysis based on statistical tests to assess the homogeneity of the annotations across textbooks. Criteria for readability annotation This section discusses various techniques that have been used to measure the difficulty of texts for reading. This issue is influenced by the fashion we define the term \"difficulty\", which is an elusive concept corresponding to a multifaceted reality. A large corpus of studies in psycholinguistics have addressed this question (for a survey, see (Ferrand, 2007) ), but there is currently no integrated model that precisely explains what causes reading difficulty. However, the pragmatic vision underlying readability studies cannot be satisfied with a fuzzy definition. It is therefore common in the field to use a single variable, easily measured and based on theoretical arguments from psycholinguistics, as an estimation of the reading difficulty of texts. This variable is called \"criterion\" and various ones have been used for readability purposes. We briefly discuss each of them and explain why collecting texts from textbooks is currently considered as one of the best criteria. The first criterion used in readability was expert judgements. It dates back as early as the seminal work by (Lively and Pressey, 1923) and consists in gathering a small set of expertssupposed to share a good vision of the reading difficulties encountered by the population of interest -and ask them to judge the difficulty of a set of texts. Using a scale such as educational grades to label the texts, the experts need to project themselves into the mind of potential readers whose characteristics they know from their professional activity. However, the quality of this projection is variable. Gilbert de Landsheere (1978) had six texts annotated by twenty experts and noticed a high variation between their judgements. More recently, van Oosten et al. (2011) addressed this question with modern statistical techniques: 105 excerpts were assessed by pair (e.g. text A is more difficult than text B) by a group of experts. The experts were then grouped according to the similarity of their annotations via a clustering algorithm. Each expert group corresponded to a set of texts, which was divided into a training and a test corpus. Then, for each training corpus available, binary classification models were trained and their performance was assessed both on the test corpus from the same cluster (intra-cluster validation) and on test corpora from other clusters (inter-cluster validation). Interestingly, the performance of all models significantly deteriorates in the inter-cluster condition, leading the authors to question the possibility of reaching a satisfactory agreement between experts. Carver (1974) and Singer (1975) adopted the reverse view, considering that the human annotation of text difficulty can be reliable under some conditions. Their method, called levelling, involves defining a small subset of passages, each of them being typical of a level. Then, three experts compare the same text with this yardstick and the final label of the text corresponds to the average of the three judgements. Carver (1974) carried out two experiments using this technique and showed that it is slightly more valid than classic formulas such as (Dale and Chall, 1948) or (Flesch, 1948) . Later, (Bj\u00f6rnsson, 1983, 482) reached a similar conclusion: Traditionally it has been thought that judges' ratings of absolute difficulty are unreliable. From our experience they are not, i.e., when they are made by a fairly large group of persons, when the passages are relatively long, and when the range in difficulty in the text battery is wide. Beyond this crucial and still opened question of the validity of experts' judgements, this criterion presents another shortcoming, namely the availability and cost of experts that limits the amount of data that one can collect. The second criterion to be used in readability is comprehension tests. Faced with the questionable validity of experts' judgements, Dale and Tyler (1934) and Ojemann (1934) investigated another approach: testing the reading comprehension of subjects directly with tests. The difficulty level of a text therefore corresponds to the mean score obtained by all the subjects that took the test. This approach has the benefit of directly measuring the comprehension, taking into account the interaction existing between the text and the reader. This criterion appeared for some time as the best criterion for readability, even though it was more costly than expert judgements. However, a major shortcoming was soon stressed: the interaction existing between the difficulty of the text and the difficulty of the questions. Davis (1950 , cited by de Landsheere (1978, 33) ) confirmed this issue with the following experiment: he designed two versions of a test on the same text, manipulating only the frequency of the words used in the questions, and noticed a significant difference in the scores of the subjects between both conditions. Further issues with comprehension tests also arose: the order of the questions matters and comprehension tests are not able to focus on all parts of the texts. In spite of these problems, comprehension tests were largely used as a readability criterion between 1930 and 1960. They were gradually abandoned to the advantage of a third criterion: the cloze test. Introduced by Taylor (1953) , this test simply consists in deleting a word out of five in a text before asking subjects to fill those gaps. The amount of filled blanks is supposed to be correlated with the subject understanding of the text. Since there is no need to formulate questions, the main flaw of comprehension tests (the interaction between the questions and the text) is removed. Moreover, with such a simple design process, it is possible for two researchers to produce exactly the same test for a text. As a result of these advantages, the cloze test was quickly adopted by researchers in readability (Miller and Coleman, 1967; Aquino et al., 1969; Bormuth, 1969; Caylor et al., 1973; Kincaid et al., 1975) . Bormuth (1969) also highlighted another advantage of this criterion: its ability to measure the difficulty of smaller units than a text, such as a sentence or even a word. The main issue with cloze test is to determine what exactly is measured. Bormuth (1969, 365) believes that cloze tests \"measure skills closely related or identical to those measured by conventional multiple-choice reading comprehension tests\". Taylor (1957) compared the outputs of cloze tests and multiple-choice question (MCQ) tests and he obtained correlations between 0.51 and 0.92. Similarly, Jenkinson (1957 , cited by Jongsma (1969) ) compared cloze test scores with results at standardized reading tests and she got a 0.78 correlation with the section of this standard test that measures lexical knowledge and she got a correlation of 0.73 with the section measuring comprehension. However, Weaver and Kingston (1963) stand up for the opposite view, arguing that it is textual redundancy which is rather measured. They obtained weak correlations between the Davis Reading Test and cloze test. Another critic addressed to the cloze test is that it is hardly necessary to use clues located beyond the local context of the current sentence to correctly fill one gap. Miller and Coleman (1967) investigated this issue with a protocol in which subjects had to guess 150 consecutive words from excerpts. It appears that the answers produced were not much constrained by the previous sentences. Shanahan et al. (1982) confirmed that sentential information is paramount to correctly perform a cloze task. This obviously appears as a major weakness of this criterion, especially for more advanced readers for whom reading problems are more global than local. Other criteria also have been investigated, but only by a limited number of researchers. Recall, or more precisely the number of words memorized, was used by Richaudeau (1974) . However, this criterion was criticized by Kintsch et al. (1975) , since it does not match any psychological reality. Another criterion explored is reading time. Brown (1952) compared the time spent on two texts by subjects, the former being considered as difficult and the latter as very difficult. On the former, the average reading speed was 306 words/min. while it only reached 235 words/min. for the latter. This association between reading comprehension and reading speed has been later experimentally corroborated by Oller (1972) and supported by the theoretical model by (Just and Carpenter, 1980) . Despite these favourable studies, reading speed has been very little used in readability. One of the problems is the necessity to ensure that the subjects read naturally, while the experimental cost is also an issue. In view of all these considerations, there is no criterion that stands out as the most valid and practical. This fact led current approaches of readability to use a criterion convenient enough to collect the large amount of texts required by the NLP and IA techniques. This criterion consists in collecting texts from textbooks or simplified readers, provided that these books are labelled accordingly to an educational scale. Such approach relies on the assumption that the calibration of those texts have been carried out by experts, which amounts to use experts' judgements. This way of colecting labelled data has been widely used in readability. Most of the famous classic formulas (Lorge, 1944; Dale and Chall, 1948; Flesch, 1948; Gunning, 1952) have been trained on the McCall and Crabbs lessons. Spache (1953) trained, on a corpus of primary textbooks, a formula intended for primary schoolchildren that has been acknowledged as one of the most reliable for this specific population. However, it is with the advent of what Fran\u00e7ois and Fairon (2012) call the \"IA readability\" that this criterion has somehow becomed the standard approach. This is also due to the fact that Si and Callan (2001) suggested to view text readability assessment as a classification task. It implies to assign training texts to a few number of classes, which may quite logically corresponds to educational levels. Most of the recent readability formulas have adopted this approach (Schwarm and Ostendorf, 2005; Feng et al., 2009; Fran\u00e7ois, 2009; Tanaka-Ishii et al., 2010; Vajjala and Meurers, 2012; Pil\u00e1n et al., 2014) , but, to our knowledge, none of them have systematically addressed the issue of their corpus homogeneity. Although textbooks are indeed written by experts and may even benefit from updates based on teachers' feedback, the criteria used to select texts are likely to differ from one author to another as well as from one textbook series to another. This is why we decided to investigate this problematic using a corpus of FFL textbooks, which is described in the next section. A textbook corpus for French as a foreign language The collect With the intent of later training a readability formula, we have collected a corpus of texts from FFL textbooks. This choice was motivated by the following three requirements: (1) as said above, the size of our corpus must be large enough to allow the training of modern machine learning algorithms; (2) the difficulty labels used for annotation must be convenient for the end users of the readability model, and (3) the content and the genre of the texts should be as diverse as possible to ensure a better generability of the model. Therefore, extracting texts from FFL textbooks compliant with the the Common European Framework of Reference for Language (CEFR) appeared to be a good solution to these three constraints. Released in 2001 by the Council of Europe, the CEFR \"provides a common basis for the elaboration of language syllabuses, curriculum guidelines, examinations, textbooks, etc. across Europe\" (Council of Europe, 2001, 1) . The document has achieved a wide success in Europe, being translated into at least 20 European languages (Little, 2006) and being implemented in most of the institutions providing L2 education. One of the flagship features of the CEFR is its competency scale that has been described according to two dimensions: vertical and horizontal. The vertical dimension is the best known and describes six levels: A1 (Breakthrough); A2 (Waystage); B1 (Threshold); B2 (Vantage); C1 (Effective Operational Proficiency) and C2 (Mastery). This scale has been calibrated with a mixed methodology that combines experts knowledge with data from qualitative and quantitative studies on learners (Council of Europe, 2001, 150) . As a result, the CEFR scale appears quite reliable and the large majority of textbooks posterior to 2001 bear a CEFR level. Unfortunately, the Council of Europe has not developed a system validating the adequacy between the self-declared level of textbooks and their actual content (Alderson, 2007) . This lack of control is prone to generate some heterogeneity between textbooks series. To investigate this potential issue, we collected 2,042 texts from 28 textbooks. Not all textbooks available on the market were selected, because they had to meet the three following criteria: (1) to be published after 2001 in order to bear a CEFR level, (2) to be intended for adults or teenagers learning FFL for general purposes and (3) not to be tailored for a public with a specific L1 background. These two last considerations were implied by the type of population that we wanted to model for our readability model: young adults and adults with varied L1 backgrounds. Furthermore, all extracted texts had to be related to a reading comprehension task. Each of the 2,042 collected texts was scanned and automatically transformed into a machine-readable format (XML) using an optical character recognition tool. We then manually revised and corrected the scanned texts, removed peripheral information such as instructions, images, tables, etc. and assigned to each text the level of the textbook it came from. We met an unexpected difficulty during this last operation. Some textbooks cover more than one CEFR level and have a mixed tag (e.g. A1/A2). In this case, we had to analyse each textbook introductory comments, organisation and structure to gather enough information to distribute each text in one of the two levels 1 . The corpus collected at the end of the process is summarised at Corpus characteristics In this section, we further discuss some characteristics of our corpus, namely (1) the metadata used; (2) the distribution of text genres in the corpus, and (3) the distribution of texts per level. As for the metadata, the tags were kept very simple since most of the contextual features of texts (such as instructions, images, figures, etc.) had been removed. We defined the six following tags: Level: take one value among the six levels the CEFR scale (A1, A2, B1, B2, C1, and C2); Lesson: the textbook lesson in which the text is studied. It was normalized as follows: Lesson localization index = number of the lesson total number of lessons in the textbook This lesson localization index appeared propitious in case we would like to transform the CEFR ordinal scale into a continuous scale. Source: the textbook name from which the text was extracted; Page: the page(s) of the textbook from which the text comes; Date: the publication date of the textbook; Type of text: the genre of the text (see below for details), Title: the title of the text. Among those tags, the only one that required some manual classification was the genre of the texts. The following genres were distinguished: text (either narrative or informative), collection of disconnected sentences (mostly in A1 and A2 textbooks), dialogue (including interviews), mail, e-mail, advertisement (e.g. reproduction of leaflet), poem and recipe. As these types of texts can be quite easily identified thanks to stereotypical clues, the classification was performed by two humans annotators on the basis of simple guidelines. The distribution of texts and words across genres and levels is displayed in Table 2 . For exposition purposes, we merged the rare genres (ads, songs, poems, and recipes) within the Varias category. It should also be mentioned that although the corpus does not seem very balanced across text genres and levels at first glance, we believe that these figures are pretty representative of the distribution of texts within the population of FFL textbooks. Table 2 : Number of texts and words per level and genre. The distribution of texts per level at Table 2 is clearly unbalanced: A1 includes almost ten times more texts than C2. This situation is due to the fact that at the later stages of learning, L2 learners are able to read almost any authentic texts and the need for carefully calibrated texts thus decreases. As a consequence, there are not many textbooks available for higher levels, especially for C2. The problem of having unbalanced classes is that \"classification is sensitive to the relative sizes of the (...) component groups and will always favour classification into the larger group\" (Hosmer and Lemeshow, 1989, 147) . In the next section, we will also further discuss this issue of unbalanced classes along with the main issue of this paper: the heterogeneity of the level annotations. Analyses of the corpus The previous section has related the collection process of our corpus and detailed some of its characteristics. It has also stressed two main issues regarding the corpus: (1) the possible heterogeneity of the difficulty annotations due to a lack of control in the adequacy between textbook contents with the CEFR scale and (2) the shortage of high level texts, which results into an unbalanced dataset likely to cause bias in any readability model trained on the corpus. In this section, we report analyses investigating both issues, starting with the latter. The class imbalanced experiment In order to determine whether having an unbalanced dataset would impact subsequent learning on that corpus, we applied the following methodology. We sampled two different datasets from the whole corpus. For the first (Corpus6Apriori), we simply applied a stratified sampling that respects the a priori probability of each class. This amounts to 66 texts for A1, 72 for A2, 99 for B1, 29 for B2, 27 for C1 and 7 for C2. For the second dataset (Corpus6Equi), we also applied a stratified sampling, but selected a fixed amount of texts in each class-about 50, which corresponds to the size of the least populated class (C2). Finally, we sampled 120 texts (20 per level) in the remaining texts 2 to be used as the test set. Concerning the readability model, since the aim was not to reach the highest performance possible, we selected two simple and broadly-used linguistic features as predictors: the mean number of letter per words (NLM) and the mean number of words per sentence (NWS). They were combined with a proportional-odds model, also known as ordinal logistic regression (Agresti, 2002, 274-282) . Their performance were assessed with the multiple correlation coefficient (R 2 ), estimated on the training set, the test set and using a bootstrap .632 procedure 3 . The results are detailed in Table 3 .  Surprisingly, the Corpus6Apriori model performs better in all of the three conditions (training, test and bootstrap). However, this apparent superiority must be qualified when we look more closely at the confusion matrix. Tables 4 and 5 show the confusion matrix for both models on the test set. It clearly appears that the high number of B1 texts in the Corpus6Apriori condition distorts the regression space (about 50% of the texts are predicted as B1). The model trained on Corpus6Equi presents a more balanced distribution that slightly favours the extreme classes (A1 and C2) 4 . Moreover, the Corpus6Apriori model is not able to classify any text as B2, which is a very critical flaw for a tool aiming to be used in real contexts by L2 learners or teachers. We conclude from this first experiment that a readability corpus should have, as much as possible, a balanced number of observations per class. Testing the homogeneity of the corpus Methodology and hypotheses For the reasons exposed in Section 3.1, the difficulty annotations in our corpus are likely to be more heterogeneous than expected. To investigate this issue, we applied the following methodology. First, we selected two readability indices whose relation with text difficulty has been confirmed by many studies in the literature: the mean number of letter per words (NLM) and the mean number of words per sentence (NWS). They are representative of the lexical and syntactic dimensions of the texts in our corpus, but we also wanted to have a semantic index, so we opted for the density of ideas in a text (ConcDens). The efficiency of this last feature is not as well-acknowledged as that of the two previous ones, but ConcDens has the advantage of taking into account textual dimensions that have been deemed critical for comprehension since the 1970's. However, parameterizing the density of ideas in a text is not as straightforward as counting the number of letters or the number of words. It underlies a more complex theoretical model, which also involves more complex NLP routines. Our measure of the density of ideas is based on Kintsch et al. (1975) 's propositional model 5 . These authors showed that the number of propositions and the number of different arguments in a sentence influence its reading time and therefore, most likely, its comprehension. To implement Kintsch's model, we used the recently published French tool Densid\u00e9es (Lee et al., 2010) . This program draws from previous attempts for English : Snowdon et al. (1996) showed that it is possible to estimate the propositional density of a text from the number of verbs, adjectives, adverbs, prepositions, and conjunctions divided by the number of words, while Brown et al. (2008) implemented this approach using 37 rules. Densid\u00e9es is based on a similar approach. It is able to estimate the mean number of propositions per word in a text using 35 rules making use of lexical and part-of-speech clues. In a second step, we computed, for each of the three above variables, their means on all texts belonging to a given textbook and classified within one given level 6 . Then, these means were compared using a twofold approach: (1) a qualitative analysis of the tables 6, 7, and 8 first helped to detect irregularities, (2) then quantitative analyses were performed to determine whether these irregularities were large enough to conclude to the corpus heterogeneity. More precisely, we aimed to test the three following hypothesis: 1. the means of each variable per level (computed on all the texts of this level), which is shown at the last row of each table, should increase with the level of difficulty. 2. if the annotations within a given level are homogeneous, the means of each textbook from this level will not be significantly different from all other means from that level. 3. within the same textbook series, the mean of a given level will be greater than the means of all textbooks at a lower level. The hypothesis (1) and (3) were investigated manually, while for the (2), the analyses were based on the analysis of variance (ANOVA), which takes in account each of the three predictors independently, and its multivariate variant (MANOVA), in which the effect of all three variables can be taken into account in a combined way. Qualitative analysis As regards the qualitative analysis of the three tables 6, 7, and 8, it first appears that the means by level indeed increase, as expected from the hypothesis (1). There are however a few exceptions: the lexical complexity of B2 textbooks is surprisingly lower than that of B1 textbooks, whereas ConcDens is not very efficient to distinguish between A2, B1 and B2 texts, as well as between C1 and C2. This could be due either to the fact that the content of textbooks series does not increase in terms of conceptual difficulty or to the fact that ConcDens is a less reliable predictor of difficulty than NLM and NWS. A manual skimming of sampled texts of various levels tends to let us discard the first explanation. To test the second one, we sampled 50 texts per level with a stratified sampling by textbooks and then computed Pearson correlations between each of our three features and these text annotations. It was obvious that meanNWS (r = 0, 62) and NLM (r = 0, 52) are better predictors than ConcDens (r = 0, 37). This last feature is interesting for it takes higher textual dimension into account. However, it does not seem reliable enough to undertake a critical analysis of our corpus annotations. This is why we will not discuss it any further in the rest of the paper.   conform to the average. Finally, the progression within series may also be also problematic. This is the case for two series: Comp. \u00e9crite and, especially, Rond Point. This observation can be explained by some characteristics of this last series: (1) it is intended for false beginners and therefore quickly progresses in the learning process; (2) the learning process is based on tasks and operates in spiral. The learner is thus quickly brought into contact with more complex forms, which are however not comprehensively studied. As a result, the texts encountered at the initial stages are more difficult than in other textbooks, but the lexical complexity later hardly increase, probably because this is the difficulty of the task to be performed by the learners that rather increases. To conclude, the qualitative analysis raised strong clues showing that the homogeneity of our corpus is questionable. The nearly \"flat\" profiles of Compr\u00e9hension \u00e9crite and Rond Point are particularly of concern. However, globally, most of the series respect the ascending profile requested by hypothesis (1) and presents a coherent progression within the same series i accordance with hypothesis (3). It should also be reminded that our predictors are not perfectly correlated with text difficulty and only approach it from a unique point of view although it is actually a very complex phenomenon. In the next section, we will further investigate hypothesis (2) with quantitative techniques in order to produce a more clear-cut diagnosis on our corpus homogeneity. Quantitative analysis The qualitative analysis has provided an accurate picture of the complexity of each textbook as described by lexical and syntactic predictors. As explained above, it is not easy to decide whether or not the corpus must be considered as heterogeneous on this basis alone. To investigate more systematically this issue and determine whether the divergences reported in previous section are significant, we applied ANOVA tests (Howell, 2008, 305-352) . ANOVA is a statistical test used to compare two or more means of a quantitative variable across conditions (here, the textbooks within a level). It compares the variation between textbooks and within each textbook. If this ratio reaches a sufficiently high value (depending on the significance level \u03b1, here 0.05), we must conclude that all texts from a level do not come from the same population, which means that they were not annotated by a coherent set of experts. Before the ANOVA analysis, we checked whether the distributions of meanNWS and NLM by textbooks are normally distributed and whether their distributions by level have an homoscedastic variance. These are the two main conditions required to apply ANOVA to a dataset. We respectively used the Shapiro-Wilk (Shapiro and Wilk, 1965) test to check the normality and the Levene test (Brown and Forsythe, 1974) for variance homoscedasticity. Normality was rejected by 27 out of 82 tests 7 , whereas only 4 levels out of the 12 presented an unequal variance. Since ANOVA can bear to see its conditions violated to a certain extent, we did not deem these results problematic enough to resort to using a non-parametric test such as Kruskal-Wallis. Results of the ANOVA analysis are reported in Table 9 . They clearly show that only a few levels appear to be homogeneously labelled: the texts in C2 for NLM and the texts from B2 to C2 for meanNWS. The divergences stressed in the qualitative analysis seem large enough to conclude to the heterogeneity of our corpus. However, it should be mentioned that the ANOVA test is an omnibus test, which means that it is enough that a single textbook deviates from the mean to reject the homogeneity hypothesis. As notified previously, textbook series characterized by specific pedagogical orientation are the most problematic and might be the main cause for rejecting the homogeneity hypothesis. We therefore performed the same ANOVA analysis without the two problematic series: Compr\u00e9hension \u00e9crite and Rond Point. Results of these new tests are also reported in Table 9 as Corpus6Cleaned and show some global improvements: B1 becomes homogeneous and B2 is very close to homogeneity, when we consider NLM. For meanNWS, the quality of annotations slightly improves for A1, but decreases for B2. In the whole, the situation remains problematic. The ANOVA tests the homogeneity through a unique predictor, whereas we noticed that some textbooks deviate from their level average for one predictor, but not for the other (e.g. Tout va bien ! A2). This limited point of view could have as a result to intensify the seemingly heterogeneity of the corpus. We therefore applied a multivariate version of the ANOVA, the MANOVA (Lewis-Beck, 1993, 340-368) . The results are however very similar to those of the ANOVA: the only homogeneous level is C2 (p = 0.69); B2 is already considered as heterogeneous, although only slightly (p = 0.02); the other four levels are clearly heterogeneous, with p-values lower than 0.001. This is a rather expected finding, as the MANOVA is even stricter than the ANOVA, requiring all textbook means for NLM AND meanNWS to be similar. Conclusion This paper focused on a very often overlooked issue in the modern readability literature based on complex machine learning algorithm and trained on texts from educational resources: the coherence of the annotations. Indeed, when one collects a large corpus of texts previously annotated -which means that he/she cannot control the annotation process -, it is very likely that the various experts involved in the educational material creation apply incoherent criteria. This issue was confirmed by the results of van Oosten et al. ( 2011 )'s experiment with real judges. Interestingly, when researchers in readability use real experts, they are more prone to question the reliability of their annotation, applying, for instance, standard inter-annotators agreement metrics. On the contrary, the quality of a corpus largely used in the field such as the Weekly Reader has been hardly questioned. Feng et al. (2010) computed the mean number of words per documents and per sentences and showed a clear progression as the levels increases. However, it is generally agreed in the community that the annotations are coherent, even though not much is known on the text calibrating criteria. Deeming that this question is crucial, we have investigated it, taking advantage of the fact that our corpus is based on textbooks. Each textbook is indeed designed by a well-identified team. It is therefore possible to consider each of them as a kind of \"cluster\" in the sense of van Oosten et al. (2011) . We therefore suggested an alternate methodology to assess the quality of the annotations in a textbook-based readability corpus. Further contributions of this paper are a discussion about the state-of-the-art of the available criteria for the annotation of text difficulty as well as the description of the collection process of texts from textbooks to the aim of training a readability model. Apart from the heterogeneity issue discussed above, we have stressed other issues that may prove interesting for future similar attempts: (1) the lack of control from the Council of Europe onto the textbook annotations, (2) the lack of texts for advanced levels (C1, and especially C2) that is unfortunate since most of the lower level texts collected could not be used. For future attempts, we suggest starting collecting C2 texts and, afterwards, gather an equivalent number of texts for the lower levels. Finally, we also identified that some types of pedagogical approaches -in our case, the task-oriented approach -are more prone to include heterogeneous materials than textbooks based on a more communicative approach. Future work regarding the collection and annotation of texts for readability could explore various paths. First, it would be interesting to compare another corpus for FFL, but including only texts intended to a public with a specific L1. This would allow to assess to which extent the L1 impacts the readability of texts for this population. Another interesting experiment would be to compare the textbook annotations with other criteria either classic ones such as those presented at Section 2, or more recent ones, such as eye-tracking or annotations by the crowd (van Oosten and Hoste, 2011). Such comparison would help to be more informed about the validity of the current practice of collecting texts in textbooks or readers. Acknowledgments This work has been partially funded by the Belgian Fund for Scientific Research (FNRS). We would like to thank C\u00e9drick Fairon, Jean-L\u00e9on Bouraoui and Laurent Hubert for their valuable comments on this work, as well as Bernadette Dehottay for her invaluable help in the collection process of the corpus. ",
    "abstract": "Readability aims to assess the difficulty of texts based on various linguistic predictors (the lexicon used, the complexity of sentences, the coherence of the text, etc.). It is an active field that has applications in a large number of NLP domains, among which machine translation, text simplification, text summarisation, or CALL (Computer-Assisted Language Learning). For CALL, readability tools could be used to help the retrieval of educational materials or to make CALL platforms more adaptive. However, developing a readability formula is a costly process that requires a large amount of texts annotated in terms of difficulty. The current mainstream method to gather such a large corpus of annotated texts is to get them from educational resources such as textbooks or simplified readers. In this paper, we describe the collection process of an annotated corpus of French as a foreign language texts with the purpose of training a readability model. We follow the mainstream approach, getting the texts from textbooks, but we are concerned with the limitations of such \"annotation\" approach, in particular, as regards the homogeneity of the difficulty annotations across textbook series. Their reliability is assessed using both a qualitative and a quantitative analysis. It appears that, for some educational levels, the hypothesis of the annotation homogeneity must be rejected. Various reasons for such findings are discussed and the paper concludes with recommandations for future similar attempts.",
    "countries": [
        "Belgium"
    ],
    "languages": [
        "French"
    ],
    "numcitedby": "22",
    "year": "2014",
    "month": "November",
    "title": "An analysis of a {F}rench as a Foreign Language Corpus for Readability Assessment"
}