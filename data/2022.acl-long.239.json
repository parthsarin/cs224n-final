{
    "article": "As language technologies become more ubiquitous, there are increasing efforts towards expanding the language diversity and coverage of natural language processing (NLP) systems. Arguably, the most important factor influencing the quality of modern NLP systems is data availability. In this work, we study the geographical representativeness of NLP datasets, aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers. In doing so, we use entity recognition and linking systems, presenting an approach for good-enough entity linking without entity recognition first. Last, we explore some geographical and economic factors that may explain the observed dataset distributions. 1 2 Datasets designed to capture dialectal variations, e.g., SD-QA (Faisal et al., 2021), are culturally-aware in terms of annotator selection, but there is no guarantee that their content is also culturally-relevant for the language speakers. 3 See \u00a72 of their paper. Introduction The lack of linguistic, typological, and geographical diversity in NLP research, authorship, and publications is by now widely acknowledged and documented (Caines, 2019; Ponti et al., 2019; Bender, 2011; Adelani et al., 2021) . Nevertheless, the advent of massively multilingual models presents opportunity and hope for the millions of speakers of under-represented languages that are currently under-served by language technologies. Broadening up the NLP community's research efforts and scaling from a handful up to the almost 7000 languages of the world is no easy feat. In order for this effort to be efficient and successful, the community needs some necessary foundations to build upon. In seminal work, Joshi et al. (2020) provide a clear overview of where we currently stand with respect to data availability for the world's languages and relate them to the languages' representation in NLP conferences. Choudhury and Deshpande (2021) study how linguistically fair are multilingual language models, and provide a nuanced framework for evaluating multilingual models based on the principles of fairness in economics and social choice theory. Last, Blasi et al. (2022) provide a framework for relating NLP systems' performance on benchmark datasets to their downstream utility for users at a global scale, which can provide insights into development priorities; they also discuss academic incentives and socioeconomic factors that correlate with the current status of systematic cross-lingual inequalities they observe in language technologies performance. These works provide insights into current data availability and estimated utility that are paramount for making progress, as well as an evaluation framework for future work. However, there is one missing building block necessary for real progress: a way to estimate how representative of the underlying language speakers is the content of our datasets. Any evaluation framework and any utility estimates we build can only be trustworthy as long as the evaluation data are representative. Gebru et al. (2021) and Bender and Friedman (2018) recognize the importance of this information, including them in their proposed guidelines for \"datasheets\" and \"data statements\" respectively; but most datasets unfortunately lack such meta-information. To the best of our knowledge, MaRVL (Liu et al., 2021) is the only dataset that is culturally-aware by design in terms of its content. 2  We propose a method to estimate a dataset's cultural representativeness by mapping it onto the physical space that language speakers occupy, producing visualizations such as Figure 1 . Our contributions are summarized below: \u2022 We present a method to map NLP datasets unto geographical areas (in our case, countries) and use it to evaluate how well the data represent the underlying users of the language. We perform an analysis of the socio-economic correlates of the dataset maps we create. We find that dataset representativeness largely correlates with economic measures (GDP), with geographical proximity and population being secondary. \u2022 We test a simple strategy for performing entity linking by-passing the need for named entity recognition. We evaluate its efficacy on 19 languages, showing that we can get within up to 85% of a NER-informed harder-to-obtain model. We also show that encouragingly, using either model largely leads to similar dataset maps. Mapping Datasets to Countries Assumptions This work makes two assumptions: that (a) data locality matters, i.e., speakers of a language are more likely to talk about or refer to local news, events, entities, etc as opposed to ones from a different side of the world, and (b) that we can capture this locality by only focusing on entities. Kumar et al. (2019) discuss these topical correlations that are present in datasets, 3 noting that they exist and that L1 language identification models tend to pick up on them, i.e. if a text mentions Finland, a L1 langid model is probably going to predict that the speaker is Finnish, because p(Finland L1 = Finnish) is generally high. In that work Kumar et al. (2019) make explicit effort to avoid learning such correlations because they are interested in building models for p(L1 text) (i.e. p(L1 = Finnish Finland)) that are not confounded by the reverse conditional. The mere fact they need to do this, though, confirms that realworld text has such topical confounds. As for our second assumption that we can capture these topical correlations by only looking at entities, one need only to take a look at Table 2 of Kumar et al. (2019) , which lists the top topical confounding words based on log-odds scores for each L1 language in their dataset: all lists include either entities related to a country where that language is spoken (e.g. 'Merkel', the name of a former chancellor, for German) or topical adjectives (e.g. 'romanian' for Romanian). Approach For a given dataset, our method follows a simple recipe: 1. Identify named entities present in the dataset. 2. Perform entity linking to wikidata IDs. 3. Use Wikidata to link entities to countries. We discuss each step below. Entity Recognition Step Standard entity linking is treated as the sequence of two main tasks: entity recognition and entity disambiguation. One approach is to first process the text to extract entities and then disambiguate these entities to the correct entries of a given knowledge base (eg. Wikipedia). This approach relies on NER model quality. However, to perform analysis on several datasets spanning several low-resource languages, one needs good-quality NER models in all these languages. The interested reader will find a discussion on the cross-lingual consistency of NER models in Appendix F. 4 As we show in Section \u00a74, we can bypass this NER step if we tolerate a small penalty in accuracy. Entity Linking Step In this step we map named entities to their respective Wikidata IDs. We further discuss this step in Section \u00a74. From Entities to Countries We produce maps to visualize the geographical coverage of the datasets we study, discussing their properties and our findings in Section \u00a73. To link entities to countries, 5 we rely on Wikidata entries, depending on the type of entity: \u2022 for persons, we log their places of birth (P19) and death (P20), and country of citizenship (P27); \u2022 for locations, we search for their associated country (P17); and \u2022 for organizations, we use the links of the 'lo-cated_at' (P276) and 'headquartered_at' (P159) relations. Since places of birth/death and headquarters are not necessarily at the country level, we perform a second step of associating these locations with countries. In cases where the result does not correspond to a modern-day country (as can often be the case with historical figures), we do not make any attempts to link it to any modern day countries, excluding them from the analysis. For example, the entry for Nicolaus Copernicus (Q619) lists him as born in Toru\u0144 (Q47554) which is then mapped to Poland; as having died in Frombork (Q497115) that also maps to Poland; and as a citizen of the Kingdom of Poland (Q1649871) which is not mapped to any modern-day country; so he is only linked to Poland. Albert Einstein is similarly mapped to both Germany and the United States, due to his places of birth (Ulm) and death (Princeton). Dataset-Country Maps Before delving into our case studies, we first list a set of statistics of interest that one could extract from our produced dataset-country maps, in order to gauge a dataset's representativeness. Representativeness Measures We will avoid providing a single metric, largely because the ideal metric to use will be very dataset-specific and related to the goals of the creators of the dataset and the socioeconomic correlates they are interested in (see discussion in Section \u00a73.3). As a first straightforward representativeness measure, we will compute the percentage of entities associated with countries where the language is largely spoken. For example, according to Ethnologue (Eberhard et al., 2021) , most Swahili speakers 6 reside in Tanzania, Kenya, Uganda, DR. Congo, and Rwanda. For a Swahili dataset, then, we compute the percentage of all entities associated with this set of countries (\"in-country\"). Notions of equity or fairness across countries could be measured by various fairness metrics, given the distribution of entities over countries in a dataset: from simply computing the standard deviation of the observations, 7 to treating countries as a population and computing fairness indices like the popular Gini index (Gini, 1912; Gastwirth, 1972) or the indices proposed by Speicher et al. (2018) . We will opt for a simpler, much more interpretable measure, the number of countries not represented in the dataset i.e. countries with associated entity count below a given threshold (we use zero for simplicity but higher values would also be reasonable for large datasets). Last, especially for languages with significant amounts of speakers in more than one country, it is important to go deeper and measure the representativeness of this in-country portion. For a simple example, an English dataset with entities only from the UK is probably not representative of Nigerian or Jamaican English speakers. Hence, we will create two distributions over the countries where the language is largely spoken: the distribution of speaker populations (as available from Ethnologue and other public data), and the distribution of entities observed in the dataset. Discrepancies between these two distributions will reveal potential issues. While one could easily compute some measure of distance between the two distributions (e.g. the Bhattacharyya coefficient (Bhattacharyya, 1943) ), in this work we will rely on the interpretable advantages of the visualizations. Measures of fairness could be computed for this portion of the dataset, similarly as discussed above. In the example dataset of the Swahili portion of MasakhaNER in Figure 1 , the utility of our method is apparent. Through the visualization, a researcher can quickly confirm that the dataset seems to not reflect the users of the language to a large extent: only about 17% of the entities indeed correspond to Tanzania, Kenya, Uganda, DR. Congo, or Rwanda (where Swahili and its varieties are treated as a lingua franca, at least in portions of these countries). Wealthy or populous countries like USA, France, and China, are well-represented, 8 as one would expect, while 156 countries and territories have no representation. At the same time, the visualization allows a researcher to identify gaps: Natural Questions MLQA SQuAD TyDi-QA (English) Figure 2 : Visualizing the datasets' geography allows easy comparisons of their representativeness (best viewed in color and zoomed-in). NQ is the most representative of English speakers, with in-country percentage (higher is better) of 80% (SQuAD: 63%; TyDi-QA: 57%; MLQA: 53%) and less countries left unrepresented (lower is better; NQ: 49; MLQA: 80; SQuAD: 93; TyDi-QA: 113). beyond the neighboring African countries and perhaps the Middle East, north-west African countries as well as central America or central/south-east Asia are clearly under-represented in this dataset. Between the main Swahili-speaking countries, Tanzania, Kenya, and Uganda are well-represented (DR Congo and Rwanda less so, but they have less Swahili speakers), with the former two perhaps slightly over-represented and the latter (as well as Rwanda) being under-represented relative to the speakers population, c.f. red (dataset entities) and green (proportional to population) bars in Figure 1 . Datasets and Settings We apply the process described above on several datasets, chosen mostly for their language and typological diversity. Our process is not dataset-or language-dependent, 9 and could easily be applied on any NL dataset. We briefly describe the datasets we include in our study below, with detailed statistics in Appendix C. NER Datasets We study the WikiANN dataset (Pan et al., 2017) that is commonly used in the evaluation of multilingual models. We additionally study the MasakhaNER dataset (Adelani et al., 2021) , which was created through participatory design (\u2200 et al., 2020) in order to focus on African languages. Since these datasets are already annotated with named entities, we only need to perform entity linking. Question Answering We study four question answering datasets (focusing on the questions rather than contexts), namely SQuAD (Rajpurkar et al., 2016) , MLQA (Lewis et al., 2020) , TyDi-QA (Clark et al., 2020) , and Natural Questions (Kwiatkowski et al., 2019, NQ; ) for the X-FACTR benchmark (Jiang et al., 2020) , and several machine translation benchmarks) are available in the project's webpage: https://nlp.cs.gmu.edu/ project/datasetmaps/. Discussion Beyond Figure 1 , we also show example maps in Figure 2 for NQ, MLQA, SQuAD, and the English portion of TyDi-QA. We provide additional maps for all other datasets in Appendix G. Comparing datasets The comparison of MasakhaNER to the WikiANN dataset (see Appendix G) reveals that the former is rather more localized (e.g. more than 80% of the identified entities in the Dholuo dataset are related to Kenya) while the latter includes a smaller portion from the countries where most native speakers reside (between 10%-20%) and almost always also includes several entries that are very European-or western-centric. The effect of the participatory design (\u2200 et al., 2020) approach on creating the MasakhaNER dataset, where data are curated from local sources, is clear in all language portions of the dataset, with data being highly representative of the speakers. In Figures 8-9 (App. G) the majority of entities in the Wolof portion are from Senegal and neighboring countries (as well as France, the former colonial power of the area), and the Yoruba and Igbo ones are centered on Nigeria. Figure 2 allows for a direct comparison of different QA datasets (also see maps for other TyDi-QA languages in Appendix G). The first notable point has to do with NQ, which was built based on real-world English-language queries to the Google search engine. Since such queries happen all over the world, this is reflected in the dataset, which includes entities from almost all countries in the world. Two types of countries are particularly represented: ones where English is an official language (USA, UK, Australia, but also, to a lesser extent, India, Nigeria, South Africa, and the Philippines); and wealthy ones (European, Japan, China, etc). In our view, NQ is an exemplar of a representative dataset, because it not only includes representation of most countries where the language is spoken (with the sum of these entities being in their large majority in-country: 80%) but due to its size it also includes entities from almost all countries. SQuAD also has a large percentage in-country (63%) but it is less representative of different Englishes than NQ. India, for instance, is relatively under-represented in all datasets; in SQuAD it ranks 7 th , but it ranks 3 rd in NQ (see red bars in bottom left of figures). On the other hand, the geographical representativeness of both MLQA and TyDi-QA (their English portion) is lacking. Since these datasets rely on Wikipedia articles for their creation, and Wikipedia has a significant westerncountry bias (Greenstein and Zhu, 2012; Hube and Fetahu, 2018) , most entities come from Europe, the US, and the Middle East. All these datasets underrepresent English speakers from English-speaking countries of the Global South like Kenya, South Africa, or Nigeria, since there are practically almost no entities from these countries. MLQA further under-represents the speakers of all other languages it includes beyond English, since all data are translations of the English one. Contrast this to TyDi-QA and its visualized Swahili portion which, even though still quite western-centric, does have a higher representation from countries where Swahili is spoken than the TyDi-QA English portion. This discussion brings forth the importance of being cautious with claims regarding systems' utility, when evaluated on these datasets. One could argue that a QA system that is evaluated on NQ does indeed give a good estimation of real-world utility; a system evaluated on TyDi-QA gives a distorted notion of utility (biased towards western-based speakers and against speakers from the Global 1 : Empirical comparison of factors on QA datasets, averaging over their respective languages (number in parentheses). We report the five-fold cross-validation explained variance and mean absolute error of a linear model. South); a system evaluated on MLQA will give an estimation as good as one evaluated on TyDi-QA, but only on the English portion. We clarify that this does not diminish the utility of the datasets themselves as tools for comparing models and making progress in NLP: MLQA is extremely useful for comparing models across languages on the exact same data, thus facilitating easy comparisons of the cross-lingual abilities of QA systems, without the need for approximations or additional statistical tests. But we argue that MLQA should not be used to asses the potential utility of QA systems for German or Telugu speakers. Similar observations can be made about comparing two similar projects that aim at testing the memorization abilities of large language models, namely X-FACTR and multi-LAMA (mLAMA; Kassner et al., 2021) -see corresponding Figures in Appendix G. Both of these build on top of Wikidata and the mTREx dataset. However, mLAMA translates English prompts and uses entity-relation triples mined from the English portion of Wikidata, unlike X-FACTR which uses different data for each language, mined from their respective portion of Wikidata. Both are still western-biased, since they rely on Wikipedia, but one (X-FACTR) is better at giving an indication of potential downstream utility to users. Socioeconomic Correlates In this section we attempt to explain our findings from the previous section, tying them to socioeconomic factors. Empirical Comparison of Factors We identify socioeconomic factors \u03c6 that could be used to explain the observed geographic distribution of the entities in the datasets we study. These are: \u2022 a country's population \u03c6 pop \u2022 a country's gross domestic product (GDP) \u03c6 gdp \u2022 a country's GDP per capita \u03c6 gdppc \u2022 a country's landmass \u03c6 land \u2022 a country's geographical distance from country/ies where the language is spoken \u03c6 geo The first four factors are global and fixed. The fifth one is relative to the language of the dataset we are currently studying. For example, when we focus on the Yoruba portion of the mTREx dataset, we use Nigeria (where Yoruba is spoken) as the focal point and compute distances to all other countries. The assumption here is that a Yoruba speaker is more likely to use or be interested in entities first from their home country (Nigeria), then from its neighboring countries (Cameroon, Chad, Niger, Benin) and less likely of distant countries (e.g. Argentina, Canada, or New Zealand). Hence, we assume the probability to be inversely correlated with the country's distance. For macro-languages or ones used extensively in more than one country, we use a population-weighted combination of the factors of all relevant countries. To measure the effect of such factors it is common to perform a correlational analysis, where one measures Spearman's rank correlation coefficient \u03c1 between the dataset's observed geographical distribution and the factors \u03c6 . It is important, though, that the factors are potentially covariate, particularly population and GDP. Hence, we instead compute the variance explained by a linear regression model with factors \u03c6 as input, i.e., a\u03c6 pop + b\u03c6 gdp + c\u03c6 gdppc + d\u03c6 geo + e with a-e learned parameters, trained to predict the log of observed entity count of a country. We report explained variance and mean absolute error from five-fold cross-validation experiments to avoid overfitting. Socioeconomic Correlates and Discussion The results with different combination of factors for the QA datasets are listed in Table 1 . 10 The best sin-gle predictor is, perhaps unsurprisingly, the GDP of the countries where the language is spoken: all datasets essentially over-represent wealthy countries (e.g. USA, China, or European ones). Note that GDP per capita is not as good a predictor, neither is landmass. A combination of geographical distance with GDP explains most of the variance we observe for all datasets, an observation that confirms the intuitions we discussed before based solely on the visualizations. Importantly, the fact that including population statistics into the model deteriorates its performance is further proof that our datasets are not representative of or proportional to the underlying populations. The only dataset that is indeed better explained by including population (and GDP per capita) is NQ, which we already argued presents an exemplar of representativeness due to its construction protocol. Limitations It is important to note that our assumptions are also limiting factors in our analyses. Mapping languages to countries is inherently lossy. It ignores, for instance, the millions of immigrants scattered throughout the world whose L1 language could be different than the dominant language(s) in the region where they reside. Another issue is that for many languages the necessary granularity level is certainly more fine than country; if a dataset does not include any entities related to the Basque country but does include a lot of entities from Spain and France, our analysis will incorrectly deem it representative, even though the dataset could have been a lot more culturally-relevant for Basque speakers by actually including Basque-related entities. Another limitation lies in the current state of the methods and data resources on which our approach relies. Beyond discrepancies in NER/EL across languages (addressing which is beyond the scope of this work), we suspect that Wikidata suffers from the same western-centric biases that Wikipedia is known for (Greenstein and Zhu, 2012) . As a result, we might be underestimating the cultural representativeness of datasets in low-resource languages. An additional hurdle, and why we avoid providing a single concrete representativeness score or something similar, is that the ideal combination of socioeconomic factors can be subjective. It could be argued, for instance, either that geographic proximity by itself should be enough, or that it should not matter at all. Even further, other factors that we did not consider (e.g. literacy rate or web access) might influence dataset construction decisions. In any case, we share the coefficients of the NQ model, since it is the most representative dataset we studied, at least for English: a = 0.1.46 (for \u03c6 pop ), b = 0.87 (\u03c6 gdp ), c = 25.4 (\u03c6 gdppc ), d = 0.41 (\u03c6 geo ). We believe that ideally GDP should not matter (b \u2192 0) and that a combination of speaker population and geographic proximity is ideal. 11 Geographical Breakdown of Models' Performance Beyond the analysis of the datasets themselves, we can also break down the performance of models by geographical regions, by associating test (or dev) set samples containing entities with the geographical location of said entities. Since most test sets are rather small (a few hundred to a couple thousand instances) we have to coarsen our analysis: we map each country to a broader region (Africa, Americas, Asia, Europe, Oceania), keeping historical entities in a separate category (History). 12  We perform such a case study on TyDi-QA, comparing the performance on the TyDi-QA development sets of two models: one trained monolingually on the training set of each language of TyDi-QA (gold task), and another model trained by Debnath et al. (2021) on English SQuAD and automatically generated translations in the target languages. Example results on Telugu shown in Figure 3 reveal some notable trends. 13 First, training set representation (green bars in the Figures) is not a necessary condition for good test set performance (red bars). Some test set instances (e.g. with historical and African entities) receive similar test F1 score from both models. Perhaps the most interesting though, is the comparison of the Asian and European portions of the test set: the Telugu monolingual model achieves similar performance in these two subsets; but the SQuAD-trained model is almost 20 percentage points worse on the Asian subset, showing the potential unfairness of translation-based models (Debnath et al., 2021) . For most TyDi-QA languages (Indonesian being an exception, see Table 2 ) the macro-standard deviation (computed over the averages of the 6 region subsets) is larger for the SQuAD-trained model (which is, hence, less fair than models trained on  Compare, for instance, the differences in performance between Asia and Europe of the two models. TyDi-QA). Bypassing NER for Entity Linking We use mGENRE (Cao et al., 2021) for the task of multilingual entity linking, a sequence to sequence system that predicts entities in an auto-regressive manner. It works particularly well in a zero-shot setting as it considers 100+ target languages as latent variables to marginalize over. Typically, the input to mGENRE can be informed by a NER model that provides the named entity span over the source. For instance, in the Italian sentence \"[START] Einstein [END] era un fisico tedesco.\" (Einstein was a German physicist.) the word Einstein is enclosed within the entity span. mGENRE is trained to use this information to return the most relevant Wikidata entries. Due to the plasticity of neural models and mGE-BRE's auto-regressive token generation fashion, we find that by simply enclosing the whole sentence in a span also yields meaningful results. In particular, for the previously discussed Italian sentence now the input to mGENRE is \"[START] Einstein era un fisico tedesco. [END]\". The advantage of this approach is two-fold. First, one does not need a NER component. Second, exactly because of bypassing the NER component, the EL model is now less constrained in its output; in cases where the NER component made errors, there's a higher chance that the EL model will re- turn the correct result. Experiments and Results We conduct experiments to quantify how different a model uninformed by a NER model (NER-Relaxed) will perform compared to one following the typical pipeline (NER-Informed). Given the outputs of the two models over the same set of sentences, we will compare their average agreement@k, as in the size of the intersection of the outputs of the two models divided by the number of outputs of the NER-Informed model, when focusing only on their top-k outputs. 14 We aggregate these statistics at the sentence level over the whole corpus. We focus on two datasets, namely WikiANN and MasakhaNER, summarizing the results in Figure 4 . 15  Comparing the general performance between these two datasets, it is clear that general agreement is decent. In 7 Out of 9 typologically diverse languages from WikiANN, more than 60% top-1 entities are linked by both models. The African languages from MasakhaNER are low-resource ones yielding less than 40% EL agreement to English in all cases. Given that most of these languages have not been included in the pre-training of BART (the model mGENRE is based on), we expect that using AfriBERTa (Ogueji et al.) or similar models in future work would yield improvements. Effect on downstream maps We compare the dataset maps we obtain using NER-Relaxed and NER-Informed (using gold annotations) models in our pipeline for the MasakhaNER dataset. Overall, the maps are very similar. An example visualization of the two maps obtained for Swahili is in Figure 5 in Appendix E.1. The NER-Informed model produces slightly fewer entities overall (likely exhibiting higher precision for lower link recall) but there are minimal differences on the representativeness measures e.g., the in-country percentage changes from 15.3% (NER-Informed) to 16.9% (NER-Relaxed). We can compare the distributions of the top-k countries obtained with the two models using Ranked Biased Overlap (RBO; higher is better; Webber et al., 2010) . 16 The results for varying values for k (top-k countries) are presented in Table 6 in Appendix E.1. We overall obtain very high RBO values (> .8 for k = 10) for all language portions and all values of k. For example for 8 of the 10 MasakhNER languages the two models almost completely agree on the top-10 countries with only slight variations in their ranking. Dholuo and Amharic are the ones exhibiting the worse overlap (but still > .5 RBO). Conclusion We present a recipe for visualizing how representative NLP datasets are with respect to the underlying language speakers. We plan to further improve our tool 17 by making NER/EL models more robustly handle low-resource languages. We will also expand our dataset and task coverage, to get a broader overview of the current utility of NLP systems. A Responsible NLP Notes We use this section to expand on potential limitations and risks of this work. An inherent limitation of this work is that many datasets are constructed with the goal of answering scientific questions -not necessarily to be used to build NLP systems that serve language users. If our tool is applied without the assumptions behind dataset construction in mind, it might lead to undue criticisms of existing datasets. It us also important to reiterate that no tool, including ours, will ever be 100% accurate, so our tool should be used as an indicator of the cultural representativeness of language datasets, not as a tool that can provide definitive answers. All scientific artifacts used in this paper are publicly available under permissive licenses for fair use. We are not re-distributing any data or code, beyond the code that we wrote ourselves (which will be released under a CC-0 license) and the additional annotations on top of the existing datasets which map the datasets to Wikidata entries (Wikidata data are also available under a CC-0 license). Our use of our data is consistent with their intended use. B Related Work Effective measurement of dataset quality is an aspect of fast-growing significance. Training large language models require huge amount of data and as a result, the inference generated by these pretrained language model as well as the fine-tuned models often show inherent data bias. In a recent work (Swayamdipta et al., 2020) , the authors present how data-quality aware design-decision can improve the overall model performance. They formulated categorization of data-regions based on characteristics such as out-of-distribution feature, class-probability fluctuation and annotation-level discrepancy. Usually, multilingual datasets are collected from diverse places. So it is important to assess whether the utility of these datasets are representative enough to reflect upon the native speakers. We find the MasakhaNER (Adelani et al., 2021) is one such dataset that was collected from local sources and the data characteristics can be mapped to local users as a result. In addition, language models often requires to be truly language-agnostic depending on the tasks, but one recent work shows that, the current state-of-the-art language applica-tions are far from achieving this goal (Joshi et al., 2020) . The authors present quantitative assessment of available applications and language-resource trajectories which turns out not uniformly distributed over the usefulness of targeted users and speakers from all parts of the world. Linking dataset entities to geospatial concept is one integral part of our proposed methodology. Ongoing geospatial semantics research mostly focuses on extracting spatial and temporal entities (Kokla and Guilbert, 2020; Purves et al., 2018) . The usual approach is to first extract geo-location concepts (i.e. geotagging) from semi-structured as well as unstructured data and then linking those entities to location based knowledge ontology (i.e. geocoding). In (Gritta et al., 2019) , the authors propose a task-metric-evaluation framework to evaluate existing NER based geoparsing methods. The primary findings suggest that NER based geo-tagger models in general rely on instant word-sense while avoiding contextual information. One important aspect of our study is the evaluation of cross-lingual consistency while performing multilingual NER or El tasks. In (Bianchi et al., 2021) , the authors focus on the consistency evaluation of language-invariant properties. In an ideal scenario, the properties should not be changed via the language transformation models but commercially available models are not prone to avoid domain dependency. C Dataset Statistics See details in Table 3 . D Geographical Breakdown of Models Performance See details in Table 4 . E NER-Informed vs NER-Relaxed Models In this section, we report the detailed results (see Table 5 ) from our experiment with using intermediate NER model vs skipping this step. E.1 Comparison of NER-Informed and NER-Relaxed Maps This experiment was performed on MasakhaNER data. See Figure 5 for example maps in Swahili. The distributions of the top-k countries we obtain with the two models (one using the gold NER   annotations for NEL and one using our NERrelaxed approach) are compared using Ranked Biased Overlap (RBO; higher is better) (Webber et al., 2010) , a metric appropriate for computing the weighted similarity of disjoint rankings. We choose a \"weighted\" metric because we care more about having similar results in the top-k countries (the ones most represented) so that the metric is not dominated by the long tail of countries that may have minimal representation and thus similar rank. We also need a metric that can handle disjoint rankings, since there's no guarantee that the top-k countries produced by the processes using different models will be different. 18  The results for varying values for k (top-k countries) are presented in Table 6 . We overall obtain very high RBO values (> .75) for all language portions and all settings. F On the Cross-Lingual Consistency of NER/EL Models  LIPs include meaning, topic, sentiment, speaker demographics, and logical entailment We propose a definition tailored to entity-related tasks: crosslingual consistency is the desirable property that two parallel sentences in two languages, which should in principle use the same named entities (since they are translations of each other), are actually tagged with the same named entities. F.1 NER Experiments Models We study two models: SpaCy (Honnibal and Montani, 2017) : a state-of-art monolingual library that supports several core NLP tasks; and a mBERT-based NER model trained on datasets from WikiANN using the transformers library (Wolf et al., 2020) . Training To task-tune the mBERT-based model on the NER task we use the WikiANN dataset with data from the four languages we study: Greek (el), Italian (it), Chinese (zh), and English (en). Evaluation To evaluate cross-lingual consistency, ideally one would use parallel data where both sides are annotated with named entities. What we use instead, since such datasets do not exist to the best of our knowledge, is 'silver' annotations over parallel data. We start with unannotated parallel data from the WikiMatrix dataset (Schwenk et al., 2021) and we perform NER on both the English and the other language side, using the respective language model for each side. In the process of running our experiments, we identified some sources of noise in the WikiMatrix dataset (e.g. mismatched sentences that are clearly not translations of each other). Thus, we calculated the average length ratio between two matched sentences, and discarded data that diverged by more than one standard deviation from the mean ratio, in order to keep 95% of the original data that are more likely to indeed be translations of each other. We use the state-of-the-art AWESOME-align tool (Dou and Neubig, 2021) as well fastalign (Dyer et al., 2013) to create word-level links between the words of each English sentence to their corresponding translations. Using these alignment links for cross-lingual projection (Pad\u00f3 and Lapata, 2009; Tiedemann, 2014; Ni et al., 2017, inter alia) allows us to calculate cross-lingual consistency, measuring the portion of labels that agree following projection. In particular, we use the cross-lingual projections from the English side as 'correct' and measure precision, recall, and F-score against them. Results In preliminary experiments we found that, consistently with the literature, AWESOMEalign performed generally better than fast-align, hence for the remainder of our experiments we only use AWESOME-align. For the three languages we study, the crosslingual consistency of the monolingual SpaCy models is really low, with scores of 8.6% for Greek-  English, 3.1% for Italian-English and 14.1% for Chinese-English. The SpaCy models are independently trained for each language and can produce 18 fine-grained NE labels e.g. distinguishing dates from time, or locations to geopolitical entities. As such, there was no a priori expectation for high cross-lingual consistency. Nevertheless, these extremely low scores reveal deeper differences, such as potentially widely different annotation protocols across languages. 19  For the mBERT-based model we again label both sides of the parallel data, but now evaluate only on locations (LOC), organizations (ORG) and persons (PER) (the label types present in WikiANN). The mBERT models have significantly higher crosslingual consistency: on the same dataset as above, we obtain 53.4% for Greek to English, 62.9% for Italian to English and 25.5% for Chinese to English. Discussion To further understand the source of cross-lingual discrepancies, we performed manual analysis of 400 Greek-English parallel sentences where the mBERT-based model's outputs on Greek and the projected labels through English disagreed. 20 We sampled 100 sentences where the English-projected label was 0 but the Greek one was LOC (location), 100 sentences with Englishprojected as LOC but Greek as 0, and similarly for persons (PER). We performed annotation using the following schema: \u2022 Greek wrong: for cases where only the Englishside projected labels are correct \u2022 English wrong: for cases where the English-side projected labels are wrong but the Greek-side are correct \u2022 both wrong: for cases where the labels on both sides are incorrect \u2022 alignment wrong: for cases where the two aligned phrases are not translations of each other, so we should not take the projected labels into account nor compare against them. \u2022 all correct: both sides as well as the alignments are correctly tagged (false negatives). Encouragingly, the entity alignments were wrong in less than 10% of the parallel sentences we manually labelled. This means that our results are quite robust: a 10%-level of noise cannot account for an almost 50% lack of consistency on the Greek-English dataset. 21 Hence, the system definitely has room for improvement. A second encouraging sign is that less than 2% of the cases were in fact false negatives, i.e. due to the phrasing of the translation only one of the two sides actually contained an entity. Going further, we find that mistakes vary significantly by label type. In about 75% of the 0-LOC cases it was the Greek-side labels that were wrong in outputting LOC tags. A common pattern (about 35% of these cases) was the Greek model tagging months as locations. In the case of 0-PER cases, 62% of the errors were on the English side. A common pattern was the English-side model not tagging persons when they are the very first token in a sentence, i.e. the first token in 'Olga and her husband [...] .' Appendix K extends this discussion with additional details and examples. The above observations provide insights into NER models' mistakes, which we were able to easily identify by contrasting the models' predictions over parallel sentences. We argue this proves the utility and importance of also evaluating NER mod- F.2 Entity Linking Experiments We now turn to entity linking (EL), evaluating mGENRE's cross-lingual consistency (under the NER-Relaxed setting, so the results below should be interpreted under this lens, as the NER-Informed -which we cannot run due to the lack of NER models for some languages-could very well yield different results and analysis). Dataset We use parallel corpora from the WMT news translation shared tasks for the years 2014 to 2020 (Bojar et al., 2014 (Bojar et al., , 2015 (Bojar et al., , 2016 (Bojar et al., , 2017 (Bojar et al., , 2018;; Barrault et al., 2019 Barrault et al., , 2020)) . We work with 14 English-to-target language pairs, with parallel sentence counts in the range of around 1-5k. Evaluation Unlike our NER experiment settings, we do not need word-level alignments to calculate cross-lingual consistency. We can instead compare the sets of the linked entities for both source and target sentences. As before, we use mGENRE in a NER-Relaxed manner. In an ideal scenario, the output of the model over both source and target language sentences will include the same entity links, yielding a perfect cross-lingual consistency score of 1. In this manner, we calculate and aggregate sentence-level scores for the top-k linked entities for k = 1,3,5. In Figure 6 , we present this score as a percentage, dividing the size of the intersection (of the source and target sentence outputs) by the number of source sentence entities. Additionally, in Table 8 , we report the detailed cross-lingual consistency score percentages for 14 english-language source-target pairs from WMT news translation shared tasks (Bawden et al., 2020) . Results As Figure 6 shows, we obtain low consistency scores across all 14 language pairs, ranging from 19.91% for English-Romanian to as low as 1.47% for English-Inukitut (k = 1). The particularly low scores for languages like Inuktitut, Gujarati, and Tamil may reflect the general low quality of mGENRE for such languages, especially because they use non-Latin scripts, an issue already noted in the literature (Muller et al., 2021) . The low percentage consistency scores for all languages makes it clear that mGENRE does not produce similar entity links for entities appearing in different languages. In future work, we plan to address this limitation, potentially by weighting linked-entities according to the cross-lingual consistency score when performing entity disambiguation in a multilingual setting. Discussion We further analyze whether specific types of entities are consistently recognized and linked across language. We use SpaCy's English NER model to categorize all entities. Figure 7 presents a visualization comparing consistent entity category counts to source-only ones.  From Figure 7 , it is clear that geopolitical entities (GPE) are the ones suffering the most from low cross-lingual consistency, with an order of magnitude less entities linked on both the English and the other language side. On the other hand, person names (PER) seem to be easier to link. While the most common types of entities are PERSON, ORG (i.e. organization) and GPE (i.e. geopolitical entity), we found that the NER model still failed to correctly categorize entities like (Surat, Q4629, LOC), (Aurangzeb, Q485547, PER). However, these entities were correctly linked by the NER-Relaxed pipeline, indicating its usefulness. We hypothesize, and plan to test in future work, that a NER-Relaxed entity further regularized towards cross-lingual consistency will perform better than a NER-Informed pipeline, unless the NER component also shows improved cross-lingual consistency. P E R S O N O R G G P E D A T E C A R D I N A L N O R P L O C F A C P R O D U C T Q U A N T I T Y entity type From Figure 7 , it is clear that geopolitical entities (GPE) are the ones suffering the most from low cross-lingual consistency, with an order of magnitude less entities linked on both the English and the other language side. On the other hand, person names (PER) seem to be easier to link. While the most common types of entities are PERSON, ORG (i.e. organization) and GPE (i.e. geopolitical entity), we found that the NER model still failed to correctly categorize entities like (Surat, Q4629, LOC), (Aurangzeb, Q485547, PER). However, these entities were correctly linked by the NER-Relaxed pipeline, indicating its usefulness. We hypothesize, and plan to test in future work, that a NER-Relaxed entity further regularized towards cross-lingual consistency will perform better than a NER-Informed pipeline, unless the NER component also shows improved cross-lingual consistency. G Additional Dataset Maps We present all dataset maps for the datasets we study: \u2022 H NER Dataset Socioeconomic Factors Table 1 presents the same analysis as the one described in Section 3.3 for the X-FACTR and the NER datasets. The trends are similar to the QA datasets, with GDP being the best predictor and including population statistics hurting the explained variance. I Socioeconomic Correlates Breakdown You can find the breakdown of the socioeconomic correlates in Table 12 for TyDi-QA, Table 13 for MasakhaNER, and Table 14 for WikiANN. J NER Models Confusion Matrices See Figure 18 for the confusion matrices of the SpaCy and our WikiANN neural model. K Greek-English NER Error Discussion We find that the mistakes we identify vary significantly by label. In about 75% of the 0-LOC cases it was the Greek-side labels that were wrong in tagging a span as a location. A common pattern we identified (about 35% of these cases) was the Greek model tagging as location what was actually a month. For instance, in the sentence Ton M\u00e1io tu 1990 episk\u00e9ftikan yia t\u00e9sseris im\u00e9res tin Ouggaria(In May 1990 , they visited Hungary for four days.) the model tags the first two words (\"in May\") as a location, while the English one correctly leaves them unlabelled. In the case of LOC-0 cases, we found an even split between the English-and the Greek-side labels being wrong (with about 40% of the sentences each). Common patterns of mistakes in the English side include tagging persons as locations (e.g. \"Heath\" in \"Heath asked the British to heat only one room in their houses over the winter.\" where \"Heath\" corresponds to Ted Heath, a British politician), as well as tagging adjectives, often locative, as locations, such as \"palaeotropical\" in \"Palaeotropical refers to geographical occurrence.\" and \"French\" in \"A further link [..] by vast French investments and loans [...]\". Last, in the case of 0-PER cases we studied, we found that 62% of the errors were on the English side. A common pattern was the English-side model not tagging persons when they are the very first token in a sentence, i.e. the first tokens in \"Olga and her husband were left at Ay-Todor.\", in \"Friedman once said, 'If you want to see capitalism in action, go to Hong Kong.' \", and in \"Evans was a political activist before [...]\" were all tagged as 0. To a lesser extent, we observed a similar issue when the person's name followed punctuation, e.g. \"Yavlinsky\" in the sentence \"In March 2017 , Yavlinsky stated that he will [...]\". L Comparing X-FACTR to mLAMA These two similar projects aim at testing the memorization abilities of large language models (X-FACTR and multi-LAMA (mLAMA; Kassner et al., 2021) ) -see corresponding Figures in Table ??. Both of these build on top of Wikidata and the mTREx dataset. Hence, their English portions are equally representative of English speakers, sufferring from under-representation of English speakers of the Global South. For the other language, however, mLAMA translates English Acknowledgements This work is generously supported by NSF Awards 2040926 and 2125466. Pan-X (WikiANN) Geographic Coverage Greek Spanish Estonian Basque Chinese Finnish Pan-X (WikiANN) Geographic Coverage French Hebrew Hungarian Indonesian Japanese Korean Pan-X (WikiANN) Geographic Coverage Vietnamese Yoruba SQuAD Geographic Coverage",
    "abstract": "As language technologies become more ubiquitous, there are increasing efforts towards expanding the language diversity and coverage of natural language processing (NLP) systems. Arguably, the most important factor influencing the quality of modern NLP systems is data availability. In this work, we study the geographical representativeness of NLP datasets, aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers. In doing so, we use entity recognition and linking systems, presenting an approach for good-enough entity linking without entity recognition first. Last, we explore some geographical and economic factors that may explain the observed dataset distributions. 1 2 Datasets designed to capture dialectal variations, e.g., SD-QA (Faisal et al., 2021), are culturally-aware in terms of annotator selection, but there is no guarantee that their content is also culturally-relevant for the language speakers. 3 See \u00a72 of their paper.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Swahili",
        "English"
    ],
    "numcitedby": "4",
    "year": "2022",
    "month": "May",
    "title": "Dataset Geography: Mapping Language Data to Language Users"
}