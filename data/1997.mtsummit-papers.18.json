{
    "article": "Introduction The beginnings of machine translation in Canada can be dated to 1965: that year while the ALPAC committee was preparing the infamous report that would decimate MT around the world for years to come, the National Research Council of Canada blithely decided to fund two machine translation projects, one at the University of Saskatchewan and the other at the University of Montreal. The Saskatchewan project came to an end a few years later; but the CETADOL project in Montreal (which later changed its name to TAUM) went on to become a world leader in the field. In this paper, I adopt the Summit's general theme -MT Past, Present and Futureto present a general overview of MT research and development in Canada. I begin with a brief and necessarily partial survey of its history, then provide a personal assessment of the current state of the field, and conclude with some sober speculation on the future of MT R&D in this country. The Past 2.1 The early years to the end of TAUM MT flourished in Canada during the late sixties and early seventies, at a time when it was in serious decline almost everywhere else in the world, and in retrospect we can point to a number of factors, in addition to the NRC's support, which account for this apparent anomaly. Canadian funding agencies may not have been aware of the recommendations published in the 1966 ALPAC report, but the scientific directors at the newly founded TAUM group certainly were, and the orientation they gave to the group's early work conformed to many of that report's key recommendations. Indeed, their able leadership must surely be cited as a major reason for the TAUM group's success. 1 Alain Colmerauer made a significant contribution toward increasing the modularity of monolithic MT systems by developing his Q-Systems at TAUM; this was one of the first high-level program languages specially designed to facilitate linguistic descriptions. Richard Kittredge, a subsequent director of the group, was instrumental in introducing what came to be know as the sublanguage approach to MT. Given the current state of the art, the best way to ensure high-quality machine output, he argued, was to focus on the linguistic systems used by experts in specialized domains, since these display a narrower range of ambiguities than the unrestricted texts of the language at large (c.f. Kittredge [15] ). Another major factor that helped buttress the arguments for continued MT funding in Canada came with the passage of the Official Languages Act in 1969, which guaranteed citizens access to government services in either official language while also allowing public servants to work in the official language of their choice. As a result of this legislation, the institutional demand for translation began to grow by leaps and bounds. The government was consequently open to any proposal that promised to help reduce translation costs -including what must then have appeared as far-fetched computerized systems. In 1975, ten years after the group's founding, TAUM received a contract from the federal Translation Bureau to develop a specialized MT system for the sublanguage of public weather forecasts. The resulting system, as everyone knows, became one of MT's all-time success stories. M\u00c9T\u00c9O, as it was called, was delivered by TAUM in 1976, and later taken over and ported to a micro-computer by John Chandioux, who also extended it to new types of bulletins and the opposite language direction (i.e. French to English). The system has been in daily operation for over twenty years and now translates more than 45 thousand words a day, with the output requiring only minimal post-editing. 2   Buoyed by the success of this venture into practical MT, the federal government granted the TAUM group a second contract in 1977, this one for a more ambitious system designed to translate the maintenance manuals of its newly acquired Aurora aircraft. It soon became apparent, however, that both parties had grossly underestimated the magnitude of the task and the complexity of this new domain. The TAUM group was obliged to quickly expand and considerable effort had to be invested in training new linguists, computer scientists and lexicographers. After three years of intensive effort, the group produced TAUM-AVIATION, a second generation system which took as its corpus the Aurora's hydraulics manuals. This MT system was no doubt among the most sophisticated in the world at the time, but it came nowhere close to meeting the client's expectations for a fully operational, user-friendly document production chain capable of handling all of the aircraft's many sub-domains. In 1980, an outside consultant evaluated the system and concluded it would be too costly to extend it to the full range of the Aurora's documentation. The AVIATION project was abandoned; and because the group had invested all its resources in this one mega-project, TAUM too had to be disbanded one year later. The Optimistic Eighties Of course, the demise of TAUM did not eliminate all interest in machine translation in Canada. On the contrary, as the demand for translation continued to swell, more and more people began to look to computers for a solution to the growing logjam in the country's translation services. At the same time, south of the border, new commercial MT systems were emerging, and quite naturally, their developers cast their eyes north to the sizable and lucrative Canadian market. A number of large-scale MT trials were organized in Canada during the early eighties, some resembling development projects in which vendors undertook to customize their system to the particular needs of the client. There is no space to go into the details of these trials and their tribulations here; the interested reader is referred to Macklovitch [16] and Chandioux [3] , among other reports. Suffice it to say that none of these commercial systems wound up being retained by the translation services in question. Moreover, the reasons for their rejection were generally the same: when put in the hands of professional translators accustomed to producing high-quality output, no system could be shown to reduce the cost or overall turn-around time of the service, and none succeeded in winning over the translators by significantly improving their working conditions. With its own internal demand for translation rapidly approaching 300 million words a year, and ready-made commercial solutions shown to be inadequate, the Canadian government decided to renew its commitment to MT research and development. In 1985, the Department of Communications opened a new research center in Montreal, initially called the CWARC and later renamed the CITI, and invited Pierre Isabelle, former scientific director at TAUM, to set up a group in machine-aided translation. The program elaborated by the CITI's TAO team was a conscientious attempt to respond to what was seen as the past failures of machine translation, in particular, the persistent tendency to confuse the aims and methods of research with efforts to provide short-term solutions to the practical problems facing working translators. 3 For the latter, instead of doggedly insisting that they post-edit the often sub-standard output of fully automatic systems, the CITI proposed a variety of machine-aids designed to support rather than supplant the human translator: first, a series of translator's workstations integrating off-the-shelf software intended to help translators perform their peripheral tasks more efficiently (c.f. Macklovitch [17] ); and later, a series of innovative tools based on the notion of bi-text (or parallel text alignment), meant to assist translators with tasks more central to their function. (More on these below.) Today, it is probably true that the vast majority of translators in North America and Europe work on variants of what Martin Kay first called a translator's amenuensis; but ten years ago, someone had to go to the trouble of testing these ideas by actually assembling such workstations and organizing operational trials to see how they could be improved. If, in the estimate of the CITI's TAO team, fully automatic, high-quality MT was generally infeasible for unrestricted texts, the success of METEO did prove that FAHQT was attainable for certain restricted sublanguages. And so a second component of the TAO group's program was to attempt to locate one or more domains whose texts displayed the same configuration of characteristics that made weather bulletins such an ideal application: high, steady volume; tolerance of repetition and literal style; limited syntax and vocabulary; and a closed semantic universe. Specialized MT systems could then be developed for these sublanguages (provided, of course, we could persuade the end-users to pick up the tab). Considerable effort was expended in canvassing translation services in the hope of locating such text types, for it was felt that nothing would better bolster the cause of MT than another resounding success.The results of these efforts, however, proved disappointing. Although we did find certain texts that were drafted in simple, repetitive language, and others that were produced in very high volume, none of the scores of samples we examined combined all the requisite characteristics. In the end, we were forced to admit that weather bulletins probably constitute the exception that sadly confirms the rule. But even if we had been successful in locating several sublanguages for which specialized systems could have been developed, the contribution that such systems could make to relieving the overburdened translation profession would necessarily remain minimal. Ultimately, if we hope to allow for the timely translation of all the texts that need to be translated, at a quality level that end-users require, the only solution is to augment the capabilities of current MT technology. This was the goal of the research component of the CITI's TAO program: to apply the most recent advances in Al and NLP in an effort to develop more intelligent MT systems. Among its realizations, it produced the CRITTER prototype, which was among the first fully reversible MT systems (c.f. Isabelle et al. [12] ). To the best of my knowledge, the CITI's program constituted Canada's only sizable R&D effort in machine translation during the 1980's and early 90's. There were a number of smaller private-sector firms that remained active in MT during this period, notably the ones headed by former TAUM members John Chandioux and Richard Kittredge; but even these tended (by necessity) to focus more on short-term development projects than long-term research. Chandioux's firm has continued to develop specialized systems for various clients, often involving as much intelligent document processing as actual translation (c.f. Chandioux [4] ); whereas Kittredge's company has made a name for itself in the area of multilingual generation (c.f. lordanskaja et al. [10] ). This period also saw the founding of Lexi-tech Inc. in Moncton, New Brunswick, a spin-off of the Canadian navy's hefty contract to construct a new fleet of frigates. Lexi-tech's initial mandate was to translate the frigates' maintenance manuals, and to do so within the required timeframe, it brought in a major commercial MT system as part of its hi-tech publishing chain. Another noteworthy, if less illustrious event was the scandal surrounding the funding of an MT project that promised to translate the accumulated English legislation of the province of Saskatchewan, going back to 1870 ... within two years! Not surprisingly, the translations were never delivered and the project was summarily abandoned; though not before the whole endeavor of machine translation was once more sullied in the public eye. 4 The Present Decade There was not a great deal of MT-related activity in Canada during the early 1990's -much less, say, than at the beginning of the previous decade. However, there has been a notable upswing in recent years, owing principally to the explosive growth of the Internet. Most of this recent activity has been concentrated in commercial development rather than in applied research. The CITI remained one of the country's only MT R&D groups, along with the Natural Language Lab at Simon Fraser University (although, as its name indicates, this lab's focus is wider than just MT). Little of the enthusiasm for the new paradigms that appeared in the late 1980's and early 1990's, such as Examplebased MT and Statistical MT, which took hold of groups working on MT in other countries, seems to have made its way to Canada. 5 Most of the projects that have appeared here in recent years have sought to develop new applications for what is basically standard, well-known MT technology. Which is not to say that these applications are without interest; very much to the contrary, as I hope to show. However, the crucial factor upon which the success of these projects depends consists in finding \"good applications for crummy MT,\" to quote the catchy title of Church and Hovy's controversial paper [5] . Put another way, high-quality MT output is not a sine qua non for such applications; and a corollary of this is that their targeted clientele is not likely to be professional translators. I will now try to illustrate this new trend by examining a few Canadian examples, before returning to the issue of MT research in Canada. MT on the Internet: Where the action is! This was a major theme at the last AMTA Conference in Montreal, and one of the things that emerged through all the excited debate is that the Internet is transforming classic MT in at least two distinct ways. On the one hand, it is facilitating general access to MT systems and MT service providers; and on the other, it is generating a tremendous new demand for translation, as millions of Internet surfers increasingly come across Web pages in \"foreign\" languages they do not fully master. The MT industry has been quick to recognize the enormous commercial potential of this new demand, which, owing to its sheer volume and transitory nature, cannot possibly be satisfied by traditional translation services. Among the solutions that the industry has thus far proposed for this pressing problem, we may mention Globalink's Web Translator product, which is a fully compatible net browser add-on; and referred Web page translation like that offered by Systran via its HTML Translation Page. Alis Technologies, a Montreal-based software firm that has been marketing its own multilingual Internet browser, is proposing yet another solution: \"an integrated set of tools and services for language translation and handling\", to quote the company's promotional literature. In other words, Alis wants to act as a value-added reseller for other MT systems, many of which are independently available on the Internet. There are a number of arguments in favor of this approach. For one thing, no one MT system can currently provide the full range of language pairs that many users now require. For another, Alis can customize their \"translation gateway,\" selecting among various high-end and low-end products, to suit the particular needs of each client. 6 And finally, acting as a VAR allows Alis to enter this expanding market in a fraction of the time (and at fraction of the cost) that it would take for them to develop their own proprietary MT system. On the other hand, Alis is not the first company to try to the VAR approach to MT on the Internet, although to my knowledge, they are the first to attempt to bundle various MT systems together into a single package. Thoroughly mastering one MT system can be difficult enough for a non-initiated user. Will Alis be able to provide a unified interface that will allow their clients to manipulate several MT products -and make dictionary updates to them? Furthermore, given that most of the MT systems in question are already available on the Net, will Alis be able to offer sufficient value-added services and products to attract new customers? In short, the viability of this approach still has to be demonstrated. I will return to the general question of MT as a means of overcoming the Internet's language barriers in section 4 below. MT on the tube Another interesting Canadian development project is being conducted by TCC Communications, a British Columbia firm, in collaboration with the Natural Language Lab at Simon Fraser University. They are developing an MT system for the real-time translation of the closed-caption text that accompanies the majority of North American TV broadcasts and all major video releases. Now this is an intriguing application, for which there would appear to be an obvious demand; although here too the difficulties should not be underestimated. As Popowich et al. [21] point out, the variety of colloquial English that is generally found in television dialogue is very often ungrammatical when evaluated from the point of view of a standard, full-sentence grammar. An MT system designed to handle such fragmentary dialogue will therefore have to be particularly robust. Furthermore, television dialogue is the very antithesis of a restricted semantic domain, which cannot fail to exacerbate problems of lexical ambiguity. But as Popowich et al. explain, these difficulties are offset to a large extent by the availability of contextual information, in the form of the television image, which provides the viewers with helpful clues to disambiguate and perhaps even correct a less than perfect translation. Their market studies indicate, moreover, that viewers tend to be tolerant of such translations, insofar as they manage to arrive at a general understanding of the dialogue. Hence, this could well constitute an excellent application for crummy MT. Research into MAHT at the CITI Whether or not such applications eventually turn out to be commercially successful, the fact is they will do little to improve the overall quality of the translations produced by today's MT systems. As we stated above, the only real solution for this is applied research aimed at extending the capabilities of current MT technology. In the early 1990's, the group with the longest tradition in MT research in Canada was the one at the CITI, headed by Pierre Isabelle. Around that time, the TAO team undertook a fundamental change in orientation, as it began to explore the possibilities of the newly emerging corpus-based, probabilistic approaches to NLP. The reasons for this paradigm shift are outlined in Isabelle [14] ; but basically, what it came down to was the inescapable conclusion that, after more than forty years of effort, the classic rule-based approach had not allowed for the emergence of anything but fully automatic MT. And not believing that these systems offer a viable solution in the great majority of translation situations, the group decided that it would have to look elsewhere if it wanted to develop translators' aids that went beyond the office automation of its PTT. It took as its starting point the pioneering work on parallel text alignment done by Brown et al. [2] and Gale and Church [8] , and generalized this into the concept of translation analysis. Up until then, the problem of translation automation had almost always been conceived in terms of translation production: given a text in one language, how can we program a machine to produce an equivalent text in another language? Translation analysis views the translation relation from another angle. Given two texts that are known to be mutual translations, can we program a machine to automatically identify the segments in one text which are the translation of the corresponding segments -be they paragraphs, sentences, phrases or words -in the other text? At first blush, this may appear an odd thing to want to do, since it requires that the translation already be known. But when this linking of translation units is carried out on large corpora of past translations, it allows for the conversion of these past archives into an active resource that can provide the translator with considerable help in tackling new texts more efficiently. To quote Pierre Isabelle's celebrated dictum: \"Existing translations contain more solutions to more translation problems than any other available resource.\" Translation analysis provides the basis for the productive recycling of all this knowledge lying dormant in translators' past production. See Isabelle et al. [13] , where these arguments are articulated in more detail. Between 1992 and 1995, the CITI's TAO group set out to demonstrate the productivity of this new paradigm for MAHT by developing a series of novel translation support tools. These included TransSearch, a bilingual concordancing tool, or interactive translation memory (c.f. Simard et al. [22] ); TransCheck, a translation checker that can detect certain simple types of errors in a draft translation (like false cognates and illicit borrowings), as well as help enforce terminological consistency (c.f. Macklovitch [19] ); and TransTalk, a dictation system for translators that improves upon the performance of monolingual voice recognition by using its knowledge of the source text (c.f. Brousseau et al. [1] ). The first of these is an all but finished product, 7 while the latter two are closer to proof-of-concept prototypes; and though it may be some time before such tools make it onto translators' desks in the form of commercial products, it nevertheless seems reasonable to maintain that the productivity of the paradigm has been amply demonstrated. In addition to these applications, the TAO team has continued to refine the underlying alignment algorithms in order to make them more robust and accurate, and to allow as well for the alignment of finer-level units; c.f. Simard and Plamondon [23] . Better alignment algorithms should make it possible to develop even more powerful translation support tools; c.f. Macklovitch and Hannan [20] . In the remainder of this section, I want to briefly describe another ambitious research project called TransType (c.f. Foster et al. [7] ). Trans Type represents a new attempt at the old dream of interactive MT, in which the machine and the translator work in tandem to jointly produce a translation. In almost all previous interactive systems, the locus of this interaction has been the source text: typically, the system requests the human's assistance in disambiguating various aspects of its meaning, even though this is something that translators (as opposed to linguists) are not generally trained to do, at least not explicitly or formally. In TransType, on the other hand, the locus of the interaction is the target text itself, and the system can be conceived as a kind of accelerated typewriter designed to speed up the translator's work by automatically completing the target unit that s/he has begun to key in. 8 It does this by reconciling the \"prefix\" of the translation unit that the human has already typed with the various translations predicted by its own translation and language models. In principle, these predictions should be recalculated with each new character the user adds, but this may not as yet be computationally feasible without causing undesirable waiting time. Obviously, this approach raises non-negligible ergonomic problems, and, generally speaking, its success is far from a foregone conclusion; it is, after all, a research project. Still, the results of preliminary experiments designed to test the feasibility of this new kind of interactive MT are encouraging; Foster et al. report a reduction of approximately 70% in the number of keystrokes needed to type target text words. The main challenge, of course, will be to extend the system beyond simple word completion, so that its language models can predict longer and longer units of the target text. And the future...? In 1996, as part of the federal government's sweeping deficit reduction and downsizing program, Industry Canada announced its intention to divest itself of the CITI. Several months later, the ministry signed an agreement authorizing the transfer of the TAO group to the University of Montreal. At the end of June 1997, the group finally took up residence in the University's computer science department, where it has joined several professors to form a new laboratory called the RALI (a French acronym for \"Recherche Appliqu\u00e9e en Linguistique Informatique\"). According to the terms of the agreement, Industry Canada will continue to fund the RALI until March 1999, after which the lab must become financially self-sufficient. This move therefore constitutes a major challenge for a group which has always had a guaranteed annual operating budget. Still, given the talented members that make up the RALI and the stimulation of new input from the university environment, I am confident that we will be able to meet this challenge. I wish I felt as optimistic about the prospects for the future of MT R&D in Canada. The conditions for the RALI's survival are quite clear: as of April 1999, the group must bring in sufficient revenue to support itself, by selling either its research, its services or its products. This means going where the money is and, in Canada at least, there has traditionally been little money outside the government for applied research in the areas of machine and machine-aided translation. In point of fact, the RALI has already begun commercial negotiations with parties that are interested in two of our products. In both cases, however, it turns out that the programs in question have nothing to with translation, but are monolingual spin-offs of our work on probabilistic language modeling. 9 This suggests that if the RALI is to survive after the end of its government subsidy, it may well have to abandon its traditional concentration on M(A)T in order to focus more generally on NLP. 10   To say that there is little money for MT in Canada does not mean that there is no money at all; that would be an exaggeration. TCC's project described in section 3.2 above has benefited from some public funds; and the Quebec government recently awarded the Montreal-based firm of Machina Sapiens a modest grant for the development of an MT system designed to assist French speakers with English language Web pages. But in both cases, these are specific development projects. Since 1965, the Canadian government has maintained, more or less uninterrupted, a long-term commitment to fund both research and development in machine translation, as a sort of corollary to its official bilingualism policy. This is what is in danger of disappearing. At present, the only source of funding for MT research comes from Canada's national research councils, and typically the amounts they grant are barely sufficient to support one senior researcher and at most a couple of assistants. For a country with a small population, Canada has played a relatively important role in the history of MT. Judging from this country's experience, MT R&D is most productive when conducted by a multidisciplinary team that is guaranteed a certain critical mass and stable, long-term funding. When the TAUM group was disbanded in 1981, it took years to replace the expertise that was lost. The CITI's TAO group, which inherited the TAUM legacy, was unique in Canada; indeed, it was one of the few research groups in the world that maintained professional translators as its targeted clientele. The federal government has sought to avoid repeating this same mistake in closing the CITI by assuring the RALI's funding for the next year and a half. But what will happen after that? There is little reason to believe that the private sector will pick up the mantle and suddenly begin to fund MT research and development; it hasn't in the past, in spite of the fact that it spends tens of millions of dollars on translation each year. If the private sector remains true to past form, then the Canadian government's decision to abandon its longterm commitment to machine translation cannot fail to have dire, though altogether predictable consequences. Professional translators -those people whose job it is to produce the enormous volume of high-quality translations that are still required for publication -will continue to be completely overwhelmed. And the general public -in particular, those who in navigating the Internet are increasingly confronted with texts in languages they do not fully master -will continue to be fed a diet of crummy MT. Not everyone may find this situation catastrophic. Indeed, there are some who see in the explosive growth of the Internet the salvation of machine translation, perhaps even a commercial bonanza. I do not happen to share this enthusiasm, although I do, of course, recognize that for many Internet surfers, MT currently offers the only way around a vexing problem. But in reality, many people currently resort to (and may even be prepared to pay for) machine translation because there is no alternative. I suspect that for many such users, recourse to MT in this situation is a little like flipping a hamburger with a shovel. It may on occasion help get the job done; but whew! is it unwieldy! What these users really want is not full-blown (and often garbled) machine translation, but a quick and easy way to surmise the content of a foreign language Web page; and for this, something less than full MT -perhaps just a gloss of certain keywords -may be altogether sufficient. 11 The day someone comes along with this sort of spatula, my bet is that the shovel will be quickly tossed aside. In fact, some interesting work is already being done in the European Communities' Compass project (c.f. Breidt and Feldweg [6] ) to develop a specialized tool to aid in the comprehension of electronic documents in a foreign language. And in any case, all the current euphoria over MT on the Internet should not lead us to forget the difficult predicament of professional translators. Crummy quality may (or may not) suffice for occasional, casual users of MT; but it will not do for most professional translators who must produce high-quality output. For them, the only long-term solution remains more intelligent MT and more powerful translation tools, of the sort we will only see if we continue to invest in research. Acknowledgments: My thanks to Michel Simard and Graham Russell for helpful comments on an earlier version of this paper.",
    "abstract": "",
    "countries": [
        "Canada"
    ],
    "languages": [
        "French",
        "English"
    ],
    "numcitedby": "2",
    "year": "1997",
    "month": "October 29 {--} November 1",
    "title": "{MT} {R}{\\&}{D} in {C}anada"
}