{
    "article": "Since the first of the T&C conferences in 1978 the field of machine(-aided) translation has seen many changes: from mainframe computers to personal computers and the Internet; from a mainly academic research pursuit to a competitive commercial field; from discussions of 'future potentials' to discussions of actual uses; from a translation profession largely antagonistic to mechanization to one making cost-effective exploitation of computerized translation tools. This presentation will look at the situation 20 years ago, describe the major changes and developments, and consider what the future may bring. The beginning in 1978 The first conference held under the title 'Translating and the computer' was held on the 14th November 1978. Two events the previous year (in May 1977) had alerted members of the Aslib Technical Translation Group to developments in machine translation and in computer aids for translators. One was the conference held in Luxembourg on 'Overcoming the language barrier'; the other was the news of the successful launch of the Meteo program for translating weather forecasts from English into French. The Luxembourg conference had been organised by the Commission of the European Communities (as it was then called) to bring together all those involved world-wide in the development and use of computerised translation, whether fully automatic systems (true MT), translation aids or terminology databases. It is perhaps worth remembering that it had in fact been not much more than twenty years earlier that serious research in machine translation had begun. Although last year marked the fiftieth anniversary of the 'birth' of MT -the letter sent by Warren Weaver (a prominent administrator at the Rockefeller Foundation) to Norbert Wiener (the founder of cybernetics) in March 1947 --it was only in 1955-56 that the major US research groups began work with substantial financial support. Just ten years later, most of this activity was to come to an end with the publication of the famous ALPAC report in 1966, which had concluded that MT research had no foreseeable prospect of success, that there were in any case no economic reasons for its use, and that machine aids in the form of automatic dictionaries and the like represented a more worthwhile aim. For the next ten years or more, MT research was abandoned in many places -it became, in effect, a discredited area of activity. And this aura of disreputableness remained with it for many years. It was certainly the case in 1978 when the first of the Aslib conferences took place. It would have been a surprise, therefore, to hear that MT was being considered seriously by the European Commission. The organisers clearly felt that these developments ought to be known about. In the words of Barbara Snell, the editor of the proceedings of that first gathering, the intention was: to alert fellow translators in the Aslib TTG and the TG [Translator's Guild] to this prospect [of computers in their work] and to encourage contact with computer people. We felt that some of the scientists whose primary interest lay in expanding the sphere of computer activity, including machine translation, might be unaware of certain basic aspects of translating... Maybe we could help to clarify their objectives and in return, learn what computers could do for us. In essence these have remained the basic objectives of all the subsequent 'Translating and the Computer' conferences. With the success of the 1978 conference, it was decided to hold a second two years later, this time concentrating on machine aids, and then another conference in the following year. From this time forth, the \"Aslib conferences\" (as they have come to be known) were firmly established as an annual event, taking place every November, usually in the second week of the month. This historical review cannot, of course, cover all developments over twenty years, it is necessarily selective. 1  The first conference's chairman was Juan Sager (UMIST), a prominent figure in the British translation profession (and later a founder of the Institute of Translation and Interpreting) who was particularly active as a consultant to the European Commission on translation matters. He was therefore asked to summarise the situation in the Commission's translation service and to outline their plans for automation. He was at pains to assure translators in the audience that MT was no threat. \"What is aimed at is not the total elimination of the human translator, but firstly to assist communication by making translations more widely available, even at the cost of loss of quality -as long as this is acceptable. Secondly to reduce cost by reserving human intervention for the final stages. Thirdly to speed up the process as much as possible.\" Inevitably, he saw MT in terms of the systems then in operation, in which large volumes of texts were input in batches to mainframe computers, and the (printed) output was 'revised' by human post-editors and/or translators. However, current MT research assumed that translation was a uniform process, and that a general-purpose system could form the basis for all MT applications; Sager advocated a future diversification of approaches: ... more research is required into the process of translation as such, into the communication processes involved, and the types and characteristics of texts which require translation. The machine process is not an imitation of the human process but as we have different types of texts, different types of translation, and different translators, we must recognise that different machine processes are required for dealing with the considerable diversity of products and demands A major aim of the conference was to illustrate practical aspects of using MT and machine aims. Two papers were concerned with Systran, the system acquired by the European Commission. The first, by Frank Knowles, took a critical look at what kind of translations the Russian-English version of Systran was producing, what errors were being made and how they could be improved. It was the kind of detailed linguistic analysis which is still rare in the field; but it was to be the first of many later papers at these conferences devoted to questions of evaluation. The second paper, by John Elliston, described how Systran was being used at Xerox for the translation of English documentation into multiple languages. The aim was reduction of translation costs, faster and timelier output of documents, and improved clarity of communication. The solution involved the control of the vocabulary and syntax of the texts before input to the MT systems -it was the first example of the now popular \"controlled language\" approach to MT applications. The other approach to 'controlled' input was illustrated by Shiu-Chang Loh's description of the CULT (Chinese University Language Translator) system designed specifically for the translation of Chinese mathematics texts. Preparation of the input involved a large measure of human pre-editing to correct the source texts, insert punctuation, update dictionaries, etc. It illustrated how much human involvement was actually needed in 'automatic' translation even in highly restricted domains. To cover basic machine aids, there were papers by Eberhard Tanke on the pioneer TEAM 1 I shall also not provide detailed references of every paper mentioned; instead contributions will be identified by the speaker and the year of the conference. A list of the proceedings in which they appear is given at the end. system at Siemens for producing text-related glossaries on demand and for producing conventional printed dictionaries, and by Goetschalckx from the European Commission's terminology bureau, on the now well-known EURODICAUTOM terminology database. Another representative from the European Commission was Peter Arthern who cast a critical eye on machine aids and asked himself was whether MT (i.e. Systran) was any use, practically or economically, and concluded, on the basis of experience with the English-French system, that the raw output was \"quite inadequate\" even as a first rough draft, and that post-editing was a chore that should not be imposed on anyone. But his experience led him to reflect on what else computers could do for translators. Apart from the now obvious assistance in multilingual word processing, automatic spelling correction, automatic dictionaries and personal glossaries, he foresaw one development that has only recently come to fruition. He noticed that a high proportion of EC documents were repetitive, both in content and in the actual sentences and phrases used. Why then should there not be a bilingual store of original documents and their translations to which translators could have access? It was, in effect, a proposal for what is now known as the 'translation memory'. His vision of the \"translator of tomorrow\" was remarkably accurate: My hunch is that our translator... will continue to work at the same type of desk in the same time of office... with his standard dictionaries and reference works around him. Instead of a traditional typewriter, however, he will have a text-processing terminal with keyboard and screen so that he, or a secretary to whom he dictates, types his translations into the system memory so that they can be corrected on the screen before final \"typing\" on a separate printer... If he has access to a local term bank, he will be able to interrogate it simply by typing his question on the keyboard of his textprocessing terminal, when the answer will appear on the screen... In a large organisation using my proposed new system of machine-aided \"translation by text-retrieval\"... our translator will be given, when he reports for duty, not only the original of the text he is required to \"process\", but... [a] version of it in the target language... both presented on paper... [H]e will complete the target-language version of the text on paper, using his text-processing terminal to type any completely new passages. He will also use his terminal to get terminological information from the organization's term bank if necessary, either on line or in the form of a text-related glossary... He would then check the complete translation and pass it on, either for revision... or straight for typing by a secretary into the text-processing system for storage in the text-memory and printing out... It would of course be technically possible to do all translating, editing and revision operations on the screen at the terminal, without printing the texts on paper at all... That it should take another 15 years or more before this vision became a reality can be attributed on the one hand to the relatively low storage capacities of computers until recently, to the relatively high costs of 'translator workstations', and to the lack of statistical and computational methods for the exploitation of translation memories. The research perspective was provided by Yorick Wilks -who continued for many years to be a popular and entertaining speaker at these conferences. His theme was the potential contribution of artificial intelligence to the task of computer-aided translation. In essence, this was an argument for the much greater use of semantic analysis than was found in most systems of the time (both research and operational) and for the incorporation of 'real world' knowledge for the resolution of ambiguities. The prospects were encouraging at the time; however, it is perhaps indicative of how much research approaches change that in subsequent years almost nothing more was heard about artificial intelligence in Aslib conferences. What has happened is that today the methods of AI have become so familiar to those working in the field that they no longer merit particular mention. Finally, there was Margaret Masterman who, in her typically idiosyncratic manner, argued forcefully and impressively for the greater involvement of working professional translators in the development of MT systems. Some were undoubtedly encouraged to do so -notably Veronica Lawson, who was to be a major figure in the organisation and leadership of many following conferences. In general, however, professional translators have not been participants in MT development itself, preferring to keep their distance from direct involvement and to wait for practical tools that they can test in the real world. The early 1980s The second conference, in 1980, focussed on the practicalities of using translation aids -not just computer-based (for these were still rare), but other aids such as dictation machines and electric typewriters. Word processors were still expensive at \u00a38,000 -and they were still crude by today's standards. They were not general-purpose micro-computers with wordprocessing software but task-specific machines with virtually no internal memory; all texts had to be repeatedly and regularly loaded from and stored on floppy disks (not yet standardised) or magnetic tapes, commands and functions were entered by users and displayed in texts; screens did not often show what would be actually printed (no WYSIWYG then); and making printers work as required was a hit-and-miss process (as it still can be!) As for electronic transmission, the use of acoustic modems was fraught with interruption (human and system) and uncertain reception; corrupt text was almost the norm. Nevertheless, there were pioneers who were keen to report their experiences and to encourage other translators that machine aids were the future. Despite the costs of equipment, the high labour costs involved in translation could be reduced by efficient use of machine aids. This was already possible even for the independent free-lance translator, as Bob Clark demonstrated. As in 1978, there was much talk of the use of termbanks on line, with a survey of those available by John McNaught in a discussion of a possible British terminological data bank -which did not materialise. Peter Arthern spoke again about the potential of MT in the European Commission's translation service -although he stressed that other machine aids were more useful: typewriters, photocopiers, microfilms and microfiche -and closed with his vision of the future translator's workstation. And finally, Juan Sager elaborated on the diversity of translation types he could foresee: \"the end user will be offered a wider range of products such as raw machine output for information scanning, controlled language versions of abstracts and other relatively stereotype text forms, human revision of machine output beside the traditional full human translation.\" The next year (1981), the third conference in the series saw a return to machine translation, but this time with an emphasis on practical experience. There was further information about Xerox's application of Systran, using controlled language input; but in particular there were several papers describing the use of Systran in the European Commission -a summary of the Commission's evaluation of the economic use of MT (by Georges Van Slype), the first detailed accounts of what post-editing means in practice (Roy Green, Bernard Lavorel, Francesco Rossi), and how feedback from translators could improve quality (Ian Pigott). Systran figured also in papers from two North American speakers, Stanley Sereda of General Motors, and Dale Bostad from the US Air Force. However, it was not all Systran. There were descriptions of the Meteo system (Benoit Thouin), of the recently launched commercial Weidner system (Michael Hundt), wisely marketed not as a machine translation system but as a computer aid, and of the then recently announced ambitious EUROTRA project (Margaret King), intended to replace Systran with an advanced multilingual fully-automatic system within the decade. As we now know, the project did not result in a working system. As it happened, a cautionary warning about overambitious MT aims was given at the same conference by Margaret Masterman, who stressed the inevitable limitations of any system given our limited knowledge of language and translation. In 1982, the conference went back again to machine aids, specifically to the availability and benefits of using termbanks. Although most of the papers were concerned with the situation in Europe and the problems of European languages, there were also speakers from Japan (the first at Aslib), Malaysia, and India, and a speaker dealing with the problems of Arabic scientific terminology. This conference represented an important stage in the development of terminological research and activity; but such a concentration on one aspect of computerization in the translation field was not repeated in the Aslib series. The alternation of MT and machine aids for translators did not extend beyond these first four conferences. Subsequently, all have combined descriptions of current research in MT, practical applications of and experience with MT systems, the use and availability of various machine aids, reflections (often critical and controversial) on differences and conflicts between MT and unaided human translation, and surveys of general trends and developments in the computer technologies of interest and relevance to working translators. Among the latter in particular, there have been a number of presentations oriented towards translators quite new to the area which have necessarily been at a rather simpler and basic level than other papers in the same conferences. This concern with answering real, practical, basic questions was met successfully, particularly in the early years when the field was new and many translators were antagonistic towards any suggestion that their \"art\" could be 'aided' by computers. For many years, the basic facilities of 'word processors' and other mechanised aids were still considered essential topics. It is noteworthy how much of what was new and unfamiliar is now commonplace: spelling checkers, display of non-English characters, switching from English to French and German style keyboards, facilities for receiving texts by telex and fax (even telephone answering machines were a novelty), and means of storing and printing documents (the modem was also a new development). Several word processors now enabled translators to make their own glossaries, and ALP Systems from Provo, Utah, had recently started marketing a range of translation aids. The move towards the translator workstation was on its way, and Merle Tenney from ALPS (1983) outlined the future prospects, including 'interactive' machine translation (a rough draft presented to users for correction) and facilities for storing and accessing previously translated texts. There were, of course, those claiming that dictation machines and electric typewriters could achieve as much as the use of modern equipment. This was not all surprising in view of the still major problems of different incompatible machines; e.g. even word processors from the same company had different software in other countries, and the almost impossible difficulty of sending data on diskette. (We may complain today about quasi-monopolies in computer hardware and software, but it has made life much easier.) As for the actual use of MT, the main emphasis continued to be the experience at the European Commission with their Systran system (e.g. Ian Pigott in 1983). By now, versions for other language pairs were being introduced: English into Italian, and French into English. Experience with 'rapid' post-editing for those not needing top-quality translation was reported by Elizabeth Wagner (1983), who stressed that Systran \"occupies only a very small place in our overall workload, both at individual level and in the translation service.\" In fact, this low level continued for many years, throughout the 1980s; it was not until the current decade that Dorothy Senez (in the 1994 conference) was able to report dramatic increases in usage, and this primarily for draft translations for administrators and report writers and not within the EC translation service itself. In previous conferences Systran had been the only system operational in large organisations (and in 1978 there were only two language pairs running: for translation from Russian into English, and for translation from English into French.) Now, however, there were also other systems: Muriel Vasconcellos described the Spanish-English and English-Spanish systems at the Pan American Health Organization; Ralph Hawes and Klaus Tschira gave the first presentations (at Aslib) of the Logos system for German to English, there was more on Weidner, and there was a description by Streiff (1983) of the already over a decade old restricted-language system TITUS of the Institut Textile de France. Finally, Ulla Magnusson-Murray (1983) gave the first report of how a translation agency (ITT) had introduced and successfully implemented the Weidner system, with all the organizational changes that necessarily followed. The second half of the 1980s In the next few years, new topics were added to the agenda: the implications of the new technology on the training of translators and the translation profession as a whole (whether free-lancers, part-timers or in organisations), the question of translation quality standards, developments in the still-emerging field of international telecommunications and the still primitive (by today's standards) facilities for electronic mail, developments in optical character recognition, and the perennial problem of handling non-Roman character sets with computers. As for the more distant future, there was a presentation by Raj Gunawardana in 1984 of the latest advances in speech recognition, with no hint that within ten to fifteen years software would be available for the personal computer. This was not surprising, for miniaturisation of computer hardware and widespread use of commercial software was still at an early stage; the IBM XT and AT had yet to reach the marketplace. In the early 1980s, MT system development and operation had been conducted predominantly in North America and Western Europe. There was still research going on in the Soviet Union, but little was known about it -indeed, Soviet activity did not figure in the Aslib conferences. Not until after the break-up of the Soviet Union was there any presentation of a Russian system, the PC-based STYLUS from St.Petersburg (described by Svetlana Sokolova in 1995). By the mid 1980s, however, Japanese developments were beginning to become known, and Nishida and Doshita were invited in 1985 to survey the situation. After a brief description of the problems of handling Japanese (probably essential for Western audiences), and a brief history of Japanese MT research, they reported on where and how research was being undertaken. It was mainly computer manufacturers and not, as in the West, universities and independent companies. Software was being developed for time-sharing computers (e.g. the Fujitsu ATLAS) and networked computers (by NEC and Toshiba). However, very soon afterwards (though unmentioned at the time), there appeared on the market the first of the Japanese-English and English-Japanese systems designed to run on relatively cheap machines -the personal microcomputers manufactured by the companies themselves. The changes were beginning to be evident by the following year (1986) when Peter Whitelock surveyed the situation. There were already many more systems being marketed, still mainly for mainframes but others about to be launched for minicomputers. And, there was another notable development: the Bravice company had purchased Weidner and was selling its PC-based MicroPack English-Japanese system with considerable success in Japan. It appeared that widespread public uptake of MT was about to happen. But it was a false dawn. Sales did not reach a high enough volume for the mass market, and within a few years Bravice had closed its operations (subsequently the Weidner MacroPack systems were sold to Intergraph and marketed under a new name.) However, during the next five years, new Japanese systems appeared at a rate of more than one every year -a valuable update was provided by Makoto Nagao in 1991 -and indeed have continued to do so (at an even higher rate) during the 1990s. Japanese MT activity has equalled and often outstripped that of Europe and North America; but (perhaps for obvious reasons) relatively little of this has been heard about it at the Aslib conferences. By the mid 1980s speakers were beginning to talk about how the new technologies could be integrated into the office environment. It became normal to speak about introducing new technologies gradually and sensitively -a typical example from Jean Datta in 1986: \"There are opportunities for computerisation in the language operations of organisations that do not call for such far-reaching changes in procedures as does MT, but which none the less can improve the flow of work, bringing about at least modest economies and at the same time serve as a training ground for staff who are not yet computer literate. Therefore, the best approach to MT in many organisations will be a gradual, 'layered' introduction of new technologies.\" The dangers of over-hasty mechanisation, particularly in a traditionally non-automated profession of people with predominantly non-technical backgrounds, were being recognised. However, as this quote indicates, the common assumption was still that full MT should be the ultimate aim; machine aids were seen as steps in the process of more or less complete computerisation of the translation process. What was not foreseen was the appearance of the translation workstation, which has shown that genuine human-machine collaboration is both feasible and more suitable for the professional translator. Today, few would argue that computerisation must lead ultimately to translators as mere ancillaries (post-editors) of full MT systems. The problems of selecting the 'best' or most appropriate MT systems for a particular organization was another theme that came to prominence. A number of presentations on this topic included detailed costings and comparisons of the options available at the time (e.g. Isabella Moore in 1988). In truth, the range was then much narrower than today. For most, Systran and Logos were economically out of the question. For many years, the only real option was one of the systems from Weidner. One presentation of this period which remains in the memory for many of the regular Aslib conference goers was the description by Peter Pym in 1988, of the successful use of the Weidner system at Perkins Engines. Some time before, the company had sought to improve the quality of its technical documentation by introducing a 'simplified' language for authors, PACE (Perkins Approved Clear English). The use of a 'controlled' language with Weidner proved a great success -Pym could report major savings in translation costs, particularly when texts were to be translated into a number of languages -and demonstrated to many sceptics that MT was a realistic option even for relatively small organizations. In truth, the impression had often been given (and is often still given) that MT was really only economic in very large scale operations -a typical example was the use of Logos in New Brunswick, Canada, for translating massive quantities of technical documents (described by Ron Fournier in 1989), and there was, of course, the ever-present example of Systran at the European Commission. Large-scale technical translation was clearly seen as the main market by developers, and as far as mainframe systems is concerned it is still. At the end of the decade, Systran and Logos were joined by the long-awaited METAL systems from Siemens -a description of its use at Philips and at Boehringer Ingelheim being given in 1989 and 1992 by Patrick Little and Alain Paillet respectively. One new topic was introduced in the late 1980s, whose significance may well not have been fully appreciated at the time. This was the translation of computer software (Mike Scott in 1988 , Siegrun O'Sullivan in 1989) , the translation of screen interfaces and commands as well as translation of user manuals. The technical and managerial problems were aired, but the time constraints and the huge volumes involved were mentioned only in passing. But development was rapid and by the early 1990s, software 'localization' was beginning to grow into today's major world-wide industry with turnovers of millions of dollars and with its own association (LISA) and its own conferences and workshops. The Aslib conference delegates have been kept informed of general developments from time to time (e.g. Orlagh Neary in 1996), but not in as much depth as perhaps the topic merits. By the later 1980s the personal computer was becoming a familiar tool, and translation aids were being developed or adapted for it (already nearly always for the ubiquitous IBMcompatible systems; Macintosh systems were already in a minority). Out was going slowly the stand-alone special-purpose 'word processor' and in was coming (more rapidly) PC software for word processing, spelling checking, desktop printing, and facilities for on-line access to databases and even MT services (cf. Cay-Holger Stoll in 1987) . Multilingual word processing continued to be problematic, particularly when going between systems from different manufacturers. Nevertheless, it was now possible to describe at length how translators could make practical and realistic use of 'desktop publishing' software (e.g. Rainer Reisenberger 1987), and of modems and international telecommunication networks (e.g. Barry Mahon 1988) . But not all were persuaded by the new aids. There were then (and I suspect still are) many translators convinced that \"dictating is faster than keyboarding\" and would agree with the assertion that \"computers, word processors, modems, fax machines, desktop publishing, even teleprinters and typewriters have little or nothing to do with translating. Translating is converting text from one language to another. That, and that alone.\" (quoting from the 1987 presentation by John Hayes) Trends of the 1990s A paper from a research group at Surrey University in 1989 (by Khurshid Ahmad, et al.) can be regarded, in retrospect, as heralding probably the most important development of the 1990s. This paper concerned the Translator's Workbench Project, funded by the European Commission. Its goal was to integrate at one terminal a set of facilities for multilingual text processing, grammar, style and spelling checking, remote access to termbanks and on-line access to a MT system. The aim of the project was to elicit and quantify the actual requirements of working translators, and to develop methodologies for creating lexicographic and specialised terminology tools. It was a recognition that fully automatic systems -traditional MT -were not appropriate for professional translators. It was the beginning of the practical realisation of what Peter Arthern had envisaged at the first Aslib conference in 1978, a translator's workstation. Within a few years, there were commercial workstations on the market: the Trados Workbench, the IBM Translation Manager, and the STAR Transit workstation. Initially they combined the text processing facilities, desktop printing, and terminology management tools that were already familiar as separate software products. What was still lacking was, however, the component that was to have most impact -the translation memory. The initial research on translation memories was a by-product of MT research, in particular the experiments on statistics-based MT in the late 1980s; later important research on the alignment of bilingual text corpora was undertaken at AT&T, and some in the EU-funded projects -one reported by Stelios Piperides at the 1994 conference, the Translearn project. Once translation workstations had added translation memories to their other facilities, their advantages to the professional translator became much greater. Translators could now make use of previously translated texts, in whole or in part, when and as they wished. The benefits of consistency, avoidance of unnecessary repetition, and higher productivity were quickly apparent. They were being heralded as a revolution, putting to an end at last the threat of inhuman, unfriendly machine translation -as Ian Gordon has argued forcefully in two recent conferences (1996, 1997) . In truth, they were the products long advocated by many in the MT field since the early 1980s. In addition, technological developments were bringing other desirable auxiliary facilities for translators. For the very beginning, input had been problematic unless texts were already in electronic form (rare until the late 1980s); optical character recognition was a top priority. The earliest machines were unreliable and expensive, and improvements came slowly, but today scanners with OCR software are readily affordable. Likewise, speech input was long desirable, particularly for those translators who prefer to dictate translations. The first products came just four years ago from IBM (McCready and Moreau-Johnson 1994), but they are already well established in offices of translators, supplanting older dictation machines. While the translator's workstation and 'translation memory' systems have undoubtedly had the greatest impact on the translation profession, elsewhere the most significant development was the growing number of MT systems designed for personal computers and, in more recent years, systems for the Internet. Globalink launched its first PC-based translation systems at the beginning of the decade; now it is the leading manufacturer in this area in the West, occupying the place previously taken by Bravice-Weidner. With the exception of Globalink (Nigel Burford 1991), the Tovna MTS system (Ami Segal 1990) and the already mentioned Russian system STYLUS, presentations of these systems have not appeared on the programmes of the Aslib conferences -in fact, many have not even been demonstrated in the exhibitions. The numerous Japanese PC-based systems have hardly been mentioned at all. The reason must be the legitimate assumption that professional translators are not interested in these products; their quality is so low that they can make little or no use of them. They see themrightly, in my view -as aids for the casual 'occasional' translator who wants just to get some idea of content and does not want or need a full (professional) translation. Most dramatic of all perhaps has been the huge growth of translation on the Internet. A leading player has been CompuServe (Flanagan 1997), which offered users of its 'forums' the opportunity of translating electronic messages (the system used has been Intergraph's DP/Translator, now Transcend). At the same time, many Japanese MT manufacturers began to produce versions of their software for translating Web pages, and many Western manufacturers began to offer MT services over the Internet (e.g. Systran and Globalink). Finally, in the last year, the Internet service provider AltaVista has been offering (for free, so far) immediate online translations (using the Systran systems). In all these cases, MT is providing a service that would be impossible for traditional human translation. The output may well be poor in quality (and usually is), but it is clear that rapid 'translation' of something that would otherwise be inaccessible and unread is welcomed by an increasingly large number of people. It is an area of growth that the Aslib conference, with its orientation towards the translation profession, has covered weakly as yet. But it is a trend that translators should be aware of. Developments in systems for producing publication-quality translations have been less dramatic, but nevertheless significant. For the larger organization, the integration of translation tools into the working process continued to be a topic of much interest -and rightly so, since it is even more evident now that efficiency and savings as well as job satisfaction demand effective documentation and translation workflow practices in organizations. In a trend that began with Xerox in the late 1970s, many companies are now concerned with the quality of source texts (e.g. Van der Steen 1992 , Lee 1993 , Adriaens 1994) . For some, the solution lies in the use of controlled language for technical documents, allowing closer integration of authoring and translating in total documentation processes. The contrast could not be greater with the implicit naive assumption of the late 1970s that MT could only be a separate operation producing output for post-editing by a traditional translation service. Surveys of MT use (e.g. by Veronica Lawson and Muriel Vasconcellos in 1993) demonstrated that MT systems were being installed world-wide at numerous multinational companies and organizations; the volumes of documentation were running into millions of pages a year. Yet they still represent only a fraction (perhaps less than 10%) of the total world translation activity. The reason continues to be poor quality of 'raw' unrevised MT output, and for all but the largest multinationals, the costs of post-editing are still too great. During the 1990s there has emerged a growing consensus that, in general, the best solution for most companies is not investment in a large-scale general-purpose MT system, involving the added costs of controlled input, post-edited output, etc., but investment in translator workstations with translation memory facilities. From the mid 1980s onwards, the evaluation of MT has been a major topic. In 1978, Knowles had seen it solely in linguistic terms (how translation quality could be improved); but now, economic and operational considerations became equally important. During the 1990s, nearly every conference has had one or more discussions on evaluation methodology (e.g. Durand 1991 , Humphreys 1991 , King 1993 , Lewis 1997 , Maegaard 1997) , and no doubt it will be a frequent topic in the next ten to twenty years. As yet, it has to be admitted, there are no agreed benchmarks or methodologies for evaluating systems, either from the point of view of translation quality or of cost-effectiveness and usability. In the 1980s the translation service of the European Commission put much effort into improvements of its various Systran systems (cf. Fran\u00e7ois Scheresse 1991), and the quality of its 'raw' output is undoubtedly higher now in the late 1990s. There are, however, some inherent limitations (or at best, major obstacles) that, it seems, will always mean that MT can never approach the quality of human translation (Masterman 1981 , Hutchins 1991) . Such limitations have been interpreted as justification for the outright rejection of MT, or at the very least, for questioning the basic aims of MT researchers and manufacturers (e.g. Derrington 1993) . The low quality of cheaper PC-based software does not perhaps help the cause of MT in the least. It is an easy matter to find examples of ridiculously bad 'translations' produced by such systems -and by their older, bigger (and normally better) large-scale systems. This should however no longer be the basis for judging systems. It should no longer be a question of intrinsic 'quality' -i.e. whether a system can do as well as a human -but a question of usability. Can the recipient make use of the output? -either to get directly the essential content from a text in an unknown language, or as the basis (as a draft) for a revised translation of publishable quality, or indeed as a draft for writing a document in a language known imperfectly. Research systems During the latter half of the1980s, the emphasis had gradually turned away from research systems towards concrete applications. There were still from time to time presentations of wholly experimental MT research systems (SUSY at Saarbr\u00fccken, GETA at Grenoble) -in 1986 by Karl-Heinz Freigang and Christian Boitet respectively. Information about the ambitious Eurotra project had been scarce since its launch until an update in 1987 by Peter Lau; and then, in 1991, there was what proved to be the final review of this ill-fated research project (by Doug Arnold and Louisa Sadler) -for many it was confirmation that the project had been misguided from the start. Much MT research, however, did not figure on the conference agendas. For example, there was almost nothing (after Yorick Wilks in 1978) about AI and knowledge-based approaches, even many European projects went unmentioned (e.g. the two Dutch experiments in Utrecht and Eindhoven), and there has been virtually no account of current corpus-based, non-symbolic statistical MT methodologies (except for Yorick Wilks in 1993). Most descriptions of new systems were of ones designed for specific applications, e.g. the Finnish-English system from Kielikone built for Nokia in Finland (Harri Arnola in 1996) . However, one important trend has been present: MT as a component in a larger communication or documentation system, e.g. the multilingual documentation and enquiry service for the BMW car manufacturer in Munich (Johann Haller 1996), the trans-national system aiding communication between European police forces (Edward Johnson 1996), the UMIST project for interactive generation of multilingual texts, where there is no already written source text (Harold Somers 1991) , and the research at Brighton University on the computer-guided composition of multilingual documentation (Donia Scott in 1993 and 1996) . In this category may be put also the more futuristic research on translation of spoken language (e.g. the research at SRI, Cambridge, Hiyan Alshawi et al. 1992 , David Carter in 1995) , where speech recognition and speech synthesis are as much a challenge as translation of 'ungrammatical', elliptical dialogue itself. Speech translation research has had substantial financial backing in the United States, Japan and Germany -although little of this has been heard at the Aslib conferences -but the languages involved are, of course, the familiar 'big three' English, German and Japanese. It will be noted that current research focuses almost always on special-purpose systems within restricted task-specific domains. In principle, and as experience has shown, domainrestricted language is easier to handle automatically than general non-specialised language. It has indeed been argued persuasively by Alan Melby (1996), particularly in his book \"The possibility of language\" (Amsterdam: John Benjamins, 1995), that the very formalism that is necessary for computer manipulation of natural language excludes in itself the possibility of dealing effectively with normal every-day dynamic language. Formalism requires a regulated, standardised 'frozen' form of language, which can be achieved only in controlled situations and/or in narrow scientific and technical domains. It follows that MT is most successful when the source texts are 'normalised' in some respect, but that it is least effective when texts are innovative, exploratory, creative, etc. It follows also that special-purpose domain-specific MT systems are able to produce good-quality translations, but that general-purpose systems normally produce poor quality output. The future role of translators in the changing telecommunications environment was presented in the stimulating and memorable 'vision of the future' by Minako O'Hagan in 1994. She envisaged a world-wide network of translation services and providers, where translators would communicate with their clients and transmit documents and translations across the Internet. It was the natural extension of current developments in electronic mail, home banking, teleshopping, remote access to databases and other information resources, and tele-conferencing. At the same time, through the availability of automatic translation on the Internet, the role of translation itself as a major and crucial feature of global communication would come to be accepted and recognised to a greater degree than ever experienced before. Translators will receive a much higher profile than in the past. People using the crude output of MT systems on their PC systems and over the Internet will come to realise the added value (i.e. higher quality) of professionally produced translations. As a result, the demand for human produced translation will rise, and the translation profession will be busier than ever. Fortunately, professional translators will have the support of a wide range of computer-based translation tools -in particular the translator's workstations -enabling them to increase productivity and to improve consistency and quality. In brief, automation and MT will not be a threat to the livelihood of the translator, but will be the source of even greater business and will be the means of achieving considerably improved working conditions. Diversification Ten years ago, the tenth conference in 1988 was inaugurated by reflections by Juan Sager, who had chaired and opened the first Aslib conference in 1978. He noted the tendency of manufacturers to look for general-purpose systems to meet everyone's needs. They had failed to recognise that different users with different expectations and abilities want different types of systems. Sager foresaw the beginnings of the diversification which is now becoming clearer. He recognised \"at least four functionally different types of MT -as a tool for translators; for monolingual writers; for readers of scientific and technical literature; and for databases\", and he added, \"electronic mail, factual databases, electronic books and journals will never be translated by conventional means; all these forms offer new challenges to machine translation \" For each of these needs, he envisaged the development of different kinds of systems. Today, the discussion of different needs is phrased in terms of (a) assimilation (production of 'rough' versions for readers wanting just the essence of texts in unknown languages), (b) dissemination (production of high-quality texts for potential publication, inevitably with human co-operation at various stages), and (c) interchange (immediate production of 'rough' translations of mail, telephone communication, and access to databases). Nearly all of the types envisaged by Sager are now becoming available; the main exception is translation for drafting, i.e. systems for authors writing in non-native languages. Interactive MT systems enabling monolinguals to compose letters or other short messages in languages they do not know, e.g. with the use of templates, have already been marketed and there has been some research in this area. The market is clearly attractive. Authors who do know the target language (although not fluently) have been aided by 'drafts' produced by existing MT systems, e.g. at the European Commission; the possibility of systems designed for this specific application may well feature in the future. But although, the different needs are now more clearly recognised, diversification of systems is still only at an early stage. The MT systems used for electronic mail are essentially the same systems as those used for more carefully composed printed documents, and inevitably produce very poor output from often 'ungrammatical' input (Flanagan 1996) . For translating Web pages the systems are no more than minor variants of PC software, adapted to deal with HTML scripts. In effect then, systems originally intended for use in the production of domainspecific publication-quality translations (i.e. after post-editing of machine output) are being used for general-purpose 'rough' translations where output quality is of low priority. Likewise systems for authors writing in non-native languages. As mentioned above, interactive MT systems enabling monolinguals to compose letters or other short messages in languages they do not know have been marketed. However, in most cases, use is made of existing systems. For example, administrators in the European Commission who do know the target language (although not fluently) are being aided by 'drafts' produced by existing MT systems. Systems for this specific application may well feature in the future. For most of the 1980s, there was in fact little alternative to the large-scale mainframe MT systems, where either the output had to be heavily revised or the input language had to be 'controlled'. Cheaper PC systems were hardly more than toys -scarcely good enough even for 'rough' translations by undemanding users. There were plenty of good computer aids for translators, particularly for terminology, but incompatibilities of software and hardware, deterred many potential purchasers. The turning point came in the early 1990s, with the appearance of the translator's workstation, more sophisticated PC-based translation software (mainly because of increased storage capacities of modern machines), and the greater accessibility of expanding world-wide electronic communications. As yet diversification has not enlarged the range of languages offered in MT systems. The language coverage of commercial MT (whether PC-based or not) is still extremely narrow. There are systems (in abundance) for most Western European languages (particularly French, German, and Spanish) from and into English, there are some for Scandinavian languages, but there are very few for Eastern European languages (except Russian). There are a large number of systems for translation between Japanese and English, a few for Chinese and English and for Korean and English, and now three commercial systems for Arabic and English (and one for French-Arabic). But elsewhere, particularly for South Asia and Africa, the language coverage is abysmal. Hardly any languages of India and Pakistan (even Hindi, the third most widely spoken language in the world) have not been yet investigated as MT languages. One company in Pretoria is tackling Zulu, Xhosa, and other languages of South Africa; but other African languages (even Swahili) have been utterly neglected. Last year (1997), Harold Somers pleaded for research on the minority languages of the United Kingdom, particularly those of South Asia. An equally good case can be made for other languages, which are unfortunately of little 'commercial' interest. As in the past, it will probably be primarily in the hands of the politicians and businessmen of the countries concerned to support development in this area. There seems to be a regrettable lack of financial support for 'altruistic' research on 'minor' languages, even when the need for translation of scientific and technical literature is urgent for their industrial development. Standards and benchmarks The diversification of applications and the much wider utilization of all kinds of computer-based translation tools and systems bring with them even more urgent demands for soundly based methodologies of evaluation, for product standards and for consumer benchmarks. It is obvious that there can never be a single evaluation metric applicable in all situations. Criteria that are valid for one user or purchaser will not be relevant to another. Nevertheless, it should be possible to reach agreement about what factors companies should take into account when evaluating systems, and it should be possible to provide them with tried and tested methodologies. Some progress has been made by JEIDA in Japan and by EAGLES in Europe, but no widely agreed methodologies have yet emerged. As for the general public, it should be possible to agree on standard labelling of products, so that, for example, a hand-held phrase dictionary cannot be marketed as if it were a translation system. It should be possible to agree on the minimum requirements for a MT system, whatever the quality of the output. For example, a MT system should at least attempt to translate complete sentences and not produce a crude word-for-word rendition. Some proposals for this are being discussed within the International Association for Machine Translation. The need is obvious, and hopefully, in the next few years it will be met. Finally, as speakers at Aslib conferences have recognised, there will be a demand for quality standards for human translation as well. Some criteria may be the same as for MT, e.g. consistency of terminology, readability, comprehensibility, etc., but others will be specific to the higher quality expected and to the particular needs and requirements of customers and clients. Future perspectives What new developments may be expected in the near future? 1. MT systems installed as integral components of standard word processing packages. The 'occasional' need for a 'rough' translation will then be just a click away as one optional function. The languages covered will initially be the familiar ones (English, French, German, and Spanish), but once there are more powerful graphics facilities and non-Roman fonts readily available then we can expect Japanese and Chinese in all WP packages. 2. With speech technologies as integral parts of WP packages (as they surely will be soon) there will eventually be the prospect of speech-to-text input and text-to-speech output for non-English languages as well. 3. Translation systems for electronic mail, Web pages, Internet documents, etc. will be integral components of any software for Internet access, i.e. in most cases already installed on every new personal computer and therefore most probably as another option in word processing packages. 4. For those 'occasional' users wanting a higher-quality translation than their WP or Internet access software can deliver, there will be many competitive services providing postedited MT to the general public, probably specialising by language, subject, and document type. 5. Systems for authors writing in non-native languages; as mentioned already, systems for this specific purpose may well come, the market is clearly attractive. 6. Products integrating automatic summarisation and translation, initially perhaps only as a paid Internet service but eventually perhaps also integrated with word processing packages. For many purposes, users would rather have a translated summary of a foreignlanguage text than a translation of the complete text. Initially, automatic summarisation and translation may be restricted to specific domains and certainly not available in many languages; but eventually general-purpose systems will be developed. 7. Multilingual access and retrieval from databases of factual information, documents, statistical sources, etc. is another desideratum. This may not involve translation as such but multilingual generation from formatted databases (tables, statistics, specifications, records, etc.) Many of the current EU-funded projects are devoted to this area, motivated by the EU policy of widening access to information for all citizens in EU member states. 8. Automatic translation for television programmes. Teletext is already provided for over 80% of TV broadcasts, and MT for some US programmes is already available (so far in Spanish only). Shortly there will be automatic 'translations' of teletext into sign language for deaf viewers. There is no reason why current MT systems could not be adapted for this purpose -interestingly, this application was mentioned ten years ago by George-Michael Luyken in 1988. We can expect viewers to have the option of seeing subtitles in their own language produced automatically as required -the quality will not be high, but it should be acceptable to those who would otherwise understand nothing. 9. Further facilities for the professional translator's workstation -which should be affordable for the independent translator -will include in-built OCR and speech-to-text input, access to a fully automatic translation system, either in the WP package (no. 1) or via a MT service provider on the Internet (no. 4). 10. More companies will develop their own MT systems dedicated to their own particular conjunction of languages and subjects -more companies will follow the examples of Cap Volmac (Van der Steen 1992) and Terence Lewis (at Hook & Hatton) by investing funds for local systems rather than attempting to adapt commercial systems. 11. Finally we may expect speech translation systems within highly restricted domains, e.g. travel information, hotel accommodation, weather reports, etc. In terms of commercial return, such systems would be highly attractive and desirable. Undoubtedly research will continue, but progress will continue to be slow, and the prospects of marketable and operational systems lie still some way ahead in the future. Machine translation and human translation With greater specialisation and diversification of system types we may also expect a change in the general image of computer-aided translation and of human translation itself. There will in the future be a much clearer separation between systems designed and intended for the professional translator and systems intended for general public, scientist or administrator with occasional needs. Hopefully, this will put an end to the unconsidered and unhelpful accusations that MT is \"no good\" or \"no use\", when what is really meant is that it is inappropriate for a particular purpose. In time, it will become common knowledge that automatic translation cannot, unaided, produce anything better than 'rough' (occasionally barely comprehensible) 'translations', that quality of output depends greatly on quality of input, and that 'success' varies markedly between different subjects and different source languages. It will be recognised that for higher quality, e.g. for translations of publishable quality, the solution will remain with human translators, aided by all the computer aids that are appropriate. This 'value-added' translation service will be the future for the translation profession, and those translators who can make efficient and effective use of the latest computer and communication facilities will thrive. Twenty years ago, it was natural for translators to view computer-based translation systems and tools with suspicion and perhaps fear. It was the hope of the organisers of the first conference to open a dialogue between translators and researchers developing MT systems, and to dispel ignorance on both sides. Today, system developers and manufacturers are much more realistic in their aims and more familiar with the limitations of natural language automation, and they are also much more aware of the practical needs of their users. At the same time, many translators are now using computer aids with profit and advantage, particularly translation workstations, and they are increasingly aware that 'poor quality' computer-produced translations do have their places and times, when human translation is not feasible, necessary or readily available. The achievement of mutual respect within the communities of MT and human translation owes much to the continuing influence and impact of the Aslib conferences.",
    "abstract": "Since the first of the T&C conferences in 1978 the field of machine(-aided) translation has seen many changes: from mainframe computers to personal computers and the Internet; from a mainly academic research pursuit to a competitive commercial field; from discussions of 'future potentials' to discussions of actual uses; from a translation profession largely antagonistic to mechanization to one making cost-effective exploitation of computerized translation tools. This presentation will look at the situation 20 years ago, describe the major changes and developments, and consider what the future may bring.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "English",
        "French",
        "German",
        "Spanish",
        "Russian"
    ],
    "numcitedby": "2",
    "year": "1998",
    "month": "November 12-13",
    "title": "Twenty Years of Translating and the Computer"
}