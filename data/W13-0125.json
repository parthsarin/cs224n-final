{
    "article": "We present the first freely available large German dataset for Textual Entailment (TE). Our dataset builds on posts from German online forums concerned with computer problems and models the task of identifying relevant posts for user queries (i.e., descriptions of their computer problems) through TE. We use a sequence of crowdsourcing tasks to create realistic problem descriptions through summarisation and paraphrasing of forum posts. The dataset is represented in RTE-5 Search task style and consists of 172 positive and over 2800 negative pairs. We analyse the properties of the created dataset and evaluate its difficulty by applying two TE algorithms and comparing the results with results on the English RTE-5 Search task. The results show that our dataset is roughly comparable to the RTE-5 data in terms of both difficulty and balancing of positive and negative entailment pairs. Our approach to create task-specific TE datasets can be transferred to other domains and languages. Introduction Textual Entailment (TE) is a binary relation between two utterances, a Text T and a Hypothesis H, which holds if \"a human reading T would infer that H is most likely true\" (Dagan et al., 2005) . Example 1 shows a positive entailment (T entails H 1 ) and a negative entailment (T does not entail H 2 ). (1) T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon, to complete the official renaming of England's Liverpool Airport as Liverpool John Lennon Airport. H 1 : Yoko Ono is John Lennon's widow. H 2 : John Lennon renamed Liverpool Airport. The appeal of Textual Entailment is that it can arguably meet a substantial part of the semantic processing requirements of a range of language processing tasks such as Question Answering (Harabagiu and Hickl, 2006) , Information Extraction (Romano et al., 2006) , or Summarisation (Harabagiu et al., 2007) . Consequently, there is now a research community that works on and improves Textual Entailment technology. In this spirit, the main TE forum, the yearly Recognising Textual Entailment (RTE) Challenge, has created a number of datasets that incorporate the properties of particular tasks, such as Semantic Search in RTE-5 (Bentivogli et al., 2009) or Novelty Detection in RTE-7 (Bentivogli et al., 2011) . At the same time, work on RTE on has focused almost exclusively on English. There is at most a handful of studies on Textual Entailment in other languages, notably German and Italian (Wang and Neumann, 2008; Negri et al., 2009; Bos et al., 2009) as well as a study on cross-lingual entailment (Mehdad et al., 2010) . 1 Consequently, virtually no TE technology is available for non-English languages. What is more, it is not clear how well existing algorithms for English RTE carry over to other languages, which might show very different types of surface variation from English. The same limitation exists in terms of genre/register. Virtually all existing datasets have been created from \"clean\" corpora -that is, properly tokenised, grammatical text, notably Wikipedia. Again, the question arises how well TE algorithms would do on noisier genres like transcribed speech or user-generated content. Arguably, it would benefit the community to have a larger variety of datasets at hand for such investigations. This paper reports our creation and analysis of a German dataset for TE that is derived from social media data, as is produced every day on a large scale by of non-professional web users. This type of data respects linguistic norms such as spelling and grammar less than traditional textual entailment datasets (Agichtein et al., 2008) , which present challenges to semantic processing. We concentrate on a search task on a computer user forum that deals with computer problems: given a problem statement formulated by a user, identify all relevant forum threads that describe this problem. We created queries for a sample of forum threads by crowdsourcing. We asked annotators to summarise the threads and to paraphrase the summaries to achieve high syntactic and lexical variability. The resulting summaries can be understood as queries (problem statements) corresponding to the original posts. The search for relevant posts given a query can be phrased as a TE problem as follows: queries are hypotheses that are entailed by forum posts (texts) T iff the forum post is relevant for the query (Pe\u00f1as et al., 2008) . Plan of the paper. Section 2 defines the task in more detail and describes the rationale behind our definition of the crowdsourcing tasks. Section 3 provides a detailed analysis of the queries that were produced by crowdsourcing. Section 4 assesses the difficulty of the dataset by modelling it with the RTE system EDITS (Kouylekov and Negri, 2010) . Finally we relate our study to prior work and sum up. 2 Creating a German Social Media TE Dataset with Crowdsourcing Rationale As mentioned above, the promise of TE lies in its ability to model NLP tasks. One of the best-established of these tasks is search, which has been a part of the RTE challenges since RTE-5 (Bentivogli et al., 2009) . In this setup, given a query statement and a set of documents, a document is relevant if it entails the query. That is, the documents serve as candidate texts T for a hypothesis H given by the query. We apply this setup to social media texts that discuss computer problems. Our use case is that a user has a particular problem with their machine and wants to retrieve the set of relevant documents from computer problem forums. In terms of the entailment-based search task, the Ts are given by a corpus of German computer forum threads. More specifically, we use the first post of each thread, since an analysis showed that the first post usually contains the problem description. What is missing, however, are plausible queries (i.e., Hs). We create these queries by asking laypersons to summarise posts through Amazon Mechanical Turk (AMT) (Snow et al., 2008) . This involves three steps: Summarisation. Given the first post of a forum thread (T), summarise the content in one sentence (H*). Paraphrasing. Paraphrase H* into another sentence (H) by changing both syntax and lexical choice. Validation. Given original post (T) and paraphrased summary (H), assess if H correctly summarises T. Step 1 maps documents onto potential queries; these queries might however be still very close to the original verbalisation in the document. On the semantic level, we assume that summarisation can lose information, but not create new information; thus, summaries should be entailed by the original texts (Harabagiu et al., 2007) . Step 2 allows that there is an amount of syntactic and lexical variance between T and H that is realistic for a search task. On the semantic level, we assume that paraphrasing preserves information; that is, input and output of this step should generally exhibit a high degree of semantic equivalence. Finally, Step 3 allows us to detect and remove bad queries produced by unmotivated or sloppy turkers. Thus, queries validated by Step 3 will be entailed by the original documents. Crowdsourcing Details We sampled 25 first posts of threads from a corpus of German computer self-help forums as Ts, each for which we generate several Hs. The posts were selected so that their length matches the distribution over lengths for all first posts in the corpus. All 25 posts have a length between 50 and 120 words. Task 1: Summarisation. In the first step, we asked AMT workers to write a concise summary of a forum post, summarising the central points of the original text in a declarative sentence. We also provide an example text with summary. Turkers could mark a text as unsummarisable, but had to indicate a reason. The task was conducted by five turkers for each forum post, leading to 25 * 5 = 125 potential summaries. Two posts were discarded as unsummarisable since they referred heavily to another forum post, which left us with 115 summaries. We paid 0.20 USD for each summary. (Total: 23 USD) Task 2: Paraphrasing. In this task, workers had to reformulate the summaries produced in the first task. They were asked to replace words by appropriate synonyms and to change the sentence structure, while still maintain the meaning of the original sentence. The workers of Task 2 were not shown the original forum posts, only the summaries. Again, there was the possibility to leave the text unparaphrased, indicating a reason. Each sentence was paraphrased by two turkers, resulting in 115 * 2 = 230 paraphrases. We decided to discard four of the 230 paraphrases, including their two input sentences (summaries from Task 1). We found that these input sentences provide overly generic summaries of their posts to be usable. For example, a post which dealt with various strategies to solve pop-up problems in Firefox was summarised as \"Mein Rechner \u00f6ffnet selbstst\u00e4ndig Webseiten [...] .\" (\"My computer opens web pages on its own [...].\"). We paid 0.10 USD for each of the 230 paraphrases. (Total: 23 USD) Task 3: Validation. This task asked workers to judge whether the paraphrased summaries resulting from Task 3 are correct summaries of the problem described in T. 2 Possible answers were (a) perfect summary (\"ps\"); (b) incomplete summary that is missing central concept (\"is\"); (c) no (\"ns\"). We also asked turkers to verify that the paraphrased summaries were complete, grammatical, declarative sentences. Each T/H pair was assessed by 3 turkers who were paid 0.05 USD for each assessment. (Total: 35 USD) Surprisingly, the most frequently chosen category was not \"is\" (41% of all assessments), but \"ps\" (43%). About 16% of the paraphrases summaries are judged as \"ns\". To assess reliability, we computed a confusion matrix. In our three-annotation AMT setup where annotators are not necessarily constant across sentences, we decided to count the three pairwise annotations (a1-a2, a2-a3, a1-a3) for each sentence. Since the order of the annotators is random, we normalised to the order \"ps\" < \"is\" < \"ns\". Table 1 shows the results. Satisfactorily, the diagonal, corresponding to matching judgements, shows the highest numbers. In total, 49% of the judgement pairs agree. The largest group of disagreements is \"ps\"/\"is\"; the number of \"is\"/\"ns\" cases is lower by a factor of two, and the number of \"ns\"/\"ps\" cases smaller by another factor of 2. We interpret these number as indication that the annotation task is fairly difficult, but that there is in particular a large number of clear correct cases. We build on this observation below. Compilation of the Dataset For each T/H pair, Task 3 provides us with three judgements on an ordinal scale with three categories: perfect summary (\"ps\"), incomplete summary (\"is\"), no summary (\"ns\"). The next question is how to select cases of true entailment and true non-entailment from this dataset. Positive entailment pairs. As for entailment, we start by discarding all pairs that were tagged as \"ns\" by at least one rater. The situation is less clear for \"is\" cases: on one hand, hypotheses can drop information Table 2 : Association between AMT assessments and final entailment relations present in the text while preserving entailment; on the other hand, the absence of important information in the summary can indicate difficulties with the original text or the summary. Thus, to keep precision high, we decided to manually check all \"is\"/\"ps\" T/H pairs. The left-hand part of Table 2 shows that in fact, the ratio of proper entailments trails off from almost 100% for \"ps-ps-ps\" to about one third for \"is-is-is\". In total, we obtained 127 positive entailment pairs in this manner. Assessments ps-ps-ps ps-ps-is ps-is-is is-is-is ns-ns-ns ns-ns-is ns-is-is Entailment Y Y Y Y N N N During the extraction, we noted that one of the 23 forum posts did not yield reliable assessments for any of its generated hypotheses and discarded it. Negative entailment pairs. Negative entailment pairs come from two sources. First, \"ns\" T/H pairs are cases where turkers missed the semantic core of the original text. These cases might be particularly informative non-entailment pairs because they are near the decision boundary. For example, one author asks whether a virus can settle down on the motherboard. The corresponding generated hypothesis turned the question into a fact, stating that \"My BIOS has been infected by a virus.\". Again, we checked all pairs with at least one \"ns\" judgement by hand. As the right-hand side of Table 2 shows, we find the same pattern as for positive pairs: perfect non-entailment for instances with perfect agreement on \"ns\", and lower non-entailment ratio for increasing \"is\" ratio. Rejected pairs are e.g. very generic and fuzzy summaries or refer only to a minor aspect of the problem described in the forum. Unfortunately, this strategy only results in 10 negative entailment T/H pairs. The second source of negative pairs are combinations of verified Hs with \"other\" Ts, that is, Ts from which they were not created. In fact, we can pair each of the 137 validated distinct Hs with all other Ts, resulting in 21 * 137 = 2877 additional non-entailment T/H pairs. However, since the domain of computer problems is relatively narrow, a few post topics are so close to each other that generated hypotheses are entailed by multiple texts. While this effect is usually ignored in machine learning (Bergsma et al., 2008) , our goal is a clean dataset. Therefore, we manually checked all cross-pairs with similar topics (e.g. virus attacks) for positive entailment relations. Indeed, we found hypotheses which were general enough to match other texts. We removed 45 such pairs from the negative entailment pairs and added them to the set of positive pairs. In total, we obtained 172 positive and 2842 negative entailment T/H pairs for 22 Ts and 137 distinct Hs. At a cost of 82 USD, this corresponds to an average of 50 cents for each explicitly generated positive pair, but just 3 cents for each T/H pair in the complete dataset. From the 226 AMT-generated pairs, we use 56% as positive pairs and 4% as negative pairs. We discard the remaining, inconsistently judged, 40%. Discussion The three tasks vary in their nature and difficulty. As mentioned above, we paid more for Task 1 than for Task 2, since it involved authoring a text. The amount of time needed for the tasks confirms this assumption: Task 1 took about 80 seconds per annotation, Task 2 only about 60 seconds. In terms of difficulty, Task 1 seems to be the easier one, though: We removed only a small number of post summaries from Task 1, but had to disregard a number of paraphrases from Task 2 (cf. Section 2.3). We believe that two factors contribute to this observation: (a), it is easier to summarise a complete text than to paraphrase a sentence out of context; (b), we deliberately asked workers in Task 2 to introduce as much variance as possible, which can lead to somewhat unnatural statements. Finally, the assessment Task 3 is the fastest one, requiring only about 30 seconds per annotation.  Our results show that both with regard to positive and negative entailment, three consistent judgements are sufficient for an almost perfect guarantee of the respective relation (cf. Table 2 ), but only a comparatively small sample of our data fall into these categories (around 15% for positive and 3% for negative entailment, respectively). Creators of a dataset therefore have the option of either making up for this loss by starting with more initial data, which leads to a higher overall cost, or to perform a subsequent expert-driven manual pass over the inconsistent candidates, as we did. Analysis of Created Data This Section illustrates the properties and problems of each step. Task 1: Summarisation Linguistic properties. Table 3 illustrates some of the phenomena appearing in the summarisation task, which seem to be largely specific to the particular genre (web forum texts) that we consider, while appearing less frequent in standard trainig data like newswire. Example 1/1 shows a typical \"telegram style\" summary which omits determiners and copula; Example 25/1 shows that not all summaries are even grammatical (underlined word). A comparison of examples 1/2 and 1/3 shows that the summaries either retain the personal point of view typically used by the original posts (using first-personal personal or possessive pronouns) or employ generic, impersonal formulations such as (pseudo-)passives. In one example, the AMT worker even cited the author of the original post using the introduction \"Micha fragt, ob [. . . ]\" (\"Micha asks whether [. . . ]\"). Similarly, 12 summaries use interrogative form (Example 25/3) like the original posts even though we explicitly asked the turkers to generate declarative sentences. Finally, example 20/2 illustrates typical writing errors, including the omission of punctuation and the defiance of German capitalisation rules. It is notable that turkers used this style, which is typically used for writing forum posts, even in the rather more formal AMT task environment. It occurs more frequently for original posts with the same style. Arguably, the turkers perceived this as the \"correct\" manner to summarise such posts, as our guidelines did not address this question. Content properties. Most summaries reproduced the original content correctly. The turkers apparently concentrated more on the content, i.e. writing a good summary, than formal task details, resulting, e.g. in interrogative formulations. This is not untypical for crowdsourcing tasks (Chen and Dolan, 2010) . Nonetheless, reproducing the context correctly was not trivial: some forum posts are rambling or vague and difficult to summarise. Summaries of such posts often either (a) do not cover the whole content or (b) are incorrect. Cases (a) lead to assessments of medium reliability in Task 3 (\"H is an incomplete, but valid summary of T\"). Cases (b) lead to negative entailment cases. As intended, the results of Task 1 are significantly shorter than the original texts, with an average length of 11 words (min 3, max 39 words). Often, they use more general wording, e.g. \"Der Prozessor l\u00e4uft schnell hei\u00df.\" (\"The processor runs hot quickly\") for a description containing a concrete temperature. Task 2: Paraphrasing Linguistic properties. In the paraphrasing task, workers were asked to change both syntax and word choice whenever possible. Although texts can contain many content words that are hard to paraphrase (e.g. basic level terms such as table), the problem is alleviated in the software domain where abbreviations and English loanwords that can be substituted easily are frequent (examples 2/1/1, 2/1/2, 10/2/1 in Table 4 ). The most frequent change was the replacement of verbs by synonyms and nouns by synonyms or hypernyms, as in examples 3/3/1 and 9/5/2. Some turkers modified both syntax and lexemes to vary support verb constructions (5/4/2). While these phenomena are all \"generic\" paraphrasing devices that have been observed in previous studies on English and newswire text (Lin and Pantel, 2002; Bannard and Callison-Burch, 2005) , we find two more classes of paraphrasing patterns that are specific to German and the social media domain, respectively. Prominent among German-specific changes are the large number of nominalisations (8/3/2) as well as active/passive switches (13/5/2). Next to the regular passive construction with the auxiliary werden, we often see \"pseudo-passives\" which use lassen combined with the reflexivised verb (4/4/2). As for domain-specific patterns, we frequently observe the alternation of interrogative and declarative sentences (17/3/2) noted before which is caused by the tendency of the original posts to formulate problems as questions. Again, personalised and generic expressions alternate (4/4/2), which typically involves rephrasing first-person statements as third-person or impersonal ones -often though (pseudo-)passives. The quality is generally higher in Task 2 than it is in Task 1. Although we asked the turkers to generate paraphrases by changing both syntax and lexis, they frequently modified just the syntax. However, this is not critical, since the summaries already exhibit varied word choice, so that there is enough variance between T and the corresponding true entailment Hs to avoid oversimplifying the TE task. Content properties. Recall that no context was given in the paraphrasing task to avoid influencing the turkers with regard to vocabulary and syntax. In most cases, context was also not necessary. However, this also meant that some semantic errors occurred as a result of ambiguous formulations in summaries that were propagated into the paraphrase. For example, the author of one forum post explains that a BIOS update has failed and that he is afraid of restarting the computer. The corresponding summary \"Fehlermeldung nach Bios-Update, Rechner trotzdem neustarten?\" (\"Error message after Bios update, restart computer anyway?\") is paraphrased with \"Ich erhalte nach dem Update meines BIOS eine Fehlermeldung, soll ich den PC neu starten?\" (\"I get an error message after the BIOS update, should I restart the PC?\"), which has rather the meaning of restarting the PC in order to overcome the problem. Consequently, the assessment in Task 3 was controversial (ps-is-ns, see Section 2.3) and lead to a rejection of the T/H pair. In the best case, such errors can also lead to clear rejections (ns-ns-ns). A specific problem that we observed was the lack of domain knowledge by turkers. For example, the summary \"Anschluss von einem zus\u00e4tzlichem SATA-Ger\u00e4t . . . \" (\"Connection of an additional SATA device . . . \") becomes \"ich m\u00f6chte Hardware von SATA . . . anschlie\u00dfen\" (\"I want to connect hardware (made) by SATA . . . \"). This is an incorrect paraphrase: SATA is not a hardware manufacturer, but a type of interface. This problem extended to Task 3, where assessments were controversial (ps-is-ns). Finally, some turkers, contrary to instructions, produced summaries of the summaries. These texts became very short and were often marked as \"is\" (valid but incomplete) in Task 3. We observed that it was mostly turkers who already participated in Task 1 who acted in this manner. We feel that there is a tension regarding re-employing workers who participated in previous tasks: quality may profit from their previous training, but suffer from their bias to approach the second task with the same mindset as the first one. Task 3: Validation The output of the validation task allows us to correlate the quality ratings of T/H pairs to their linguistic properties. We observe a broad overlap between assessments of the type \"is\" and hypotheses which are very short or whose content is very general, e.g. due to the usage of hypernyms. Accordingly, T/H pairs which are marked consistently as \"ps\" concern either hypotheses which are relatively comprehensive, or texts which describe rather simple situations. At the opposite end of the scale, T/H pairs with three \"ns\" assessments arise from to propagated errors. T/H pairs marked with all three categories, ps-is-ns, make up only about 3%. These cases frequently refer to posts with complex queries such as users describing a sequence of problems. Such posts are hard to summarise and to evaluate, but are also unlikely search queries. The average length of the Hs selected through Task 3 is 11.4 words (min 5, max 22). In sum, we interpret the three-stage crowdsourcing task as a success: The first two tasks generate a broad variation of potentially true T/H pairs, while the third task enables a filtering of dubious pairs. Although the linguistic quality of the obtained hypotheses shows clear imperfections, the quality of the original texts is equally low: the resulting T/H pairs reflect particularities of the social media domain. Example 2 shows (part of) a T/H pair; note the ungrammaticality in both T and H. (2) T: [...] Ich habe heute alles zusammengebaut, aber aheb folgende probleme. Modelling the Dataset with Textual Entailment Systems In order to evaluate the difficulty of the dataset that we have created, we performed experiments with two different TE engines. We split our dataset into a development and a test set. Both sets are identical in terms of size (1507 T/H pairs) and amount of positive and negative pairs (86 and 1421 pairs, respectively). The first system is EDITS (Negri et al., 2009) , version 3.0. 3 EDITS uses string edit distance as a proxy of semantic similarity between T and H and classifies pairs as entailing if their normalised edit distance is below a threshold \u03b8 which can be optimised on a development set. While additional entailment knowledge can be included, no such knowledge is currently available for German and we use the default weights. The second system is a simple word overlap strategy which approximates semantic similarity through the fraction of H words that also occur in T (Monz and de Rijke, 2001) . Again, pairs are classified as entailing if this fraction is larger than a threshold \u03b8. We preprocessed the data by lemmatising it with TreeTagger (Schmid, 1994) and removing stop words, employing a German stop word list which includes keywords from the social media domain. 4 The thresholds \u03b8 for both systems were set by optimising the F 1 score for positive entailment on the train set. Table 5 shows the results for the word overlap model and EDITS. The very high accuracy values merely reflect the predominance of the negative entailment class; we therefore concentrate on the F-score statistics for positive entailment. We find that edit distance outperforms word overlap with F 1 scores of .44 and .38, respectively. Since the main difference between the two approaches is that edit distance is sensitive to word order, order information appears to be indeed informative: reordering between T and H do not incur costs in the word overlap model, but they do in the edit distance model. Example 3 shows a T/H pair with high word overlap, but negative entailment. It is correctly classified by EDITS, but misclassified by the word overlap model. ( The most direct point of comparison for our dataset is the RTE-5 search pilot (Bentivogli et al., 2009) . The two main differences are language (English vs. German) and genre (newswire vs. social media). We found our dataset to be slightly easier to model. Part of the reason is the somewhat more balanced positive/negative distribution in our dataset: a random baseline achieves an F-Score of 8.4% on RTE-5 and 10.4% on our data. However, the improvement of informed models is also somewhat higher: EDITS without additional knowledge resources achieves 32.6% F-Score on RTE-5 (+24% over the baseline) (Bentivogli et al., 2009) and 44% F-Score on our dataset (+34% over the baseline). We believe that this is due to the greater coherence of our dataset: it deals with just one topic, while the RTE-5 dataset covers ten topics. We also observe that the Hs in RTE-5 are shorter than ours (avg. length 8.75 words vs. 11.4) which presumably leads to worse sparsity problems. Nevertheless, the results on the two datasets for baselines and simple methods are still remarkably similar. Related work In the Textual Entailment community, particularly in the studies who create datasets and resources, there is a strong focus on the English language (Androutsopoulos and Malakasiotis, 2010) . All RTE datasets, the most widely used experimental materials, are in English. A few datasets have been created for other languages. To our knowledge, only an Italian one (Bos et al., 2009 ) and a Spanish one are freely available (Pe\u00f1as et al., 2006) . Datasets for other languages have been created in the context of the CLEF QA Answer Validation and Machine Reading tasks, but do not appear to be available to the general community. We have employed crowdsourcing, a technique whose practice has expanded greatly over the last years (Snow et al., 2008) . It has rarely been used for Textual Entailment, though, since high-quality crowdsourcing relies on the ability to formulate the task in layman's terms, which is challenging for entailment. We avoided this problem by asking turkers to provide summaries and paraphrases in two separate steps. Wang and Callison-Burch (2010) also use crowdsourcing to collect hypotheses for TE. In contrast to us, they do not ask turkers for full summaries and paraphrases, but have them extract facts from texts and create counter-facts from facts by inserting negations, using antonyms, or changing adverbs. Finally, Bernhard and Gurevych (2008) present a study on data that is similar to ours. Their goal is the automatic collection of paraphrases for English questions on social Q&A sites. Employing similar methods to us (e.g., word overlap and edit distance), they achieve very good results. Their task is simpler in that in concentrates on paraphrase relations among statements rather than summarisation relations between texts and statements. Conclusions This paper makes two contributions. The first one is a freely available dataset 5 for Textual Entailment tasks which covers (a) a new language, namely German; and (b), a new genre, namely web forum text. The dataset models a search task on web forums, with short queries as hypotheses and forum posts as text candidates. Being constructed from real social media data, our data is more noisy than existing RTE datasets and shows novels linguistic paraphrasing phenomena such as switches between interrogative and declarative sentences. We consider our dataset to be a test bed for TE algorithms that have to deal with spontaneous and sloppy language, e.g. for other social media areas or on transcribed spoken language. Our second contribution is a crowdsourcing-based procedure to create the dataset which can be applied to other languages and data sources in order to create comparable datasets quickly and at modest expense. The three-step setup that we introduce consists of a summarisation step, a paraphrasing step, and a validation step. This setup guarantees syntactic and lexical variation and makes it possible to detect and remove the sizable portion of the data that consists of queries that are either invalid or hard to judge. The number of summaries and paraphrases can be chosen according to the requirements of the dataset; as for validation, we found that three judgments were sufficient for a final categorisation. An alternative to our rather artificial way to collect data is presented in (Baldwin et al., 2010) , employing web forum structure. We have presented an experiment with two basic TE algorithms which establishes that the difficulty of the dataset is roughly comparable with the RTE-5 Search task testset. However, both algorithms were essentially knowledge-free, and we will conduct experiments with more informed algorithms. We expect the inclusion of lexical entailment knowledge (such as hyponymy relations) to provide a clear benefit. However, the top systems on the RTE-5 Search-Task, where the best result was 46% F-Score (+13% F-Score over edit distance) crucially employed lexico-syntactic paraphrase knowledge \u00e0 la DIRT (Lin and Pantel, 2002) . It remains to be seen how such syntax-based TE algorithms do on our dataset, where we expect parsing results to be substantially more noisy than for traditional RTE datasets. Acknowledgments. This work was supported by the EC project EXCITEMENT (FP7 ICT-287923).",
    "abstract": "We present the first freely available large German dataset for Textual Entailment (TE). Our dataset builds on posts from German online forums concerned with computer problems and models the task of identifying relevant posts for user queries (i.e., descriptions of their computer problems) through TE. We use a sequence of crowdsourcing tasks to create realistic problem descriptions through summarisation and paraphrasing of forum posts. The dataset is represented in RTE-5 Search task style and consists of 172 positive and over 2800 negative pairs. We analyse the properties of the created dataset and evaluate its difficulty by applying two TE algorithms and comparing the results with results on the English RTE-5 Search task. The results show that our dataset is roughly comparable to the RTE-5 data in terms of both difficulty and balancing of positive and negative entailment pairs. Our approach to create task-specific TE datasets can be transferred to other domains and languages.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German",
        "English"
    ],
    "numcitedby": "7",
    "year": "2013",
    "month": "March",
    "title": "A Search Task Dataset for {G}erman Textual Entailment"
}