{
    "article": "Automatic generation of text summaries for spoken language faces the problem of containing incorrect words and passages due to speech recognition errors. This paper describes comparative experiments where passages with higher speech recognizer confidence scores are favored in the ranking process. Results show that a relative word error rate reduction of over 10% can be achieved while at the same time the accuracy of the summary improves markedly. Introduction The amount of audio data on-line has been growing rapidly in recent years, and so methods for efficiently indexing and retrieving non-textual information have become increasingly important (see, e.g., the TREC-7 branch for \"Spoken Document Retrieval\" (Garofolo et al., 1999) ). One way of compressing audio information is the automatic creation of textual summaries which can be skimmed much faster and stored much more efficiently than the audio itself. There has been plenty of research in the area of summarizing written language (see (Mani and Maybury, 1999) for a comprehensive overview). So far, however, very little attention has been given to the question how to create and evaluate a summary of spoken audio based on automatically generated transcripts from a speech recognizer. One fundamental problem with those summaries is that they contain incorrectly recognized words, i.e., the original text is to some extent \"distorted\". Several research groups have developed interactive \"browsing\" tools, where audio (and possibly video) can be accessed together with various types of textual information (transcripts, summaries) via a graphical user interface (Waibel et al., 1998; Valenza et al., 1999; Hirschberg et al., 1999) . With these tools, the problem of misrecognitions is alleviated in the sense that the user can always easily listen to the audio recording corresponding to a passage in a textual summary. In some instances, however, this approach may not be feasible or too expensive to pursue, and a short, stand-alone textual repre-sentation of the spoken audio may be preferred or even required. This paper addresses in particular this latter case and (a) explores means of making textual summaries less distorted (i.e., reducing their word error rate (WElt)), and (b) assesses how the accuracy of the summaries changes when methods for word error rate reduction ar e applied. Summary accuracy will be a function of how much relevant information is present in the sun'mary. Our results from experiments on four television shows with multiple speakers show that it is possible to reduce word error rate while at the same time also improving the accuracy of the summary. Furthermore, this paper presents a novel method for evaluation of textual summaries from spoken language data. .... The paper is organized as follows: In the next : section, we review related work on spoken language summarization. In section 3 we describe our sum: marizer. Next, we present and discuss our proposal for an audio summarization evaluation metric (section 4). In section 5 we describe the Corpus that we use for our experiments and how it was annotated. Sections 6 and 7 describe experixnents on both hu .... man and machine generated transcripts of the audio data. Finally, we discuss and summarize the results in sections 8 and 9. Related work \u2022 \" (Waibel et al., 1998) report results of their summarization system on automatically transcribed SWITCHBOARD data (Godfrey et al., 1992) , the word error rate being about 30%. In a question-answer test with summaries of five dialogues, subjects could identify most of the key concepts using a summary size of only five turns. However, the results vary widely across five different dialogues tested in this experiment (between 20% and 90% accuracy). (Valenza et al., 1999) went one step further and report that they were able to reduce the word error rate in summaries (as opposed to full texts) by using speech recognizer confidence scores. They combined inverse frequency weights with confidence scores for each recognized word. Using summaries composed of one 30-gram per minute (approximately 15% length of the full text), the WER dropped from 25% for the full text to 10% for these summaries. They also conducted a qualitative study where human subjects were given summaries of n-grams of different length and also summaries with speaker utterances as minimal units, either giving a high weight to the inverse frequency scores or to the confidence scores. The utterance summaries were considered best, followed closely by 30-gram summaries, both using high confidence score weights. This suggests that not only does the WER drop by extracting passages that are more likely to be correctly recognized but also do summaries seem to be \"better\" which are generated that way. While the results of (Valenza et al., 1999) are indicative for their approach, we want to investigate the benefits of using speech recognizer confidence scores in more detail and particularly find out about the trade-off between WER and summarization accuracy when we vary the influence of the confidence scores. To our knowledge, this paper addresses this trade-off for the first time in a clear, numerically describable way. To be able to obtain numerical values for summary accuracy, we had our corpus annotated for relevance (section 5) and devised an evaluation scheme that allows the calculation of summary accuracy for both human and machine generated transcripts (section 4). Summarization system Prior to summarizing, the input text is cleaned up for disfluencies, such as hesitations, filled pauses, and repetitions. I In the context of multi-topical recordings we use for our experiments, summaries are generated for each topical segment separately. The segment boundaries were determined to be at those places where the majority Cat least half) of the human annotators agreed (see section 5). Intercoder agreement for topical boundaries is fairly good (and higher than the agreement on relevant words or passages).2 To determine the content of the summaries, we use a \"maximal marginal relevance\" (MMR) based summarizer with speaker turns as minimal units (cf. (Carbonell and Goldstein, 1998) ). The MMR formula is given in equation 1. It generates a list of turns ranked by their relevance and states that the next turn to be put in this ranked list will be taken from the turns which were not yet ranked (tar) and has the following properties: it is (a) maximally similar to a \"query\" and (b) maximally dissimilar to the turns which were already 1 More details about this component and other parts of the summarization system can be found in (Zechner and Walbei, 20oo). 2For details see (Zechner, 2000) . ranked (tr). As \"query\" we use a frequency vector for all content words within a topical segment. The A-parameter (0.0 < A < 1.0) is used to trade off the influence of C a) vs. (b). Both similarity metrics (sire1, sire2) are inner vector products of (_stemmed) term frequencies (see equations 2 to 4); tft is a vector of stem frequencies in a turn; f, are in-segment frequencies of a stem; f, rna= are maximal segment frequencies of any stem in the topical segment, sirnl can be normalized or not. The formulae for tfa (equation 4) are inspired from Cornell's SMART system (Salton, 1971); we will call these parameters \"smax', \"log\", and \u00a2'freq\", respectively. neztturn = argmax(Asima(tn,j,query) tnr,~ -(1 -A) maxsim2 (tnrd, tr,t~)) (1) tr, k siml : tf~tft or [tf=tit~ (2) I /I -I tftztft~ I II I tfi,, = 0.5 + 0.5 /~\" or 1 + logfi., or ~,, (4) Using the MMR algorithm, we obtain a list of ranked turns for each topical segment. We compute this both for human and machine generated transcripts of the audio files (\"reference text\" vs. \"hypothesis text\") .3 4 Evaluation metrics The challenge of devising a meaningful evaluation metric for the task of audio summarization is that it has to be applicable to both the reference (human transcript) and the hypothesis transcripts (automatic speech recognizer (ASR) transcripts). We want to be able to assess the quality of the summary with respect to the relevance markings of the human annotators (see section 5), as well as to relate this \"summary accuracy\" to the word error rate present in the ASR transcripts. The approach we take is to align the words in the summary with the words in the reference transcript (wa). For ASR transcripts, word substitutions are aligned with their \"true original\" and word insertions are aligned with a NIL dummy. That way, we can determine for each individual word wa in the summary (a) whether it occurs in a \"relevant phrase\" and (b) whether it is correctly recognized or a recognition error (for ASR transcripts). We define word error rate as WER = (S + I + D)/(S + I + C) (I=insertion, D=deletion, S=substitution, C=correct). Each word's relevance score r is the average number it occurs in the human annotators' relevant phrases (0.0 < r <_. 1.0). Relevance scores for insertions and substitutions are always 0.0. We choose to define the summary accuracy sa (\"relevance\") as the sum of relevance scores of all n aligned words ~--~\u00b0 r~, divided by the maximum achievable relevance score with the same number of n words somewhere in the text (i.e., 0.0 < sa <_ 1.0). Word deletions obviously do not show up in the summary, but are accounted for, as well, to make the WER computation sound. To better illustrate how these metrics work, we demonstrate them on a simplified example of only two speaker turns (Figure 1 ). The first line represents the relevance score r for each word (the number this word was within a \"relevant phrase\" divided by the number of annotators for that text). In turn 1, \"this is to illustrate\" was only marked relevant by two annotators, whereas \"the idea\" by 3 out of 4 annotators. The second line provides the reference transcript, the third line the ASB. transcript. Line 4 gives the type of word error, and line 5 the confidence score of the speech recognizer (between 0.0 and 1.0, 1.0 meaning maximal confidence). Now let us assume that turn 2 shows up in the summary. The scores are computed as follows: \u2022 When summarizing the reference: Here, the word error rate is trivially 0.0; the summary accuracy sa is the sum of all relevance scores (-6.0) divided by the maximal achievable score with the same number of words (n = 7). l\"hrn 2 has 6 words which were marked relevant by all coders (r --1.0), turn l's highest score is r = 0.75. Therefore: sa2 = 6.0/(6.0 + 0.75) = 0.89. This is higher than the summary accuracy for turn 1: sal = 3.5/6.0 = 0.58(n = 6). \u2022 When summarizing the ASR transcript (\"hypothesis\"): Selecting turn 2 will give sa2 = 0.0\u20222.25 = 0.0 (n = 5). For turn 1, sal = 2.25/(0.75 + 0.5 + 0.5 + 0.5 + 0.0 + 0.0) = 1.0 (n = 6; the sum in the denominator can only use relevance scores based on the aligned words wa which were correctly recognized, therefore the 1.0-scores in turn 2 cannot be used). Turn 2 has WER=6[5=l.2, turn 1 has WER=3/6=0.5. Obviously, when summarizing the ASB. output, we would rather have turn 1 showing up in the summary than turn 2, because turn 2 is completely off from the truth and turn 1 only partially. The fact that turn 2 was considered to be more relevant by human coders cannot, in our opinion, be used to favor its inclusion in the summary. An exception would be a situation where the user has immediate access to the audio as well and is able to listen to selected passages from the summary (see section 1). In our case, where we focus on text-only summaries to be used stand-alone, we have to minimize their word error rate. Given that, turn 1 has to be favored over turn 2, both because of its lower WEB, and because of its higher accuracy with respect to the relevance annotations. In order to increase the likelihood that turns with lower WEB, are selected over turns with higher WEB., we make use of the speech recognizer's confidence scores which are attached to every word hypothesis and can be viewed as probabilities: they are in [0.0,1.0], high values reflecting a high confidence in the correctness of the respective word. 4 Following (Valenza et al., 1999) we conjecture that we can use these confidence scores to increase the probability of passages with lower WEB, to show up in the summary. To test how far this assumption is justified, we correlated the WEB. with various metrics of confidence scores: (i) sum of scores, (ii) average of scores, (iii) number of scores above a threshold, (iv) the latter normalized by the number of all scores, and (v) the geometric mean of scores. Table 1 shows the correlation coefficients (Pearson r) for the four ASK transcripts we used in our experiments (see section 5). To prevent the influence of large differences in turn length, those computations were done for subsequent \"buckets\" of 50 words each. Since in most cases we achieve the highest correlation coefficient (absolute value) for method (iv = avgth) (average number of words whose confidence score is greater than a threshold of 0.95), we apply this metric to the computation of turn-query similarities (sire1 in equation 1). We use the two following formulae to adjust the similarity,scores. (We shall call these adjustments MULT and EXP in the followins.) [mutt] sirn~ = Siml (1 + aavgth) (5) [ezp] sim'l' = simlavgth ~ (6) For both equations it holds that if a = 0.0, the scores don't change, whereas if c~ > 0.0, we enhance the weights of turns with many high confidence scores (\"boosting\") and hence increase their likelihood of showing up earlier in the summary. 5 Even though our evaluation method looks like it would \"guarantee\" an increase in summary accu-4The speech recognizer computes these scores based on the acoustic stability of words during lattice rescoring. 5For EXP, we define 0 \u00b0 ----O. racy when the word error rate is reduced, this is not necessarily the case. For example, it could turn out that while we can reduce WER by \"boosting\" passages with higher confidence scores, those passages might have (much) fewer words marked relevant than those being present in the summary with-~ out boosting. This way, it would be conceivable to create low word error summaries that contain also very few relevant pieces of information. However, as we will see later, WER reduction goes hand in hand with an increase of summary accuracy. 5 Data characteristics and annotation Table 2 describes the main features of the corpus we used for our experiments: we selected four audio excerpts from four television shows, together with human generated textual transcripts. All these shows are conversations between multiple speakers. The audio was sampled at 16kHz and then also automatically transcribed using a gender independent, vocal tract length normalized, large vocabulary speech recognizer which was trained on about 80 hours of Broadcast News data (Yu et al., 1999) . The average word error rates for our 4 recordings ranged from 25% to 50%. The reference transcripts of the four recordings were given to six human annotators who had to segment them into topically coherent regions and to decide on the \"most relevant phrases\" to be included in a summary for each topical region. Those phrases usually do not coincide exactly with speaker turns and the annotators were encouraged to mark sections of text freely such that they would form meaningful, concise, and informative phrases. Three an-\u2022 notators could listen to the audio while annotating the corpus, the other three only had the human generated transcripts available. 2 of the 6 annotators only finished the NewsHour data, so we have the opinion of 4 annotators for the recordings BUCHANAN and GRAY and of 6 annotators for BACK and 19CENT. 6 Experiments on human generated transcripts We created summaries of the reference transcripts using different parameters for the MMR computation: For tf we used \"freq\", \"log\", and \"smax\"; further, we did or did not normalize these weights; finally, we varied the MMR-A from 0.85 to 1.0. Summarization accuracy was determined at 5%, 10%, 15%, 20%, and 25% of the text length of each summarized topical segment and then averaged over all sample points in all segments. Since these were word-based lengths, words were added incrementally to the summary in the order of the turns ranked via MMR; turns were cut off when the length limit was reached. As explained in the example in section 4, the accuracy score is defined as the fraction of the sum of all individual word relevance scores (as de- Table 3 shows the summary accuracy results for the best parameter setting (if=log, no normalization) ~. Experiments on automatically generated transcripts Using the same summarizer as before, we now created summaries from ASR transcripts. Additionally to the summary accuracy, we evaluate now also the WER for each evaluation point. Again, we ran a series of experiments for different parameters of the MMR formula (if=log, smax, freq; with/without normalization). As before, we achieved the best results for non normalized scores and tf=log. We varied a from 0.0 to 10.0 to see how much of an effect we would get from the \"boosting\" of turns with many high confidence scores (see equations 5 and 6). The ExP formula yielded better results than MULl? (Table 4 ), the optimum for ExP was reached for = 3.0 with a WER of 26.6%, an absolute improvement of over 8% over the average of WER=35.1% for the complete ASR transcripts (non-summarized). The summarization accuracy peaks at 0.47, a 9% absolute improvement over the a = 0.0-baseline and only about 5% absolute lower than for reference summaries (Table 4 and Figure 2 ). When we compare the baseline of ~ = 0.0 (i.e., no \"boosting\" of high confidence turns) to the best result (a = 3.0), we see that the WER drops markedly by about 12% relative from 30.1 to 26.6%. At the same time, the summarization accuracy increases by about 18% relative form 0.401 to 0.472. \u00b0If we use non-normalized scores, the value of the MMR-X does not have any measurable effect; we assigned it to be 0.95 for all subsequent experiments.  Results for the MULT formula confirm this trend, but it is considerably weaker: approximately 6% WER reduction and 14% accuracy improvement for c~ = 10.0 over the c~ = 0.0 baseline. An appendix (section 11) provides an example of actual summaries generated by our system for the first topical segment of the BACK conversation. It illustrates how WER reduction and summary accuracy improvement can be achieved by using our confidence boosting method. Discussion The most significant result of our experiments is, in our opinion, the fact that the trade-off between word and summary accuracy indeed leads to an optimal parameter setting for the creation of textual summaries for spoken language (Figure 2 ). Using a formula which emphasizes turns containing many high confidence scores leads to an average WER reduction of over 10% and to an average improvement in summary accuracy of over 15%, compared to the baseline of a standard MMR-based summary. Comparing our results to those reported in (Valenza et al., 1999) , we find that their relative  WER reduction for summaries over full texts was considerably larger than ours (60% vs. 24%). We conjecture that reasons for this may be due to the different nature and quality of the confidence scores, and (not unrelated), to the different absolute WER of the two corpora (25% vs. 35%): in transcripts with higher WER, the confidence scores are usually less reliable (eft Table 1 ). Looking at the four audio recordings individually, we see that the improvements vary strongly across different recordings. We conjecture that one reason for this fact may be due to the high variation in the correlation between WER and confidence scores on a turn basis (Table 5 ). This would explain why, e.g., BACK'S improvements are much stronger than those of the BUCHANAN recording or why there are no improvements for the 19CENT recording. However, GRAY does improve despite its very low absolute correlation. 9 Summary In this paper, we presented experiments on summaries of both human and machine generated transcripts from four recordings of spoken language. We explored the trade-off of word accuracy vs. summary accuracy (relevance) using speech recognizer confidence scores to rank passages with lower word error rate higher in the summarization process. Results comparing our approach to a simple MMR ranking show that while the WER can be reduced by over 10%, summarization accuracy improves by over 15% as measured against transcripts with relevance annotations.  Figure 3 shows the human transcript of this segment which is about two minutes long and consists of 5 speaker turns. Figure 4 contrasts the machine generated summaries for this segment (a) without confidence boosting (a --0.0) and (b) using the optimal confidence boosting (c~ = 3.0, method ExP). Insertions and substitutions are capitalized and marked with I-or S-prefixes. Table 6 compares the relative summary accuracies (sa) and word error rates (WER in %) for these two summaries (average over the 5 sample points from 5% to 25% summary length). Additionally, the turns that show up in the summaries are listed in their ranking order. Table 7 provides the average relevance scores, word error rates, and confidence scores (\"avgth\") for each turn of this topical segment. We observe that the most relevant turn is turn 1 which has, incidentally, also the lowest WER. Whereas in case (a) (o = 0.0), turn 2 is ranked first and therefore dominates the lower relevance and 1 elizabeth: it has been eight months since dan roetenkowski ualked out of a uisconsin federal prison nix months since he left a halfuay house in chicago the former chairman of the house ways and means committee is ready to step back into the public eye 2 elizabeth: the reception was-warm the banquet hall packed with the city's movers and shakers the thirty five dollars a plate invitation referred to rostenkowski an mr. chairman rostenkowski made no reference to his conviction for misusing federal funds only a brief reference to his fifteen months of prison time 3 dan: i graduated from oxford and i really had a rhodes scholarship the past three years have been a constantly challenging time for me change never comes easily and given the circumstances of my situation that has particule~rly true for me at times things have been dosnright bleak and i uouldn't want to wish my experience on my uorst enemy but there were some silver linings i've had an opportunity to read and reflect in a bay that uasn't possible when i was in constant moment in these remarks today i'd like to share some of my conclusions 4 elizabeth: the conclusions did'not duell on the demise of dan rostenkuwski's career but the demise of party politics 5 dan: those who say that the president's political poser has been ueakened by scandal have truly short memories the sad fact is that president clinton has never had a democratic base in congress a group of people uhom one could support the white house on any given issue are not there Acknowledgements We thank the six human annotators for their tedious work of annotating the corpus with topical segment boundaries and relevance information. We also want to thank Alon Lavie and the three anonymous reviewers for useful feedback and comments on earlier drafts of this paper. This work was funded in part by ATR -Interpreting Telecommunications Research Laboratories of Japan, and the US Department of Defense.",
    "abstract": "Automatic generation of text summaries for spoken language faces the problem of containing incorrect words and passages due to speech recognition errors. This paper describes comparative experiments where passages with higher speech recognizer confidence scores are favored in the ranking process. Results show that a relative word error rate reduction of over 10% can be achieved while at the same time the accuracy of the summary improves markedly.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "98",
    "year": "2000",
    "month": "",
    "title": "Minimizing Word Error Rate in Textual Summaries of Spoken Language"
}