{
    "article": "As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form \"observation X is found in model Y \", using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models. Introduction While modern deep neural language models achieve impressive performance on various benchmarking datasets, the question of how this is achieved is gaining increased attention. This line of inquiry includes a new avenue of research: probing. There are many methods to probe a neural network. Among these, diagnostic classification is by far the most common. To probe a neural network in a classification configuration, we specify a classification task that examines an ability (e.g., detecting verb tense). We encode the texts with a deep neural network and apply a post-hoc classifier to the encoded representations. If the classifier can easily predict an attribute, we consider this deep neural network capable of encoding the specified ability. Researchers have expanded the targets of probing classifications to a wide range of abilities including syntax and semantics (Jawahar et al., Figure 1 : Sizes of the datasets in some common probing suites. Depending on the tasks, they vary from hundreds to larger than 10 5 . 2019; Tenney et al., 2019a; Kulmizev et al., 2020; Vuli\u0107 et al., 2020) , discourse (Koto et al., 2021; Zhu et al., 2020; Chen et al., 2019) and commonsense reasoning (Petroni et al., 2019; Lin et al., 2020) . These probing papers are associated with datasets of varying sizes, as shown in Figure 1 . What is a suitable size for probing datasets? Larger training datasets lead to tighter generalization bounds. With other conditions fixed, larger testing datasets allow for higher statistical power in comparing probing models and classifiers. That said, it is neither realistic nor desirable to increase the probing dataset sizes arbitrarily. It is therefore essential to find a balanced size for probing classifications. We propose a framework to formally estimate the data requirements of probing configurations ( \u00a73). Our framework considers the scenario of comparing probing configurations given some data. How many additional data samples may be necessary to reliably reproduce this comparison effect? We propose a novel method to estimate the required data samples by adapting a generalization bound. We evaluate our framework on various probing tasks. First, we verify that the choice of generalization bound agrees with the probing results ( \u00a74.2). On a case study recognizing synthetic Gaussian noise, we verify that larger datasets provide higher statistical power ( \u00a74.3). We evaluate the utility of our framework on a collection of comparison problems ( \u00a74.4 - \u00a74.5), where the probing data sizes recommended by the theoretical framework always supports power larger than 0.8. This paper helps to formalize probing experiments. Our framework can be used by the research community in collecting probing datasets. Related Work Understanding the datasets There is increased research attention paid to the datasets used for deep learning models. One way to study the dataset is to visualize each of the dataset samples in a map. Swayamdipta et al. (2020) mapped the data samples in NLP datasets to regions such as \"hard-tolearn\" and \"easy-to-learn\" using signals observed during the training process. Yauney and Mimno (2021) mapped the difficulty of data samples by the data-dependent complexity (Arora et al., 2019) . Vania et al. (2021) used Item Response Theory (Baker and Kim, 2004) , a statistical framework from psychometrics to describe various attributes related to the difficulty of test set items. Similarly, researchers also describe the effects of datasets as a whole. Several papers attempted through information theory (Pimentel and Cotterell, 2021; Zhu et al., 2021) . Le Scao and Rush (2021) compared the effectiveness of prompts to those of classification samples. When collecting datasets for probing, showing the effect of data is an important goal. Our framework considers the classification data samples, but we do not impose restrictions on the signal to probe in the classification tasks. Probing methods The probing literature has proposed numerous diagnostic methods. This paper focuses on diagnostic classifiers, where post-hoc classifiers predict labels from representation vectors. These vectors can be a unified representation of a sequence (Conneau and Kiela, 2018; Tenney et al., 2019b) , a collection of vectors from different tokens in a sequence (Hewitt and Manning, 2019) , or a pair of carefully set-up vectors that contrast between the \"control\" and the \"treatment\" (Hewitt and Liang, 2019; Elazar et al., 2021) . We defer to Belinkov (2021) for a systematic overview. Note that there are also many probing papers without post-hoc classifiers (Zhou and Srikumar, 2021; Torroba Hennigen et al., 2020; Li et al., 2021) . While many of these do not mention the term \"probing\", they nevertheless probe the intrinsics of deep neural models. In this paper, we consider only classification-based probing methods. Our framework can generalize to unsupervised probing methods in future work. Reliability of tests When all data samples follow i.i.d assumptions, larger datasets allow higher reliability. The reliability of testing is essential in quantitative studies, including medical, societal, and educational contexts (Kraemer, 1992; Drost, 2011; Golafshani, 2003) . Reliability describes how possible the test results can be reproduced. Depending on the forms of these tests, there are many ways to measure reliability. The test-retest reliability (usually quantified by Pearson correlations between the test and the retest results) measures the consistency of the results across time. The internal consistency reliability (usually quantified by Cronbach's \u03b1 (Cronbach, 1951)) measures the consistency of participants responding to a set of items. The inter-rater reliability (quantified by Cohen's \u03ba (Cohen, 1960) or Krippendorff's \u03b1 (Krippendorff, 2018) ) measures the extent of agreements between the annotators. To our knowledge, existing works reflecting on the reliability of model diagnostics rely on repeating the tests on a variety of controlled conditions (Aribandi et al., 2021; Novikova, 2021) . A larger variance in results indicates lower test reliability. The reliability is related to two other attributes -validity and robustness. Validity measures how well the test measures what it intends to measure. In a valid test, the result is right for the right reasons (McCoy et al., 2019; Ravichander et al., 2021) . Robustness measures how well the results of a test can generalize from the experimental setting to realworld settings (Xing et al., 2020; Niu et al., 2020) . Methodology Problem statement Much of probing research considers some form of comparison problem. For example: \u2022 Which deep neural language model encodes certain linguistic signal in an easier-to-extract manner? \u2022 For a neural model, does pretraining with set-ting A support higher probing classification performances than models pretrained on setting B? \u2022 Is the accuracy of a simple probing classifier (e.g., logistic regressor) higher than a more complicated one (e.g., MLP with hidden layers)? These problems are instances of comparison problems, where we compare two probing configurations. We formalize them as follows. Definition 1 (Probing configuration). A probing configuration C consists of {T, E, f }, where T specifies the probing task (e.g., past-vs-present from the SentEval suite), E is the encoder that encodes the text specified by T into representation vectors (e.g., the output from the 11 th layer of BERT_base), and f is the probing classifier. Remark. Task T can be a text-based classification dataset, with either word inputs or sequential inputs. Here we only consider the classification problems with fixed number of classes. For more complex problems (e.g., generalization problems), the generalization bounds need to be adapted. Definition 2 (Comparison problem). A comparison problem consists of a pair of probing configurations, C A = {T, E A , f A } and C B = {T, E B , f B }. Remark. Usually, the two configurations of a comparison problem, C A and C B differ by only one of {E, f } to avoid confounds. A comparison problem can collapse if the two configurations are identical. Recommending the required number of data samples for a collapsed comparison problem is not meaningful. An overview of the framework For researchers collecting a probing dataset, we recommend the following procedure to estimate the probing dataset sizes: 1. Identify a comparison problem by specifying two probing configurations, and collect a small set of data in a pilot study. Using the existing data, run the two probing classifications. Let R 1 and R 2 denote the probing performances, respectively. 2. When |R 1 \u2212 R 2 | is small, it is likely that the comparison problem collapses - \u00a73.6 describes some heuristics to verify. 3. When the comparison problem has a difference in the performances |R 1 \u2212R 2 |, our framework can recommend the data requirements: Plug in |R 1 \u2212R 2 | 2 to the generalization bounds to retrospectively solve for a recommendation of train data size N train . \u00a73.3 elaborates the generalization bounds in probing classifiers. \u00a73.4 presents numerical examples. 4. Without loss of generality, we assume that the train, validation, and test data have relative sizes of \u03b7 : 1 : 1. Then (1 + 2 \u03b7 )N train is the total data requirement. Generalization bounds Machine learning theory literature provides many generalization bounds. These bounds usually occur in the following form: P |R( f ) \u2212 R(f * )| > B(n, \u03b4) < \u03b4, (1) where f is the classifier, R(\u2022) is the risk, n is the number of training data points, and \u03b4 is a hyperparameter. Given n, a generalization bound states that, with a probability of at least 1 \u2212 \u03b4, the risk of the empirically optimal classifier R( f ) differs from the risk of the globally optimal classifier R(f * ) by at most B(n; \u03b4). The risk is usually assumed to refer to the cross entropy loss. We show that several metrics used in probing have bounds with the form as well. Accuracy The most widely used scores to measure the probing performance include accuracy, precision, recall, and F1 score. If we substitute the risk with accuracy, the bounds can apply without loss of generality, modulo two differences: accuracy (etc.) is bounded by B = 1 (whereas the upper bounds for loss could be larger), and is the highest with f * (whereas the loss is the lowest with f * ). Note that most behavioural probes 1 use evaluation metrics in this category. Many structural probes use additional evaluation metrics. We discuss them below. Control tasks It is possible that the probes, as diagnostic classifiers, rely on some irrelevant dataset statistics to boost the performance. To factor out this effect, Hewitt and Manning (2019) proposed to use control tasks. In a control task setting, we need to set up an auxiliary diagnostic classification task, and take the difference of the two classifications. Note that the difference is related to information theoretic terms (Pimentel et al., 2020b; Zhu and Rudzicz, 2020) . Regardless, their formulations involve some intractable terms that have to be empirically ignored. Dependent on the goal of the tasks, there are different ways to set up the auxiliary task. In the part-of-speech (PoS) probing task, for example, Hewitt and Manning (2019) associated each word type to a fixed PoS label. Another example is amnesic probing (Elazar et al., 2021) , which uses iterative null-space projection (Ravfogel et al., 2020) to remove the probing task information from the representations. Theorem 1. The probing results for control tasks are subject to the generalization bounds in the following form: P |R( f ) \u2212 R(f * )| > 2B(n, \u03b4) < \u03b4 (2) Proof. The proofs of all theorems are listed in Appendix B. Minimum description length Recently, Voita and Titov (2020) presented an alternative viewpoint of structural probing based on the minimum description length (MDL). The MDL of a probe or classifier is defined by the sum of (a) the code length required to transmit the data, and (b) the code length required to transmit the model for compressing the data. Voita and Titov (2020) gives two ways to approximate the MDL values: variational and prequential. The variational MDL consists of two terms: the cross entropy loss L( f ), and the KL divergence between the posterior (\u03b2) and prior distribution (\u03b1) of the model parameters \u03b8. M DL var = L( f ) + KL(\u03b2 \u03b8 \u03b1 \u03b8 ) Theorem 2. The probing results of variational MDLs subject to the identical bounds as Eq. 1. The prequential MDL computes the code length required in this \"transmission protocol\". First, transmit the first t 1 data points using random coding. Then, optimize the model with the transmitted data, and transmit the next portion with the new model. The first portion t 1 constitutes of as few as 0.1% of the dataset. M DL pre = t 1 logK\u2212 S\u22121 i=1 logp f i (y t i +1..t i+1 |x t i +1..t i+1 ) = t 1 logK + S\u22121 i=1 R(f i ; n i ) Theorem 3. The generalization bound for prequential MDL takes the following form: P |R( f ) \u2212 R(f * )| > Cn t 1 B(n, \u03b4) < \u03b4, (3) where C is a constant. From generalization bounds to training data requirement To estimate the required number of training data samples n, we can fix \u03b4 and enforce an upper bound on the excess risk |R( f ) \u2212 R(f * ))|. Then the corresponding n would be the required number of training data samples. Following is a numerical example where we consider the textbook finite function space bound 2 : P \uf8eb \uf8ed |R( f ) \u2212 R(f * )| > B 2log 2|F | \u03b4 n \uf8f6 \uf8f8 < \u03b4. Here, we set \u03b4 = 10 \u22128 . In a probing classification configuration C = {T, E, f }, the encoder E produces vectors with D = 768 dimensions and f is a logistic regressor. Additionally, we assume that the D + 1 weight parameters in f are stored in 32-bit floating point numbers 3 , so each weight parameter takes 2 32 possible values. Then the probing classifier constitutes a finite space with cardinality |F| = 2 32 \u00d7 (D + 1). When there are n = 65, 536 training data points, with probability of at least 1\u221210 \u22128 , the empirically optimal accuracy is different from the global minimum by at most 0.039 for D = 4, 096 (InferSent) classifications. More importantly, we can also plug in an expectation on the generalization bound to retrospectively solve for the training data requirement. For example, a bound of 0.05 requires N = 40k i.i.d data samples at D = 4, 096. If the datasets for both probing configurations in a probing classification are sufficiently large, the generalization bounds would be sufficiently small, so that the result of the comparison problem is reliable. As a heuristic, we let the bound be |R 1 \u2212R 2 | 2 , where R 1 and R 2 are the probing performances from the existing datasets. A tighter bound (e.g., |R 1 \u2212R 2 | 10 ) requires more data samples (hence larger statistical power) as well as higher expectation for budgets. We consider |R 1 \u2212R 2 | 2 to be a balanced choice. Following are some justifications. In the most ideal case, both R 1 and R 2 are the true global minima R(f * ) 1 and R(f * ) 2 , then the comparison results will remain consistent regardless of the number of data samples. In the less ideal case, both R 1 and R 2 are the empirical minima R( f ) 1 and R( f ) 2 , then a generalization bound of |R 1 \u2212R 2 | 2 guarantees that the relative preference in the comparison will remain consistent (yet the scale of the comparison may fluctuate). We expect that most probing classifiers resemble this scenario, since they reach almost perfect training accuracies. In the most unfortunate case, R 1 and R 2 deviate from the empirical minima. The extent they differ contributes to the randomness. While the scales of the empirical imperfectness remain unknown, one can consider some heuristics reduce this imperfectness. First, a probing classifier with higher accuracy tend to have smaller empirical imperfectness, hence smaller unknown instability. Second, identifying the collapsed comparisons helps reduce the uncertainties introduced by the classifier imperfectness. Power analysis We use power analysis to evaluate the reliability of our data recommendations. For a statistical test, the power is the probability of correctly rejecting the null hypothesis. In the context of this paper, we compare the reliability of the prediction results provided by two probing configurations, C A and C B . The hypothesis is stated as follows. H 0 : On a test set {x i } M i=1 , the results f A and f B are not significantly different. To accept or reject H 0 on the two probing classifiers, one can apply the McNemar's test (McNemar, 1947) , which checks if the \u03c7 2 statistic is significant. The \u03c7 2 can be computed as \u03c7 2 = (p 01 \u2212p 10 ) 2 p 01 +p 10 , where p 00 , p 01 , p 10 , p 11 are the probabilities specified by the contingency table (Table 1) . Card et al. ( 2020 ) described a framework that estimates the power by simulation. One repetitively samples a portion of test data and computes \u03c7 2 . The portion of simulations with significant \u03c7 2 is taken as the estimated power. Empirically, one runs multiple classifications with distinct random seeds to increase robustness. To account for multiple classifications, we run the simulations of Card et al. (2020) for each random seed, and then count the total number of significant simulations to compute the power. Usually, we expect that a reliable decision to reject the null hypothesis should have a statistical power of at least 0.8. f B incorrect f B correct f A incorrect p 00 p 01 f A correct p 10 p 11 Detecting collapsed comparison problems When we have data for a comparison problem from a \"pilot study\" and observe very small classification performance differences (e.g., of 0.5%), we might fall back to the null hypothesis -that the comparison problem collapses -in this case, increasing the data size does not \"uncollapse\" this comparison problem. Here we describe some heuristics to increase the confidence of detecting a collapse. In our experiments, our data are subsampled from a larger dataset, so we can test if a probing configuration collapses by repeatedly subsampling the data, and run statistical tests. In the real-world, this is similar to running multiple \"pilot studies\" and collecting small-scale probing data, repeatedly. If the probing configurations output almost indistinguishable results, one can infer that the probing configuration collapses. Alternatively, one can consider this augmentation method based on cross validation folds. For each dataset in a comparison problem, we divide it into, e.g., 6 folds. For each of i = 1..6 runs, take Fold i as the validation split, Fold (i + 1) mod 6 as the test split, and the rest as the train split. Considering the probing classification results of all 6 runs can lead to higher confidence. Experiments Data and Models Probing task We run probing classifiers on several classification tasks in one of the largest existing probing suites, SentEval (Conneau and Kiela, 2018) : Past_present (tense prediction), bi-gram_shift (whether two words are flipped in a sentence), and coordination_inversion (whether two sentences are flipped) are binary classification tasks with 120k samples per class. Sentence_length con-tains 6 classes with 12k samples per class. To test the data requirements, we stratify sample subsets with {2 7 , 2 9 , 2 11 , 2 13 , 2 15 } training data samples per class, where applicable 4 . Encoders \u2022 BERT (Devlin et al., 2019) is a contextualized language model. We take the multilingual BERT base model. \u2022 SBERT (Reimers and Gurevych, 2019) encouraged semantically similar sentences to be mapped to nearby vectors in the representation space. \u2022 GloVe (Pennington et al., 2014 ) is a static word embedding model. It maps each token to a fixed, 300-dimensional vector. We average all embedding vectors of a SentEval sequence as the input representation. \u2022 InferSent (Conneau et al., 2017 ) is a contextualized language model. It processes the GloVe embeddings with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with 2,048 hidden dimensions. Probing classifiers We use a logistic regressor and a multilayer perceptron (MLP) with 20 hidden units ( \u00a74.6) as probing classifiers. In addition, we run several MDL probes, whose results are described in Appendix C.5. Verifying the theoretical bounds We run probing classifications using a collection of subsets. Each subset is subsampled in a stratified manner from the dataset. We run 5 probing classifications with different random seeds on each subset. To qualitatively examine the extent that the generalization bounds agree with the probing classifications, we plot both the empirical and the theoretical margins. Figure 2 shows an example. Appendix C.1 contains additional plots. The empirical classification results reside within the theoretical margins, except for an outlier classification trial -this is the classification suboptimality, and we extend the discussion in Appendix D.1. Larger datasets support higher power Intuitively, adding noise into the representation vectors makes it harder to decode the referen- tial attribute. In this case study, we add Gaussian noise drawn from N (0, \u03c3 2 ), where \u03c3 2 \u2208 {.01, .03, .1, .3, 1, 3}, and compare against the probing classification with the original representations. Figure 3 shows the effect of noise on a configuration. Appendix C.2 contains additional figures. Adding noise with a larger scale results in a configuration that is easier to distinguish. In addition, a larger training dataset usually leads to a higher power to distinguish the configurations. This case study shows that we can verify the data requirement by incrementally collecting larger datasets for comparisons until we have sufficient power. For example, on the tense prediction task, distinguishing GloVe embeddings from its counterpart with N (0, 0.1) noise, 1, 024 testing data samples is sufficient to lead to 0.8 power. However, the same comparison with N (0, 0.03) noise requires up to 16, 384 testing data points. The scale of Gaussian noise constitutes a spectrum. When we keep reducing the Gaussian scale, the comparison problem becomes more datahungry. This leads to a question: where, on the \"min-max\" spectrum, do some other comparison settings (e.g., comparing between encoders) reside? In subsequent case studies, we verify that the numbers predicted by learning theory bounds have sufficient power. Comparing to corrupted encoders In this case study, we finetune the BERT models on WikiText 5 sentences with scrambled tokens for 200 steps. Table 2 shows the recommended N train values in the probing comparisons with corresponding \"pilot data\" (subsampled) sizes. As shown in Figure 4 , the probing datasets with sizes no fewer than N test = 256 (i.e., N train = 1024) have sufficient power, and all recommended values fall within the \"sufficient-power\" range. 4 given different subsample sizes. Comparing between encoders In this case study, we compare pairs of configurations containing the same task, data, and probing classifier but different encoders. Figure 5 Comparing between classifiers Here, we compare two probing configurations with different classifiers: LogReg vs. MLP with one hidden layer of H = 20 neurons. Although the two configurations involve the same task, they have different training data requirements 6 . We take the larger one as the recommendation. Table 4 recommends N train that are larger than the SentEval dataset sizes. These numbers are actually not necessary -Figure 6 shows that N test = 16.4k (corresponding to N train = 65.6k) is still insufficient to distinguish the results of the two probing classifier configurations on the bigram_shift task. There is insufficient evidence to reject the null hypothesis. In other words, the comparison problem between LogReg vs. MLP on T = bigram_shift collapses 7 . The exceedingly large data recommendations are meaningless. 6 given different subsample sizes. Discussion What does a high accuracy entail? Our framework implicitly considers the probing classification performances, but the causal relationship between, e.g., the accuracy, the data requirements, and the reliability can be explored further in future frameworks. A high probing accuracy indicates a small empirical risk R(f ). This could result from a small |R(f ) \u2212 R( f )| (the probe \"learns the task\"), or a small R( f ) \u2212 R(f * ) (the distribution of the data samples represent the \"true distribution\" well). The two possibilities resemble the dichotomy raised by Hewitt and Liang (2019) , but do they describe the same phenomenon? We leave this as an open question to future researchers. On the stability of theoretical recommendations How stable are our theoretical recommendations? For those comparisons with sufficient evidence to reject the null hypotheses, the recommended N train sometimes varies (e.g., at N test = 4096 in Table 3 ). This is brought in by the suboptimality of several probing classifications. We extend the discussions about how to interpret and reduce classifier suboptimality in Appendix D.1. Future methods to estimate data requirement may improve the stability. Cross-task comparison problems Our framework does not consider cross-task comparison, i.e., when comparing C A = {T A , E A , f A } vs. C B = {T B , E B , f B } where T A = T B , because McNemar's test requires pairwise data. Alternative power tests would be necessary to consider cross-task comparison problems. We leave this to an open problem for future research. Why not just collect as much data as possible? We argue in favor of knowing how many data samples we need, instead of directly collecting as many samples that budgets allow. The two views resemble the \"top-down\" vs. \"bottom-up\" research approaches mentioned in Bender and Koller (2020) . Practically, our experiments show that many comparison problems do not need as many data samples as the sizes of some existing large probing datasets. A \"recipe\" for probing datasets To systematically probe the linguistic abilities of neural networks, many more datasets need to be collected. To make the probing dataset collection procedure systematic, a complete \"recipe\" would be beneficial. Several recent papers called for this goal (Ethayarajh and Jurafsky, 2020; Rodriguez et al., 2021) . Our framework is one component of such a recipe, by quantifying questions of dataset sizes. Additional components for future work include quantifying the label distributions and the inherent 'difficulties' of samples. Conclusion This paper presents a novel framework to estimate the data requirements for probing experiments. This framework uses generalization bounds from formal learning theory to determine minimum training set sizes. In a series of comparison problems, we verify that our recommendations provide sufficient power. Our framework describes an actionable procedure to double check if an experiment needs additional data samples to be scientifically meaningful. Additionally, this paper calls for further attention to complete a systematic methodology in evaluating probing datasets and methods. A Probing is a unique classification problem The learning theory literature provides a rich collection of theories for classification. One might consider that these theories can directly apply to probing classifiers, but we argue for the alternative. Compared to conventional classifiers, the probing classifiers differ in many aspects. Following are some of them. Goals. Conventional classifiers try to reach high performance on both the experimental and the realworld data distribution. Probing classifiers, while also maximize the probing performance, aim at quantifying the \"easiness to decode\" from the input representations (Belinkov, 2021) . Therefore, some papers (e.g., Hewitt and Liang (2019) ) argued in favor of selectivity. A highly selective probe should output results that differ a lot between hardto-decode and easy-to-decode representations. Models. In general, conventional classifiers use models with many more parameters than probing classifiers. Researchers have raised several concerns for larger probing classifiers. First, larger probing classifiers may \"learn the task\", introducing a confounding factor in the result interpretation: a high probing classification performance could result from the probe itself \"learns the task\". Hewitt and Liang (2019) raised this hypothesis, and Zhu and Rudzicz (2020) confirmed from an informationtheoretic perspective. Second, larger probing classifiers require more data to train, which slows down the diagnosis procedure. Ideally, the computation effort spent in diagnosis should be much smaller than training the neural models. Datasets. The data to train a conventional classifier should be abundant, so the classifier could learn sufficient inductive bias that can generalize beyond the experimental conditions. The data to train a probing classifier, however, should contain a collection of specific \"test cases\", covering the \"corner cases\" of the deep neural models, akin to the diagnostic suites in software engineering (Ribeiro et al., 2020) . Considering the differences, it is necessary to formulate a framework to study the validity of probing rigorously. Adapting the tools in machine learning theory can be a good start. Then, with probability B Proofs of theorems (1 \u2212 \u03b4 o )(1 \u2212 \u03b4 c ), we have |A o ( f ) \u2212 A c ( fc ) \u2212 (A o (f * ) \u2212 A c (f c * )) | \u2264 B o + B c . In other words, a bound with the same form, hence the same convergence rate, as the count-based metrics still applies to the results of control tasks. Theorem 2. The probing results of variational MDLs subject to the identical bounds as Eq. 1. Proof. The generalization error of variational MDL is bounded by that of R(\u2022), plus the estimation uncertainty of KL(\u03b2 \u03b8 \u03b1 \u03b8 ). In a Bayesian network implementation, the KL(\u03b2 \u03b8 \u03b1 \u03b8 ) can be acquired with less than 2e-3 variance (Molchanov et al., 2017) , bringing in a negligible additional uncertainty. In short, the generalization of variational MDL is bounded by the cross entropy term. Theorem 3. The generalization bound for prequential MDL takes the following form: P |R( f ) \u2212 R(f * )| > Cn t 1 B(n, \u03b4) < \u03b4, (3) where C is a constant. Proof. Following likewise analysis, the generalization error of individual loss term is bounded by (n i ) = R(f i ; n i ) \u2212 R(f i * ) \u2264 B(n i , \u03b4) with probability of at least 1 \u2212 \u03b4. Using a union bound, the error of summing up all these terms is bounded by the sum of all individual bound. The error bound of prequential MDL is dominated by the first a few terms (i.e., the cross entropy losses with n = {0.1%, 0.2%, ...}N ). Remark. Naturally, the theoretical bounds for prequential MDL appear \"looser\" than the bounds of previous metrics. C Additional experiment details C.1 Theory bounds vs experiment plots We include additional theory vs. experiment plots in Figure 7 . The purple regions represent the em- pirical margin (mean \u00b1 stdev), while the lines represent the margins predicted by the empirical mean \u00b1 the learning theory bound. C.2 Power vs scale of noise plots We include additional power vs. scale of noise plots in Figure 8 . C.3 Results on additional tasks We list some results of additional tasks here: \u2022 Table 5 and Figure 9 for T =sentence_length, BERT vs InferSent, f =LogReg. C.4 Hyperparameter configurations We use Ray Tune to find the optimal hyperparameters for training. The search space include: \u2022 Learning rate: 1e-4, 5e-4, 1e-3, 5e-3, 1e-2 \u2022 Batch size: 8, 16, 32, 64 \u2022 Number of epochs: we set it to 50. We stop running when the validation loss reaches a plateau for 5 epochs. Then, we report the result from the epoch with the highest validation accuracy. We use pytorch to implement the models, and Adam (Kingma and Ba, 2014) to optimize. To reduce the training time, we cache the representation vectors. The runtime is about one minute per 200 training data points. Our analysis scripts are available at https://github.com/ SPOClab-ca/probing_dataset. C.5 Other probing methods To show the generalizability of our framework, we extend the experiments to two probing classifiers motivated by minimum description lengths: variational and prequential MDL probes (Voita and Titov, 2020) . For the variational MDL probe, its results are affected by the arbitrary choice of prior (Pimentel et al., 2020a) . Empirically, when we apply a uniform prior, the variational MDL usually   degenerates 8 , resulting in 0.5 accuracy. To alleviate this problem, varying of hidden layers and neurons in the probing classifiers is beneficial. For the prequential MDL probes, the results depend on the input sequence of data. Lovering et al. (2021) mentioned that the early steps sometimes produce cross-entropy losses that are larger than the uniform coding codelength. We also observe this effect, especially when the early steps contain imbalanced data. D Additional discussions D.1 The suboptimality of probing classifier results The probing classifiers are usually imperfect. Due to the presence of, e.g., degenerative runs and local 8 The classifiers output the same label for all inputs. E Current sizes of some probing datasets This section surveys some commonly used probing datasets, as well as their sizes. Table 8 lists the number of classes and the total number of samples. As listed in the table, most probing classification tasks contain more than enough data for, e.g., comparing BERT vs. InferSent. Regardless, we recommend that future researchers to consider the data requirements for reliability when collecting probing datasets. Here is how we count the numbers of data samples: For SentEval (Conneau and Kiela, 2018) , each data sample contains a text sequence and a label. Universal Dependencies (McDonald et al., 2013) contains rich annotations, and have been used as, e.g., the part-of-speech tagging probing task (Pimentel et al., 2020b) . Here we count the number of words in the train, validation, and the test set respectively. For BLiMP (Warstadt et al., 2020) , there are multiple \"phenomenon\" categories for each task, with 1,000 pairs in each phenomenon. The oLMpics (Talmor et al., 2020) suite splits the task datasets train and test divisions, and we list the numbers of both.",
    "abstract": "As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form \"observation X is found in model Y \", using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.",
    "countries": [
        "Canada"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "2",
    "year": "2022",
    "month": "May",
    "title": "On the data requirements of probing"
}