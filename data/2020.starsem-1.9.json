{
    "article": "There is growing evidence that the prevalence of disagreement in the raw annotations used to construct natural language inference datasets makes the common practice of aggregating those annotations to a single label problematic. We propose a generic method that allows one to skip the aggregation step and train on the raw annotations directly without subjecting the model to unwanted noise that can arise from annotator response biases. We demonstrate that this method, which generalizes the notion of a mixed effects model by incorporating annotator random effects into any existing neural model, improves performance over models that do not incorporate such effects. Introduction A common method for constructing natural language inference (NLI) datasets is (i) to generate text-hypothesis pairs using some methodcommonly, crowd-sourced hypothesis elicitation given a text from some existing resource (Bowman et al., 2015; Williams et al., 2018) or automated text-hypothesis generation (Zhang et al., 2017) ; (ii) to collect crowd-sourced judgments about inference from the text to the hypothesis; and (iii) to aggregate the possibly multiple annotations provided for a single text-hypothesis pair into a single label. This final step follows common practice across annotation tasks in NLP; but for NLI in particular, there is growing evidence that it is problematic due to disagreement among annotators that is not captured by the probabilistic outputs of standard NLI models (Pavlick and Kwiatkowski, 2019) . One way to capture this disagreement would be to directly model the variability in the raw annotations. But this approach presents a challenge: it can *Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/ by/4.0/. be difficult to assess how much disagreement arises from disagreement about the interpretation of a text-hypothesis pair and how much is due to biases that annotators bring to the task. Such biases can be extreme. For instance, Figure 1 plots the distribution by annotator of [-50, 50] ratings-with -50 clear contradiction and 50 clear entailment-for the same 20 NLI pairs in Pavlick and Kwiatkowski's dataset. Despite describing responses to the same items, the distributions are quite variable, suggesting variability in how annotators approach the task. This difference in approach may be relatively shallow-e.g. given some true label (or distribution thereon), annotators merely differ in their mapping of that value to the response scale-or they may be quite deep-e.g. annotators differ in how they interpret the relationship between texts and hypotheses. We investigate both of these possibilities within a mixed effects modeling framework (Gelman and Hill, 2014) . The core idea is to incorporate annotator-specific parameters into standard NLI models that either (i) merely modify the output of a standard classification/regression head or (ii) modify the parameters of the head itself. These two options correspond to the mixed effects modeling concepts of random intercepts and random slopes, respectively. For the same reason that such random effects can be incorporated into effectively any generalized linear model in a modular way, our components can be be similarly incorporated into any NLI model. We describe how this can be done for a simple RoBERTa-based NLI model. We find (i) that models containing only random intercepts outperform both standard models and models containing random slopes when annotators are known; and (ii) that when annotators are not known, performance drops precipitously for both random effects models. Together, these findings suggest that those building NLI datasets should provide annotator information and that those developing NLI systems should incorporate random effects into their models. Extended Task Definition In the standard supervised setting, NLI datasets are (graphs of) functions from text-hypothesis pairs T i , H i \u2208 \u03a3 * \u00d7 \u03a3 * to inference labels y i \u2208 Ywhere Y is commonly {contradicted, neutral, en-tailed} or {not-entailed, entailed}, but may also be a finer-grained (e.g. five-point) ordinal scale (Zhang et al., 2017) or bounded continuous scale (Chen et al., 2020) . The NLI task is to produce a single label from Y given a text-hypothesis pair. We extend this setting by assuming that NLI datasets are (graphs of) functions from texthypothesis pairs and annotator identifiers a i \u2208 A to inference labels and that the NLI task is to produce a single label given a text-hypothesis pair and an annotator identifier. A particular model need not make use of the annotator information during training and may similarly ignore it at evaluation time. Though many existing datasets do not provide annotator information, it is trivial for a dataset creator to add (even post hoc), and so this extension could feasibly be applied to any existing dataset. Models We assume some encoder that maps from T i , H i \u2208 \u03a3 * \u00d7 \u03a3 * to x T i , x H i \u2208 R M \u00d7 R N independently of annotator a i , and we focus mainly on the mapping from z i \u2261 x T i , x H i and a i to y i . We consider two types of model: one containing only annotator random intercepts and another additionally containing annotator random slopes. The first assumes that differences among annotators are relatively shallow-e.g. given some true label for a pair (or distribution thereon), annotators have their own specific way of mapping that value to a response-and the second assumes that the differences among annotators are deeper-e.g. annotators differ in how they interpret the relation between texts and hypotheses. This distinction is independent of the labels Y: regardless of whether the labels are discrete or continuous, random effects can be incorporated. In the language of generalized linear mixed models, the link functions are the only thing that changes. We consider two label types: three-way ordinal and bounded continuous. Annotator random intercepts amount to annotator specific bias terms \u03c1 a i on the raw predictions of a classification/regression head. Unlike standard fixed bias terms, however, what makes these terms random intercepts is that they are assumed to be distributed according to some prior distribution with unknown parameters. This assumption models the idea that annotators are sampled from some population, and it yields 'adaptive regularization' (McElreath, 2020) , wherein the biases for annotators who provide few labels will be drawn more toward the central tendency of the prior. Random intercepts for categorical outputs can take two forms, depending on whether the model enforces ordinality constraints-as linked logit models do (Agresti, 2014) -or not. Since most common categorical NLI models do not enforce ordinality constraints, we do not enforce them here, assuming that the model has some independently tunable function h \u03b8 : R M \u00d7 R N \u2192 R |Y| that produces potentials for each label and that: f (y i | z i , \u03b8, \u03c1 a i ) = softmax (h \u03b8 (z i ) + \u03c1 a i ) where \u03c1 a i \u223c N (0, \u03a3) with unknown \u03a3. Random intercepts for continuous outputs are effectively shifting terms on the single value predicted by some independently tunable function h : R M \u00d7 R N \u2192 R. If the continuous output is furthermore bounded, a squashing function g is necessary. In the bounded case, we assume that the variable-scaled to (0, 1)-is distributed Beta (following Sakaguchi and Van Durme, 2018) with mean \u00b5 i and precision \u03bd i = exp (\u03c1 a i 1 + \u03bd 0 ). \u00b5 i = g (h \u03b8 (z i ) + \u03c1 a i 2 ) \u03b1 i ; \u03b2 i = \u00b5 i \u03bd i ; (1 \u2212 \u00b5 i )\u03bd i f (y i | z i , \u03b8, \u03c1 a i ; \u03bd 0 ) = Beta(y i | \u03b1 i , \u03b2 i ) where \u03c1 a i \u223c N (0, \u03a3) with unknown \u03a3. This implies that \u03bd i \u223c log N (\u03bd 0 , \u03c3 2 11 ) with unknown \u03bd 0 . The precision parameter \u03bd i controls the shape of the Beta: with small \u03bd i , a i tends to give responses near 0 and 1 (whichever is closer to \u00b5 i ); with large \u03bd i , a i tends to give responses near \u00b5 i . Annotator random slopes amount to annotatorspecific classification/regression heads h \u03c6 i . We swap these heads into the above equations in place of h \u03b8 . As for the random intercept parameters, we assume that the annotator-specific parameters \u03c6 i , which we refer to as the annotator random slopes, are distributed \u03c6 i \u223c N (\u03b8, \u03a3) with unknown \u03b8, \u03a3. One way to think about this model is that h \u03b8 produces prototypical interpretation around which annotators' actual interpretations are distributed. Experiments We compare models both with and without random effects when fit to NLI datasets conforming to the extended setting described in \u00a72. The model without random intercepts (the fixed model) simply ignores annotator information-effectively locking \u03c1 a i to 0 for all annotators a i . Encoder All models use pretrained RoBERTa (Liu et al., 2019) as their encoder. We use the basic LM pretrained versions (no NLI fine-tuning). Data To our knowledge, the only NLI datasets that both publicly provide annotator identifiers and are large enough to train an NLI system are MegaVeridicality (MV; White and Rawlins, 2018; White et al., 2018) , which contains three-way categorical annotations aimed at assessing whether different predicates give rise to veridicality inferences in different syntactic structures, and MegaNegRaising (MN; An and White, 2020), which contains bounded continuous [0, 1] annotations aimed at assessing whether different predicates give rise to neg(ation)-raising inferences in different syntactic structures. Table 1 shows example pairs from each dataset. Both datasets contain 10 annotations per text-hypothesis pair from 10 different annotators. MV contains 3,938 pairs (39,380 annotations) with 507 distinct annotators, and MN contains 7,936 pairs (79,360 annotations) with 1,108 distinct annotators. In both datasets, each pair is constructed to include a particular main clause predicate and a particular syntactic structure. To test each model's robustness to lexical and structure variability, we use this information to construct folds of the crossvalidation (see Evaluation). Classification/Regression Heads We consider heads with one hidden affine layer followed by a rectifier. We use a hidden layer size of 128 and the default RoBERTa-base input size of 768. Training All models were implemented in Py-Torch 1.4.0 and were trained for a maximum of 25 epochs on a single Nvidia GeForce GTX 1080 Ti GPU, with early stopping upon a change in average per-epoch loss of less than 0.01. We use Adam optimization (lr=0.01, \u03b2 1 =0.9, \u03b2 2 =0.999, =10 \u22127 ) and a batch size of 128. All code is publicly available. Loss We use the negative log-likelihood of the observed values under the model as the loss. Evaluation We evaluate all of our models using 5-fold cross-validation. We consider four partitioning methods: (i) RANDOM: completely random partitioning; (ii) PREDICATE: partitioning by the main clause predicate found in the text (a particular main clause predicate occurs in one and only one partition); (iii) STRUCTURE: partitioning by the syntactic structure found in the text (a particular structure occurs in one and only one partition); and (iv) ANNOTATOR: a particular annotator occurs in one and only one partition. For the first three methods, we ensure that each annotator occurs in every partition, so that random intercepts and random slopes for that annotator can be estimated. For the ANNOTATOR method, where we do not have an estimate for the random effects of annotators in the held-out data, we use the mean of the prior. 1  We report mean accuracy on held-out folds for the categorical data (MV); and following Chen et al. (2020), we report mean rank correlation on heldout folds for the bounded continuous data (MN). To make these metrics comparable, we report them relative to the performance of both a baseline model and the best possible fixed model. For the categorical data, the baseline model predicts the majority class across all pairs, and the best possible fixed model predicts the majority class across annotators for each pair. Similarly, for the bounded continuous data, the baseline model predicts the mean response across all pairs, and the best possible fixed model predicts the mean response across annotators for each pair. 2 These relative scores are 0 when the model does not outperform the baseline and 1 when the model performs as well as the best possible fixed model. It is possible for a random effects model to obtain a score of greater than 1 by leveraging annotator information or less than 0 if it overfits the data. Results Table 2 shows the results. The random intercepts models reliably outperform the fixed models in all cross-validation settings except ANNOTATOR in Bonferroni-corrected Wilcoxon rank-sum tests (ps<0.05). Indeed, they tend to reliably outperform even the best possible fixed model, having rescaled scores above 1. The random slopes models, while in many cases comparable to the random intercepts models, confer no additional benefit over them. In the one instance in which the random slopes model performs best (the random partition for categorical data), the advantage relative to the random intercepts model is not statistically significant. Consistent with Pavlick and Kwiatkowski's findings, these results suggest that variability in annotators' responding behavior is substantial; otherwise, it would not be possible for the random effects models to outperform the best possible fixed model, and we would not expect the observed drops in performance when annotator information is removed. But this variability is likely relatively shallow: if these differences were due to deeper differences in annotators' interpretation of the pair, we would expect this to manifest in better performance by the random slopes models, as the latter subsumes the random intercepts model and can leverage the additional power of annotator-specific classification or regression heads. Of course, it remains a live possibility that the encoder we used does not extract features that are linearly related to the relevant interpretive variability, and so further investigation of random slopes models with different encoders may be warranted (see Geva et al., 2019) . Contrasting the results on ordinal and bounded continuous data, the fixed model tends to perform better on ordinal data than on bounded continuous data. A similar trend is not seen for the random effects models. Indeed, the random intercepts model performs substantially better on the bounded continuous data under all settings except for AN-NOTATOR. These results could be due to the link function we used for the bounded continuous data: the fixed model consistently learned small values for the precision parameter \u03bd 0 , resulting in sparse (bimodal) beta distributions. But the fact that the random intercepts model reliably outperforms the best possible fixed model implies that any tweaks to the link function would not bring the fixed model up to the level of the random intercepts model. Analysis To understand how annotator biases tend to pattern with ordinal and bounded continuous scales, we investigate the mean \u03c1 a for each annotator a in the random intercepts models across folds under the RANDOM partition method. Figure 2 plots the distribution of biases across categorical annotators when the fixed effect potentials-h \u03b8 (z i ) in the equations in \u00a73-are set to 0: softmax(\u03c1 a ). This distribution can be thought of as an indicator of how an annotator would respond in the absence of any correct answer. We see the most variability in terms of annotators' biases for or against neutral: the interquartile range for neutral biases is [0.23, 0.42] compared to [0.24, 0.41] for contradiction and [0.25, 0.40] for entailment. Interestingly, these biases do not reflect the fact that the scale is ordinal: if they did, we would expect more positive correlations between adjacent values; but neutral biases are more strongly rank anticorrelated with contradiction (r = \u22120.57) and entailment (r = \u22120.48) than contradiction is with entailment (r = \u22120.35). This finding suggests that three-value \"ordinal\" NLI scales are better thought of as nominal. Figure 3 plots the analogous distribution for the bounded continuous annotators, with the y-axis showing \u03c1 a1 and the x-axis showing logit \u22121 (\u03c1 a2 ). The lines behind the points show, for particular values of h \u03b8 (z i ), the \u03c1 a1 at which the distribution for a particular annotator becomes sparsei.e. where \u03b1, \u03b2 < 1-heavily favoring responses very near zero or one, rather than the mean. We see a weak rank correlation (r = 0.24, p < 0.05) between precision and annotators' biases to give responses nearer to one, suggesting that one-biased annotators tend to give less sparse responses. This correlation might, in part, explain the poor performance of the bounded continuous models in the ANNOTATOR cross-validation setting. Related Work The models developed here are closely related to models from Item Response Theory (IRT). IRT has been used to assess annotator quality (Hovy et al., 2013 (Hovy et al., , 2014;; Rehbein and Ruppenhofer, 2017; Paun et al., 2018a,b; Zhang et al., 2019; Felt et al., 2018) and various properties of an item (Passonneau and Carpenter, 2014; Sakaguchi and Van Durme, 2018; Card and Smith, 2018) , including difficulty (Lalor et al., 2016 (Lalor et al., , 2018 (Lalor et al., , 2019)) . Other non-IRT-based work attempts to measure the relationship between annotator disagreement and item difficulty (Plank et al., 2014; Kalouli et al., 2019) . Other recent work focuses on incorporating annotator information in modeling annotatorgenerated text. Geva et al. (2019) find that concatenating annotator IDs as input features to a BERTbased text generation model yields improved performance on several datasets. Although we reach similar conclusions about the importance of annotator information in this work, our approach differs in at least one critical respect: by explicitly distinguishing linguistic input from annotator information, our model cleanly separates the linguistic representations from representations of the annotators interpreting or producing those representations. This clean separation is of potential benefit not only to those interested in using NLI models (or deep learning architectures more generally) in an experimental (psycho)linguistics setting, where distinguishing the two sorts of representations can be crucial, but also to those interested in possibly quite substantial reductions in model size. Conclusion We find (i) that models containing only random intercepts outperform standard models when annotators are known, and (ii) that models that further contain random slopes do not yield any additional benefit. These results indicate that, though differences among NLI annotators' response behavior are important to model, these differences may not be particularly deep, limited to the ways in which annotators use the response scale, but not relating to deeper interpretive differences. Acknowledgments This research was supported by the University of Rochester, DARPA AIDA, DARPA KAIROS, IARPA BETTER, and NSF-BCS (1748969). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government.",
    "abstract": "There is growing evidence that the prevalence of disagreement in the raw annotations used to construct natural language inference datasets makes the common practice of aggregating those annotations to a single label problematic. We propose a generic method that allows one to skip the aggregation step and train on the raw annotations directly without subjecting the model to unwanted noise that can arise from annotator response biases. We demonstrate that this method, which generalizes the notion of a mixed effects model by incorporating annotator random effects into any existing neural model, improves performance over models that do not incorporate such effects.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "5",
    "year": "2020",
    "month": "December",
    "title": "Natural Language Inference with Mixed Effects"
}