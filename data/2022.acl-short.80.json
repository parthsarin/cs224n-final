{
    "article": "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards. Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference. Past systems using such semantic similarity rewards further suffer from repetitive outputs and overfitting. To address these issues, we propose metrics that replace the greedy alignments in BERTSCORE with optimized ones. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. Additionally, we find that this model enjoys stable training relative to a non-RL setting. Introduction Automatic evaluation metrics often score natural language generation (NLG) system outputs based on how well they lexically align to humanannotated references. In tasks such as machine translation and summarization, these metrics may unfairly penalize outputs that express the correct semantics despite a lower n-gram overlap with reference strings. As a result, models overfitting to certain token-level patterns may dominate those generating more creatively (e.g., through synonyms or varied sentence structure). NLG systems are typically trained to maximize likelihood of a single set of references. Conditioning models on gold prefixes shields them from their own predictions during training-an issue known as exposure bias. Adding reinforcement learning (RL) objectives (Ranzato et al., 2016; Edunov et al., 2018) can aid exploration by giving a model feedback on sequences sampled from its own distribution. However, it is common practice to use automatic evaluation scores like BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2002) as sequence-level rewards. This results in the same lack of semantic signal described earlier. Instead of hinging evaluation on hard n-gram overlap, recent metrics (Zhang et al., 2019; Zhao et al., 2019) rely on vector similarity between contextualized subword embeddings to make more semantically faithful judgments. BERTSCORE, in particular F BERT , computes a token-level F1 score based on greedy alignment of similar embeddings. With their strength in offline evaluation, it is natural to ask how these embeddings-based metrics can help provide more realistic training feedback. Past approaches to train models with semantic similarity scores include both non-differentiable and differentiable objectives. Wieting et al. (2019) separately train paraphrastic sentence embeddings that provide semantic similarity rewards to a neural machine translation (NMT) system. Rewards were included in a mixed minimum risk and maximum likelihood training phase. Besides an embedding training overhead, the model needed a length penalty term to limit repetitive outputs. Li et al. (2019) adopt a similar fine-tuning approach using an RL objective with F BERT for abstractive summarization. While their models were less repetitive, their news domain corpora may have been a natural match for BERT embeddings. Finally, Jauregi Unanue et al. (2021) also propose to optimize F BERT but with fully differentiable training objectives in NMT. Yet their models overfit after only a few epochs and scored lower in BLEU at the cost of higher F BERT . We hypothesize that metrics employing external pretrained vectors may suffer from domain mismatch with downstream data. This can hurt the accuracy of semantic similarity scores computed during training. In this work, we focus on text generation from Abstract Meaning Representations (AMRs, Banarescu et al., 2013) , sentence-level semantic graphs that are rooted, directed, and acyclic. This task's models may especially benefit from an emphasis on semantic rather than lexical similarity. It also provides a challenging setting to evaluate overfitting given the relatively small corpus size. In our analysis of F BERT rewards, we note that F BERT could worsen repetition and incomplete outputs in NLG systems. Due to its greedy token alignment, F BERT precision may assign extra credit to a reference token 'retrieved' multiple times. In response, we contribute the following. \u2022 We introduce metrics that apply discrete and continuous alignments to BERTSCORE, mitigating the pitfalls of greedy alignment. \u2022 For text generation from AMR, we are the first to train on RL objectives with embeddingsbased evaluation metrics. \u2022 As RL rewards, we compute BERTSCOREbased metrics on a model's own token representations rather than BERT embeddings. This is more memory-efficient and does not overfit relative to pure cross-entropy training. Greedy Token Alignment The main insight behind BERTSCORE and related metrics is to align hypothesis and reference tokens using their pairwise vector similarity scores. These alignments are later used to weight the contribution of token-level similarity scores towards a final sequence-level score. Concretely, given (\u0177 1 , . . . , \u0177m ) and (y 1 , . . . , y k ) hypothesis and reference token embeddings, precision in F BERT is P BERT = 1 m \u0177i \u2208\u0177 max y j \u2208y cos(\u0177 i , y j ), where cos(\u0177, y) = \u0177\u22a4 y/ \u2225\u0177\u2225 \u2225y\u2225 denotes cosine similarity. Each hypothesis token \u0177i is greedily aligned to the reference token y j with the highest corresponding embedding cosine similarity. Unlike in BLEU, P BERT does not clip the number of times \u0177i can align to a unique y j by its count in y. As such, a hypothesis will get excess credit by repeating a reference token beyond this count. While the authors claim greedy alignments have little effect on BERTSCORE evaluation performance, they perform poorly relative to metrics based on optimized alignments in our experiments. Optimized Token Alignment Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (\u0177 i , y j ) is highly weighted if it incurs low cost (i.e., distance). Here, we describe discrete token matching (oneto-one) and soft alignment (one-to-many). For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009; Peyr\u00e9 and Cuturi, 2019) transport matrix. We weight pairwise token similarities as in F BERT using each of these two alignments to provide metrics F DISC and F CONT . Discrete word matching To avoid the issues with greedy alignment in P BERT , we can extract one-to-one alignments between the two sequences. Let C \u2208 R m\u00d7k denote the pairwise cosine distance matrix such that C ij = 1 \u2212 cos(\u0177 i , y j ). For notational clarity, let C = 1 \u2212 C. We wish to find alignments T d = arg min T \u2208{0,1} m\u00d7k m i=1 k j=1 T ij C ij , (1) such that no element in h = T 1 k and r = T \u22a4 1 m exceeds one. In other words, each \u0177i can align to at most one y j (exactly one when m = k), and vice versa. This linear sum assignment problem can be solved in low-order polynomial time (Crouse, 2016) , making it suitable for use during training. Metric The updated precision is found as P DISC = 1 m m i=1 k j=1 T d ij C ij . (2) Recall R DISC takes an analogous form and is combined with P DISC to produce an F1 score, F DISC . Continuous word alignment We also experiment with soft alignments, where weights in T are continuous. In the case of P BERT , one-to-many alignments between each hypothesis token \u0177i and those in {y j } j\u2208[k] are permitted. Inspired by work applying EMD to semantic text similarity (Kusner et al., 2015; Clark et al., 2019) , we frame alignment as minimizing the transportation cost between token embeddings from the hypothesis and reference distributions. The amount of token-level mass to transport between the two distributions is h and r, respectively. Instead of assigning IDF as the mass per token (Zhao et al., 2019) , we use the norm of its embedding (i.e., \u2225y\u2225, Yokoi et al., 2020) for simplicity. The EMD, or optimal transport, problem is T c = arg min T \u2208R m\u00d7k \u22650 m i=1 k j=1 T ij C ij , (3) s.t. h = T 1 k , r = T \u22a4 1 m . Intuitively, if we view T ij as the joint probability of aligning \u0177i with y j , the row and column sums are marginals (Cuturi, 2013) . Metric To compute F CONT , we normalize the alignment weights such that the rows of T sum to one for precision, and the columns for recall. P CONT = 1 m m i=1 1 h i k j=1 T c ij C ij , (4) R CONT = 1 k k j=1 1 r j m i=1 T c ij C ij (5) Semantic Similarity Rewards We propose to fine-tune on our optimized F1 metrics, applying a weighted average of cross-entropy and RL objectives. Given source sequence x (e.g., a linearized AMR), the former is computed as L e = \u2212 k i=1 log p(y i | y <i , x). To encourage close evaluation scores between sampled \u0233 and reference y, the RL objective is L r = (\u2206(\u0233 g , y) \u2212 \u2206(\u0233, y)) k i=1 log p(\u0233 i | \u0233<i , x), where \u2206 is the chosen evaluation metric and \u0233g is a greedily decoded baseline relative to \u0233. This baseline helps reduce variance in REINFORCE (Williams, 1992) . The combined cross-entropy and RL loss is L = \u03bbL r + (1 \u2212 \u03bb)L e , where \u03bb is empirically set to 0.3. Experiments We examine the performance of our proposed metrics as RL rewards on AMR-to-text generation. Setup Dataset The LDC2017T10 dataset that we experiment on contains \u223c36K training and \u223c1.4K each of development and test AMR-sentence pairs. To leverage strong pre-trained language models, the AMRs are linearized as in Ribeiro et al. (2021) . Evaluation We report results in terms of BLEU (Papineni et al., 2002) , METEOR (Banerjee and Lavie, 2005) , CHRF (Popovi\u0107, 2015) , and BLEURT (Sellam et al., 2020) . Only the latter metric makes use of pre-trained contextualized embeddings. Baselines For all experiments, we fine-tune the small capacity T5 model (Raffel et al., 2020) from Ribeiro et al. (2021) . The model has 60M parameters and features a Transformer-based encoder and decoder. We compare our F DISC and F CONT metrics for RL-based training against three baseline approaches. XENT is a pure cross-entropy objective. For RL-based approaches, we include a BLEU reward (BL-R) and one with F BERT -computed on the lowest level token embeddings in T5. 1 The \u03bb scaling factor for the RL objective is set to 0.3 across all RL-based experiments. Implementation details Adam (Kingma and Ba, 2015) is used to optimize the model with an initial (1) REF There are 12 teams totally participating in the competition. XENT The competition was part of a total of 12 teams. FBERT The competition is part of a total of 12 teams. FDISC The total of 12 teams participated in competition. (2) REF Raymond zilinskas stated that in the worst case the bacteria would be defrosted from minus 70 degrees and it would be a real mess to clean up afterward because it would not be known for certain whether all the bacteria was dead. XENT Raymond Zilinskas stated that the bacterium was defrost in the worst case and that afterward cleaning up was a real mess because there is certainly no known cause of death for all the bacteriums. FBERT Raymond Zilinskas stated that the bacterium was defrosting in the worst case and the afterward cleaning up was a real mess because the bacterium was certainly not known to die of all the bacteriums. FDISC Raymond Zilinskas stated that the bacterium was defrost in the worst case and the afterward cleaning up was a real mess because the bacterium was certainly not known to have all died. 2021 ) do not release their training methodology, we train until validation BLEU does not increase for three epochs-an approach found in previous work fine-tuning T5 for AMR-to-text generation (Hoyle et al., 2021) . We use SciPy 2 and the Python Optimal Transport library 3 to solve Eqs. 1 and 3. Results Table 1 shows that F DISC achieves the highest scores on all metrics, surpassing F CONT as well. It scores higher than XENT by 1.28 BLEU and 0.71 BLEURT points. Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point on that metric. There is a clear hierarchy among the approaches based on F1 score, with F DISC above F CONT , followed by F BERT at the bottom. This dynamic suggests that the optimized alignments may provide higher quality reward signals during training. We note that although F CONT performed comparably to BL-R, it could exploit tensor operations and was far faster to compute than BLEU. On the other hand, F BERT achieved significantly lower scores than BL-R. As noted in \u00a72, perhaps the clipped precision counts in BLEU gave BL-R an advantage over the greedy nature of F BERT . Analysis Training stability As shown in Fig. 1 , F DISC continues to improve on validation BLEU long after XENT overfits at epoch 18. This runs counter to the expectation of unstable RL-based training. It is also interesting that while F CONT validation performance looks fairly low relative to BL-R, it achieves similar scores at test time. This may be due to irrelevant differences between the validation and test sets, however. Manual inspection Table 2 lists a few examples of model outputs for detailed analysis. In example (1), both XENT and F BERT make the error of predicting \"part\" instead of \"participating\". Only F DISC approaches the meaning of the reference. This may be a side-effect of weighting lexical over semantic similarity in the former two systems. In (2), F BERT repeats the word \"bacterium\", while XENT takes an anthropomorphic view of the bacterium. The repetition may be a result of F BERT rewarding multiple instances of the same token by mistake during greedy alignment. Conclusion This paper proposes new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens. Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extract alignments as a constrained optimization problem. In the discrete case, we treat alignment as a matching problem between hypothesis and reference tokens. In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions. We apply new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards. Despite being computed on a downstream model's token embeddings, the metrics still provide informative rewards during training without signs of overfitting. Acknowledgments Research supported by NSF awards IIS-1813823 and CCF-1934962.",
    "abstract": "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards. Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference. Past systems using such semantic similarity rewards further suffer from repetitive outputs and overfitting. To address these issues, we propose metrics that replace the greedy alignments in BERTSCORE with optimized ones. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. Additionally, we find that this model enjoys stable training relative to a non-RL setting.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Rewarding Semantic Similarity under Optimized Alignments for {AMR}-to-Text Generation"
}