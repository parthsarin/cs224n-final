{
    "article": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate fe atures one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words. Introduction Re cently, many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context-free grammars (PCFGs), including Magerman (1995) , Charniak (1996) , Collins (1996; 1997) , Black, Lafferty, and Roukos (1992) , Eisele (1994) and Brew (1995) . Each has tried to capture the hierarchical nature of language, as typified by context-free grammars, and to then augment this with additional context sensitivity based on various features of the input. Unfortunately, none of these works combines the most important benefits of all the others, and most lack a certain elegance. We have therefore tried to synthesize these works into a new formalism, probabilistic fe ature grammar (PFG) . PFGs have several important properties. First, PFGs can condition on features beyond the nonterminal of each node, including features such as the head word or grammatical number of a constituent. Also, PF,Gs can be parsed using efficient polynomial-time dynamic programming algorithms, and learned quickly from a treebank. Finally, unlike most other formalisms, PFGs are potentially useful for language modeling or as one part of an integrated statistical system (e.g. Miller et al., 1996) or for use with algorithms requiring outside probabilities. Empirical results are encouraging: our best parser is comparable to those of Magerman ( 19 95) and Collins (1996) when run on the same data. When we run using part-of-sp eech (POS) tags alone as input, we perform significantly better than comparable parsers. Motivation PFG can be regarded in several different ways: as a way to make history-based grammars (Magerman, 1995) more context free, and thus amenable to dynamic programming; as a way to generalize the work of Black et al. (1992) ; as a way to turn Collins' parser (Collins, 1996) into a generative probabilistic language model; or as an extension of language-modeling techniques to stochastic grammars. The resulting formalism, which is relatively simple and elegant , has m9st of the advantages of each of the systems from which it is derived. Consider the following simple parse tree for the sentence \"The man dies\" : s NP VP \ufffd I the man dies While this tree captures the sim ple fact that sentences are composed of noun phrases and verb phrases, it fails to capture other important restrictions. For instance, the NP and VP must have the same number, both singular, or both plural. Also, a man is far more likely to die than spaghetti, and this constrains the head words of the corresponding phr ases. This additional information can be captured in a par se tree that has been augm ented with featur es, such as the category, number, and head word of each constituent, as is traditionally done in many feature-based formalisms, such as HPSG , LFG, etc. While a normal PCFG has productions such as S\u2794 NP VP ,ve will ,v-rite these augmented productions as, for instance, (S, singular, dies) \u2794 (NP, singular, man) ( VP, singular, dies) In a traditional pr obabilistic context-free grammar, we could augment the first tree with probabilities in a simple fashion. We estimate the probability of S \u2794 NP VP using a tree bank to determine C(S ;r ; VP) , the number of occurrences of S -4 NP VP divided by the number of occurrences of S. For a reasonably large treebank, probabilities estimated in this way would be reliable enough to be useful (Charniak, 1996) . On the other hand, it is not unlikely that we would never have seen any counts at all of C((S, singular, dies) \u2794 (NP, singular, man) ( VP, singular, dies) ) C((S, singular, dies)) which is the estimated probability of the corresponding production in our grammar augmented with features. The introduction of features for number and head word has created a data sparsity problem. Fortunately, the data-sparsity problem is well known in the language-modeling community, and we can use their techniques, n-gram models and smoothing, to help us. Consider the probability of a five word sentence, w 1 ... w 5 : While exactly computing P(w5 lw1w2w 3 w4) is difficult,' a good approximation is P(w5 lw1 w2 w 3 w4 ) \ufffd P(w5 lwsw4). Let C(w 3 w4w5) represent the number of occurrences of the sequence w 3 w4w5 in a corpus. We can then empiricall y approximate P(w5 IW 3 W4 ) with cbc\ufffd\ufffd:\ufffd\u2022 )). Unfortun atel y , this approximation alone is not enough; there may still be many three word combinations that do no t occur in the corpus, but that sho uld not be assigned zero probab ilities. So we smooth this approximation, for instance by using P(w, l waw\u2022 ) \"\" A 1 \u00b0b7::::t + ( 1-Ai ) ( A2 \u00b0b7::; ,) + ( 1-A2 ) I: \ufffd ( \ufffd(\ufffd ) ) Now, we can use these same approximations in PFGs. Let us assume that our PFG is binary branching and has g features, numbered l... g ; we will call the parent, features ai , the left child features b i , and the right child features ci . In our earlier example, a 1 represented the parent nonterminal category; a 2 represented the parent number (singular or plural); a 3 represented the parent head word; b 1 represented the left child category; etc. We can write a PFG production as ( a 1 , a 2 , ... , a 9 ) -+ ( b 1 , b 2 , ... , b 9 ) ( c 1 , c 2 , ... , c 9 ). If we think of the set of fe atures for a constituent A as being the random variables A1 , ... , A. 9 , then the probability of a pr oduction is the conditional probability P(B 1 =b1 , ... ,B 9 =b 9 ,C1 = c1 , ... ,C 9 =c 9 IA1 =a 1 , ... , A 9 =a 9 ) We write af to represent A 1 = a 1 , ... , A1.: = a1.: , and sometimes write ai as shorthand for Ai = ai . \\\\'e can then write this conditional probability as P( bf , cf laf ) This joint probability can be factored as the product of a set of conditional probabilities in many ways. One simple way is to arbitrarily order the features as b 1 , ... ,b 9 , c1 , .\u2022\u2022 , c 9 . \\\\ 1 e then condition each feature on the parent features and all features earlier in the sequence. We can now approximate the various terms in the factorization by making independence assumptions. For instance, returning to the concrete example above, consider feature c 1 , the right child nonterminal or terminal category. The following approximation should work fairly well in practice: That is, the category of the right child is well determined by the category of the parent and the category of the left child. Just as n-gram models approximate conditional lexical probabilities by assuming independence of words that are sufficiently distant, here we approximate conditional feature probabilities by assuming independence of features that are sufficiently unrelated. Furthermore, we can use the same kinds of backing-off techniques that are used in smoothing traditional language models to allow us to condition on relatively large contexts. In practice, a grammarian determines which features should be considered independent, and the optimal order of backoff, possibly using experiments on development test data for feedback. It might be possible to determine the optimal order of backoff automatically, a subject of future research. Intuitively, in a PFG, features are produced one at a time. The probability of a feature being produced depends on a subset of the features in a local context of that fe ature. Figure 1 shmvs an example of this feature at-a-time generation for the noun phrase \"the man.\" To the right of the figure, the independence assumptions made by the grammarian are shown. Formalism In a PCFG, the important concepts are the terminals and nonterminals, the productions involving these, and the corresponding probabilities. In a PFG, a vector of features corresponds to the terminals and nonterminals. PCFG productions correspond to PFG events of the form (a 1 , ... , a 9 ) -+ (b 1 , ... , b 9 )(c 1 , .\u2022\u2022 , c 9 ), and our PFG rule probabilities correspond to products of conditional probabilities, one for each feature that needs to be generated. Events and EventProbs There are two kinds of PFG events of immediate interest. The first is a binary event, in which a feature set af (the parent features) generates features bf cf (the child features). Figure 1 is an example of a single such event. Binary events generate the whole tree except the start node, which is generated by a start event. The probability of an event is given by an EventProb, which generates each new feature in turn, as\ufffdigning it a conditional probability given all the known features. For instance, in a binary event , the EventProb assigns probabilities to each of the child features, given the parent features and any child features that have already been generated. __ Formall ri_ an EventProb [ is a 3-tuple (JC, N, F), where JC is the set of conditioning feat \ufffd es ( the Known features), N = N 1 , N2 , ... , N 11 is an ordered list of conditioned features (the New features), and F = Ji , h, ... , f n  (ni, k1 , ... , kk , n1 , n2 , ... , nil) returns P(Ni = nilI<1 = k1 , ... Kk = kk , N1 = n1, N2 = n2 , ... , Ni-l = ni-1), the probability that feature Ni = ni given all the known features and all the lmver indexed new features. For a binary event , we may have E B = ( { a1 , a2, .\u2022. , a 9 }, (b 1 , ... , b 9 , c 1 , ... , c 9 ),F B); that is, the child fea tures are conditioned on the parent features and earlier child features. For a start event we have Es = ({}, (a1 ,a2, . .. ,a 9 ), Fs); i.e. the parent features are conditioned only on each other. Terminal Function, Binary PFG We need one last element: a function T from a set of g features to (T, N) which tells us whether a part of an event is terminal or nonterminal: the terminal fu nction. A Binary PFG is then a quadruple (g, E B , Es , T): a number of features, a binary EventProb, a start EventProb, and a terminal function. Of course, using binary events allows us to model n-ary branching grammars for any fixed n: we simply add additional features for terminals to be generated in the future, as well as a feature for whether or not this intermediate node is a \"dummy\" node (the continuation feature) . 1 Comparison to Previous Wo rk PFG bears much in common with previous work, but in each case has at least some advantages over previous formalisms. Some other models (Charniak, 1996 ; Brew, 1995 ; Collins, 1996 ; Black, Lafferty, and Ro ukos, 1992 ) use probability approximations that do not sum to 1, meaning that they should not be used either for language modeling, e.g. in a speech recognition system, or as part of an integrated model such as that of Miller et al. (1996 ) . Some models (Magerman, 1995 ; Collins, 1996) assign probabilities to parse trees conditioned on the strings, so that an unlikely sentence with a single parse might get probability 1, making these systems unusable for language modeling. PFGs use joint probabilites, so can be used both for language modeling and as part of an integrated model. Furthermore, unlike all but one of the comparable systems, PFGs can compute outside probabilities, which are useful for grammar induction, some parsing algorithms (Goodman, 1996 ) , and, as we will show, pruning (Goodman, 1997) . Collins (1996 ) introduced a parser with extremely good performance. From this parser, we take many of the particular conditioning features that we will use in PFGs. As noted, this model cannot be used for language modeling. There are also some inelegancies in the need for a separate model for Base-NPs, and the treatment of punctuation as inherently different from words. The model also contains a non-statistical rule about the placement of commas. Finally, Collins' model uses memory proportional to the sum of the squares of each training sentence's length. PFGs in general use memory ,vhich is only linear. Collins (1997) worked independently from us to construct a model similar to ours. In particular, Collins wished to adapt his previous parser (Collins, 1996) to a generative model. In this he succeeded. However, while we present a fairly simple and elegant formalism, which captures all information as features, Collins uses a variety of different techniques: variables (analogous to our features); a special stop category; modifications of nonterminal categories; and information computed as a function of child nodes. This lack of homogeneity fails to show the underlying structure of the model, and the ways it could be expanded. Bigram Lexical Dependency Parsing Generative Lexicalized Parsing Furthermore, our model of generation is very general. V V hile our implementation captures head words through the particular choice of features, Collins' model explicitly generates first the head phrase, then the right children, and finally the left children. Thus, our model can be used to capture a wider variety of grammatical theories, simply by changing the choice of features. Finally, there are some subtle interesting differences with respect to the distance metric. While both bigram lexical dependency parsing and generative lexicaliz ed parsing use a distance metric, generative lexicalized parsing does not include symbols in the child node on the parent head side as part of the distance, because it is difficult to do so in that model. Our use of features allows this information to be captured. Simple PCFGs Charniak (1 996) showed that a simple PCFG formalism in which the rules are simply \"read off\" of a treebank can perform very competitively. Furtherm ore, he showed that a simple modification, in which productions at the right side of the sentence have their probability boosted to encourage right branching structures, can improve performance even further. PFGs are a su perset of PCFGs, so we can easily model the basic PCFG grammar used by Charniak, although the boosting cannot be exactly duplicated. However, we can use more principled techniques, such as a feature that captures whether a particular constituent is at the end of the sentence, and a feature for the length of the constituent . Charniak's boosting strat egy means that the scores of constituent s are no longer probabilities, meaning that they cannot be used with the inside-outside algorithm. Furthermore, the PFG feature-based technique is not ext ra-grammatical, meaning that no additional machinery needs to be added for pa rsing or grammar induction. Stochastic HPSG Brew (1 995) introduced a stochastic version of HPSG . In his formalism, in some cases even if two features have been constrained to the same valu e by unification, the probabilities of their productions are assumed independent . The resulting probability distribution is then normalized so that probabilities sum to one. This leads to problems with grammar induction pointed out by Abney (1996) . Our formalism, in cont rast , explicitly models dependencies to the ext ent possible given data sparsity constraints. IBM Language Modeling Group Researchers in the IBM Language Modeling Group developed a series of successi vely more complicated models to integrat e st atistics with features. The first model (Black, Garside, and Leech, 1993 ; Black, Lafferty, and Roukos, 1992) essentially tries to convert a unification grammar to a PCFG, by instantiating the valu es of the features. Due to data sparsity, however, not all features can be instantiated. Instead, they create a grammar where many features have been instantiated, and many have not ; they call these partially inst antiated features set s mnemonics. They then create a PCFG using the mnemonics as terminals and nont erminals. Features instantiated in a particular mnemonic are generated probabilistically, while the rest are generated through unification. Because no smoothing is done, and because features are grouped, data sparsity limits the number of features that can be generated probabilistically, whereas because we generate features one at a time and smooth, we are far less limited in the number of features we can use. Their technique of generating some features probabilistically, and the rest by unification, is somewhat inelegant ; also, for the probabilities to sum to one, it requires an additional step of normalization, which they appear not to have implement ed. In their next model (Black et al. , 1992; Magerman, 1994, pp. 46-56) , which strongly influenced our model, five attributes are associat ed with each nont erminal: a synt actic category, a semantic cat egory, a rule, and two lexical heads. The rules in this grammar are the sam e as the mnemonic rules used in the previous work, developed by a grammarian. These five attributes are generated one at a time, with backoff smoothing, conditioned on the parent attributes and earlier attributes. Our generation model is essentially the sam e as this. Notice that in this model, unlike ours, there are two kinds of features: those features captured in the mnemonics, and the five categories; the categories and mnemonic features are modeled very different ly. Also, notice that a great deal of work is required by a grammarian, to develop the rules and mnemonics. The third model Magerman: 94a, extends the second model to capture more dependencies, and to remove the use of a grammarian. Each decision in this model can in principal depend on any previous decision and on any word in the sent ence. Because of these potentially unbounded dependencies, there is no dynamic programming algorithm: without pruning, the time complexity of the model is exponential. One motivation for PFG was to capture similar information to this third model, while allowing dynamic programming. Probabilistic LR Parsing with Unification Grammars Briscoe and Carroll describe a formalism (Briscoe and Carroll, 1993; Carroll and Briscoe, 1992) similar in many ways to the first IBM model. In particular, a context-free covering grammar of a unification grammar is constructed. Some features are captured by the covering grammar, while others are modeled only through unifications. Only simple plus-one-style smoothing is done, so data sparsity is still significant. The most important difference between the work of Briscoe and Carroll (1993) and that of Black, Garside, and Leech (1993) Parsing The parsing algorithm we use is a simple variation on probabilistic versions of the CKY algorithm for PCFGs, using feature vectors instead of nonterminals (Baker, 1979; Lari and Young, 1990) . The parser computes inside probabilities ( the sum of probabilities of all parses, i.e. the probability of the sentence) and Viterbi probabilities ( the probability of the best parse), and, optionally, outside probabilities. In Figure 2 we give the inside algorithm for PFGs. Notice that the algorithm requires time O(n 3 ) in sentence length, but is potentially exponential in the number of children, since there is one loop for each parent feature, a 1 through a 9 \u2022 When parsing a PC FG, it is a simple matter to find for every right and left child what the possible parents are. On the other hand, for a PFG, there are some subtleties. We must loop over every possible value for each feature. At first, this sounds overwhelming, since it requires guessing a huge number of feature sets, leading to a run time exponential in the number of features; in practice, most values of most features will have zero probabilities, and we can avoid considering these. For instance, features such as the length of a constituent take a single value per cell. Many other features take on very few values, given the children. For example, we arrange our parse trees so that the head word of each constituent is dominated by one of its two children. This means that ,ve need consider only two values for this feature for each pair of children. The single most time consuming feature is the Name feature, which corresponds to the terminals and non-terminals of a PCFG. For efficiency, we keep a list of the parent/left-child/right-child name triples which have non-zero probabilities, allmving us to hypothesize only the possible values for this feature given the children. Careful choice of features helps keep parse times reasonable. Pruning We use two pruning methods to speed parsing. The first is a simple beam-search method, inspired by techniques used by Collins (1996) and Charniak (1996) , and described in detail by Goodman (1997) . Within each cell in the parse chart, we multiply each entry's inside probability by the prior probability of the parent features of that entry, using a special EventProb, [p . We then remove those entries whose combined probability is too much lower than the best entry of the cell. In speech recognition, multiple-pass recognizers (Zavaliagkos et al., 1994) have been very successful. V\\7e can use an analogous technique, multiple-pass parsing (Goodman, 1997) with either PCFGs or PFGs. We use a simple, fast grammar for the first pass, which approximates the later pass. We then remove any events whose combined inside-outside product is too low: essentially those events that are unlikely given the complete sentence. The first pass\u2022 is fast, and does not \u2022 slow things down much, but allows us to speed up the second pass significantly. The technique is particularly natural for PFGs, since for the first pass, we can simply use a grammar with a superset of the features from the previous pass. Actually, the features we used in our first pass were Name, Continuation, and two new features especially suit able for a fast first pass, the length of the constituent and the terminal symbol following the constit uent . Since these two features are uniquely determined by the chart cell of the constituent, they are especially suit able for use in a first pass, since they provide useful information without increasing the number of elements in the ch art. However, when used in our second pass, these features did not help performance, presumably because they captured information similar to that captured by other features. Multiple-pass techniques have dramatically sped up PFG parsing. Experimental Results The PFG formalism is an extremely general one that has the capability to model a wide variety of phenomena. Still, it is useful to apply the formalism to actual data, as a proof of concept . Vve used two PFGs, one which used the head word feature, and one otherwise identical grammar with no word based features, only POS tags. The grammars had the features shown in Figure 4 . A sample parse tree with these features is given in Figure 3 . The feature D L is a 3-tuple, indicating the number of pun ct uation ch aracters, verbs, and words to the left of a constituent's head word. To avoid data sparsity, we do not count higher than 2 punctuation characters, 1 verb, or 4 words. So , a value of D L = 014 indicates that a constituent has no pun ctuation, at least one verb, and 4 or more words to the left of it s head word. DR is a similar feature for the right side. Finally, DB gives the numbers between the constituent's head word, and the head word of its other child. \"\\1/ords, verbs, and punctuation are counted as being to the left of themselves: that is, a terminal verb has one verb and one word on its left . Notice that the continuation of the lower NP in Figure 3 is Rl , indicating that it inherits it s child from the right, with the 1 indicating that it is a \"dummy\" node to be left out of the final tree. The probability functions we used were similar to those of Section 2. There were three important differences. The first is that in some cases, we can compute a probability exactly. For instance, if we know that the head word of a parent is \"man\" and that the parent got its head word from its right child , then we know that with probability 1, the head word of the right child is \"man.\" In cases where we can compute a probability exactly, we do so. Second, we smoothed slight ly differently. In particular, when smoothing a probability estimate of the form C(abc) p(ajbc) \ufffd ,,\\ C(bc) + (1 -.\\)p (alb) we set ,,\\ = !.:\ufffd\ufffdtic ), using a separate k for each probability distribution. Finally, we did addit ional smoothing for words, adding counts for the unknown word. It would take quite a bit of space to give the table that shows for each feature the order of backoff for that feature. We instead discuss a si:\u00b5gle example, the order of backoff for the 2R feature, the category of the second child of the right feature set. The least relevant features are first: Tt VR,PR, CL,NL,CP,NP, lR, HR ,C R,NR. We back off from the head word feature first, because, although relevant, this feature creates significant data sparsity. Notice that in general, features from the same feature set, in this case the right features, are kept longest; parent features are next most relevant; and sibling features are considered least relevant . We leave out  entirely features that ar e unlikely to be relevant and that would. cause data sparsity, such as Hl L, the head word of th e left sibling. C 1 2 H p w D L D R D B 1 C 1 2 H p \\ \\T D L D R D B 0 DET DET the 001 000 000 R DET ADJ N N man 003 000 002 C 1 2 H p w D L D R D B C 0 1 2 H ADJ P ADJ \\\\1 normal D L 001 D R 000 D B 000 C R l NP 2 VP H VP p V W dies D L 014 D R 000 D B 011 Rl ADJ N N N man 002 000 001 0 N N man 001 000 000 I C 1 2 H p \\V D L D R D B C 1 2 H p w D L D R D B Results We used th e same machine-lab eled data as Collin s (1996 ; 1997) : TreeBank II sections 2-21 for training, section 23 for test, section 00 for development, using all sentences of 40 words or less. 2 We also used th e same scoring method (replicating even a minor bug for the sake of comparison). Our results are th e best we know of from POS tags alone, and, with th e head word featur e, fall between the results of Collins and Magerman, as given by Collin s (1997) While the empirical performance of probabilistic feature grammars is very encouraging, we think there is far mor e potential. First, for grammarians wishing to integrate statistics into mor e conventional models, the features of PFG are a very useful tool, corresponding to the features of DCG, LFG, HPSG , and similar formalisms. Tr eeBank II is annotated with many semantic features, currently unused in all but the simplest way by all systems; it should be easy to integrate th ese features into a PFG. PFG has other benefits that we would like to explore, including the possibility of its use as a language model, for applications such as speech recognition. Furthermore, th e dynamic programming used in the model is amenable to effi cient rescoring of lattices output by speech recognizers. Another benefit of PFG is that both inside and outside probabilities can be computed, making it possible to reestimate PFG parameters. We would like to try experiments using PFGs and the inside/outside algorithm to estimate parameters from unannotated text. \u2022 while the generality and elegance of the PFG model makes these and many other experiments possible, we are also encouraged by the very good experimental results. \\Vordless model performance is excellent, and the more recent models with words are comparable to the state of the art.",
    "abstract": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate fe atures one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "61",
    "year": "1997",
    "month": "September 17-20",
    "title": "Probabilistic Feature Grammars"
}