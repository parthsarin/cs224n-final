{
    "article": "This paper accompanies the release of Opusparcus, a new paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The corpus consists of paraphrases, that is, pairs of sentences in the same language that mean approximately the same thing. The paraphrases are extracted from the OpenSubtitles2016 corpus, which contains subtitles from movies and TV shows. The informal and colloquial genre that occurs in subtitles makes such data a very interesting language resource, for instance, from the perspective of computer assisted language learning. For each target language, the Opusparcus data have been partitioned into three types of data sets: training, development and test sets. The training sets are large, consisting of millions of sentence pairs, and have been compiled automatically, with the help of probabilistic ranking functions. The development and test sets consist of sentence pairs that have been checked manually; each set contains approximately 1000 sentence pairs that have been verified to be acceptable paraphrases by two annotators. Introduction This paper introduces the first release of the Opusparcus multilingual corpus of paraphrases (Creutz, 2018) . Paraphrases are pairs of phrases in the same language that essentially convey the same meaning, such as \"Have a seat.\" versus \"Sit down.\". Paraphrase resources have been published earlier, for instance by Quirk et al. (2004) , Dolan et al. (2004) , Dolan and Brockett (2005) , Ganitkevitch et al. (2013) , Ganitkevitch and Callison-Burch (2014) , and Pavlick et al. (2015) . However, Opusparcus has a few distinctive characteristics. Firstly, and most importantly, all paraphrases in Opusparcus (OpenSubtitlesParaphraseCorpus) consist of movie and TV subtitles extracted from the OpenSubtitles2016 collection of parallel corpora (Lison and Tiedemann, 2016) . Previous paraphrase collections mostly contain fairly formal language in the form of news text and transcripts of parliamentary proceedings. The more colloquial language used in subtitles can be a valuable addition, for instance, in computer assisted language learning, to help learners find natural and idiomatic expressions in real-life situations. Secondly, in this work the pivot language technique introduced by Bannard and Callison-Burch (2005) is applied using multiple pivot languages rather than just one or a few. The technique consists in finding paraphrases in one target language by translating to another, so-called pivot language and then translating back. For example, English \"Have a seat.\" can be translated to French \"Asseyezvous.\", which can be translated back to \"Sit down.\". Now, a well known fact is that different languages make different distinctions; for instance, the English pronoun you corresponds to French toi or vous, depending on number and degree of politeness. If French paraphrases are extracted using English as a pivot, then the toi/vous distinction will typically disappear, such that \"Asseyez-vous.\" and \"Assieds-toi.\" emerge as paraphrases, because they can both be translated as \"Sit down.\". Whether this is desirable or not depends on the application. However, if multiple pivot languages are used rather than one, more distinctions can be preserved. Bannard and Callison-Burch (2005) use four pivot languages in order to identify English paraphrases. Denkowski and Lavie (2010) use one, two, or three pivot languages for their five target languages. Ganitkevitch and Callison-Burch (2014) produce paraphrases for an impressive number of 21 languages, but they limit themselves to using one language, English, as their pivot (in order to be able to use syntactic information, which is available only for English). Opusparcus contains paraphrases in six European languages representing four different language branches: German, English, Swedish (Germanic), French (Romance), Russian (Slavic), and Finnish (Finnic). For each of the six languages, all other five languages are used as pivots. Thirdly, simplicity is reflected in several aspects of the work. On one hand, only full sentences, so called sentential paraphrases, are produced, unlike Ganitkevitch et al. (2013) , Ganitkevitch and Callison-Burch (2014), and Pavlick et al. (2015) , who also extract sub-sentential paraphrases, such as individual word pairs, and include the counts of all such fragments in their reported figures. On the other hand, typically subtitles are fairly short, which makes it easier to evaluate and annotate the paraphrase candidates, unlike the complex sentences in the news data of Dolan and Brockett (2005) . Furthermore, sub-sentential features or syntactic constraints (Callison-Burch, 2008) are not utilized to assess the likelihood that two sentences are paraphrases. If one favors similar sentence structures, there is a risk to miss some interesting idiomatic variation, such as in \"It's what we do.\" \u2194 \"This is our job.\". Finally, particular to this work is that paraphrases and scores for ranking paraphrases are symmetric. The two phrases are equal, for instance in contrast to the incorporation of fine-grained entailment relations (Pavlick et al., 2015; Bowman et al., 2015) and the asymmetric conditional probabilities used by Bannard and Callison-Burch (2005) . The rest of this article is split into two main blocks, followed by some concluding remarks. The data sets and annotation scheme are described in Section 2. Alternative ranking functions that can be utilized to produce large paraphrase corpora are evaluated in Section 3. Category Description Examples Good \"Green\" The two sentences can be used in the same situation and essentially \"mean the same thing\". It was a last minute thing. \u2194 This wasn't planned. Honey, look. \u2194 Um, honey, listen. I have goose flesh. \u2194 The hair's standing up on my arms. Mostly good \"Light green\" It is acceptable to think that the two sentences refer to the same thing, although one sentence might be more specific than the other one, or there are differences in style, such as polite form versus familiar form. Hang that up. \u2194 Hang up the phone. Go to your bedroom. \u2194 Just go to sleep. Next man, move it. \u2194 Next, please. Calvin, now what? \u2194 What are we doing? Good job. \u2194 Right, good game, good game. Mostly bad \"Yellow\" There is some connection between the sentences that explains why they occur together, but one would not really consider them to mean the same thing. Another one? \u2194 Partner again? Did you ask him? \u2194 Have you asked her? Hello, operator? \u2194 Yes, operator, I'm trying to get to the police. Bad \"Red\" There is no obvious connection. The sentences mean different things. She's over there. \u2194 Take me to him. All the cons. \u2194 Nice and comfy. Table 1 : The four annotation categories used, with examples. Each category is also associated with a color, which corresponds to the color of a button in the user interface of the annotation tool. Data Sets and Annotation Scheme OpenSubtitles2016 (Lison and Tiedemann, 2016 ) is a collection of translated movie and TV subtitles from www. opensubtitles.org. OpenSubtitles2016, which is a subset of the larger OPUS collection 1 , provides a large number of sentence-aligned parallel corpora in 65 languages. When subtitles exist for the same film in multiple languages, then sentence alignments are available for each language pair. For the present work, fifteen such bitexts were used, that is, all language-pair combinations for the six target languages German, English, Finnish, French, Russian and Swedish. In principle, we work on full sentences only, and thus the terms sentence and phrase are used fairly interchangeably in this paper. Only one-to-one aligned sentences are used, that is, one sentence in the target language must be aligned with one sentence in the pivot language. There are occasional OCR errors and incorrect sentence segmentation in the data. For each language, the data have been partitioned into separate training, development, and test sets, based on the release year of the movie; the test sets were extracted from years ending in 4, development sets from years ending in 5, and training sets from the rest. Four different categories have been used when annotating sentence pairs. The annotation scheme is illustrated in Table 1. Other annotation schemes exist as well, such as slightly more complex, five-level Likert scales (Callison-Burch, 2008) . Training Sets The so-called training sets are orders of magnitudes larger than the development and test sets and consist of lists of automatically ranked sentence pairs, where a high rank means a higher probability that the two sentences are paraphrases. 1 OPUS (\"... the open parallel corpus\"): opus.lingfil. uu.se The training sets, or subsets of them, are intended to be used freely for any useful purpose. The training sets were produced as follows: First, around 1000 randomly selected sentence pairs were annotated by the author for each of the six languages. Then, an automatic ranking function was applied to all sentence pairs (annotated and unannotated alike), as explained later in Section 3. By extrapolating from the manually annotated data points to the entire set, an estimate of the quality of the training sets can be obtained. The result for English is shown in Figure 1 . Another view to the quality of the training sets is provided in Table 2 , where approximate corpus sizes are given for each of the six languages, at three different accuracy levels. Language 95% 90% 75% German (de) 590,000 1,000,000 4,700,000 English (en) 1,000,000 1,500,000 7,000,000 Finnish (fi) 480,000 640,000 3,000,000 French (fr) 940,000 2,400,000 11,000,000 Russian (ru) 150,000 170,000 3,400,000 Swedish (sv) 240,000 600,000 1,400,000 Table 2 : Number of phrase pairs in the training sets at three different cut-off points, where 95%, 90%, and 75% of the sentence pairs are estimated to be \"Good\" or \"Mostly good\" paraphrases. Development and Test Sets Whereas the training sets have been produced semiautomatically, the development and test sets consist exclusively of sentence pairs that have been annotated manually. This is to guarantee the high quality of these sets. However, quality comes at the expense of quantity, so the development and test sets are smaller than the training sets. The number of annotations produced for each language are shown i Table 3 The sentence pairs to be annotated manually were subject to more rigorous pre-filtering than the sentence pairs in the training sets. In the data, there are many sentences that differ only slightly from each other, such as: \"He is not your friend.\" \u2194 \"He isn't your friend.\". It would have been a waste of human labor to have such simple and predictable variations annotated manually. Therefore, only pairs of sentences that differ sufficiently from each other are accepted into the development and test sets. The difference is mea-sured using relative edit distance; in general, the edit distance between the two sentences has to be at least 0.4 times the length of the shorter of the sentences (and for very short sentences containing less than 24 characters, the distance threshold is even higher). Two persons annotated every sentence pair. If the annotators agreed on the category, the annotation was accepted as is. If the annotators disagreed but picked adjacent categories (such as \"Good\" versus \"Mostly good\" or \"Mostly good\" versus \"Mostly bad\"), then the annotation was also accepted, but the lower category was assigned (such that \"Mostly good\" and \"Mostly bad\" yields \"Mostly bad\"). If there was stronger disagreement between the annotators (such as \"Mostly good\" versus \"Bad\"), then the sentence pair was discarded. The annotators were also able to discard a sentence pair, if the language of either sentence was wrong or there were spelling or grammar errors. The number of trashed sentences turned out to be highest for French and Russian: It appears that French orthography is complex and mistakes are fairly common in written text. In the Russian data, some non-Russian Cyrillic as well as Latin characters show up occasionally, apparently because of inaccurate optical character recognition (OCR). The detailed outcome of the annotation effort is summarized in Table 4 for the development sets and Table 5 for the test sets. Automatic Ranking of Paraphrase Candidates For the data sets that are intended to be used as training sets, a number of ranking schemes have been tested in order to identify paraphrases. Five of the ranking schemes are presented below, followed by a description how these approaches were evaluated. In the examples, English is used as our target language, and we are looking for English paraphrases. In the actual experiments, English was just one of the languages, and the same procedure was carried out for German, Finnish, French, Russian, and Swedish, as well. Conditional Probability Bannard and Callison-Burch (2005) propose a conditional paraphrase probability P (e 2 |e 1 ) as the probability that the English phrase e 1 is translated to a foreign phrase f i , which in turn is translated back into another English phrase e 2 . Since there are typically multiple possible foreign translations, we need to marginalize over the different possible f i : P (e 2 |e 1 ) = i P (e 2 |f i )P (f i |e 1 ) (1) This ranking formula tends to assign high ranks to phrase pairs, where e 1 is more specific than e 2 . For instance, consider the case, where e 1 is \"I was taken from my family when I was a boy.\" and e 2 is \"I was taken from my family.\". In the English-French parallel corpus, both English phrases have been aligned with the French phrase f 1 : \"On m'a enlev\u00e9 \u00e0 ma famille.\". However, e 1 occurs aligned against f 1 only once, whereas e 2 21 times. Thus, P (f Table 4 : Detailed breakdown of the results of the annotation of the development sets. A sentence pair qualifies as a \"good\" paraphrase, when both annotators have chosen the \"good\" category, visualized as a green button in the annotation tool. A sentence pair qualifies as \"mostly good\", when either one annotator has pushed the green button and the other annotator has pushed the light green button or both annotators have chosen the light green button. Similarly, sentence pairs have been categorized as \"mostly bad\" or \"bad\", if both annotators have agreed on the same category or if the annotators ended up pushing adjacent buttons. Sentence pairs were discarded in the following scenarios: The pair was trashed, if at least one of the annotators judged it to contain incorrect spelling or grammar. The sentence pair was also discarded, if the annotators disagreed about the category by more than one step on the four-level scale. Table 5 : Detailed breakdown of the results of the annotation of the test sets. Exactly the same procedure was applied as for the development sets. Annotators were unaware of which set a particular sentence pair belonged two; in fact, most annotators were unaware of the existence of separate development and test sets. 1 |e 1 ) is high (=1), Good This tendency produces numerous errors, when there are occasional misaligned phrases in the corpus, such as in: \"We're staying in the army.\" \u2192 \"Aah.\", where \"We're staying in the army.\" has been aligned against French \"Aah.\" once, which in turn has been aligned with English \"Aah.\" 1401 times. Joint Probability Instead of a conditional probability, which is asymmetric, one can use the corresponding joint probability, which includes a prior probability, and is symmetric. Thus, the probability of e 1 being a paraphrase of e 2 is the same as the probability of e 2 being a paraphrase of e 1 : P (e 1 , e 2 ) = P (e 2 |e 1 )P (e 1 ) = P (e 1 |e 2 )P (e 2 ) (2) P (e 2 |e 1 ) and P (e 1 |e 2 ) are calculated as in Equation (1), and P (e 1 ) and P (e 2 ) are the (prior) probabilities of the phrases, which are simply estimated as relative frequencies over all sentences in the corpus. Now, at the top of the ranking, we find pairs consisting of frequently used phrases: \"Yes.\" \u2194 \"Yeah.\", \"Of course.\" \u2194 \"Sure.\", \"Hello.\" \u2194 \"Good morning.\", \"Are you okay?'' \u2194 \"Are you all right?\". A few spurious phrase pairs also score high, where it appears that two frequent phrases might have found a common translation mostly by chance, by the fact that they occur frequently in general: \"You 're welcome.\" \u2194 \"Sure.\", \"Yeah.\" \u2194 \"I am.\", \"Hi.\" \u2194 \"Goodbye.\", \"I do.\" \u2194 \"I know.\" Pointwise Mutual Information Pointwise Mutual Information (PMI) divides the joint probability by the probability that the two phrases e 1 and e 2 occur independently. Thus, PMI penalizes phrase pairs that co-occur mostly by chance, by the fact that they occur frequently in general: pmi(e 1 ; e 2 ) = log P (e 1 , e 2 ) P (e 1 )P (e 2 ) = log P (e 2 |e 1 ) P (e 2 ) = log P (e 1 |e 2 ) P (e 1 ) (3) This scoring favors phrase pairs e 1 and e 2 that have a limited set of translations f i , such that e 1 and e 2 are not aligned with phrases other than f i , and f i are not aligned with other phrases than e 1 and e 2 . For instance, the phrases \"You sound a little homesick.\" \u2194 \"Do you miss being home?\" have a common French translation \"Vous avez le mal du pays ?\", which occurs twice in the corpus, aligned once against each of the two English phrases. However, similarly to the conditional probability in Equation (1), PMI is sensitive to misaligned, infrequent sentences. The phrase pair \"Lost the phone now.\" \u2194 \"I'm from the agency.\" scores high because \"Lost the phone now.\" has been misaligned against French \"Je viens de l'agence.\", which occurs only twice. Joint Probability and PMI Combined Our experiments show that rather than using joint probability (2) or PMI (3) in isolation, we obtain a better ranking by multiplying the two together: P (e 1 , e 2 ) \u2022 pmi(e 1 ; e 2 ) = P (e 1 , e 2 ) log P (e 1 , e 2 ) P (e 1 )P (e 2 ) (4) This leverages the strengths and alleviates the shortcomings of the two approaches. Multiple Multilingual Parallel Corpora The four formulae presented above, (1), ( 2 ), (3), and (4), easily generalize beyond bilingual parallel corpora. One can simply concatenate all parallel corpora, such that English is kept on one side and the other (pivot) languages on the other side. All frequencies and probabilities are then calculated over the merged bitext as a whole. Interestingly, another approach can produce better results. PMI in Equation (3) may rank rare phrase pairs very high, and in case of misalignments, these pairs are unreliable. However, if a rare phrase pair ranks high in multiple bitexts, then this seems to signal much higher confidence. In order not to lose the information that a phrase pair emerges in multiple different corpora, rather than merging the parallel corpora into one, we can keep them separate. We then compute PMI scores separately for each bitext (English-German, English-Finnish, English-French, English-Russian, and English-Swedish). To obtain a combined score, we compute the sum of the PMI values obtained from each different corpus (English vs. pivot language L i ): i pmi(e 1 ; e 2 |L i ) = i log P (e 1 , e 2 |L i ) P (e 1 |L i )P (e 2 |L i ) (5) The probabilities are calculated exactly as previously. The notation merely highlights the fact that every value is conditioned on alignments between English and a specific pivot language L i . Since the number of languages is constant, the sum in (5) can also be interpreted as the average PMI across pivot languages. Evaluation of Ranking Schemes A symmetric score is desired in this work, and therefore the conditional probability in Equation (1) cannot be used as such. However, one could obtain a symmetric score by combining P (e 2 |e 1 ) and P (e 1 |e 2 ) in some way, such as taking the minimum, maximum or average value. In practice, this would make this score behave fairly similarly as the more elegantly formulated PMI in Equation (3), so the conditional probability scheme was not investigated further. This leaves us with the four remaining schemes in Equations (2), ( 3 ), (4), and (5). They were compared with the help of a set of phrase pairs that were drawn randomly from the training set and annotated manually, as described in Section 2.1. The training set is then reordered using the ranking scheme to be tested. Depending on the ranking scheme, the manually annotated phrase pairs will appear at different ranks in the full, ordered collection. An ideal ranking scheme will place the phrase pairs that are true paraphrases at the head of the ordering and the phrase pairs that are not paraphrases at the tail. The results were then plotted as in Figure 1 and compared visually. Across the six languages, the results were consistent: the best performing rankings were PMI summed over multiple corpora (5) followed by joint probability mutliplied by PMI (4). The types of phrase pairs that rank high are different in both cases: the former favors less frequent, more specific phrase pairs, such as \"It was a difficult and long delivery.\" \u2194 \"The delivery was difficult and long.\", whereas the latter favors frequent, less informative phrase pairs, such as: \"Excuse me.\" \u2194 \"I'm sorry.\". PMI summed over multiple corpora, in Equation ( 5 ), was judged to be the best ranking function. The final training sets were produced using this particular ranking. Conclusion Paraphrase extraction from movie subtitle data has been described in this paper. Six languages were included in this initial phase, but there is no principal reason why not more of the 65 languages in the OpenSubtitles2016 collection could be exploited. As there is considerable manual annotation effort involved, crowdsourcing could be considered; see, for instance, Tschirsich and Hintz (2013) . Another improvement could be to reduce the number of OCR errors that still occur in the data. Acknowledgments The following people have participated in the annotation effort: Thomas de Bluts, Aleksandr Semenov, Eetu Sj\u00f6blom, Mikko Aulamo, Olivia Engstr\u00f6m, Janine Siewert, Carola Carpentier, Svante Creutz, Yves Scherrer, Anders Ahlb\u00e4ck, Sami Itkonen, Riikka Raatikainen, Kaisla Kajava, Tiina Koho, Oksana Lehtonen, Sharid Lo\u00e1iciga S\u00e1nchez, Tatiana Batanina, and the author himself. The annotation tool was implemented by Mikko Aulamo. The author is grateful to Mikko and the annotators for their very valuable contributions. The author would also like to thank professor J\u00f6rg Tiedemann together with the anonymous reviewers for their valuable comments and suggestions as well as the Academy of Finland for the financial support of the annotations through Project 314062 in the ICT 2023 call on Computation, Machine Learning and Artificial Intelligence. Furthermore, warm thanks go to Mietta Lennes and the Language Bank of Finland (Kielipankki) for publishing and hosting Opusparcus as part of their corpus collections. , C. and Callison-Burch, C. (2005). Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL '05, pages 597-604, Ann Arbor, Michigan. Association for Computational Linguistics. Bibliographical References Bannard"
}