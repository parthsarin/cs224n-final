{
    "article": "Extreme Multi-label classification (XML) is an important yet challenging machine learning task, that assigns to each instance its most relevant candidate labels from an extremely large label collection, where the numbers of labels, features and instances could be thousands or millions. XML is more and more on demand in the Internet industries, accompanied with the increasing business scale / scope and data accumulation. The extremely large label collections yield challenges such as computational complexity, inter-label dependency and noisy labeling. Many methods have been proposed to tackle these challenges, based on different mathematical formulations. In this paper, we propose a deep learning XML method, with a word-vector-based self-attention, followed by a ranking-based AutoEncoder architecture. The proposed method has three major advantages: 1) the autoencoder simultaneously considers the inter-label dependencies and the feature-label dependencies, by projecting labels and features onto a common embedding space; 2) the ranking loss not only improves the training efficiency and accuracy but also can be extended to handle noisy labeled data; 3) the efficient attention mechanism improves feature representation by highlighting feature importance. Experimental results on benchmark datasets show the proposed method is competitive to state-of-the-art methods. Introduction and Related Work In multi-label classification (Tsoumakas and Katakis, 2007; Zhang and Zhou, 2014) , one assigns multiple labels to each instance. Multi-label classification has many real-word applications: for example, a movie may be associated with multiple genres, a web page may contain several top-ics, and an image can be tagged with a few objects. In these classification tasks, labels often exhibit complex dependencies: for example, Documentary and Sci-Fi are usually mutually exclusive movie genres, while Horror and Thriller are typically highly correlated. Predicting labels independently fails to capture these dependencies and suffers suboptimal performance (Tsoumakas and Katakis, 2007; Ghamrawi and McCallum, 2005; Li et al., 2016) . Several methods that capture label dependencies have been proposed, including Conditional Random Fields (CRF) (Lafferty et al., 2001; Ghamrawi and McCallum, 2005) , Classifier Chains (CC) (Read et al., 2011; Dembczynski et al., 2010) , Conditional Bernoulli Mixtures (CBM) (Li et al., 2016) , and Canonical Correlated AutoEncoder (C2AE) (Yeh et al., 2017) . However, these methods typically only work well on smallto-medium scale datasets. Extreme multi-label classification (XML) is a multi-label classification task in which the number of instances, features and labels are very large, often on the order of thousands to millions (Zubiaga, 2012; Bhatia et al., 2015) . It has numerous real-world applications such as merchandise tagging and text categorization. Although the label vocabulary is large, typically each instance only matches a few labels. The scale of the classification task, the inter-dependent labels, and label sparsity all pose significant challenges for accurate and efficient classification. Many methods have been proposed for extreme multi-label classification. We group them into different categories and describe representative methods in each category. Independent Classification: A popular method is to divide the multi-label classification problem into multiple binary classification prob-lems (Tsoumakas and Katakis, 2007; Hariharan et al., 2012; Babbar and Sch\u00f6lkopf, 2017; Yen et al., 2016 Yen et al., , 2017)) . A typical implementation is to treat labels independently and train one-vs-all classifiers for each of the labels. These independent classifiers can be trained in parallel and thus are computationally efficient in practice. Ignoring the inter-label dependency also enables efficient optimization algorithm, which further reduces computational cost. However, ignoring label dependency inherently limits prediction accuracy. A competitive method in this category is called PD-Sparse (Yen et al., 2016) , with a variant of the Block-Coordinate Frank-Wolfe training algorithm that exploits data sparsity and achieves complexity sub-linear in the number of primal and dual variables. PD-Sparse (Yen et al., 2016) shows better performance with less training and prediction time than 1-vs-all Logistic Regression or SVM on extreme multi-label datasets. Tree Based Classifiers: Following the success of tree-based algorithms in binary classification problems, people also proposed tree-based algorithms for multi-label classification (Agrawal et al., 2013; Weston et al., 2013; Prabhu and Varma, 2014) , which achieve promising prediction accuracy. Similar to decision trees, these methods make classification decisions in each branch split. Different from decision trees, each split evaluates all features, instead of one, to make a decision. Also, each decision is for a subset of labels rather than one label. Finally, via ensembling and parallel implementation, trees can boost their prediction accuracy with practically affordable computational cost. Among these tree based classifiers, FastXML (Prabhu and Varma, 2014) further optimizes an nDCG-based ranking loss function and achieves significantly higher accuracy than other peer methods. Embedding: A major difficulty of extreme multilabel classification is the large number of labels. When labels are inter-dependent, one can attempt to find a lower dimensional latent label space from which one can fully reconstruct the original label space. Over the past decade, many methods were proposed to find this latent label space. In early work, methods were proposed to linearly project the original label space into a lowerdimension space and reconstruct predictions from that space (Tai and Lin, 2012; Balasubramanian and Lebanon, 2012) . However, there are two as-sumptions: (1) the label dependency is linear and (2) the label matrix is low-rank, which do not always hold, as reflected by the low prediction accuracy of these methods. To overcome the limitation of the linear assumption, different methods were proposed using non-linear embeddings, including kernels, sub-sampling (Yu et al., 2014) , feature-aware (Lin et al., 2014; Yeh et al., 2017) and pairwise distance preservation (Bhatia et al., 2015) . Among these methods, SLEEC (Bhatia et al., 2015) stands out for less training time and higher accuracies. SLEEC introduces a method for learning a small ensemble of local pairwise distance preserving embeddings which allows it to avoid the low-rank and linear-dependency assumption. Deep Learning: Deep learning has not been well studied for XML, although it has achieved great successes in binary and multi-class classification problems (Lin et al., 2017; Kim, 2014) . FastText (Grave et al., 2017) reconstructs a document representation by averaging the embedding of the words in the document, followed by a softmax transformation. It is a simple but very effective and accurate multi-class text classifier, as demonstrated in both sentiment analysis and multi-class classification (Grave et al., 2017) . However, FastText may not be directly applicable for more complicated problems, like XML. BoW-CNN (Johnson and Zhang, 2014) learns powerful embedding of small text regions by applying CNN to high-dimensional text data. The embedding of all regions are sent to one or multiple convolutional layers, a pooling layer and the output layer at the end. XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with a hidden bottleneck layer much smaller than the output layer. However, this method has a few drawbacks. First, it is trained using the binary cross entropy loss. This loss tends to be sensitive to label noise, which is frequently observed in extreme multi-label data. Since the label vocabulary is large, it is quite common for human annotator to miss relevant tags. When the classifier's prediction (which might be correct) disagrees with the annotation, the cross entropy loss can potentially assign an unbounded penalty to the classifier during training procedure. The second issue is that because labels are trained independently as separate binary classification tasks, their prediction probabilities/scores may not be directly comparable. This is problematic because in many applications the requirement is to rank all labels according to their relevance, as opposed to making an independent binary decision on each label. The third defect is that XML-CNN requires raw documents as input since it adopts the CNN structure on top of sentences (Kim, 2014) ; this is problematic when datasets are given in other formats such as bag-of-words for text. C2AE (Yeh et al., 2017) uses a ranking loss as the training objective. But the ranking loss employed there needs to compare all (positive label, negative label) pairs, and therefore does not scale well to extreme data. Furthermore, C2AE only takes the bag-of-words representation (onehot encoding) as the input, which makes it harder to learn powerful representations from extreme multi-label dataset. Our Contribution In this paper, we propose a new deep learning method to address extreme multi-label classification. Our contributions are as follows: \u2022 Motivated by the recent success of attention techniques, we propose an efficient attention mechanism that can learn rich representations from any type of input features, including but not limited to bag-of-words, raw documents and images. \u2022 Inspired by C2AE, our proposed model projects both features and labels onto common latent spaces wherein correlations between features and labels are exploited. By decoding this latent space into the original label space in the prediction stage, the dependencies between labels are implicitly captured. \u2022 We propose a margin-based ranking loss that is simultaneously more effective for extreme settings and more tolerant towards noisy labeling. Data Format In XML, we are given a set of label candidates Y = {1, 2, . . . , L}. The dataset D consists of features and labels: D = {(x i , y i )} N i=1 , wherein N is number of data, and each instance x \u2208 R V (V is the feature dimension) matches a label subset y \u2286 Y, which can be written as a binary vector y = {0, 1} L , with each bit y l representing the presence or absence of label l. Given such dataset, our goal is to build a classifier c: R V \u2192 {0, 1} L , mapping an instance to a subset of labels with arbitrary size. Auto-Encoder Network Inspired by the C2AE (Yeh et al., 2017) , we propose a Ranking-based Auto-Encoder (Rank-AE), as depicted in Figure 1 . Similar to C2AE, Rank-AE includes three mapping functions to be trained: a mapping from input features x to feature embeddings x h , denoted as F(x), where h is the embedding size; an encoder from output labels y to label embeddings y h as E(y); a decoder from label embeddings y h to output labels y , written as D(y h ). The proposed model is built on two assumptions: first, each instance can be represented from two different aspects, features x and labels y, so there exists a common latent space between x and y; second, labels can be reproduced by an autoencoder. Based on these two assumptions, we design the object function as below: L(D) = min F ,E,D L h (x h , y h ) + \u03bbL ae (y, y ) (1) wherein loss L h (x h , y h ) aims to find the common latent space for input x and output y and L ae (y, y ) enforces the output to be reproducible. \u03bb is a hyper-parameter to balance these two losses. During the training, the model learns a joint network including F, E and D to minimize the empirical loss Eq (1). During inference, a given input x will be first transformed into a vector in latent space xh = F(x), which will then be fed into the label decoder to compute the predictions \u0177 = D(x h ). It is worth mentioning that although the label encoder E is ignored during the prediction, it is able to exploit cross-label dependency during the label embedding stage (Yeh et al., 2017) . Recent work (Kurata et al., 2016; Baker and Korhonen, 2017 ) also show that using co-occurring labels information to initialize the neural network can further improve accuracy in multi-label classification. L h and L ae Loss Functions Learning Common Embedding (L h ). Minimizing the common hidden space loss L h has been proposed based on different considerations (Zhang and Schneider, 2011; Yeh et al., 2017; Shen et al., 2018) , ranging from canonical correlation analysis to alignment of two spaces with a perspective of cross-view. Since the hidden space is usually small and requires less computational cost, we simply employ the mean squared loss for L h . Reconstructing Output (L ae ). Unlike L h with small space, L ae loss usually involves a large number of labels. Moreover, L ae also directly affects the classification performance significantly since different loss functions lead to their own properties (Hajiabadi et al., 2017) . Accordingly, solving such problems with large scale and desirable properties presents open challenges in three aspects: 1) how to improve time efficiency, 2) how to produce comparable labels scores and 3) how to deal with noise labels. Unfortunately, most of the related deep learning methods only target one or two aspects. C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of input features or labels due to its inefficient implementation 1 . XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with hidden layers much smaller than the output layer with binary cross-entropy loss (BCE), which has linear complexity in number of labels. Despite this, BCE loss could neither capture label dependencies nor produce directly comparable label scores, since each label is treated inde-1 https://github.com/dhruvramani/ C2AE-Multilabel-Classification pendently. Moreover, BCE loss tends to be sensitive to label noise, which is frequently observed in XML data (Reed et al., 2014; Ghosh et al., 2017) . To void the aforementioned issues, we propose a marginal-based ranking loss in AutoEncoder: L ae (y, y ) = L P (y, y ) + L N (y, y ) (2) L P (y, y ) = n\u2208N (y) max p\u2208P (y) (m + y n \u2212 y p ) + (3) L N (y, y ) = p\u2208P (y) max n\u2208N (y) (m + y n \u2212 y p ) + (4) wherein N (y) is the set of negative label indexes, P (y) is the complement of N (y), and margin m \u2208 [0, 1] is a hyper-parameter for controlling the minimal distance between positive and negative labels. The loss consists of two parts: 1) L P targets to raise the minimal score from positive labels over all negative labels at least by m; 2) L N aims to penalize the most violated negative label under all positive labels by m. The proposed loss has the following attractive properties: 1) having linear complexity in number of labels O(L); 2) capturing the relative rankings between positive and negative labels; 3) tolerating the noisy labels with a tunable hyper-parameter m. To explain the last property, assume y n and y p are the predicted probabilities bounded in [0, 1], then on one extreme case with noise-free labels and m = 1, all positive labels are raised to probability 1, while negatives are penalized to 0; on the other extreme case with all random labels, e.g. from i.i.d. Bernoulli distribution, setting m = 0 indicates that the annotated labels are completely random noises. Dual Attention Extracting rich feature representations in XML is helpful for predicting more complicated labels structures, but on the other hand, requires an efficient and feasible method. A recent work (CBAM) (Woo et al., 2018) proposes a block attention module, with a Channel-Attention and a Spatial Attention for images tasks only, wherein Channel-Attention emphasizes information from channels, e.g. RBG, and Spatial-Attention pays attention to partial areas in an image. By sequentially applying channel and spatial attentions, CBAM is shown to be effective in images classification and object detection. We take advantage of the attentions in CBAM and apply it on textual data. Word Embedding  In our proposed attention module, it also consists of spatial-wise and channel-wise attentions. First, we force spatial-wise attention to attend on a list of important words in a way that simply multiply word embeddings by term-frequency or tf-idf (whichever is provided in the feature matrix). It is worth noting that spatial-wise module does not involve any parameters, but it efficiently captures the importance of words with numerical statistics, like tf-idf. We demonstrate spatial-wise attention on the left side of Figure 2 , where the input x = (I, V ) contains bag-of-words vector w 1 w 2 w 4 w 3 v 1 V 2 V 4 V 3 V I x F 2 F 1 Attention a 1 a 2 a 3 a 4 X A m 1 m 2 m 3 m 4 P E V' M I = (w 1 , w 2 , . . . , w n ) T \u2208 R n and tf-idf vector V = (v 1 , v 2 , . . . , v n ) T \u2208 R n . Bag-ofwords I are fed into an embedding layer E = (e 1 , e 2 , . . . , e n ) T \u2208 R n\u00d7C to get the word embeddings, where e j \u2208 R C is word embedding vector of w j . Then we multiply word embeddings by V to obtain weighted word embeddings: V = (v 1 e 1 , v 2 e 2 , . . . , v n e n ) T \u2208 R n\u00d7C . The channel attention is designed to emphasize the significant aspects by assigning different weights on bits in a word embedding. For example, in the word embedding of \"apple\", some of the bits may reflect fruit, while others may indicate the company name. To achieve this, we adopt the excitation network from the SENet (Hu et al., 2017) with a slight increase in model complexity. The excitation network includes two fully connected layers with a non-linearity activation function in between, see the top-right part in Figure 2 : reduction ratio r. After obtaining the attention matrix A, we can apply those attentions to the weighted word embeddings to get a re-scaled word embedding matrix For all datasets, we reserve another 20% of training data as validation for tuning hyper-parameters. A T = \u03c3 F 2 \u03b4(F 1 V T ) = (a 1 , a 2 , . . . , a n ) (5) wherein A \u2208 R n\u00d7C , M: M = V \u2022 A = (m 1 , m 2 , . . . , m n ) T \u2208 R n\u00d7C , After tuning, all models are trained on the entire training set. Among these datasets, three of them are only provided with BoW feature matrix: Delicious, Mediamill (dense feature matrix extracted from image data) and RCV, which are only feasible for the non-deep learning methods (SLEEC, FastXML, PDSparse) and Rank-AE. We provide both feature matrix and raw documents for IMDb, EURLex and Wiki10, which are feasible for both deep learning and non-deep learning methods. For those data with both formats, we remove the words from the raw documents that do not have corresponding BoW features so that the vocabulary size is the same for both deep and non-deep learning methods. Evaluation Metrics. To evaluate the performances of each model, we adopt the metrics that have been widely used in XML: Precision at top k (P @k), and the Normalized Discounted Cummulated Gains at top k (n@k) (Bhatia et al., 2015; Prabhu and Varma, 2014; Yen et al., 2016; Liu et al., 2017) . P @k is a measure based on the fraction of correct predictions in the top k predicted scoring labels and n@k is a normalized metric for Discounted Cumulative Gain: P @k = 1 k l\u2208rank k (\u0177) y l (6) DCG@k = l\u2208rank k (\u0177) y l log(l + 1) (7) nDCG@k = DCG@K min(k,|y|) l=1 1 log(l+1) (8) wherein the rank k returns k largest indices of the prediction \u0177 in a descending order, and |y| is the number of positive labels in ground truth. In the results, we report the average P @k and n@k on testing set with k = 1, 3, 5 respectively. Hyper-parameters. In Rank-AE, we use the fixed neural network architecture, with two fully connected layers in both Encoder and Decoder, and one fully connected layer following Embedding & Atten network in Feature Embedding. We also fix most of the hyper-parameters, including hidden dimension h (100 for small number of labels data and 200 for large ones), word embedding size C = 100, and reduction ratio r = 4. The remaining hyper-parameters, such as balance \u03bb between L h and L ae , margin m in L ae , and others (decay, learning rate) in the optimization algorithms, are tuned on validation set. In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove 4 , a pre-trained word embeddings of 100 dimensions; if it is not, e.g. Mediamill, Delicious and RCV, a random initialization is employed. For the existing methods with the same train/test split, we take the scores from the original papers for SLEEC, FastXML and PD-Sparse directly. For the new datasets and splits, the hyperparameters are tuned on the validation set for all methods, as suggested in their papers. Comparisons with Related Methods We evaluate the proposed Rank-AE with other six state-of-the-art methods, SLEEC, FastXML, PD-Sparse, FastText, Bow-CNN and XML-CNN, which are the leading methods among their categories. Among them, FastText, Bow-CNN and XML-CNN only take raw documents, which are not available for Delicious, Mediamill and RCV datasets. For Rank-AE, we adopt the raw text as the input for IMDb, and feature matrix for the rest. The performances evaluated on P @k and n@k with k = 1, 3, 5 are summarized in Table 2 (a) and (b) separately. As reported, Rank-AE reaches the best performances on two datasets (IMDb and EURLex) out of 6 datasets, while SLEEC achieves the best performances on Mediamill and Wiki10, and FastXML performs the best on Delicious and RCV. In general, SLEEC and FastXML are very competitive to each other in non-deep learning methods, but PD-Sparse performs worse. Rank-AE always performs better than PD-Sparse with at least 1% increase, up to almost 20% improvement on Delicious data. When compared with FastXML, Rank-AE outperforms on 4 datasets with 1% to 10% growth, but underperforms on Delicious and RCV with 1% decrease. SLEEC, as the best non-deep learning method in our experiments, performs almost identical to Rank-AE, but on IMDb data, it performs 7% \u223c 15% less than non-deep methods, and even worse than Rank-AE. Comparing Rank-AE with deep learning methods, we narrow down to three datasets with available raw documents: IMDb, EURLex and Wiki10. As shown in Table 2 , FastText and Bow-CNN, not planned for XML but for multi-class, perform much worse than XML-CNN and Rank-AE as expected. On the other hand, XML-CNN achieves close performance to Rank-AE: with similar performance on IMDb dataset, but lower scores on EURLex and Wiki10 with 2% drop in P @k and n@k. In spite of this, Rank-AE, trained on feature matrix for EURLex and Wiki10, surprisingly performs better than XML-CNN on raw data. In the comparisons, there is no such method that  could perform the best on all datasets. We discover that each dataset has its own intrinsic properties, such as diversity of labels, number of features, average number of relevant labels per instance and average number of training instances per label, see Table 1 . All those properties will affect training procedure, for example, how much flexibility a model should be in order to explain labels well by the given training data. Because those factors are always changing from data to data, they also influence the performances on different models. In order to have a reasonable comparisons, we report the average ranking score for each method. To compute the average ranks, we first rank the methods based on their performance in each row in Table 2, then average them through all rows, and re-port the final ranking scores in the last row of each table. The average ranking scores show that Rank-AE is the best model with ranking scores 1.78 in P @k and 1.89 in n@k. Comparisons with Noise Labels As mentioned previously, noisy labels in XML are a quite common issue in the real-world applications (Yeh et al., 2017; Ghosh et al., 2017) , but our proposed marginal ranking loss naturally mitigates this problem. Since IMDb is a real-world dataset with relatively clean labels, we conduct the noise experiments on it. In the experiments, we control the noise labels in two different ways: 1) missing labels: changing each positive label from y l = 1 to y l = 0 with certain rate, 2) both missing and invalid labels: flipping either from positive to negative or from negative to positive with a noise rate. The noise rates are varied from 0% to 60% on 80% of the training set, and the rest of 20% is noise-free validation set for model selection. We select five algorithms: FastXML, PD-Sparse, XML-CNN, Rank-AE and BCE-AE, wherein BCE-AE is our proposed method but using binary cross-entropy loss in L ae (y, y ). Comparing BCE-AE with Rank-AE can be used to verify whether the robustness to label noise is due to the use of marginal ranking loss. The performances are reported on the same clean test set, shown in Figure 3 . Rank-AE consistently outperforms other four approaches and has the best robustness tolerating noise labels. Besides, FastXML and PD-Sparse are more tolerant to missing noises than XML-CNN, which may due to XML-CNN has greater capacity and thus more prone to over-fitting the noise. Furthermore, when comparing Rank-AE with BCE-AE, both of which share the same structure but have different loss functions, the proposed marginal-based ranking loss seems to be robuster than binary crossentropy loss. More Analysis in Rank-AE Ablation Study. The effectiveness and robustness of Rank-AE have been demonstrated in the previous section. However, it is not clear to us yet that if the effectiveness benefits from the proposed components, such as attention mechanism and marginal ranking loss. To further understand the impacts from these two factors, we conduct a controlling experiment with three different settings: 1) removing the Attention component A in Figure 2 from Rank-AE, in which case V is directly passed to the average pooling to obtain x , called No attn; or 2) examining the performances by replacing the marginal ranking loss (L ae ) with a binary cross entropy loss, named No loss; or 3) keeping the original Rank-AE without any change. In Figure 4 , P @k is reported on the six datasets for the ablation experiment, because n@k is similar to P @k, thus eliminated here. The comparisons results show that Rank-AE without any change works better than the other two on all datasets consistently, especially on Wiki10. First, channel-attention extracts richer information from the word embeddings by introducing the channel weights. Thus, it is more suitable when classification tasks become more complicated and a word more likely represents multiple aspects. Second, Rank-AE gains some advantage of tolerating noise labels with marginal ranking loss comparing to BCE loss. We could even further infer that IMDb and RCV may have relatively less noise labels since the performance does not benefit much from the marginal ranking loss. Channel-Attention Visualization. Our channelattention is implemented by an excitation network, which is adopted from SENet (Hu et al., 2017) and only applied to images before. To demonstrate its effectiveness and feasibility on textual data, we . employ the visualization tool (Lin et al., 2017) to highlight important words based on the attention output. Specifically, we run our method on IMDb dataset, wherein each instance is a movie story associated with relevant genres as labels. Instead of extracting V matrix using the proposed spatialwise attention, we obtain a fixed size embeddings from a bidirectional LSTM on variable length of sentence, fed to our channel-attention network. Through the channel-attention network, we can observe the attention matrix A for each input document. By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualization tool 5 . We randomly select three movies from IMDb testing set (See Figure 5 ). By looking at the highlighted regions, we can see that the proposed channel-attention is able to focus more on the words that are highly related to the topics. Conclusion In this paper, we propose a marginal ranking loss, which not only predicts comparable labels scores between labels, more suitable for ranking metrics, 5 The visualization tool is provided by https://github.com/kaushalshetty/ Structured-Self-Attention but also consistently performs better on noisy labeling data, with both missing and invalid labels. In addition, the dual-attention component allows Rank-AE to learn more powerful feature representations efficiently. By integrating those components, Rank-AE usually achieves the best or the second best on six benchmark XML datasets comparing with other outstanding methods in state-ofthe-art. Acknowledgements Most of this work was done when Bingyu Wang and Wei Sun were interning at JD.Com Inc. We thank Javed Aslam, Virgil Pavlu and Cheng Li from Khoury College of Computer Sciences at Northeastern University for comments that greatly improved the manuscript. We would also like to show our gratitude to our anonymous NAACL-HLT reviwers for the helpful suggestions to make the paper better.",
    "funding": {
        "defense": 0.0,
        "corporate": 1.981665536443522e-06,
        "research agency": 0.0,
        "foundation": 1.9361263126072004e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The acknowledgements section specifically thanks individuals from Khoury College of Computer Sciences at Northeastern University and mentions the gratitude towards anonymous NAACL-HLT reviewers, but it does not explicitly mention any funding sources from defense, corporate entities, research agencies, or foundations. Therefore, based on the provided text, there is no direct evidence of funding from the specified categories."
}