{
    "article": "Syntax has been a useful source of information for statistical RST discourse parsing. Under the neural setting, a common approach integrates syntax by a recursive neural network (RNN), requiring discrete output trees produced by a supervised syntax parser. In this paper, we propose an implicit syntax feature extraction approach, using hidden-layer vectors extracted from a neural syntax parser. In addition, we propose a simple transition-based model as the baseline, further enhancing it with dynamic oracle. Experiments on the standard dataset show that our baseline model with dynamic oracle is highly competitive. When implicit syntax features are integrated, we are able to obtain further improvements, better than using explicit Tree-RNN. Introduction Discourse parsing is an important task in natural language processing (NLP), which aims to identify the relations between text units in documents. It has received great attention (Sagae, 2009; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Wang et al., 2017; Braud et al., 2017; Liu and Lapata, 2017) , especially by performing the task based on rhetorical structure theory (RST) (Mann and Thompson, 1988) . The RST-based parsing represents a document by a hierarchical tree, where leaf nodes are basic text units referred as elementary discourse unit (EDU), and non-terminal nodes define the discourse relations between adjacent tree nodes. Figure 1 shows an example, where each discourse relation has two parts, specifying its label and indicating its nuclearity, respectively. Early studies adopt traditional statistical models for this task, using sophisticated manually-designed discrete features (Soricut and Marcu, 2003; Sagae, 2009; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017) . Recently, inspired by the success of neural network models in NLP (Collobert et al., 2011; Devlin et al., 2014) , several neural models for RST discourse parsing have been proposed as well (Li et al., 2014; Li et al., 2015b; Li et al., 2016; Braud et al., 2016; Braud et al., 2017; Liu and Lapata, 2017) . Compared with statistical models, neural models exploit low-dimensional dense features, being able to avoid the feature sparsity problem, and on the other hand well-designed neural network structures such as long short term memory (LSTM) (Schmidhuber and Hochreiter, 1997) are capable of capturing high-order compositional features as well as global features automatically. In addition, we can use pre-trained neural word embeddings (Mikolov et al., 2013) on large scale corpus for neural network initialization. These characteristics show that neural models are promising for RST discourse parsing. Intuitively, syntax is a potential avenue for the task, as it offers long-distance syntax relations between sentential words as well as key components of sentences. Syntax features have been demonstrated helpful in statistical models with human designed features (Soricut and Marcu, 2003; Sagae, 2009; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Feng and Hirst, 2014; Heilman and Sagae, 2015) . For neural network models, recursive neural network is a natural choice to represent tree-structural syntax trees globally. Only Li et al. (2015b) apply such a neural structure to incorporate syntax trees for RST discourse parsing. Other studies still adopt discrete syntax features proposed by statistical models, feeding them into neural network models (Braud et al., 2016; Braud et al., 2017) . The above approaches model syntax trees in an explicit way, requiring discrete syntax parsing outputs as inputs for RST parsing. These approaches may suffer from the error propagation problem. Syntax trees produced by a supervised syntax parsing model could have errors, which may propagate into discourse parsing models. The problem could be extremely serious when inputs of discourse parsing have different distributions with the training data of the supervised syntax parser. Recently, Zhang et al. (2017) suggest an alternative method, which extracts syntax features from a Bi-Affine dependency parser (Dozat and Manning, 2016) , and the method gives competitive performances on relation extraction. It actually represents syntax trees implicitly, thus it can reduce the error propagation problem. In this work, we investigate the implicit syntax feature extraction approach for RST parsing. In addition, we propose a transition-based neural model for this task, which is able to incorporate various features flexibly. We exploit hierarchical bi-directional LSTMs (Bi-LSTMs) to encode texts, and further enhance the transition-based model with dynamic oracle. Based on the proposed model, we study the effectiveness of our proposed implicit syntax features. We conduct experiments on a standard RST discourse TreeBank (Carlson et al., 2003) . First, we evaluate the performance of our proposed transitionbased baseline, finding that the model is able to achieve strong performances after applying dynamic oracle. Then we evaluate the effectiveness of implicit syntax features extracted from a Bi-Affine dependency parser. Results show that the implicit syntax features are effective, giving better performances than explicit Tree-LSTM (Li et al., 2015b) . Our codes will be released for public under the Apache License 2.0 at https://github.com/yunan4nlp/NNDisParser. In summary, we mainly make the following two contributions in this work: (1) we propose a transitionbased neural RST discourse parsing model with dynamic oracle, (2) we compare three different syntactic integration approaches proposed by us. The rest of the paper is organized as follows. Section 2 describes our proposed models including the transition-based neural model, the dynamic oracle strategy and the implicit syntax feature extraction approach. Section 3 presents the experiments to evaluate our models. Section 4 shows the related work. Finally, section 5 draws conclusions. Transition-based Discourse Parsing We follow Ji and Eisenstein (2014) , exploiting a transition-based framework for RST discourse parsing. The framework is conceptually simple and flexible to support arbitrary features, which has been widely used in a number of NLP tasks (Zhu et al., 2013; Dyer et al., 2015; Zhang et al., 2016) . In addition, a transition-based model formalizes a certain task into predicting a sequence of actions, which is essential similar to sequence-to-sequence models proposed recently (Bahdanau et al., 2014) . In the following, we first describe the transition system for RST discourse parsing, and then introduce our neural network model by its encoder and decoder parts, respectively. Thirdly, we present our proposed dynamic oracle strategy aiming to enhance the transition-based model. Then we introduce the integration method of implicit syntax features. Finally we describe the training method of our neural network models. The Transition-based System The transition-based framework converts a structural learning problem into a sequence of action predictions, whose key point is a transition system. A transition system consists of two parts: states and actions. The states are used to store partially-parsed results and the actions are used to control state transitions. The initial state is an empty state, and the final state represents a full result. There are three kinds of actions in our transition system: \u2022 Shift (SH), which removes the first EDU in the queue onto the stack, forming a single-node subtree. \u2022 Reduce (RD) (l,d), which merges the top two subtrees on the stack, where l is a discourse relation label, and d \u2208 {NN, NS, SN} indicates the relation nuclearity (nuclear (N) or satellite (S)). \u2022 Pop Root (PR), which pops out the top tree on the stack, marking the decoding being completed, when the stack holds only one subtree and the queue is empty. Given the RST tree as shown in Figure 1 , it can be generated by the following action sequence: {SH, SH, RD(attr,SN), SH, SH, RD(elab,NS), RD(elab,SN), PR}. Table 1 shows the decoding process in detail. By this way, we naturally convert RST discourse parsing into predicting a sequence of transition actions, where each line includes a state and next step action referring to the tree. Encoder-Decoder Previous transition-based RST discourse parsing studies exploit statistical models, using manuallydesigned discrete features (Sagae, 2009; Heilman and Sagae, 2015; Wang et al., 2017) . In this work, we propose a transition-based neural model for RST discourse parsing, which follows an encoder-decoder framework. Given an input sequence of EDUs {e 1 , e 2 , ..., e n }, the encoder computes the input representations {h e 1 , h e 2 , ..., h e n }, and the decoder predicts next step actions conditioned on the encoder outputs. Encoder We follow Li et al. (2016) , using hierarchical Bi-LSTMs to encode the source EDU inputs, where the first-layer is used to represent sequencial words inside of EDUs, and the second layer is used to represent sequencial EDUs. Given an input sentence {w 1 , w 2 , ..., w m }, first we represent each word by its form (e.g., w i ) and POS tag (e.g. t i ), concatenating their neural embeddings. By this way, the input vectors of the first-layer Bi-LSTM are {x w 1 , x w 2 , ..., x w m }, where x w i = emb(w i ) \u2295 emb(t i ), and then we apply Bi-LSTM directly, obtaining: {h w 1 , h w 2 , ..., h w m } = Bi-LSTM({x w 1 , x w 2 , ..., x w m }) (1) The second-layer Bi-LSTM is built over sequential EDUs. We should first obtain a suitable representation for each EDU, which is composed by a span of words inside a certain sentence. Assuming an EDU with its words by {w s , w s+1 , ..., w t }, after applying the first-layer Bi-LSTM, we obtain their representations by {h w s , h w s+1 ..., h w t }, then we calculate the EDU representation by average pooling: x e = 1 t \u2212 s + 1 t s h w k (2) When the EDU representations are ready, we apply the second-layer Bi-LSTM directly, resulting: S \u2190 Apply(S, a p ); {h e 1 , Decoder The decoder part is to predict the next-step action based on a given state. As mentioned before, each transition state has a sequence of subtrees on the stack and a sequence of unprocessed EDUs in the queue. We simply choose the top three stack subtrees (s 0 , s 1 , s 2 ) and the first EDU (q 0 ) in the queue as major clues for prediction, applying a feed-forward neural layer to calculate next-step action scores: o = W(h sbt s 0 \u2295 h sbt s 1 \u2295 h sbt s 2 \u2295 h e q 0 ) (4) where o is the output scores and W is a model parameter. The hidden vector h e q 0 is obtained directly from the encoder outputs. The hidden vectors h sbt s 0 , h sbt s 1 and h sbt s 2 are representations of partially-parsed subtrees. For efficiency, we exploit average pooling over the covering EDUs {e i , e i+1 , ..., e j } of a subtree s to derive its representation: h sbt s = 1 j \u2212 i + 1 j i h e k (5) Dynamic Oracle The transition-based model converts discourse parsing into an incremental action prediction problem. For a given state, we require its answer for training. When the state follows the traces of gold-standard actions, its answer is exactly the next gold-standard action. Majority work trains a classifier based on gold-standard state-answer pairs (Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Braud et al., 2017; Wang et al., 2017) . However, we usually have error states during the test stage, which are resulted by historical error actions. These error states have never been seen during training, which makes prediction difficult for them. The phenomenon brings inconsistency between training and testing. Here we adopt dynamic oracle to alleviate the above problem, which has been firstly exploited by Goldberg and Nivre (2012) for dependency parsing. The main idea is to add error states and their oracle actions into the training corpus randomly. On the one hand, by exploring a percentage of error states, the transition-based model is able to handle these states more confidently. On the other hand, by defining oracle actions over error states, the model is able to make best choices even when errors occur. In this work, we suggest a similar dynamic oracle strategy for our transition-based neural model. Algorithm 1 shows the pseudo codes. During training, we explore into predicted states randomly by a probability \u03b1. At each step, we generate a random number pick gold between [0,1]. If the value is less than \u03b1, we use the predicted state as the next-step input, making the training process consistent with the testing. The oracle function is defined by minimizing the future decoding loss referring to gold-standard RST trees, which is defined as follows: a g = RD, where s 0 , s 1 are top two stack subtrees, and G is the set of gold-standard subtrees. Simply speaking, if s 0 and s 1 can be composed as a subtree according to the gold-standard RST-tree, we apply RD, and otherwise we apply SH. Figure 2 : The framework of Bi-Affine neural parser. x w 1 x w i x w j x w n h 1 1 h 1 i h 1 j h 1 n h 2 1 h 2 i h 2 j h 2 Syntax Features Intuitively, syntax information could be a valuable source for RST parsing (Soricut and Marcu, 2003; Sagae, 2009; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Feng and Hirst, 2014; Heilman and Sagae, 2015) . Previously, the information is integrated into RST discourse parsing by manually-designed discrete features, except the work of Li et al. (2015b) , which exploits a bi-directional Tree-LSTM to model syntax trees. In addition, all previous studies use deterministic parsing outputs as inputs for RST discourse parsing, which may suffer from error propagation problem since syntax parsing outputs can have errors. To alleviate this problem, Zhang et al. (2017) suggest an alternative method, which extracts intermediate hidden outputs of a neural parsing model as inputs for relation-extraction. The method exploits an implicit way to incorporate syntax features without the the need of parsing outputs. Inspired by their work, we investigate the same method for RST discourse parsing. We follow Zhang et al. (2017) to use the Bi-Affine dependency neural parser of Dozat and Manning (2016) for syntax feature extraction. Zhang et al. (2017) formalize the Bi-Affine parser as an encoderdecoder architecture, and exploit the encoder outputs as inputs for relation extraction. Here we study the Bi-Affine parser in deep, examining other hidden layers as well. Next we will introduce the Bi-Affine parser briefly, and then describe our implicit syntax feature extraction method. The Bi-Affine Parser The Bi-Affine neural dependency parser has achieved the top performances (Dozat and Manning, 2016) . As shown by Figure 2 , the model has four components: (1) a three-layer Bi-LSTM (Bi-LSTM \u00d73 ) over the input sentential words; (2) a multi-layer perceptron (MLP) layer; (3) a Bi-Affine layer; (4) an arg max decoder. Their outputs are {h 1 1 , h 1 2 ..., h 1 n }, {h 2 1 , h 2 2 , ..., h 2 n }, {s l ij , i \u2208 [1, n], j \u2208 [1, n], l \u2208 L ( The set of dependency labels.)}, and {(head 1 , label 1 ), ..., (head n , label n )}, respectively, which are computed by follow formulas: {h 1 1 , h 1 2 , ..., h 1 n } = Bi-LSTM \u00d73 ({x w 1 , x w 2 , ..., x w n }) {h 2 1 , h 2 2 , ..., h 2 n } = MLP({h 1 1 , h 1 2 , ..., h 1 n }) s l i,j = Bi-Affine(h 2 i , h 2 j , l) head i , label i = argmax {h,l} (s l i,h ) (7) Noticeably, the above formulas are only an approximate description of the Bi-Affine parser. For more details, one can refer to their paper. Implicit Syntax Feature Extraction Zhang et al. ( 2017 ) formalize the Bi-Affine parser as an encoder-decoder model, where the encoder part includes the neural structures ending by the Bi-LSTM \u00d73 , and they use the encoder outputs for implicit syntax feature extraction. Additionally, by carefully examining, we find that the MLP outputs have similar attributes to the Bi-LSTM \u00d73 outputs, both of which are aligned with the input sentential words perfectly, being closely related to the words at the aligned positions. In addition, the MLP outputs are closer to the final syntax parsing outputs, thus are able to encode syntax information more sufficiently. Concretely, for a given sentence {w 1 , w 2 , ..., w n }, we feed it into the Bi-Affine parser, extracting the encoder outputs {h 1 1 , h 1 2 , ..., h 1 n } or the MLP outputs {h 2 1 , h 2 2 , ..., h 2 n }, respectively. Then we concatenate them with the encoder outputs of our transition-based neural RST parsing model. The resulting vectors are used as inputs for the decoder part. The overall syntax incorporation process can be formalized as follows: {h 1 , h 2 , ..., h n } = {h w 1 \u2295 h * 1 , h w 2 \u2295 h * 2 , ..., h w n \u2295 h * n } (8) where * could be either 1 or 2, denoting the Bi-LSTM \u00d73 or the MLP outputs. Training We follow the majority work of transition-based deterministic neural models, using a cross-entropy loss plus with an l 2 regularization as our training objective. Given a state S and its oracle action a, we calculate the decode outputs by using equation ( 4 ), and then apply a softmax operation to obtain the oracle action probalility: p ag = exp(oa g ) a \u2208A exp(o a ) , which is then fed into the cross-entropy function: L(\u0398) = \u2212log(p ag ) + \u03bb 2 ||\u0398|| 2 (9) where \u0398 is the set of model parameter and \u03bb is an l 2 regularization factor. We train our model by online learning with mini-batch, updating model parameters by using Adam algorithm (Kingma and Ba, 2014). Experiments 3.1 Settings Data We follow previous studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Heilman and Sagae, 2015; Li et al., 2016; Wang et al., 2017; Braud et al., 2017) , conducting experiments by using the RST discourse Treebank (Carlson et al., 2003) . It has annotated 385 documents, where 347 for training and the remaining 38 for testing. All the documents are collected from Wall Street Journal (WSJ), belonging to newswire gene. We use 18 coarse-grained relations in our experiments. For preprocessing, we exploit the StanFord CoreNLP toolkit 1 (Manning et al., 2014) to lemmatize words and get their POS tags. To facilitate parameter tuning, we select 35 documents from the training corpus randomly as the development corpus. All experiments are conducted on manually segmented EDUs. Evaluation We adopt standard evaluation methods (Marcu, 2000) to test model performances, including three metrics Span, Nuclearity and Relation. The Span evaluates the capability of predicting tree skeletons, the Nuclearity evaluates tree skeletons and nuclearity indications, and the Relation evaluates tree skeletons together with discourse relations, while ignoring nuclearities. The Full evaluates the tree skeletons together with both nuclearity indications and discourse relations. We follow (Morey et al., 2017) to report the micro-averaged F scores. Hyper-Parameters There are several hyper-parameters in our proposed models. We set the values according to their development performances. We set sizes of all hidden vectors (e.g. the dimension of word embeddings, the Bi-LSTM outputs and etc.) by 200. The word embeddings are initialized by pre-trained GloVe embeddings 2 (Pennington et al., 2014) . For training, the l 2 regularization factor, the mini-batch size, the initial learning rate, the dropout probability and the max norm of gradient clipping are set by 10 \u22126 , 8, 0.001, 0.5 and 10, respectively. We train models iteratively on the entire training corpus by 60 rounds, and use the best-performance iteration on the development corpus as the final model. Development Results We conduct two sets of development experiments to show important factors of our proposed model, which are related to dynamic oracle and syntax features, respectively. Dynamic Orcale First, we examine the strategy of dynamic oracle. As shown in Algorithm 1, there is a hyper-parameter \u03b1 to control the exploration, which is used to produce extra training instances. Here we study how \u03b1 influences the model performances. Figure 3 shows the development performances with respect to \u03b1. The performances are relatively stable surrounding 0.7, where all metrics under both settings are close to their peak values. Thus we exploit \u03b1 = 0.7 as the final setting. Syntax Features There are two choices of our implicit syntax feature extraction approach. We can use either the encoder Bi-LSTM \u00d73 outputs or the MLP outputs as described in section 2.4. In addition, we implement a bidirectional dependency based Tree-LSTM (Teng and Zhang, 2017) as well for comparisons. Figure 4 shows the results. First, we find that syntax features 3 are very useful for RST discourse parsing, which is consistent with previous observations (Soricut and Marcu, 2003; Sagae, 2009; Braud et al., 2016) . Second, the implicit syntax feature extraction method is slightly better than explicit Tree-LSTM. In particular, the model can achieve the best performances by using the MLP outputs. Thus we exploit the MLP outputs as the final implicit syntax features in our RST discourse parsing model. Model Span Nuclearity Relation Full (Hayashi et al., 2016) 82.6 66.6 54.6 54.3 (Surdeanu et al., 2015) 82.6 67.1 55.4 54.9 (Joty et al., 2015) 82.6 68.3 55.8 55.4 (Feng and Hirst, 2014) 84.3 69.4 56.9 56.2 (Braud et al., 2016) 79.7 63.6 47.7 47.5 (Li et al., 2016) 82.2 66.5 51.4 50.6 (Braud et al., 2017) 81.3 68.1 56.3 56.0 (Ji and Eisenstein, 2014) Figure 6 : The span-level F-measures with respect to the span length. Final Results Table 2 shows the final results of our proposed neural models on the test corpus. Our baseline model achieves a Span F-measure of 85.0 and a Nuclearity F-measure of 71.0, which are exceed most models. When dynamic oracle and implicit syntax features are exploited, our final model achieves 60.2 on the Relation F-measure, resulting overall improvements 60.2 -57.6 = 2.6. The Span, Nucleartiy and Full metrics have similar tendencies as well. We compare the proposed neural model with other state-of-the-art systems as well. (Hayashi et al., 2016) is a greedy bottom-up parser with a linear SVM classification. (Surdeanu et al., 2015) is also a greedy bottom-up parser, which uses the perceptron for tree skeleton building and nuclearity indication, the logistic regression for relation labelling. (Joty et al., 2015) is a two-stage (intra-sentential then multisentential parsing) parser with dynamic conditional random field models, and (Feng and Hirst, 2014 ) is a two-stage parser with linear-chain conditional random field models. (Braud et al., 2016) is a sequence-tosequence hierarchical neural parser, and (Li et al., 2016 ) is an attention-based hierarchical neural parser. (Ji and Eisenstein, 2014 ) is a transition-based statisitical model with the representation learning. (Braud et al., 2017 ) is a transition-based neural model by embedding and composing a number of manuallydesigned sophisticated atomic features. As shown by Table 2 , we can see that transition-based models are able to achieve state-of-the-art performances under both discrete and neural settings, demonstrating the robustness of this framework. Our final model is able to achieve competitive results compared with these systems. Analysis In this subsection, we perform several analysis experiments on the test corpus to illustrate the benefits from dynamic oracle and syntax features, respectively. First, in order to understand the influence of dynamic oracle, we evaluate the Relation performances with respect to the EDU number. In our transition-based model, a subtree with more EDUs requires more transition actions to be produced. As introduced in section 2.3, our neural model with dynamic oracle is robust to error input states. Figure 5 shows the comparison results. We can find that the model with dynamic oracle performs better on spans of the EDU number between [2,4], which is consistent with our intuitions. However, when the EDU number increases to 5+, both settings perform very poorly. Second, we investigate the benefits by using our proposed syntax features. Intuitively, the dependency parsing outputs exploit tree structures to re-organize sentential words, thus a number of long-distance word pairs are connected directly. We would expect that spans covering a long range of words could be better handled with syntax features. We verify this assumption by comparing performances with respect to the word number in span. As shown by Figure 6 , we can see that syntax features do improve the performances of spans containing more words. The observation is consistent with our assumption. Related Work RST discourse parsing has been studied intensively since early (Marcu, 1997; Marcu, 1999; Soricut and Marcu, 2003) . Initial work focuses on statistical models by using manually-designed feature (Soricut and Marcu, 2003; Sagae, 2009; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017) . Among these studies, transition-based models have achieved state-of-the-art performances (Sagae, 2009; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Wang et al., 2017) . Recently, neural network models have been investigated. Braud et al. (2017) have proposed a transtion-based neural model by using several well-designed atomic features, embedding them directly and then feeding them into feed-forward neural networks. In this work, we suggest hierarchical Bi-LSTMs, following Li et al. (2016) to encode documents, which are more popular in document modeling (Li et al., 2015a; Chen et al., 2016) . Syntax features have been demonstrated useful for RST discourse parsing by a number of studies (Soricut and Marcu, 2003; Sagae, 2009; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Feng and Hirst, 2014; Heilman and Sagae, 2015) . The majority work exploits manually-designed features to achieve this goal. The work of Li et al. (2015b) have compared a Tree-LSTM structrue over syntax trees with a sequential LSTM over input sentences, finding that the syntax tree does not show better performances. In this work, we propose a different implicit feature extraction to represent syntax information, using them as a supplementary for RST discourse parsing. The exploration of dynamic oracle has been proposed for transition-based dependency parsing (Goldberg and Nivre, 2012), bringing significantly better performances for dependency parsing. Our dynamic oracle is mainly inspired by this work, and we apply it on RST discourse parsing. To our knowledge, it is the first work of transition-based RST parsing by using dynamic oracle. Zhang et al. (2017) suggest a new method of integrating syntax features implicitly. First they extract a sequence of hidden vectors from a supervised neural dependency parsing model, and then feed these implicit syntax features into a neural relation extraction model. The method has been also applied to targeted sentiment analysis (Gao et al., 2017) . We follow their work to investigate the implicit syntax feature extraction for RST discourse parsing, and improve this method accordingly for our task. Conclusion In this work, we investigated an implicit syntax feature extraction method for neural RST discourse parsing. First, we proposed a transition-based neural baseline, and further enhanced it with a dynamic oracle mechanism. Second, we examined the implicit syntax feature extraction method proposed by Zhang et al. (2017) and suggested use outputs of a different neural layer of a Bi-Affine dependency parsing model (Dozat and Manning, 2016) . We conducted experiments on standard RST discourse TreeBank (Carlson et al., 2003) to evaluate our proposed models. Results show that our transition-based parser is very competitive after applying dynamic oracle. Further, we find that the proposed implicit syntax features are highly effective, better than explicit Tree-LSTMs. Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper. This work is supported by National Natural Science Foundation of China (NSFC) grants 61672211 and 61602160, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Special business expenses in Heilongjiang Province (China) grant 2016-KYYWF-0183.",
    "abstract": "Syntax has been a useful source of information for statistical RST discourse parsing. Under the neural setting, a common approach integrates syntax by a recursive neural network (RNN), requiring discrete output trees produced by a supervised syntax parser. In this paper, we propose an implicit syntax feature extraction approach, using hidden-layer vectors extracted from a neural syntax parser. In addition, we propose a simple transition-based model as the baseline, further enhancing it with dynamic oracle. Experiments on the standard dataset show that our baseline model with dynamic oracle is highly competitive. When implicit syntax features are integrated, we are able to obtain further improvements, better than using explicit Tree-RNN.",
    "countries": [
        "China"
    ],
    "languages": [],
    "numcitedby": "53",
    "year": "2018",
    "month": "August",
    "title": "Transition-based Neural {RST} Parsing with Implicit Syntax Features"
}