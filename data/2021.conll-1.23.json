{
    "article": "The most straightforward approach to joint word segmentation (WS), part-of-speech (POS) tagging, and constituent parsing (PAR) is converting a word-level tree into a char-level tree, which, however, leads to two severe challenges. First, a larger label set (e.g., \u2265 600) and longer inputs both increase computational cost. Second, it is difficult to rule out illegal trees containing conflicting production rules, which is important for reliable model evaluation. If a POS tag (like VV) is above a phrase tag (like VP) in the output tree, it becomes quite complex to decide word boundaries. To deal with both challenges, this work proposes a two-stage coarse-to-fine labeling framework for joint WS-POS-PAR. In the coarse labeling stage, the joint model outputs a bracketed tree, in which each node corresponds to one of four labels (i.e., phrase, subphrase, word, subword). The tree is guaranteed to be legal via constrained CKY decoding. In the fine labeling stage, the model expands each coarse label into a final label (such as VP, VP * , VV, VV * ). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the pipeline approach on both settings of without and with BERT, and achieves new state-of-the-art performance. Introduction As shown in Figure 1 (a), the results of word segmentation (WS), part-of-speech (POS) tagging, and constituent parsing (PAR) can be organized in a unified hierarchical tree, where leaf nodes include words and their POS tags, and non-terminal nodes correspond to phrases (or constituents) with their syntactic label. Before the deep learning (DL) era, there had been intensive research interest in jointly modeling of the three tasks, i.e., WS-POS-PAR (Luo, 2003; Qian and Liu, 2012; Zhang et 2013; Wang et al., 2013) . The motivations are three-fold: 1) alleviating error propagation, 2) promoting knowledge sharing and interaction during inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015) . Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1 (b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with pre-DL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007) , the major characteristic of graphbased parsers (Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020) is that the score of a constituent tree is decomposed into scores of labeled spans (i.e., (2, 3, VP) or VP 2,3 ), without any consideration on production rules (i.e., VP 2,3 \u2192 ADVP 2,2 VP 3,3 ). For char-level graph-based WS-POS-PAR, there exist two severe challenges. (1) The first challenge is high model complexity. On the one hand, a char sequence is almost twice longer than its word sequence. On the other hand, the size of the la-bel set is very large (e.g., \u2265 600) after transforming char-level trees into Chomsky normal form (CNF), as illustrated in Figure 2 (a). Both factors greatly increase time and memory complexity. (2) The second challenge is rule conflict. Since neural graph-based parsers do not consider production rules, there is no way to guarantee the output tree is legal. Conflicting rules, such as \"VP \u2192 VA * VA * \" (two subwords into a phrase) and \"VA \u2192 ADVP+AD VP+VA\" (POS tag above phrases), make it very difficult to decide word boundaries and thus unfeasible to evaluate model outputs. In this work, we propose a two-stage coarse-tofine labeling framework for joint WS-POS-PAR, which can deal with both challenges. In the coarse labeling stage, the joint model outputs a bracketed tree with coarse labels (i.e., phrase, subphrase, word, subword). The constrained CKY algorithm is used to guarantee that the predicted tree contains no illegal production rules. In the fine labeling stage, the model expands each coarse label into a final fine-grained label (such as VP, VP * , VV, VV * ). Experiments on three Chinese Treebank (CTB) benchmark datasets show the joint framework is superior to the pipeline framework on both settings of without and with BERT (Devlin et al., 2019) , and achieves new state-of-the-art performance. We will release our code at https://github.com/ ironsword666/JointParser. 2 Joint WS-POS-PAR as Char-Level Tree Parsing From Word-level Tree to Char-level Tree As illustrated in Figure 1 , a word-level constituent tree with POS tags in 1(a) is converted into a charlevel tree in 1(b). In a word-level tree, a leaf node corresponds to a word with its POS tag. In contrast, in a char-level tree, a leaf node is always a character, and POS tags become non-terminal nodes. We use flat structures for multi-char words, such as \"NN 1,3 \u2192 \u5de7 \u514b \u529b\".  Figure 2 (a) gives an example of char-level tree in CNF. The non-terminal labels can be divided into four categories. (1) NN * 1,1 means that \"\u5de7\" is a subword. (2) NP+NN 1,3 means that \"\u5de7\u514b\u529b\" corresponds to a complete word with NN as its POS tag, and the word is also a constituent labeled as NP. Chomsky normal form (CNF). (3) IP * 1,6 spans a subphrase. ( 4 ) IP 1,7 means that the span x 1 ...x 7 corresponds to a complete phrase with IP as its label. Non-terminal labels corresponding to subwords and subphrases, such as NN * and IP * , are created during breaking nary (n > 2) production into binary ones. Complex labels such as NP+NN are created during collapsing unary chains such as \"NP \u2192 NN \u2192 \u5de7\u514b\u529b\". Two Challenges Directly performing char-level constituent parsing confronts two severe challenges, i.e., high complexity, and illegal trees containing word-vs-phrase conflicts. (1) The high complexity challenge. This challenge concerns two aspects. (i) large number of non-terminal labels. A mass of new labels is introduced by CNF transformation, most of which are produced from the collapsing process of consecutive unary chains. Taking CTB7 as an example, the number of labels grows from 63 to 638 after CNF transformation for joint WS-POS-PAR. In contrast, for word-level constituent parsing, the number of labels grows from 28 to 265. (ii) longer inputs. Moreover, since words are broken into characters in char-level parsing, the same sentence is almost twice the length of that in word-level parsing, posing further challenge on model efficiency. In summary, longer inputs and a larger label set both increase computational cost from the aspects of time and GPU memory size. This work handles the high complexity issue in two ways. First, we adopt the two-stage parsing framework of Zhang et al. (2020) . The first stage produces a bracketed tree without labeling nonterminal nodes; the second stage independently predicts the label for each node. In this way, with the CKY decoding algorithm, the total time complexity decreases to O(n 3 + n|L|), versus O(n 3 + n 2 |L|) for one-stage parsing. In terms of space complexity, two-stage parsing needs to compute only O(n 2 + n|L|) scores, versus O(n 2 |L|) for onestage parsing. Second, we prune low-frequency labels by replacing them with most-similar high-frequency labels in the training data. On CTB7, less-than-10 pruning decreases the label number from 638 to 293. (2) The word-vs-phrase conflict challenge. In the DL era, mainstream graph-based constituent parsers (Stern et al., 2017; Kitaev and Klein, 2018; Zhang et al., 2020) determine the label for each nonterminal node in a local manner. As a result, there is no guarantee that productions in 1-best parse trees are all legal. For example, \"IP \u2192 VP * VP\" is an illegal production, where \"VP * \" should probably be changed to \"IP * \". In the case of word-level constituent parsing, a popular way to handle such conflicts is ignoring such intermediate nodes during evaluation, since they are created to meet CNF. However, in the case of char-level parsing, such conflicts are very difficult to handle. Figure 2 presents two typical kinds of conflicts. In Figure 2(b), the top POS tag \"VA\" tells that \"\u5f88\u7f8e\u5473\" is a word; \"ADVP+AD\" tells that \"\u5f88\" is a word and a phrase; \"VP+VA\" tells that \"\u7f8e\u5473\" is word and a phrase. However, there is only one way to segment a sentence into a word sequence. In Figure 2 (c), the marked \"VP\" tells that \"\u7f8e\u5473\" is a phrase; the two \"VA * \" tell that \"\u7f8e\" and \"\u5473\" are subwords. Obviously there is a gap in between. In summary, we can see that word-vs-phrase conflicts pose great challenge on reliable evaluation of joint WS-POS-PAR results. One possible way to handle this is to post-process illegal trees into legal ones based on heuristic rules, which, however, is a non-trivial task itself. BiLSTM \u00d7 3 \u5b83 It0 \u5f88 Very1 \u7f8e Beautiful2 \u5473 Taste3 MLP l MLP r MLP l MLP r Coarse-grained Biaffines Fine-grained Biaffines s(i, j, c) s(i, j, f ) As an extension to the two-stage parsing framework of Zhang et al. (2020) , this work proposes a coarse-to-fine labeling framework to elegantly deal with the word-vs-phrase conflict issue. In the first stage, the joint model not only produces a bracketed tree, but also assigns a coarse label to each node. We ensure the coarsely-labeled bracketed tree is legal via constrained CKY decoding. In the fine labeling stage, the model expands each coarse label into a final fine-grained label via local classification. P P W W W W * W * IP VP VP+VA NP+PN ADVP+AD VA * VA * Coarse-to-Fine Proposed Two-stage Coarse-to-fine Labeling Framework As shown in Figure 3 , our proposed framework consists of two independent decoders and a shared encoder. During training, two parts of losses are directly added, i.e., the weights of two tasks are both one 2 , and the whole model is trained via standard multi-task learning (MTL). Given a sentence consisting of n characters x = x 1 , . . . , x n , we use (i, j, c) to denote a candidate span x i ...x j with a coarse label c, and (i, j, f ) to denote one with a fine-grained label f . Section 3.3 illustrates how to obtain their corresponding scores, i.e., s(i, j, c) and s(i, j, f ). Stage 1: Produce Tree w/ Coarse Labels While Zhang et al. (2020) produce a completely unlabeled bracketed tree in the first stage, we obtain a bracketed tree with coarse-grained labels, as illustrated by the left tree on top of Figure 3 . As illustrated in Table 1 , all fine-grained nonterminal labels are categorized into four types of coarse-grained labels. We denote a bracketed tree with coarse labels as y = {(i, j, c)}. As a mainstream practice adopted by most neural graph-based parsers, the tree score is decomposed into scores of spans. s(x, y) = (i,j,c)\u2208y s(i, j, c) (1) Constrained CKY decoding. During evaluation, given an input sentence, the model aims to produce a highest-scoring tree. \u0177 = arg max y\u2208Y(x) s(x, y) (2) where Y(x) denotes the set of all possible legal trees for x. Using the standard CKY algorithm, we can efficiently find the highest-scoring tree. However, the legality of the tree, i.e., free from word-vs-phrase conflicts, cannot be guaranteed, since the scores of spans are mutually independent and the model does not consider production rules at all. In this work, we propose an efficient constrained CKY algorithm to elegantly handle this issue, as illustrated in Algorithm 1. The basic idea is introducing two sets of production scores into decoding, i.e., binary productions s(A \u2192 BC), and unary leaf productions s(A \u2192 x i ), where A/B/C are non-terminal labels in coarse label set N and x i is a character. For illegal productions such as \"P \u2192 W * W * \", we set their scores into \u2212\u221e. Scores of all legal productions are set to 0. By ruling out illegal productions, we can exclude all illegal trees eventually. Algorithm 1 Constrained CKY Algorithm. 1: input: labeled span scores s(i, j, A); production scores s(A \u2192 a) and s(A \u2192 BC) 2: initialize all \u03c0(\u2022 \u2022 \u2022 ) to 0 3: for i := 1 to n do 4: for A \u2208 N do 5: \u03c0(i, i, A) = s(i, i, A) + s(A \u2192 x i ) 6: for width := 2 to n do 7: for i := 1 to n \u2212 width + 1 do 8: j := i + width \u2212 1 9: for A \u2208 N do 10: \u03c0(i, j, A) = max i\u2264k<j, {B,C}\u2208N 2 \u03c0(i,k,B)+\u03c0(k+1,j,C) +s(i,j,A)+s(A\u2192BC) 11: return \u0177 TreeCRF training loss. We follow (Zhang et al., 2020) and adopt a global TreeCRF loss. As a probabilistic model, TreeCRF defines the conditional probability of a bracketed tree as: p(y|x) = e s(x,y) Z(x) \u2261 y \u2208Y(x) e s(x,y ) (3) where Z(x) is known as normalization term and can be efficiently computed via the inside algorithm. Analogously to CKY decoding, we employ a constrained inside algorithm to exclude illegal trees from Y(x), which leads to improved performance. Please see Subsection 5.2 for more details. Given a sentence x and its gold-standard coarsely-labeled tree y * , the TreeCRF training loss is: loss coarse (x, y * ) = \u2212 log p(y * |x) (4) Stage 2: Expand into Fine-grained Labels In the second stage, we predict a fine-grained label for each non-terminal node in the 1-best tree \u0177 obtained in the first stage. f = arg max f \u2208F (\u0109) s(i, j, f ) (5) where \u0109 is the predicted coarse-grained label for span (i, j), and F(\u0109) is the corresponding finegrained label set for \u0109. Cross-entropy training loss. As a typical multiclass classification task, we adopt cross-entropy loss independently in the second stage. loss fine (x, z * ) = (i,j,f )\u2208z * \u2212 log e s(i,j,f ) f e s(i,j,f ) (6) where z * is the gold-standard tree with fine-grained labels. Model Details for Span Scoring This subsection introduces model details for obtaining span scores, i.e., s(i, j, c) and s(i, j, f ), corresponding to the lower part of Figure 3 , most of which are borrowed from Zhang et al. (2020) . Input vectors. The input vector for the i-th position is composed of a char embedding and a bichar embedding. e i = emb(x i ) \u2295 emb(x i x i+1 ) (7) Encoder. We employ three BiLSTM (Hochreiter and Schmidhuber, 1997) layers over the input vectors for context encoding. The final representation of i-th position is: h i = f i \u2295 b i+1 (8) where f i and b i are the output vectors of the toplayer forward and backward LSTMs for the i-th position. Span boundary representation. Two MLPs are used to obtain two representation vectors for each position. r l i ; r r i = MLP l (h i ) ; MLP r (h i ) (9) where l/r represent the situation that the i-th position is the left/right boundary of some spans. Biaffine scoring. We compute scores of coarsely labeled spans (i, j, c) using biaffine operations. s(i, j, c) = r l i 1 T W coarse c r r i 1 ( 10 ) where W coarse c \u2208 R 501\u00d7501 is the biaffine parameter matrix for the label c. Analogously, for scoring fine-grained labeled spans (i, j, f ), two extra MLPs and an extra set of Biaffines are used. Since the number of finegrained labels are very large, we use a smaller biaffine matrix dimension W fine f \u2208 R 101\u00d7101 to reduce computational cost. Label Pruning As mentioned above, CNF transformation introduces a large number of labels, largely due to collapsing of unary rules, such as \"VP+NP\". Taking CTB7 as an example, the number of fine-grained labels in CTB7 increases from 63 to 638, among which 355 labels occur less than 10 times in the training data and only 172 labels simultaneously appear in all train/dev/test datasets. To boost computational efficiency, we propose to reduce the number of fine-grained labels by projecting a low-frequency label into a most similar one. For example, both \"CP+VV\" and \"NP+CP+IP+VP+VV\" can be used to replace \"NP+CP+CP+IP+VP+VV\", and the latter is chosen to recover more constituents. Table 1 and 2 show the results after pruning low-frequency labels (< \u03b1 = 10). Effect of label pruning on model performance is investigated in Figure 4 . The Pipeline Framework We adopt the typical cascaded pipeline framework as our baseline. In the training phase, three separate models (WS/POS/PAR) are separately trained. In the evaluation phase, the first step is word segmentation; then the word sequence is fed into the POS tagger; finally the word sequence is fed into the constituent parser. Constituent parsing. We directly adopt the competitive two-stage CRF parser of Zhang et al. (2020) , which is also backbone of our joint WS-POS-PAR model. As a word-level parser, its input is composed of two parts: 1) word embedding and 2) CharLSTM word representation (Lample et al., 2016) . e (w) i = emb(w i ) \u2295 CharLSTM(w i ) (11) Other components, such as encoding and span scoring, are the same with those in Section 3.3. POS tagging. The inputs and encoder of the POS tagging model are the same with the above wordlevel constituent parser. We then feed h i into a MLP layer to directly calculate the scores of different POS tags. We directly use the local word-level cross-entropy loss in the training phase, and greedily select the highest-scoring tag for each word during evaluation. Our preliminary experiments show that applying a global CRF layer does not lead to better performance. Duan et al. (2007) . For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) to redefine the span by the index of its beginning and ending characters. We adopt the standard constituent-level labeled precision, recall, F-score (Par P/R/F) as the evaluation metrics for constituent parsing, where POS tags are discarded. Metrics of labeled precision, recall, and F-score (Tag P/R/F) of POS tags are used to evaluate the POS tagging task. For word segmentation, the unlabeled precision, recall, F-score (Seg P/R/F) of POS tagging are served as the evaluation metrics. Parameter settings. We adopt most of hyperparameter settings from Zhang et al. (2020) with the exception that we increase the decay steps of learning rate from 5000 to 7500/6500/15000 (CTB5/CTB5-big/CTB7) in favour of more training steps. In our joint framework, the dimensions of character and bichar embedding are 100, which are identical to the dimensions of word embedding and output vector of CharLSTM in the pipeline framework. All embeddings are randomly initialized. For experiments with BERT, we adopt \"bertbase-chinese\" to get contextual char/word representations. Both pipeline and joint frameworks use 3 BiLSTM layers with the hidden size of 400 as their encoder. The dimensions of coarse-grained and fine-grained label MLPs are 500 and 100 respectively. We use the Adam optimizer (Kingma and Ba, 2014) for all models. All dropout ratios are 0.33. The mini-batch size is 5000 words (or characters). We stop the training process when the peak performance on dev data does not increase in 100 consecutive epochs. Results on the Dev Datasets Results of the pipeline framework. The upper part of Table 3 shows the results of pipeline framework. We conduct the study on dev datasets to understand the influence of automatic word segmentation. The \"Pipeline\" row presents the results of using automatic word segmentation as the input of the tagger and parser. Then, in the \"w/ gold-standard WS\" row, we feed the gold-standard word segmentation into the tagger and the parser. Although the performance of automatic word seg- mentation is fairly high, the tagger and parser using automatic word segmentation are still distinctly inferior to those using gold-standard segmentation. It proves that the error propagation issue has a strong impact on the performance of downstream tasks. Results of the joint framework. First, to study the impact of constrained inside algorithm, we retain illegal trees in Y(x). The \"Joint\" row uses the constrained inside and CKY algorithms. In the \"w/o constrained inside\" row, we try to use a standard inside algorithm in the training phase but still perform constrained CKY decoding. It is interesting that using the constrained inside algorithm achieves a consistent improvement in POS tagging and constituent parsing. One possible reason is that the constrained inside algorithm produces a more precise normalization term Z(x) while the standard inside algorithm creates a mismatch between training and decoding. Second, we explore the impact of threshold value \u03b1 used to prune low-frequency labels for joint framework. As shown in Figure 4 , we select four different threshold values (i.e., 1, 10, 100, 1000) and conduct experiments on CTB7 dev dataset. We can clearly see that (Figure 4 (a)) the number of labels is halved from 638 to 293 when \u03b1 is set to 10. A fairly large value 1000 should be chosen to further prune labels to a quarter of the original. In terms of model performance, different \u03b1 values have very slight impact, except constituent parsing, in which \u03b1 = 10 outperforms others and higher thresholds lead to worse results. Efficiency Comparison Table 4 compares two frameworks from the aspects of speed and parameter size. Our models are all run on a Nvidia Tesla V100 GPU. The comparison is made on CTB7 test dataset. First, in the upper part of the table, we list the results of the three tasks (WS/POS/PAR). As illustrated in the Comparisons with previous works. Finally, we list results of all recent related works of the joint framework on CTB5. Please note that our pipeline framework is already remarkably superior to the previous state-of-the-art joint framework proposed by Zhang et al. (2013) . Compared with it, our joint framework achieves an absolute improvement of 0.57, 0.84, 3.14 on word segmentation, POS tagging, and constituent parsing respectively on CTB5 test. Related Work Coarse-to-fine Parsing. Directly performing dynamic programming parsing with fine-grained grammar is computationally expensive from the aspects of time and memory size. Coarse-to-fine parsers introduce complexity gradually by coarseto-fine utilizing a sequence of grammars. Charniak and Johnson (2005) succeed in reranking n-best parses with a two-stage method. They predict nbest parses with a coarse-grained grammar in the first stage, then, the best parse is selected from the n-best parses with the finer-grained second-stage grammar which makes use of more important contextual information. Charniak et al. (2006) extend the basic two-stage coarse-to-fine parsing to multi-stage parsing by constructing multiple levels of grammars. They cluster constituent labels of the raw treebank into coarser categories. For example, in the most coarse grammar, there only exists one label \"P\" which corresponds to the phases. The proposed clustering idea in their work is similar to that used in our work. The work of Petrov and Klein (2007) also builds a multi-stage parser which constructs a sequence of increasingly refined grammars in a automatic fashion. However, the final grammar is finer than the raw treebank grammar. Joint modeling in the pre-DL era. In this part, we try to briefly discuss works on joint modeling of word segmentation, POS tagging, and parsing, which were conducted in the pre-DL era, and most of which are transition-based (shift-reduce) systems instead of graph-based. Joint POS-ConPAR. Wang and Xue (2014) integrate POS tagging and constituent parsing based on a transition-based parsing model. They modify the action to assign a POS tag when the word is shifted into the stack, making POS tagging as a part of parsing naturally. Joint POS-DepPAR. Li et al. (2011) extend graph-based dependency parsing to handle POS tagging simultaneously based on a dynamic programming algorithm for joint decoding. Hatori et al. (2011) combine POS tagging and dependency parsing into a shift-reduce parsing system. Joint WS-POS-ConPAR. Qian and Liu (2012) design a graph-based joint decoding algorithm to aggregate the outputs of three independently trained models for the three tasks. Zhang et al. (2013) integrate word segmentation and POS tagging information into parse trees, and then use a transition-based parser to perform joint learning and decoding. They manually annotated the intrastructures of words on the CTB5 dataset, which is proven to be very beneficial. Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter-and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can bring considerable gains to parsing performance. Compared with their work, we simply use left binarization to decide the intra-word structures, and our model achieves much higher performance by adopting the state-of-the-art BiLSTM-based parsing model. Our main contribution is proposing an elegant way to handle word-vs-phrase conflicts, which unfortunately are not mentioned in their work. Kurita et al. (2017) for the first time apply neural networks to jont WS-POS-DepPAR. They enhance the transition-based parser with char/word embeddings and BiLSTM which alleviate the efforts of feature engineering. Li et al. (2018) annotate a character-level dependency treebank for joint WS-POS-DepPAR. They use a neural characterlevel transition-based parsing model to reduce computational cost. Yan et al. (2020) focus on joint WS-DepPAR. They adopt the character-level graphbased parsing approach. For intra-word dependencies, a character is designed to modify its subsequent character with the label of \"app\". Wu and Zhang (2021) split joint WS-POS-DepPAR into joint WS-POS and joint WS-DepPAR by using a shared character-level encoder and two independent decoders. They adopt outputs from joint WS-POS as final word segmentation results. Conclusions In this work, we propose a two-stage coarse-to-fine labeling framework of joint WS-POS-PAR, which is shown to be able to handle both challenges for char-level parsing, i.e., high model complexity, and word-vs-phrase label conflicts. In particular, we believe that it is a novel and elegant way to remove illegal trees using constrained CKY decoding in the coarse-labeling stage. It is also very interesting that under a TreeCRF loss, performance is consistently improved by ruling out illegal trees in computing the normalization factor via the constrained inside algorithm. Experiments and analysis on three Chinese benchmark datasets (CTB5 and CTB7) show that 1) the joint framework is clearly superior to the pipeline framework, and achieves new state-of-theart performance; 2) our joint framework has very similar parsing speed compared with the pipeline counterpart, but only requires a half number of parameters. Acknowledgements We would like to thank the anonymous reviewers for their helpful comments. This work was supported by National Natural Science Foundation of China (Grant No. 61876116 and 62176173), and a Project Funded by the Priority Academic Program Development (PAPD) of Jiangsu Higher Education Institutions.",
    "abstract": "The most straightforward approach to joint word segmentation (WS), part-of-speech (POS) tagging, and constituent parsing (PAR) is converting a word-level tree into a char-level tree, which, however, leads to two severe challenges. First, a larger label set (e.g., \u2265 600) and longer inputs both increase computational cost. Second, it is difficult to rule out illegal trees containing conflicting production rules, which is important for reliable model evaluation. If a POS tag (like VV) is above a phrase tag (like VP) in the output tree, it becomes quite complex to decide word boundaries. To deal with both challenges, this work proposes a two-stage coarse-to-fine labeling framework for joint WS-POS-PAR. In the coarse labeling stage, the joint model outputs a bracketed tree, in which each node corresponds to one of four labels (i.e., phrase, subphrase, word, subword). The tree is guaranteed to be legal via constrained CKY decoding. In the fine labeling stage, the model expands each coarse label into a final label (such as VP, VP * , VV, VV * ). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the pipeline approach on both settings of without and with BERT, and achieves new state-of-the-art performance.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "0",
    "year": "2021",
    "month": "November",
    "title": "A Coarse-to-Fine Labeling Framework for Joint Word Segmentation, {POS} Tagging, and Constituent Parsing"
}