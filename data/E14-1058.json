{
    "article": "The question of data reliability is of first importance to assess the quality of manually annotated corpora. Although Cohen ' s \u03ba is the prevailing reliability measure used in NLP, alternative statistics have been proposed. This paper presents an experimental study with four measures (Cohen's \u03ba, Scott's \u03c0, binary and weighted Krippendorff ' s \u03b1) on three tasks: emotion, opinion and coreference annotation. The reported studies investigate the factors of influence (annotator bias, category prevalence, number of coders, number of categories) that should affect reliability estimation. Results show that the use of a weighted measure restricts this influence on ordinal annotations. They suggest that weighted \u03b1 is the most reliable metrics for such an annotation scheme. Introduction The newly intensive use of machine learning techniques as well as the need of evaluation data has led Natural Language Processing (NLP) to develop large annotated corpora. The interest for such enriched language resources has reached domains (semantics, pragmatics, affective computing) where the annotation process is highly affected by the coders subjectivity. The reliability of the resulting annotations must be trusted by measures that assess the inter-coders agreement. While medecine, psychology, and more generally content analysis, have considered for years the issue of data reliability, NLP has only investigated this question from the mid 1990s. The influential work of Carletta (1996) has led the \u03ba statistic (Cohen, 1960) to become the prevailing standard for measuring the reliability of corpus annotation. Many studies have however questioned the limitations of the \u03ba statistic and have proposed alternative measures of reliability. Krippendorff claims that \"popularity of \u03ba notwithstanding, Cohen's \u03ba is simply unsuitable as a measure of the reliability of data\" in a paper presenting his \u03b1 coefficient (Krippendorff, 2008) . Except for some rare but noticeable studies (Arstein and Poesio, 2005) , most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to various statistical biases (Arstein and Poesio, 2008) . On the opposite, this paper investigates experimentally these questions on three different tasks: emotion, opinion and coreference annotation. Four measures of reliability will be considered: Cohen's \u03ba (Cohen, 1960 ), Scott's \u03c0 (Scott, 1955) and two measures of Krippendorff's \u03b1 (Krippendorff, 2004 ) with different distance. Section 2 gives a comprehensive presentation of these metrics. Section 3 details the potential methodological biases that should affect the reliability estimation. In section 4, we explain the methodology we followed for this study. Lastly, experimental results are presented in section 5. Reliability measures Any reliability measure considers the most pertinent criterion to estimate data reliability to be reproducibility. Reproducibility can be estimated by observing the agreement among independent annotators (Krippendorff, 2004) : the more the coders agree on the data they have produced, the more their annotations are likely to be reproduced by any other set of coders. Pure observed agreement is not considered as a good estimator since it does not give any account to the amount of chance that yields to this agreement. For instance, a restricted number of coding categories should favor chance agreement. What must be estimated is the proportion of observed agreement beyond the one that is expected by chance: where A o is the observed agreement between coders and A e is an estimation of the possible chance agreement. Reliability metrics differ by the way they estimate this chance agreement. Cohen's \u03ba \u03ba \u03ba \u03ba (Cohen, 1960) defines chance as the statistical independence of the use of coding categories by the annotators. It postulates that chance annotation is governed by prior distributions that are specific to each coder (annotator bias). \u03ba was originally developed for two coders and nominal data. (Davies and Fleiss, 1982) has proposed a generalization to any number of coders, while (Cohen, 1968 ) has defined a weighted version of the \u03ba measure that fulfils better the need of reliability estimation for ordinal annotations: the disagreement between two ordinal annotations is no more binary, but depends on a Euclidian distance. This weighted generalization restricts however to a two coders scheme (Artstein and Poesio, 2008) : a weighted version of the multi-coders \u03ba statistics is still missing. Unlike Cohen's \u03ba, Scott's \u03c0 \u03c0 \u03c0 \u03c0 (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical independence of the data and the set of coding categories, independently from the coders. It considers therefore the annotation process and not the behaviour of the annotators. Scott's original proposal concerned only two coders. (Fleiss 1971) gave a generalisation of the statistics to any number of coders through a measure of pairwise agreement. Krippendorff's \u03b1 \u03b1 \u03b1 \u03b1 (Krippendorff, 2004) considers chance independently from coders like Scott's \u03c0, but data reliability is estimated depending on disagreement instead of agreement: where D o is the observed disagreement between coders and D e is an estimation of the possible chance disagreement. Another original aspect of this metrics is to allow disagreement estimation between two categories through any distance measure. This implies that \u03b1 handles directly any number of coders and any kind of annotation (nominal or ordinal coding scheme). In this paper, we will consider the \u03b1 statistics with a binary as well as a Euclidian distance, in order to assess separately the influence of the distance measure and the metrics by itself. Quality criteria for reliability metrics There is an abundant literature about the criteria of quality a reliability measure should satisfy (Hayes, 2007) . These works emphasize on two important points: \u2022 A trustworthy measure should provide stable results: measures must be reasonably independent of any factor of influence. \u2022 The magnitude of the measure must be interpreted in terms of absolute level of reliability: the statistics must come up with trustworthy reliability thresholds. These questions have mainly been investigated from a theoretical point of view. This section summarizes the main conclusions that should be drawn from these critical studies. Annotator bias and number of coders Annotator bias refers to the influence of the idiosyncratic behavior of the coders. It can be estimated by a bias index which measures the extent to which the distribution of categories differs from one coder's annotation to another (Sim and Wright, 2005) . Annotator bias has an influence on the magnitude of the reliability measures (Feinstein and Cicchetti,1990) . Besides, it concerns the invariance of the measures to the permutation or selection of annotators but also to the number of coders. A review of the literature shows that theoretical studies on annotator bias are not convergent. In particular, opposite arguments have been proposed concerning Cohen's \u03ba (Di Eugenio and Glass 2004 , Arstein and Poesio 2008 , Hayes, 2007) . This is why we have carried on experiments that investigate: \u2022 to what extent measures depend on the selection of a specific set of coders ( \u00a7 5.3), \u2022 to what extent the stability of the measures depends on the number of coders ( \u00a7 5.4). Arstein and Poesio (2005) have shown that the greater the number of coders is, the lower the annotator bias decreases. Our aim is to go further this conclusion: we will study whether one measure needs fewer coders than another one to converge towards an acceptable annotator bias. Category prevalence Prevalence refers to the influence on reliability estimation of a coding category under which a disproportionate amount of annotated data falls. It can be estimated by a prevalence index which measures the frequency differences of categories on cases where the coders agree (Sim and Wright, 2005) . When the prevalence index is high, chance-corrected measures are spuriously reduced since chance agreement is higher in this situation (Brennan and Sliman, 1992; Di Eugenio and Glass, 2004) . This yields some authors to propose corrected coefficients like the PABAK measure (Byrt and al., 1993) , which is a prevalence adjusted and annotator bias adjusted version of Cohen's \u03ba. The influence of prevalence will not be investigated here, since no category is significantly prevalent in our data. Number of coding categories The number of coding categories has an influence on the reliability measures magnitude: the larger the number of categories is, the less the coders have a chance to agree. Even if this decrease should concern chance agreement too, lower reliability estimations are observed with high numbers of categories (Brenner and Kliebsch, 1996) . This paper investigates this influence by comparing reliability values obtained with a 3-categories and a 5-categories coding scheme applied on the same data (see \u00a7 5.1). Interpreting the magnitude of measures in terms of effective reliability One last question concerns the interpretation of the reliability measures magnitude. It has been particularly investigated with Cohen's \u03ba. Carletta (1996) advocates 0.8 to be a threshold of good reliability, while a value between 0.67 and 0.8 is considered sufficient to allow tentative conclusion to be drawn. On the opposite, Krippendorff (2004b) claims that this 0.67 cutoff is a pretty low standard while Neuendorf (2002) supports an even more restrictive interpretation. Thus, the definition of relevant levels of reliability remains an open problem. We will see how our experiments should draw a methodological framework to answer this crucial issue. Introduction We have conducted experiments on three different annotation tasks in order to guarantee an appreciable generality of our findings. The first two experiments correspond to an ordinal annotation. They concern the affective dimension of language (emotion and opinion annotation). They have been conducted with na\u00efve coders to preserve the spontaneity of judgment which is searched for in affective computing. The third experiment concerns coreference annotation. It is a nominal annotation that has been designed to be used as a comparison with the previous ordinal annotations tasks. The corresponding annotated corpora are available (TestAccord database) on the french Parole_Publique 1 corpus repository under a CC-BY-SA Creative Commons licence. Emotion corpus Emotion annotation consists in adding emotional information to written messages or speech transcripts. There is no real consensus about how an emotion has to be described in an annotation scheme. Two main approaches can be found in the literature. On the one hand, emotions are coded by affective modalities (Scherer, 2005) , among which sadness, disgust, enjoyment, fear, surprise and anger are the most usual (Ekman, 1999; Cowie and Cornelius, 2003) . On the other hand, an ordinal classification in a multidimensional space is considered. Several dimensions have been proposed among which three are prevailing (Russell, 1980) : valence, intensity and activation. Activation distinguishes passive from active emotional states. Valence describes whether the emotional state conveyed by the text is positive, negative or neutral. Lastly, intensity describes the level of emotion conveyed. Whatever the approach, low to moderate interannotator agreements are observed, what explains that reference annotation must be achieved through a majority vote with a significant number of coders (Schuller and al. 2009 ). Inter-coder agreement is particularly low when emotions are coded into modalities (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008) . This is why this study focuses on an ordinal annotation. Our works on emotion detection (Le Tallec and al., 2011) deal with a specific context: affective robotics. We consider an affective multimodal interaction between hospitalized children and a companion robot. Consequently, this experiment will concern a child-dedicated corpus. Although many works already focused on child language (MacWhinney, 2000), no emotional child corpus is currently available in French, our studied language. We have decided to create a little corpus (230 sentences) of fairy tales, which are regularly used in works related to child affect analysis (Alm and al., 2005; Volkova and al., 2010) . The selected texts come from modern fairy tales (Vassallo, 2004; Vanderheyden, 1995) which present the interest of being quite confidential. This guarantees that the coders discover the text during the annotation. We asked 25 subjects to characterize the emotional value conveyed by every sentence through a 5-items scale of values, ranging from very negative to very positive. As shown on Table 1 , this affective scale encompasses valence and intensity dimensions. It enables to compare without methodological bias an annotation with 3 coding categories (valence: negative, positive, neutral) and the original 5categories (valence+intensity) annotation. A preliminary experiment showed us that children meet difficulties to handle a 5-values emotional scale. This is why the annotation was conducted on the fairy tales corpus with adults (11 men/14 women; average age: 31.6 years). All the coders have a superior level of education (at least, high-school diploma), they did not know each other and worked separately during the annotation task. Only four of them had a prior experience in corpus annotation. The coders were not trained but were given precise annotation guidelines providing some explanations and examples on the emotional values they had to use. They achieved the annotation once, without any restriction on time. They had to rely on their own judgment, without considering any additional information. Sentences were given in a random order to investigate an out-of-context perception of emotion. We conducted a second experiment where the order of the sentences followed the original fairy tale, in order to study the influence of the discourse context. The criterion of data significance -at least five chance agreements per category -proposed by (Krippendorff, 2004) is greatly satisfied for the valence annotation (3 categories). It is approached on the complete annotation where we can assure 4 chance agreements per category. Opinion corpus The second experiment concerns opinion annotation. Emotion detection can be related to a certain extent, with opinion mining (or sentiment analysis), whose aim is to detect the attitude of people in the texts they produce. A basic task in opinion mining consists in classifying the polarity of a given text, which should be either a sentence (Wilson and al., 2005) , a speech turn or a complete document (Turney, 2002) . Polarity plays the same role as valence does for affect analysis: it describes whether the expressed judgment is positive, negative, or neutral. One should also characterize the sentiment strength (Thelwall and al., 2010) . This feature can be related to the notion of intensity used in emotional annotation. Both polarity and sentiment strength are considered in our annotation task. This experiment has been carried out on a corpus of film reviews. The reviews were relatively short texts written by ordinary people on dedicated French websites (www.senscritique.com and www.allocine.fr). They concerned the same French movie. The corpus contains 183 sentences. Its annotation was conducted by the 25 previous subjects. The methodology is identical to the emotion annotation task. The subjects were asked to qualify the opinion that was conveyed by every sentence of the reviews by means of the same scale of values (Table 1 ). This scale encompasses this time the polarity and sentiment strength dimensions. Once again, the sentences were given in a random order and contextual order respectively. The criterion of data significance is satisfied here too. On both annotations, experiments with the random or the contextual order give similar results. Results from the contextual annotation will be given only when necessary. Coreference corpus The last experiment concerns coreference annotation. We have developed an annotated corpus (ANCOR) which clusters various types of spontaneous and conversational speech. With a total of 488,000 lexical units, it is one of the largest coreference corpora dedicated to spoken language (Muzerelle and al. 2014) . Its annotation was split into three successive phases: \u2022 Entity mentions marking, \u2022 Referential relations marking, \u2022 Referential relations characterization The experiment described in this paper concerns the characterization of the referential relations. This nominal annotation consists in classifying relations among five different types: \u2022 Direct coreference (DIR) -Coreferent mentions are NPs with same lexical heads. \u2022 Indirect coreference (IND) -These mentions are NPs with distinct lexical heads. \u2022 Pronominal anaphora (PRO) -The subsequent coreferent mention is a pronoun. \u2022 Bridging anaphora (BRI) -The subsequent mention does not refer to its antecedent but depends on it for its referential interpretation (example: meronymy). \u2022 Bridging pronominal anaphora (BPA) - Bridging anaphora where the subsequent mention is a pronoun. This type emphasizes metonymies (example: Avoid Central Hostel\u2026 they are unpleasant) The subjects (3 men / 6 women) were adult people (average age: 41.2 years) with a high proficiency in linguistics (researchers in NLP or corpus linguistics). They know each other but worked separately during the annotation, without any restriction on time. They are considered as experts since they participated to the definition of the annotation guide. The study was conducted on an extract of 10 dialogues, representing 384 relations. Krippendorff's (2004) criterion of significance is therefore satisfied here too. Reliability measures The experiments have been conducted with four chance-balanced reliability measures 2 : \u2022 Multi-\u03ba : multiple coders/binary distance Cohen's \u03ba (Davies and Fleiss, 1982) , \u2022 Multi-\u03c0 : multiple coders/binary distance Scott's \u03c0 (Fleiss, 1971 ), \u2022 \u03b1 b : Krippendorff's \u03b1 with binary distance, \u2022 \u03b1 : standard Krippendorff's \u03b1 with a 1dimension Euclidian distance. The use of Euclidian distance is unfounded on coreference which handles a nominal annotation. Thus, \u03b1 will not be computed on this last corpus. 2 Experiments were also conducted with Cronbach'\u03b1 c \uf020 (Cronbach, 1951) . This metrics is based on a correlation measure. Krippendorff (2009) considers soundly that correlation coefficients are inappropriate to estimate reliability. Our results show that \u03b1 c is systematically outperformed by the other metrics. In particular, it is highly dependent to coder bias. For instance we observed a relative standard deviation of \u03b1 c measures higher than 22% when measuring the influence of coders set permuation ( \u00a7 5.3, table 5 ). This observation discards Cronbach'\u03b1 c \uf020 as a trustworthy measure. Results Influence of the number of categories Our affective coding scheme enables a direct comparison between a 3-classes (valence or polarity) and a 5-classes annotation. The 3-classes scheme clusters the coding categories with the same valence or polarity. For instance {-2,-1} negative values are clustered in the same category which receive the index 1. For the computation of the weighted \u03b1, the distance between negative (-1) and positive (1) classes will be equal to 2. Several general conclusions can be drawn from these figures. At first, low inter-coder agreements are observed on affective annotation, which is coherent with many other studies (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008) . Non-weighted metrics (multi-\u03ba, multi-\u03c0, \u03b1 b ) range from 0.29 to 0.58, depending on the annotation scheme. This confirms that these annotation tasks are prone to high subjectivity. Higher levels of agreement may have been obtained if the annotators were trained with supervision. As said before, this would have reduced the spontaneity of judgment. Furthermore, a comprehensive meta-analysis (Bayerl and Paul, 2011) has shown that no difference may be found on data reliability between experts and novices. The reliability measures given by the weighted version of Krippendorff's \u03b1 on the two affective tasks are significantly higher: \u03b1 values range from 0.57 to 0.80, which suggests a rather sufficient reliability. These results are not an artifact. They come from better disagreement estimation. For instance, the difference between a positive and a negative annotation is more serious than between the positive and the neutral emotion, what a weighted metrics accounts for. Satisfactory measures are found on the contrary on the coreference task (0.69 with every metric). This result was expected, since a large part of the annotation decisions are based on objective (syntactic or semantic) considerations. Whatever the experiment you consider, multi\u03ba, multi-\u03c0 and \u03b1 b coefficients present very close values (identical until the 3rd decimal). A similar observation was made by (Arstein and Poesio, 2005) with 18 coders. This validates the theoretical hypothesis on the convergence of individualdistribution and single-distribution measures when the number of coders increases. Our experiments show that annotator bias is moderate with 25 coders when inter-coders agreement is rather low (affective tasks), while 9 coders are enough to guarantee a low annotator bias when data reliability is higher (coreference task). Lastly, the comparison between the two annotation schemes (3 or 5 classes) in affective tasks provides some indications on the influence of the number of coding categories on reliability estimation 3 . As expected (see \u00a7 3.3), multi-\u03ba, multi-\u03c0 and \u03b1 b values increase significantly when the number of classes decreases. On the contrary, weighted \u03b1 is significantly less affected by the increase of the number of categories. The \u03b1 value remains unchanged on the emotional corpus and its variation restricts to 0.05 on the opinion task. It seems that the use of a Euclidian distance counterbalances the higher risk of disagreement when the number of categories grows. Such an independence of the number of coding categories is an interesting property for a reliability measure, which has never been reported as far as we know. Finally, Table 3 presents as an illustration the reliabilities measures we obtained with the contextual annotation of the opinion corpus. These results are fully coherent with the previous ones. One should note in addition that reliability measures are significantly higher on these contextual annotations: the context of discourse helps the coders to qualify opinions more objectively. Metric M-\u03ba \u03ba \u03ba \u03ba M-\u03c0 \u03c0 \u03c0 \u03c0 \u03b1 \u03b1 \u03b1 \u03b1 b \u03b1 \u03b1 \u03b1 \u03b1 3- Influence of prevalence Table 4 presents the distribution of the annotations on the three corpora. (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008) reported that more than 80% of the speech turns are classified as neutral in their emotional corpora. This prevalence was not found on our affective corpora. Positive annotations are nearly as frequent as the neutral ones on the emotion task. This observation is due to the deliberate emotional nature of fairy tales. Likewise, the neutral opinion is minority among the film reviews, which aim frequently at expressing pronounced judgments. Positive opinions are slightly majority on the opinion corpus but this prevalence is limited: it represents an increase of only 50% of frequency, by comparison with a uniform distribution. Corpus Emotion (fairy tales) In the coreference corpus, two classes are highly dominant, but they are not prevalent alone. There is no indication in the literature that the prevalence of two balanced categories has a bias on data reliability measure. For all these reasons, we didn't investigate the influence of prevalence. Besides, relevant works are questioning the importance of the influence of prevalence on inter-coders agreement measures (Vach, 2005) . 5-classes \u22122 \u22122 \u22122 \u22122 \u22121 \u22121 \u22121 \u22121 0 0 0 0 1 Influence of coders set permutation \"a coefficient for assessing the reliability of data must treat coders as interchangeable (Krippendorff, 2004b) . We have studied the stability of reliability measures computed on any combination of 10 coders (among 25) on the affective corpora, and 4 coders (among 9) on the corefer-ence corpus. The influence of permutation is quantified by a measure of relative standard deviation (e.g. related to the average value) among the sets of coders (Table 5 ). Binary metrics do not differ on this criterion: multi-\u03ba, multi-\u03c0 and \u03b1 b present very similar results. On the opposite, the benefit of a Euclidian distance of agreement is clear: \u03b1 is significantly less influenced by coders set permutation. Influence of the number of coders A good way to limit annotator bias is to enroll an important number of annotators. This need is unfortunately contradictory with a restriction of annotation costs. The estimation of data reliability must thereby remain trustworthy with a minimal number of coders. As far as we know, there is no clear indication in the literature about the definition of such a minimal size. We have conducted an experiment which investigates the influence of the number of coders on the relevancy of reliability estimation. Considering N annotations (N=25 for affective annotation and N=9 for coreference annotation), we compute all the possible reliability values with any subsets of S coders, S varying from 2 to N. As an estimation of the trustworthiness of the coefficients, the relative standard deviation of the reliability values is computed for every size S (Figures 1 to 3 ). The influence of the number of coders is obvious: detrimental standard deviations are found with small coders set sizes. This finding concerns above all multi-\u03ba, multi-\u03c0 and \u03b1 b , which present very close behaviors on all annotations. One the opposite, the weighted Conclusion and perspectives Our experiments were conducted on various annotation tasks which assure a certain representativeness of our conclusions: \u2022 Cohen's \u03ba, Krippendorff's \u03b1 \uf020 and Scott's \u03c0\uf020 provide close values when they use the same measure of disagreement. \u2022 A convergence of these measures has been noticed in the literature when the number of coders is high. We observed it even on very restricted sets of annotators. \u2022 The use of a weighted measure (Euclidian distance) has several benefits on ordinal data. It restricts the influence on reliability measure of both the number of categories and the number of coders. Unfortunately, Cohen's \u03ba \uf020 \uf020 statistics cannot consider a weighted distance in a multi-coders framework contrary to Krippendorff's \u03b1. \u2022 There is no benefit of using Krippendorff's \u03b1 on nominal data, since a binary distance is mandatory on this situation. To conclude, the main interest of Krippendorff's \u03b1 is thus its ability to integrate any kind of distance. In light of our results, the weighted version of this coefficient must be preferred every time an ordinal annotation with multiple coders is considered. Our experiments leave open an essential question: the objective definition of trustworthy thresholds of reliability. We propose to investigate this question in terms of expected modifications of the reference annotation. A majority vote is generally used as a gold standard to create this reference with multiple coders. As a preliminary experiment, we have compared our reference affective annotations (25 coders) with those obtained on any other included set of coders.  Figure 4 presents the average percentage of modifications of the reference according to the number of coders. We wonder to what extent these curves can be related to reliability measures. It seems indeed that the higher the measures are, the lower the modifications are too. For instance, almost all of the coefficients present higher or equal reliability values with 3 coding categories (Tables 2 & 3 ), which corresponds to lower levels of modifications on Figure 3 . Likewise, reliability measures are higher on the opinion annotation, where we observe lower modifications of the reference. As a result, we expect results like those presented on figure 4 to enable a direct interpretation of reliability measures. For instance, with a multi-\u03ba values of 0.41, or a \u03b1 b value of 0.57 (Table 2, 3-classes emotion annotation), one should expect around 8% of errors on our reference annotation if 10 coders are considered. We plan to extend these experiments with simultated synthetic data to characterize precisely the relations between absolute reliability measures and expected confidence in the reference annotation. We expect to obtain with simulated annotation a sufficient variety of agreement to establish sound recommendations on data reliability thresholds. We intend to modify randomly human annotations to conduct this simulation."
}