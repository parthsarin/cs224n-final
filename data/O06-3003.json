{
    "article": "In this paper, we present an integrated method to machine translation from Cantonese to English text. Our method combines example-based and rule-based methods that rely solely on example translations kept in a small Example Base (EB). One of the bottlenecks in example-based Machine Translation (MT) is a lack of knowledge or redundant knowledge in its bilingual knowledge base. In our method, a flexible comparison algorithm, based mainly on the content words in the source sentence, is applied to overcome this problem. It selects sample sentences from a small Example Base. The Example Base only keeps Cantonese sentences with different phrase structures. For the same phrase structure sentences, the EB only keeps the most simple sentence. Target English sentences are constructed with rules and bilingual dictionaries. In addition, we provide a segmentation algorithm for MT. A feature of segmentation algorithm is that it not only considers the source language itself but also its corresponding target language. Experimental results show that this segmentation algorithm can effectively decrease the complexity of the translation process. Introduction Although Machine Translation has been an important research topic for many years, the development of a useful Machine Translation system has been very slow. Researchers have found that developing a practical MT system is a very challenging task. Nevertheless, in our age of increasing internationalization, machine translation has a clear and intermediate attraction. There are many methods for designing machine translation systems [Carl 1999; Carpuat 2005; Kit 2002b; Mclean 1992; Mosleh and Tang 1999; Somers 2000; Knight and Marcu 2005; Tsujii 1986; Brown 1997; Zhou et al. 1998; Zens 2004] , such as the rule-based method, knowledge-based method, and example-based method. In recent years, with the development of bilingual corpora, the example-based method has become a better choice than the rule-based method, although statistical MT systems are now able to translate across a wide variety of language pairs [Knight and Marcu 2005] . This is because the rule-based MT system has some disadvantages, such as a lack of robustness and poor rule coverage [Zhou and Liu 1997 ]. On the other hand, the large-scale, high-quality bilingual corpora are seldom readily available, so the example-based method has encountered a lot of problems in machine translation, such as a lack of sufficient example sentences and redundant example sentences. The good performance of an EBMT system depends on there being a sentence in the example base which is similar to the one that is to be translated. In contrast, an SMT system may be able to produce perfect translations even when the sentence given as input does not resemble any sentence in the training corpus. However, such a system may be unable to generate translations that use idioms and phrases that reflect long-distance dependencies and contexts, which are usually not captured by current translation models [Marcu 2001 ]. On the other hand, the example-based method can effectively solve the problem of insufficient knowledge that the rule-based method often encounters during the translation process [Chen and Chen 1995] . In view of this fact, a machine translation prototype system, called LangCompMT05, has been implemented. It integrates rule features, text understanding, and a corpus of example sentences. In this paper, a brief review of the MT method is given first. This is followed by an introduction to the framework for LangCompMT05. In section 3, a detailed description of this system, whose implementation involves combining example-based and rule-based methods, is presented. Experimental results are discussed in section 4. The last section gives conclusions and discusses future work. Design Constructs Figure 1 shows the architecture of the LangCompMT05 system. The implementation mechanism of the LangCompMT05 system is as follows: 1) The source Cantonese sentence is segmented with a new segmentation algorithm, whose implementation is based on the word frequency, and the criterion for segmentation considers not only the source sentence itself but also its corresponding translation. The source sentence \"\u5979\u6709\u4e9b\u795e\u7d93\u904e\u654f\" (She is a little bit hypersensitive), for example, can be segmented as \"\u5979/\u6709\u4e9b/\u795e\u7d93/\u904e\u654f\" in general. Because \"\u795e\u7d93\u904e\u654f\" can be translated into the English word \"hypersensitive\", for MT, the sentence is segmented as \"\u5979/\u6709\u4e9b/\u795e\u7d93\u904e\u654f\". 2) The rule-based method is applied to analyze the source sentence, and its phrase structure is generated. The Rule Base (RB) of this system is established through analysis of the real corpus. The phrases are classified as noun phrases (NPs) or verb phrases (VPs). Some of the rules for phrases are as follows: NP= : [a] [n] | [m] (q) (n), VP= : [d] (v) . Here, \"a\", \"n\", \"m\", \"q\", \"d\", and \"v\" denote adjective, noun, numeral, quantifier, adverb, and verb, respectively. 3) A new knowledge representation, called SST, is applied to store the sentence structure. The target sentence can be generated with this tree. 4) The example-based method and rule-based method are combined and used to select, convert, and generate the target sentence. 5) The principle for classifying a Cantonese content word, such as \"\u55ae\uf902 (bike)\" or \"\u8fd4\u5de5 (go to work) \", is dependent not only on the syntactic features of the word but also its semantic features; for a function word, such as \"\u7684\", \"\u88ab\", or \"\u56e0\u6b64 (so)\", the principle for classification is only based on its syntactic features. 6) The understanding model of the system includes two parts: a word model and a phrase model. Both of them consist of six parts: a Cantonese word, a category, a frequency, and three corresponding English words: word1, word2, and word3. The phrase model has the same structure as the word model. Table 1 show examples of these two models, where \"d\", \"c\", and \"v\" represent adverb, conjunction and verb, respectively. 8) The system is portable and extendable. Its dictionaries, rule bases, and algorithms are in separate modules (see Figure 1 ) that can be maintained independently. 9) The system can translate written Cantonese into English. Implementation The implementation of the LangCompMT05 system is composed of the following parts: an example base, dictionaries, rule bases, the main program and five additional function modules (see Figure 1 ). It integrates rule features, text understanding, and a corpus of example sentences. For the preprocessing stages, it uses a rule-based method to deal with the source sentence. Then, the EBMT method is used to select the translation template. In the target sentence construction stage, which involves the translation of sentence components, the system is mostly based on a rule-based method. Segmentation Algorithm Word segmentation is the basic tack in many word-based applications, such as machine translation, speech processing, and information retrieval. Chinese word segmentation, being an interesting and challenging problem, has drawn much attention from many researchers [Hu 2004; Kit 2002a; Dunning 1993; Hou 1995; Liu 1994; Nie 1995 ]. We will present the segmentation algorithm in detail in another paper. POS Tagging Parts of speech can help us analyze the syntax structure of a sentence, and they are fundamental to the understanding and transformation of MT. A knowledge base and rules are used to tag each Cantonese sentence. The knowledge base consists of records that contain words and their parts-of-speech. After segmentation, all of the words in the source sentence are tagged. For ambiguous words that have more than one part-of-speech, the rules in RB 0 are used to perform disambiguation. Suppose { } , , , , , , , , , , , , , , , T n np m q r v a p w d u f c t b g = is the tag set of the system, and A is the set of all Cantonese words. The formal presentation of the disambiguation rules is as follows: { } , , , , . A T T T \u03b1 \u03b2 \u03b1 \u03b2 \u03b1 \u03b2 * \u2135 \u2192 \u2208 \u222a \u2135 \u2286 \u2208 (1) Here, \u03c7 is the subset of POS set T, is the element of T, and \u03b1 and \u03b2 are null, a Cantonese word or an element of T. \u2192 denotes that if an ambiguous word that has the POS \u03c7 is preceded by POS \u03b1 and succeeded by POS \u03b2 , then it can be tagged as . For example, the POS rule ( { , } m u n mn \u2192 ) means that if a word has the property of an auxiliary word (u) or a noun (n) and is preceded by a quantifier, then it is a noun. The following is an example of this process: \uf978/m \u5730/(u,n)\u76f8\u8ddd/n \u4e09/m \u54e9(u,q) \u23af \u23af \u23af \u23af \u23af \u23af \u23af \u2192 \u23af \u2192 \u2192 q m q u m n m n u m } , { , } , { \uf978/m \u5730/n \u76f8\u8ddd/n \u4e09/m \u54e9/q (The distance between the two locations is 3 miles) \u4ed6/r \u9a0e/v \u55ae\uf902/n \u8ffd/v \u4e0a\uf92d/(u,v) \u23af \u23af \u23af \u2192 \u23af \u2192 u v v u v } , { \u4ed6/r \u9a0e/v \u55ae\uf902/n \u8ffd/v \u4e0a\uf92d/u (He catches up by bike) \u5979/r \u7d42\u65bc/d \u4e0a\u4f86/(u,v)\u4e86/u \u23af \u23af \u23af \u23af \u2192 \u23af \u2192dv u v u d } , { \u5979/\u7d42\u65bc/d \u4e0a\u4f86/v \u4e86/u (Finally, she comes up) Parsing The function of parsing is to identify the phrase structure of a sentence. At this stage, both the input and output sentences are parsed. This procedure works with some paring rules that have been generated from the corpus. These rules in RB 1 include the following: The sentence is scanned backwards from the end; i.e. the last two words of the sentence are checked first, then the next two prior words, and so on till the first word of the sentence is scanned. After parsing, the system only needs to match out the POS. This procedure can reduce the searching time needed to identify the most similar example sentence in the EB. For example, a tagged Cantonese sentence \u4ed6/r \u662f/v \u4e00\u500b/q \u5b78\u751f/n (He is a student) is parsed as S=[\u4ed6/r]NP[\u662f/v[\u4e00\u500b/q \u5b78\u751f/n]NP]VP. Its parsing tree is shown in Figure 2. After parsing, the sentence is converted into SST as shown in Figure 3 . Definition 3. SST is a Binary Tree; it is used to store the natural language sentence. Let s=w 1 w 2 ... w n be a sentence: 1) w i is a root if and only if w i is the center word of the predicate in the sentence. \u662f/v/VP \u4ed6/r/NP \u5b78\u751f/n/NP \u4e00\u500b/q Figure 3. An example of SST S=NP VP r v NP q n \u4ed6 \u662f \u4e00\u500b \u5b78\u751f Figure 2. The parsing tree of a sentence 2) w 1 ...w i-1 forms the left sub-tree of the root, while w i+1 ...w n forms the right sub-tree of the root. 3) The left sub-tree and the right sub-tree are formed as follows: a) If w 1 ...w i-1 or w i+1 ...w n is a sub-sentence, then go to 1). b) If w 1 ...w i-1 or w i+1 ...w n is a phrase, then the root of the sub-tree is the center word (or content word), while the following word is the modifier of the center word. This type of knowledge representation can easily reflect the structure of a sentence, and can be implemented for the translation process. Similarity Comparison and Example Selection In general, an example-based MT system should address the following problems: 1) building the map relation of bilingual alignment, based on characters, words, phrases, sub-sentences or sentences; 2) similarity calculation and example selection; 3) constructing a target. Among these problems, problem 2 is the most important one in example-based MT. Many researchers have focused on the above problems [Li 2005; Chen 2002; Church 1994; Fung 1993; Carl 1999; FuRusE 1992; Mosleh 1999; Carl 1999] and tried to solve it in different ways. For problem 2, our research addresses three important questions as follows: 1) Determining the matching level: The matching level includes the sentence level and sub-sentence level. For the former, it is easy to determine the boundary of a sentence. Because the sentence can contain a certain number of messages, the possibility of having an exact match is very low, so the system lacks flexibility and robustness. In contrast, matching at the sub-sentence level has the advantage of exact matching and the disadvantage of boundary ambiguity. In addition, there are no exact chunking or cover algorithms. Our matching algorithm is sentence-based. 2) The algorithm for calculating the similarity: There is no exact definition for the similarity between sentences. Many researchers have addressed this issue and presented similarity algorithms based on words. Some of the algorithms [e.g., Sergei 1993 ] firstly calculate the word similarity according to the word font, word meaning, and semantic distance of words, and then calculate the sentence similarity based on word similarity. Other algorithms [Brown 1997; Carl 1999; Markman et al. 1996; Mclean 1992; Mosleh et al. 1999; Zhang et al. 1995] are based on syntax rules, characters and hybrid methods. Our similarity algorithm is based on the phrases in the sentence; it has the following features: a) The example base consists of a variety of sentences whose phrase structures are different. b) The phrases of a sentence are the fundamental calculating cells for aligning the content words of the input sentence and example sentence, i.e., calculate the similarity between the same positional phrase in the input and example sentence. For the same positional phrases, the similarity calculation is based on the content words. This is based on the principle that in a natural language sentence, the content words form the framework of the sentence and depict the central meaning of the sentence. c) The system does not need lexical, syntax, and semantic analysis to perform similarity comparison. d) The system can deal with a variety of Cantonese inputs, such as sentences, sub-sentences, and phrases. 3) The efficiency of this algorithm: Normally, there will be a lot of example sentences in the example base. The algorithm proposed here has to calculate the similarity between the input sentence and every sentence in the example base. So the efficiency of the algorithm is very important. The example base contains the different structures of Cantonese sentences. For sentence with the same structure, we select the shortest one as an example sentence. So the example base will keep the smallest number of sentences yet maintain the largest number of sentence structure types. In addition, the similarity algorithm is not recursive, and it saves computing time. The Example Base Each translation example in the example base consists of four components: a Cantonese sentence, a tagged Cantonese sentence, an English sentence, and a tagged English sentence. A Cantonese-English translation example is given as follows: \u4ed6\u9a0e\u55ae\uf902\u8fd4\u5de5\u3002; \u4ed6/r \u9a0e/v \u55ae\uf902/n \u8fd4\u5de5/v\u3002/w; he goes to work by bike. he/He goes to work/V by/P bike/N ./W; In the example base, the four components of an example sentence have no relationship with each other and don't need to align Cantonese to English sentences. All the Cantonese sentences in the example base are segmented and tagged. Cantonese segmentation is based on English translation, i.e. if the English translation is a phrase; then the corresponding Cantonese part is segmented as a word, such as \"\u8fd4\u5de5\". This part of the English sentence serves as a translation template, the tagged Cantonese sentence and tagged English sentence are to construct a target (see section 3-5). Similarity Comparison Similarity comparison is used to choose the most similar Cantonese example sentence in the example base with the input sentence, and then its corresponding English translation sentence will serve as the translation template to translate the input Cantonese sentence. The similarity of two sentences is calculated on the basis of a phrase in the parsed input sentence and the parsed example sentence. The parts-of-speech within the same phrase, in the phrase structure pattern of the input sentence, and in each example sentence in the bilingual corpus are compared. In case of a mismatch between the parts-of-speech, a penalty score is incurred, and the comparison proceeds for the next part-of-speech within the same phrase. The score calculation progresses from the left-most phrase structure to the last one of the sentence. In fact, the similarity comparison mechanism is mainly based on the content words in the sentence. The example base can only store Cantonese framework sentences. For sentences that have the same phrase structure, the shortest is stored in the example base so as to avoid information redundancy in the example base. The mathematical model of this procedure is as follows [Wu and Liu 1999; Zhou and Liu 1997 ]: Suppose A=w 1 w 2 ...w n =p A1 p A2 ......p Ak , B=w 1 w 2 ...w m =p B1 p B2 ......p Bl , where w Ai (w Bj ), p Ai (p Bj ) is the i th (j th ) Cantonese word and phrase, respectively, in sentence A (B). F is the whole feature set of a certain word category, E is a subset of F, and |E| stands for the number of features in E. fea k (w), sub_pos(w), and pos(w) represent the k th feature, sub-category, and part-of-speech of word w, respectively. Ss(S 1 ,S 2 ) represents the metric between S 1 and S 2 ; ( ) max( , ) 1 2 1 ( , ) , k l Ai Bi i Ss S S Sp p p = = \u2211 , (2) ( ), ( ) 0 ( , ) ( ), ( ) 0 ( \u23a7 \u2212 = \u23aa \u23aa = \u2212 = \u23a8 \u23aa + \u23aa \u23a9 , (3) ( )  We set the weights in equations 4 and 5 based on the results of many experiments. We think that the function word and content word have the equal function in the comparison of sentences, so they have the same similarity score, i.e. 1.5. In equation 4 (for function words), if the parts-of-speech of the function words in A i and B i are equal, we think we can simply exchange the function word in the example sentence with the source function word, which will not affect the translation sequence. In this case, we give the higher similarity score of 1.1. If there is a function word in A i , and no function word in the corresponding location in B i , we think the structures of both A i and B i are not equal, so we assign a negative similarity. Otherwise, the function words of A i and B i are totally different, so the lower negative weight is given. Equation 5 is used to calculate the content word similarity. All content words have their own semantic features, which can be used to calculate their similarity. If the parts-of-speech of the content word in Ai and B i are equal, and if most of their features are equal, then we give the higher similarity weight, 1.2; otherwise, their identical features are less than half of the whole feature set F, and we think they belong to different categories, so we assign a weight of 1.1. If their features are totally unequal and their POSs are equal, we think the difference between A i and B i is semantic, so the weight is 1.0. If the parts-of-speech of the content words of A i and B i are not equal and belong to (n,r), we think this difference doesn't affect the translation sequence, so the weight is 0.8. When the content words of A i and B i are equal and the function words before them are not equal, we think this may affect the translation result, so a 0.6 weight is given. If the POSs of the content words in A i and B i are equal and the function words before them are not equal, we think their similarity is low, so the weight is 0.4. Otherwise, they are totally different. Because the content word plays the main function in determining meaning of the sentence, we give a weight of -1.5. This procedure calculates the similarity between the input sentence and every sentence in the example base, and selects the example sentence whose score is the highest as the best matching sentence. If an input sentence matches both a fragment and a full sentence that contains (or does not completely contain) the fragment, or that matches two examples that are syntactically identical but lexically different, then the highest score of the example sentence will be selected. 1.5, 1.1, ( ) ( ) 0.3, ( ) 0 ( ) 0 ( , ) ( ) 0 f f Bi Ai f f Bi Ai f f f f Bi Ai Bi Ai f Ai if p p if POS p POS p if len p AND len p Sw p p OR len p AND le = = \u2212 = < > = <> ( ) ( ) 0 0.6, f Bi n p otherwise \u23a7 \u23aa \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa = \u23aa \u23aa \u2212 \u23a9 , ( 4 ) 1 1 1 0.5* 1 1 0.5* 1.5, 1.2, ( ) ( ) ( ) ( ) 1.1, ( ) ( ) ( ) ( ) 1 ( , ) k ki c c Ai Bi c c Ai Bi c c k Ai k Bi fea E F E F c c Ai Bi c c k Ai k Bi fea E F E c c \u23a7 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa = \u23aa \u23aa \u23aa \u23aa = \u23aa \u2212 \u23aa \u23aa \u23a9 . ( 5 The example base was created by Yu Shiwen of Beijing University and more Cantonese sentence pairs have been has added. Now, there are about 9000 Cantonese and English sentence pairs, and all the sentences have been annotated with parts-of-speech. The average sentence length for Cantonese is 11 characters and for English is 14 words. Moreover, many sub-dictionaries of nouns, verbs, adjectives, pronouns, classifiers, and prepositions, etc. are employed. There are many specific features that are helpful for sentence comparison in each of these dictionaries. For the parsed Cantonese sentence \"S=[\u4ed6/r]NP[\u662f/v[\u4e00\u500b/q \u5b78\u751f/n]NP]VP(He is a student)\", the example sentence could be \"S=[\u5979/r]NP[\u662f/v[\u4e00\u500b/q \u5de5\u4eba/n]NP]VP (She is a worker)\". Target Construction This stage involves using the Cantonese and English phrase structure relations of the example translation as a template to build the target English sentence. The SST of the source Cantonese sentence contains the following types of nodes: 1) Bilingual corresponding Node (BN): it provides a correspondence between the example English sentence tree and translation template tree (see Figure 4 ). Figure 4. An example of a BN in the SST. The nodes \"\u662f(be)\" , \"\u5b78\u751f(student)\" , and \"\u4e00(a)\u500b\" belong to BN . 2) Single corresponding Node (SN): this type of node only has a corresponding node in the example English sentence tree and has no corresponding node in the translation template tree. An example is the node\"\u6211(I) \" in the above source sentence. 3) Non-corresponding Node (NN): this type of node provides no correspondence between the example English sentence tree and translation template tree (see Figure 5 ). There are two types of NNSs: a) NN c : the word depicted by this node is a content word. See the node \"\uf981\u5152(daughter)\" in the following example. b) NN f : the word depicted by this node is a function word. See the node \"\u548c(and)\" in the following example. 4) Tense Node (TN): this type of node can determine the tense of a target English sentence. Table 2 shows Cantonese words that can represent the tense of the corresponding English sentence. Source sentence Table 2 . The correspondence between English sentence tense and Cantonese words. \u662f/v \u6211/r \u5b78\u751f/n \u4e00\u500b/q Example sentence \u662f/v \u4ed6/r \u5b78\u751f/n \u4e00\u500b/q translation template is/V he/R English sentence tense Corresponding Cantonese words The present continuous \u6b63(just), \u6b63\u5728(in progress of), \u5373\u6642(at present), \u5373\u523b(immediately), \u5728\u9032\ufa08(in progress)... The present perfect \u5df2(already), \u5df2\u7d93(already), \u7d93\u5df2(already), \u66fe\u7d93(ever) ... The past indefinite \u904e(over), \uf9ba(end), \u904e\u53bb(past), \u4ee5\u5f80(previously), \u4ee5\u524d(ago), \u4ece\u524d (aforetime), \u4e0a\u6b21(last time), \u6628\u65e5(yesterday) ... The future indefinite \u6703(be able to), \u5c07(shall), \u5c31\u8981(going to), \u7d42\u5c06(eventually), \u5c07\u6703(will be able to), \u5373\u5c07(be about to), \u5c31\u6703(will be able to), \u5c31\u5feb(soon), \u5c31\uf92d(come soon), \u5feb\u8981(soon), \u660e\u65e5(tomorrow), \u660e\uf98e(next year)... 5) Type, Voice, and Mood Node (TVMN): this type of node can determine the voice and mood of a target English sentence. Table 3 shows Cantonese words that can represent the tense of the corresponding English sentence. For the above different types of nodes in the SST, the system applies different replacement rules to translate the phrases stored in these nodes. For the node BN, m=0; i.e., the system does not need any replacement action because the source word has the corresponding target word in the translation template. For the node SN, replacement-action ::= look(ew), look(sw), repl (E-ew, E-sw) . Here, look is the action of looking up the bilingual dictionary; repl is the action of replacing the translation template; ew and sw are the Cantonese words in the example sentence and source sentence, respectively; E-ew and E-sw are the English words corresponding to ew and sw, respectively. For the node NN, replacement-action ::= look(sw),loca(sw), inst(E-sw). Here, loca is the action of determining where to insert E-sw in the translation template; inst is the action if inserting E-sw in the translation template. For the node TN, replacement-action ::= look(sw v ), chan(E-sw v ). Here, sw v is the current verb in the source sentence, and chan is the action of changing E-sw v , for example, E-sw+...\"ing\uff02for the present continuous tense, E-sw+\"ed \uff02for the past tense, E-sw +\"will\uff02+ sw for the future tense, and so on. For the node TVMN, replacement-action ::= recv(E-sw v ), chan(tran-tmplate). Here, recv is the action of recovering the verb of the template, chan(tran-tmplate) is the action of changing the voice of the translation template, such as \"do\" + subj+verb , \"will\"+ subj+verb , \" have\" + subj+verb for query sentence, or \" do not\" +verb, \"did not\"+ verb for a negative sentence. The process of target construction can be described as follows (see Figure 6 for an example): 1) Recovering the words in the translation template: Because the criterion of similarity matching is based on content words, and because in a Cantonese sentence, the function words determine the word form change of its corresponding English sentence, when the system gets an example sentence from the example base, the chance of having an example sentence with a different tense and voice from that of the source sentence is quite high. So the system first deletes the tense and voice of the translation template, and then adds the tense and voice corresponding to the source sentence. For example, Translation template: he worked in the factory. he work in the factory. 2) The replacement rules are applied to change the translation template and generate the target sentence. 3) Experimental results The LangCompMT05 system was realized using MS Visual C++ for Windows. Users can easily interact with the system to perform translation. Table 4 lists some experiential results. They indicate that the accuracy of the system is 80.6% (see Table 5 ). The test sentences were created by the authors. Four translation experts manually scored the system's translation results. The score range was from 0 to 100, and we got the accuracy of the system by averaging the scores. The average translation time per sentence was 36 seconds. Most of the translation errors are due to the following cases: 1) The preposition and noun in the sentences are replaced with error words. The corrected translation for \"\u5728\u684c\u4e0a\" is \"on the desk\", not \"in the desk\". 2) Some Cantonese phrasal words has no corresponding English words. \"\u6025\u6025\u8173\"\ufe50for example, is a special Cantonese phrasal word. An insufficient knowledge base is the cause of most of the problems in natural language processing. 3) Segmentation errors also cause the translation errors. For example, \"\u662f/\u975e\u5e38/\u5e38/\u6df7\u6dc6(Is extremely confused)\", \"\u5979/\u662f/\u975e\u5e38/\u6f02\uf977/\u7684 (She is very pretty)\". 4) POS errors also cause the translation errors. POS tagging is mainly statistic-based, and it selects categories that often occur in the corpus. For example, \"\u66f8/n \u5728/p \u684c/n \u4e0a/u (The book is in the desk)\", \"\u4ed6/r \u4e0a/u \u5c71/n (He is climbing up the mountain)\". This type of error can be solved by means of syntactic analysis. The price changes according to the market reaction. Conclusion and Future Work We have proposed an integrated method for Cantonese-English machine translation that makes use of morphological knowledge, syntax analysis, translation examples, and target-generation-based rules. The principles and algorithms used in this MT system have been well tested. The source sentence is segmented first, then it is tagged and parsed it, and the SST of the source sentence formed for its structural representation. Finally, using the computational linguistic method, an example sentence is selected from the EB; its corresponding English translation sentence is used as the translation template, and the target sentence (English) is generated based on rules. Machine translation especially in the Cantonese-English domain is quite a difficulty task. Based on our research on the LangCompMT05 system, we have proposed an integrated MT method that is mainly based on an example-based machine translation method, and we believe that this integrated method is feasible for solving many translation problems. With the computational method, we find that it is possible to acquire bilingual knowledge from a small-scale, representable EB. We have proposed a number of algorithms, such as a Cantonese segmentation algorithm, similarity calculation algorithm, and a target sentence construction algorithm. We have created databases, which contain many Cantonese words and related information. For example, our Cantonese dictionary contains part-of-speech and word frequency information. The EB stores many Cantonese-English sentence pairs that have been segmented and tagged with POSs. The bilingual dictionary stores the Cantonese words and corresponding English words. This information source will be valuable for future development of other NLP systems. ",
    "abstract": "In this paper, we present an integrated method to machine translation from Cantonese to English text. Our method combines example-based and rule-based methods that rely solely on example translations kept in a small Example Base (EB). One of the bottlenecks in example-based Machine Translation (MT) is a lack of knowledge or redundant knowledge in its bilingual knowledge base. In our method, a flexible comparison algorithm, based mainly on the content words in the source sentence, is applied to overcome this problem. It selects sample sentences from a small Example Base. The Example Base only keeps Cantonese sentences with different phrase structures. For the same phrase structure sentences, the EB only keeps the most simple sentence. Target English sentences are constructed with rules and bilingual dictionaries. In addition, we provide a segmentation algorithm for MT. A feature of segmentation algorithm is that it not only considers the source language itself but also its corresponding target language. Experimental results show that this segmentation algorithm can effectively decrease the complexity of the translation process.",
    "countries": [
        "China",
        "Hong Kong"
    ],
    "languages": [
        "English",
        "Cantonese"
    ],
    "numcitedby": "3",
    "year": "2006",
    "month": "June",
    "title": "A Structural-Based Approach to {C}antonese-{E}nglish Machine Translation"
}