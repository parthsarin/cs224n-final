{
    "article": "In this paper, we present the results of the parallel Czech coreference and bridging annotation in the Prague Dependency Treebank 2.0. The annotation is carried out on dependency trees (on the tectogrammatical layer). We describe the inter-annotator agreement measurement, classify and analyse the most common types of annotators' disagreement. On two selected long texts, we asked the annotators to mark the degree of certainty they have in some most problematic points; we compare the results to the inter-annotator agreement measurement. Introduction The coreference and bridging annotation in the Prague Dependency Treebank (PDT) is one of the largest existing manually annotated corpora for pronominal, zero and nominal coreference and bridging relations. Contrary to the majority of similarly aimed corpus projects (Poesio 2004 , Poesio -Artstein 2008 , Poesio et al. 2004 , Recasens 2009 , Krasavina -Chiarchos 2007, etc.) , coreference and bridging relations have been annotated directly on the syntactic trees and technically they are a part of the tectogrammatical (complex semantic) layer of PDT. This approach allows us to include relevant syntactic phenomena annotated earlier (such as e.g. appositions, coreference relations between subject and predicate nominals, etc.) into the coreference representation, and to take advantage of the syntactic structure itself (resolution of elliptical structures, coordinations, parentheses, foreign expressions and identification structures, direct speech, etc.) 1 . Also, from the perspective of querying and visualizing the treebank, all the different types of linguistic information are interlinked, available and visible at once. One of the important advantages is that PDT includes information on topic-focus articulation (Haji\u010d et al. 2006 ) and discourse annotation (Mladov\u00e1 2011) . Comparing the results of inter-annotator agreement in manual annotations of language phenomena at different language levels makes evident that the degree of agreement goes down when proceeding from phonological to \"higher\" language levels. On the one hand, relations that cross the sentence boundary are not so systematically described both in classical linguistics and in annotation guidelines, causing disagreements due to different understanding of terms. On the other hand, such relations are much more vague and in many cases ambiguous. Both these problems influence the measurement of the inter-annotator agreement. In this paper, we present results of the inter-annotator agreement measurement for nominal coreference and bridging relations for Czech and compare them to the degree of certainty the annotators had while marking these relations. The Annotation Scheme Within the bounds of coreference-like phenomena, three types of relations are marked in PDT: a) grammatical coreference (coreference of relative and reflexive pronouns, verbs of 1 The benefits of the tectogrammatical structure for coreference annotation are described in detail in Nedoluzhko -M\u00edrovsk\u00fd (2013) . control arguments, arguments in constructions with reciprocity and verbal complements), b) pronominal and nominal textual coreference (including zero anaphora), which is further specified into coreference of specific (type SPEC) and generic (type GEN) noun phrases, and c) bridging relations, which mark some semantic relations between non-coreferential entities. The following types of bridging relations are distinguished: PART-OF (e.g. roomceiling), SUBSET (students -some students) and FUNCT (state -president) traditional relations (see e.g. Clark 1977) , CONTRAST for coherence relevant discourse opposites (this year -last year), ANAF for explicitly anaphoric relations without coreference, e.g. for metalinguistic references (rainbow -that word) and the further underspecified group REST 2 . Grammatical coreference typically occurs within a single sentence, the antecedent being able to be derived on the basis of grammar rules of a given language. For this reason, grammatical coreference is the least ambiguous among the coreference types, its annotation is the most reliable, being close to other grammatical phenomena annotated in PDT. Solving Coreference Ambiguity in Similar Projects Problems of low inter-annotator agreement and ambiguity in annotation of coreference and bridging relations have been topics of active discussions during the last few years. Shortcomings of straightforward definitions of coreference were pointed out in Poesio and Artstein (2005) . They were later analyzed in detail using linguistic and computational methods in Versley (2008 ), and Recasens et al. (2010 , 2011) . The group of so called \"nearidentity\" relations, where the discourse entities to which the noun phrases refer cannot be called coreferential in all senses but still are rather coreferential than not, was separated from the cases of full-coreference. Coreference was thus redefined as a scalar relation between linguistic expressions that 2 For a detailed classification of identity coreference and bridging anaphora used in PDT, see e.g. Nedoluzhko -M\u00edrovsk\u00fd (2011) . refer to discourse entities considered to be at the same granularity level relevant to the linguistic and pragmatic context (Recasens et al. 2011) . The \"near-identity\" relation holds e.g. between several hundred disabled people and the congregated in Versley's (2008) example (1). The groups of people addressed by these noun phrases are not the same but the difference is neutralized by the context: (1) For a \"barrier-free Bremen,\" several hundred disabled people went onto the streets yesterday-and demonstrated for \"Equality, not Barriers.\" . . . \"Why always us\" the congregated asked on the posters. However, the attempt to annotate \"nearidentity\" explicitly has proved to be unreliable, because it is difficult for annotators to recognize such relations (Recasens et al. 2012) . Also ambiguity seems to be much better identified not by asking annotators to code ambiguous expressions but by comparing the annotations produced by different annotators (Poesio and Artstein 2005) . Explicitly marked ambiguity is annotated in the PoCoS corpus for German (Krasavina -Chiarchos 2007) but was not analysed in detail yet. In order to evaluate the inter-annotator agreement on selected texts annotated by two or more annotators, we used F1-measure for the agreement on arrows and Cohen's \u03ba (Cohen 1960) for the agreement on types of arrows. During the annotation period, 11 measurements between two coders have been 3 Evaluation of Parallel Annotations As reported in a technical report from the annotation of PDT (Ku\u010dov\u00e1 et al. 2003) . provided for (in total) 1,606 sentences in 39 documents. Table 1 shows average results of the interannotator agreement measurements for all types of textual coreference and bridging relations. Cases of Typical Disagreement Proceeding to further phases of the annotation process didn't give us any dramatic enhancement of the inter-annotator agreement. Some later measurements have shown even lower agreement than the earlier ones, although the quality of annotating was very high. That indicated that the results primarily depend neither on the annotators' experience in the field nor on their ability to follow the guidelines. Technically, as for the annotators, four general issues appeared to be difficult to decide: whether the relation is to be annotated for coreference/bridging at all, what is the correct antecedent of a given noun phrase, to distinguish between the bridging anaphora and the textual coreference and to select the type of the bridging anaphora or the textual coreference. These issues are closely analysed in the sections 5.1 to 5.4, with real-data examples. Annotating / not annotating a relation There is a relatively high degree of disagreement in the very recognition of a coreference or bridging relation in some typical cases. The most frequent example is a general reference of noun phrases, which may and may not be annotated as coreferential. (2) A kdy\u017e u\u017e byla kn\u00ed\u017eka hotova, tak se zjistilo, \u017ee je praktick\u00e1 i pro rodi\u010de. V t\u00e9to knize je pou\u010den\u00ed, jak sn\u00e1\u0161ej\u00ed d\u011bti rozvod a jak na n\u011bj reaguj\u00ed, a n\u00e1vod, jak se maj\u00ed rodi\u010de chovat, aby se utrpen\u00ed d\u011bt\u00ed sn\u00ed\u017eilo. (=After the book had been already written, it was clear, that it is quite useful for parents too. The book contains explanations, how children go through divorce, how they react to it, and the instructions how parents should behave to minimize the suffering of their children..) The disagreement is even more likely if the generic antecedent is relatively far from the noun phrase in question (example 3): (3) Preferuji \u0161ir\u0161\u00ed p\u0159edveden\u00ed s mnoha vnit\u0159n\u00edmi souvislostmi, proto\u017ee n\u00e1m chyb\u011bj\u00ed krit\u00e9ria pro hodnocen\u00ed sou\u010dasn\u00e9 \u010desk\u00e9 v\u00fdtvarn\u00e9 kultury. {11 sentences inbetween} M\u011bli bychom se znovu pokusit \u2026 z\u00edsk\u00e1vat sou\u010dasn\u00e9 um\u011bn\u00ed, abychom jednou m\u011bli autentick\u00fd soubor na\u0161\u00ed doby (= I prefer wider demonstration with many internal connections because we lack criteria for evaluation of contemporary Czech art. We should try ... to acquire the contemporary art again, in order to get an authentic set of our time.) Different selecting the antecedent / anaphoric element Compare ( 4 ) -( 6 ) for identity coreference. In (4), the anaphoric noun phrase the new structure corefers with the type F railing in one coder's annotation and with the G Street Bridge in the other's. (4) In Richmond, Ind., the type F railing is being used to replace arched openings on the G Street Bridge. Garret Boone, who teaches art at Earlham College, calls the new structure ``just an ugly bridge'' and one that blocks the view of a new park below. The measure in (5) corefers with the House bill on airline leveraged buy-outs in one coder's annotation and with the extended noun phrase legislation similar to the House bill on airline leveraged buy-outs in the other annotation: (5) While the Senate Commerce Committee has approved legislation similar to the House bill on airline leveraged buy-outs, the measure hasn't yet come to the full floor. The following example (6) demonstrates disagreement in constructions with measure and time-period words. The year earlier may corefer with prior-year or the prior-year period: (6) Reasons for Disagreement The evaluations of parallel annotations of selected texts brought up some interesting observations. The nature of disagreements corresponds to the general problem of a formal description on such a high level of language, namely -the texts sometimes allow for different, equally relevant interpretations. Moreover, the guidelines restrict us by the number of arrows leading from one node, and only a few formalized types of coreference and bridging relations are annotated in PDT, thus it does not fully reflect the real situation of text cohesion. See e.g. ( 4 ), where the semantically correct decision would be to annotate both relations as (near-)coreference, but not disposing such rich annotation guidelines, coders have to choose one variant and disagreement is to be expected. Reflecting the results, we were able to distinguish two main textual factors for disagreement: the text size and the degree of its abstractedness. Especially long texts with a large number of generic nouns, abstracts and deverbatives have the lowest inter-annotator agreement. A detailed manual comparison of parallel annotations revealed that almost three quarters of the coders' disagreements come from the text ambiguity (the relations may be empirically ambiguous as in (5), where coreferring with different antecedents may change the meaning, or rather near-identical in the sence of Recasens (2010) , when different interpretations are possible that do not actually change the meaning of the text as a whole). Constructions with nouns of measure and time periods appear to be hard to agree on (see e.g. 6) -in spite of quite detailed descriptions in the guidelines, coders tend to mark them differently in different types of context according to their intuition in every particular case. Generic noun phrases, abstract nouns and deverbatives cause really rich ambiguity in almost all coreference annotation projects. However, for Czech, it results in even more disagreements (examples (2), ( 3 ) and ( 8 )), because Czech does not have grammatical means to mark definiteness, thus forcing not to make any distinction in marking coreference between definite and indefinite noun phrases. Marking coreference between indefinite noun phrases results in a further reason of disagreement, and that is a different level of thoroughness of the coders' interpretation. For example, in (3), the antecedent for the contemporary art was used 11 sentences before, the noun phrase in question is positioned as (it has focus value in the TFA-annotation) and a coder doesn't need to see any serious reason to connect it by a coreference relation with such a distant antecedent. The similar situation is in (10) where, although not distant, the identity of the safety and health deficiencies and the hazards is up to the coder's intuition. (10) Gerard Scannell, the head of OSHA, said USX managers have known about many of the safety and health deficiencies at the plants for years, ``yet have failed to take necessary action to counteract the hazards.'' The rest of the coders' disagreements are caused be either a coder's mistake (cca 15% of occurences) or guidelines inconsistency (cca 10% of occurences). Certainty of the Manual Annotations To find out which part of problematic cases the coders are aware of, we organized one special inter-annotator agreement measurement. We asked the annotators to annotate the data as usual and also mark the certainty they had in several parts of the task. 4  They were asked to mark the certainty for their annotation decisions on the scale of 1 to 3 (1 means quite certain, 2 means moderately 4 This measurement was performed on 190 sentences in 2 documents. certain, 3 means not really certain). The certainty was marked for four types of decision (tasks), according to cases of frequent disagreement described in sections 5.1-5.4, i.e. certainty in the presence of a relation, certainty in selecting the antecedent, certainty in distinguishing between the bridging relation and the textual coreference and certainty in selecting the type of the bridging anaphora or the textual coreference. The certainty and the inter-annotator agreement were then measured separately for these tasks and (where applicable) also separately for various levels of certainty. Certainty in the presence of a relation Table 2 shows the average certainty the annotators expressed in various situations in the task of detecting the presence of a relation.  The numbers show that the lower the agreement is, the less sure the annotators are. However, if we look at the absolute numbers of (non-)annotating textual coreference, we see that the number of cases where the annotators didn't mark uncertainty but still disagreed exceeds all other cases. In the analysed documents, uncertainty was marked in 26 cases of disagreement. In another 30 cases where only one coder annotated a coreference relation, the uncertainty was not marked. Again, the numbers show a lower agreement in cases where the annotators were not sure about the antecedent. However, from 27 disagreements in choosing the antecedent, only 16 were marked as uncertain by at least one annotator. The difference in agreement between \"certain\" and \"uncertain\" relations in this case is not so relevant. As seen from the table, the agreement is very high. In most cases (21 out of 32), the annotators marked ambiguity but still made the same decision. Certainty in selecting the antecedent Certainty in distinguishing between the bridging anaphora and the textual coreference Certainty in selecting the type of the bridging anaphora or the textual coreference The following Discussion Analyzing the inter-annotator agreement together with the results of annotators' certainty about the relations reveals the following challenging issues: Firstly, it points out the complexity of real corpus data which can never be reflected by any annotation guidelines in full detail. See e.g. examples (2), ( 4 ) and (6) that are not empirically ambiguous but cannot be captured by single yes/no identity rules. The same is true for bridging anaphora: a small set of relations which can yet be reasonable in largescale corpora annotation cannot capture all cases of text cohesion. Unlike syntax, annotation of \"higher\" levels (coreference, bridging relations, discourse, etc.) does not reflect a language phenomenon as a whole. It rather excerpts a part of it, which is relevant for a certain task, and formalizes it to a reasonable degree. Contra-intuitivity, such formalized decisions result in a lower inter-annotator agreement. Also the annotators' certainty is lower in cases where intuition goes against the guidelines. Entities might seem to be very coherent, but there may be no good formal relation to be identified. Secondly, empirical ambiguity seems to be more frequent on text level than on syntax level and lower. However, a detailed analysis of our data confirms the Recasens' at al. (2010) and Poesio-Artstein's (2005) statements: ambiguity is much better seen when comparing parallel annotations than when asking annotators to mark it by themselves. Thirdly, weak points of the annotation guidelines are revealed. Not having precise and exhaustive rules, annotators naturally doubt more. In our case, this concerns first of all classifying generic noun phrases, abstract nouns and deverbatives. Also noun phrases with measures of different kind, time periods and some language specific constructions appear to be problematic. Annotators are much less certain about relations between generic and abstract nouns. Also the inter-annotator agreement for these cases is always lower than that for specific nouns with concrete meaning. Generally, we can say that in Czech, the most frequent reason for inter-annotator disagreement is not so much metonymy and different cases of near-identity relations in the sense of Recasens, but rather the relations between noun phrases with a generic and an abstract meaning. An improvement of such a problematic area would be to have the semantic information assigned to nouns themselves, as a part of tectogrammatical information. However, this task is very timeconsuming. Comparing the parallel annotations also shows that annotators are more sure about relations between noun phrases in topic and contrastive topic than about those in focus. More than other nouns, this fact concerns generic and abstract nouns and deverbatives. Coreference of these types of nouns in focus is not always obvious. Presented as new, coreference relation with a preceding noun phrase referring to the same type loses its relevance. However, this statement is rather a hypothesis, it needs further investigation. Conclusion We presented an evaluation and analysis of disagreements in the annotation of coreference and bridging relations in the Prague Dependency Treebank. As demonstrated by the results of parallel annotations, the agreement decreases in the direction from pronominal and zero coreference towards bridging relations. We extracted four most frequent types of problematic cases, exemplified them and described the possible reasons of inter-annotator disagreements. Then we asked annotators to mark the certainty they had in these cases and compared the results to the results of inter-annotator agreement. Although the percentage numbers were quite predictable (the less sure the annotators were, the lower was the agreement), the absolute numbers indicate that there remain many disagreements where uncertainty was not marked by any annotator. Acknowledgments We gratefully acknowledge support from the Grant Agency of the Czech Republic (grants P406/12/0658 and P406/2010/0875).",
    "abstract": "In this paper, we present the results of the parallel Czech coreference and bridging annotation in the Prague Dependency Treebank 2.0. The annotation is carried out on dependency trees (on the tectogrammatical layer). We describe the inter-annotator agreement measurement, classify and analyse the most common types of annotators' disagreement. On two selected long texts, we asked the annotators to mark the degree of certainty they have in some most problematic points; we compare the results to the inter-annotator agreement measurement.",
    "countries": [
        "Czech Republic"
    ],
    "languages": [
        "Czech"
    ],
    "numcitedby": "3",
    "year": "2013",
    "month": "August",
    "title": "Annotators{'} Certainty and Disagreements in Coreference and Bridging Annotation in {P}rague Dependency Treebank"
}