{
    "framing_question": "If this paper introduces a dataset, how is data filtered out during its creation, if at all?",
    "response": "This paper does not introduce a dataset, so I cannot answer this question from the provided context.",
    "article": "This paper describes how grammar-based language models for speech recognition systems can be generated from Grammatical Framework (GF) grammars. Context-free grammars and finite-state models can be generated in several formats: GSL, SRGS, JSGF, and HTK SLF. In addition, semantic interpretation code can be embedded in the generated context-free grammars. This enables rapid development of portable, multilingual and easily modifiable speech recognition applications. Introduction Speech recognition grammars are used for guiding speech recognizers in many applications. However, there are a number of problems associated with writing grammars in the low-level, systemspecific formats required by speech recognizers. This work addresses these problems by generating speech recognition grammars and semantic interpretation components from grammars written in Grammatical Framework (GF), a high-level, typetheoretical grammar formalism. Compared to existing work on compiling unification grammars, such as Regulus (Rayner et al., 2006) , our work uses a type-theoretical grammar formalism with a focus on multilinguality and modular grammar development, and supports multiple speech recognition grammar formalisms, including finite-state models. We first outline some existing problems in the development and maintenance of speech recognition grammars, and describe how our work attempts to address these problems. In the following two sections we introduce speech recognition grammars and Grammatical Framework. The bulk of the paper then describes how we generate context-free speech recognition grammars, finite-state language models and semantic interpretation code from GF grammars. We conclude by giving references to a number of experimental dialogue systems which already use our grammar compiler for generating speech recognition grammars. Expressivity Speech recognition grammars are written in simple formalisms which do not have the powerful constructs of high-level grammar formalisms. This makes speech recognition grammar writing labor-intensive and error prone, especially for languages with more inflection and agreement than English. This is solved by using a high-level grammar formalism with powerful constructs and a grammar library which implements the domain-independent linguistic details. Duplicated work When speech recognition grammars are written directly in the low-level format required by the speech recognizer, other parts of the system, such as semantic interpretation components, must often be constructed separately. This duplicated work can be avoided by generating all the components from a single declarative source, such as a GF grammar. Consistency Because of the lack of abstraction mechanisms and consistency checks, it is difficult to modify a system which uses hand-written speech recognition grammars. The problem is multiplied when the system is multilingual. The developer has to modify the speech recognition grammar and the semantic interpretation component manually for each language. A simple change may require touching many parts of the grammar, and there are no automatic consistency checks. The strong typing of the GF language enforces consistency between the semantics and the concrete representation in each language. Localization With hand-written grammars, it is about as difficult to add support for a new language as it is to write the grammar and semantic interpretation for the first language. GF's support for multilingual grammars and the common interface implemented by all grammars in the GF resource grammar library makes it easier to translate a grammar to a new language. Portability A grammar in any given speech recognition grammar format cannot be used with a speech recognizer which uses another format. In our approach, a GF grammar is used as the canonical representation which the developer works with, and speech recognition grammars in many formats can be generated automatically from this representation. Speech Recognition Grammars To achieve acceptable accuracy, speech recognition software is guided by a language model which defines the language which can be recognized. A language model may also assign different probabilities to different strings in the language. A language model can either be a statistical language model (SLM), such as an n-gram model, or a grammarbased language model, for example a context-free grammar (CFG) or a finite-state automaton (FSA). In this paper, we use the term speech recognition grammar (SRG) to refer to all grammar-based language models, including context-free grammars, regular grammars and finite-state automata. Grammatical Framework Grammatical Framework (GF) (Ranta, 2004 ) is a grammar formalism based on constructive type the-ory. In GF, an abstract syntax defines a semantic representation. A concrete syntax declares how terms in an abstract syntax are linearized, that is, how they are mapped to concrete representations. GF grammars can be made multilingual by having multiple concrete syntaxes for a single abstract syntax. The Resource Grammar Library The GF Resource Grammar Library (Ranta et al., 2006) currently implements the morphological and syntactic details of 10 languages. This library is intended to make it possible to write grammars without caring about the linguistic details of particular languages. It is inspired by library-based software engineering, where complex functionality is implemented in reusable software libraries with simple interfaces. The resource grammar library is used through GF's facility for grammar composition, where the abstract syntax of one grammar is used in the implementation of the concrete syntax of another grammar. Thus, an application grammar writer who uses a resource grammar uses its abstract syntax terms to implement the linearizations in the application grammar. The resource grammars for the different languages implement a common interface, i.e. they all have a common abstract syntax. This means that grammars which are implemented using resource grammars can be easily localized to other languages. Localization normally consists of translating the application-specific lexical items, and adjusting any linearizations which turn out to be unidiomatic in the language in question. For example, when the GoTGoDiS (Ericsson et al., 2006) application was localized to Finnish, only 3 out of 180 linearization rules had to be changed. An Example GF Grammar Figure 1 contains a small example GF abstract syntax. Figure 2 defines an English concrete syntax for it, using the resource grammar library. We will use this grammar when we show examples of speech recognition grammar generation later. In the abstract syntax, cat judgements introduce syntactic categories, and fun judgements declare constructors in those categories. For example, the  items constructor makes an Items term from an Item, a Number and a Size. The term items pizza two small is an example of a term in this abstract syntax. In the concrete syntax, a lincat judgement declares the type of the concrete terms generated from the abstract syntax terms in a given category. The linearization of each constructor is declared with a lin judgement. In the concrete syntax in Figure 2 , library functions from the English resource grammar are used for the linearizations, but it is also possible to write concrete syntax terms directly. The linearization of the term items pizza two small is {s = \"two small pizzas\"}, a record containing a single string field. By changing the imports and the four lexical items, this grammar can be translated to any other language for which there is a resource grammar. For example, in the German version, we replace (regN \"beer\") with (reg2N \"Bier\" \"Biere\" neuter) and so on. The functions regN and reg2N implement paradigms for regular English and German nouns, respectively. This replacement can be formalized using GF's parameterized modules, which lets one write a common implementation that can be instantiated with the language-specific parts. Note that the application grammar does not deal with details such as agreement, as this is taken care of by the resource grammar. 4 Generating Context-free Grammars Conversion to CFG The GF grammar is first converted into a context-free grammar annotated with functions and profiles, as described by Ljungl\u00f6f (2004) . Cycle elimination All directly and indirectly cyclic productions are removed, since they cannot be handled gracefully by the subsequent left-recursion elimination. Such productions do not contribute to the coverage to the grammar, only to the set of possible semantic results. Bottom-up filtering Productions whose righthand sides use categories for which there are no productions are removed, since these will never match any input. Top-down filtering Only productions for categories which can be reached from the start category are kept. This is mainly used to remove parts of the grammar which are unused because of the choice of start category. One example where this is useful is when a speech recognition grammar is generated from a multimodal grammar (Bringert et al., 2005) . In this case, the start category is different from the start category used by the parser, in that its linearization only contains the speech component of the in- put. Top-down filtering then has the effect of excluding the non-speech modalities from the speech recognition grammar. The bottom-up and top-down filtering steps are iterated until a fixed point is reached, since both these steps may produce new filtering opportunities. Left-recursion elimination All direct and indirect left-recursion is removed using the LC LR transform described by Moore (2000) . We have modified the LC LR transform to avoid adding productions which use a category A\u2212X when there are no productions for A\u2212X. Identical category elimination In this step, the categories are grouped into equivalence classes by their right-hand sides and semantic annotations. The categories A 1 . . . A n in each class are replaced by a single category A 1 +. . .+ A n throughout the grammar, discarding any duplicate productions. This has the effect of replacing all categories which have identical sets of productions with a single category. Concrete syntax parameters which do not affect inflection is one source of such redundancy; the LC LR transform is another. EBNF compaction The resulting context-free grammar is compacted into an Extended Backus-Naur Form (EBNF) representation. This reduces the size and improves the readability of the final grammar. The compaction is done by, for each category, grouping all the productions which have the same semantic interpretation, and the same sequence of non-terminals on their right-hand sides, ignoring any terminals. The productions in each group are merged into one EBNF production, where the terminal sequences between the non-terminals are converted to regular expressions which are the unions of the original terminal sequences. These regular expressions are then minimized. Conversion to output format The resulting nonleft-recursive grammar is converted to SRGS, JSGF or Nuance GSL format. A fragment of a SRGS ABNF grammar generated from the GF grammar in Figure 2 is shown below. The left-recursive and rule was removed from the grammar before compilation, as the left-recursion elimination step makes it difficult to read the generated grammar. The fragment shown here is for the singular part of the items rule. $FE1 = $FE6 $FE9 $FE4; $FE6 = one; $FE9 = large | small; $FE4 = beer | pizza; The corresponding fragment generated from the German version of the grammar is more complex, since the numeral and the adjective must agree with the gender of the noun. $FG1 = $FG10 $FG13 $FG6 | $FG9 $FG12 $FG4; $FG9 = eine; $FG10 = ein; $FG12 = gro\u00dfe | kleine; $FG13 = gro\u00dfes | kleines; $FG4 = Pizza; $FG6 = Bier; Discussion The generated grammar is an overgenerating approximation of the original GF grammar. This is inevitable, since the GF formalism is stronger than context-free grammars, for example through its support for reduplication. GF's support for dependently typed and higher-order abstract syntax is also not yet carried over to the generated speech recognition grammars. This could be handled in a subsequent semantic interpretation step. However, that requires that the speech recognizer considers multiple hypotheses, since some may be discarded by the semantic interpretation. Currently, if the abstract syntax types are only dependent on finite types, the grammar can be expanded to remove the dependencies. This appears to be sufficient for many realistic applications. In some cases, empty productions in the generated grammar could cause problems for the cycle and left-recursion elimination, though we have yet to encounter this in practice. Empty productions can be removed by transforming the grammar, though this has not yet been implemented. For some grammars, the initial CFG generation can generate a very large number of productions. While the resulting speech recognition grammars are of a reasonable size, the large intermediate grammars can cause memory problems. Further optimization is needed to address this problem. Finite-State Models Algorithm Some speech recognition systems use finite-state automata rather than context-free grammars as language models. GF grammars can be compiled to finite-state automata using the procedure shown in Figure 3 . The initial part of the compilation to a finite-state model is shared with the context-free SRG compilation, and is described in Section 4. Regular approximation The context-free grammar is approximated with a regular grammar, using the algorithm described by Mohri and Nederhof (2001) . Compilation to finite-state automata The regular grammar is transformed into a set of nondeterministic finite automata (NFA) using a modified version of the make fa algorithm described by Nederhof (2000) . For realistic grammars, applying the original make fa algorithm to the whole grammar generates a very large automaton, since a copy of the sub-automaton corresponding to a given category is made for every use of the category. Instead, one automaton is generated for each category in the regular grammar. All categories which are not in the same mutually recursive set as the category for which the automaton is generated are treated as terminal symbols. This results in a set of automata with edges labeled with either terminal symbols or the names of other automata. If desired, the set of automata can be converted into a single automaton by substituting each category-labeled edge with a copy of the corresponding automaton. Note that this always terminates, since the sub-automata do not have edges labeled with the categories from the same mutually recursive set. Minimization Each of the automata is turned into a minimal deterministic finite automaton (DFA) by using Brzozowski's (1962) algorithm, which minimizes the automaton by performing two determinizations and reversals. Conversion to output format The resulting finite automaton can be output in HTK Standard Lattice Format (SLF). SLF supports sub-lattices, which allows us to convert our set of automata directly into a set of lattices. Since SLF uses labeled nodes, rather than labeled edges, we move the labels to the nodes. This is done by first introducing a new labeled node for each edge, and then eliminating all internal unlabeled nodes. Figure 4 shows the SLF model generated from the example grammar. For clarity, the sub-lattices have been inlined. Discussion Finite-state models are even more restrictive than context-free grammars. This problem is handled by approximating the context-free grammar with an overgenerating finite-state automaton. This may lead to failure in a subsequent parsing step, which, as in the context-free case, is acceptable if the recognizer can return all hypotheses. Semantic Interpretation Semantic interpretation can be done as a separate parsing step after speech recognition, or it can be done with semantic information embedded in the speech recognition grammar. The latter approach resembles the semantic actions used by parser generators for programming languages. One formalism for semantic interpretation is the proposed Semantic Interpretation for Speech Recognition (SISR) standard. SISR tags are pieces of ECMAScript code embedded in the speech recognition grammar. Algorithm The GF system can include SISR tags when generating speech recognitions grammars in SRGS and JSGF format. The SISR tags are generated from the semantic information in the annotated CFG (Ljungl\u00f6f, 2004) . The result of the semantic interpretation is an abstract syntax term. The left-recursion elimination step makes it somewhat challenging to produce correct abstract syntax trees. We have extended Moore's (2000) LC LR transform to preserve the semantic interpretation. The LC LR transform introduces new categories of the form A\u2212X where X is a proper left corner of a category A. The new category A\u2212X can be understood as \"the category A, but missing an initial X\". Thus the semantic interpretation for a production in A\u2212X is the semantic interpretation for the original Aproduction, abstracted (in the \u03bb-calculus sense) over the semantic interpretation of the missing X. Conversely, where-ever a category A\u2212X is used, its result is applied to the interpretation of the occurrence of X. Discussion As discussed in Section 4.2, the semantic interpretation code could be used to implement the non-context-free features of GF, but this is not yet done. The slot-filling mechanism in the GSL format could also be used to build semantic representations, by returning program code which can then be executed. The UNIANCE grammar compiler (Bos, 2002) uses that approach. 7 Related Work Unification Grammar Compilation Compilation of unification grammars to speech recognition grammars is well described in the literature (Moore, 1999; Dowding et al., 2001) . Regulus (Rayner et al., 2006) is perhaps the most ambitious such system. Like GF, Regulus uses a general grammar for each language, which is specialized to a domain-specific one. Ljungl\u00f6f (Ljungl\u00f6f, 2007b) relates GF and Regulus by showing how to convert GF grammars to Regulus grammars. We carry compositional semantic interpretation through left-recursion elimination using the same idea as the UNIANCE grammar compiler (Bos, 2002) , though our version handles both direct and indirect left-recursion. The main difference between our work and the existing compilers is that we work with typetheoretical grammars rather than unification grammars. While the existing work focuses on GSL as the output language, we also support a number of other formats, including finite-state models. By using the GF resource grammars, speech recognition language models can be produced for more languages than with previous systems. One shortcoming of our system is that it does not yet have support for weighted grammars. Generating SLMs from GF Grammars Jonson ( 2006 ) has shown that in addition to generating grammar-based language models, GF can be used to build statistical language models (SLMs). It was found that compared to our grammar-based approach, use of generated SLMs improved the recognition performance for out-of-grammar utterances significantly. Results Speech recognition grammars generated from GF grammars have already been used in a number of research dialogue systems. GOTTIS (Bringert et al., 2005; Ericsson et al., 2006) , an experimental multimodal and multilingual dialogue system for public transportation queries, uses GF grammars for parsing multimodal input. For speech recognition, it uses GSL grammars generated from the speech modality part of the GF grammars. DJ-GoDiS, GoDiS-deLUX, and GoTGoDiS (Ericsson et al., 2006) are three applications which use GF grammars for speech recognition and parsing together with the GoDiS implementation of issuebased dialogue management (Larsson, 2002) . GoT-GoDiS has been translated to 7 languages using the GF resource grammar library, with each new translation taking less than one day (Ericsson et al., 2006) . The DICO (Villing and Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl\u00f6f, 2007a) . DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemon, 2006; Lemon et al., 2006a) uses an SLF language model generated from a GF grammar. Perera and Ranta (2007) have studied how GF grammars can be used for localization of dialogue systems. A GF grammar was developed and localized to 4 other languages in significantly less time than an equivalent GSL grammar. They also found the GSL grammar generated by GF to be much smaller than the hand-written GSL grammar. Conclusions We have shown how GF grammars can be compiled to several common speech recognition grammar formats. This has helped decrease development time, improve modifiability, aid localization and enable portability in a number of experimental dialogue systems. Several systems developed in the TALK and DICO projects use the same GF grammars for speech recognition, parsing and multimodal fusion (Ericsson et al., 2006) . Using the same grammar for multiple system components reduces development and modification costs, and makes it easier to maintain consistency within the system. The feasibility of rapid localization of dialogue systems which use GF grammars has been demonstrated in the GoTGoDiS (Ericsson et al., 2006) system, and in experiments by Perera and Ranta (2007) . Using speech recognition grammars generated by GF makes it easy to support different speech recognizers. For example, by using the GF grammar compiler, the DUDE (Lemon and Liu, 2006) system can support both the ATK and Nuance recognizers. Implementations of the methods described in this paper are freely available as part of the GF distribution 1 . Acknowledgments Aarne Ranta, Peter Ljungl\u00f6f, Rebecca Jonson, David Hjelm, Ann-Charlotte Forslund, H\u00e5kan Burden, Xingkun Liu, Oliver Lemon, and the anonymous referees have contributed valuable comments on the grammar compiler implementation and/or this article. We would like to thank Nuance Communications, Inc., OptimSys, s.r.o., and Opera Software ASA for software licenses and technical support. The code in this paper has been typeset using lhs2TeX, with help from Andres L\u00f6h. This work has been partly funded by the EU TALK project, IST-507802.",
    "funding": {
        "military": 0.0,
        "corporate": 0.31405117968896135,
        "research agency": 0.7606500898122321,
        "foundation": 0.0006071795318584039,
        "none": 1.8624621656027074e-06
    }
}