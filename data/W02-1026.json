{
    "article": "In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task (Lewis, 1992) , (Yang and Wilbur, 1995) , (Baker and McCallum, 1998) , (Lam and Ho, 1998) , (Mc-Callum, 1999) , (Dumais and Chen, 2000) . Most of them use the Reuters-21578 articles 1 in the evalu-ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578 (Joachims, 1998) , (Dumais et al., 1998) , (Yang and Liu, 1999) . More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications (Mladenic and Grobelnik, 1998) , (McCallum, 1999) , (Dumais and Chen, 2000) . The increasing number of documents and categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems (Dietterich, 2000) . There are at least two strategies for solving these problems. One is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable. McCallum et. al. presented a method called 'shrinkage' to improve parameter estimates by taking advantage of a hierarchy (McCallum, 1999) . They tested their method using three different real-world datasets: 20,000 articles from UseNet, 6,440 web pages from the industry sector, and 14,831 pages from Yahoo, and showed improved performance. Dumais et. al. used SVMs and classified hierarchical web content consisting of 50,078 web pages for training, and 10,024 for testing, with promising results (Dumais and Chen, 2000) . The other is to use \u00a2\u00a1 \u00a4\u00a3 \u00a5 \u00a7\u00a6 \u00a9 methods which are learning algorithms that construct a set of classifiers and then classify new data by taking a (weighted) vote of their predictions (Dietterich, 2000) (Freund and Schapire, 1996) . Dietterich has compared these methods (Dietterich, 2000) . He reported that in low-noise data, AD-ABOOST performs well, while in high-noise cases, it yields overfitting because ADABOOST puts a large amount of weight on the mislabeled examples. Bagging works well on both the noisy and the noisefree data because it focuses on the statistical problem which arises when the amount of training data available is too small, and noise increases this statistical problem. However, it is not clear whether 'works well' means that it exponentially reduces the amount of computation necessary for classification, while sacrificing only a small amount of accuracy, or whether it is statistically significantly better than other methods. In this paper, we address the problem of dealing with a large collection of data and report on an empirical study for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB probabilistic classifiers are based on the assumption of word independence in a text which makes the computation of the NB classifiers far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, since SVMs use overfitting protection which does not necessarily depend on the number of features, and thus makes it possible to produce better performance. The basic idea of our approach is quite simple: We solve simple classification problems using NB and more complex and difficult problems using SVMs. As in previous research, we use category hierarchies. We use all the training data for NB. The training data for SVMs, on the other hand, is extracted using NB classifiers. The training data is learned by NB using cross-validation according to the hierarchical structure of categories, and only the documents which could not classify correctly by NB classifiers in each category level are extracted as the training data of SVMs. The rest of the paper is organized as follows. The next section provides the basic framework of NB and SVMs. We then describe our classification method. Finally, we report some experiments using 279,303 documents in the Reuters 1996 corpus with a discussion of evaluation. Classifiers NB Naive Bayes(NB) probabilistic classifiers are commonly studied in machine learning (Mitchell, 1996) . The basic idea in NB approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. The NB assumption is that all the words in a text are conditionally independent given the value of a classification variable. There are several versions of the NB classifiers. Recent studies on a Naive Bayes classifier which is proposed by McCallum et. al. reported high performance over some other commonly used versions of NB on several data collections (McCallum et al., 1998) . We use the model of NB by McCallum et. al. which is shown in formula (1) . \"! $# &% (' \u00a2) 10 32 &45 76 98 \"! @# % ' A45 B6 $C ED F HG D I \u00a2P RQ \"! @S F G $T ' U# % 2 &45 B6 D V WD X P RQ \"! @# X ' A45 B6 $C D F YG `D I \u00a2P Q \"! @S F G @T ' U# X 2 &45 76 a cb d \u00a2e W \"! @S \u00a4f W' g# % \u00a22 &45 B6 h8 i (p D q rD 0 P RQ &s ! @S f 2 t) 0 6 \"! @# % ' g) 0 6 ' vu w' p D x dD y P Q D q rD 0 P RQ s ! @S y 2 t) 10 6 ! @# &% ' \u00a2) 0 6 ! @# % ' 45 B6 h8 D q rD 0 P RQ \"! @# % ' \u00a2) 0 6 H ' \u00a2 ' (1) refers to the number of vocabularies, denotes the number of labeled training documents, and shows the number of categories. SVMs SVMs are introduced by Vapnik (Vapnik, 1995) ), where x is an arbitrary data point, and u = ( a ~} ,v vv v , a { ) and \u00a8are learned from a training set of linearly separable data. Figure 1 shows an example of a simple two-dimensional problem that is linearly separable 2 . In the linearly separable case maximizing the margin can be expressed as an optimization problem: (1 is, the more the word a d Margin | t \u00a4 0 P RQ & 0 p Q 0 % P RQ 0 % ` 70 v% o 0 1 \u00a2 0 (2) s.t 0 P RQ 0 \u00a50 8 0 8 0 P RQ 0 \u00a50 0 (3) where x = ( } ,v vv v , { ) features positive examples. We note that SVMs are basically introduced for solving binary classification, while text classification is a multi-class, multi-label classification problem. Several methods using SVMs which were intended for multi-class, multi-label data have been proposed (Weston and Watkins, 1998) . We use \u00a5 \u00a1 \u00a4 - v r v \u00a1 \u00a6\u00a3 \u00a7 \u00a7 - \u00a7 &b d -\u00a8 \u00a5\u00a3 \u00a7 \u00a7 version of the SVMs model in the work. A time complexity of SVMs is known as \u00a5 \u00a9f \u00aa j \u00ab \u00a5 f \u00ac j , where is the number of training data. We consider a time complexity of \u00a5 \u00a1 \u00a6 - v r v \u00a1 \u00a6\u00a3 \u00a7 \u00a7 - \u00a7 &b d -\u00a8 \u00a5\u00a3 \u00a7 \u00a7 method. Let \u00b0\u00a9f \u00ae j \u00b2\u00a2 v \u00ae \u00ac , where is a constant. \u00a5 \u00a1 \u00a4 - v \u00a1 \u00a4\u00a3 \u00a7 \u00a7 - \u00a7 &b d -\u00a8 \u00a5\u00a3 \u00a7 \u00a7 method is thus done in time H\u00ac . 3 System Design Hierarchical classification A well-known technique for classifying a large, heterogeneous collection such as web content is to use category hierarchies. Following the approaches of Koller and Sahami (Koller and Sahami, 1997) , and Dumais's (Dumais and Chen, 2000) , we employ a hierarchy by learning separate classifiers at each internal node of the tree, and then labeling a document using these classifiers to greedily select subbranches until a leaf is reached. Manipulating training data Our hypothesis regarding NB is that it can work well for documents which are assigned to only one category within the same category level in the hierarchical structure. We base this on some recent papers claiming that NB methods perform surprisingly well for an 'accuracy' measure which is equivalent to the standard precision under the one-category-perdocument assumption on classifiers and also equivalent to the standard recall, assuming that each document has one and only one correct category per cat-egory level (Lewis and Ringuette, 1994) , (Koller and Sahami, 1997) . SVMs, on the other hand, have the potential to handle more complex problems without sacrificing accuracy, even though the computation of the SVM classifiers is far less efficient than NB. We thus use NB for simple classification problems and SVMs for more complex data, i.e., the data which cannot be classified correctly by NB classifiers. We use ten-fold cross validation: All of the training data were randomly shuffled and divided into ten equal folds. Nine folds were used to train the NB classifiers while the remaining fold(held-out test data) was used to evaluate the accuracy of the classification. For each category level, we apply the following procedures. Let e \u00b3 be the total number of nine folds training documents, and e \u00b4be the number of the remaining fold in each class level. Figure 2 illustrates the flow of our system. 1-2 This process is repeated ten times so that each fold serves as the source of the test data once. The threshold, the probability value which produces the most accurate classifier through ten runs, is selected. 1-3 The held-out test data which could not be classified correctly by NB classifiers with the optimal parameters are extracted (e E\u00b5 n \u00b6 n \u00b6 g\u2022 g \u00b6 in Figure 2 ). They are used to train SVMs. The procedure is applied to each category level. Classifying test data 2-1 We use all the training data, e ~\u00b3 +e \u00b8\u00b4, to train NB classifiers and the data which is produced by procedure 1-3 to train SVMs. 2-2 NB classifiers are applied to the test data. The test data is judged to be the category l whose probability is larger than the threshold which is obtained by 1-2. 2-3 If the test data is not assigned to any one of the categories, the test data is classified by SVMs classifiers. The test data is judged to be the category l whose distance } \u00b9 \u00b9 \u00ba \u00bb \u00b9 \u00bc\u00b9 is larger than zero. We employ the hierarchy by learning separate classifiers at each internal node of the tree and then assign categories to a document by using these classifiers to greedily select sub-branches until a leaf is reached. Evaluation Data and Evaluation Methodology We evaluated the method using the 1996 Reuters corpus recently made available. The corpus from 20th Aug. to 31st Dec. consists of 279,303 documents. These documents are organized into 126 categories with a four level hierarchy. We selected 102 categories which have at least one document in the training set and the test set. The number of categories in each level is 25 top level, 33 second level, 43 third level, and 1 fourth level, respectively. Table 1 shows the number of documents in each top level category. After eliminating unlabelled documents, we obtained 271,171 documents. We divided these documents into two sets: a training set from 20th Aug. to 31th Oct. which consists of 150,939 documents, and test set from 1th Nov. to 31st Dec. which consists of 120,242 documents. We obtained a vocabulary of 183,400 unique words after eliminating words which occur only once, stemming by a partof-speech tagger (Schmid, 1995) , and stop word removal. Figure 3 We use ten-fold cross validation for learning NB parameters. For evaluating the effectiveness of category assignments, we use the standard recall, precision, and \u00bd \u00bf\u00be measures. Recall denotes the ratio of correct assignments by the system divided by the total number of correct assignments. Precision is the ratio of correct assignments by the system divided by the total number of the system's assignments. The \u00bd \u00be measure which combines recall ( e ) and precision (\u00c0 ) with an equal weight is \u00bd \u00bf\u00be 1f e 1i \u00c0 j q\u00a2 \u00aa \u00b6 g\u00c1 \u00b6 &\u00c2 A\u00c1 . Results and Discussion The result is shown in 2 shows the results of all 102 categories. The micro-averaged F1 score of our method in 'all' (0.704) is higher than the NB (0.519) and SVMs scores (0.285). We note that the F1 score of SVMs (0.285) is significantly lower than other models. This is because we could not obtain a classifier to judge the category 'corporate/industrial' in the top level within 10 days using a standard 2.4 GHz Pentium IV PC with 1,500 MB of RAM. We thus eliminated the category and its child categories from the 102 categories. The number of the remaining categories in each level is 24 top, 14 second, 29 third, and 1 fourth level. 'Parts' in Table 2 denotes the results. There is no significant difference between 'all' and 'parts' in our method, as the F1 score of 'all' was 0.704 and 'parts' was 0.700. The F1 of our method in 'parts' is also higher than the NB and SVMs scores. Table 3 denotes the amount of training data used to train NB and SVMs in our method and test data judged by each classifier. We can see that our method makes the computation of the SVMs more efficient, since the data trained by SVMs is only 23,243 from 150,939 documents. Table 4 illustrates the results of three methods according to each category level. 'Training' in 'Manipulating data' denotes the number of documents used to train SVMs. The overall F1 value of NB, SVMs, and our method for the 25 top-level cate- There is no significant difference between the overall F1 value of the second(0.608) and third level categories(0.606) in our method, while the accuracy of the other methods drops when classifiers select sub-branches, in third level categories. As Dumais et. al. mentioned , the results of our experiment show that performance varies widely across categories. The highest F1 score is 0.864 ('Commodity markets' category), and the lowest is 0.284 ('Eco-nomic performance' category). The overall F1 values obtained by three methods for the fourth level category ('Annual result') are low. This is because there is only one category in the level, and we thus used all of the training data, 150,939 documents, to learn models. The contribution of the hierarchical structure is best explained by looking at the results with and without category hierarchies, as illustrated in Table 5 . It is interesting to note that the results of both NB and our method clearly demonstrate that incorporating category hierarchies into the classification method improves performance, whereas hierarchies degraded the performance of SVMs. This shows that the separation of one top level category(C) from the set of the other 24 top level categories is more difficult than separating C from the set of all the other 101 categories in SVMs. Table 6 illustrates sample words which have the highest weighted value calculated using formula (3). Recall that in SVMs each value of word a d (1 \u00a1 ) is calculated using formula (3), and the larger value of a d is, the more the word a d features positive examples. Table 6 denotes the results of two binary classifiers. One is a classifier that separates documents assigned the 'Economics' category(positive examples) from documents assigned a set of the other 24 top level categories, i.e. 'hierarchy'. The other is a classifier that separates documents with the 'Economics' category from documents with a set of the other 101 categories, i.e., 'non-hierarchy'. Table 6 shows that in 'Nonhierarchy', words such as 'economic', 'economy' and 'company' which feature the category 'Economics' have a high weighted value, while in 'hierarchy', words such as 'year' and 'month' which do not feature the category have a high weighted value. Further research using various subsets of the top level categories is necessary to fully understand the influence of the hierarchical structure created by  Finally, we compare our results with a wellknown technique, \u00a7\u00a1 \u00a6\u00a3 \u00a5 \u00a7\u00a6 \u00a9& strategies. In the experiment using ensemble, we divided a training set into ten folds for each category level. Once the individual classifiers are trained by SVMs they are used to classify test data. Each classifier votes and the test data is assigned to the category that receives more than 6 votes 3 . The result is illustrated in Table 7 . In Table 7 , 'Non-hierarchy' and 'Hierarchy' denotes the result of the 102 categories treated as a flat nonhierarchical problem, and the result using hierarchical structure, respectively. We can find that the result of \u00a7\u00a1 \u00a6\u00a3 7 \u00a2\u00a6 \u00a9& with hierarchy(0.704 F1) outperforms the result with non-hierarchy(0.532 F1). A necessary and sufficient condition for an ensemble of classifiers to be more accurate than any of its individual members is if the classifiers are l l o\u00c3 e \u00a7 & and \u00c5\u00c4 \u00a2e W\u00a3 \u00a5 (Hansen and Salamon, 1990) score) when we use hierarchical structure. However, the computation of the former is far more efficient than the latter. Furthermore, we see that our method (0.596 F1 score) slightly outperforms \u00a7\u00a1 \u00a6\u00a3 \u00a5 \u00a7\u00a6 \u00a9& (0.532 F1 score) when the 102 categories are treated as a flat non-hierarchical problem. Conclusions We have reported an approach to text classification which manipulates large corpora using NB and SVMs. Our main conclusions are: AE Our method outperforms the baselines, since the micro-averaged \u00bd \u00be score of our method was 0.704 and the baselines were 0.519 for NB and 0.285 for SVMs. AE As shown in previous researches, hierarchical structure is effective for classification, since the result of our method using hierarchical structure led to as much as a 10.8% reduction in error rates, and up to 1.3% with NB. AE There is no significant difference between the F1 scores of our method and the \u00a7\u00a1 \u00a6\u00a3 \u00a5 \u00a7\u00a6 \u00a9& method with hierarchical structure. However, the computation of our method is more efficient than the \u00a7\u00a1 \u00a6\u00a3 \u00a5 \u00a7\u00a6 \u00a9& method in the experiment. Future work includes (i) extracting features which discriminate between categories within the same top-level category, (ii) investigating other machine learning techniques to obtain further advantages in efficiency in the manipulating data approach, and (iii) evaluating the manipulating data approach using automatically generating hierarchies (Sanderson and Croft, 1999) . Acknowledgments We would like to thank Prof. Virginia Teller of Hunter College CUNY for her valuable comments ",
    "abstract": "In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy.",
    "countries": [
        "Japan"
    ],
    "languages": [],
    "numcitedby": "7",
    "year": "2002",
    "month": "July",
    "title": "Manipulating Large Corpora for Text Classification"
}