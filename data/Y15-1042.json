{
    "article": "This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-ofthe-art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an Icelandic-Korean lexicon from Wikipedia. Introduction Data-driven approaches to natural language processing have been shown to be greatly effective, and the case of bilingual lexicon extraction is no exception. Recent advances in this area have enabled the construction of large, highquality bilingual lexicons, requiring less parallel data by making use of comparable corpora. While such comparable corpora are readily available for many language pairs, particularly when one of those languages is English, previous direct approaches fail when there is no such data available. For many language pairs there simply does not exist comparable (and even less so parallel) data. Even for languages with a large volume of available parallel data, most corpora cover only limited domains. There are two natural methods to deal with this problem: constructing or mining new data for the direct approach, and finding new ways to make better use of what data is already available. For an example of the construction of comparable corpora, see Zhu et al. (2013) . We take the second approach and design pivot-based models for bilingual lexicon extraction. The major advantage of using a pivot language is that it is possible to take advantage of the large volume of comparable data sharing a common language such as English. In this paper we develop pivot-based approaches to make use of modern bilingual lexicon extraction methods that can be trained on comparable corpora. We present a selection of efficient algorithms using the framework of topic modelling (Blei et al., 2003) . Topic modelling has been a popular approach for bilingual lexicon extraction, however its use as a pivot model has yet to be explored. The use of topic models as a semantic similarity measure is a scalable method for low-resource languages because document-aligned comparable pivot training data (such as for English and a low-resource language) is growing ever more widely available. Examples of such sources are Wikipedia, multilingual newspaper articles and mined Web data. While there have been many studies on bilingual lexicon extraction, there has been little focus on the important problem of resource construction for low-resource language pairs. We present a variety of solutions to this problem, demonstrating their application to a practical scenario, and compare their effectiveness to mainstream approaches. Related Work The use of pivot models has been a common theme in the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and 'non-aligned signatures' (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012) . We use the method of bilingual topic modelling (Vuli\u0107 et al., 2011) , which has been recently applied to a variety of fields such as transliteration mining (Richardson et al., 2013) . Model Details We consider the task of translating a source word s from language S to a target word t from language T . The baseline model is a direct approach using S-T training data. After describing the baseline model (bilingual LDA), we introduce three novel methods of taking advantage of data including a pivot language P , such as S-P + P -T and S-P -T data. Baseline: Bilingual LDA We begin with a baseline non-pivot lexicon extraction model M ST : S \u00d7 T \u2192 R that gives a similarity score to a source-target word pair (using S-T training data). The non-pivot lexicon extraction model M ST makes use of a bilingual topic similarity measure. We elected to use bilingual topic models rather than the more intuitive method of comparing monolingual context vectors (Rapp, 1995) as we believe topic modelling is more suitable for processing uncommon language pairs. This is because a bilingual seed lexicon is required for methods that learn a mapping between source and target vector spaces, such as Haghighi et al. (2008) , in order to match crosslanguage word pairs. This data is unlikely to be available in sufficient quantity for low-resource language pairs, however comparable documents can be found from sources such as Wikipedia. We base our implementation on the state-ofthe-art system of Vuli\u0107 et al. (2011) for comparison. This method uses the bilingual Latent Dirichlet Allocation (BiLDA) algorithm (Mimno et al., 2009) , an extension of monolingual LDA (Blei et al., 2003) . Monolingual LDA takes as its input a set of monolingual documents and generates a word-topic distribution \u03d5 classifying words appearing in these documents into semantically similar topics. Bilingual LDA extends this by considering pairs of comparable documents in each of two languages, and outputs a pair of word-topic distributions \u03d5 and \u03c8, one for each input language. The graphical model for polylingual LDA is illustrated in Figure 1 . In order to apply bilingual topic models to a lexicon extraction task, we must construct an effective word similarity measure for translation candidates. This can be achieved by a variety of methods comparing the similarity of K-dimensional word-topic vectors. We use the simple and well-studied cosine similarity measure (as defined below) to measure the similarity between topic distribution vectors \u03c8 k,we and \u03d5 k,w f for translation candidates w e and w f . \u03c9 ). For further details of the LDA formulation see Blei et al. (2003) . Cos(w e , w f ) = \u2211 K k=1 \u03c8 k,we \u03d5 k,w f \u221a \u2211 K k=1 \u03c8 2 k,we \u221a \u2211 K k=1 \u03d5 2 k,w f (1) Trilingual LDA Model A simple yet interesting extension to applying bilingual LDA to source-target data is training trilingual LDA on a set of source-pivot-target language documents. Although in practice there may not exist such a large quantity of available trilingual data, we show in our experiments that this method is able to outperform the bilingual case even when there is a smaller volume of available trilingual data. An advantage of this approach is that we can expect the additional (pivot) language to provide an additional point of reference, stabilizing the topic-document distribution. We show that this leads to a considerable reduction in noise, improving the translation accuracy. The mathematical formulation is a natural extension of the bilingual case. We generate a triple of word-topic distributions \u03d5, \u03c8 and \u03c9 and a shared document-topic distribution \u03b8 using the same method as described above for bilingual LDA. The model is trained on triples of aligned comparable documents. Pivot Model In this section we consider an efficient method to construct a pivot model M SP,P T : S \u00d7 T \u2192 R (using S-P and P -T training data) that builds upon the non-pivot models M SP and M P T , which are built with the baseline (bilingual LDA) approach. The generation of a target word t \u2208 T is modelled as the two-step translation of a source word s \u2208 S to a pivot word p \u2208 P and then this p into T . We assume that for any translation candidate pair s, t: M SP,P T (s, t) = max p\u2208P M SP (s, p)M P T (p, t) (2) We would now like to generate the n-best distinct translations, however the size of the search space has increased to |P ||T | compared to |T | for the non-pivot model. The natural method for searching this space is to score every pivot translation s \u2192 p i with M SP (|P | scoring operations) and then for each p i to score every target translation p i \u2192 t j with M P T (|P ||T | scoring operations). These scores are then multiplied together and sorted to generate an n-best list. As we have no further information about M it is not possible to reduce the complexity of this search without making some approximations. We use a faster, approximate algorithm that greatly reduces the number of scoring operations required by using a beam search. The scoring operation, i.e. calculating M (s, t), is the most time consuming step and therefore the most important to be avoided. Using a beam width b, the top-b pivot candidates p 1 , ..., p b \u2208 P for s are first generated, requiring |P | scoring operations as we have no way to sort the p in advance. Then for each p i , we generate the top-b target candidates t i,1 , ..., t i,b for the translation of p i into T . This step requires only b|T | scoring operations. 1 There will be some search errors with this method and therefore b should be increased if a very accurate n-best list is required. The approximate algorithm collapses into the exact method as b increases. If there are many s to translate, it would be possible to cache the M P T , further improving the performance. See Figure 2 for an illustration of our search algorithm. 'Box' Model For many low-resource language pairs there does not exist source-target or trilingual data and therefore the pivot model is the only available option. However this is not always the case. For comparison we create one further model, the 'box' model, using all available data. The 'box' model uses source-pivot, pivottarget, source-target and source-pivot-target data. The data is combined by creating (source, pivot, target) triples for each document. For each language L, if there is a version of the document written in L, we add it to the triple, otherwise we insert an empty string. We liken this method to packing boxes, one per document for each language, with whatever data is available. These triples are then used to train a trilingual topic model as in Section 3.2. This approach has the advantages of avoiding noise and search errors that can be introduced by the pivot model in Section 3.3, however it relies on the availability of sufficient training data. When such data is not available we are still able to use the pivot model. Experiments In this section we consider a task where we wish to extract a Korean-Icelandic (KO-IS) and Icelandic-Korean (IS-KO) lexicon from comparable Wikipedia documents using English (EN) as a pivot language. This is a realistic scenario in which we have a sufficient quantity of aligned pivot-source and pivot-target document pairs but considerably less source-target data. We chose this language pair to demonstrate the effectiveness of our model on both low-resource and distant language pairs. English was the most natural pivot language for this task, however in some cases it might be preferable to use a different language. The topic models were all trained on document-aligned Wikipedia data. We extracted these documents from mid-2013 Wikipedia XML dumps and they were aligned using Wikipedia 'langlinks'. The distribution of aligned document pairs including combinations of these three languages is shown in Table 1 . Note that there is considerably less IS-KO data than for either EN-IS or EN-KO (only 60% of EN-IS, 10% of EN-KO). In fact the majority of trilingual data covers the same documents as the IS-KO subset, as the documents with IS and KO data very commonly also have an English version. EN IS KO Documents \u2713 \u2713 ? 22K \u2713 ? \u2713 140K ? \u2713 \u2713 14K \u2713 \u2713 \u2713 14K 2+ languages 190K While it is true that there does exist some IS-KO data in Wikipedia that could be used directly to build an IS-KO lexicon, we show that there is not enough to extract translation pairs with high accuracy. Furthermore, we also show that the proposed pivot model in Section 3.3 functions well without requiring any of this data. Settings We used an in-house English lemmatizer and tokenizer to prepare the English data. Icelandic data was processed with IceNLP (Loftsson and R\u00f6gnvaldsson, 2007) and Korean analyzed with HanNanum (Park et al., 2010) . For each language we extracted the most frequent 100K nouns for our experiments, a vocabulary size over 10 times larger than in previous work (Vuli\u0107 et al., 2011) . The test data consisted of N = 200 (EN, KO, IS) translation triples. These were created by randomly selecting 200 nouns from our English  Wikipedia vocabulary and translating these by hand into Korean and Icelandic. For comparison the same test data was used for all experiments. We used the PolyLDA++ tool (Richardson et al., 2013) to generate multilingual topic models. The training was run over 1000 iterations using K = 2000 topics. We set the LDA hyperparameters as \u03b1 = 50/K and \u03b2 = 0.01, which are the settings used most commonly in previous work on topic modelling. The models were evaluated by generating an n-best list of translations for each word in the test set. The following statistics were then measured for the extracted lexicon, where rank i was the rank given to the correct translation in the n-best list (\u221e if not in n-best list). We used n = 10. We also used b = 10 for the search beam width. \u2022 Top-1 accuracy: 1 N N \u2211 i=1 \u03b4 rank i ,1 (3) \u2022 Mean Reciprocal Rank (MRR): 1 N N \u2211 i=1 1 rank i (4) Comparison between Direct and Pivot Model Before applying the proposed pivot-based approaches to a realistic lexicon extraction scenario, we first verified the effectiveness of the pivot model in Section 3.3 using a controlled data set. We consider a task where we have a corpus of aligned triples of (EN, KO, IS) documents. Our data contained 14K triples (see Table 1 ) with a combined vocabulary size of 30K nouns. The experiment is to test the effectiveness of using the KO-IS data directly (baseline) with the nonpivot model M ST against using the pivot model M SP,P T with only KO-EN and EN-IS data. This is designed to be a fair comparison as we have the same number of documents in the pivot Table 2 shows the experimental results. These results show that when the same amount of data is available the pivot model is even more effective than using the source-target data directly. In fact the scores are higher for the pivot model and we believe there could be two reasons for this. Despite the same number of documents being used, the English articles are on average longer than their Icelandic and Korean counterparts and this could improve the effectiveness of training. It is also possible that many of the Icelandic and Korean articles were produced by partially or fully translating their corresponding English pages. This would lead to a tighter similarity in the models containing the pivot language. Lexicon Extraction Experiment We now turn to the main experiment, in which we consider the task of extracting a bilingual lexicon from Wikipedia for a low-resource language pair (IS-KO and KO-IS). In order to demonstrate the practical application of the proposed model, we use all the available data in Wikipedia, combining pivot and non-pivot models. \u2022 The baseline score ('baseline') is calculated for the non-pivot model M ST using only KO-IS data. This emulates the current state-of-the-art non-pivot lexicon extraction algorithm, which is only able to use the KO-IS data and model for direct translation. See Section 3.1. \u2022 The trilingual score ('trilingual') is the accuracy of our model trained using a trilingual topic model on trilingual (KO-EN-IS) data, which in practice is the most difficult to obtain. See Section 3.2. \u2022 The pivot score ('pivot') is evaluated for the proposed pivot model M SP,P T , able to make use of the KO-EN and EN-IS data. See Section 3.3. \u2022 The score ('box'), using all possible data, is constructed by combining baseline (KO-IS), pivot (EN-KO, EN-IS) and trilingual (EN-KO-IS) data. See Section 3.4. Figure 4 shows the data that is required (and was used) for each method. The results of the experiment are shown in Table 3 . Lang Pair Method Analysis and Discussion It can be seen from the results that all three proposed models considerably outperform the baseline. This demonstrates that these approaches are able to improve the quality of extracted lexicons for low-resource language pairs by making use of pivot language data, giving a large accuracy improvement over previous work. An interesting observation is that the trilingual model is able to greatly improve upon the baseline even though it uses less training data. It is probable that the addition of the additional language (English) has helped to reduce the noise in the Korean-Icelandic model by stabilizing the document-topic distribution. The pivot approach further improves on this by making use of the relatively large volume of EN-KO and EN-IS data pivot model score is not far from the most effective method 'box', which requires all the data, some of which is difficult in general to obtain (trilingual and KO-IS data). This shows that the pivot model is still able to compete with a model trained directly on source-target data. The most effective method was the 'box' approach and this is perhaps to be expected as it was able to make use of the largest volume of data. For relatively high-resource language pairs this method is likely to be the most effective as more data is available, however the pivot model becomes the only available option as the source-target data becomes sparse. When the necessary data is available, the 'box' approach can improve upon the pivot model. Tables 4 and 5 give examples of successful and incorrect translations using the pivot model. The model can be seen to perform more effectively on words with a concrete meaning (Table 4) and less so on abstract concepts (Table 5 ), which often have more variation in their represention across languages. Analysis of the n-best lists revealed a tendency for clumping of pivot words. As in the example in Table 6 , the same pivot word was often used to generate groups of consecutive target language words. This how- ever seems not to reduce the quality of the output, as we did not notice any significant change in the MRR scores when adding the restriction that only one target word could be generated from any pivot word. An example with less clumping is shown in Table 7 . Conclusion and Future Work In this paper we have presented three novel pivot-based approaches for bilingual lexicon extraction with low-resource language pairs. The proposed models are able to generate a highquality lexicon for language pairs with no direct source-target training data, and we have shown that each model considerably outperforms a state-of-the-art non-pivot baseline. With a variety of approaches it is possible to select an appropriate method based on the size and nature of available training data. There is much still to explore in the area of the construction of lexicons for low-resource language pairs. A possible extension to the proposed model is to use a larger pivot base, of not just one but of multiple pivot languages acting as a form of interlingua, similar to the idea in Dabre et al. (2014) . This could improve the quality of the model in cases where there is not such a clear choice for an appropriate pivot language. Another possibility for improvement is removing the assumption that there is an appropriate pivot word, using instead a direct mapping between the word-topic vector spaces for sourcepivot and pivot-target topic models. In the future we would like to use the proposed method to improve machine translation by extracting a large lexicon and applying it to a low-resource translation task. Acknowledgments We would like to thank the reviewers for their instructive comments. The first author is supported by a Japanese Government Scholarship (MEXT). ",
    "abstract": "This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-ofthe-art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an Icelandic-Korean lexicon from Wikipedia.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "English",
        "Ko",
        "Korean",
        "Japanese",
        "Icelandic"
    ],
    "numcitedby": "1",
    "year": "2015",
    "month": "October",
    "title": "Pivot-Based Topic Models for Low-Resource Lexicon Extraction"
}