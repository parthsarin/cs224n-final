{
    "article": "Supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics, such as hLLDA and hLDA. Supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels, but cannot explore new topics; while unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, but does not make use of any information from hierarchical labels. In this paper, we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical labels into the modeling process, called Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA). We also prove that hLDA and hLLDA are special cases of SSHLDA. We conduct experiments on Yahoo! Answers and ODP datasets, and assess the performance in terms of perplexity and clustering. The experimental results show that predictive ability of SSHLDA is better than that of baselines, and SSHLDA can also achieve significant improvement over baselines for clustering on the FScore measure. Introduction Topic models, such as latent Dirichlet allocation (LDA), are useful NLP tools for the statistical analysis of document collections and other discrete data. Furthermore, hierarchical topic modeling is able to obtain the relations between topics -parent-child and sibling relations. Unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, such as hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004) . hLDA makes use of nested Dirichlet Process to automatically obtain a L-level hierarchy of topics. Modern Web documents, however, are not merely collections of words. They are usually documents with hierarchical labels -such as Web pages and their placement in hierarchical directories (Ming et al., 2010) . Unsupervised hierarchical topic modeling cannot make use of any information from hierarchical labels, thus supervised hierarchical topic models, such as hierarchical Labeled Latent Dirichlet Allocation (hLLDA) (Petinot et al., 2011) , are proposed to tackle this problem. hLLDA uses hierarchical labels to automatically build corresponding topic for each label, but it cannot find new latent topics in the data space, only depending on hierarchy of labels. As we know that only about 10% of an iceberg's mass is seen outside while about 90% of it is unseen, deep down in water. We think that a corpus with hierarchical labels should include not only observed topics of labels, but also there are more latent topics, just like icebergs. hLLDA can make use of the information from labels; while hLDA can explore latent topics. How can we combine the merits of the two types of models into one model? An intuitive and simple combinational method is like this: first, we use hierarchy of labels as basic hierarchy, called Base Tree (BT); then we use hLDA to build automatically topic hierarchy for each leaf node in BT, called Leaf Topic Hierarchy (LTH); finally, we add each LTH to corresponding leaf in the BT and obtain a hierarchy for the entire dataset. We refer the method as Simp-hLDA. The performance of the Simp-hLDA is not so good, as can be seen from the example in Figure 3 (b). The drawbacks are: (i) the leaves in BT do not obtain reasonable and right words distribution, such as \"Computers & Internet\" node in Figure 3 (b), its topical words, \"the to you and a\", is not about \"Computers & Internet\"; (ii) the non-leaf nodes in BT cannot obtain words distribution, such as \"Health\" node in Figure 3 (b); (iii) it is a heuristic method, and thus Simp-hLDA has no solid theoretical basis. To tackle the above drawbacks, we explore the use of probabilistic models for such a task where the hierarchical labels are merely viewed as a part of a hierarchy of topics, and the topics of a path in the whole hierarchy generate a corresponding document. Our proposed generative model learns both the latent topics of the underlying data and the labeling strategies in a joint model, by leveraging on the hierarchical structure of labels and Hierarchical Dirichlet Process. We demonstrate the effectiveness of the proposed model on large, real-world datasets in the question answering and website category domains on two tasks: the topic modeling of documents, and the use of the generated topics for document clustering. Our results show that our joint, semi-hierarchical model outperforms the state-of-the-art supervised and unsupervised hierarchical algorithms. The contributions of this paper are threefold: (1) We propose a joint, generative semi-supervised hierarchical topic model, i.e. Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA), to overcome the defects of hLDA and hLLDA while combining the their merits. SSHLDA is able to not only explore new latent topics in the data space, but also makes use of the information from the hierarchy of observed labels; (2) We prove that hLDA and hLLDA are special cases of SSHLDA; (3) We develop a gibbs sampling inference algorithm for the proposed model. The remainder of this paper is organized as follows. We review related work in Section 2. In Section 3, we introduce some preliminaries; while we introduce SSHLDA in Section 4. Section 5 details a gibbs sampling inference algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990) , pLSA (Hofmann, 1999) , LDA (Blei et al., 2003) , Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b) , Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwise correlations are modeled, and the number of parameters in the covariance matrix grows as the square of the number of topics. However, the above models cannot capture the relation between super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) (Blei et al., 2004) , Hierarchical Dirichlet processes (HDP) (Teh et al., 2006) , Pachinko Allocation Model (PAM) (Li and McCallum, 2006) and Hierarchical PAM (HPAM) (Mimno et al., 2007) etc. The relations are usually in the form of a hierarchy, such as the tree or Directed Acyclic Graph (DAG). Blei et al. (2004) proposed the hLDA model that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi-ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b) , Author TM (Rosen-Zvi et al., 2004) , Flat-LDA (Rubin et al., 2011) , Prior-LDA (Rubin et al., 2011) , Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. For hierarchical labeled data, there are also few models that are able to handle the label relations in data. To the best of our knowledge, only hLLDA (Petinot et al., 2011) and HSLDA (Perotte et al., 2011) are proposed for this kind of data. HSLDA cannot obtain a probability distribution for a label. Although hLLDA can obtain a distribution over words for each label, hLLDA is unable to capture the relations between parent and child node using parameters, and it also cannot detect automatically latent topics in the data space. In this paper, we will propose a generative topic model to tackle these problems of hLLDA. Preliminaries The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions (Blei et al., 2004) . It generalizes the Chinese restaurant process (CRP), which is a distribution over partitions. The CRP can be described by the following metaphor. Imagine a restaurant with an infinite number of tables, and imagine customers entering the restaurant in sequence. The d th customer sits at a table accord- The set of paths in the sub-tree whose root is the j th leaf node in the hierarchy of observed topics m A document m that consists of words and labels wm The text of document m, w i is i th words in w cm The topic set of document m co m The set of topics with observed labels for document m ce m The set of topics without labels for document m ce \u2212m The set of latent topics for all documents other than m ze m The assignment of the words in the m th document to one of the latent topics we m The set of the words belonging to one of the latent topics in the the m th document zm,n The assignment of the n th word in the m th document to one of the L available topics z The set of zm,n for all words in all documents c i A topic in the i th level in the hierarchy \u03b8 The word distribution set for Z, i.e., {\u03b8}z\u2208c \u03b1 Dirichlet prior of \u03b8 \u03b4c i The multinomial distribution over the sub-topics of c i\u22121 \u00b5c i Dirichlet prior of \u03b4c i \u03b7 Dirichlet prior of \u03b2 \u03b2 The multinomial distribution of words \u03b8m The distributions over topics for document m \u03b8 The set for \u03b8m, m \u2208 {1, ..., D} ing to the following distribution, p(c d = k|c 1:(d\u22121) ) \u221d { m k if k is previous occupied \u03b3 if k is a new tabel, (1) where m k is the number of previous customers sitting at table k and \u03b3 is a positive scalar. After D customers have sat down, their seating plan describes a partition of D items. In the nested CRP, imagine now that tables are organized in a hierarchy: there is one table at the first level; it is associated with an infinite number of tables at the second level; each second-level table is associated with an infinite number of tables at the third level; and so on until the L th level. Each customer enters at the first level and comes out at the L th level, generating a path with L tables as she sits in each restaurant. Moving from a table at level l to one of its subtables at level l +1, the customer draws following the CRP using Formula (1). In this paper, we will make use of nested CRP to explore latent topics in data space. To elaborate our model, we first define two concepts. If a model can learn a distribution over words for a label, we refer the topic with a corresponding label as a labeled topic. If a model can learn an unseen and latent topic without a label, we refer the 4 The Semi-Supervised Hierarchical Topic Model In this section, we will introduce a semisupervised hierarchical topic model, i.e., the Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA). SSHLDA is a probabilistic graphical model that describes a process for generating a hierarchical labeled document collection. Like hierarchical Labeled LDA (hLLDA) (Petinot et al., 2011) , SSHLDA can incorporate labeled topics into the generative process of documents. On the other hand, like hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004) , SSHLDA can automatically explore latent topic in data space, and extend the existing hierarchy of observed topics. SSHLDA makes use of not only observed topics, but also latent topics. The graphical model of SSHLDA is illustrated in Figure 1 . In the model, N is the number of words in a document, D is the total number of documents in a collection, M is the number of leaf nodes in hierarchical observed nodes, c i is a node in the i th level in the hierarchical tree, \u03b7, \u03b1 and \u00b5 c i are dirichlet prior parameters, \u03b2 k is a distribution over words, \u03b8 is a document-specific distribution over topics, \u03b4 c i is a multinomial distribution over observed sub-topics of topic c i , w is an observed word, z is the topic assigned to w, Dir k (.) is a k-dimensional Dirichlet distribution, T j is a set of paths in the hierarchy of latent topics for j th leaf node in the hierarchy of ob- served topics, \u03b3 is a Multi-nomial distribution over paths in the tree. All notations used in this paper are listed in Table 1 . SSHLDA, as shown in Figure 1 , assumes the following generative process: (1) For each table k \u2208 T in the infinite tree,  (ii) Draw w n from the topic associated with restaurant c z . As the example showed in Figure 2 , we assume that we have known a hierarchy of observed topics: {A1,A2,A17,A3,A4}, and assume the height of the desired topical tree is L = 5. All circled nodes are latent topics, and shaded nodes are observed topics. A possible generative process for a document m can be: It starts from A1, and chooses node A17 at level 2, and then chooses A18, A20 and A25 in the following levels. Thus we obtain a path: c m = {A1, A17, A18, A20, A25}. After getting the path for m, SSHLDA generates each word from one of topics in this set of topics c m . Probabilistic Inference In this section, we describe a Gibbs sampling algorithm for sampling from the posterior and corresponding topics in the SSHLDA model. The Gibbs sampler provides a method for simultaneously exploring the model parameter space (the latent topics of the whole corpus) and the model structure space (L-level trees). In SSHLDA, we sample the paths c m for document m and the per-word level allocations to topics in those paths z m,n . Thus, we approximate the posterior p(c m , z m |\u03b3, \u03b7, w, \u00b5). The hyper-parameter \u03b3 reflects the tendency of the customers in each restaurant to share tables, \u03b7 denotes the expected variance of the underlying topics (e.g., \u03b7 1 will tend to choose topics with fewer high-probability words), \u00b5 c i is the dirichlet prior of \u03b4 c i , and \u00b5 is the set of \u00b5 c i . w m,n denotes the n th word in the m th document; and c m,l represents the restaurant corresponding to the l th -level topic in document m; and z m,n , the assignment of the n th word in the m th document to one of the L available topics. All other variables in the model, \u03b8 and \u03b2, are integrated out. The Gibbs sampler thus assesses the values of z m,n and c m,l . The Gibbs sampler can be divided into two main steps: the sampling of level allocations and the sampling of path assignments. First, given the values of the SSHLDA hidden variables, we sample the c m,l variables which are associated with the CRP prior. Noting that c m is composed of c om and c em , c om is the set of observed topics for document m, and c em is the set of latent topics for document m. The conditional distribution for c m , the L topics associated with document m, is: p(c m |z, w, c \u2212m , \u00b5) =p(c om |\u00b5)p(c em |z em , w em , c e \u2212m ) \u221dp(c om |\u00b5)p(w em |c em , w e \u2212m , z em ) p(c em |c e \u2212m ) (2) where p(c om |\u00b5) = |co m |\u22121 \u220f i=0 p(c i,m |\u00b5 ci ) (3) and p(w em |c em , w e \u2212m , z em ) = |ce m | \u220f l=1 ( \u0393(n . c em,l ,\u2212m + |V |\u03b7) \u220f w \u0393(n w c em,l ,\u2212m + \u03b7) \u00d7 \u220f w \u0393(n w c em,l ,\u2212m + n w c em,l ,m + \u03b7) \u0393(n . c em,l ,\u2212m + n \u2022 c em,l ,m + |V |\u03b7) ) (4) c e \u2212m is the set of latent topics for all documents other than m, z em is the assignment of the words in the m th document to one of the latent topics, and w em is the set of the words belonging to one of the latent topics in the the m th document. n w c em,l ,\u2212m is the number of instances of word w that have been assigned to the topic indexed by c em,l , not including those in the document m. Second, given the current state of the SSHLDA, we sample the z m,n variables of the underlying SSHLDA model as follows: p(z m,n = j|z \u2212(m,n) , w, c m , \u00b5) \u221d n m \u2212n,j + \u03b1 n m \u2212n,. + |c m | \u2022 n wm,n \u2212n,j + \u03b7 wm,n n . \u2212(m,n) + |V | (5) Having obtained the full conditional distribution, the Gibbs sampling algorithm is then straightforward. The z m,n variables are initialized to determine the initial state of the Markov chain. The chain is then run for a number of iterations, each time finding a new state by sampling each z m,n from the distribution specified by Equation ( 5 ). After obtaining individual word assignments z, we can estimate the topic multinomials and the per-document mixing proportions. Specifically, the topic multinomials are estimated as: \u03b2 c m,j ,i = p(w i |z c m,j ) = \u03b7 + n z w i c m,j |V |\u03b7 + \u2211 n . zc m,j (6) while the per-document mixing proportions fixed can be estimated as: \u03b8 m,j = \u03b1 + n m .,j |c m |\u03b1 + n m .,. , j \u2208 1, ..., |c m | (7) Relation to Existing Models In this section, we draw comparisons with the current state-of-the-art models for hierarchical topic modeling (Blei et al., 2004; Petinot et al., 2011) and show that at certain choices of the parameters of our model, these methods fall out as special cases. Our method generalises not only hierarchical Latent Dirichlet Allocation (hLDA), but also Hierarchical Labeled Latent Dirichlet Allocation (hLLDA). Our proposed model provides a unified framework allowing us to model hierarchical labels while to explore new latent topics. Equivalence to hLDA As introduced in Section 2, hLDA is a unsupervised hierarchical topic model. In this case, there are no observed nodes, that is, the corpus has no hierarchical labels. This means c m is equal to c em,m ; meanwhile the factor p(c om,m |\u00b5) is always equal to one because each document has root node, and this allows us to rewrite Formula (2) as: p(c m |z, w, c \u2212m , \u00b5) \u221dp(w cm |c, w \u2212m , z)p(c m |c \u2212m ) (8) which is exactly the same as the conditional distribution for c m , the L topics associated with document m in hLDA model. In this case, our model becomes equivalent to the hLDA model. Equivalence to hLLDA hLLDA is a supervised hierarchical topic model, which means all nodes in hierarchy are observed. In this case, c m is equal to c om,m , and this allows us to rewrite Formula (2) as: p(c m |z, w, c \u2212m , \u00b5) = p(c m |\u00b5) \u221d p(c om |\u00b5) (9) which is exactly the same as the step \" Draw a random path assignment c m \" in the generative process for hLLDA. Consequentially, in this sense our model is equivalent to hLLDA. Experiments We demonstrate the effectiveness of the proposed model on large, real-world datasets in the question answering and website category domains on two tasks: the topic modeling of documents, and the use of the generated topics for document clustering. Datasets To construct comprehensive datasets for our experiments, we crawled data from two websites. First, we crawled nearly all the questions and associated answer pairs (QA pairs) of two top cat- In addition, we first crawled two categories of Open Directory Project (ODP) * : Home and Health. Then, we removed all categories whose number of Web sites is less than 3. Finally, for each of Web sites in categories, we submited the url of each Web site to Google and used the words in the snippet and title of the first returned result to extend the summary of the Web site. We denote the data from the category Home as O Home, and the data from the category Health as O Hlth. The statistics of all datasets are summarized in Table 2. From this table, we can see that these datasets are very diverse: Y Ans has much fewer labels than O Hlth and O Home, but have much more documents for each label; meanwhile the depth of hierarchical tree for O Hlth and O Home can reach level 9 or above. All experiments are based on the results of models with a burn-in of 10000 Gibbs sampling iterations, symmetric priors \u03b1 = 0.1 and free parameter \u03b7 = 1.0; and for \u00b5, we can obtain the estimation of \u00b5 c i by fixed-point iteration (Minka, 2003) . Case Study With topic modeling, the top associated words of topics can be used as good descriptors for topics in a hierarchy (Blei et al., 2003; Blei and McAuliffe, 2010) . We show in Figure 3 We have three major observations from the example: (i) SSHLDA is a unified and generative model, after learning, it can obtain a hierarchy of topics; while Simp-hLDA is a heuristic method, and its result is a mixture of label nodes and topical nodes. For example, Figure 3 (b) shows that the hierarchy includes label nodes and topic nodes, and each of labeled nodes just has a label, but label nodes in Figure 3 (a) have their corresponding topics. (ii) During obtaining a hierarchy, SSHLDA makes use of the information from observed labels, thus it can generate a logical, structual hierarchy with parent-child relations; while Simp-hLDA does not incorporate prior information of labels into its generation process, thus although it can obtain a hierarchy, many parent-child pairs have not parent-child relation. For example, in Figure 3 (b), although label \"root\" is a parent of label \"Computers & Internet\", the topical words of label \"Computers & Internet\" show the topical node is not a child of label \"root\". However, in Figure 3 (a) , label \"root\" and \"Computers & Internet\" has corresponding parent-child relation between their topical words. (iii) In a hierarchy of topics, if a topical node has correspending label, the label can help people understand descendant topical nodes. For example, when we know node \"error files click screen virus\" in Figure 3 (a) has its label \"Computers & Internet\", we can understand the child topic \"hard screen usb power dell\" is about \"computer hardware\". However, in Figure 3 (b), the labels in parent nodes cannot provide much information to understand descendant topical nodes because many label nodes have not corresponding right topical words, such as label \"Computers & Internet\", its topical words, \"the to you and a\", do not reflect the connotation of the label. These observations further confirm that SSHLDA is better than the baseline model. Perplexity Comparison A good topic model should be able to generalize to unseen data. To measure the prediction ability of our model and baselines, we compute the perplexity for each document d in the test sets. Perplexity, which is widely used in the language modeling and topic modeling community, is equivalent algebraically to the inverse of the geometric mean perword likelihood (Blei et al., 2003) . Lower perplexity scores mean better. Our model, SSHLDA, will compare with three state-of-the-art models, i.e. Simp-hLDA, hLDA and hLLDA. Simp-hLDA has been introduced in Section 1, and hLDA and hLLDA has been reviewed in Section 2. We keep 80% of the data collection as the training set and use the remaining collection as the held-out test set. We build the mod-els based on the train set and compute the preplexity of the test set to evaluate the models. Thus, our goal is to achieve lower perplexity score on a held-out test set. The perplexity of M test documents is calculated as: perplexity(Dtest) = exp { \u2212 \u2211 M d=1 \u2211 N d m=1 log p(w dm ) \u2211 M d=1 N d } (10) where D test is the test collection of M documents, N d is document length of document d and w dm is m th word in document d. We present the results over the O Hlth dataset in Figure 4 . We choose top 3-level labels as observed, and assume other labels are not observed, i.e. l = 3. From the figure, we can see that the perplexities of SSHLDA, are lower than that of Simp-hLDA, hLDA and hLLDA at different value of the tree height parameter, i.e. L \u2208 {5, 6, 7, 8}. It shows that the performance of SSHLDA is always better than the state-of-the-art baselines, and means that our proposed model can model the hierarchical labeled data better than the state-of-the-art models. We can also obtain similar experimental results over Y Ans and O Home datasets, and their detailed description is not included in this paper due to the limitation of space. Clustering performance To evaluate indirectly the performance of the proposed model, we compare the clustering performance of following systems: 1) the proposed model; 2) Simp-hLDA; 3) hLDA; 4) agglomerative clustering algorithm. There are many agglomerative clustering algorithms, and in this paper, we make use of the single-linkage method in a software package called CLUTO (Karypis, 2005) to obtain hierarchies of clusters over our datasets, with words as features. We refer the method as h-clustering. Given a document collection DS with a H-level hierarchy of labels, each label in the hierarchy and corresponding documents will be taken as the ground truth of clustering algorithms. The hierarchy of labels denoted as GT-tree. The process of evaluation is as follows. First, we choose top l-level labels in GT-tree as an observed hierarchy, i.e. Base Tree (BT), and we need to construct a L-level hierarchy (l < L <= H) over the documents DS using a model. The remaining labels in GT-tree and corresponding documents are the ground truth classes, each class denoted as C i . Then, (i) for h-clustering, we run single-linkage method over the documents DS. (ii) for Simp-hLDA, hLDA runs on the documents in each leaf-node in BT, and the height parameter is (L \u2212 l) for each hLDA. After training, each document is assigned to top-1 topic according to the distribution over topics for the document. Each topic and corresponding documents forms a new cluster. (iii) for hLDA, hLDA runs on all documents in DS, and the height parameter is L. Similar to Simp-hLDA, each document is assigned to top-1 topic. Each topic and corresponding documents forms a new cluster. (iv) for SSHLDA, we set height parameter as L. After training, each document is also assigned to top-1 topic. Topics and their corresponding documents form a hierarchy of clusters. Evaluation Metrics For each dataset we obtain corresponding clusters using the various models described in previous sections. Thus we can use clustering metrics to measure the quality of various algorithms by using a measure that takes into account the overall set of clusters that are represented in the new generated part of a hierarchical tree. One such measure is the FScore measure, intro-duced by (Manning et al., 2008) . Given a particular class C r of size n r and a particular cluster S i of size n i , suppose n ri documents in the cluster S i belong to C r , then the FScore of this class and cluster is defined to be F (C r , S i ) = 2 \u00d7 R(C r , S i ) \u00d7 P (C r , S i ) R(C r , S i ) + P (C r , S i ) (11) where R(C r , S i ) is the recall value defined as n ri /n r , and P (C r , S i ) is the precision value defined as n ri /n i for the class C r and the cluster S i . The FScore of the class C r , is the maximum FScore value attained at any node in the hierarchical clustering tree T . That is, F (C r ) = max Si\u2208T F (C r , S i ). ( 12 ) The FScore of the entire clustering solution is then defined to be the sum of the individual class FScore weighted according to the class size. F Score = c \u2211 r=1 n r n F (C r ), (13) where c is the total number of classes. In general, the higher the FScore values, the better the clustering solution is. Experimental Results Each of hLDA, Simp-hLDA and SSHLDA needs a parameter-the height of the topical tree, i.e. L; and for Simp-hLDA and SSHLDA, they need another parameter-the height of the hierarchical observed labels, i.e l. The h-clustering does not have any height parameters, thus its FScore will keep the same values at different height of the topical tree. With choosing the height of hierarchical labels for O Home as 4, i.e. l = 4, the results of our model and baselines with respect to the height of a hierarchy are shown in Figure 5 . From the figure, we can see that our proposed model can achieve consistent improvement over the baseline models at different height, i.e. L \u2208 {5, 6, 7, 8}. For example, the performance of SSHLDA can reach 0.396 at height 5 while the hclustering, hLDA and hLLDA only achieve 0.295, 0.328 and 0.349 at the same height. The result shows that our model can achieve about 34.2%, 20.7% and 13.5% improvements over h-clustering, hLDA and hLLDA at height 5. The improvements are significant by t-test at the 95% significance level. We can also obtain similar experimental results over Y Ans and O Hlth. However, for the same reason of limitation of space, their detailed descriptions are skipped in this paper. Conclusion and Future work In this paper, we have proposed a semi-supervised hierarchical topic models, i.e. SSHLDA, which aims to solve the drawbacks of hLDA and hLLDA while combine their merits. Specially, SSHLDA incorporates the information of labels into generative process of topic modeling while exploring latent topics in data space. In addition, we have also proved that hLDA and hLLDA are special cases of SSHLDA. We have conducted experiments on the Yahoo! Answers and ODP datasets, and assessed the performance in terms of Perplexity and FScore measure. The experimental results show that the prediction ability of SSHLDA is the best, and SSHLDA can also achieve significant improvement over the baselines on Fscore measure. In the future, we will continue to explore novel topic models for hierarchical labeled data to further improve the effectiveness; meanwhile we will also apply SSHLDA to other media forms, such as image, to solve related problems in these areas. Acknowledgments This work was partially supported by NSFC with Grant No.61073082, 60933004, 70903008 and NExT Search Centre, which is supported by the Singapore National Research Foundation & Interactive Digital Media R&D Program Office, MDA under research grant (WBS:R-252-300-001-490)."
}