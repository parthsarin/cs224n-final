{
    "article": "Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing promptlearning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc., need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present Open-Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. Open-Prompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. 1 Introduction Pre-trained language models (PLMs) (Han et al., 2021a; Qiu et al., 2020) have been widely proven to be effective in natural language understanding and generation, ushering in a new era of modern natural language processing (NLP). In the early stage of this revolution, a standard approach to adapt PLMs to various specific NLP tasks is the pretraining-finetuning paradigm, where additional parameters and task-specific objectives are introduced in the tuning procedure. However recently, the paradigm of the adaptation of PLMs is shifting. Originated in T5 (Raffel et al., 2019) and GPT-3 (Brown et al., 2020) , researchers find that PLMs can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Take a simple prompt-based sentiment classification for example, the pipeline consists of a template and a verbalizer, where a template is used to process the original text with some extra tokens, and a verbalizer projects original labels to words in the vocabulary for final prediction. Assume the template is \"<text> It is <mask>\", where the token <text> stands for the original text, and the verbalizer is {\"positive\":\"great\", \"negative\":\"terrible\"}. The sentence \"Albert Einstein was one of the greatest intellects of his time.\" will first be wrapped by the pre-defined template as \"Albert Einstein was one of the greatest intellects of his time. It is <mask>\". The wrapped sentence is then tokenized and fed into a PLM to predict the distribution over vocabulary on the <mask> token position. It is expected that the word great should have a larger probability than terrible. As illustrated above, prompt-learning projects the downstream tasks to pre-training objectives for PLMs with the help of textual or softencoding prompts. A series of studies of promptlearning (Liu et al., 2021a) have been proposed to investigate the strategies of constructing templates (Schick and Sch\u00fctze, 2021; Gao et al., 2021; Liu et al., 2021b) , verbalizers (Hu et al., 2021) , optimization (Lester et al., 2021) , and application (Li and Liang, 2021; Han et al., 2021b; Ding et al., 2021a) for this paradigm. A prompt-learning problem could be regarded as a synthesis of PLMs, human prior knowledge, and specific NLP tasks that need to be handled. (Gao et al., 2021) Table 1 : Some examples implemented by OpenPrompt, where M. is the abbreviation of manually defined and A. is the abbreviation of automatically generated. Note that different approaches focus on different parts in promptlearning. Additional to the whole pipeline, our specific implementations of these methods are integrated into the specific classes of OpenPrompt. Hence, it is hard to support the particular implementations of prompt-learning elegantly with the current deep learning or NLP libraries while there is also a lack of a standard paradigm. Previous works pursue the most efficient way to implement promptlearning with the least modification to the existing framework for traditional fine-tuning, resulting in poor readability and even unstable reproducibility. Moreover, the performance of a prompt-learning pipeline varies greatly with the choice of templates and verbalizers (Zhao et al., 2021) , creating more barriers for implementations. Lastly, there is no comprehensive open-source framework particularly designed for prompt-learning at present, which makes it difficult to try out new methods and make rigorous comparisons for previous approaches. We present OpenPrompt, an open-source, easyto-use, and extensible toolkit for prompt-learning. OpenPrompt modularizes the whole framework of prompt-learning and considers the interactions between each module. We highlight the feature of combinability of OpenPrompt, which supports flexible combinations of diverse task formats, PLMs, and prompting modules. For example, we can easily adapt prefix-tuning (Li and Liang, 2021) to a text classification task in OpenPrompt. This feature enables users to assess the generalization of their prompt-learning models on various tasks, but not only the performance on specific tasks. Specifically, a Template class is used to define or generate textual or soft-encoding templates to wrap the original input. To flexibly support various templates under a unified paradigm, we design a new template language that could easily conduct token-level customization for the corresponding attributes. A Verbalizer projects the classification labels to words in the vocabulary, and a PromptModel is responsible for the training and inference process. Each module in OpenPrompt is clearly defined while retaining its independence and coupling so that researchers can easily deploy a model and make targeted improvements. We also implement baselines with OpenPrompt and evaluate them on a broad scope of NLP tasks, demonstrating the effectiveness of OpenPrompt. The area of prompt-learning is in the exploratory stage with rapid development. Hopefully, Open-Prompt could help beginners quickly understand prompt-learning, enable researchers to efficiently deploy prompt-learning research pipeline, and empower engineers to readily apply prompt-learning to practical NLP systems to solve real-world problems. OpenPrompt will not only keep all the code open source, but will also continue to update the documentation to provide detailed tutorials. Design and Implementation As stated in \u00a7 1, prompt-learning is a comprehensive process that combines PLMs, human knowledge, and specific NLP tasks. Keeping that in mind, the design philosophy is to simultaneously consider the independence and mutual coupling of each module. As illustrated in Figure 1 , OpenPrompt provides the full life-cycle of prompt-learning based on PyTorch (Paszke et al., 2019) . In this section, we first introduce the combinability of OpenPrompt, and then the detailed design and implementation of each component in OpenPrompt. Combinability In the NLP world, we usually adopt different PLMs with corresponding objective functions to different underlying tasks (roughly, classification and generation). But in prompt learning, given that the core idea of the framework is to mimic pre-training tasks in the downstream task, which are essentially \"predicting words based on context\", we can further unify the execution of downstream tasks. Open- Prompt-related Class: These classes are unique modules for prompt-learning, and they can be implemented by users. \u2f2f\u5177\u5305\u8bbe\u8ba1\u56fe Dataset-related Class: These classes support the uAliAes for datasets across different NLP tasks. wrapped example Figure 1 : The overall architecture of OpenPrompt. Note that according to the prompt-learning strategies, not all the modules are necessarily used. For example, in generation tasks, there are no verbalizers in the learning procedure. The PromptTrainer is a controller that controls the data flow and the training process with some unique attributes, users can also implement the training process in a conventional fashion. Prompt supports a combination of tasks, PLMs, and prompt modules in a flexible way. For example, from a model perspective, T5 (Raffel et al., 2019) is not only used for span prediction and GPT (Brown et al., 2020) is not only used for generative tasks. From the perspective of prompting, prefix-tuning can also be used for classification, and soft prompt can be used for generation. All these combinations can easily be implemented and validated on NLP tasks in our framework so that we can better understand the mechanisms involved. Pre-trained Language Models One core idea of prompt-learning is to use additional context with masked tokens to imitate the pre-training objectives of PLMs and better stimulate these models. Hence, the choice of PLMs is crucial to the whole pipeline of prompt-learning. PLMs could be roughly divided into three groups according to their pre-training objectives. The first group of PLMs use masked language modeling (MLM) to reconstruct a sequence corrupted by random masked tokens, where only the losses of the masked tokens are computed. Typical PLMs with MLM objective include BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , etc, and such an objective is regarded suitable for natural language understanding (NLU). The second group exploits the autoregressive-style language model-ing (LM) to predict the current token according to its leading tokens. GPT-3 (Brown et al., 2020) is one of the representative works adopting this objective. The third part is the sequence-to-sequence (Seq2Seq) models, which aim to generate a sequence with a decoder conditioned on a separate encoder for an input sequence. Typical seq2seq PLMs include T5 (Raffel et al., 2020) , MASS (Song et al., 2019) and BART (Lewis et al., 2020) , etc. Different PLMs have different attributes, resulting in various adaptation capabilities for different NLP tasks in prompt-learning. Practically in Open-Prompt, we support directly loading PLMs from huggingface transformers (Wolf et al., 2020) , and PLMs implemented by other libraries will be supported in the future. Once the PLM is determined, researchers could deploy a known valid promptlearning pipeline (e.g., RoBERTa for few-shot sentiment classification) or explore other uses of PLM that could exploit its potential. Users of Open-Prompt do not need to implement objective heads for different PLMs to calculate the corresponding loss, a unified interface can perform these operations automatically ( \u00a7 2.6). Tokenization Tokenization is a crucial step in processing data for NLP, and it faces new challenges in promptlearning. After designing the template, the spe-cific implementation of the tokenization for original input and the designed template could be time-consuming and error-prone. First, in promptlearning, some specific information such as the indices of entities and masked tokens should be carefully tackled in tokenization. Some small errors, such as the mismatch of masked token indices, may lead to serious consequences. Moreover, concatenation and truncation issues after tokenization (templates are not supposed to be truncated) should also be handled. Since different PLMs may have different tokenization strategies, we should also consider the inconsistency in the details of additional context processing. We specifically design the tokenization module for prompt-learning and significantly simplify the process. By using our encapsulated data processing APIs, users could use the human-readable style to design templates and operate on the input and the template at the same time. Our component integrates complex information from input and template and then conducts tokenization. Based on the choice of PLMs, OpenPrompt automatically chooses the appropriate tokenizer in prompt-learning, which could save considerable time for users to process prompt-related data. Templates As one of the central parts of prompt-learning, a template module wraps the original text with the textual or soft-encoding template. A template normally contains contextual tokens (textual or soft) and masked tokens. In OpenPrompt, all the templates are inherited from a common base class with universal attributes and abstract methods. Previous works design a wide variety of templates, including manually written template (Schick and Sch\u00fctze, 2021) and pure soft template (Lester et al., 2021) . Gu et al. (2021) report a mix of manual template tokens and soft (trainable) tokens sometimes yields better results than separate manual template and soft template. In Liu et al. (2021b) , a promising performance is achieved by fixing the majority of manual tokens while tuning a small number of the others. In Han et al. (2021b) , the template is contextualized, which needs to be filled with the head entity and the tail entity to form a complete one, moreover, the output of multiple positions is used in the loss calculation in their template. Logan IV et al. ( 2021 ) design null template with simple concatenation of the inputs and an appended <mask> token. It's not reasonable to design a template format for each prompt since it will require high learning cost for practical use. To this end, in OpenPrompt, we design a template language to ease the problem, with which we can construct various types of templates under a unified paradigm. Our template language takes insight from the dict grammer of Python. And such a design ensures flexibility and clarity at the same time, allowing users to build different prompts with relative ease. More specifically, a template node is a text (or empty text) with an attributes' description. In our template language, one is free to edit the attributes of each token in the template, such as which characters are shared embedding, how the characters are post-processed (e.g. by MLP), etc. We show some template examples in Figure 2 , and the detailed tutorial for writing templates is in the documentation 2 . Verbalizers When it comes to prompt-based classification, a verbalizer class should be constructed to map original labels to label words in the vocabulary. When a PLM predicts a probability distribution over the vocabulary for one masked position, a verbalizer will extract the logits of label words and integrate the logits of label words to the corresponding class, thereby responsible for the loss calculation. Figure 3 shows a simple way to define a binary sentiment classification verbalizer. Similar to templates, all the verbalizer classes are also inherited from a common base class with necessary attributes and abstract methods. Additional to manually-defined verbalizers, we implement automatic verbalizers like AutomaticVerbalizer and KnowledgeableVerbalizer (Hu et al., 2021) . Moreover, important operations like calibrations (Zhao et al., 2021) are also realized in OpenPrompt. Prompt-learning could also facilitate the unification of NLP tasks. In such kind of paradigm, a span of text (i.e., the target text) is expected to be generated in the masked position. Then the final prediction will be based on a mapping from the target texts to the labels (Ye et al., 2021; Du et al., 2021) . To fully support such a paradigm, we implement a novel GenerationVerbalizer, which supports designating any kind of text, including a piece of text from the input, as the target text. To compose a target text for a 1 # Example A. Hard prompt for topic classification 2 a {\"mask\"} news: {\"meta\": \"title\"} {\"meta\": \"description\"} 3 4 # Example B. Hard prompt for entity typing 5 {\"meta\": \"sentence\"}. In this sentence, {\"meta\": \"entity\"} is a {\"mask\"}, 6 7 # Example C. Soft prompt (initialized by textual tokens) 8 {\"meta\": \"premise\"} {\"meta\": \"hypothesis\"} {\"soft\": \"Does the first sentence entails the second ?\"} {\"mask\"} {\"soft\"}. 9 10 # Example D. Pure soft template in Lester et al., 2021. 11 {\"soft\": None, \"duplicate\": 100} {\"meta\": \"text\"} {\"mask\"} 12 13 # Example E. Post processing script support 14 # e.g. write an lambda expression to strip the final punctuation in data 15 {\"meta\": \"context\", \"post_processing\": lambda s: s.rstrip(string.punctuation)}. {\" soft\": \"It was\"} {\"mask\"} 16 17 # Example F. Mixed prompt with two shared soft tokens 18 {\"meta\": \"premise\"} {\"meta\": \"hypothesis\"} {\"soft\": \"Does\"} {\"soft\": \"the\", \" soft_id\": 1} first sentence entails {\"soft_id\": 1} second? 19 20 # Example G. Specify the title should not be truncated 21 a {\"mask\"} news: {\"meta\": \"title\", \"shortenable\": False} {\"meta\": \"description\"} Figure 2 : Some examples of our template language. In our template language, we can use the key \"meta\" to refer the original input text (Example B), parts of the original input (Example A, C, G), or other key information. We can also freely specify which tokens are hard and which are soft (and their initialization strategy). We could assign an id for a soft token to specify which tokens are sharing embeddings (Example F). OpenPrompt also supports the post processing (Example E) for each token, e.g., lambda expression or MLP. 1 from openprompt import ManualVerbalizer GenerationVerbalizer, the syntax is the same as the template language (See Figure 4 ). Different evaluation metrics are then used for different types of task, For example, exact match for classification tasks and BLEU score (Papineni et al., 2002) for generation tasks. PromptModel In OpenPrompt, we use a PromptModel object to be responsible for training and inference, which contains a PLM, a Template object, and a Verbalizer object (optional). Users could flexibly combine these modules and define advanced interactions among them. A model-agnostic forward method is implemented in the base class to predict words for the masked positions. One goal of this module is that users do not need to specifically implement heads for different PLMs, but use a unified API to \"predict words for positions that need to be predicted\" regardless of the pre-training objective. An example to define a PromptModel is shown in Figure 6 . Training From the perspective of trainable parameters, the training of prompt-learning could be divided into two types of strategies. The first strategy simulta-  neously tunes the prompts and the PLM, which is verified to be effective in a low-data regime (OpenPrompt also provides a FewshotSampler to support the few-shot learning scenario). The second strategy is to only train the parameters of prompts and keep the PLM frozen, this is regarded as a parameter-efficient tuning method and is considered as a promising way to stimulate super-large PLMs. Both of these strategies can be called with one click in the trainer (or runner) module of Open-Prompt. Trainer modules in OpenPrompt implement training process accompanied with promptoriented training tricks, e.g. the ensemble of templates. Meanwhile, OpenPrompt supports experimentation through configuration to easily drive large-scale empirical study. We provide several complete tutorials 3 to use the basic and advanced attributes of OpenPrompt. Evaluation OpenPrompt aims to support a broad set of NLP tasks under the paradigm of prompt-learning. In terms of evaluation, we use OpenPrompt to implement various baselines and assess them on the corresponding NLP tasks. We show the validation space in Figure 5 . And the evaluation tasks include WebNLG (Gardent et al., 2017) for conditional generation, GLUE (Wang et al., 2018) and Super-GLUE (Wang et al., 2019) for natural language understanding; SemEval (Hendrickx et al., 2010) , Few-NERD (Ding et al., 2021b) for information extraction; MNLI (Williams et al., 2017 ), AG's News (Zhang et al., 2015) , DBPedia (Lehmann et al., 2015) and IMDB (Maas et al., 2011) for text classification; LAMA (Petroni et al., 2019) for knowledge probing. The processors of these datasets have already been implemented in Open-Prompt, and they are all inherited from a common base DataProcessor class. To keep the results up to date, we are constantly updating and reporting the latest results on our GitHub repository 4 . Discussion Although PLMs have achieved tremendous success on almost all the subtasks in NLP, one problem still hangs in the air, have we really fully exploited the potential of PLMs, especially the big ones? Conventional fine-tuning uses extra task-specific heads and objectives for adaptation, but this strategy may face two issues. On the one hand, such an approach creates a natural gap between model tuning and pre-training. On the other hand, as the number of model parameters increases, this finetuning approach becomes increasingly difficult to operate due to the massive computational volume (e.g., GPT-3 (Brown et al., 2020)) . By mimicking the process of pre-training, prompt-learning intuitively bridges the gap between pre-training and model tuning. Practically, this paradigm is surprisingly effective in low-data regime (Le Scao and Rush, 2021; Gao et al., 2021) . For example, with appropriate template, zero-shot prompt-learning could even outperform 32-shot fine-tuning (Ding et al., 2021a) . Another promising empirical attribute of prompt-learning is the potential to stimulate large-scale PLMs. When it comes to a 10B model, solely optimizing prompts (the parameters of the model are fixed) could achieve comparable performance to full parameter finetuning (Lester et al., 2021) . These practical studies imply that we may use prompts to more effectively and efficiently dig the knowledge kept in PLMs, leading to a deeper understanding of the underlying principles of their mechanisms (Wei et al., 2021; Qin et al., 2021; Vu et al., 2021) . In addition to prompt-based methods, there are also other techniques exploring the parameter-efficient stimulation of large-scale PLMs (Houlsby et al., 2019; Hu et al., 2022; He et al., 2022; Ding et al., 2022) . Although it is possible to achieve non-trivial results on the large-scale PLMs by just adjusting the prompt. However, in small and medium-sized models, prompt still faces optimization problems that need to be addressed. From a practical implementation point of view, prompt-learning is actually complex and requires a lot of detailed consideration. With general-purpose NLP under the prompt-learning paradigm as our target, we present OpenPrompt, a unified toolkit to effectively and efficiently implement promptlearning approaches. OpenPrompt demonstrates a comprehensive view of the programming details of prompt-learning, and enables practitioners to quickly understand the mechanisms and practical attributes of this technique. And one can quickly deploy existing representative promptlearning algorithms that are already implemented in the package under a unified programming framework. Moreover, OpenPrompt allows researchers or developers to quickly try out new ideas of prompt-learning, which not only includes newly designed templates or verbalizers, but also the exploration of the attributes of prompt-learning, e.g., prompt-based adversarial attacking. Conclusion and Future Work We propose OpenPrompt, a unified, easy-to-use, and extensible toolkit for prompt-learning. Open-Prompt establishes a unified framework with clearly defined blocks and flexible interactions to support solid research on prompt-learning. At the application level, OpenPrompt could facilitate researchers and developers to effectively and efficiently deploy prompt-learning pipelines. In the future, we will continue to integrate new techniques and features to OpenPrompt to facilitate the research progress of prompt-learning. Focusing on organizing input and output and training processes, Openprompt will be easily combined with tools that focus on specific optimization execution processes in the future. Acknowledgements Contributions Zhiyuan Liu, Ning Ding and Hai-Tao Zheng initiated and led the project. Ning Ding and Shengding Hu designed the original working flow and APIs. Shengding Hu, Weilin Zhao, Yulin Chen, Ning Ding developed basic classes and advanced attributes of OpenPrompt, as well as the tutorials. Ning Ding and Shengding Hu drafted the documentation and the paper. Zhiyuan Liu, Hai-Tao Zheng and Maosong Sun gave suggestions and feedback about the organization of the project.",
    "abstract": "Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing promptlearning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc., need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present Open-Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. Open-Prompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. 1",
    "countries": [
        "China"
    ],
    "languages": [
        "Ding"
    ],
    "numcitedby": "22",
    "year": "2022",
    "month": "May",
    "title": "{O}pen{P}rompt: An Open-source Framework for Prompt-learning"
}