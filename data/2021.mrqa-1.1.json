{
    "article": "In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa (Conneau et al., 2019)   achieves the best results, except for English. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the model on simple word changes. We publicly release our dataset 1 , model 2 and training script 3 . Introduction Organizations create Frequently Asked Questions (FAQ) pages on their website to provide a better service to their users. FAQs are also useful to automatically answer the most frequent questions on different communication channels: email, chatbot, or search bar. FAQ retrieval is the task of locating the right answer within a collection of candidate question and answer pairs. It is closely related to the tasks of non-factoid QA and community QA, although it has its own specificities. The total number of possible answers is generally small (the average FAQ page on the web has 6 answers), and only one is correct. Retrieval systems cannot rely on named entities, as they are typically shared by many possible answers. For example, three out of four answers in queries are matched against pairs of questions and answers, as opposed to passages for non-factoid QA. Since FAQ-Finder (Hammond et al., 1995) , researchers applied different methods to the task of FAQ retrieval (Sneiders, 1999; Jijkoun and de Rijke, 2005; Riezler et al., 2007; Karan and \u0160najder, 2016; Sakata et al., 2019) . However, since the advent of deep-learning and Transformers, the interest has somewhat faded compared to other areas of QA (Rogers et al., 2021) . One possible explanation is the lack of a dedicated large-scale dataset. The ones available are mostly limited to English, and domain-specific. On the other hand, the task of factoid question answering received the attention of many researchers. Recently, Transformers encoders such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) have been successfully applied to the retrieval part of factoid QA, overcoming strong baselines such as TF-IDF and BM25. However, we show that DPR's performance on passage retrieval is not directly transferable to FAQ retrieval. Lewis et al. (2021) recently released PAQ, a dataset of 65M pairs of Probably Asked Questions. However, answers are typically short in PAQ (a few words), which differs from FAQs where answers are longer than questions. Another way to answer users' questions is to use Knowledge Grounded Conversation models as it does not require the pre-generation of all possible pairs of questions and answers (Komeili et al., 2021; De Bruyn et al., 2020) . However, at the time of writing these models can hallucinate knowledge (Shuster et al., 2021) , which limits their attractiveness in a corporate environment. In this paper, we provide the first multilingual dataset of FAQs. We collected around 6M FAQ pairs from the web in 21 different languages. This is significantly larger than existing datasets. However, collecting data from the web brings its own challenges: duplication of content and uneven distribution of topics. We also provide the first multilingual FAQ retriever. We show that models trained on all languages at once outperform monolingual models (except for English). The remainder of the paper is organized as follows. We first review the existing models and datasets available for the task of FAQ retrieval. We then present our own dataset and apply different models to it. We finally perform some analysis on the results and conclude. Our dataset and model are available on the HuggingFace Hub 4 . Related Work In this section, we review the existing literature on FAQ retrieval. We first start by reviewing available models and then look at the available datasets. Models Since the release of FAQ-Finder (Hammond et al., 1995; Burke et al., 1997) and Auto-FAQ (Whitehead, 1995), several methods have been presented. We grouped them into three categories: lexical, unsupervised, and supervised. Lexical FAQ-Finder (Hammond et al., 1995; Burke et al., 1997) matches user queries to FAQ questions of the Usenet dataset using Term Frequency-Inverse Document Frequency (TF-IDF). The system tries to bridge the lexical gap between users' queries and FAQ pairs by using the semantic network WordNet (Miller, 1995) to establish correlations between related terms. FAQ-Finder assumes that the question half of the QA pair is the most 4 dataset, model and training script relevant for matching to a new query. Tomuro and Lytinen (2004) improved upon FAQ-Finder by including the other half of the QA pair (the answer). Xie et al. (2020) uses a knowledge graph-based QA framework that considers entities and triples in texts as knowledge anchors. This approach requires the customization of a knowledge graph, which is labor-intensive and domain-specific. Sneiders (1999) used a rule-based technique called Prioritized Keyword Matching on top of a traditional TF-IDF approach. The use of shallow language understanding means that the matching is based on keyword comparison. Each FAQ entry must be manually annotated with a set of required and optional keywords. Sneiders (2002a Sneiders ( ,b, 2009 Sneiders ( , 2010) ) brings further developments on the idea. Moreo et al. (2013) proposes an approach based on semi-automatic generation of regular expression for matching queries with answers. Yang (2009) integrates a domain ontology, user modeling, and a template-based approach to tackle this problem. Unsupervised Kim and Seo (2008, 2006) presented a clustering-based method of previous user queries to retrieve the right FAQ pair. The authors used a Latent Semantic Analysis (LSA) method to overcome the lexical mismatch between related queries. Jijkoun and de Rijke (2005) experimented with several combinations of TF-IDF retrievers based on the indexing of different fields (question, answer, with or without stop words, the full text of the page). Riezler et al. (2007) extended this method by incorporating a translation-based query expansion, as initially investigated in Berger et al. (2000) . Supervised Moschitti et al. (2007) proposed an approach based on tree kernels. Tree kernels can be defined as similarity metrics that compare a query to an FAQ pair by parsing both texts and calculating the similarity based on the resulting parse trees. Semantic word similarity can also be added to the computation. Filice et al. (2016) expanded on this method and achieved first place in the Community QA shared task at SemEval 2015 (Nakov et al., 2015) . Sakata et al. (2019) were the first to use BERTbased models (Devlin et al., 2018) for the specific task of FAQ retrieval. The relevance between the query and the answers is learned with a fine-tuned BERT model which outputs probability scores for a pair of (query, answer). The scores are then combined using a specific method. Mass et al. (2020) also used a BERT model. Their method is based on an initial retrieval of FAQ candidates followed by three re-rankers. De Bruyn et al. ( 2021 ) used a ConveRT (Henderson et al., 2019) model to automatically answer FAQ questions in Dutch. Datasets In this section, we review the different datasets publicly available. FAQ retrieval datasets can be evaluated on four axes: source of data (community or organizational), the existence of user queries (paraphrases), domain, and language. See Table 2 for an overview. Faq-Finder (Hammond et al., 1995; Burke et al., 1997) used a dataset collected from Usenet news groups. FAQs were created on several topics so that newcomers do not ask the same questions again and again. This dataset is multi-domain. More recently, Karan and \u0160najder (2016) released the FAQIR dataset. It was collected from the \"maintenance & repairs\" section of the QA website Yahoo! Answers. The StackFAQ (Karan and \u0160najder, 2018) dataset was collected from the \"web apps\" sections of StackExchange. Feng et al. (2015) collected a QA dataset from the insurancelibrary.com website where a community of insurance expert reply to users' questions. Several authors (for example Filice et al., 2016 ) also rely on Sem-Eval 2015 Task 3 (Nakov et al., 2015) on Answer Selection in Community Question Answering. It contains pairs of questions and answers in English and Arabic. There exist few publicly available datasets for organizational FAQs. OrgFAQ (Lev et al., 2020) is a notable exception. At the time of writing, it is not yet publicly available. Multilingual FAQ dataset In this section, we introduce our new multilingual FAQ dataset. Data collection Instead of implementing our own web crawler, we used the Common Crawl: a non-profit organization which provides an open repository of the web. 5 Common Crawl's complete web archive consists of petabytes of data collected over 10 years of web crawling (Ortiz Su\u00e1rez et al., 2020) . The repository is organized in monthly bucket of crawled data. Web pages are saved in three different formats: WARC files for the raw HTML data, WAT files for the metadata, and WET files for the plain text extracts. For our purposes, we used WARC files as we are interested in the raw HTML data. Similar to Lev et al. (2020) , we looked for JSON-LD 6 tags containing an FAQPage item. Web developers use this tag to make it easy for search engines to parse FAQs from a web page. 7 The language of each FAQ pair is determined with fastText (Joulin et al., 2016) . We also apply some filtering to remove unwanted noise. 8 Using this method, we collected 155M FAQ pairs from 24M different pages. Deduplication A common issue with datasets collected from the web is the redundancy of data (Lee et al., 2021) . For example, hotel pages on TripAdvisor typically have an FAQ pair referring to shuttle services from the airport to the hotel. 9 The only changing term is the name of the hotel. Algorithms such as SimHash (Charikar, 2002) and MinHash (Broder, 1997) can detect such duplicates. MinHash is an approximate matching algorithm widely used in large-scale deduplication tasks (Lee et al., 2021; Versley and Panchenko, 2012; Gabriel et al., 2018; Gyawali et al., 2020) . The main idea of MinHash is to efficiently estimate the Jaccard similarity between two documents, represented by their set of n-grams. Because of the sparse nature of n-grams, computing the full Jaccard similarity between all documents is prohibitive. MinHash alleviates this issue by reducing each document to a fixed-length hash which can be used to efficiently approximate the Jaccard similarity between two documents. MinHash has the additional property that similar documents will have similar hashes, we can then use Locality Sensitive Hashing (LSH) (Leskovec et al., 2014) to efficiently retrieve similar documents. In our experiments, we represented each page as a set of 3 consecutive tokens (n-grams). We worked with a document signature length of 100, and 20 bands with 5 rows as parameters for LSH. 6 JavaScript Object Notation for Linked Data 7 More information on FAQPage markup 8 Questions need to contain a question mark (including the Arabic question mark) to avoid keyword questions. Question and answer cannot start with a \"<\", \"{\", or \"[\" to remove \"code like\" data. 9 Does Ritz Paris have an airport shuttle? Does Four Seasons Hotel George V have an airport shuttle? These parameters ensure a 99.6% probability that documents with a Jaccard similarity of 0.75 will be identified. We subsequently compute the true Jaccard similarity for all matches. We follow the approach of NearDup (Lee et al., 2021) and subsequently create a graph of documents. Each node on the graph is an FAQ page, and they share an edge if their true Jaccard similarity is larger than 0.75. We then compute all the independent sub-graphs, each representing a graph of duplicated pages. We only keep one page per sub-graph. Using this method, we trimmed the number of FAQ pages from 24M to 1M. Description After deduplication, our dataset contains around 6M FAQ pairs coming from 1M different web pages, spread on 26K root web domains. 10 This is significantly bigger than other FAQ datasets publicly available at the time of writing (see Table 2 for comparison). Our dataset is composed of pairs of FAQs grouped by language and source page (URL). We collected data in 21 different languages. 11 The most common one is English, with 58% of the FAQ pairs, followed by German and Spanish with 13% and 8% respectively. Training and validation sets For a given language, the target size of the validation set is equal to 10% of the total number of pairs. However, two features of our dataset call for a more fine-grained approach. Root domain distribution Even though we deduplicated the dataset, FAQ pages tend to originate from the same root domain. As an example, kayak (kayak.com, kayak.es, etc.) is the largest contributor to the dataset. While this is not a problem for the training set (one can always restrict the number of pages per domain), it is an issue for the validation set, as we want to assess the quality of the model on a broad set of topics. Having several large root domain contributors skews the dataset to these topics. We make the simplifying assumption that different web domains have different topics of interest. Research on the true topic distribution is left for future work. We artificially increased the topic breadth of the validation set by restricting the contribution of each root domain. In the validation set, a single root domain can only contribute up to 3 FAQ pages. This method reduces the contribution of the largest domain from 21% in the training set to 3% in the validation set. Furthermore, we make sure there is no overlap of root domain between the training and validation set. 12 Pairs per page concentration The distribution of the number of pairs per page is highly uneven (see Figure 1 ). Around 50% of the pages have 5 or fewer pairs per page. Intuitively, we prefer pages with a higher number of FAQs as it is harder to pick the right answers amongst 100 candidates than 5. We thus artificially increased the Figure 1 : Bucketing of our dataset according to the number of FAQs per page. To make the validation set more challenging, we started by selecting pages with a higher number of pairs. difficulty of the validation by first selecting pages with a higher number of FAQ pairs per page. See Figure 1 for a comparison between the training and validations set. Cross-lingual leakage The fact that our dataset is multilingual can lead to issues of cross-lingual leakage. Having pages from expedia.fr in the training set, and pages from expedia.es in the validation set can overstate the performance of the models. We avoid such problems by restricting root domains in the validation associated with only one language (e.g. expedia would be excluded from the validation set because it is associated with French and Spanish pages). Models In this section, we describe the FAQ retrieval models used in our experiments. Let P be the set of all user queries and F = {(q 1 , a 1 ), ..., (q n , a n )} be the set of all FAQ pairs for a given domain. An FAQ retrieval model takes as input a user's query p i \u2208 P and an FAQ pair f j \u2208 F , and outputs a relevance score h(p i , f j ) for f j with respect to p i . However, our dataset does not contain live user queries (or paraphrases) P , we thus use questions q as queries P = {q 1 , ..., q n } and restrict the FAQ set to the answers F = {a 1 , ..., a n }. The task becomes to rank the answers A according to the questions Q. Baselines We experimented with several baselines: two unsupervised and one supervised. Language TF-IDF The traditional information retrieval method (Salton et al., 1975 ) uses a vector representation for q i and a i and computes a dot-product as similarity relevance score h(q i , a i ). We use n-grams of size (1, 3) and fit one model per FAQ page. Universal Sentence Encoder Encoding the semantics of a question q i and an answer a i can be achieved with the Universal Sentence Encoder (Cer et al., 2018) . The model works on monolingual and multilingual data. We encode each question and answer independently, and then perform a dot-product of the questions' and answers' representations. Dense Passage Retrieval (DPR) Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) is a state of the art method for passage retrieval. It uses a bi-encoder to encode questions and passages into a shared embedding space. We finetune DPR on our dataset using the same procedure described in Section 4.2.2. XLM-Roberta as bi-encoders Bi-encoders encode questions q i and answers a i independently and output a fixed d-dimensional representation for each query and answer. The encoder can be shared or independent to generate the representations. 13 At run-time, new queries are encoded using the encoder, and the top-k closest answers are returned. The representations for the answers can be computed once, and cached for later use. Similarity is typically computed using a dot product. Multilingual The state-of-the-art encoders such as RoBERTa (Liu et al., 2019) and BERT (Devlin et al., 2018) are trained for English only. As our dataset is multilingual we opted for XLM-RoBERTa (Conneau et al., 2019) , it was trained using masked language modeling on one hundred languages, using more than 2TB of filtered CommonCrawl data. This choice allows us to leverage the size of the English data for less represented languages. Training Given pairs of questions and answers, along with a list of non-relevant answers, the bi-encoder model is trained to minimize the negative log-likelihood of picking the positive answer amongst the nonrelevant answers. Non-relevant answers can be divided into in-batch negatives and hard negatives. In-batch negatives In-batch negatives are the other answers from the batch, including them into the set of non-relevant answers is extremely efficient as their representations are already computed. Hard negatives Hard negatives are close but incorrect answers to the questions. Including them improves the performance of retrieval models (Karpukhin et al., 2020; Xiong et al., 2020) . Hard negatives can either come from a standard retrieval system such as BM25, or an earlier iteration of the dense model (Xiong et al., 2020; Oguz et al., 2021) . The structure of our dataset, pages of FAQs, facilitates the search for hard negatives. As an example in Table 1 , three out of four answers share the term COVID-19. The model now has to understand the semantic of sentences instead of matching on shared named entities. By including all the pairs of the same page in the same training batch, we ensure that in-batch negatives act as hard negatives. 14 Multilingual Although XLM-Roberta is multilingual, we do not expect the model to perform cross-lingual retrieval (i.e. using one language for the query and another for the answer). We make sure that each batch is composed of pairs from the same language. This increases the difficulty of the task. Otherwise, the model could rely on the language of answers as a differentiating factor. answers answers questions questions Shared Q 1 A 1 Q 2 A 1 ... Q N A 1 Q N A 2 ... Q 2 A 2 Q 1 A 2 ... ... ... ... Q N A 3 ... Q 2 A 3 Q 1 A 3 A 1 A 2 ... A N Q 1 Q 2 ... Experiments In this section, we evaluate the retrieval performance of our model on MFAQ. In all our experiments, we use three metrics to evaluate the performance: precision-at-one (P@1), mean reciprocal rank (MRR), and recall-at-5 (R@5). For space reasons, we only report on MRR in the main text, the full results are available in the annex. We used the same parameters for all experiments unless mentioned otherwise. 15 We insert a special token <question> before questions to let the shared encoder know it is encoding a question. Answers are size reaches the desired size, we start over with the remaining pairs. 15 We used a batch size of 800, sequences were limited to 128 tokens (capturing the entirety of 90% of the dataset), an Adam optimizer with a learning rate of 0,0001 (warmup of 1000 steps). Dropout of 25%. respectively prepended with <answer>. All of our experiments use a subset of the training set: only one page per domain as this technique achieves higher results. Refer to Section 5.3 for more information. We start by studying the performance of multilingual models, then compare it against monolingual models. Multilingual We present in Table 4 a summary of the results of our multilingual training. The model is trained concurrently on the 21 available languages. XLM-RoBERTa achieves a higher MRR on every language compared to the baselines. Low resource languages achieve a relatively high score which could indicate inter-language transfer learning. Monolingual Next, we attempt to study if a collection of monolingual models are better suited than a single monolingual model. We use language-specific BERT-like models for each language. The list of BERT models per language is available in the annex. We followed the same procedure as described in Section 4.2, except for the encoder which is language-specific. We limited our study of monolingual models to the ten largest languages of MFAQ. We choose these languages as they have sufficient training examples, and pre-trained BERT-like models are readily available. To study the performance of monolingual models we train models using the same procedure as described in Section 4.2 except for the encoder. The results in Table 5 indicates that a multilingual model outperforms monolingual models in all cases, except for English. These results indicate that leveraging additional languages is beneficial for the task of FAQ retrieval, especially for languages with fewer resources available. Interestingly, RoBERTa slightly beats DPR in English. This underperformance could be explained by the difference in batch size. Because of the dual encoder nature of DPR, we had to reduce the batch size to 320 compared to 800 for RoBERTa. Cross-lingual Our training procedure ensures that the model never has to use language as a cue to select the appropriate answer. Batches of training data all share the same language. We tested the cross-lingual retrieval capabilities of our multilingual model by translating the queries to English while keeping the answers in the original language. The French performance drops from 80.7 to 78.2, which is still better than the unsupervised baselines. The full results are presented in Table 6 6 Qualitative analysis In this section, we dive into the model's predictions and try to understand why and where it goes wrong. We do so by focusing on a single FAQ page from the admission center of the Tepper School of Business. 16 The FAQs are displayed in Table 8 Although it can cope with some synonyms (activities -experiences), this qualitative analysis shows our model is overly reliant on keywords for matching questions and answers. Further research on adversarial training of FAQ retrieval is needed. Future Work Important non-Indo-European languages such as Chinese, Hindi, or Japanese are missing from this dataset. Future work is needed to improve data collection in these languages. Second, we did not evaluate the model on a real-life FAQ retrieval dataset (with user queries). Future work is needed to see if our model can perform question-to-question retrieval, or if it needs further training to do so. A linguistic study could analyze the model's strengths and weaknesses by studying the model's performance by type of questions, answers, and entities. Conclusion In this work, we presented the first multilingual dataset of FAQs publicly available. Its size and breadth of languages are significantly larger than other datasets available. While language-specific BERT-like models can be applied to the task of FAQ retrieval, we showed it is beneficial to use a multilingual model and train on all languages at once. This method of training outperforms all monolingual models, except for English. Our qualitative analysis reveals our model is overly reliant on keywords to match questions and answers. Monolingual P@1 MRR R@5 P@1 MRR R@5 P@1 MRR R@5 P@1 MRR R@5 P@1 MRR R@5 P@1 MRR R@5 English 5.9 18.9  1 Are the hours flexible enough for full-time working adults? Yes, the MSBA program accommodates students working full-time. Required weekly live sessions, lasting 75 minutes, are held in the evening, and the three residential components, two strongly recommended and one optional, take place over weekends. Students complete all other coursework on their own schedule, but must adhere to deadlines and be prepared to participate in weekly live sessions. Language 2 Can I take a course from a third-party provider, like Lynda or Coursera, to prepare for the programming requirements of this program? Our goal is to make sure that everyone entering the program has the necessary background to be successful. We strongly recommend that applicants who feel they need additional preparation in programming languages take a for-credit course from an accredited two-or four-year institution. 3 Can I transfer credits into the program? No, the Tepper School does not accept transfer credits. 4 Can the GMAT or GRE requirement be waived? No, these test scores are required. 5 Do I have to maintain a certain GPA in the program to graduate? Yes, MSBA degree candidates must maintain a minimum cumulative GPA of 3.0 to graduate. 6 Do you offer the opportunity to preview courses in your program to get a feel for what they are like? Yes we do. To preview one of our courses, please visit our Virtual Class Visit page. You'll be able to register to virtually participate in a course of your choosing. 7 How do I learn more about the online learning environment? To preview one of our courses, please visit our Virtual Class Visit page. You'll be able to view upcoming courses and register to virtually attend a course of your choosing. Acknowledgements This research received funding from the Flemish Government under the \"Onderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen\" programme. What is the average Quant and Verbal scores for the GRE and GMAT? There is no average score expectation. The test scores are simply one component of the multifaceted admissions process that we consider when making an admissions decision. 24 What separates the Tepper School of Business' online MSBA program from other MSBA programs, either online or on-campus? The Tepper School of Business is globally renowned for its analytical approach to business problem solving. It is an integral part of Carnegie Mellon University, a top-tier research university that has become the center for disciplines including data science, robotics, business intelligence and additive manufacturing. Several faculty members of the online MS in Business Analytics also hold appointments in other schools including Carnegie Mellon's top-ranked School of Computer Science. Our MSBA program provides students with exceptionally robust coursework in business analytics techniques, with a special focus on machine learning and optimization. All of the advanced analytics skills we teach are delivered in a business context, ensuring that students graduate knowing how to efficiently, effectively and creatively apply their analytics expertise to business problems. Furthermore, the Tepper School's Accelerate Leadership Center offers students the opportunity to improve their leadership, inter-personal and communication skills, through online assessments and one-on-one coaching. Additionally, the program's on-campus experiences provide opportunities for online students to interact closely with faculty, each other and industry professionals working at companies that look to Carnegie Mellon and the Tepper School for high-caliber talent. 25 What time(s) do the synchronous sessions take place? The weekly live sessions are in the evening (U.S. Eastern Time) and typically last 75 minutes. 26 What types of financial aid or scholarships are available to online students? Students may be eligible to take out federal and/or private education loans to cover tuition and other education-related costs. Please view our Tuition page for details. At this time, the Tepper School does not provide scholarships for the MSBA program.",
    "abstract": "In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa (Conneau et al., 2019)   achieves the best results, except for English. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the model on simple word changes. We publicly release our dataset 1 , model 2 and training script 3 .",
    "countries": [
        "Belgium"
    ],
    "languages": [
        "Arabic",
        "German",
        "English",
        "Spanish"
    ],
    "numcitedby": "2",
    "year": "2021",
    "month": "November",
    "title": "{MFAQ}: a Multilingual {FAQ} Dataset"
}