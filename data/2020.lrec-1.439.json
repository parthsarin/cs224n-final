{
    "article": "We consider the problem of disambiguating the lemma and part of speech of ambiguous words in morphologically rich languages. We propose a method for disambiguating ambiguous words in context, using a large un-annotated corpus of text, and a morphological analyser-with no manual disambiguation or data annotation. We assume that the morphological analyser produces multiple analyses for ambiguous words. The idea is to train recurrent neural networks on the output that the morphological analyser produces for unambiguous words. We present performance on POS and lemma disambiguation that reaches or surpasses the state of the art-including supervised models-using no manually annotated data. We evaluate the method on several morphologically rich languages. Introduction The problem of disambiguation is defined as selecting the correct analysis from a set of possible analyses for a word in a sentence-e.g., from among the analyses produced by a morphological analyser. Disambiguation is performed by utilizing information in the surrounding context. Morphological analysers are commonly used in various NLP applications. These normally produce a significant amount of ambiguous analyses. In this work we tackle the problem of disambiguation by training a model for predicting the correct part-of-speech (POS) and lemma. We show that for the majority of cases, this is sufficient to disambiguate from the set of possible analyses. We use manually annotated data only for evaluation, which means that to train our model we need only a morphological analyser for the language and an unlabelled corpus. The main idea of our approach is to use bidirectional LSTMs (Gers et al., 2000) -BiLSTMs-to disambiguate the output of morphological analysers, by utilizing only the unambiguous outputs during the training procedure. We train bidirectional models using a sequence of embeddings for the surface form for each target word. The objective of the network is to produce output probability distributions over the possible POS tags and lemmas. The model is trained using only the unambiguous input tokens; the loss is computed only for those unambiguous instances. Ambiguous tokens are not considered as target tokens during training. Since we only input unlabelled data for training, the quality of the model itself is only affected by the amount of available unlabelled data for the language. In our experiments, we evaluate our models on manually annotated data sets for Finnish, Russian and Spanish. For Finnish and Russian, at least, annotated (i.e., disambiguated) data is in limited supply, whereas for all three languages unlabelled data is in abundant supply. The paper is organized as follows. In Section 2. we point to some relevant prior work. In Section 3. we describe the problem of morphological ambiguity and provide a brief motivation for the interest in the problem. In Section 4. we provide a classification for the different types of ambiguity that appear in the corpus, as well as an analysis of the viable and appropriate strategies for each type of ambiguity. Section 5. describes our data pre-processing steps and model architecture. Section 6. specifies our experimental setup, as well as the parameters used in training. In Section 7. we discuss the results obtained from the experiments. Section 8. concludes with current directions of research. Related work There is an abundance of work on disambiguation in the context of various NLP tasks, we focus on just a few relevant ones here. The work of Yatbaz and Yuret (2009) is conceptually similar to ours. Their work presents a probabilistic model for selecting the correct analysis from a set of morphological analyses for Turkish. Turkish and Finnish, as synthetic agglutinative languages, share the problem of a high number of possible analyses for a given word. This limits the amount of unambiguous data and presents a bigger problem than analytic or morphologically poor synthetic languages such as English. The LSTM based approach by Zalmout and Habash (2017), for Arabic, is also similar to our method. They train a POS tagging model on an annotated corpus, using added features, and use the resulting model to disambiguate a morphological analyser, achieving a lemma accuracy of 96.8%. The POS tagger by Inoue et al. (2017) for Arabic utilizes a form of multi-task learning. Tkachenko and Sirts (2018) present another neural morphological tagger, for Estonian, in which the output of an analyser is also used to augment the input to their neural models. In contrast to the above mentioned neural models, we use the unambiguous outputs of the analyser to learn to disambiguate remaining ones, instead of learning a POS tagger on an annotated corpus. Problem description Definitions Throughout this work, we make use of the following concepts: \u2022 The part-of-speech (POS) of a word (of a surface form) is its morpho-syntactic category or class. This indicates the role the word plays in the sentence, as well as the inflectional paradigm-the pattern of inflection-that the word follows. Examples of POS are: noun, verb, and adjective. 1 \u2022 The lemma is the canonical, or \"dictionary,\" form of a word. For example, for nouns the lemma is the nominative case singular and for verbs the lemma is the infinitive. 2 \u2022 A surface form is the form in which the word appears in text. The surface form may be an inflected form of the lemma, or may be identical to the lemma; for uninflected POSs, the surface form is always identical to the lemma. \u2022 Morphological tags are values that the morphological analyser assigns to morphological features of the word. For example, the feature number may have values such as singular and plural; the feature case may have values such as nominative and genitive, depending on the feature inventory of the language. Morphological analysis is the task of breaking down a surface form into its lemma, POS and morphological features (tags), by means of a morphological analyser. As an example, consider the Finnish surface form \"kotiin\" (into/toward home). A morphological analysis of \"kotiin\" would be: koti+N+Sg+Ill This indicates that the lemma is koti, the POS is N (Noun), and the morphological features are Sg (singular number) and Ill (illative case, meaning \"into/toward\"). Ambiguity Natural language is inherently ambiguous, and there are many ways in which ambiguity manifests itself. For written text, we have several types of ambiguity. POS ambiguity is a kind of syntactic ambiguity, where a word may be considered to have one of several syntactic roles inside a sentence. Lemma ambiguity occurs when a surface form is a form of more than one lemma. Morphological ambiguity occurs when a surface form has several possible analyses-several sets of morphological tags. Word sense ambiguity-when a single lemma may have several different meanings. In spoken language, other kinds of ambiguities exist, such as homophones-two words which are written differently but are pronounced the same. Spoken language ambiguity is outside the scope of our work, we concentrate on written text. One example of ambiguity is the Finnish surface form \"tuli\", which has the following analyses: tuli (fire) This exhibits all of the above kinds of ambiguity: POS, lemma, morphological tags, and word sense are all ambiguous. Disambiguation is a central problem in many NLP tasks, for many reasons. In particular, morphological disambiguation in morphologically rich languages is crucial for translation. In our application, 3 where we build tools to aid in language learning, when a student points at an unfamiliar surface form in the text which happens to be ambiguous, we need to identify the lemma appropriate to the context-so as not to confuse the learner with extraneous translations. 4  Especially in agglutinative, morphologically rich languages such as Finnish, unambiguous lemmatization is central for NLP applications that build a vocabulary from corpora. For these languages the size of the vocabulary becomes very large without lemmatization. If the lemmatization is ambiguous, then subsequent models are based on an inaccurate vocabulary. Our approach is based on the assumption that the context primes the selection of the appropriate reading from a set of several readings for an ambiguous surface form. By \"priming,\" we mean the following: a simple experiment with Google's translator shows that the ambiguous word \u0431\u0435\u043b\u043a\u0438, is easily disambiguated by its immediate context. The surface form has two lemmas: \"\u0431\u0435\u043b\u043a\u0430\" (squirrel) and \"\u0431\u0435\u043b\u043e\u043a\" (protein). Google easily translates \"\u0431\u0435\u043b\u043a\u0438 \u0438 \u043c\u0435\u0434\u0432\u0435\u0434\u0438\" as \"squirrel and bears\", whereas it translates \"\u0431\u0435\u043b\u043a\u0438 \u0438 \u0443\u0433\u043b\u0435\u0440\u043e\u0434\u044b\" into \"proteins and carbons.\" Thus, Google's translation problem subsumes the disambiguation problem that we are trying to solve; in fact, Google's translator could be viewed as a \"poor man's solution\" to the disambiguation problem. However, because we are trying to solve the simpler problem-disambiguationkey point is that we may be able to solve it with a more lightweight solution. This would offer 3 benefits: A. we could achieve it with fewer and cheaper resourcestranslation requires supervision from massive parallel corpora; B. we may be able to achieve it with simpler models; and C. we may be able to achieve better performance on disambiguation, than if we tried to use a full translation machine to perform disambiguation. We further rely on the assumption that a large corpus will contain enough unambiguous contexts for each POS and for each lemma, so that the model should be able to learn to disambiguate the ambiguous instances. Types of ambiguity We discuss briefly a taxonomy of the types of ambiguity that are of interest to us. Additional examples are given Declinable-Declinable Declinable-Indeclinable Indeclinable-Indeclinable = POS = lemma POS dis- amb. POS disamb. POS disamb. = POS = lemma lemma dis- amb. n/a n/a = POS = lemma either POS dis- amb. n/a = POS = lemma neither n/a n/a Table 1 : Viable approaches for each type of ambiguity. in Appendix A, for several languages. In many cases, the problem of disambiguation can be reduced to one of two problems: POS tagging or lemmatization-given a surface form in context (running text), find its POS or lemma, respectively. We outline the main types of morphological ambiguity, and whether we can use one approach or the other to resolve it. We classify lemmas into two types-depending on whether they accept inflectional morphemes: declinable lemmas accept them, and indeclinable lemmas do not. Thus, an indeclinable lemma has only one surface form. Declinable lemmas can have many surface forms. We use the term reading to denote a unique combination of lemma and POS. We divide surface form ambiguities into three categories in the following subsections: two (or more) declinable lemmas, one declinable and one indeclinable lemma, or two indeclinable lemmas. Surface forms with two declinable lemmas This is the easiest case to train for, since, in general, the sets of surface forms derived from the two lemmas rarely overlap. For example, Finnish surface form FI \"tuli\" has two readings, as above: tuli (fire) Noun, nominative, sing. || tulla (come) Verb, indicative, active, past, 3rd person, sing. In this example, the lemmas and the POS's of the readings are both different. This is the most common type of ambiguity, and either method (POS or lemma disambiguation) can be applied. Lastly, we turn to word-sense ambiguity. For example, in English, the word/lemma \"spirit\" may mean \"soul\" or \"alcohol\". These are unrelated semantically, but have the same lemma, same POS, and follow identical inflectional patterns. Although this type of ambiguity is also important for translation, it is outside the scope of this paper. 5  To sum up, we are concerned with disambiguating among different readings-i.e., POS or lemma disambiguation. We are not concerned with disambiguating different possible morphological tags of a given reading, nor with disambiguating multiple word senses of a given reading. Surface forms with one declinable and one indeclinable lemma In this case, trying to predict the lemma may be less effective: although the lemmas may be different, every instance of the reading with the indeclinable lemma is ambiguoussince it always \"drags along\" the other readings with it. Finnish and Russian have many instances of such surface forms. In Finnish, many adverbs or post-positions originate historically from an inflected form of a semantically related noun. For example, FI \"j\u00e4lkeen\": 6 j\u00e4lkeen (after) Post-position || j\u00e4lki (into a footprint) Noun, illative, sing. Thus, every occurrence of the post-position \"j\u00e4lkeen\" drags along with it the readings for the illative case of \"j\u00e4lki\" (which is also a valid reading of \"j\u00e4lkeen\"). However, the model can still hope to learn that the POS of this surface form is post-position, since other unambiguous post-positions may occur in similar contexts elsewhere in the corpus. Thus, POS tagging is an effective solution to this type of ambiguity. The POS determines whether the reading is declinable or indeclinable. Thus a surface form cannot have a declinable and an indeclinable reading with the same POS, as seen in Table 2 , and in fact such instances do not appear in the corpus. Two indeclinable lemmas If the readings are both indeclinable-neither can be inflected-and if their lemmas are different, there is no ambiguity, as the surface forms will always differ. Model We next turn to the technical description of our approach. First, we outline the steps involved in preparing the data for our model. We then proceed to present the architecture of our model, and the training procedure. Data pre-processing First, we tokenize each document as a flat list of surface forms (tokens). We then use morphological analysers to obtain the readings of each surface form. For Finnish, we use analysers from the Giellatekno platform (Moshagen et al., 2013) 8 . Giellatekno analysers are based on Two-level Morphology, by Koskenniemi (1983) . For Russian, we use the analyser from Klyshinsky et al. (2011) . For Spanish, we use the analyser from Forcada et al. (2011) . Since the goal is to disambiguate the output of the analyser, the coverage of said analyser-the percentage of tokens that have an analysis-is a relevant concern. The Finnish, Russian and Spanish analysers have 95.14%, 97.79% and 96.78% coverage, respectively. Most of the unknown tokens are foreign or misspelled words. For Finnish-which has compounding-we split the surface form of the compounds into their \"maximal\" pieces, i.e., the largest parts for which there is a lemma in the analyser's lexicon. For example, the Finnish compound word el\u00e4inl\u00e4\u00e4k\u00e4riasema (\"veterinary clinic\") is made up of three elementary stems: el\u00e4in (\"animal\") + l\u00e4\u00e4k\u00e4ri (\"doctor\") + asema (\"station\"). However, since the analyser has el\u00e4inl\u00e4\u00e4k\u00e4ri (\"veterinarian\") in its lexicon, we split as el\u00e4inl\u00e4\u00e4k\u00e4ri + asema. This helps us keep the vocabulary smaller-since there is a potentially infinite number of possible compounds in Finnish-while keeping the meaning of commonly used compounds, which usually differs a little from that of the sum of its parts. For Russian, this is not a concern, as there are generally no compound words. There may be cases in which a lemma is formed by joining two other lemmas, but this is considered a new lemma in its own right. The same applies to Spanish, where we additionally have clitic pronouns attached to verbs and prepositional contractions (preposition + article), which are treated as separate tokens. While we do preserve information about sentence boundaries in the form of punctuation, we do not explicitly preserve sentence structure in terms of the training window. We found that several sentences in our corpora were too short to provide the contextual information necessary for disambiguating the target words, and that this information was partially found in the adjacent sentences. Instead, we make a sliding window of radius r over this list of tokens, i.e. we take r tokens to the left and r tokens to the right of some given target token, as well as the token itself. Tokens are selected as targets for the training set only if they are unambiguous. Each training instance consists of said window, and the label for the target word, given by the analyser-the lemma or POS, depending on the desired target for the model. The target for the lemma is the index of said lemma in our vocabulary. For the test set, we instead select only the ambiguous tokens, since the unambiguous ones will trivially give us a 100% accuracy. Each testing instance consists of the window, the possible labels and the true label for the target word. We then obtain the word embeddings for each surface form in the window using the FastText (Bojanowski et al., 2017) Common Crawl 9 pre-trained models for each language. This allows us to get an embedding even for outof-vocabulary words, and to efficiently get embeddings for Finnish, which has a very large surface form vocabulary in our corpus-around 2 million unique tokens. FastText allows us to avoid using an embedding matrix and instead obtain the embeddings dynamically during training. For the positions in the window which go beyond the limits of the document, we insert a zero-valued embedding as padding. Each language has its own morphological analyser, and their outputs differ slightly. For compatibility between different analysers and our model, we map all POS to a common universal set. This also allows us to simplify the problem, by aggregating POS that fulfil a similar role, such as: postpositions + prepositions \u2192 adpositions. We use a set of 10 POS: noun, pronoun, numeral, adjective, verb, adverb, adposition, conjunction, punctuation, other (a catch-all category for things like acronyms or symbols). Lemmas which are composed entirely of numerical digits are assigned a special embedding for numbers, since we consider them to always have a more or less equivalent role in the context. Ambiguities between common nouns and proper nouns are ignored, as names are out of the scope of what we try to accomplish here and should be solved using Named Entity Recognition (NER) techniques instead. We keep punctuation in order to recover the sentence boundaries within the window, as these may still prove useful to generate a proper context. Architecture & training Our model architecture is based on context2vec (Melamud et al., 2016) , which itself is a modification of the original word2vec CBOW model (Mikolov et al., 2013) . In con-text2vec, the context for each word is computed using a bidirectional LSTM, rather than as a vector average (as in word2vec), which enables the embeddings to capture sentence structure and word order, rather than only word cooccurrence. The training procedure is analogous to that of word2vec, since our objective is similar: given a context, compute the probability that the word belongs to that context, for each word in the vocabulary-in our case, instead of word tokens, we compute the probability for the lemma or the POS. Thus, our input and target vocabularies are different, in contrast to word2vec. The architecture borrows some ideas from neural machine translation (NMT) encoder-decoder models, such as that developed by Google (Wu et al., 2016) . In that model, encoding the context of a token into one vector is enough to be able to translate-and therefore disambiguate-that token. We therefore use the encoder part of the architecture to capture the necessary information to disambiguate a token. The model consists of three trainable parts: \u2022 Bi-LSTM which produces the left and right context embeddings. \u2022 multi-layer Perceptron (MLP), which merges these into a single context embedding. \u2022 projection matrix, to transform the context embedding into scores for all possible labels. Each training instance consists of a window of surface form embeddings around an unambiguous target word, and the label for such word, defined as the index of the corresponding lemma or POS in the vocabulary. To obtain the predictions, we proceed as follows. First, we feed the window from the beginning to the target word to the left LSTM, and from the end to the target word to the right LSTM. Their hidden states serve as the left and right context embeddings. We then concatenate the left and right context with the surface form embedding for the target word, and feed the result to the MLP. This \"residual\" connection, where some input is fed to several layers of the network, is also a concept from NMT, and is done in order to separate important parts of the input from the encoded state. Next, the output of the MLP is multiplied by the projection matrix, to get an array of scores of length equal to the number of possible labels. Finally, we apply a softmax function to get the probability distribution for all labels. An overview of the model can be observed in Figure 1 . 10  To obtain the loss for the model, we compute the crossentropy between the predicted probability distribution and the real distribution, which is the one-hot encoding of the true label index. 6. Experiments Data The data we use are obtained from two different sources. The annotated evaluation data is acquired from the Universal Dependencies Treebank, (Nivre et al., 2018) . These data are in the CoNLL-U 2006/2007 format, (Nivre et al., 2007) . The annotations in the data are used purely for the evaluation of the resulting models. For Finnish, as the annotated data sets were quite small, we used a collection of 600K 10 The input (bottom line) is a window of words in Finnish: \". . . is\u00e4ns\u00e4 tuli aamuy\u00f6ll\u00e4 . . . \" (\". . . his father arrived in the early hours . . . \") The target surface form, for which we will try to get a prediction, is ambiguous: it may be a verb (arrive) with lemma \"tulla\", or a noun (fire) with lemma \"tuli.\" (This is the same as the example discussed in Section 3.2..) Table 6 : The columns mean: Blind: percentage of ambiguities resolved with \"blind\" predictions-without using the analyser output; Guided: percentage of disambiguated by picking the top prediction from the analyser output; Token: overall token-level accuracy, by applying the best method. SOTA: for comparison, shows the state-of-the-art results. proprietary news articles for training the model, after processing the text with the Finnish morphological analyser. The Russian annotated data set was large enough to use its predefined train-test split. Technically this split would not have been necessary, as we do not use annotations from data as labels. Experimental setup For each language, we trained two separate models: one to predict the correct POS, and one to predict the lemma. We learn the model by training on the unambiguously analysed tokens; we do not train on the ambiguous instances-this allows us to explore the unsupervised approach, with no need for manual disambiguation. In each case, we evaluate our models by two metrics. Firstly, we pick the analysis with the highest value in the softmax output probability vector. We call this the \"blind\" disambiguation. Secondly, rather than picking the highestscoring softmax output overall, we rather pick the highestscoring output from the same output probability vector, but only from among the options deemed possible by the morphological analyser. For example, if the model is predicting the POS, the blind approach selects the POS that receives the highest output score from all possible POSs in the language. The analyser-based approach selects the highest scoring POS only from those POS values that are among the possibilities admitted by the morphological analyser for the given surface form. We proceed analogously for the lemma-based models. The \"blind\" predictions are thus equivalent to plain POS tagging and lemmatization. We evaluate each model with the manually annotated, disambiguated corpus for each language. We compute both evaluation metrics (precision, recall and F 1 score) as well as the percentage of correct predictions, for direct comparison with the state of the art. In addition, we evaluate each metric with respect to a \"confidence\" measure, defined as the probability given by the softmax function for each prediction. To this end, we set a confidence threshold \u03b8 conf such that any prediction with confidence below that threshold will be deemed invalid. In doing so, we wish to test whether the more confident predictions will have a higher precision without a significant loss in recall, for applications where the goal is to obtain the highest possible precision. Table 3 details the parameters used for the network. Increasing the number of trainable parameters yields no significant increase in accuracy. It is possible that with a more complex model the prediction accuracy could be slightly higher. Table 4 details the training hyper-parameters. We use the Adam optimizer, (Kingma and Ba, 2015) , to minimize the loss. Results For the three languages on which we performed an evaluation of our models, we significantly reduced the number of remaining ambiguities. Table 5 illustrates the results of our experiments in terms of number of ambiguities and evaluation metrics.  For Russian, the best result to date for POS tagging was reported by Dereza et al. (2016) , achieved using Tree-Tagger, (Schmid, 1994) , at 96.94%. We could not find lemmatization results for Russian, but the work by Korobov (2015) solves the broader problem of morphological ambiguity with an accuracy of 81.7%. For Finnish POS tagging and lemmatization, the TurkuNLP neural model (Kanerva et al., 2018) achieves 97.7% and 95.3% accuracy, respectively, evaluated on the same dataset as our method. For Spanish POS tagging and lemmatization, the model by Carreras et al. (2004) achieves an accuracy of 89% and 88%, respectively, according to the evaluation done by Parra Escart\u00edn and Mart\u00ednez Alonso (2015). As for the confidence analysis, we see that, for every language, we can in fact build a POS model which has very high precision (>0.9) at the cost of being unable to obtain a prediction for a fraction of the instances. Figure 2 , Figure 3 and Figure 4 show the results for Finnish, Russian and Spanish, respectively. Conclusions We have shown that the output of morphological analysers can be disambiguated to a significant degree for Finnish, Russian and Spanish. The requirements for this procedure are: the language must have a morphological analyser, there must exist a text corpus, and preferably a small amount of annotated data for evaluation purposes. The same procedure we used should perform comparatively for any language with a morphological analyser, assuming it is of sufficient quality-unknown tokens must rely on the less accurate \"blind\" predictions for inference. There are many morphologically rich languages that could benefit from this, such as other Uralic languages, Turkic languages, many Indo-European languages, etc. There is limited annotated training data for many of these languages, but morphological analysers are available for most of them. The quality of the analyser in terms of percentage of unambiguous output does affect the final total token accuracy. The difference between the two cases end result presented in this work was small in the end. It is unclear how much ambiguity will begin to significantly impair our method. Named Entity Recognition (NER) could theoretically be used in conjunction with our procedure to further disambiguate the proper noun analyses. We have achieved different performance depending on whether the objective used was disambiguating the lemma or POS. We have seen that different types of ambiguity are solved to varying degrees by predicting either POS or lemma. A natural next step would be to combine the two different models in an ensemble model. In table 2 we saw that, although POS tagging works for most of the cases, around 9% of the ambiguities are only solvable by lemma prediction. Since it is possible to identify these instances during inference, an ensemble solution could use the lemma prediction model to disambiguate these. Moreover, around 6% of the instances currently cannot be disambiguated using either method. This puts the upper limit on accuracy to 85% for the better model (POS prediction) . Using an ensemble model to also capture the lemma-only ambiguities would therefore push this limit to 94%. Another approach we have explored is the use of multi-task learning to predict both POS and lemma at the same time. We tried a na\u00efve approach, reusing the LSTM parameters and alternating between the two different objectives during training. So far this has been somewhat unsuccessful, yielding an accuracy around 10% lower than that of either of the single-task models, but we believe there is still much room for improvement. To push the performance nearer to 100%, it will be necessary to make a model that predicts morphological tags, either as an addition to the existing models, or as a standalone model that we can then invoke for these instances where the POS and lemma are the same. morphological disambiguation using statistical language models. In Pro. of the NIPS 2009 Workshop on Grammar Induction, Representation of Language and Language Learning, Whistler, Canada, pages 321-324. Zalmout, N. and Habash, N. (2017) . Don't throw those morphological analyzers away just yet: Neural morphological disambiguation for Arabic. pages 704-713, 01. Additional examples of the kinds of ambiguities that our method handles (and does not handle): We divide surface form ambiguities into three categories in the following subsections: two (or more) declinable lemmas, one declinable and one indeclinable lemma, or two indeclinable lemmas. We classify lemmas into two types-depending on whether they accept inflectional morphemes: declinable lemmas accept them, and indeclinable lemmas do not. Thus, an indeclinable lemma has only one surface form. Declinable lemmas can have many surface forms. In the examples, we use the following annotation convention: \u2022 \"the surface form\" Same lemma, different POS: Each of the following words (surface forms) has two readings, where the lemmas are the same, but the POS are different: ES \"parecer\": parecer (seem), Verb, infinitive || parecer (opinion), Noun, sing. RU \"\u0437\u043d\u0430\u0442\u044c\": \u0437\u043d\u0430\u0442\u044c (know) Verb, infinitive || \u0437\u043d\u0430\u0442\u044c (nobility) Noun, nominative, sing. RU \"\u0441\u0442\u0430\u0442\u044c\": \u0441\u0442\u0430\u0442\u044c (become) Verb, infinitive || \u0441\u0442\u0430\u0442\u044c (posture) Noun, nominative, sing. RU \"\u043f\u0435\u0447\u044c\": \u043f\u0435\u0447\u044c (bake) Verb, infinitive || \u043f\u0435\u0447\u044c (hearth) Noun, nominative, sing. Different lemma, same POS: This is type of ambiguity is present in all languages. The following surface forms have two (or more) readings: FI \"palaa\": palaa (returns) Verb, present, 3rd, sing. || palata (burns) Verb, present, 3rd, sing. FI \"alusta\": alusta (pad, base) Noun, nominative, sing. || alus (ship) Noun, partitive, sing. || alunen (underlay) Noun, partitive, sing. RU \"\u0447\u0435\u0440\u0442\u0430\": (mark || of the devil) \u0447\u0435\u0440\u0442\u0430 (mark) Noun, nominative, sing. || \u0447\u0435\u0440\u0442 (devil) Noun, genitive, sing. RU \"\u0431\u0435\u043b\u043a\u0443\": (squirrel (acc.) || to the protein) \u0431\u0435\u043b\u043a\u0430 (squirrel) Noun, accusative, sing. || \u0431\u0435\u043b\u043e\u043a (protein) Noun, dative, sing. ES \"fui\": (I was || I went) ser (be) Verb, past perf., 1st, sing. || ir (go) Verb, past perf., 1st, sing. Same lemma, same POS: These are the kinds of ambiguities that our methods do not address, since both the lemma and POS are identical for the different analyses: FI \"nostaa\": nostaa (raise), Verb, infinitive || nostaa (raise), Verb, present, 3rd, sing. RU \"\u043a\u043e\u0442\u0430\": \u043a\u043e\u0442 (cat)",
    "abstract": "We consider the problem of disambiguating the lemma and part of speech of ambiguous words in morphologically rich languages. We propose a method for disambiguating ambiguous words in context, using a large un-annotated corpus of text, and a morphological analyser-with no manual disambiguation or data annotation. We assume that the morphological analyser produces multiple analyses for ambiguous words. The idea is to train recurrent neural networks on the output that the morphological analyser produces for unambiguous words. We present performance on POS and lemma disambiguation that reaches or surpasses the state of the art-including supervised models-using no manually annotated data. We evaluate the method on several morphologically rich languages.",
    "countries": [
        "Finland"
    ],
    "languages": [
        "Russian",
        "Spanish",
        "Finnish"
    ],
    "numcitedby": "1",
    "year": "2020",
    "month": "May",
    "title": "Neural Disambiguation of Lemma and Part of Speech in Morphologically Rich Languages"
}