{
    "article": "Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset (Hudson and Manning, 2019) to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and-vice versa-multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M 3 P) in zeroshot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling. 1 grounding, visual question answering, referring expression comprehension and image-text retrieval (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Zhang et al., 2021; Ni et al., 2021; Kamath et al., 2021; Miech et al., 2021; Frank et al., 2021; Bugliarello et al., 2021; Radford et al., 2021; Jia et al., 2021; Eichenberg et al., 2021; Singh et al., 2021; Fu et al., 2021; Yang et al., 2021; Yuan et al., 2021; Wang et al., 2021a; Li et al., 2021; Geigle et al., 2022, inter alia ). Yet, progress in this area has been limited mostly to the English language, as the main multimodal datasets consist only of English text. Due to the scarcity of multilingual evaluation benchmarks, there has been limited development of models that tackle this joint problem. Aiming to address this gap, in this paper we propose xGQA, a multilingual evaluation benchmark for the visual question answering task, extending the monolingual English-only GQA dataset (Hudson and Manning, 2019) . For xGQA we manually translate and adapt the balanced GQA test-dev set into 7 new languages from 7 language families, covering 5 distinct scripts; see Figure 1 and Ta-ble 1 later. In addition, we provide new fixed data splits to guide cross-lingual few-shot learning experiments, where only a small number of examples in the target language are utilized. As pretraining is (i) notoriously computationally expensive for high-resource languages and (ii) only limited amounts of multilingual multimodal resources are available, we also propose computationally efficient adapter-based (Houlsby et al., 2019) approaches as additional baselines for constructing multilingual multimodal models. In a nutshell, we extend multimodal models pretrained only on English text (Zhang et al., 2021) to become multilingual and-vice versa-multilingual models (Devlin et al., 2019) to become multimodal. To this end, we follow the approaches of Artetxe et al. (2020) and Pfeiffer et al. (2020b Pfeiffer et al. ( , 2021) ) and extend monolingual and multilingual models to new languages and scripts via learning new tokenizers and corresponding word-embedding matrices, as well as adapters for the target languages. To transfer the respective multilingual multimodal adapter-based models to the target task, we propose a novel modality-specific split architecture, which uses modality dependent adapter weights (see Figure 2 for an illustration of the architecture). Our results clearly indicate that the proposed adapter-based architecture outperforms the recent state-of-the-art pretrained multilingual multimodal M 3 P model (Ni et al., 2021) in zero-shot crosslingual settings. However, the overall performance of zero-shot transfer remains low across the board, with an average drop of around 38 accuracy points across target languages. Using a small number of target language examples in a few-shot setup considerably improves performance for all approaches, but cross-lingual transfer performance still lags substantially behind source language performance. This demonstrates the inherent difficulty of the task, even though the corresponding questions are arguably simple as they are template based and only contain 8.5 words on average (see Figure 1 ). Contributions. 1) We propose the first evaluation benchmark for cross-lingual visual question answering, covering 7 diverse target languages; 2) we propose novel adapter-based approaches for the creation of multilingual multimodal models; 3) we systematically benchmark state-of-the-art and new multilingual multimodal models in zero-shot and few-shot learning setups, demonstrating the difficulty of the proposed task and serving as strong reference points for future work; 4) we provide a thorough analysis of the different approaches, highlighting the aspects and question types that lead to the most common model failures, again motivating future work in this domain. Background and Related Work Multilingual Language Models. Pretrained multilingual transformer-based LMs such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) adopt the same pretraining regime as their respective monolingual counterparts: BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) . They are pretrained via self-supervised masked language modelling objective (MLM) on concatenated text corpora of more than 100 languages, where text is tokenized using WordPiece, SentencePiece or BytePair encodings. These multilingual models have been shown to work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020; Rust et al., 2021) . Vision and Language Models. Most transformerbased multimodal models (Lu et al., 2019; Tan and Bansal, 2019; Chen et al., 2020; Li et al., 2020a; Gan et al., 2020; Li et al., 2020b; Bugliarello et al., 2021; Ni et al., 2021, inter alia) jointly encode text tokens and image region features by preprocessing images using object detection models-such as Faster R-CNN (Ren et al., 2015) -to extract features for regions of interest (RoI) (Anderson et al., 2018) . The image region features are passed through an affine layer, which learns to project the region features to the joint embedding space of the multimodal transformer. The bounding box coordinates of the RoI act as positional embeddings for the visual features. As such, they undergo an affine transformation to the embedding space and are combined with their respective image region representation. The position-aware image region embeddings get passed into the transformer. The multi-head attention then attends over all text and image inputs at every layer, learning a joint representation of both modalities. On the other hand, Kamath et al. (2021) avoid using object detectors as a black-box for pre-extracting these region features and instead make it a central part of the multimodal transformer architecture. Training the object detector end-to-end with the multimodal transformer adds flexibility and better representation capacity. Similar to MLM, multimodal transformer-based models are trained with self-supervised objectives such as masked feature regression, masked object detection, masked attribute detection, and contrastive losses such as cross-modality matching (Tan and Bansal, 2019) . Typically, image captioning datasets are used for pretraining such as COCO (Lin et al., 2014 ), Flickr30k (Plummer et al., 2015) , Conceptual Captions (CC) (Sharma et al., 2018) , and SBU (Ordonez et al., 2011) . Similar to unimodal language models, the [CLS] token is used as a contextual representation for classification tasks. Multilingual multimodal models have also been proposed recently: M 3 P (Ni et al., 2021) is trained on the Wikipedias of 50 different languages and the English multimodal CC dataset. In order to align tokens of languages other than English with image representations, M 3 P utilizes a code-switching mechanism, where words of the English CC examples are randomly replaced with words from corresponding bilingual dictionaries. In UC 2 , Zhou et al. (2021) augment English multimodal datasets with other languages via machine translation and propose masked region-to-token modeling and visual translation language modeling. 2 (Rebuffi et al., 2017; Houlsby et al., 2019) have been introduced as a more efficient finetuning strategy for transfer learning in NLP and CV. Instead of fine-tuning all the weights of a pretrained model on the target task, small feed-forward layers are introduced at each layer of the pretrained model. During task fine-tuning, only the adapter weights are updated, while the pretrained parameters remain fixed/frozen. Adapters have been shown to be very training efficient (R\u00fcckl\u00e9 et al., 2021) , and among an increasing amount of applications they can be utilized to transfer between domains (R\u00fcckl\u00e9 et al., 2020) and tasks (Poth et al., 2021) , and in machine translation (Bapna and Firat, 2019; Philip et al., 2020; Le et al., 2021) and cross-lingual transfer (Pfeiffer et al., 2020b (Pfeiffer et al., , 2021;; \u00dcst\u00fcn et al., 2020; Ansell et al., 2021 , inter alia) scenarios. Adapters Datasets. Pretraining and fine-tuning data for multilingual multimodal models is typically based on (multimodal information from) Wikipedia (WikiCaps, WIT, Schamoni et al., 2018; Srinivasan et al., 2021) , or on available downstream task data. Multi30k (Elliott et al., 2016) is a multi-lingual image captioning dataset for retrieval-type questions, covering English, German, French, and Czech; GEM (Su et al., 2021 ) covers image and video retrieval tasks across 20 and 30 different languages, respectively; HowTo100M (Huang et al., 2021) is a multilingual and multimodal pretraining dataset for image and video retrieval; Multi-Subs (Wang et al., 2021b) focuses on fill-in-theblank tasks and lexical translation, covering English, Spanish, German, Portuguese, and French. Gao et al. (2015) ; Shimizu et al. (2018) propose bilingual visual question answering datasets for English, and Chinese and Japanese respectively. In contemporary work Liu et al. (2021) propose MaRVL, a binary multilingual question answering dataset similar to NLVR2 (Suhr et al., 2019) , spanning 5 typologically diverse languages (Chinese, Tamil, Swahili, Indonesian, and Turkish). Previous datasets predominantly focus on (arguably simpler) retrieval-type tasks, only cover a small set of similar languages (e.g., Multi30k, Mul-tiSubs), or only cover binary questions. In contrast, we propose the first multilingual visual question answering dataset, which covers a typologically more diverse set of languages. Most recently, IGLUE (Bugliarello et al., 2022)-a multilingual multimodal benchmark that integrates xGQA-was proposed: IGLUE brings together visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. xGQA The original English GQA dataset (Hudson and Manning, 2019) was constructed by leveraging Visual Genome scene graphs (Krishna et al., 2017) . An English question engine that utilizes content (i.e. information about objects, attributes, and relations provided) and structure (a linguistic grammar that couples hundreds of structural patterns and detailed lexical semantic resources) was used to generate over 22 million diverse questions, which are visually grounded in the image scene graphs. As the questions are automatically generated using templates, they do not necessarily reflect the wide spectrum of natural language, making any assumptions on the performance in the wild difficult. Each question is associated with additional metadata such as structural types: (1) verify for yes/no questions (e.g. \"Do you see any cats?\"), ( 2 jeans?\"), ( 3 ) choose for questions that present two alternatives to choose from (e.g. \"Is it red or blue?\"), ( 4 ) logical which involve logical inference (e.g. \"Is the field soft and snowy\"), and ( 5 ) compare for comparison questions between two or more objects (e.g. \"Are all the animals zebras?\"). For further details regarding the metadata, we refer the reader to Hudson and Manning ( 2019 ). Dataset Design. The principal objective when devising xGQA was to create a genuinely typologically diverse multimodal and multilingual evaluation benchmark for visual question answering. We utilize the balanced 3 test-dev set of GQA, which consists of 12,578 questions about 398 images. 4  Due to the defined structural patterns, the formulation of the questions is simple, with an average length of 8.5 words. 5 The resulting xGQA dataset covers translations in 7 languages, each representing a distinct language family, and contains examples written in 5 different scripts (see Table 1 ). Few-Shot Data Splits. In order to conduct crosslingual few-shot learning experiments, we provide new data splits of different sizes. We split on images and add all questions associated with the image to the respective set. The development and test sets consist of 50 and 300 images, respectively. The training splits consist of 1, 5, 10, 20, 25, and 48 images, see Table 2 . We ensure that the distribution 3 To reduce biases in the conditional answer distribution Hudson and Manning (2019) utilize the structural metadata to downsample and create balanced datasets that are more robust against shortcuts and guesses. 4 We chose to translate the test-dev set of GQA, as the labels for test-std are not released. 5 For this reason, we chose to hire university students that are currently conducting their (Computer Science or Computational Linguistics) studies in English and are all fluent English speakers to translate the question into their native language. They were paid above the minimum hourly wage of the country of their respective university. After all questions have been translated, another, independent native speaker then verified the translations based on random spot checks.  of structural types within each set is maintained. xGQA is the first truly typologically diverse multilingual multimodal benchmark, unlocking new experimentation and analysis opportunities in crosslingual zero-shot and few-shot scenarios. While the questions in xGQA are intuitive and easy for humans to solve, we later show that current stateof-the-art models still have difficulty with transfer. Baselines To analyze the performance and current gaps on xGQA, we first evaluate the recently proposed M 3 P model, which has been pretrained on multilingual and multimodal data. However, pretraining is computationally expensive and only limited amounts of multilingual multimodal resources are available. Therefore, we further propose new and more efficient approaches that (1) extend state-of-the-art multilingual language models to the multimodal domain and (2) provide multilingual capabilities to state-of-the-art multimodal models. Unless noted otherwise, we follow the predominant fine-tuning strategy for GQA; a prediction head is placed on top of the output of a pretrained transformer. All possible 1853 answers of the GQA task are mapped to a class label. The question associated with an image together with the positionaware region features are passed as input to the transformer, supervised using a cross-entropy loss. 6 4.1 Multimodal \u2192 Multilingual OSCAR+ Emb . To extend a monolingual transformer LM to a multilingual domain, Artetxe et al. (2020) fine-tune a new word-embedding layer in the target language. Inspired by this idea, we now describe how we extend the current state-of-theart monolingual multimodal transformer model OSCAR+ (Zhang et al., 2021) to learn new embeddings for the target languages. In the language-extension phase, we replace the embedding matrix of OSCAR+ with a randomly initialized embedding matrix. 7 The transformer weights are frozen while only the newly introduced embeddings are fine-tuned on unlabeled text data of the target language with the MLM objective. In the target-task phase, the original OSCAR+ model is fine-tuned on the English training data of GQA, where the transformer layers are fine-tuned, but the embedding layer is frozen. During inference, the embedding layer is replaced with the target language's embedding layer. OSCAR+ Ada . We extend this by adding adapters. In the language-extension phase we follow Pfeiffer et al. ( 2021 ) in order to extend the model to the target languages. Similar to OSCAR+ Emb , we train a new embedding layer. We further add language adapters at every transformer layer. Given that OSCAR+ is trained on English text, we follow Pfeiffer et al. (2020b) when training English language adapter modules, without replacing the embedding matrix. The transformer weights are frozen while only the newly introduced embeddings and language adapter weights are fine-tuned on unlabeled text data of the language. For the target-task phase, we propose a novel modality-split architecture (see Figure 2 ) inspired by the cross-lingual transfer method of Pfeiffer et al. (2020b) . At each transformer layer, text and image representations are passed through the pretrained multi-head attention (MHA) and feed-forward (FFN) layers. Both image and text representations are also passed through the pre-trained language adapters. Each modality is then passed through modality-specific text and image task adapters and next through a shared multimodal alignment adapter. 8 We follow Pfeiffer et al. (2020b) , freezing transformer, embedding and language adapter weights during training, thus fine-tuning only the task and multimodal aligner adapter weights, together with the prediction head. At inference time, the embedding layer and the language adapters are replaced with the target language weights. 4.2 Multilingual \u2192 Multimodal mBERT Ada . For experiments where we extend a multilingual model to become multimodal, we utilize mBERT (Devlin et al., 2019) . Given that mBERT is able to represent many different languages, it is not necessary to learn new embedding layers for the target languages in the language-extension phase. Instead, we utilize the mBERT-compatible language adapters available on AdapterHub.ml (Pfeiffer et al., 2020a) . 9  For the target-task phase, we follow OSCAR+ for the image representation layer, where image features are combined with their respective positional information and passed through an affine transformation layer. We experiment with the same adapter architecture from Figure 2 , as described for OSCAR+ Ada . We again freeze transformer, embedding and language adapter weights during training. However, in contrast to OSCAR+ * , we randomly initialize and fine-tune the affine image transformation layer. We also fine-tune the task, multimodal aligner adapter weights, and prediction head, all on the GQA task. At inference time, the embedding layer and the language adapters are replaced with the corresponding target language weights. Experimental Setup Language-Extension Phase For OSCAR+ Emb and OSCAR+ Ada , we follow the general setups proposed by Pfeiffer et al. (2020b Pfeiffer et al. ( , 2021)) . We train a new word-piece tokenizer for each target language with a vocabulary size of 30k. We fine-tune the randomly initialized embedding layer, and (for OSCAR+ Ada ) adapter layers for 100k update steps with a batch size of 64 and a learning rate of 1e\u22124. For mBERT Ada , we utilize the language adapters from AdapterHub.ml. Fine-tuning on GQA We follow the standard setup proposed by Li et al. (2020b) , passing the representation of the [CLS] token through a prediction head. We fine-tune the respective models using a cross-entropy loss with labels being all possible answers in the GQA dataset. Following prior work (Li et al., 2020b) , we use a batch size of 192 and train for 5 epochs on the unbalanced GQA training portion. M 3 P. We fine-tune all weights of the pretrained model with a learning rate of 3e\u22125. OSCAR+ Emb , OSCAR+ Ada , and mBERT Ada . We use the pretrained weights and image region features provided by Zhang et al. (2021) . However, we do not pass the object attribute labels as inputs to the model. The object attribute labels are in English and utilizing them in cross-lingual scenarios is non-trivial. 10 We leave this for future work. For the OSCAR+ Emb setting, we fine-tune the transformer weights and the prediction head and freeze the embedding layer, using a learning rate of 3e\u22125. For the OSCAR+ Ada and mBERT Ada settings, we add adapter layers as described in \u00a74.1 and illustrated in Figure 2 . We freeze all pretrained weights-including embeddings, transformer layers, and language adapters-and only fine-tune the newly introduced adapters and the prediction head. For mBERT Ada , we also add and train the affine image transformation layer. We fine-tune the adapterbased models with a learning rate of 1e\u22124. Zero-Shot Cross-Lingual Transfer For zero-shot cross-lingual evaluation, we utilize the model fine-tuned on the GQA training data and evaluate on the multilingual xGQA test data. The model checkpoint that performed best on the English GQA validation data is selected for transfer. M 3 P. As the model is pre-trained to cover, among others, xGQA languages, no additional steps are required for cross-lingual transfer. OSCAR+ Emb . We replace the English embedding layer with the target-language embedding layer. OSCAR+ Ada . We replace the English embedding and language adapter layers with the embedding and adapters layers of the target language. mBERT Ada . We replace the language adapter layers with the adapters layers of the target language. Few-Shot Cross-Lingual Transfer For few-shot cross-lingual scenarios we follow Lauscher et al. (2020) and start from the same finetuned model as for zero-shot transfer (see \u00a75.3). We then fine-tune the same parts of the model as when training on the English training data as in \u00a75.2, but on the small portions of multimodal data available in the target language. We train on the different data splits, consisting of 1, 5, 10, 15, 20, 25, and 48 images (see Table 2 ). We experiment with training for a different number of epochs (5, 10) using different learning rates (1e\u22125 and 5e\u22125 for M 3 P and OSCAR+ Emb , and 5e\u22125 and 1e\u22124 for OSCAR+ Ada and mBERT Ada ). We find that training for longer and with a larger learning rate performed best for all settings. Results and Discussion The main results are presented in Table 3 (zero-shot experiments) and in Table 4 (few-shot). Zero-Shot Cross-Lingual Transfer One of our core findings is that multimodal zeroshot cross-lingual transfer is extremely difficult; we witness an average drop in accuracy of more than 38 points on the target languages of the xGQA dataset compared to English GQA scores (e.g., compare the results with M 3 P). While, as expected, OSCAR+ achieves the best accuracy on the English test set, the massively multilingual models-M 3 P and mBERT-perform considerably better in cross-lingual transfer. 11 This 11 The superior accuracy of OSCAR+ on the English test set is expected as the model was pretrained on large English multimodal data. We find that fine-tuning all transformer weights (OSCAR+ Emb ) achieves slightly better results than only training adapter weights (OSCAR+ Ada ). Our slightly lower scores compared to results by Zhang et al. (2021) can be explained by us (1) not fine-tuning the embedding layer, and (2) not utilizing the attribute labels. Further, previous works that focus only on English add the official validation set to the training set, use the official test-dev set as their dev set, and report their test scores of the official GQA test benchmark test-std for which labels are not available. Our scores follow the training splits, where we use the official test-dev set as the final test set, as described before in \u00a73.   indicates, that joint multilingual pretraining is important and a simple multilingual adapter-based or embedding-based extension of monolingual models achieves inferior cross-lingual performance. While the pretraining method M 3 P achieves better accuracy on the English test set, the adapterbased multimodal extension of mBERT outperforms M 3 P in cross-lingual transfer. We hypothesize that, when fine-tuning all transformer weights on monolingual multimodal data, the cross-lingual alignment breaks within M 3 P. However, this does not happen in adapter-based settings, as the multilingual weights are frozen and thus remain intact.  questions. Query type questions are free-form and thus semantically the most difficult to answer, even in the source language (English). This explains the overall low accuracy across all approaches in zero-shot settings for this question type. This is in stark contrast with the choose-type questions, which the models perform very well on in the source language. However, we report a substantial accuracy drop in zero-shot cross-lingual transfer. This decrease is most likely due to the nature of the question formulation and the modelling implementation. Choose-type questions are formulated such that the answer to the question is a word or phrase which appears in the question, i.e. \"Is it red or blue?\". The label classes, and consequently the prediction head, are constructed as a set of all answers appearing in the dataset. This means that the model learns a distributed repre- sentation of each answer in its final layer. Consequently, in cross-lingual transfer, the model is required to automatically align the question's options \"red\" or \"blue\" (translated in their respective language), with their English latent representation of the model's prediction head. The very low results in this category indicate that this cross-lingual word alignment breaks in zero-shot scenarios. Overall, zero-shot transfer with our proposed multimodal adapter-based extension of mBERT (mBERT Ada ) achieves the best accuracy, with almost 3 points increase over M 3 P and almost 5 points increase over OSCAR+. However, the overall accuracy of all approaches remains low in comparison to the results in English. This indicates that zero-shot multimodal cross-lingual transfer is extremely difficult, most likely due to the misalignment issue between visual and cross-lingual internal representations. To investigate this conjecture further, we run similar tests in few-shot setups, which should potentially mitigate the misalignment issue observed in zero-shot setups. Analysis of Structural Question Types. Few-Shot Cross-Lingual Transfer The main results of few-shot experiments are provided in Table 4 , while the plot illustrating the im-pact of different amounts of training data is shown in Figure 5 . One crucial finding is that, as expected, utilizing an increasing amount of data instances in the target language consistently improves accuracy for all methods. This culminates in an improvement of up to 20 accuracy points when specializing the model with only 48 images in the target language. This indicates that a small number of target-language examples supports the models in partially repairing its internal cross-lingual multimodal alignment. Interestingly, we find that with as little as 5 images, and their corresponding questions, M 3 P begins to outperform mBERT Ada -the best performing zero-shot model. We again analyze the impact of few-shot learning on accuracy across different structural question types, with the results depicted in Figure 4 . The overall accuracy increases across all types compared to zero-shot scenarios (cf., Figure 3 ). However, the most pronounced gains are reported for query and chose-type questions, on which the model performed the worst in zero-shot setups. This implies the improved alignment between latent multimodal and multilingual representations, achieved via fine-tuning the model on a small amount of examples in the target language. Language Transfer We witness cross-lingual transfer capability patterns similar to those shown by previous work, where our models perform best on typologically close languages (Pires et al., 2019; Lauscher et al., 2020) . Our models transfer best to German (de) and Portuguese (pt), both being part of the Indo-European (IE) language family and also sharing the same script (Latin) with the source language English (en). We see a small drop in accuracy for Russian (ru), Indonesian (id), and Chinese (zh) and a larger drop in accuracy for Bengali (bn) and Korean (ko). All of these languages are typologically different to the source language and in most cases do not share the same script. These differences highlight the importance of language diversity in cross-lingual transfer. Our benchmark thus enables experimentation and evaluation of multilingual multimodal models on a representative set of truly typologically diverse languages. Contemporary Work With the recent rise in interest in multilingual vision and language learning, contemporary work has  already further analyzed and extended the proposed xGQA dataset. We provide a brief description and pointers to this work in what follows. Further Analysis. Liu et al. (2022) provide an extensive analysis of multilingual and multimodal models trained on cross-lingual visual question answering, and propose several approaches to mitigate the multilingual misalignment problem discussed in \u00a76.1. Their results suggest that standard approaches taken from text-only cross-lingual transfer scenarios (Pires et al., 2019; Hu et al., 2020) do not leverage the full multilingual capability of the pretrained models. Interestingly, they find that a deeper prediction head does not have any measurable impact on the model's performance in the source language, while at the same time it considerably improves zero-shot transfer results across all target languages. Translated Test Data. Bugliarello et al. (2022) propose the first benchmark for transfer learning across modalities, tasks, and languages, covering visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. They extend the xGQA dataset by providing machine translated testset questions and evaluate state-of-the-art monolingual multimodal models in a translate-test setup. In this setting, they achieve slightly better results. However, the performance remains to fall behind source language performance. The translate-test data can be found at iglue-benchmark.github.io. Conclusion We have proposed xGQA, a first cross-lingual evaluation benchmark for the visual question answering task. xGQA extends the English GQA dataset with development and test data in 7 more typologically diverse languages, covering 5 different scripts. As additional baselines, we have further proposed new adapter-based methods to extend unimodal multilingual models to become multimodal and-viceversa-monolingual multimodal models to become multilingual. Our results have indicated that 1) efficient adapter-based methods slightly outperform the pretrained multilingual multimodal model M 3 P in zero-shot scenarios, but 2) the overall zero-shot cross-lingual transfer yields harsh accuracy drops compared to the English performance for all models in comparison. Further, accuracy can be partially recovered via few-shot learning, where small amounts of training data are available in the target language. However, the large gaps remain, suggesting the inherent complexity of the cross-lingual task despite it being extremely intuitive and easy to solve by (bilingual) humans. We hope that our dataset and error analysis will motivate future work on this task and, more broadly, in the exciting emerging domain of multilingual multimodal representation learning. 6 ) when transferring from English GQA. Average accuracy is reported. Best results for each language and model type are highlighted in bold; mean scores are not averaged over the source language (English). Acknowledgments The Ubiquitous Knowledge Processing Lab acknowledges the financial support of the German Federal Ministry of Education and Research (BMBF) under the promotional reference 13N15897 (MISRIK), and the LOEWE initiative (Hesse, Germany) within the emergenCITY center. Jan-Martin O. Steitz is supported by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. We thank Leonardo F. R. Ribeiro, Ji-Ung Lee, and Chen Liu for insightful feedback and suggestions on a draft of this paper. A Appendix We experiment with different multimodal adapter architectures as illustrated in Figure 6 . In initial experiments we find that splitting the modalities (settings 2-5) outperforms a joint adapter (setting 1). However, a joint \"alignment\" architectures (settings 4-5) outperform settings where we only use modality-specific adapters (settings 2-3). We more thoroughly investigate settings 4-5 and report scores in Table 5 . Interestingly, we find that when only using the language adapter for the textual inputs, cross-lingual accuracy drops for both OSCAR+ and mBERT; The difference is more pronounced for OSCAR+. We speculate that this is due to a latent misalignment of the representation spaces, partly due to the residual connection. Due to the better performance of setting 5 on average, we have reported scores of this architecture in the main paper (as illustrated in Figure 2 ).",
    "abstract": "Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset (Hudson and Manning, 2019) to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and-vice versa-multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M 3 P) in zeroshot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling. 1",
    "countries": [
        "United States",
        "United Kingdom"
    ],
    "languages": [
        "Tamil",
        "Turkish",
        "English",
        "Swahili",
        "Indonesian",
        "Chinese"
    ],
    "numcitedby": "7",
    "year": "2022",
    "month": "May",
    "title": "x{GQA}: Cross-Lingual Visual Question Answering"
}