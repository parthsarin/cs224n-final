{
    "article": "This paper describes a questionanswering system that returns relevant documents and snippets (with particular emphasis on snippets) from a large medical document collection. The system is implemented as part of our participation to Phase A of Task 4b in the 2016 BioASQ Challenge. The proposed system retrieves candidate answer sentences using a cluster-based language model. Then, it re-ranks the retrieved top-n sentences using five independent similarity models based on shallow semantic analysis. The experimental results show that the proposed system is the first to find snippets in batches 2 (MAP 0.0604), 3 (MAP 0.0728), 4 (MAP 0.1182), and 5 (MAP 0.1187). Introduction BioASQ 2016 is the fourth annual BioASQ challenge as an established international competition for large-scale biomedical semantic indexing and question-answering, since 2013 (Tsatsaronis et al., 2015) . The challenge consists of two tasks: Task 4a on large-scale online biomedical semantic indexing and Task 4b on biomedical semantic question-answering. Task 4b is further divided into two phases: Phase A and Phase B. In Phase A, participating systems are required to return a maximum of 10 relevant concepts, documents, snippets, and triples during five batches. Participation in Phase A can be partial, which means that it is acceptable to participate in only some of the batches and to return only relevant documents without snippets, triples, and concepts. This paper 2 Question-answering system based on sentence retrieval and re-ranking techniques KSAnswer consists of two submodules: A retrieval model for finding candidate answer sentences from a large medical collection and a reranking model for determining the final answer among the retrieved candidate answer sentences. Sentence retrieval model Prior to indexing documents, KSAnswer first splits documents into a sequence of sentences using LingPipe (Baldwin et al., 2003) . Then, it performs morphological analysis of the sentences and extracts content words (i.e., proper noun, common noun, verb, number, and so on) from the sentences. This is followed by stemming of content words except proper nouns using Porter Stemmer (Porter, 1980) . Finally, KSAnswer uses the stemmed content words and the proper nouns as indexing terms. For cluster-based sentence retrieval, KSAnswer generates two types of indexing units from the document collection comprising full data sets of PubMed journals: a sentence trigram unit and a document unit. The sentence trigram unit consists of an indexing target sentence and its context sentences (the previous and the next sentences) to address the lexical disagreement between a query and an indexed sentence. If a document consists of three sentences, KSAnswer generates three sentence trigrams (NULL-1 st sentence-2 nd sentence, 1 st sentence-2 nd sentence-3 rd sentence, Figure 1 : Relationship between a query and an answer sentence 2 nd sentence-3 rd sentence-NULL). The document unit consists of a title sentence and abstract sentences. The document unit assists in addressing the lexical disagreement between a query and a sentence trigram. Then, KSAnswer performs indexing of each unit and constructs two indexing databases using Lucene 4.0.0 (Bia\u0142ecki et al., 2012) . To rank candidate answer sentences, KSAnswer uses a cluster-based language model (Liu et al., 2004; Merkel et al., 2007) , as shown in Eq. ( 1 ): Sim IR (Q, S) =\u221d Sim tri (Q, T ) + (1\u2212 \u221d)Sim doc (Q, D) (1) where Sim tri (Q, T ) is the similarity of the language model between the query Q and the sentence trigram T in the document D. Then, Sim doc (Q, D) is the similarity of the language model between the query Q and the document D. The weighting parameter \u221d has a value between 0 and 1. Finally, Sim IR (Q, S) returns similarities between the query Q and the indexing target sentence S, which is located in the middle of the sentence trigram T . Sentence re-ranking model Prior to re-ranking of candidate answer sentences, KSAnswer selects top-n retrieved sentences and normalizes their similarities, as shown in Eq. ( 2 ): Sim IR (Q, S) = Sim IR (Q, S) \u2212 m \u03c3 (2) where m and \u03c3 are the average and standard deviation of similarity scores of top-n retrieved sentences, respectively. KSAnswer re-ranks the top-n retrieved sentences using five independent similarity models, namely,   Eq. ( 3 ) shows the similarity scores between a query and each top-n retrieved sentences for reranking. Sim SN T (Q, S), Sim EM B (Q, S), Sim EAT (Q, S), Sim F OCU S (Q, S), and Sim M E (Q, S). Sim SN T (Q, S) ReSim(Q, S) = \u03b1Sim IR +(1 \u2212 \u03b1){\u03b2Sim snt (Q, S) +(1 \u2212 \u03b2) 4 i=1 \u03b3 i Sim i sem (Q, S)}, where 0 \u2264 \u03b1 \u2264 1, 0 \u2264 \u03b2 \u2264 1, 4 i=1 \u03b3 i = 1 (3) where Sim i sem (Q, S) is the ith similarity model among Sim EM B (Q, S), Sim EAT (Q, S), Sim F OCU S (Q, S), and Sim M E (Q, S). Then, \u221d, \u03b2, and \u03b3 are the weighting parameters set by experiments. The word-based similarity models (i.e., models for calculating similarities between words in Q and S), such as Sim SN T (Q, S), Sim F OCU S (Q, S), and Sim M E (Q, S), are calculated using the well-known Okapi BM25 (Robertson et al., 1999) . Then, the category-based similarity model (i.e., a model for calculating similarities between category names in Q and S), Sim EAT (Q, S), is calculated using OR similarity of the Paice model (Paice, 1984) , as shown in Eq. ( 4 ). Sim EAT (Q, S) = n i=1 (r i\u22121 w i ) n i=1 r i\u22121 , where 0 \u2264 r \u2264 1 and w i s are considered in descending order (4) In Eq. ( 4 ), w i is a TF\u2022IDF value of the ith word in ME's of S that have the same semantic category with EAT of Q. Finally, the vector-based similarity model, Sim EM B (Q, S), is calculated using a feed-forward neural network with one hidden layer (Svozil et al., 1997) , as shown in Figure 2 . The feed-forward neural network uses the sentence embedding vectors of Q and S as input values and uses a degree of relevance (from 0 to 1) between the two sentence embedding vectors as an output value. It is trained using gold standard answers as relevant snippets and by using top-n retrieved sentences except gold standard answers as irrelevant snippets. Experiments Experimental setting We indexed the full data set of PubMed journals using Lucene 4.0.0. The number of document units was 12,208,342 and the number of sentence trigram units was 99,911,516. The language model parameters (\u00b5 values) for the document and sentence trigram units were set to 500 and 100, respectively. The weighting parameter \u221d in Eq. ( 1 ) was 0.8. Then, the weighting parameters \u221d, \u03b2, and \u03b3 i in Eq. (3) were 0.5, 0.9, and 0.3, respectively. Experimental results In Phase A of Task 4b, our best submission was the first to find snippets in batches 2, 3, 4, and 5. In batch 1, we indexed the limit set of PubMed and achieved the second place in finding snippets. Table 1 shows the best performances of KSAnswer. The parenthesized values are informal performances that are calculated using gold standard answers for each batch. In an additional experiment, we found that the degree of the sub-model importance in the reranking model is as follows: Sim SN T (Q,S) >> Sim EAT (Q,S) > Sim F OCU S (Q,S) \u2248 Sim M E (Q,S) \u2248 Sim EM B (Q,S) Conclusion We proposed a question-answering system for finding candidate answer snippets from a large medical document collection. The proposed system retrieves candidate answer sentences using cluster-based language model. Then, it re-ranks top-n retrieved sentences using various similarity models based on shallow semantic analysis of sentences. In Phase A of task 4b, the proposed system showed excellent performance by being the first to find snippets in batches 2,3,4 and 5. Acknowledgments This research was supported by LG Electronics. It was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (2013R1A1A4A01005074).",
    "abstract": "This paper describes a questionanswering system that returns relevant documents and snippets (with particular emphasis on snippets) from a large medical document collection. The system is implemented as part of our participation to Phase A of Task 4b in the 2016 BioASQ Challenge. The proposed system retrieves candidate answer sentences using a cluster-based language model. Then, it re-ranks the retrieved top-n sentences using five independent similarity models based on shallow semantic analysis. The experimental results show that the proposed system is the first to find snippets in batches 2 (MAP 0.0604), 3 (MAP 0.0728), 4 (MAP 0.1182), and 5 (MAP 0.1187).",
    "countries": [
        "South Korea"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "7",
    "year": "2016",
    "month": "August",
    "title": "{KSA}nswer: Question-answering System of Kangwon National University and Sogang University in the 2016 {B}io{ASQ} Challenge"
}