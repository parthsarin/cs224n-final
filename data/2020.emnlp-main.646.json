{
    "article": "Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: \"the major cities\", \"the capital cities\" and \"two European cities\". Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of PERSON, ORGANIZATION, and LO-CATION named entities. 1 The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate. Introduction Abstractly speaking, abstraction can be defined as the process of deriving general concepts from specific instances. In automatic summarization, however, \"abstractive\" summarization often means any type of rewriting of words in some source document into an output summary. Concretely, recent summarization datasets including XSum (Narayan Input Entities Franc \u00b8ois Bayrou, Nicolas Sarkozy, S\u00e9gol\u00e8ne Royal Document Context Franc \u00b8ois Bayrou, Nicolas Sarkozy, and S\u00e9gol\u00e8ne Royal are the main contenders in the French presidential elections. Possible Aggregations \u2022 the French politicians \u2022 the French presidential candidates \u2022 the politicians Table 1 : An example of semantic entity aggregation. The input consists of a tuple of named entities, a situational (document) context, and background information about the entities (not shown here). The expected output is an aggregation of the tuple of entities. et al., 2018) and NEWSROOM (Grusky et al., 2018) quantify the degree of abstractiveness of a summary in terms of its novel N-grams. While such a surface-level definition of abstractiveness is certainly useful and convenient, it is nevertheless only a proxy for abstraction in the broader sense which concerns semantic generalization. We argue that it is important to also focus explicitly on semantic abstraction, as this capability is required for more difficult types of summarization which are out of reach of current methods. For example, generating a plot summary of a novel might require describing sequences of events using one sentence. Writing a survey of a scientific field would require categorizing papers and ideas, and being able to refer to them as a whole. Outside of domain-specific settings such as opinion summarization (Ganesan et al., 2010; Gerani et al., 2014, inter alia) , and tasks such as sentence fusion (Barzilay and McKeown, 2005) , there has been little work focusing on semantic generalization and abstraction. In this paper, we start to tackle this issue by focusing on the specific task of semantic aggregation of entities; i.e., how to refer to a tuple of named entities using a noun phrase instead of enumerating them (See Table 1 for an example). We define a task to evaluate summarization models on semantic entity aggregation, which we call TESA (A Task in Entity Semantic Aggregation). In TESA, a system is presented with a list of named entities in an original textual context, and it must produce a non-enumerating noun phrase which refers to the designated entities. Solving this task requires finding a semantic link between all the entities in the list (e.g., London and Paris are cities of considerable sizes), then using this information to generate a noun phrase (e.g., \"the major cities\"). We introduce an accompanying dataset of entities in context drawn from the New York Times corpus (Sandhaus, 2008) , and their aggregations which were written by crowd workers. Our dataset contains 5.3K aggregation expressions. Each example, contains a tuple of PERSON, ORGANIZATION or LOCATION named entities, a paragraph context from an NYT article discussing the entities, and background information about entities in the form of summary snippets from Wikipedia. We also introduce the first models for the TESA task which are based on an encoder-decoder system pretrained for abstractive summarization, BART (Lewis et al., 2019) . We present two ways of fine-tuning BART to TESA, either in a discriminative or in a generative fashion, and compare them against simpler statistical and frequency-based methods. The simple classifier achieves decent results on TESA. It is however outperformed by a wide margin by BART, when fine-tuned on our task in a discriminative manner. When fine-tuned as a generative model, BART yields similar performance as the simple classifier. Yet, the generative model is able to freely generate entity aggregations with diversity and quality, despite some factual inconsistencies. Related work Abstractive summarizers have gained prominence with the popularization of RNNs (Sutskever et al., 2014; Nallapati et al., 2016) , and more recently Transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019) . Several abstractive models have achieved state-of-the-art performances on benchmark summarization datasets in terms of ROUGE, including ProphetNet (Yan et al., 2020) , PEGA-SUS (Zhang et al., 2019) and BART (Lewis et al., 2019) . Recent work has also focused on specific issues such as preventing inappropriate repetition (Kry\u015bci\u0144ski et al., 2018) , word-level rewriting, and evaluating factual consistency (Kry\u015bci\u0144ski et al., 2019; Maynez et al., 2020) . Abstraction is critical for certain domains and applications, but has not been thoroughly explored in many. For example, in scientific article summarization the particular structure and length of scientific articles make extractive techniques much easier to apply (Agarwal et al., 2011) , therefore abstractive summarizers (Lloret et al., 2013) remain a minority. In opinion summarization, there have been abstractive systems that leverage cues specific to this task, such as redundancy in opinions (Ganesan et al., 2010) and specific discourse structures (Gerani et al., 2014) . As abstractive systems have become strong in terms of generation capabilities, the time is apt to examine issues in semantic abstraction that could be useful in many summarization domains and tasks. Our work is a step in this direction. Our proposed entity aggregation task is related to referring expression generation (REG). REG is concerned with determining the form and content that entity references should take during generation (Krahmer and van Deemter, 2012; Castro Ferreira et al., 2018; Cao and Cheung, 2019) . It emphasizes finding the right distinguishing characteristics of the intended referent or referents. Our work can be seen as a specific REG task that focuses on semantically abstracting multiple named entities. Our work is also related to coreference resolution, especially those that examine multi-antecedent resolution (Burga et al., 2016; Vala et al., 2016) , an inverse problem to ours. To the best of our knowledge, no previous work has directly addressed entity aggregation. The TESA dataset We used the New York Times (NYT) Annotated Corpus (Sandhaus, 2008) to extract tuples of named entities and their document context. The NYT corpus contains high-quality metadata listing the salient named entities mentioned in each article. We form our tuples from entities tagged in the metadata for the same article. We refer to a tuple of entities and its associated information as an aggregatable instance. We first describe the components of an aggregatable instance in more detail. Then, we describe our data extraction and crowd-sourcing experiments. An aggregatable instance The starting point of an aggregatable instance is the tuple of named entities which should be aggregated and the type of its entities (e.g., PERSON). As we aim for contextual entity aggregations, an aggregatable instance also contains a document context; i.e., a passage from a document in which all the entities are mentioned. To provide additional background knowledge, we also include introductory summaries for the entities taken from Wikipedia. An example of aggregatable instance, as presented to the annotators, is in Figure 1 . For more examples, see Table 8 in the appendix. Data extraction While we could have gathered naturally occurring entity aggregations, work on multi-antecedent coreference resolution is still nascent, and our initial attempts to define heuristic methods to extract entity aggregations were very noisy. We instead used crowd-sourcing to gather human-generated aggregations from sets of entities. We used the 2006 and 2007 portions of the New York Times corpus. We started with the editorial metadata which tags salient named entities in each article. These are entities we believe are likely to be included in a summary. We filtered the entity tuples to remove those that are unlikely to be naturally aggregatable using the following two constraints. First, the entities should have the same type (PERSON, LOCATION, or ORGANIZATION in this corpus). Second, the entities should be mentioned close together, within a span of consecutive sentences of the same length as the size of the tuple of entities (e.g., three consecutive sentences for three entities). We also selected those entity tuples that are mentioned together in the abstract of an article. To extract the document context, we extracted both the title of the article and the span of sentences which mentions the entities. If the same entity tuple is mentioned in different qualifying sentence spans in the same article, they would be extracted as different aggregatable instances. As for the background information, we extracted an excerpt of each entity's Wikipedia article, using the first paragraph of the article if it exists, up to 600 tokens. We used the entity name to identify  its Wikipedia page 2 , and, in case of ambiguous or incorrect linking, we corrected it manually when possible, or discarded it. After extraction, we sampled 2,100 instances uniformly at random for annotation. A tuple contains between 2 and 6 entities, for an average of 2.4. Data Annotation We used Amazon Mechanical Turk to collect entity aggregations. Annotators were asked to generate aggregations given information about an aggregatable instance. For each instance, we showed the same information as described above, including the mentions of the entities in context, and a link to the Wikipedia pages of the entities. Some of the instructions given to the annotators and examples of the annotation layout are in Figures 1, 2 The entity tuple, document context, Wikipedia background information are presented to annotators, alongside a prompt (see Figure 1 ). For the PERSON entities in our example, this prompt is \"In this article, Franc \u00b8ois Bayrou, Nicolas Sarkozy and S\u00e9gol\u00e8ne Royal are discussed. The three people...\" Annotators were asked to replace the phrase \"The three people\" with a relevant one referring to the entities. The prompt serves to prime the annotator to produce a fluent and comprehensive aggregation  covering all the entities. For other named entity types, the prompt is changed accordingly. While simple, we found this prompt to be rather effective in the collection process. We also presented detailed examples (see Figure 2 ) explaining the desired aggregations. Annotators were asked not to use generic aggregations involving only the entities' type (e.g., \"the three people\") and to avoid using \"and\", as it would often imply an enumeration. For each of the 2,100 aggregatable instances, three different annotators were asked to provide an annotation. In each annotation, an annotator could provide between zero (meaning the instance is not aggregatable) and two aggregations. The aggregations produced for the example of Figure 1 by the three annotators are below: Annotator 1 \u2022 french politicians Annotator 2 \u2022 the French politicians \u2022 the French presidential candidates Annotator 3 \u2022 the politicians We discarded instances that at least two of the three annotators considered as 'not aggregatable'. In addition, we discarded those annotations that did not conform to our instructions, and annotations from workers who performed less than five annotations. Finally, we post-processed the aggregations, removing determiners, numerical expressions and standardized the casing (e.g., \"The two cities\" became \"cities\"). Table 2 presents statistics on the size of the data collected and the final dataset. Data Splits We split the dataset into training, validation, and test sets using a 2:1:1 ratio, resulting in 858/430/430 aggregatable instances in each set, respectively (corresponding to 20592/10320/10320 ranking candidates, respectively). The entities in our dataset are quite diverse. In the validation and test sets, 29% and 30% of the aggregatable instances respectively have a set of input entities which do not overlap with entities in the training set at all. On average, each aggregatable instance has 2.7 different aggregations. 4 The TESA task Task Definition We frame TESA as a ranking task where, given an aggregatable instance as input, models must rank a list of candidates according to their plausibility as an aggregation of the input entities (in context). We choose a discriminative approach to avoid relying on word-overlap metrics, and we opt for a ranking task set-up to avoid classification between heavily imbalanced classes, as the number of gold standards remains limited. In this set-up, generative models can also be evaluated. In our experiments, the list of candidate aggregations contains 24 candidates in total, including the gold-standard, correct aggregations generated by the human annotators, as well as a list of negative candidates which serve as distractors. The candidates' number is chosen to yield approximately 10 times more negative candidates than gold standards. Negative candidates are sampled uniformly at random from other aggregatable instances sharing the same named entity type. An example of TESA's tasks is available in Table 3; for more examples, see Table 9 in the appendix. Evaluation Measures We evaluate the models' performances using three widely used ranking performance measures. Let rank(i) be the rank of candidate i, G be the set of gold-standard candidates in a ranking and R(n) be the set of candidates retrieved up to and including position n. Then, for an aggregatable instance: Average precision. AP = 1 |G| i\u2208G |G \u2229 R(rank(i))| |R(rank(i))| (1) Recall at 10. R@10 = 1 |G| |G \u2229 R(10)| (2) Reciprocal rank. RR = 1 min i\u2208G rank(i) (3) We report the mean of these values across all instances in the test set (MAP, R@10, MRR). We chose these measures because they provide different perspectives on the evaluation results. Recall  at 10 captures the models' ability to rank correct aggregations as promising or neutral at worst. Reciprocal rank focuses solely on the best ranked correct aggregation. Models We tested several simple baselines as well as models adapted from current work on abstractive summarization on TESA. Simple Baselines All the baselines and models are given as input an aggregatable instance and a list of candidates to rank with the same entity type as the aggregatable instance. The first two baselines are agnostic to the aggregatable instance: Random. This baseline produces a random ordering of the candidate entities. Frequency. This baseline ranks the candidates according to their frequency as a correct aggregation in the training set. Logistic Regression We defined a number of statistical and linguistically informed features, which we extracted from each candidate aggregation and its aggregatable instance's context and background information. These 15 features include: \u2022 the count of the \"frequency\" baseline, \u2022 the number of common tokens (with repetition) between the candidate and the union of the background information, \u2022 the size of the word overlap between a candidate and the intersection of the entities' background information, \u2022 the cosine similarity between the average word embeddings of the candidate and of the context. We detail these features in Appendix C. We trained a binary logistic regression using this representation, to discriminate between the goldstandard aggregations and the negative candidates. We used the model's predictive probability for the gold-standard class to produce a ranking over the candidate list. BART-based models We tested BART (Lewis et al., 2019) as a representative model of recent high-performance abstractive summarization systems based on an encoderdecoder architecture with a Transformer backbone. We compared three versions of BART, which differ based on whether and how they are fine-tuned on TESA. Pre-trained BART. We applied an existing pretrained version of BART in a generative set-up without fine-tuning. We formatted each aggregatable instance into a single sequence of tokens by concatenating the fields of the aggregatable instances in the following order: background information, context (title of the article and excerpt), and entity names. An example of such input can be seen in Table 3 . We fed this as input to BART's encoder, and we evaluated the probability of each candidate aggregation to be generated autoregressively by the decoder. We used these probabilities to rank the candidates. Generative BART. This version is similar to the above, but we fine-tuned BART on TESA, considering each correct aggregation as a separate target, and training the model to generate each target given the corresponding aggregatable instance. For the aggregatable instances, we used the same input format as above. We did not add any form of separation tokens, as our initial experiments showed that they slightly hurt the performance. Discriminative BART. Finally, we fine-tuned BART discriminatively as a classifier. During finetuning, we consider each candidate and its aggregatable instance as a separate sample, and the model was trained on these samples to discriminate the correct aggregations from the negative candidates. At test time, we rank the candidates by their probability of being the correct aggregation according to the classifier. Again, we did not add any separation tokens, as it did not improve the performance. The main advantage of this approach over the previous one is that it leverages the set-up of TESA as a ranking task, and the model is exposed to both correct and incorrect aggregations during training (which, on the other hand, makes it more computationally expensive). By contrast, generative BART only sees correct ones. We thus expect the discriminative model to produce higher performance. However, this comes at a cost, as this approach cannot generate freely an aggregation, but only retrieve one from a set of candidates. For all three versions above, we built upon code that is available through fairseq (Ott et al., 2019) . We use the version of BART pre-trained on the CNN/DailyMail dataset. The choice of hyperparameters is described in Appendix D. Results The results of the models on TESA's test set are presented in Table 4 . We see that most models out-Method MAP R@10 MRR Random baseline 0.222 0.442 0.289 Frequency baseline 0.570 0.655 0.761 Logistic regression 0.700 0.863 0.840 Pre-trained BART 0.389 0.682 0.505 Generative BART 0.701 0.903 0.840 Discriminative BART 0.895 0.991 0.954 Table 5 : Results of the ablation study. We report the mean average precision differences between the ablated system and the full model's performance (in parentheses) on TESA. Negative numbers mean the performance of the full model is higher. perform the frequency baseline, except pre-trained BART. Fine-tuning BART on TESA increased its performance significantly, especially if done discriminatively. Discriminative BART achieves the best results. Its high performance can be mitigated by our choice of ranking only 24 candidates, which makes unlikely confusing negative candidates. Ablation Study To understand the importance of the different components of the input for this task, we performed an ablation study, where we removed selected parts of the input: without the background information (context, entities), without context (info., entities) and with only the names of the entities (entities). We fine-tuned generative and discriminative BART on these modified datasets. The hyperparameters used are described in Appendix D. We report the mean average precision results, which are representative of the other measures, in Table 5 . Models perform best when all information is available, which validates our choice of input format. The background information seems to be more important than the context, as removing the context leads to the smallest drop in average precision. Interestingly, models perform quite well when given only the entities' names, though the performance gap is still quite significant. 7 : Aggregations generated by generative BART on the running example. The model's encoder is fed an aggregatable instance, and the decoder generates autoregressivly the aggregations without constraint. We show the input entities, and the 10 aggregations retrieved by the beam search, ranked according to their likelihoods. If a generated aggregation matches a gold standard (except for capital letters), it is in bold; the generated examples probabilities are in brackets. Qualitative analysis We compare the two best-performing models: generative and discriminative BART. In Table 6, we present an example of their results on a ranking task from TESA's test set. In general, the discriminative approach performs well, is robust and the negative candidates ranked at high positions are quite coherent (e.g., \"former presidents\" and \"police officers\"). On the other hand, generative BART performs quite well on the ranking task, but is far less robust and its negative candidates ranked at high positions are more intriguing (e.g., \"new york mafiosos\" and \"american men\"), which seems to indicate a poorer understanding of the aggregatable instance. Besides, we show some aggregations generated by the generative approach in Table 7 . Qualitatively speaking, the generated samples are quite interesting as many of them are accurate and have a diverse vocabulary (e.g., \"politicians\", \"figures\", \"candidates\", \"leader\"). However, some samples are factually inconsistent (e.g., \"american politicians\") which seems to indicate that the model does not have a deep understanding of relevant semantic concepts (e.g., nationalities cannot be substituted for each other). For other examples, including some specifically chosen as the models failed on them, see Tables 10-13 in the appendix. Conclusion and future work We have proposed TESA, a novel task and an accompanying dataset of crowd-sourced entity aggregations in context. TESA directly measures the ability of summarizers to abstract at a semantic level. We have compared several baseline models and models adapted from existing abstractive summarizers on TESA, and find that a discriminative fine-tuning achieves the best performance, though this model inherently cannot generate aggregations. In future work, we would like to expand the domains covered by our dataset, which is biased towards topics found in the source corpus, such as politics. Another important direction is to investigate how to integrate the ability to aggregate entities derived from training on TESA into an abstractive summarizer. This would require models to tackle another challenging issue which we have not addressed: which set of entities should a model aggregate in the first place? inspired by BART's finetuning on summarization datasets described here. We kept the model's parameters of the experiment and the epoch maximizing the average precision of the validation set. We performed a grid search on the following hyperparameters: \u2022 lr in {3e-6, 5e-6, 1e-5, 2e-5, 3e-5}, \u2022 max-tokens in {1024, 2048}. We used the following final hyperparameters: \u2022 lr=5e-06, \u2022 max-tokens=1024, \u2022 max-epochs=6, \u2022 update-freq=1, \u2022 total-num-updates=4974, \u2022 warmup-updates=149. total-num-updates was determined empirically as max-epochs\u2022updates-per-epoch update-freq and warmup-updates was chosen as 3% of total-num-updates. During the hyperparameter search we used total-num-updates=4974, 375 and warmup-updates=149, 67 for max-tokens=1024, 2048 respectively. To evaluate candidates' likelihood and to generate aggregations, we modified slightly the code of Ott et al. (2019) to compute all hypotheses of the beam search (not only the most probable one) and we used the same parameters as in Appendix D.2. We ran our experiments on a single V100 GPU with 32GB of memory with the fp16 option, and an experiment took typically 1 hour. This model had 401 million parameters, all of them being trained. For the ablation study, we used the final hyperparameters, except for total-num-updates and warmup-updates which were determined empirically as above. We added max-sentences=16 for the \"entities\" ablation experiment. D.4 Discriminative BART For this approach, our choice of hyperparameter search and final hyperparameters was largely inspired by BART's finetuning on GLUE tasks (Wang et al., 2018) described here. We kept the model's parameters of the experiment and the epoch maximizing the average precision of the validation set. We performed a grid search on the following hyperparameters: \u2022 lr in {5e-6, 1e-5, 2e-5, 3e-5}, \u2022 max-sentences in {4, 8, 16}. We used the following final hyperparameters: \u2022 lr=2e-5, \u2022 max-sentences=8, \u2022 num-classes=2, \u2022 max-epochs=6, \u2022 total-num-updates=18180, \u2022 warmup-updates=1090. total-num-updates was determined empirically as max-epochs\u2022updates-per-epoch update-freq and warmup-updates was chosen as 6% of total-num-updates. During the hyperparameter search we used total-num-updates=30888, 18180, 16254 and warmup-updates=1853, 1090, 975 for max-sentences=4, 8, 16 respectively. We ran each experiment on a single V100 GPU with 32GB of memory with the memory-efficient-fp16 option, and an experiment took typically 5 hours. This model had 401 million parameters, all of them being trained. For the ablation study, we used the following hyperparameters, as they yielded very similar performances: \u2022 lr=1e-5, \u2022 max-tokens=1024, \u2022 max-sentences=8, \u2022 update-freq=4, \u2022 max-epochs=5. total-num-updates and warmup-updates were determined empirically as above. Aggregatable instance Aggregations Input \u2022 The technology companies Annotation 2 \u2022 multinational corporations Annotation 3 \u2022 the multinational corporations Acknowledgements We would like to thank Ido Dagan for useful initial discussions about this work, Jingyun Liu, whose automatic preprocessing of the New York Time Corpus was a precious help, and Jos\u00e9 \u00c1ngel Gonz\u00e1lez Barba, who helped us with analysis and proofreading. We also wish to mention our two pilot annotators. Finally, we would like to thank our anonymous reviewers for their kind and helpful comments. This work was supported by the Natural Sciences and Engineering Research Council of Canada, and the last author is supported by a Canada CIFAR AI Chair. Appendix A Detailed examples In Tables 8, 9 , 10 and 11, we present several examples following the examples presented in Figure 1 and Tables 3, 6 and 7 respectively. In Tables 12 and 13 , we present two examples of aggregatable instances where BART-based models performed poorly. Appendix B Human-sourcing set-up To ensure maximal reproducibility, we provide here some details regarding the collection of the human annotations. For the task set-up, we used the Amazon Mechanical Turk website. We present in details the layout of the annotation process in Figure 3 , and its instructions in Figures 4, 5 and 6 . Appendix C Linguistically informed features For the representation of an aggregatable instance to train the logistic regression, we used the following features: \u2022 count of the \"frequency\" baseline, \u2022 number of common tokens (with repetition) between the candidate and the union of the entities' background information, \u2022 size of the word overlap between a candidate and the union of the entities' background information, \u2022 size of the word overlap between a candidate and the intersection of the entities' background information, \u2022 number of entities whose background information words are overlapping the candidate's words, \u2022 cosine similarity between the average token embeddings of the candidate and the union of the entities' background information, \u2022 cosine similarity between the average word embeddings of the candidate and the intersection of the entities' background information, \u2022 number of common tokens (with repetition) between the candidate and the context, \u2022 size of the word overlap between a candidate and the context, \u2022 cosine similarity between the average token embeddings of the candidate and the context, \u2022 number of common tokens (with repetition) between the candidate and the union of the entities' background information, the context, and the names of the entities, \u2022 size of the word overlap between a candidate and the union of the entities' background information, the context, and the names of the entities, \u2022 size of the word overlap between a candidate and the union of the context and the intersection of the entities' background information, \u2022 cosine similarity between the average token embeddings of the candidate and the union of the entities' background information, the context, and the names of the entities, \u2022 cosine similarity between the average word embeddings of the candidate and the union of the context and the intersection of the entities' background information. During the feature extraction, we removed any capitalization and any punctuation. We removed the stop-words from the candidates' tokens. We removed the stop-words and we lemmatized the tokens of the context and of the background information. Appendix D Hyperparameters In the following, we describe our choice of hyperparameters for each model, as well as any eventual hyperparameter search. D.1 Logistic regression We used a simple logistic regression for binary classification. The model has 32 parameters, and we use Adam optimizer, a learning rate of 3e \u2212 3 and the cross entropy loss. We ran the experiment for 50 epochs, which took typically 15 minutes on a CPU, and we kept the model's parameters of the epoch maximizing the average precision of the validation set. D.2 Pre-trained BART To evaluate pre-trained BART, we used the following parameters to evaluate candidates' likelihood: \u2022 beam=10, \u2022 lenpen=1.0, \u2022 max len b=100, \u2022 min len=1, \u2022 no repeat ngram size=2. This model had 401 million parameters, none of them was trained in this approach. D.3 Generative BART To finetune generative BART, our choice of hyperparameter search and final hyperparameters was Appendix E Validation results For reproducibility purposes, we include in Table 14 the validation scores corresponding to the main results, in Table 4 .",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.9999549868231234,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section of the article specifically thanks the Natural Sciences and Engineering Research Council of Canada for supporting the work, and mentions that the last author is supported by a Canada CIFAR AI Chair. This indicates funding from a research agency and potentially a foundation, given CIFAR's status as a charitable organization. There is no mention of defense, corporate funding, or an explicit statement indicating no funding from any sources.",
    "abstract": "Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: \"the major cities\", \"the capital cities\" and \"two European cities\". Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of PERSON, ORGANIZATION, and LO-CATION named entities. 1 The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.",
    "countries": [
        "Canada"
    ],
    "languages": [
        "French"
    ],
    "numcitedby": 1,
    "year": 2020,
    "month": "November",
    "title": "{TESA}: A {T}ask in {E}ntity {S}emantic {A}ggregation for Abstractive Summarization",
    "values": {
        "building on past work": "We argue that it is important to also focus explicitly on semantic abstraction, as this capability is required for more difficult types of summarization which are out of reach of current methods. Our work is a step in this direction. Our proposed entity aggregation task is related to referring expression generation (REG). Our work can be seen as a specific REG task that focuses on semantically abstracting multiple named entities. Our work is also related to coreference resolution, especially those that examine multi-antecedent resolution (Burga et al., 2016; Vala et al., 2016) , an inverse problem to ours.",
        "performance": "Fine-tuning BART on TESA increased its performance significantly, especially if done discriminatively. Discriminative BART achieves the best results.",
        "reproducibility": " We used the New York Times (NYT) Annotated Corpus (Sandhaus, 2008) to extract tuples of named entities and their document context. The NYT corpus contains high-quality metadata listing the salient named entities mentioned in each article.  In each annotation, an annotator could provide between zero (meaning the instance is not aggregatable) and two aggregations.  Finally, we post-processed the aggregations, removing determiners, numerical expressions and standardized the casing (e.g., \"The two cities\" became \"cities\").   To ensure maximal reproducibility, we provide here some details regarding the collection of the human annotations."
    }
}