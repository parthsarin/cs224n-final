{
    "framing_question": "Who are the intended users of the work described in the paper, if any?",
    "response": "The intended users of the work described in the paper are researchers and practitioners working in the field of natural language processing (NLP), particularly those interested in implicit discourse relation classification. This includes researchers and developers working on improving the performance of NLP models, as well as those applying NLP models to real-world tasks such as machine translation, natural language inference, summarization, and sentiment analysis, to name a few. The paper provides a comprehensive investigation of how to adapt the BERT neural network model to improve its performance on the task of implicit discourse relation classification, and the findings and techniques described in the paper can be valuable for those working in this area.",
    "article": "BERT, a neural network-based language model pre-trained on large corpora, is a breakthrough in natural language processing, significantly outperforming previous state-of-the-art models in numerous tasks. However, there have been few reports on its application to implicit discourse relation classification, and it is not clear how BERT is best adapted to the task. In this paper, we test three methods of adaptation. (1) We perform additional pre-training on text tailored to discourse classification. (2) In expectation of knowledge transfer from explicit discourse relations to implicit discourse relations, we add a task named explicit connective prediction at the additional pre-training step. (3) To exploit implicit connectives given by treebank annotators, we add a task named implicit connective prediction at the fine-tuning step. We demonstrate that these three techniques can be combined straightforwardly in a single training pipeline. Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not. Introduction Discourse relation classification, a task of recognizeing the semantic relations between two text spans, is beneficial for many NLP tasks including machine translation (Meyer et al., 2015) , natural language inference (Pan et al., 2018) , summarization (Isonuma et al., 2019) , and sentiment analysis (Saito et al., 2019) . In the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) , discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations have strong cues named discourse connectives such as \"because\" and \"however\", while implicit relations lack these cues. For this reason, the recognition of implicit relations is the bottleneck of discourse relation classification (Xue et al., 2016; Dai and Huang, 2019) . In this paper, we investigate how to improve the performance of implicit discourse relation classification by applying BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) . BERT is a Transformer-based neural network architecture with specialized training procedures. It is pre-trained on large corpora like Wikipedia and BooksCorpus, and fine-tuned using task-specific datasets to transfer the pre-trained representations to downstream tasks. Although BERT is conceptually simple, it significantly outperforms previous stateof-the-art models in many natural language processing tasks such as reading comprehension (Devlin et al., 2018) , syntactic analysis (Goldberg, 2019) , and sentiment analysis (Xu et al., 2019) . In implicit discourse relation classification, Nie et al. (2019) and Shi and Demberg (2019b) reported that BERT significantly outperformed previous state-ofthe-art models. Considering the broader context of research in this area, however, we expect additional improvement to be achieved with task-specific adaptation. Table 1 summarizes recent studies and techniques employed by them. It is evident that a comprehensive investigation is needed to answer the question: How is BERT best adapted to the task? We examine three adaptation methods in this paper. The first one is to exploit a large amount of unlabeled data from same domain text (referred to as Domain text in Table 1 ). In the context of BERT, a simple way to do this is to perform additional pre-training on the domain text (Shi and Demberg, 2019b) , but the domain can be more specific to this task. In fact, Rutherford and Xue (2015) automatically collected explicit argument pairs from an unlabeled corpus. In the pre-BERT era, they had no choice but to forcibly convert them into pseudo-training data for implicit discourse relation classification. Now, such explicit argument pairs can be used for BERT's additional pre-training. The second technique is a more direct use of explicit argument pairs (referred to as explicit connective prediction in Table 1 ). Explicit argument pairs have, by definition, (explicit) discourse connectives. Training a model to predict explicit connectives may help it learn the discourse relations (Wu et al., 2017) . We call this task explicit connective prediction. For BERT, Nie et al. (2019) inserted this task between pre-training and fine-tuning (additional pre-training). While their additional pre-training only covers explicit connective prediction (referred to as single-task pre-training in Table 1 ), but a more straightforward way is to do this together with BERT's ordinary pre-training tasks (multi-task pre-training). The third technique is to exploit implicit connectives (referred to as implicit connective prediction in Table 1 ). In PDTB, annotators inserted an implicit connective between an implicit argument pair to facilitate consistent annotation. Implicit connectives were exploited by Zhou et al. (2010) , Qin et al. (2017) , and Shi and Demberg (2019a) for implicit discourse relation classification. A BERT-friendly formalization of the task is a multi-task learning at the finetuning step: BERT is trained to predict the implicit connective as well as the discourse relation. In this way, we show that these three techniques can be combined straightforwardly in a single training pipeline. In the experiments, we compare multiple combinations of adaptation methods. We found that the first and second techniques yield additional gain for implicit discourse relation classification while the last one does not. Related Work Implicit Discourse Relation Classification Discourse connectives are one of the strongest cues in discourse relation classification. For this reason, some studies try to use discourse connectives for implicit discourse relations classification. Rutherford and Xue (2015) and Wu et al. (2017) tried to use explicit connectives. Rutherford and Xue (2015) proposed a Naive Bayes classifier using a large amount of unlabeled training data. They collected explicit argument pairs with freely omissible discourse connectives which can be dropped independently of the context without changing the interpretation of the discourse relation. However, Sporleder and Lascarides (2008) argued training on explicit argument pairs was not a good strategy. They investigated how feasible it is to use explicit argument pairs in implicit discourse relation classification and reported that explicit and implicit argument pairs may be too dissimilar linguistically and that removing unambiguous discourse markers in the automatic labeling process may lead to a meaning shift in the examples. However, the recent success of neural network-based transfer learning motivates us to rethink the importance of explicit connectives. In fact, Wu et al. (2017) proposed discourse-specific word embeddings. Embeddings were learned by classifying discourse connectives taken from a large amount of explicit discourse argument pairs. Zhou et al. (2010) , Qin et al. (2017) and Shi and Demberg (2019a) used pseudo-training data in which implicit connectives are inserted into implicit argument pairs. Zhou et al. (2010) proposed a language model that can predict implicit connectives. They used implicit connectives predicted by the language model as a feature of an SVM-based implicit discourse relation classifier. Qin et al. (2017) proposed a neural network model using domain adversarial training. This model focused to transfer knowledge from the recognition model supplied with implicit connectives to the model without connectives. Shi and Demberg (2019a) proposed a sequence-to-sequence neural network model. This model tried to generate implicit connectives from implicit argument pairs, and adapted the representation of the arguments to the classification task. BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) , a multi-layer bidirectional Transformer encoder, is one of the breakthrough models in natural language processing. Training of BERT consists of two stages: pre-training and fine-tuning. In pre-training, BERT learns contextual information and the relationship between two sentences from a large unlabeled corpus. In fine-tuning, BERT is trained using a task-specific dataset and adapts the pre-trained representations to downstream tasks. In the GitHub repository, 1 Devlin et al. (2018) suggest that it will likely be beneficial to run additional steps of pretraining if the downstream task has a large domain-specific corpus available. In fact, some studies showed the benefit of running domain-specific steps. Xu et al. (2019) Proposed Method BERT In this paper, we use BERT (Devlin et al., 2018) as the baseline model. For implicit discourse relation classification, the input is a pair of arguments, Arg1 and Arg2, and the output is one of the pre-defined discourse relations. Arg1 and Arg2 are concatenated into a single sequence, with the special token [SEP] indicates the end of each of the arguments. The special token [CLS] is inserted at the beginning of the sequence. In the pre-training step, BERT is trained on two unsupervised prediction tasks: MaskedLM and next sentence prediction. In the MaskedLM task, 15% of the input tokens are masked at random, and BERT is trained to recover those masked tokens. In the next sentence prediction task, BERT determines if a given pair of sentences actually occurs consecutively in this order. Using these two pre-training tasks, BERT can learn contextual information and the relationship between two sentences. In the fine-tuning step, Devlin et al. (2018) proposed four task-specific models incorporating BERT with one additional output layer. Implicit discourse relation classification can be cast as a sequence pair classification task. The Use of Domain Text Devlin et al. (2018) suggest that it will likely be beneficial to run additional steps of pre-training if a downstream task has a large domain-specific corpus available. For this reason, we collect a large amount of explicit argument pairs from an unlabeled corpus and use them in the additional pre-training step. We collect explicit argument pairs in two steps. We first locate the occurrences of discourse connectives in the unlabeled corpus. Among 100 discourse connectives defined by The PDTB Research Group (2007) , we used all connectives. We then identify the spans of the corresponding argument pairs. In this paper, we use a discourse parser by Lin et al. (2014) to predict explicit argument pairs. 2 They Figure 2 : Overview of the fine-tuning step with implicit connective prediction. The input is an implicit argument pair randomly selected from the training data, where annotators provide an implicit connective for each pair. BERT is trained to predict the implicit connective as well as the discourse relation. reported that the F-measure of partial matching on argument pairs in this parser is 80.96%, 3 which we believe is sufficiently high for pretraining. Note that Wu et al. (2017) collected explicit argument pairs using a similar method. However, they only used argument pairs located within the same sentence while we do not apply this constraint. Explicit Connective Prediction Task We aim to learn knowledge about discourse relations from explicit connectives. Explicit connectives mark the conceptual relationship between the two sentences. For example, the discourse relation of \"I cannot buy this laptop because I don't have enough money.\" is Contingency.Cause because we know \"because\" always represents a causality relation. Note that we referred to the pair as Arg1 (in italic) and Arg2 (in bold). We expect BERT to be flexible enough to transfer knowledge from explicit argument pairs to implicit ones. Accordingly, we introduce an additional pretraining step with the explicit connective prediction task. The additional pre-training with the explicit connective prediction task is illustrated in Figure 1 . In this step, the explicit connective classifier is given the representation of the [CLS] token and outputs an explicit connective. Note that Nie et al. ( 2019 ) also adapted the explicit connective prediction task to BERT. However, they singled out this task while we perform it jointly with MaskedLM and next sentence prediction. Implicit Connective Prediction Task In order to enhance the performance of implicit discourse relation classification, some studies try to extract strong cues from PDTB's annotation. In PDTB, annotators assigned implicit connectives to implicit argument pairs. Because these connectives are very similar to explicit connectives, recent studies tried to learn knowledge from implicit connectives (Qin et al., 2017; Shi and Demberg, 2019a) . (https://github.com/WING-NUS/pdtb-parser). 3 Partial matching means that a parser gets a credit if there is any overlap between the verbs and nouns of the two spans. Following these studies, we combine the implicit connective prediction task in the fine-tuning steps of BERT. The fine-tuning step with the implicit connective prediction task is shown in Figure 2 . In this task, the implicit connective classifier is given the representation of the [CLS] token and outputs an implicit connective. Experiments . Penn Discourse TreeBank We evaluated the performance of our models on the Penn Discourse TreeBank (PDTB) 2.0 (Prasad et al., 2008) , which is the most popular and largest corpus of discourse relations in English. The annotation is done as another layer on the Wall Street Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments), a relation label and a discourse connective. Relation labels are organized as a 3-level hierarchy in the PDTB. Popular experimental settings are top-level one-versus-all binary classification (Pitler et al., 2009) , top-level 4-way classification (Pitler et al., 2009; Rutherford and Xue, 2015) , second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015) , and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015) . We used top-level one-versus-all binary classification, top-level 4-way classification, and second-level 11-way classification in this experiment. In top-level one-versus-all binary classification and toplevel 4-way classification, we followed previous studies (Pitler et al., 2009; Rutherford and Xue, 2015) ; sections 2-20 as the training set, sections 0-1 as the development set, and sections 21-22 as the test set. Note that we used equal numbers of positive and negative examples in top-level one-versus-all binary classification. In second-level 11-way classification, we report results in three different settings. The first setting, PDTB-Lin, is based on Lin et al. (2009) ; sections 2-21 as the training set, section 22 as the development set, and section 23 as the test set. The second one, PDTB-Ji, is following Ji and Eisenstein (2015) ; sections 2-20 as the training set, sections 0-1 as the development set, and sections 21-22 as the test set. The last one, Cross Validation, is following Shi and Demberg (2017) ; 10-fold cross validation using the whole corpus of sections 0-24. Table 2 shows a distribution of relation labels in the Cross Validation dataset. Note that although we tried to replicate the procedures described by Shi and Demberg (2017) as closely as possible, there remained slight differences in the discourse relation distribution. Model Configurations We used a pre-trained model named BERT BASE -uncased as a baseline model. It was released with the original BERT code. 4 In fine-tuning, we set the batch size to 32 and the maximum sequence length to 128. 1 means implicit connective prediction at the fine-tuning step. For the models with +ICP in Table 1 , we added the implicit connective prediction task to the fine-tuning step. Models for Comparison For comparison, we collected state-of-the-art models from the literature: \u2022 Rutherford and Xue (2015) A Naive Bayes classifier that was trained explicit argument pairs with freely omissible discourse connectives extracted from a large amount of unlabeled data. \u2022 Wu et al. (2017) A feedforward neural network using discourse-specific word embeddings that were learned from a large amount of explicit argument pairs. Results Table 3 shows the results for each of the datasets. Discussion Shi and Demberg (2019b) reported that BERT outperformed the current state-of-the-art in second-level 11-way classification. We reconfirmed their report in 11-way classification and also found that BERT outperformed previous studies in top-level classifications. Surprisingly, BERT achieved over 70% F 1 score in One-Versus-All Binary classification while many previous models achieved less than 50% in Comparison-versus-all binary classification and Temporal-versus-all binary classification. We urge researchers to switch from One-Versus-All Binary classification to second-level or third-level classification tasks. We found that the explicit connectives prediction task resulted in 2.9% gain in PDTB-Ji while the DisSent task (Nie et al., 2019) , which runs the explicit connectives prediction task in the additional pre-training step, provides 2.0% gain. This result suggests that the combination of the explicit connectives prediction task and BERT's ordinary pre-training task is a better strategy than single-task pretraining. Qin et al. (2017) and Shi and Demberg (2019a) reported implicit connectives help implicit discorse relation classification. Comparing BERT+ICP with BERT in Table 3 , however, we can see that implicit connectives provided no significant gain. We conjecture that annotated data are too small for implicit connectives to be effective for BERT, for which data size is a key factor for success. Conclusion In this paper, we three additional training tasks to BERT, (1) additional pre-training using domain text, (2) the explicit connective prediction task at the additional pretraining step, and (3) the implicit connective prediction at the fine-tuning step to BERT. Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not. While transfer learning with BERT is demonstrated to be very effective for discourse relation classification, we feel that there are non-negligible differences between explicit and implicit argument pair. It may be worthwhile to revisit the notion of freely omissible discourse connective (Rutherford and Xue, 2015) to focus on explicit argument pairs from which knowledge can be straightforwardly transferred to implicit discourse relation classification. We plan to modify freely omissible discourse connectives to fit second-level 11-way classification. Another future direction is to adapt the BERT to incorporate external knowledge. Kishimoto et al. (2018) and Dai and Huang (2019) argued that the model for discourse classification could be further improved by incorporating external event knowledge like ConceptNet (Speer and Havasi, 2012) and temporal event knowledge. We plan to combine BERT with knowledge representation learning. Bibliographical References Bai, H. and Zhao, H. (2018) . Deep enhanced representation for implicit discourse relation recognition. In Pro-ceedings of the 27th International Conference on Computational Linguistics, pages 571-583. Beltagy, I., Lo, K., and Cohan, A. (2019)",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 4.723340845003143e-06,
        "none": 0.9999951574563252
    }
}