{
    "framing_question": "To what extent do the authors engage with the communities that are the subject of their paper, and do they claim a participatory approach?",
    "response": "The authors do not mention engaging with the communities that are the subject of their paper, nor do they claim a participatory approach.",
    "article": "A repetition is a response that repeats words in the previous speaker's utterance in a dialogue. Repetitions are essential in communication to build trust with others, as investigated in linguistic studies. In this work, we focus on repetition generation. To the best of our knowledge, this is the first neural approach to address repetition generation. We propose Weighted Label Smoothing, a smoothing method for explicitly learning which words to repeat during finetuning, and a repetition scoring method that can output more appropriate repetitions during decoding. We conducted automatic and human evaluations involving applying these methods to the pre-trained language model T5 for generating repetitions. The experimental results indicate that our methods outperformed baselines in both evaluations. Introduction Dialogues can build a trusting relationship with others, thus are essential in our daily lives (Schein, 1993; Searle and Vanderveken, 1985) . There are several types of responses in dialogues, and the one we focus on is repetitions (Tannen, 1987) . A repetition is a response that uses the previous speaker's words or phrases. Figure 1 shows an example. The phrases \"a bear\" and \"came out\" are repeated. Repetitions frequently appear in a conversation with diverse roles, e.g., to indicate attentive listening, confirm the previous utterance, and show agreement or sympathy (Machi, 2019; Shimojima et al., 2002) . Many linguistic studies investigating repetitions have concluded that they are important for building and strengthening relationships between speakers (Tannen et al., 1989; Johnstone, 2002; Norrick, 1987; Brown, 1999) . From the above linguistic point of view, we can say that repetitions are indispensable in dialogues. Repetitions are similar to paraphrases and reflections, which are component skills of counsel-Speaker: When I was driving, a bear suddenly came out . Listener: Oh. A bear came out !? ing (Theron, 2008) , in terms of using the previous speaker's utterance. Paraphrases and reflections have been generated using a template-based method (Han et al., 2013) . While many studies have tackled general response generation with neural network-based frameworks (Adiwardana et al., 2020; Zhang et al., 2020) , less attention has been paid to repetitions. This might be because they are buried in a huge amount of response data. Therefore, we focus on automatically generating repetitions. To the best of our knowledge, this is the first study on generating repetitions with a neural approach. We used the pre-trained language model T5 (Raffel et al., 2019) for generating repetitions because it has performed well in language generation in past few years (e.g., Radford et al.; Raffel et al., 2019; Lewis et al., 2020) . In generating repetitions, it is important to take into account which words should be repeated from the previous utterance. The repeated words might represent objective facts, names of people and places, and the speaker's experiences and emotions, though they are different depending on the language (Machi, 2008) . When we use a pre-trained language model, however, the model cannot explicitly learn the repeat likelihood among words during fine-tuning because it is difficult to directly teach which words are likely to be repeated at this step. To solve this problem, we propose Weighted Label Smoothing (WLS), which is an improvement upon Label Smoothing (LS) (Szegedy et al., 2016) . The method enables a language model-based re-sponse generator to learn the words it should use for each input utterance during fine-tuning. We also propose the repetition scoring method (RSM) to expand the scoring method proposed in Wu et al. (2016) for selecting repetitions that contain appropriate repeated words during decoding. We evaluated the proposed methods on a dataset we created in Japanese for automatic and human evaluations. Our methods outperformed baselines, i.e., fine-tuned pre-trained language models without our methods, in both evaluations. This indicates that our methods can generate repetitions that contain appropriate words to repeat. Our contributions are as follows: 1. To the best of our knowledge, this is the first study to use a neural model for generating repetitions. 2. We will release our code and the dataset of repetitions we created. 1 3. We propose WLS, that takes into account words that should be repeated during finetuning, for generating repetitions. 4. We propose RSM to select repetitions containing appropriate repeated words during decoding. Proposed Methods Repetitions do not necessarily mean we repeat any word. For the utterance \"Today's dinner was pizza.\", the repetition \"Oh, you ate pizza.\" is more appropriate than \"Oh, you ate today.\" However, a fine-tuned pre-trained language model alone may not be enough to generate repetitions with appropriate repeated words. Therefore, to generate a response that repeats more appropriate words, we introduce repeat scores ( \u00a72.1) to calculate how likely a word is repeated and incorporate the scores into WLS ( \u00a72.2) for fine-tuning and RSM ( \u00a72.3) for beam search in decoding. Repeat Score We should give high scores to words that tend to be used in repetitions and low scores to words that should not be. Since only content words (nouns, verbs, adjectives, or adverbs) are repeated in Japanese, we define a repeat score only for them. Since subwords are used as a unit in a pre-trained language model, all the subwords in the same content word receive the same repeat score. We use BERT (Devlin et al., 2019) to construct a model for scoring the repeat scores in the range of [0, 1]. We pass the final hidden state of BERT through SpanExtractor (Lee et al., 2017) for each word and then convert the vector to a scalar value through a multi-layer perceptron, which has a sigmoid function as the last layer. In the training data, the label is set to 1 if the target content word was repeated, and 0 if it was not. The output is then normalized by applying min-max scaling. Weighted Label Smoothing (WLS) In this section, we explain how to learn words to repeat when fine-tuning a pre-trained language model for repetition generation. Neural response generation models try to optimize cross-entropy loss. Let X be a previous utterance and Y be a response, where Y is divided into subwords as Y = y 1 , . . . , y T . Letting K be the total number of subwords and v k be the k-th subword, the crossentropy loss is defined as follows: L(q, p) = \u2212 K k=1 q(v k ) log{p(v k |y <t , X)}, where p(v k |y <t , X) is the probability of v k that the model outputs at time step t given X, and q(v k ) is the probability of v k in a target distribution that the model aims for. When a one-hot distribution is used, q(v k ) is as follows with a function \u03b4 v k ,yt , which becomes 1 when v k = y t : q(v k ) = \u03b4 v k ,yt . When LS is used, however, q(v k ) is as follows with uniform distribution u(v k ) = 1/K: q(v k ) = (1 \u2212 \u03f5)\u03b4 v k ,yt + \u03f5u(v k ), where \u03f5 is a hyperparameter. A one-hot distribution and LS cannot learn a subword to repeat explicitly because there are labels other than the target, i.e., v k when v k \u0338 = y t , that have the same q(v k ). Therefore, we propose WLS, which takes into account how likely a subword is repeated. We use repeat scores, explained in \u00a72.1, instead of u(v k ). The q(v k ) of WLS is defined as follows: where r(v k ) is the repeat score for v k , and \u03b3 is a hyperparameter. We use the q(v k ) of WLS as the distribution in the cross-entropy loss function. Subwords in the previous speaker's utterance are weighted in accordance with their r(v k ). Note that if we set \u03b3 = 0, WLS is the same as LS. q(v k ) = (1 \u2212 \u03f5)\u03b4 v k ,yt + \u03f5 r(v k ) \u03b3 K Repetition Scoring Method (RSM) Pre-trained language models usually use beam search in decoding. We propose a scoring method, RSM, to select more appropriate repetitions in the beam search. RSM is an extension of a scoring method for machine translation in Wu et al. (2016) . The original scoring method uses a length normalization procedure and coverage penalty (Tu et al., 2016) . Length normalization treats sentences of different lengths equally. The coverage penalty gives a high score to a sentence that is most likely to cover all the words in the source sentence. Since the original scoring method cannot select a repetition with appropriate repeated words, we modify the method by adding repeat scores, which indicate words to repeat. Letting Y be a candidate response during beam search and X be the previous utterance, the generation probability is P (Y |X). The scoring function s(Y, X) of RSM is as follows: s(Y, X) = log{P (Y |X)}/lp(Y )+cp(X,Y )+rs(X,Y ), lp(Y ) = (5 + |Y |) \u03b1 (5 + 1) \u03b1 , cp(Y, X) = \u03b2 * |X| i=1 log( |Y | j=1 pi,j), rs(Y, X) = log |Y | j=1 r(vj), where \u03b1 and \u03b2 are hyperparameters for length normalization and coverage penalty, respectively. We carry out two modifications to the original scoring method to yield RSM. First, we use the attention value of p i,j without suppression. In contrast to machine translation, in which an input and output have a one-to-one relationship, lengths of an input and output are not the same in repetition generation, and so it is not suitable to suppress the attention value under 1.0. Second, we add the term rs(Y, X), which represents the sum of repeat scores for subwords in the response. Dataset We manually created pairs of a speaker's utterance and its repetition as our dataset using a crowdsourcing service. 2 Since repetitions often occur when a listener replies to a speaker, we used utterances in a corpus of listening dialogues (Yoshino et al., 2018) between an elderly person and caregiver or clinical psychologist as the speaker's utterances in our dataset. 3 In this corpus, the elderly person tends to be a speaker and the others are listeners. We extracted the elderly person's utterances containing content words for creating a repetition. The number of extracted utterances was 5,548. We asked three crowdsourcing workers to create repetitions for each utterance. Specifically, a worker was shown two utterances before each target utterance and asked to create a repetition, that supports the creation of context-aware repetitions. When the workers found it difficult to create a repetition for an utterance, they could discard it. The total number of workers was 333. Examples from the dataset are given in Table 1 . The size and statistics of our repetition dataset are shown in Tables 2 and 3 . The word overlap rate is the percentage of words in an utterance that are  repeated in a repetition. The content-word overlap rate is the percentage of content words of an utterance that are repeated. Comparing the average numbers of tokens, repetitions are much shorter than utterances. This may indicate that repetitions cannot be produced simply by copying the utterances, and we need to select information that is worth repeating from the utterances. To understand what types of words overlap, Table 4 shows the percentage of all words' parts-ofspeech and overlapped words' parts-of-speech in utterances. Since \"postpositional particles\" and \"auxiliary verbs\" tend to accompany content words in a Japanese unit called 'bunsetsu', it might be natural that they also appear in repetitions in high percentages. While we can have at most three repetitions for an utterance in our dataset, we used only one randomly selected repetition for an utterance in the training data. We used all repetitions for an utterance for the evaluation on the validation and test data to consider the diversity of responses. Experiments General Setup Repeat scores were calculated from the training data. SentencePiece (Kudo and Richardson, 2018) was used to segment the dataset into subwords. With WLS, the hyperparameter \u03f5 was set to 0.1 following a previous study (Szegedy et al., 2016) , and \u03b3 was tuned to 4 with the validation data, as explained in Appendix A. With RSM, we used \u03b1 = 0.2 and \u03b2 = 0.2, following a previous study (Wu et al., 2016) , and a beam size of 5. We used MeCab 4 as a tokenizer to identify content words. Compared Methods The baseline methods were as follows: Rule-Based is a rule-based method, with which a response is created with a content word in the speaker's utterance + \"desuka\" (\"is it?\"). The content word is randomly selected from the utterance. Examples of rule-based responses are given in Table 5. Responses made with Rule-Based always contain a repeated word and have few grammatical errors. However, \"desuka\" cannot cover all situations. \"desuka\" was chosen because 52% of repetitions in our dataset ends with \"desuka\", and 6.1% of repetitions are a single word + \"desuka\". BertSumAbs (Liu and Lapata, 2019 ) is a model trained with BERT 5 as the encoder and randomly initialized Transformer as the decoder. T5 6 (Raffel et al., 2019 ) is a model that was finetuned with the repetition dataset. 7 LS is T5 fine-tuned with LS. Copy is T5 fine-tuned with the copy mechanism (See et al., 2017) . Since the copy mechanism can be considered similar to the repetition model in that it is used to generate the same words as in an input sentence, we used it for comparison. Note that the T5 and BERT were versions pretrained in Japanese. Our methods are as follows: WLS is T5 fine-tuned with WLS, as mentioned in \u00a72.2. RSM is T5 using RSM during beam search, as mentioned in \u00a72.3. WLS + RSM is T5 fine-tuned with WLS and using RSM during beam search. Automatic Evaluation The evaluation metrics were ROUGE (RG-1, RG-2, RG-L) (Lin, 2004) and the percentage of outputs containing correct repeated words. The correct repeated words are content words repeated in the gold response. The experimental results are listed in Table 6 . WLS + RSM obtained the highest scores for all metrics, confirming the effectiveness of both WLS and RSM. We conducted an ablation study to analyze the results of RSM. The results are listed in Table 7 . Since w/o rs received the lowest scores, rs was considered the most effective. Examples of an input and generated responses from the baseline and our model are shown in Table 8 . The proposed model (WLS + RSM) successfully generated a response that was close to the correct response, focusing on \"having friends who play Go\". Human Evaluation We also conducted a human evaluation by comparing three types of response generation methods: Utterance \u6628\u65e5\u306f\u540c\u3058\u50d5\u3089\u306e\u4ef2\u9593\u3067\u56f2\u7881\u3059\u308b\u4eba\u3044\u305f\u304b\u3089\u306d. (Yesterday there were our friends who play Go.) Gold \u4ef2\u9593\u3067\u56f2\u7881\u3059\u308b\u4eba\u3044\u305f\u3093\u3067\u3059\u306d. (There were friends who play Go.) Rule-Based \u56f2\u7881\u3067\u3059\u304b. (Go, is it?) T5 \u56f2\u7881\u3092\u3057\u3066\u304f\u308c\u305f\u3093\u3067\u3059\u306d. (You played Go.) Ours \u4ef2\u9593\u5185\u3067\u56f2\u7881\u3059\u308b\u4eba\u3044\u305f\u3093\u3067\u3059\u304b. (There were friends who play Go.) Rule-Based, T5, and our model (WLS + RSM). The evaluation measures were grammaticality (Gram), relevance (Rel), coherence (Cohe), and whether repeated words are included (Rep). Two hundred pairs were randomly selected from the test data. The responses were shown to five workers and evaluated on a three-point Likert scale. The response was evaluated with the previous speaker's utterance and one turn before the speaker's utterance as context, meaning the context helps in determining whether the response is an appropriate repetition. The total number of evaluators was 110. Average scores from the evaluation are listed in Table 9 . WLS + RSM outperformed the other methods for all measures, confirming its effectiveness. Conclusion We focused on repetition generation. Although repetitions play an important role in dialogues, there has been no neural approach for this task to the best of our knowledge. We proposed WLS, which is an improvement upon LS, during fine-tuning and RSM, which is an extended scoring method, during decoding for repetition generation. Through automatic and human evaluations, we confirmed that our model can generate repetitions that contain more appropriate words to repeat than baseline models. For future work, we will take into account synonyms and multiple gold repetition instances to calculate repeat scores for improving the diversity of responses. We are also planning to incorporate our repetition model into a general response generation framework. Ethical Statement Neural generative models have the potential to generate unexpected responses such as violent remarks. As we focused on repetition generation, its model repeats a user's utterance, and so there is little chance of causing unintended responses compared with chit-chat dialogue systems. However, this does not mean that unintended responses will never appear, e.g., when a user's utterance is an unintended expression. Thus, the same consideration must be taken as with other dialogue systems. Our dataset was created to repeat from utterances in a privacy-secured dataset, and so there is no privacy issue. Since the license of the original dataset is CC BY-NC 4.0, we could use it for this study. We define that the license of our dataset is also CC BY-NC 4.0. Acknowledgements We thank Dr. Koichiro Yoshino of RIKEN for providing the listening dialogue data. \u03b3 0.0 0.1 0.5 1.0 2.0 3.0 4.0 5.0 10.0 % 82.06 82.06 81.28 83.37 82.21 83.52 85.07 82.90 84.53 Table 10 : Percentage of generated responses containing a correct repeated word in the development data when \u03b3 was changed. \u03f5 = 0.1. \u03b3 = 0 indicates LS. The best score was obtained when \u03b3 = 4.0. A Exploring Hyperparameter \u03b3 We explored the effect of \u03b3 on the percentage of responses containing a correct repeated word. The model we used for experiments was the pre-trained model T5, fine-tuned with the training data in \u00a73. We generated repetitions on the development data. The results are listed in Table 10 . The best score was recorded when \u03b3 = 4.0. Therefore, we used this value. B P-values We now discuss the p-values in the experimental results. To obtain p-values, we conducted the Wilcoxon rank sum test to compare the effectiveness between baseline models and our proposed models.",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 4.227161253023137e-05,
        "foundation": 9.84906049772416e-06,
        "none": 0.9999988527586581
    }
}