{
    "article": "The paper presents a method for parsing low-resource languages with very small training corpora using multilingual word embeddings and annotated corpora of larger languages. The study demonstrates that specific language combinations enable improved dependency parsing when compared to previous work, allowing for wider reuse of pre-existing resources when parsing low-resource languages. The study also explores the question of whether contemporary contact languages or genetically related languages would be the most fruitful starting point for multilingual parsing scenarios. Introduction Developing systems for low-resource languages is a crucial issue for Natural Language Processing (NLP). Most NLP systems are built using supervised learning techniques (Weiss et al., 2015; Straka et al., 2016; Ballesteros et al., 2016) . These systems require a large amount of annotated data and are thus targeted toward specific languages for which this kind of data exists. Unfortunately, producing enough annotated data is known to be time-and resourceconsuming, which means that annotated data, especially of the type required for parsing, is lacking for most languages. To take a recent example, the 2017 CoNLL Shared Task concerned around 50 languages, roughly all of the languages for which enough syntactically annotated data is available in the Universal Dependency format. This was probably (by far) the most ambitious parsing challenge ever undertaken with regard to language diversity, but the figure of 50 languages should be viewed relative to the 6,000 languages in the world: even if one includes only those languages for which written data is available, the 50 languages targeted at CoNLL 2017 cover only a fraction of all the world's languages. When it comes to parsing, the supervised, monolingual approach based on syntactically annotated corpora has long been the most common one. However, thanks to recent developments involving feature-representation methods (a.k.a. word embeddings) and neural network models, it is now possible to develop accurate multilingual models, too. The multilingual approach has yielded encouraging results for both low- (Guo et al., 2015) and high-resource languages (Ammar et al., 2016a) . Generally speaking, the multilingual approach can be implemented in two ways. The first involves projecting annotations available for a high-resource language onto a low-resource language using a parallel corpus, while the second aims at producing a cross-lingual transfer model that can work for several languages. Guo et al. (2016) and Ammar et al. (2016a) have conducted multilingual parsing studies for Indo-European languages using the model transfer approach. They demonstrated that a multilingual model can yield better results than monolingual models for different European languages. However, their approach relied on the existence of a massive parallel corpus, as their experiment was based on Europarl. 1 Thus the problem of low-resource languages remains unaddressed, especially in cases when no parallel corpus is available. In this paper, we propose a simple but powerful method for creating a dependency parsing model when no annotated corpus or parallel corpus is available for training. Our approach requires only a small bilingual dictionary and the manual annotation of a handful of sentences. We assume that the performance one can obtain with this approach depends largely on the set of languages used to train the model. This is why we have developed several models using genetically related and non-related languages, so as to gain a better understanding of the limitations or possibilities of model transfer across different language families. In this study, we are working with languages that, until very recently, did not or still do not have a Universal Dependency corpus: North Saami 2 and Komi-Zyrian, henceforth Komi. 3 (In this paper, we use ISO 639-3 codes to refer to the languages in tables.) This alone does not make them low-resource languages, but they are still poorly equipped with regard to NLP tools. Saami language technology has been in active development for a longer time in the Giellatekno project of the University of Troms\u00f8, 4 , but in the case of Komi, resources have only very recently begun to emerge. Work has also been done on these languages within the framework of language documentation; however, with the exception of a few individual projects (Blokland et al., 2015; Gerstenberger et al., 2016) , this field has generally not paid much attention to the use of NLP tools. The same scenario applies to many other low-resource languages. On the other hand, the amount of written data in smaller languages may have grown rapidly in last years thanks to an increased online presence and various digitalization projects. This makes it possible to build word embeddings, as the only resource needed for these is a sufficiently large amount of text. A major contribution of this work also lies in the new resources that have been developed as a result of it. We have created a new UD-type corpus for Komi 5 (version 0.1 used in this study), as well as bilingual dictionaries, multilingual word embeddings for Komi and Saami, and a multilingual parser, all of which are freely available in public repositories. 6 7  The structure of the paper is as follows. We first provide an overview of our approach (Section 2) before detailing the multilingual resource representation used (Section 3) and our parsing model (Section 4). We then describe a series of experiments aiming at validating the approach (Section 5) before presenting an analysis of our results along with some thoughts for future work (Section 6). Approach The languages used for training the models have been selected with the assumption that genetically related languages or contemporary contact languages, at least in certain scenarios, share structural similarities with the lowresource languages in question. Our aim is to test whether it is possible to use the annotated data from larger languages as part of the training data, exploiting this structural similarity. Our work with Northern Saami was initially motivated by it being one of the surprise languages in the CoNLL 2017 Shared Task. After the Shared Task ended, the larger training data became available in the dev-branch of the UD North Sami GitHub repository in the Universal Dependencies project 8 , which enabled us to carry out broader testing. With Komi-Zyrian, the situation is essentially the same as it was with the North Saami before the recent release of the new data: we have 85 manually annotated examples we can use to train and test the parsing result. The training and testing set was created manually for Komi for the purpose of this study, but we are currently expanding it. This Komi-Zyrian treebank will be included in Universal Dependencies project during 2018. Currently, only English has been used as a control language with no direct contact or genetic relation with Komi or Saami. Our expectation is that the match rate between Russian and Komi-Zyrian should be exceptionally high, as there has been such a long history of contact between these languages, leading to a variety of morphosyntactic changes in Komi (Leinonen, 2006, p. 241) . North Saami has a complex contact relationship with Finnish, but in addition to this, it is also a closely genetically related language (Aikio, 2012, p. 67-69) . From this point of view, one could expect Finnish to perform well in parsing both Komi and North Saami, although the similarities between these languages have not been studied in great detail from the perspective of syntax and dependency structures. Other types of experiments have also been conducted using this approach, for example, by using a Komi-Zyrian-Russian multilingual model to parse data that contains both languages in the form of code-switching: in these tests, the parser has been shown to be able to analyse language-specific constructions when they occur within same utterance (Partanen et al., 2018) . Multilingual Lexical representation Preparation of Language Resources The bilingual lexicons were taken from the Giellatekno infrastructure (Giellatekno, 2017) , as these are rather large and have been released with a GNU GPLv3 license. As some portions of this data are derived from printed dictionaries, not all entries are directly usable for our purposes. Multi-word translations and entries containing question marks have been removed. Table 1 shows the sizes of the dictionaries used. Bilingual pairs Bi-dictionary Bi-embedding Finnish-Komi We have used the pretrained Finnish and Russian Fast-Text word embeddings published by Facebook in May 2017 (Bojanowski et al., 2016) . Since the Komi and Saami Wikipedias are relatively small, we have also trained larger word embeddings using FastText. For Komi, we have used Public Domain books digitalized in the Fenno-Ugrica collection (https://fennougrica. kansalliskirjasto.fi/) and proofread by The Finno-Ugric Laboratory for Support of the Electronic Representation of Regional Languages in Syktyvkar (http: //komikyv.org/). For North Saami, we have used the SIKOR North Saami free corpus (http://hdl. handle.net/11509/100), which has been published with a CC-BY 3.0 license. Projection of Multiple Word Embeddings onto a Single Space In the previous section, we described how we obtained and trained monolingual embeddings for each language, but each of those embeddings is trained in its own vector space. In order to transform the different embeddings into one single bilingual word embedding (encoded through a single vector space model), we apply the linear transformation method proposed by Artetxe et al. (2016) . According to comparisons presented in Artetxe et al. (2017, p. 457) , the size of dictionaries we used is well above what is needed to carry out the mapping task using this method. The method is as follows. Let target language X and source language Y be the word embedding matrix trained by two different languages. And let D={(x i ,y i )} m i=1 (where x i \u2208 X, y i \u2208 Y ) be a bilingual dictionary consisting of wordembedding vector pairs. Our goal is to find a transformation matrix W such that xW approximates y. This is done by minimizing the sum of squared errors, following Mikolov et al. (2013) : arg min W m i=1 x i W \u2212 y i 2 (1) However, the application of a linear transformation to an embedding without constraints may cause a degradation of performance since the computation of mapping may break monolingual invariance. Artetxe et al. (2016) proposed an orthogonal mapping method that lets W be an orthogonal matrix and makes it possible to avoid a degradation of performance. We followed this method in order to map two different embeddings and produce bilingual word embeddings with language-specific prefixes preceding each surface forms. For example, in case of a English-French pair, the projected embedding can have \"eng:dog\" and \"fra:chien\". To train word embeddings related to more than two languages, we first selected a standard source language S and then mapped it with each target language T=(t 1 ,t 2 ,t i ..). Based on the trained parameter W T = (W t1 , W t2 , W ti ..), we can thus build a multilingual word embedding by multiplying the parameter W ti and all the surface forms included in t i . Cross-Lingual Dependency Parsing Model Traditionally, many parsers have applied linear supervised learning models with hand-crafted feature functions. The feature function takes features for classifying head-modifier and relations among tokens (i.e. \"word forms and POS tags from first and second tokens on top of the stack\") (Kiperwasser and Goldberg ( 2016 )). Parsers also require many templates in order to make a decision about the relations between tokens. However, extracting the proper features and templates manually is a difficult and time-consuming job. In order to address the limitation of manual work, Chen and Manning (2014) proposed using non-linear classifiers with a neural network model. This method encodes lexical (words) and non-lexical (POS tags) features as vectors and then concatenates the features of each token to feed the non-linear classifiers. This has two advantages: On the one hand, non-linear classifiers show better performance than linear models in identifying relations between tokens, and on the other hand, the use of a neural network with concatenated features alleviates the need for manual work because the neural model, especially in Recurrent Neural Networks (RNNs), has access to tokens and features computed previously for a given sentence. Our basic feature representation approach is based on Chen and Manning (2014), with the exception of the method used for pretrained multilingual embeddings. In order to take into account lexical resources during parsing, we have extended the multilingual graph-based parser based on the bidirectional LSTM feature representations proposed by Lim and Poibeau (2017) . Bidirectional LSTM Feature Representations Recent advances in NLP have been possible largely due to innovative feature representations that provide an accurate overview of word relations inside the sentence (Cho, 2015; Huang et al., 2015) . A good example is the BIST-parser proposed by Kiperwasser and Goldberg (2016) , which is based on bidirectional LSTM learning. BiLSTM is a powerful learning model for sequential data because it consists of two LSTM layers, a Forward layer that reads the sentence from left to right, and another that reads it from right to left. For example, given a sentence t = (t 1 ,t 2 ,...,t n ), in which the symbol \u2022 denotes a concatenation operation, the BiLSTM function can be represented as: BiLSTM(t 1:n , i) = LSTM Forward (t 1:i ) \u2022 LSTM Backward (t i:n ). Cross-Lingual Feature Representations In their paper, Lim and Poibeau (2017) proposed a system for multilingual parsing based on bidirectional LSTM feature representations. They followed most of the approach used for the BIST-parser (see Kiperwasser and Goldberg (2016) and the previous section) and transformed it into a multi-source trainable model with multilingual word embeddings and language hot-encoding. This system obtained an LAS F1 score of 70.93% in the 2017 CoNLL Shared Task (rank 5/33). For this paper, we have extended the parser using the multilingual word embeddings proposed in Section 3, and have adapted the same token representation methods and dimensions proposed by Lim and Poibeau (2017) . Token Representation. Given an input sentence t = (t 1 ,t 2 ,...,t n ), a word form w, a corresponding POS tag p, pretrained word embedding xw and language hot-encoding l, the i th token is defined as: t i = e(w i ) \u2022 e(p i ) \u2022 e(xw i ) \u2022 e(l i ) , where e denotes the embedding vector of each feature and e(xw i ) is the pretrained word embedding introduced in Section 3. Additionally, we added a language hot-encoding vector composed of 0 and 1 for each language as proposed earlier (Naseem et al., 2012; Ammar et al., 2016a) . Compared with monolingual parsers, most use e(w i ) and e(p i ), with additional features such as distance between head node and language-specific lexical features included in the UD corpus. Note that t i feeds into BiLSTM(t 1:n i) in order to store the Forward and Backward contexts from the LSTM. Parsing Model There are two mainstream approaches to parsing, one being the transition-based model (Nivre, 2004) and the other the graph-based model (McDonald et al., 2005b) . For this study, we chose the graph-based approach based on the BIST-parser since graph-based approaches seem to show better performance for parsing UD-type corpora (Dozat et al., 2017) . From the features and tokens stored in the BiL-STM layer, the BIST-parser computes a candidate tree for each head word and modifier, after which scores attached to the different candidate trees are computed using the multilayer perceptron (MLP), which is a basic neural network model that can be used as a scoring function. Finally, the system finds the best dependency parsing trees based on the sum of the subtrees. For further information on the graph-based and arc-factored model used in the BISTparser, please see Taskar et al. (2005) Experiment We conducted a series of experiments on Saami and Komi. For Saami, we tested different language combinations for the cross-lingual model. All the experiments were carried out using 20 training sentences in Saami, as was the case for the 2017 CoNLL Shared Task, which means these results can be compared to the ones in the official CoNLL evaluation. For Komi, no annotated corpus was available, but we designed ten different experiments, again exploring different language combinations for the cross-lingual model. The experiments for Komi are representative of an extremely low-resource scenario, which is quite common, meaning the approach can be reused for a wide variety of other languages. Training Corpus. We used corpora available in the Universal Dependency 2.0 format (Nivre et al., 2017) to train and test all models except for Komi. Since there is no UD 2.0 Komi corpus, we used 10 sentences for training and 75 sentences for the testing set (the corpus was designed specifically for this study). Following previous works by Guo et al. (2015) and Zhang and Barzilay (2015) , we used gold POS sets for training and testing for Komi. For Saami, however, the teams in the CoNLL Shared Task used preprocessed POS tagging sets processed by UDpipe. In order to maintain the same conditions as in the Shared Task, we also used the preprocessed POS tagging sets to compare with others in Table 3 . Training Conditions. Since we wanted to explore lowresource scenarios (even in the case of Saami, for which larger data now exists), we assumed that there had been no development data for parameter tuning, and we restricted all training experiments to run just one epoch with the training corpus without early stopping (i.e. the 5 th -low of column titled \"Case\" in Table 2 runs only 12,563 iterations). Similar restrictions have also been suggested by Ammar et al. (2016b) , who proposed running one epoch, and by Guo et al. (2016) , who proposed restricting iterations to 20,000 for low-resource scenarios. However, when training a model with multi-source training data, the size of training corpora for low-resource languages is comparably smaller than for high-resource languages. Following the previous work of Guo et al. (2016) , we iterated 20 times more for low-resource training data than for high-resource. In Table 2 and Table 4 , the sizes of the training sets used are provided in brackets. Comparison with the CoNLL Shared Task. We used the same training environments in these experiments as for the CoNLL Shared Task (the same training sets and no development set). Moreover, in order to guarantee similar experimental conditions, we allowed the training models to  Compared with the result of the Shared Task, it seems that our approach (lexicalized cross-lingual transfer parsing with resources from relevant languages) can be effective for parsing low-resource languages. However, additional language features and the application of the ensemble mechanism also seem to be very important. This is due to the fact that the team C2L2 outperform others with more than 6.4 percent of LAS score based on character embeddings as additional features and an ensemble mechanism composed of three different dependency parsers. Analysis All the experiments we conducted using Finnish for training obtained better results than other language combinations (i.e. English for Saami and Russian and English for Komi). This means that transferring knowledge from genetically related languages is, at least in our case, a very efficient method for parsing. This is true in the case of Saami and Finnish, but also in the case of Finnish and Komi, which are arguably more distantly related to one another than Finnish and Saami. A contact language can also be of significant help in improving the results with a lowresource language, as can be seen in the case of Russian and Komi. Even in cases where the LAS scores (i.e. labels of the syntactic dependencies) were not significantly different, there was often greater variation in the UAS scores (unlabeled dependencies). This means that although the actual labels may not have been assigned correctly, the basic relations were found and the root was correctly recognized. It is also worth noting that the highest UAS scores were obtained with the Komi-Finnish pair, although the Komi-Russian-Finnish multilingual model gave the best overall result. Our results show that adding even a small number of targetlanguage example sentences into a parser that uses bilingual word embeddings can improve the result significantly. The size of the available training corpus is very important, and the quality of the bilingual dictionaries used to align word embeddings is also crucial. Conclusion In this paper, we have presented a multilingual approach to parsing that is effective for languages with few resources and no syntactically annotated corpora available for training. We have shown that our multilingual models provide better results than monolingual ones. Adding training material from other languages usually did not decrease the parsing result. It should, however, be noted that the relative size of the different corpora used for training seems to be relevant, since using corpora that are too imbalanced may weaken the result. More detailed analysis of the results, beyond the LAS and UAS scores, is most likely needed in order to determine the exact influences of different language pairs or combinations. It remains a question for further study whether the improvements observed here are actually attributable to the genetic relationship between the language, or if the same result could be obtained by simply selecting languages that are otherwise typologically similar. Acknowledgments We want to thank the LREC reviewers for their useful comments. This work has been developed in the framework of the LAKME project funded by a grant from Paris Sciences et Lettres (IDEX PSL reference ANR-10-IDEX-0001-02). Thierry Poibeau is partially supported by a RGNF-CNRS (grant between the LATTICE-CNRS Laboratory and the Russian State University for the Humanities in Moscow). Kyungtae Lim is partially supported by the ERA-NET Atlantis project. Niko Partanen's work has been carried out at the LATTICE laboratory within the project 'Language Documentation meets Language Technology: the Next Step in the Description of Komi', funded by the Kone Foundation. Thanks to Alexandra Kellner for proofreading the paper.",
    "funding": {
        "defense": 1.9361263126072004e-07,
        "corporate": 0.0,
        "research agency": 0.99999861435166,
        "foundation": 1.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article mentions that the work has been developed in the framework of the LAKME project funded by a grant from Paris Sciences et Lettres (IDEX PSL reference ANR-10-IDEX-0001-02), which is a foundation. Thierry Poibeau is partially supported by a RGNF-CNRS grant, indicating research agency support. Kyungtae Lim is partially supported by the ERA-NET Atlantis project, which could also indicate research agency or foundation support, but the specific nature of ERA-NET Atlantis is not detailed enough to categorically determine its nature. Niko Partanen's work is funded by the Kone Foundation, which is a foundation. There is no mention of defense or corporate funding.",
    "abstract": "The paper presents a method for parsing low-resource languages with very small training corpora using multilingual word embeddings and annotated corpora of larger languages. The study demonstrates that specific language combinations enable improved dependency parsing when compared to previous work, allowing for wider reuse of pre-existing resources when parsing low-resource languages. The study also explores the question of whether contemporary contact languages or genetically related languages would be the most fruitful starting point for multilingual parsing scenarios.",
    "countries": [
        "France"
    ],
    "languages": [
        "Saami",
        "Komi",
        "Finnish"
    ],
    "numcitedby": 18,
    "year": 2018,
    "month": "May",
    "title": "Multilingual Dependency Parsing for Low-Resource Languages: Case Studies on North Saami and {K}omi-{Z}yrian"
}