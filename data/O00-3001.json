{
    "article": "This paper describes a general framework for adaptive conceptual word sense disambiguation. The proposed system begins with knowledge acquisition from machine-readable dictionaries. Central to the approach is the adaptive step that enriches the initial knowledge base with knowledge gleaned from the partial disambiguated text. Once the knowledge base is adjusted to suit the text at hand, it is applied to the text again to finalize the disambiguation decision. Definitions and example sentences from the Longman Dictionary of Contemporary English are employed as training materials for word sense disambiguation, while passages from the Brown corpus and Wall Street Journal (WSJ) articles are used for testing. An experiment showed that adaptation did significantly improve the success rate. For thirteen highly ambiguous words, the proposed method disambiguated with an average precision rate of 70.5% for the Brown corpus and 77.3% for the WSJ articles. Introduction Word sense disambiguation is a long-standing problem in natural language understanding. It seems to be very difficult to statistically acquire enough word-based knowledge about a language to build a robust system capable of automatically disambiguating senses in unrestricted text. For such a system to be effective, a large number of balanced materials must be assembled in order to cover many idiosyncratic aspects of the language. There exist three issues in a lexicalized statistical word sense disambiguation (WSD) model: data sparseness, the lack of abstraction, and static learning. First, a word-based model has a plethora of parameters that are difficult to estimate reliably even with a very large corpus. Under-trained models lead to low precision. Second, word-based models lack a degree of abstraction J. N. Chen that is crucial for a broad coverage system. Third, a static WSD model is unlikely to be robust and portable, since it is very difficult to build a single model relevant to a wide variety of unrestricted texts. Several WSD systems have been developed that apply word-based models to a specific or genre domain to disambiguate senses appearing in generally easy context that has a large number of typically salient words. In the case of unrestricted text, however, the context tends to be very diverse and difficult to capture with a lexicalized model; therefore, a corpus-trained system is unlikely to transfer well to a new domain. Generality and adaptability are, therefore, keys to a robust and portable WSD system. A concept-based model for WSD requires fewer parameters and has an element of generality built in. Conceptual classes make it possible to generalize from word-specific context in order to disambiguate word senses appearing in an unfamiliar context in terms of word recurrences. An adaptive system, armed with an initial lexical and conceptual knowledge base extracted from machine-readable dictionaries (MRD), has two strong advantages over static lexicalized models trained on a corpus. First, the initial knowledge is rich and unbiased enough for a substantial portion of text to be disambiguated correctly. Second, based on the result of initial disambiguation, an adaptation step can then be performed to make the knowledge base more relevant to the task at hand, thus resulting in broader and more precise WSD. In this paper we explore in some depth the question of whether conceptual knowledge in the MRD is effective enough to provide a general solution for disambiguating contexts of unrestricted texts, such as the Brown and Wall Street Journal (WSJ) corpora. Major emphasis has previously been placed on self-adaptation [Chen and Chang 1998a ]. This approach is based on the hypothesis that a substantial part of a given text is easy or prototypical and, therefore, susceptible to interpretation based on general knowledge derived from the MRD. By adapting the contextual representation of word senses to those in the easy context, we hope to be better equipped to interpret the other part, which is usually considered a hard context. Adaptation results in gaps in the general knowledge being filled in or domain specific information being added to the initial knowledge base. Either way, adaptation makes the knowledge base more relevant to the text and, therefore, more effective for WSD in a hard context. We will give experimental results showing the effectiveness of this adaptive WSD approach based on initial knowledge base acquired from the MRD. Although our adaptive approach requires virtually no domain-specific training, it nevertheless achieves high precision rates for WSD of unrestricted text rivaling those of static methods that demand very lengthy training using a very large corpus. Figure 1 lays out the general framework for the adaptive conceptual WSD approach which this research employed. The learning process described here begins with a step involving knowledge acquisition from MRDs. With this acquired knowledge, the input text is read and a trial disambiguation step is carried out. An adaptation step follows which combines the initial knowledge base with knowledge gleaned from the partially disambiguated text. Once the knowledge base is adjusted to suit the text at hand, it is then applied to the text again to finalize the disambiguation result. For instance, the initial contextual representation (CR) extracted from the Longman Dictionary of Contempory English [Proctor 1978, LDOCE] for the bank-GEO sense contains both lexical and conceptual information: {land, river, lake, \u2022\u2022\u2022} \u222a {GEO, MOTION, \u2022\u2022\u2022}. The initial CR is informative enough to disambiguate a passage containing \"a deer near the river bank\" in the input text. The trial disambiguation step produces sense tagging of deer/ANIMAL and bank/GEO, but certain instances of bank are left untagged due to the lack of WSD knowledge. We observe that the bank-GEO sense in the context of vole is unresolved since there is no link between ANIMAL and GEOGRAPHY. Subsequently, the adaptation step adds deer and ANIMAL to the contextual representation for bank-GEO. The adapted CR is now enriched with information capable of disambiguating the instance of bank in the context of vole to produce the final disambiguation result. The rest of this paper is organized as follows. First of all, we will peresent how easy contexts are interpreted and ambiguous words are labeled in the initial disambiguation step using general knowledge derived from MRD. Next, we describe the adaptation step that uses the sense labels assigned to polysemous words. After that, we will describe the strategy of using the adapted knowledge base and defaults. Next, we will give a detailed account of experiments conducted to assess the effectiveness of the adaptive approach, including the experiment setup, results and evaluation. Following that, we will review the recent WSD literature from the perspective of various types of contextual knowledge and different representation schemes. Finally, we will draw conclusions. J. N. Chen Figure 1 General framework for adaptive WSD using MRD. Acquisition of Disambiguation Knowledge using MRD In this section, we will describe how the conceptual characterization technique is applied to MRD definitions and give examples of acquiring WSD knowledge. First, we will show word level definitions based on a lexical CR and then a conceptual CR. Next, we will show the advantage of including information gained from an example sentence. Finally, we will combine these techniques to perform adaptative WSD computation. --------------------------------------------------------------------------bank-GEO river lake land deer near \u2026 GEO MOTION ANIMAL ... bank-MONEY money account bill investigation check fraud MONEY COMMERCE CRIME ... Word Sense Lexical and Concepture Context --------------------------------------------------- Conceptualized Contextual Representation The word-based CR from MRD definitions is highly precise and effective but not broad enough to work alone effectively. Word-based sense representation is hampered by the difficulty of providing estimates for a very large parameter space leading to limited coverage in WSD. Certainly, there are many 6 J. N. Chen situations that call for a conceptual generalization of a word-based representation of word sense from an example sentence. For instance, the RIVER sense of bank in Example (1c) can be correctly interpreted by an MRD-based CR, but only when the contextual word river in the CR is generalized to all words related to RIVER, including the word stream: (1) a. a ribbon of mist along the river bank; b. a small excavation in the river bank; c. the left bank of the stream. There are many possible approaches to making such a generalization and deriving a conceptualized CR (CCR) of word sense. Chen and Chang [1998b] described one such approach based on thesaurus topics. The CCR for each MRD sense can be viewed as relating to words listed under some Longman Lexicon of Contemporary English [McArthur 1992, LLOCE] topics. By linking MRD senses to thesaurus senses and by classifying senses according to linked senses, we can derive the CCR for a sense definition. Table 2 shows the topical CCR for the senses of bank in LDOCE. Each sense in MRD is given a list of weighted LLOCE topics. The weights in the CCR are normalized to a sum of unity for the obvious reason. Table 3 shows the lists of words listed in LLOCE under the topics relevant to bank senses. We sum up the above description and outline the procedure as Algorithm 1 for creating a CCR for a word W of sense S with definition D. Algorithm 1: Creating a conceptualized contextual representation CCR(D w, s ) for a word W of sense S with definition D. Step 1: Run the TopSense algorithm described by Chen and Chang [1998b] to map D to SC(D), a set of semantic categories in a thesaurus. Step 2: Create a conceptualized contextual representation CCR(D w, s ) for sense S with definition D: CCR(D w, s ) = \u2211 \u2208 ) ( SC ) ( D T T WORD 3 , where WORD(T) is a set of related words in semantic category T. In the following, we demonstrate how Algorithm 1 works. Given the sense definition of bank.4.n.1 shown in Section 2.1.1, the CCR(bank.4.n.1) can be acquired by means of Algorithm 1. Step 1: After running the TopSense algorithm, we have SC(bank.4.n.1) = {Je, Jf, Jd}. Step 2: Next, we expand each of three topics in SC (bank.4.n.1) Contextual Representation from an Example Sentence Dictionary examples are intended to show typical use of words in context. Therefore, MRD examples provide rich information supplementary to definitions. In this section, we will describe a method for tagging bilingual sentences with sense labels based on dictionary definitions and translations in a bilingual MRD. However, the sense for each word in an example is not explicitly marked except for the word being defined. That limits the potential for using dictionary examples as knowledge sources for WSD. Gale, Church and Yarowsky [1992b] first pointed out that the strong constraint of one-sense-per-translation can be exploited to tag a bilingual corpus for training a statistical WSD model. Building on their idea, we describe a new method for tagging bilingual sentences, in the MRD or elsewhere, for automatic acquisition of the CR of senses. Sense Tagging Based on Conceptual Context Representation and Translations Now we are ready to propose a heuristic algorithm for tagging bilingual sentences with sense labels. First, the translation morphemes of an MRD definition are added to the CR so that not only the English context, but also the translation (in Chinese for the particular implementation of LDOCE/E-C we will be describing) is considered. For instance, the representation of MONEY-bank contains not only FINANCE words such as money, pay, cash, capital, account, charge, etc., but also the morphemes \"\u9280\" and \"\ufa08\" in the translation of the definition. (See Table 4 for some examples of bilingual context representations for the bank senses in LDOCE/E-C.) Subsequently, each CR for a polysemous word is compared with the bilingual sentences. The polysemous word is tagged in favor of the relevant CR that has the most overlap with the bilingual sentences. For instance, consider the case of tagging the instance of bank in Example (2) extracted from LDOCE/E-C: (2) a. the interest in my bank account accrued over the years; b. \u6211\u9280\ufa08\u5e33\u6236\u7684\uf9dd\u606f\u9010\uf98e\u6709\u6240\u589e\u52a0\u3002 Under the assumption of the one-sense-per-translation constraint, the morphemes \"\u9280\" and \"\ufa08\" in the translation are sufficient evidence for tagging the instance of bank as MONEY-bank. Even if such telling evidence is not present, there nevertheless is a great chance that the sentence contains enough words related to a relevant topic for correct sense tagging to happen. For instance, the FINANCE words, such as interest and account, in Example (2) lead to the correct sense label MONEY-bank for this instance of bank, even when it is not translated as \"\u9280\ufa08.\" The contextual representation derived from the MRD definition also acts as a safety net when the one-translation-per-sense constraint does not hold. For instance, based on the one-translation-per-sense constraint, the instance of star in Example (3) can not be labeled as ENTERTAINMENT-star because both the ENTERTAINMENT and HEAVENLY-BODY senses of star are translated as \"\u661f\": (3) a. she is a star with the theatre company; b. \u5979\u662f\u5287\u5718\u7684\u7d05\u661f\u3002 In such an event, the ENTERTAINMENT words, such as theatre and company, nonetheless result in the correct sense label: ENTERTAINMENT-star. A bilingual example in the MRD, or text in a bilingual corpus, can be tagged in the way described above, word by word and sentence by sentence. Unambiguous words with only one sense label are tagged as such. Tagging is done only for content words within the scope of this work. Function words can be treated similarly [Chang, Hsu and Chen 1996] . Sentences in English tagged as training materials can facilitate acquisition of WSD knowledge. The method for tagging a bilingual training corpus is summarized as Algorithm 2. Table 5 shows the result of applying Algorithm 2 to some LDOCE/E-C examples. Step 2: For each word W in an example sentence E, compute the similarity between its context and translation, C E , and each of the contextual representations CR(W,S) based on the Dice Coefficient: Sim (C E , CR(W, S)) = \u2211 \u00d7 \u2208 E | ) , ( | + | | ) ) , ( , In( 2 E C c S W CR C S W CR C , where In(a, B) = the weight of a in B, if a \u2208 B and 0, otherwise. Step 3: Label W in E with S * such that Sim (C E , CR(W,S * )) is maximized; Sim (C E , CR(W,S * )) = L Max Sim (C E , CR(W,L)) and is greater than a certain threshold. Acquiring Contextual Representations for Example Sentences Lexicalized and conceptualized CR can be constructed from tagged MRD examples in a fashion similar to that described in Section 2.1 for MRD definitions. Given an ambiguous word W labeled with sense S in a set of example sentences E w, s , every content word appearing in E is gathered to form LCR(E w, s ), shown as follows: LCR(E w, s ) = { x | x \u2208 E w, s and x is not a function word }. Table 6 shows some of the contextual words in the LDOCE examples that appear in the context of each of eight bank senses. Notice that the entry for MONEY-bank contains many strong collocates, such as (rob, bank), (bank, account), etc. These collocates are potentially very helpful for WSD. Although some of the contextual words merely repeat information in the definition-based representation, LCR(D w, s ) + CCR(D w, s ), many do provide new information. For instance, fifteen instances of river reaffirm the defining word river as an important collocate for RIVER-bank, while contextual words such as north, east, deer, and vole provide additional, richer context. J. N. Chen In the previous section, we showed that these contextual words are neither frequent nor necessarily likely to recur. However, when viewed as representing a typical topic or concept, they certainly are recurring. For instance, although there is only one instance of north bank in LDOCE examples, there are quite a few south bank, and right bank instances, all of which signal a recurring context of the DIRECTION concept. Therefore, it is a good idea to derive a conceptualized contextual representation from the set of examples E relevant to a sense label S. For instance, representing the co-occurring concept of the DIRECTION with RIVER-bank, CCR(E bank, river ) would contain such words as east, west, south, north, left, and right, etc.: CCR(E bank, river ) = { east, west, south, north, left, right, \u2026 }. For this purpose, we again turn to the information retrieval (IR) technique. Since the LDOCE in general strictly uses words in the controlled vocabulary for both definitions and examples, the same method described by Chen and Chang [1998b] for forming conceptual characterization of MRD definitions also works for MRD examples. Table 7 shows a list of topical words that characterize the context of each of the eight bank senses based on sense tagged LDOCE examples. The results obtained using an IR-based method seem to characterize the context in a general way that can be very useful for WSD.  To take into account the significance of each contextual word in CR(W, S), the IR technique for weighting index terms for relevancy can be applied here to good effect. Using the IR analogy, the collective context of each word sense is viewed as a document, and the relevance of a contextual word t to a sense S of word W depends on its term frequency tf and inverse document frequency idf. The term frequency tf is the number of instances of t in WORD(W, S), and idf is the percentage of CRs in which an instance of t appears. The relevancy of a contextual word is estimated using the commonly used scheme: tf \u00d7idf. Experiments show that the simple scheme tends to give a high weight to strong collocations, such as (rob, MONEY-bank) and (river, RIVER-bank), thus leading to a representation that is potentially very effective for WSD. Combining We sum up the above descriptions and outline the procedure as Algorithm 3. The algorithm combines definition-based and example-based CR into an integrated contextual representation CR(W, S) for the sense S of the polysemous word W. Algorithm 3: Combining definition-based CR Step 1: Given a polysemous word W, one of its senses S and a collection of bilingual examples C, run Algorithms 1 and 2 to obtain LCR(D w, s ), CCR(D w, s ), LCR(E w, s ) and CCR(E w, s ), where E is a set of examples that each contain an instance of S. Step 2: Merge the following word list for W and S: WORD(W, S) = LCR(D w, s ) + CCR(D w, s ) + LCR(E w, s ) + CCR(E w, s ). Step 3: For each WORD(W, S), compute a list of distinct words X with weight W X,S as follows: CR(W, S) = { X (W X, S ) | X is a distinct word in WORD(W, S)}, where tf X, S = the frequency of X in WORD(W, S), idf X = 1/the percentage of senses S such that X\u2208WORD(W, S), W X, S = tf X, S \u00d7 idf X. Step 4: The weights W X, S in CR(W, S) for each word sense S are normalized to a sum of 100. An Illustrative Example In the following, we will demonstrate how Algorithm 3 works. Given a MONEY-bank sense, the integrated CR(bank, MONEY) can be acquired by doing the following (where the numbers in parentheses following collocates denote the frequency): J. N. Chen Step 1: After running Algorithms 1 and 3, we obtain the following: Step 4: The weight W X, S in CR(W, S) for each word sense S is normalized to a sum of 100. For instance, the total of the weights CR(bank, MONEY) = 6274.5; therefore, the normalized weight W account, MONEY = 2.04. Table 8 shows more details about the contextual words and normalized weights in CR(bank, S) for all bank senses S. The ten top-weighted context words from the CRs of the bank senses listed in Table 8 seem to be very relevant to each sense and to have strong collocates listed in BBI [Benson, Benson and Ilson 1993] . These weighted context words form the general CCR knowledge for senses of bank. In the next section, we will show that this knowledge is effective for applying WSD to unrestricted text. , defense(0.20), bet(0.17), champion(0.17), competition(0.17), gamble(0.17), games(0.17), \u2026 LCR(D bank Word Sense Disambiguating Algorithms Among the recently proposed WSD systems, almost all have the property that the knowledge obtained is fixed when the system completes the training phase. This means that the acquired knowledge can not be enriched during the course of disambiguation. Such fixed knowledge is referred to as static knowledge. We believe that this property limits WSD performance. We propose lifting this limitation by adjusting the initial acquired knowledge to suit the text at hand. Alternatively, such expanded knowledge is referred to as adaptive knowledge. In this section, we will show how to distinguish between senses of text using adaptive disambiguation techniques. First, we will start with disambiguation of polysemous words in easy (trivial) contexts by using the fundamental knowledge previously acquired from MRD. Next, we will expand the acquired knowledge based on these disambiguated contexts. Finally, we will resolve the senses in the remaining contexts, called hard contexts. Disambiguating Polysemous Words in Easy Contexts The proposed WSD method starts with a simple disambiguation step using the topical CR described in a previous section. For instance, to disambiguate the word bank in Examples (4) through ( 6 ), the content words in its context are extracted, lemmatized and matched against the contextual representation of each of 16 J. N. Chen bank's word senses. Each instance of bank is given a sense label in favor of a CR most similar to the context in question. A sense label is assigned only when the match is strong enough and the runner-up sense is sufficiently weak. In the following subsections, we will describe how to distinguish between strong and weak signals. For instance, there is enough overlap between the CR for the instance of MONEY-bank in Examples ( 4 ) and ( 6 ) to warrant a sense label of MONEY-bank for the two instances of bank, but the match is not strong enough for the instance in Example (4). We call Examples ( 4 ) and ( 6 ) easy 5 contexts, while Example ( 5 ) is a hard 6 context. (4) \u2026 Participation loans are those made jointly by the SBA and banks or other private lending institutions ... (5) \u2026 individual action by every nation in position to help, we must squarely face this titanic challenge \u2026 (6) \u2026 from investment firms all over the nation, all of them wanting a part of shares that would be sold (185,000 to the public at $12.50 with another 5,000 reserved for Morton Foods employers at $11.50 a share) there was even a cable in French from a bank in Switzerland that had somehow \u2026 In addition, the contextual words closer to an ambiguous word may have greater influence on the sense of a word. For instance, consider Example (7), where the intended sense of bank is MONEY. We observe that there are two salient words, mortgage and river, around an ambiguous word bank. The word mortgage favors a MONEY sense, while the word river favors a RIVER sense. Intuitively, the MONEY sense should be given more favorable consideration since mortgage is nearer to the ambiguous word than river is. There are various representations for distance-based weights. Here we adopt the metric proposed by Hawking and Thistlewaite [1995] to weigh the relevance of salient words in a text. (7) \u2026 and an effort to get this religious center out of its rut of wild worship into a modern church organization. He emphasized to the Presiding Elder the plan of giving up the old church and moving across the river. The Presiding Elder was sure that that would be impossible. But he told Wilson to \"go ahead and try\". And Wilson tried. It did seem impossible. The bank which held the mortgage on the old church declared that the interest was considerably in arrears, and the real estate people said flatly that the land across the river was being held for an eventual development for white working people who were coming in, and that none would be sold to colored folk. When it was proposed to rebuild the church, Wilson found that the terms for \u2026 To sum up, we outline a general WSD method using MRD-based contextual representation as Algorithm 4 for labeling an instance of a polysemous word W in a particular context CON(W). Algorithm 4: (StaticSense) WSD using MRD-based contextual representation Step 1: Preprocess the context and produce a list of lemmatized content words CON(W) in W's context. Step 2: For each sense S of W, compute the similarity between the context representation CR(W, S) and topical context CON(W). Sim (CR(W,S), CON(W)) = \u2211 \u2211 + + \u2211 \u2208 \u2208 \u2208 ) , ( t ) ( s t, t s t, ) ( S W CR t W CON t M t W W W W , where M = CR(W,S) \u2229 CON(W), s t, W = the weight of a contextual word t with sense S in CR(W), t W = the weight of t in CON(W) = t 1 X , X t = the distance from t to W in number of words, S*(W, CON(W)) = s max arg Sim (CR(W,S), CON(W)), S\"(W, CON(W)) = s max arg {Sim (CR(W,S), CON(W)) | Sim (CR(W,S), CON(W)) < S*(W, CON(W))}, TSCORE(W, CON(W)) = )) ( , ( S\" )) ( , ( * S W CON W W CON W , RANK-S(W, CON(W)) = the rank of S*(W, CON(W)) among all S*(X, CON(X)) for all n instances of polysemous word X and context CON(X), RANK-T(W, CON(W)) = the rank of TSCORE(W, CON(W)) among TSCORE(X, CON(X)) for all n instances and context of polysemous word X. Step Step 4: DEFAULT(W)= S such that the count of (W, S, CON(W))\u03b5T is the largest among all the senses of W. Step 5: Assign (W, CON(W)) as the relevant sense S if (W, S, CON(W)) is in T, and assign DEFAULT(W) otherwise. Adapting the Knowledge Base to Fit the Text The adaptive approach to WSD hinges on two assumptions. First, we assume that it is possible to build J. N. Chen an initial general knowledge base so that a substantial portion of disambiguated text can be used to adapt the knowledge base to fit the text itself. The second condition for the adaptive approach to be feasible is that there is indeed new and effective information to be gained from the partially disambiguated text. In this section, we will first show the kinds of contexts in the Brown corpus and WSJ articles in which word sense ambiguity can be confidently resolved by using an MRD-based knowledge base. In these contexts, one will find an abundance of rich task-specific information not easily covered in a general or static knowledge base. We will also justify the use of contextual information and a task-specific default for WSD. Discovering Task-specific Contextual Information There is indeed an abundance of new and useful contextual information for word sense to be gained from typical, easy contexts. Such information can be extracted as long as ambiguity in these typical contexts can be interpreted successfully. For instance, the Brown corpus passage reproduced here as Example ( 8 ) is obviously very typical of MONEY-bank with salient words such as accounts, stocks and property in its context. Without a doubt, this instance of MONEY-bank can be resolved successfully using the kind of MRD-based knowledge base described in Section 3.1. Even though the overall context of this instance of MONEY-bank is a general one, it nevertheless contains many words, such as law and state, not in the MRD-based knowledge base. Such words might very well be incidental and have no intrinsic relation with the sense. For instance, the word law might just as likely be associated with RIVER-bank as MONEY-bank. Without much stretching of the imagination, it is possible to think of a likely event where the state of Texas passes a law to declare an outer bank off limits to commercial development. However, more often than not, these unexpected words will indicate real recurring contexts of word sense, either generally or in a task-specific way. Therefore, adapting the knowledge base to fit such a context is beneficial for WSD. For instance, the instances of tree and camping in the context of RIVER-bank in Example (9) seem to be reasonable additions to CR(bank, RIVER) in the sense that tree and camping are, in general, more strongly associated with RIVER-bank than with MONEY-bank. Even if that assertion generally does not hold, adding tree and camping to CR(bank, RIVER) as a way of adapting the knowledge base is still beneficial since it is likely to be valid in the very text where this association is discovered. The same argument holds for the local cue of through in the context of PILE-bank in Example (10), and for the instances of donor and transfusion in the context of MEDICINE-bank in Example ( 11 ). (See Using the Default Sense The distribution of senses of a word might not follow Zipf's law because their rank-frequency plot does not follow the power-law well, and it is often quite skewed even in a balanced corpus. In the Brown corpus, 60% of the instances of twelve polysemous words are the top-ranking sense of the word, according to an experimental report by Luk [1995] . Generally, the top-ranking sense of a word is corpus-dependent. Table 10 presents some statistics about the distribution of senses in different corpora. For instance, we find that CURIOSITY-interest is favored over MONEY-interest 194 to 49 in the Brown corpus, while preference is reversed with counts of 53 and 122 in the WSJ corpus. On the other case, GRAMMAR-sentence is favored over JUDGEMENT-sentence 22 and 10 in the Brown corpus while preference is reversed with counts of 1 to 11 in the WSJ corpus. Using a fixed default would be disastrous for interest or sentence in at least one of these corpora. The adaptive method alternatively uses a set of disambiguated samples from the text in question to estimate the default. The Adaptive WSD Algorithm We are now ready to present a new adaptive approach to WSD based on the fundamental knowledge base acquired from MRD. Previous sections have already shown how such a knowledge base can be built and described its advantages. We will show one way of using a MRD-based knowledge base for WSD. Although the knowledge base does not guarantee high precision and 100% coverage, a substantial portion, say 50%, can be disambiguated at a high precision rate. In this section, we will show how such a level of coverage and high precision can be put to use in an adaptive way to maintain the same high precision rate at 100% coverage. We will first describe the adaptive algorithm. Examples will be given in Section 3.4 to illustrate how the algorithm works and to give some idea of the potential effectiveness of adaptation. The algorithm starts with an initial disambiguation step using the knowledge base derived from the MRD. An adaptation step follows which produces a knowledge base from the partially disambiguated text. Finally, the undisambiguated part is disambiguated according to the adapted knowledge base. Algorithm 5 gives a formal and detailed description. Algorithm 5: (AdaptSense) Adaptive WSD Step 1: Run Algorithm 4 to obtain triples T 1 of word, word sense and context. Adptive 21 Step 2: From the selected triples (W, S, CON(W))\u2208T 1 , compute a new set of contextual representations: WORD(W,S) = { u | u\u2208CON(W)and (W, S, CON(W))\u2208T 1 }. Step 3: Build the contextual representation CR(W,S) of sense S of word W from WORD(W, S) according to Algorithm 3. DEFAULT(W) = S such that the count of (W, S, CON(W))\u2208T 1 is the highest among all the senses of W. Step 4: For all the instances of polysemous W and its CON(W) such that (W, S, CON(W)) is not in T 1 (for all senses S of W), Sim (CR(W,S), CON(W)) = \u2211 \u2211 + + \u2211 ) , ( in t ) ( in s t, t s t, in ) ( S W CR t W CON t M t W W W W , where M = CR(W,S) \u2229 CON(W), s t, W = the weight of a contextual word t with sense s in CR(W,S), t W = the weight of t in CON(W) = t 1 X , X t = the distance from t to W in number of words. S*(W, CON(W)) = RANK-S(W, CON(W)) = the rank of S*(W, CON(W)) among all S*(X, CON(X)) for all n instances of polysemous word X and context CON(X), RANK-T(W, CON(W)) = the rank of TSCORE(W, CON(W)) among TSCORE(X, CON(X)) for all n instances and context of polysemous word X. Step 5: Construct the set of triples T 2 , where T 2 = { (W, S, CON(W)) | S = S*(W, CON(W)) such that RANK-S(W, CON(W)) \u2264 n/c and RANK-T(W, CON(W)) \u2264 n/c, where the constant c \u2265 1 }. J. N. Chen Step 6: Assign (W, CON(W)) to the relevant sense S, such that ( W, S, CON(W))\u2208T 1 , or ( W, S, CON(W))\u2208T 2 , or DEFAULT(W), otherwise. An Illustrative Example To show how Algorithm 5 works in an adaptive fashion, we will consider the case of disambiguating the Brown corpus, focusing on the polysemous word bank. For this purpose, we will describe step by step how the algorithm operates on the two following passages in the Brown corpus containing an instance of bank. The two passages are reproduced here as Examples ( 12 ) and ( 13 ), showing a context window of 50 words before and after the polysemous word which is used in the algorithm for disambiguation. ( Huntley and her husband also will be questioned about \u2026 (13) \u2026 Of cattle in a pasture without throwin' 'em together for the purpose was called a \"pasture count\". The counters rode through the pasture countin' each bunch of grazin' cattle, and drifted it back so that it didn't get mixed with the uncounted cattle ahead. This method of countin' was usually done at the request, and in the presence, of a representative of the bank that held the papers against the herd. The notes and mortgages were spoken of as \"cattle paper\". A \"book count\" was the sellin' of cattle by the books, commonly resorted to in the early days, sometimes much to the profit of the seller. This led to the famous sayin' in the Northwest of the \"books won't freeze\". This became a common byword durin' the \u2026 Step1: Identifying an easy context This step corresponds to five substeps of Algorithm 4. First, only the salient words that are in CR(bank, S) for S in {MONEY, RIVER, EARTH, PILE, ROW, ROAD, MEDICINE, GAMBLE} are of interest; all other words are thrown out for now. To calculate similarity values, the weights for these words with respect to relevant senses are pulled out from the initial knowledge base. Tables 11 (a) and (b) show these words, their position relative to bank, and their weights according to a knowledge base extracted from LDOCE. The context of Example ( 12 ) resembles the CR of MONEY-bank the most. Table 11 (a) indicates clearly that very salient words in CR(bank, MONEY-bank), such as robbery, branch, and charge, occur in close proximity to the word bank. Although words related to other senses, such as drive and report, do occur, they are fewer and are located at quite a longer distance. It is not surprising that the similarity of this context with MONEY-bank and the t-score ranks high enough for this instance to be included in T 1 . On the other hand, Example (13) does not resemble the CR of any particular sense of bank more than it does those of other senses. That is evident from the weights shown in Table 11 (b). The only indicative word representative is not enough to enable interpretation of the intended sense of MONEY-bank. All other words are either not in any CRs or ambivalent (hold, note and paper), indicating a number of senses competing with MONEY-bank. Hence, the similarity of this context with MONEY-bank and the t-score do not rank high enough for this instance to be included in T 1 . Taylor said Mrs. Huntley and her husband also will be questioned about\" ) From the triples T 1 , a list WORD(S) of contextual words for each sense S of word bank and the most frequent sense DEFAULT(bank) are calculated. Therefore, the contextual words in the triple from Example (12) will be lemmatized. With stop words removed, we obtain a list like the following: , charge, assault, robbery, portland, detectives, say, Friday, mrs, lavaughn, huntley, accuse, drive, getaway, car, use, robbery, woodyard, bros, grocery, burnside, st, april, husband, sentence, year, federal, prison, mcneil, island, last, april, robbery, hillsdale, branch, multnomah, charge, store, holdup, secret, grand, jury, indictment, return, against, pair, last, week, detective, murray, logan, report, phoenix, arrest, culminate, year, investigation, detective, william, taylor, officer, taylor, say, mrs, huntley, husband , question, \u2026 } Adptive 25 WORD(MONEY-bank) = {face Step 3: Assigning weight to the contextual representation From the word lists for all senses of bank, the new set of CRs can be derived. The CR(bank, S) for the word sense S basically consists of every word in WORD(S) associated with a weight. Weights are assigned in favor of contextual words frequently occurring in the context of a particular word sense and that of a smaller number of other senses. For instance, the word cooperative occurs very frequently and only in the context of MONEY-bank in the part of the Brown corpus resolved in Step 1. According to our experiment, there are quite a number of bank instances in the Brown corpus that are very typical and can be reliably resolved using LDOCE-based contextual representation. Those instances are predominately resolved as MONEY-bank. Therefore, we have DEFAULT(bank) = MONEY-bank. J. N. Chen Steps 4-6: Disambiguating a hard context Armed with the new CRs, the instances that do not pass the test in Step 1 are re-evaluated again in Step 4. The similarity for each of those instances, including Example (13), is re-calculated for all possible word senses. The new weights for contextual words in Example ( 13 ) are shown in Table 13 . Contrary to the situation in Step 1, where the MRD-based CR is used, there are now more words in the context that are indicative of the intended sense. From the perspective of the new CRs, the words method, usual, request, paper, note, and book all point to the sense of MONEY-bank and not to any other sense. These words either do not exist or ambivalent with respect to the MRD-based CR. As a whole, these words provide enough evidence to reverse the previous inconclusive situation leading to the expected sense of MONEY-bank. In the event that the maximal similarity is lower than a threshold value, the default sense of MONEY-bank is used. In this particular case, the default happens to be correct. Experiment and Evaluation Experiment The experimental setup can be described in a number of steps as follows. (1) A set of 13 polysemous words was selected as the target for disambiguation and evaluation. (2) For each of the polysemous words, a sense division was established based on the LDOCE treatment of relevant nominal senses. The LDOCE's sense division was used largely as is, with only a couple of closely related senses merged. (3) Two sets of text from corpora were gathered as the test sets. (4) Two human judges were asked to assign a sense label to each nominal instance of these 13 words in the two test sets. ( 5 ) Two WSD programs were written to disambiguate nominal instances of these polysemous words in the test sets. ( 6 ) The results of running the two programs on both test sets were compared against those of human assessors. The number of test instances and that of correctly disambiguated ones in these four experiments were tallied to produce a precision rate for each experiment. In the following, we describe each step in turn. ) Test words We limited our experiment and evaluation to a set of thirteen words with higher than usual ambiguity. That is due mainly to the fact that the process of evaluation is a difficult and expensive one. It is often difficult to pin down the number of senses allowed for a word in the experiment. For the purpose of comparing results with other approaches, we stick to words that have been studied in various experiments reported in the literature on computational linguistics. These words include bank, bass, bow, cone, duty, galley, interest, issue, mole, sentence, slug, star, and taste. J. N. Chen (2) Sense division The sense division for each of these test words was very crucial in the WSD experiment. We used a sense division based on LDOCE's treatment of the nominal senses of these words. The division is somehow more fine-grained than those used in other WSD studies. This level of sense division is very close to the kind of granularity required for machine translation. For most cases, a word sense has a unique Chinese translation. (3) Test corpora We aimed to determine the effectiveness of the proposed approach for unrestricted text and to find out how domain and genre affect WSD. Therefore, we used the Brown corpus and a collection of WSJ articles from October 30 to November 2, 1989 as the test sets. Passages of 100 words centered at an instance of the test words in the two corpora were extracted using a SED program. It is in general not hard to write a regular expression in the SED program to exclude verbal instances, so only a small number of verb cases were extracted. These verbal instances were excluded from the experiment according to the marks made by human judges. For these thirteen words under investigation, we had 846 and 903 passages of nominal senses from the Brown corpus and WSJ test sets, respectively. (4) Judgement To be as subjective as possible, we asked two human judges to assign a sense label to each nominal instance of these thirteen words in the two test sets. There were also cases which fell out of the scope of our sense division. Most of these cases used proper nouns, so they bore none of the meaning represented in our sense division. Cases judged to be verbal uses or proper names were removed from the test cases. For instance, the word bow in a Brown corpus passage, reproduced here as Example ( 14 ), was an instance of a proper name and, therefore, was excluded from the test cases. (14) \u2026 The announcement that the secrets of the Dreadnought had been stolen was made in Bow St. police court here at the end of a three day hearing \u2026 (5) Static vs. Adaptive WSD In the previous sections, we argued in favor of using an MRD-derived knowledge base because we believe that the fundamental information in an MRD can be very helpful for WSD. Despite our belief in the effectiveness of the MRD-derived knowledge base, we also expected that adaptation could improve its effectiveness a bit further. Therefore, we implemented programs for both Algorithms 4 and 5. These two programs were executed in order to disambiguate the test cases in the Brown and WSJ corpora. Evaluation The results of running the two programs on both test sets were compared against those of human assessors. The number of test instances and that of correct assignments in these four experiments were tallied to Adptive 29 calculate the precision rate for each experiment. All results were based on 100% applicability 8 . Statistics for the experimental results are summarized in Tables 14 and 15 . Several observations can be made based on the results. First, evidently, the MRD-based knowledge base was reasonably helpful for WSD. The results shown in Tables 14 and 15 Discussion Although it is often difficult to compare results from experiments based on different domains, genres and setups, the experimental results presented here seem to compare favorably with the experimental results reported in previous WSD research. Our adaptive approach could disambiguate with an average precision rate of 71.2% for these thirteen words in Brown and of 76.5% for these words in WSJ. For the Brown corpus, Luk [1995] experimented with the same words we used except for the word bank and reported that there were totally 616 instances of these words (slightly less than the 749 instances we found). The precision rate for all instances was 60%. Leacock, Towell and Voorhees [1993] reported a precision rate of 76% for disambiguating the word line in a sample of WSJ articles. Besides the precision rate, a number of interesting features of this approach are also important. First, the proposed disambiguation system is robust and portable, since absolutely no corpus-specific knowledge is needed in the disambiguation procedure. It can be applied readily to test data in a variety of domains and genres with performance rivaling that of methods requiring a substantial training corpus. Second, the proposed approach is considerably more time efficient when compared to other learning strategies. Although the bootstrap approach proposed by Yarowsky [1995] has an element of adaptation to it, his method still requires a long training process to derive a static knowledge base for WSD. The differences between our method and his lie in the initial knowledge, the level of abstraction, and the learning cycle. We propose to exploit rich conceptualized knowledge from MRD at the outset, while the bootstrap method uses merely a couple of word collocations for each sense to start the learning process. Since the bootstrap method aims to derive a word-based conceptual representation with a large parameter space, a very large training corpus is required. The thesaurus used in the proposed approach provides an appropriate level of abstraction and, thus, alleviates the need for a very large corpus. The time required for learning in the two approaches is also quite different. The adaptive approach requires a single round of adaptation for effective WSD, while the bootstrap method needs many rounds of learning. Speedy adaptation is the consequence of using rich conceptualized knowledge to start the learning process. To show that this is truly the case, we have revised Algorithm 5 by adding a second and a third adaptation step and by applying the new CR to a reserved batch of low-ranking instances instead of using defaults. The results obtained using more adaptation steps are shown in Figure 2 . The precision rates show that the additional adaptation steps have only a marginal effect. Figure 2 Average precision rates with and without adaptation. One of the limiting factors of this approach is the quality of sense definition in the MRD. Short and vague definitions tend to lead to inclusion of inappropriate topics in the contextual representation. With such inferior CRs, it is not possible to produce enough precise samples in the initial step for subsequent adaptation. For instance, it is difficult to derive appropriate contextual knowledge for the LDOCE senses in (15) since their definitions mainly consist of either function words or very common words: ( The experiment and evaluation results show that adaptation is most effective when a high-frequency word with contrasting senses is involved. For low-frequency senses, such as EARTH, ROW, and ROAD senses of bank, the approach does not seem to be very effective. That is not a problem specific to the adaptive approach, and all other approaches in the literature suffer from the same problem of data sparseness. Even with static knowledge acquired from a very large corpus, these senses were disambiguated at a considerably lower precision rate than other senses. Related Work There has been increasing interest in using a machine to identify the intended sense of a polysemous word in a given context. Recently, various approaches to WSD have been proposed in the natural language processing literature, and old ideas have been superseded by newer ones at a rapid rate. Central to these development efforts are the kind of contextual knowledge encoded and the way this knowledge is represented and acquired. In this section, we review the recent literature on WSD from the perspectives of different types of contextual knowledge and their representational schemes. Lexicalized vs. Conceptual Encoding of Context Any kind of scheme for acquiring contextual information of word sense must begin with a way of identifying the word sense since word sense is an abstract concept not clear on the surface. Once this is done, we can use the surrounding words to build a contextual representation of the word sense for WSD. There are three approaches to the chicken-and-egg problem of dividing word senses. First, one can resort to human intervention to get a hand-tagged corpus of word senses. Most early WSD works used this approach and went to the trouble of hand-tagging the intended sense of each polysemous word in the training corpus [Kelly and Stone 1975; Hearst 1991 ]. Second, one can take the numbered sense entries readily available in a machine-readable dictionary and treat their definitions and examples as contextual information [Lesk 1986; Veronis and Ide 1990; Wilks et al. 1990; Guthrie et al. 1991] . The third way of identifying word sense exploits linguistic constraints. For instance, three linguistic constraints can be exploited for successful sense tagging and WSD. \u2022 One sense per discourse The senses of all instances of a polysemous word are highly consistent within any given document. \u2022One sense per collocation Nearby words provide strong and consistent clues to the sense of a target word, conditional on the relative distance, order, and syntactic relationship. \u2022One sense per translation Translations in a bilingual corpus can be used to represent the senses of words. As an example of the first constraint, consider the word suit. The constraint captures the intuition that if the first occurrence of suit is a LAWSUIT sense, then later occurrences in the same discourse are also likely to refer to LAWSUIT [Gale, Church and Yarowsky 1992a ]. The second constraint indicates that most works on statistical disambiguation have made the basic assumption that word sense is strongly correlated with certain contextual features, like occurrence of particular words in a window around the ambiguous word. However, Yarowsky [1995] proposed an approach in which strong collocations were identified for WSD. If a bilingual corpus was available, differences in translations of the polysemous word allowed one to delineate the intended sense, particularly in the case of contrasting polysemy. Gale, Church and Yarowsky [1992b] used French translations in parallel texts to disambiguate some polysemous words in English. For instance, the senses of duty were usually translated as two different French words, droit and devoir, respectively, representing the senses tax and obligation. Thus, a number of tax sense instances of duty could be collected by extracting instances of duty that were translated as droit, and the same could be done for obligation sense instances of duty. Once word senses are identified in one way or another, the context of a particular word sense can then be acquired and encoded in some way for use in the subsequent disambiguation step. There are at least two ways of encoding contextual knowledge. The obvious way, the lexicalized representation, is a surface scheme that keeps a weighted list of words appearing in the context of a particular sense. On the other hand, the conceptual representation encodes the classes of words that might appear in the context. Lexicalized Representation of Context Dictionary Definitions as Context Lesk [1986] described a word-sense disambiguation technique based on the number of overlaps between words in a dictionary definition and the fixed-size window of words surrounding the target. The author reported WSD performance ranging from 50% to 70% when the method was applied to a sample of ambiguous words. Lesk's method had failed to determine the correct senses of words when two or more senses of a word had the same number of overlaps with the context. Veronis and Ide [1990] constructed an artificial neural network from sense definitions, representing each word in the definition text as a node in the network. Different senses of each word competed with each other through the mechanism of spreading activation initiated at the nodes of contextual words. White [1988] , Guthrie et al. [1991] , and Slator [1991] used measures of words in context overlapping with dictionary definitions. One major problem of these earlier approaches was their lack of abstraction. The rich semantic information in the definition, such as the genus term, differentia, and implicit topics, was not exploited to the fullest. Gale, Church and Yarowsky [1992b] indicated that translation in a bilingual corpus could be used to provide tagged material for supervised learning of WSD knowledge. In their experiment, French translations were, in effect, used to represent the senses of some English words under the assumption of one-sense-per-translation. The Bayesian model was used to represent the contextual words in terms of their probabilities of occurrence. They reported a 90% accuracy rate in discriminating between two constrasting senses of six ambiguous nouns in the Canadian Hansards: duty, drug, land, language, position, and sentence. The weaknesses of this approach include the dreaded problem of data sparseness. Even when a very large corpus is available, it is still difficult to guarantee that each word sense will have enough contextual samples to avoid running into the problem of zero frequency, namely, the difficulty of assigning appropriate probabilistic values to words that do not appear in these contextual samples. Context as Co-occurrence Probabilities Topical vs. Local Representation of Context In almost all the studies described in Section 2.1, topical context was used in WSD. In a number of research works related to machine translation, researchers have used local context to solve a problem closely related to WSD, namely, the lexical choice problem. We will examine these two different kinds of contextual information in this section. Topical Context With topical representation of context, the context of a given sense of a target word is a bag of words without any structure. Information in topical context is generally quite helpful for WSD. For instance, consider Examples ( 16 ) and ( 17 ) extracted from the Brown corpus, each containing an instance of the ambiguous word bass. (16) \u2026 for scintillating flights of meaningless improvisations, and he has a quiet way of getting back and restating the melody after the improvising is over. In this he is sticking with tradition, however far removed from it he may seem to be. SHEARING TAKES OVER George Shearing took over with his well disciplined group, a sextet consisting of vibes, guitar, bass, drums, Shearing's piano and a bongo drummer. He met with enthusiastic audience approval, especially when he swung from jazz to Latin American things like the Mambo. Shearing, himself, seemed to me to be playing better piano than in his recent Newport appearances. A very casual, pleasant program-one of those easy-going things that make Newport's afternoon programs such a \u2026 (17) \u2026 Breakfast was at the Palace Hotel, luncheon was somewhere in the mountain forest, and dinner was either at Boulder Creek or at Santa Cruz. Gazing too long at the scenery could be tiring, so halts were contrived between meals. Then the Chinese hostler, who rode with Vernon on the box, would break open a hamper and produce filets of smoked bass or sturgeon, sandwiches, pickled eggs, and a rum sangaree to be heated over a spirit lamp. In spring and in autumn the run was made for a group of botanists which included an old friend of mine. They gathered roots, bulbs, odd ferns, leaves, and bits of resin from the rare Santa Lucia fir, which exists only on a forty-five mile strip on the westerly side of these mountains. In the Spanish \u2026 Intuitively, the first instance of bass can be disambiguated as INSTRUMENT-bass since guitar, drum, piano, jazz, etc. are likely to appear in the topical context of INSTRUMENT-bass. Similarly, the second instance can be disambiguated as FISH-bass since meal, sandwiches, egg, etc. are often found in the topical context of FISH-bass. Generally, the sense representation of topical context is acquired from a very large corpus. Gale, Church and Yarowsky [1992b] experimented on acquiring topical context from a substantial bilingual training corpus and reported good results. Local Context Local context includes structured information about word order, distance, and syntactic features. For instance, the local context of a line from does not suggest the same sense for the word line as a line for does. Brown et al. [1990] used the trigram model as a way of resolving sense ambiguity for lexical selection in statistical machine translation. This model makes the assumption that only the previous two words have any effect on the translation, and thus, the word sense of the next word. Trigram as Local Context The model was used to attack the problem of lexical ambiguity and produced satisfactory results, under some strong assumptions. For instance, the authors showed that the French sentence Je vais prendre la decision could be correctly translated as I will make the decision using this model. Although in isolation, take was more likely than make to translate as prendre, the trigram language reversed the decision in favor of make. A major problem with the trigram model is long distance dependency. For instance, the model incorrectly rendered the French sentence Je vais prendre ma propre decision as I will take my own decision. The language model did not consider make my own decision more probable since prendre and decision did not fall within a window of three words. Lexical Relation Dagan, Itai and Schwall [1991] and Dagan and Itai [1994] made use of translations of different senses from a Hebrew/English bilingual dictionary to disambiguate contexts. Local context in the form of lexical relations was analyzed in a foreign corpus. The basic idea of the algorithm is best explained with an example. Given two Hebrew words hoze and shalom, hoze has two translations in English: contract and treaty, while shalom is often translated into English as peace. Their experiment showed that all instances of peace appear before treaty and none before contract in the corpus of English language. Therefore, the authors concluded that this instance of hoze in the phrase hoze shalom was best translated as treaty. The authors experimented on lexical choice with 105 Hebrew words and 54 German words from news articles. The precision rates achieved ranged from 75% to 92% for coverage rates between 59% and 70%. Brown et al. [1991] described a statistical algorithm for partitioning the senses of a word into two groups. The authors used mutual information to find a local contextual feature that most reliably indicated which of the senses of the French ambiguous word was used. For instance, for the verb prendre, the object was a good indicator: prendre une measure translated as to take a measure, and prendre une decision as to make a decision. Therefore, words (any word, first verb or first noun) immediately to the left or right of the word were evaluated for their effectiveness as good indicators for WSD and lexical choice. The authors reported 20% improvement in the performance of a machine translation system (from 37 to 45 sentences correct out of 100) when the words were first disambiguated in this way. Approximating Lexical Relation Smoothing Co-occurrence Probabilities Yarowsky [1992] improved on the WSD method proposed by Gale, Church and Yarowsky [1992b] by smoothing the concurrence probability via predefined semantic classification. Basically, that was done by lumping the probabilities related to all the senses in a thesaurus category to smooth the zero frequency cases. For instance, the contextual information of bird and other animals was used to build a contextual representation for all the senses in the animal category in Roget's Thesaurus [1987] . His experiment showed in a close test using Grolier's Encyclopedia that instances of twelve words, bass, bow, cone, duty, galley, interest, issue, mole, sentence, slug, star, and taste, could be disambiguated with an average precision rate of 92%. However, a very large corpus is required to train such a lexicalized contextual model, and clearly this kind of static model has a portability problem. Conceptual Representation of Context Context as Definition-Based Conceptual Co-occurrence Luk [1995] advocated using defining words in the MRD for the contextual representation of word sense. Reminiscent of an earlier work by Wilks et al. [1990] , Luk proposed a definition-based concept co-occurrence model (DBCC) for WSD. With the model, the context of each word sense is represented using a vector of LDOCE defining words in the sense definition. The author argued that by using a fixed, relatively small number of concepts, a small corpus could provide enough concept co-occurrence data for statistical sense disambiguation. In a close test, the DBCC model trained on the Brown corpus was found to be capable of disambiguating 60% 9 of the instances of the same twelve ambiguous words used in Yarowsky's experiment. Context as Thesaurus Categories Many researchers have exploited the semantic categories in a thesaurus, such as Roget's and LLOCE, or the subject information in a dictionary for context representation and WSD. Walker and Amsler [1986] applied subject codes in LDOCE as semantic representation for WSD. Black [1988] reported an accuracy rate of around 50% when Walker and Amsler's algorithm was applied to a sample of five ambiguous words: interest, point, power, state, and term. Pure conceptual representation is the most economical kind of WSD model since it requires the smallest parameter space and requires no substantial texts for training. Chen et al. [1996] proposed a mixed representational scheme for context based on contextual words as well as LLOCE topics. With a contextual representation acquired from example sentences in LDOCE/E-C, the authors reported that the method could disambiguate around 70% of the instances of thirteen polysemous words in the Brown corpus. J. N. Chen Conclusions We have described an adaptive approach to word sense disambiguation. Under this new learning strategy, a contextual representation for each sense discriminator is first built based on the sense definition and example sentence in MRD and represented as a weighted-vector of concepts represented by word lists in a thesaurus. This knowledge representation acquired through MRD is based on a limited number of concepts; thus, the dreaded problem of data sparseness is avoided. Conceptual knowledge also offers the additional advantages of reduced storage requirements and increased efficiency due to reduced dimensionality. Also, we can correctly identify at least 50% of the word senses in unrestricted In addition, these disambiguated texts can be used to adjust the fundamental knowledge in an adaptive fashion so to improve disambiguation precision. We have demonstrated that this approach can outperform established static approaches based on direct comparison of results obtained for the same words. This level of performance is achieved without lengthy training or the use of a very large training corpus. Appendix A A Glossary of LLOCE Topics Here, we list 129 topics found in LLOCE. The column labeled \"Topic\" shows a set of two-character symbols representing the topics in LLOCE. Each topic is giving a gloss.",
    "abstract": "This paper describes a general framework for adaptive conceptual word sense disambiguation. The proposed system begins with knowledge acquisition from machine-readable dictionaries. Central to the approach is the adaptive step that enriches the initial knowledge base with knowledge gleaned from the partial disambiguated text. Once the knowledge base is adjusted to suit the text at hand, it is applied to the text again to finalize the disambiguation decision. Definitions and example sentences from the Longman Dictionary of Contemporary English are employed as training materials for word sense disambiguation, while passages from the Brown corpus and Wall Street Journal (WSJ) articles are used for testing. An experiment showed that adaptation did significantly improve the success rate. For thirteen highly ambiguous words, the proposed method disambiguated with an average precision rate of 70.5% for the Brown corpus and 77.3% for the WSJ articles.",
    "countries": [
        "Taiwan"
    ],
    "languages": [
        "English",
        "Chinese",
        "Latin",
        "French"
    ],
    "numcitedby": "2",
    "year": "2000",
    "month": "August",
    "title": "Adaptive Word Sense Disambiguation Using Lexical Knowledge in a Machine-readable Dictionary"
}