{
    "article": "The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-itskind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area. Project page of E-KAR can be found at https:// ekar-leaderboard.github.io. Introduction Analogy holds a vital place in human cognition, driving the discovery of new insights and the justification of everyday reasoning (Johnson-Laird, 2006; Gentner and Smith, 2012; Bartha, 2013; Bengio et al., 2021) . Due to their unique value in many fields such as creativity (Goel, 1997) and education (Thagard, 1992) , analogy and analogical reasoning have become a focus in AI research. The grand question is, are artificial neural networks also capable of recognizing analogies? Relatively little attention has been paid in NLP to answer this question. The problem of recognizing analogies is mainly benchmarked in the form Both \"teapot\" and \"teacup\" are containers for holding \"tea\" . After the \"tea\" is brewed in the \"teapot\" , it is transported into the \"teacup\" . After \"textbooks\" are printed in the \"printing factory\", they are sold in a \"bookstore\". But the terms order is inconsistent with the query. passengers :bus :taxi 1 2 3 A) bus 2 taxi 3 transportation for passengers 1 is_a is_a bus 2 taxi 3 transport passengers 1 \"Passengers\" do not need to be transported into \"taxi\" after taking a \"bus\". \"Taxi\" and \"bus\" are different ways of transportation. magazine :bookshelf :reading room 1 2 3 B) bookshelf 2 reading room 3 ? is_a is_a The \"bookshelf\" is in the \"reading room\". talents :school :enterprise 1 2 3 C) school 2 enterprise 3 organization for talents 1 is_a is_a school 2 enterprise 3 transport talents 1 Both \"school\" and \"enterprise\" are organizations. After \"talents\" are educated in \"school\", they are transported into \"enterprise\". of (A:B::C:D) (Turney et al., 2003; Mikolov et al., 2013b; Gladkova et al., 2016; Li et al., 2018a) and targeted for testing the ability of pre-trained word embeddings. Given a tuple of terms as query (e.g., tea:teapot:teacup) and a list of candidate answers as in Figure 1 , a model needs to find the most analogous candidate to the query, which is C in the example since it matches the relations inherent in the query better than others. Most methods (Mikolov et al., 2013a; Levy and Goldberg, 2014; Pennington et al., 2014) hold a connectionist assumption (Feldman and Ballard, 1982) of linear analogy (Ethayarajh et al., 2019) , that the relation between two words can be estimated by vector arithmetic of word embeddings. For example, king \u2212 man + woman = queen. However, current benchmarks focus on the recognition of binary analogies such as syntactic, morphological and direct semantic (e.g., is_a and syn-onym_of ) relations. And the analogical reasoning procedure behind them is far beyond the scope of this line of research. In addition, how to explain and rationalize analogical reasoning remains to be the major challenge. Psychological literature (Gick and Holyoak, 1983; Gentner, 1983; Minnameier, 2010) suggests that analogical reasoning follows the structure-mapping process. That is, a target (the domain where a problem must be solved, i.e., candidates) and a source (the domain where the analogy is drawn, i.e., the query) are matched, and the relevant features of the source have to be mapped onto the target. In Figure 1 , source structures are drawn (or abduced) from the query and mapped onto candidates, and candidates A, B, D all fail at certain structures. We argue that such a process can be verbalized into natural language to explain analogical reasoning. Moving from simply recognizing analogies to exploring human-like reasoning for neural models, we emphasize the importance of a new kind of analogical reasoning benchmark. To fill in this blank, we propose a first-of-its-kind benchmark for Explainable Knowledge-intensive Analogical Reasoning (E-KAR). We collect 1,655 analogical reasoning problems sourced from the publicly available Civil Service Examinations (CSE) of China. These CSE problems are challenging multiplechoice problems designed by human experts, thus solving them requires the intensive involvement of linguistic, commonsense, encyclopedic, and cultural (e.g., idiom and historical) knowledge. To justify the reasoning process, we follow the aforementioned guidelines from psychological theories and manually annotate free-text explanations for each query and candidate answers in E-KAR. Since the annotation requires intensive involvement of knowledge and reasoning, we carefully design a double-check procedure for quality control. We also translate this dataset into an English version, resulting in 1,251 problems after discarding language and cultural specific cases. In summary, our contributions include: \u2022 We advance the traditional setting of word analogy recognition by introducing a knowledge-intensive analogical reasoning benchmark (E-KAR) in Chinese and English, which is first-of-its-kind and challenging. \u2022 To justify the analogical reasoning process, we design free-text explanations according to theories on human cognition, and manually annotate them. \u2022 In E-KAR, we define two tasks (analogical QA and explanation generation) in two modes (EASY and HARD) and report the performance of some state-of-the-art language models. We discuss the potentials of this benchmark and hope it facilitates future research on analogical reasoning. Related Work Word Analogy Recognition in NLP Benchmarks for word analogy recognition (Turney et al., 2003; Mikolov et al., 2013b; Gladkova et al., 2016; Li et al., 2018a) examine mostly linear relations between words (Ethayarajh et al., 2019) . Such analogies can often be effectively solved by vector arithmetic for neural word embeddings, such as Word2Vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) . Recent studies (Brown et al., 2020; Ushio et al., 2021 ) also test such ability of pre-trained language models (PLMs) (Radford et al., 2019; Devlin et al., 2019; Brown et al., 2020) on these benchmarks. An exceptional benchmark is Li et al. (2020) , where they build a knowledgeenhanced analogy benchmark that leverages word sense definitions in a commonsense knowledge base (Ma and Shih, 2018) . However, these benchmarks are mainly set up for evaluating learned representations, and few of them ever investigated the analogical reasoning skills for neural models. Thus, the goal of this work largely differs from this line of research, as we aim to build a knowledge-intensive benchmark to teach neural models analogical reasoning for correct thinking. Reasoning Benchmarks from Examinations There are abundant benchmarks derived from human examinations to facilitate the study of machine reasoning (Clark et al., 2016; Schoenick et al., 2017) . For example, RACE (Lai et al., 2017) is collected from the English exams for middle and high school students, focusing on skills of passage summarization and attitude analysis. ARC (Clark et al., 2018) contains natural, grade-school science questions authored for human tests. MCQA (Guo et al., 2017) , GeoSQA (Huang et al., 2019) and GCRC (Tan et al., 2021) are sourced from national college entrance exams of China, measuring a comprehensive set of reasoning abilities. LogiQA (Liu et al., 2020a) consists of logical reading comprehension problems from Civil Service Exams of China, which is also our source of analogical problems. ReClor (Yu et al., 2020) and LR-LSAT (Wang et al., 2021) , collected from Law School Admission Test, aim for testing logical reasoning abilities. In our work, we focus on analogical reasoning skills for machines and additionally equip E-KAR with annotated explanations to rationalize reasoning. Explainable NLP Datasets One of the most prominent objectives in machine reasoning is giving reasons for a prediction. In current datasets for explainable NLP, such reasons can be categorized into three classes (Wiegreffe and Marasovi\u0107, 2021) : 1) highlights explanations (Camburu et al., 2018; Yang et al., 2018; Thorne et al., 2018; Kwiatkowski et al., 2019) , which are subsets of the input elements to explain a prediction, e.g., words or sentences; 2) free-text explanations (Camburu et al., 2018; Zellers et al., 2019; Aggarwal et al., 2021) that are textual explanations for justification; 3) structured explanations (Mihaylov et al., 2018; Khot et al., 2020; Clark et al., 2020; Jhamtani and Clark, 2020; Geva et al., 2021) , which are not fully free-text and generally follow certain structures such as a chain of facts. The explanations can be utilized to augment (Rajani et al., 2019) , supervise (Camburu et al., 2020) and evaluate (DeYoung et al., 2020) model predictions. In this work, we phrase analogical reasoning itself as an instance of machine reasoning tasks with free-text rationales, advancing the research on analogical reasoning from the perspectives of data collection. Explainable Analogical Reasoning In this work, we consider a classic setting of analogical reasoning within NLP: recognizing word/term analogies. 1 This task can be formulated as multiplechoice question-answering. Given a query tuple Q with k (two or three) terms, and m candidate answer tuples A = {A i } m i=1 , the goal is to find the most analogous one in the candidates to the query. We advocate that reasoning is about giving reasons explaining a prediction. In order to teach machines to analogize as humans do, we draw inspiration from theories in cognitive psychology to design the forms of explanations. Analogical Reasoning: A Psychological Perspective Before designing suitable forms of explanations, we introduce some important theories from cognitive psychology for a better understanding of analogical reasoning. In the psychological literature, analogical reasoning is described as a schemainduction (Gick and Holyoak, 1983) or structuremapping (Gentner, 1983) process. Peirce (1896) claimed that analogy is a combination of abductive and inductive reasoning. Minnameier (2010) further developed the inferential process of analogy into three steps, which we take as the guidelines for designing explanations: 1. A possibly suitable structure in the source domain is abduced from the target domain, which might also work for the target; 2. The specific concepts of the source structure have to be replaced by suitable target concepts (by an inductive inference); 3. The validity of the transformation is judged w.r.t. solving the target problem. Take Figure 1 for example: Source structures can be abduced that both term 2 (teapot) and term 3 (teacup) belong to a concept, and term 1 (tea) can be transported from term 2 to term 3. The mapping naturally reveals the validity, for example, candidate A is wrong because passengers do not follow a unidirectional transportation (i.e., from bus to taxi) but a bidirectional one. Explanations for Analogical Reasoning Following the above guidelines, the explanations for the analogical reasoning task should also include three parts: 1. Abduction: description of suitable structures for the query; 2. Mapping: how the structure is mapped onto candidates, analogous to template-filling; 3. Validation: justification for the correctness of the counterfactual mapping. To this end, we define free-text explanation for analogical reasoning, which is one of the most expressive and commonly-used explanations (Wiegreffe and Marasovi\u0107, 2021) . We ensure the free-text explanations are self-contained, knowledge-rich, and sufficient to solve the problem as a substitute for the original input. Specifically, for each query (Q) and candidate (A i ), we define free-text explanations E Q and E A i . Following the guidelines in \u00a73.1, E Q should describe the best suited inherent structure of a query abduced from the problem. E A i should decide the correctness in mapping the counterfactual A i into structure expressed in E Q , while providing facts as support evidence. 4 The E-KAR Benchmark Dataset Collection We build our dataset upon the publicly available problems of Civil Service Exams of China (CSE), which is a comprehensive test for candidates' critical thinking and problem-solving abilities. CSE consists of problems that test various types of reasoning skills, such as graphical reasoning, logical reasoning and comprehension (Liu et al., 2020b) , analogical reasoning, etc. We collect in total 1,655 Chinese analogical reasoning problems from CSE over the years, each of them consisting of a query term tuple and four candidate answer tuples of terms (as shown in Figure 1 ). One of the prominent features in CSE problems is the intensive involvement of commonsense, encyclopedic, and idiom knowledge. For example, one needs to be aware of the fact that \"the tide is caused by both Lunar gravity and Solar gravity\". More importantly, one needs to know a negated fact (Barker and Jago, 2012; Hossain et al., 2020; Hosseini et al., 2021) in order to reject a candidate, such as the fact that \"husband is not a job\" or \"a car is not made of tires\". We keep mainly those requiring knowledge and reasoning skills. The rest is manually removed, such as the ones testing mathematics, morphology, and phonics, as well as the problems with the number of terms larger than three. Manual Annotation of Explanations We work with a private company for annotating the explanations defined in \u00a73.2. Before annotation starts, we conduct a training session for all annotators to fully understand the requirements and pick the capable ones based on a selection test. The selected workers are allocated into two teams, a team of explanation constructors and a team of checkers, where the checkers achieves better scores in the test. All of them are paid above the local minimum  wage. The annotation consists of two stages: 1) the construction stage for writing explanations, and 2) the double-check stage for quality control. Construction During annotation, each problem is assigned to a constructor to build five sentences of explanations: one for query and four for candidate answers. The explanations are required to be: 1) fluent and factually correct, 2) able to solve the problem on their own, and 3) knowledge-rich. To reduce the labeling difficulty, we allow them to use the search engine for querying the Internet. First-round Checking Afterward, a problem with five annotated explanations is fed to a checker for a first-round checking. The checker decides whether to accept an explanation sentence according to the criteria in the construction stage. The rejected ones are sent back to the construction team for revision along with reasons to reject, which serve to re-train the construction team. The process repeats until a batch reaches 90% accuracy (i.e., decided to be correct according to the checker). Then, a second-round checking initiates. Second-round Checking A verified batch is presented to authors for double-checking. Authors conduct random inspections for 50% samples of a batch, and unqualified annotations are sent back with reasons to the check team to fine-tune their checking criteria, which in turn regularize the construction team. The process also repeats until a batch reaches 95% accuracy. In the end, the authors manually calibrate every explanation and acquire 1,655 analogical problems and a total number of 8,275 (5\u00d71,655) free-text explanations, with an average of 31.9 Chinese characters per sentence. Bilingual E-KAR: English and Chinese For a broader impact of this work, we also build an English version of E-KAR via translation. To translate the Chinese E-KAR into English, we ask three Chinese undergraduate students majoring in English to post-edit the machine-translated results of E-KAR by Google. Besides translation fluency, we also make sure that 1) terms in options and explanations have the same word stems; 2) the parts of speech of terms in a query or candidate answer are encouraged to be the same. However, in practice, we notice that some samples in the Chinese dataset can not be accurately translated into English, such as ones involving idioms, poems, and other knowledge of Chinese culture. Such samples could be hard for non-Chinese people and models to understand without culture-specific knowledge. Therefore, in the English E-KAR, we manually remove or rewrite these samples, resulting in 1,251 problems and 6,255 (5\u00d71,251) explanations that would require mostly commonsense and factual knowledge and reasoning skills that are universal across cultures and languages. Nevertheless, those removed samples are valid ones, and the cultural knowledge within them could be of unique value to the Chinese NLP community. Thus, we keep all samples in the Chinese E-KAR to encourage the research of Chinese NLP. In the end, we have a bilingual E-KAR for rationalizing analogical reasoning. Both versions of E-KAR are randomly split into training, development, and test set at the ratio of 7:1:2. The statistics of E-KAR as well as comparison between previous benchmarks are reported in Table 1 , including SAT (Turney et al., 2003) , Google (Mikolov et al., 2013b) and BATS (Gladkova et al., 2016) . There are 35.5%/39.5% problems with three terms in E-KAR, whereas previous ones only consist of two, making E-KAR even more challenging. Shared Tasks in E-KAR Given input X = (Q, A), the ultimate goal is to make the correct choice Y, while producing rational explanations E = {E Q , E A = {E A i } i }. To this end, we define two shared tasks, multiple-choice question-answering (QA) and explanation generation (EG), for teaching models how to analogize. Moreover, to reduce the difficulty of this task as well as follow the structure-mapping process (as in \u00a73), we propose an easier task form of the shared tasks by adding E Q into input X . Next, we will elaborate on these settings. Task 1: Analogical QA The analogical QA task is formulated as P QA (Y|X ). The QA task requires an understanding of the relationship between the query and each of the candidates to find the correct answer. For evaluation, we directly use the accuracy of multiple-choice QA. Note that all candidates may be related to the query tuple from certain perspectives. The challenge lies in finding the most related one, i.e., to identify the inherent connections and relations between terms in the query and candidates, considering properties such as linguistic features, order of terms, commonsense knowledge, etc. For example, the error for candidate D in Figure 1 can be attributed to the incorrect term order, though three terms follow similar relations as in the query. Hence, the best choice is C. Task 2: Explanation Generation This task aims to produce a pipelined rationalization for analogical reasoning, formulated as P EG (E|X ). The generated explanations E can be further utilized for the analogical QA, i.e., P QA (Y|X , E). Note that the EG task does not generate post-hoc explanations for the QA task, therefore there will not be any predicted choice labels in the input X . Rather, it indicates that the model should make implicit label predictions in explanations (Wiegreffe et al., 2021) . The generated explanations can be directly evaluated the same as text generation tasks. Or, indirectly, we can follow a pipelined rationalization paradigm and see how generated explanations can help downstream QA tasks. Task Mode: EASY vs. HARD The abduction of source structure (query explanation E Q ) is critical but difficult for making rational analogical reasoning. Therefore, we propose two task modes: \u2022 HARD mode: the original setting, where only Q and A are available in X ; \u2022 EASY mode: in addition to Q and A, E Q is allowed as part of the given input X . Essentially, EASY mode sets a much clearer playground for evaluating a system's ability to validate counterfactuals (as in \u00a73.2): What if candidate terms follow the structures in the query instead of query terms? Will they hold logically? Therefore, we believe it to be an important supplement for E-KAR benchmark. Methods In this section, we describe the baseline methods in both QA and EG tasks in EASY and HARD modes. We mainly evaluate some of the state-of-the-art language models for solving tasks in E-KAR. Some implementation details are reported in Appendix A. Baselines for Analogical QA Pre-trained Methods As pre-trained-only baselines, we adopt three static word embeddings that have shown their effectiveness in previous analogy tasks: Word2Vec (Mikolov et al., 2013a) , GloVe (Pennington et al., 2014) and FastText (Bojanowski et al., 2017) . We also test contextualized embeddings from PLMs, including BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) . The averaged token representation is taken as the term representation. A query or a candidate is estimated by the sum of the representations of each term pair, which is represented as the embedding vector differences (Hakami and Bollegala, 2017; Ushio et al., 2021) . The candidate with the highest cosine similarity to the query is chosen as the answer. Fine-tuned Methods We also set up finetuned baselines for QA with PLMs (BERT and RoBERTa). Since previous benchmarks do not have a training set, we only fine-tune the models on their development set. The query and candidates are respectively verbalized into text using simple prompts, and an example prompt can be found in Appendix A.1. Each candidate is concatenated with the query into one sentence, which is fed into a PLM for contextualized representation learning. Averaged hidden states are then fed to an MLP layer and a softmax layer for classification. Human Evaluation We ask three students to solve the QA task in E-KAR, who are undergraduate or graduate students and fluent in English and Chinese. We randomly sample 100 problems from E-KAR of each language. Subjects are asked to first solve them in HARD mode then in EASY mode, in order to reveal the change in performance of the same problem when prompted with the query explanation. The averaged score is reported as the human baseline. Baselines for Explanation Generation We formulate the EG task in a Seq2Seq paradigm, instantiated with state-of-the-art pre-trained lan-guage models for Seq2Seq tasks, including BART (Lewis et al., 2020; Shao et al., 2021) and T5 (Raffel et al., 2020; Zhang et al., 2021) . Although the explanation is individually specific to each query and candidate, the generator has to take into account the whole problem for generating with the best source structure (as in \u00a73.1) and thus finding the most analogous candidate. Similar to fine-tuned methods in QA task, the EG model takes as input the concatenation of the query Q and all candidate answers A (and the query explanation E Q if in EASY mode). Note that in HARD mode, we switch the prefix of input from generating for Q or A i in order to distinguish between generating explanations for the query or candidate answer. An example prompt is presented in Appendix A.1. Evaluation for the EG Task In HARD mode, both the generated explanations for query E Q and candidate answers E A should be evaluated. In EASY mode, since E Q is fed into the model as input, only E A are required for evaluation. The generated text can be evaluated with text generation metrics such as ROUGE (Lin, 2004) , BERTScore 2 (Zhang et al., 2020) , BLEURT (Sellam et al., 2020) and MoverScore (Zhao et al., 2019) . However, we would like to highlight that great challenges remain for automatically evaluating semantic-rich text generation (Celikyilmaz et al., 2020) . We also follow the pipelined rationalization paradigm and calculate the gain on QA accuracy as a supplement evaluation metric, i.e., the accuracy drop of P QA (Y|X , E) over P QA (Y|X , E gold ). This metric is denoted as Acc (\u2206), where Acc is the QA accuracy when including generated explanations E as input during inference, and \u2206 reflects the accuracy drop. Here we fix a trained QA model P QA (\u2022) based on a large-version RoBERTa. This model is designed to be different from the ones in the QA task, as it is fine-tuned by concatenating gold explanations to the corresponding query or candidates as input during training (prompt detail can be found in Appendix A.1). As an evaluation metric, we alter the input explanations to the model from gold E to generated E, and see their performance drops over gold. Note that the query explanation E Q is still the input for all settings in EASY mode. Results and Analysis In the experiments, we wish to answer two questions: Q1) Can models do knowledge-intensive analogical QA? Q2) Can models generate rational reasons for analogical thinking? Categorization of Problems We first manually categorize the relational types of problems in E-KAR according to a pre-defined schema. Unlike free text, we are unable to induce a comprehensive set of relations that covers all candidates due to the complexity of CSE problems. As a result, we carefully assign at least one relation to each query. To facilitate analysis, we also try to assign relations to each candidate and query in the development and test set, ending up covering 76% of the candidates and 100% of the queries. We refer to several sources of word analogy definitions and textbooks for analogy tests (listed in Appendix B), and categorize the relations into five meta-relations (as well as their coverage in the test set) and several accompanying sub-relations: 1. Semantic (R1, 8.36% for Zh, 4.12% for En), the similarity or difference in the meaning of terms, including synonym_of and antonym_of ; 2. Extension (R2, 41.25% for Zh, 42.30% for En), the relation between the extension of terms, including is_a, contradictory_to, etc.; 3. Intension (R3, 37.94% for Zh, 40.21% for En), terms relate to each other by inherent properties, including made_of, has_function, etc.; 4. Grammar (R4, 6.36% for Zh, 6.72% for En), the grammatical relations between terms, including subject-predicate, head-modifier, etc.; 5. Association (R5, 6.08% for Zh, 6.65% for En), logical association between terms, including result_of, sufficient_to, etc. Complete sub-relations are presented in Appendix B, as well as their definitions and examples. Can models do knowledge-intensive analogical reasoning? Table 2 reports the accuracy results of baseline methods on previous analogy tasks and the QA task in E-KAR. How do machines solve analogical reasoning problems? To answer this question based on Table 2, the findings can be summarized as: 1) We find contextualized word embeddings from PLMs not very competitive against static word embeddings in previous analogy tasks, which is consistent with the findings in Peters et al. (2018) . 2) In a more knowledge-intensive E-KAR, the opposite conclusion can be made, with PLMs prevailing over static word embeddings. 3) Furthermore, performance from contextualized representations can be improved in all tasks through fine-tuning, especially for E-KAR, where accuracy increases by roughly 5 to 6 points. 4) When incorporating gold source structure (i.e., EASY mode), the QA results significantly improve by roughly 5 points in both languages. 5) Moreover, despite our efforts to eliminate culture-specific samples in English E-KAR, the accuracy still falls behind its Chinese counterpart, which could be attribute to: a) fewer training samples, b) language-specific pre-training and c) language-specific information noise by translation. How do humans solve analogical reasoning problems? In contrast to machines, humans achieve in E-KAR 77.8% accuracy in HARD mode and 83.3% in EASY mode, indicating the challenge of this task as well as showing that current SOTA language models still fall far behind human performance. We also find the trend of human performance is generally aligned with machines, with accuracy boost (also \u223c5 points) when prompted with query explanations.    Error Analysis for QA We further conduct an error analysis based on the results in E-KAR (Zh) predicted by a fine-tuned RoBERTa (large). The erroneous ones are classified based on the manually annotated meta-relations and sub-relations of queries, which is a fine-grained tool for analyzing a model's predictions. Figure 2 (a) shows that the model performs poorly on nearly all meta-relations, with R2 (Extension) being the most error-prone one (only 40.3% accuracy, normalized) and R3 (Intension) being the least one (56.8% accuracy). One of the most prominent reasons is that R2 and R3 rely heavily on commonsense and encyclopedic knowledge and reasoning skills such as commonsense and world knowledge, at which current models easily fail. Figure 2 (b) shows the error rate of sub-relations with more than 10 samples. Consistent with Figure 2 (a), the three most error-prone sub-relations (is_a, part_of and juxtaposition_of ) all belong to R2 (Extension). Besides, the model seems to do well in linguistic knowledge, with verb-object achieving only 33.3% error rate. These findings may shed light on future directions for knowledge-intensive reasoning with language models. Can models rationalize analogical thinking? We report the automatic evaluation results of generated explanations in Table 3 . However, such results hardly mean anything due to the incapability to evaluate the semantic-rich text of current automatic metrics. Therefore, the following analyses mainly focus on Acc (\u2206) and human evaluation. Can (generated) explanations benefit analogical QA? To start with, we highlight again that the QA model in Table 3 is different from the one in Table 2 since the training of the former involves gold explanations. When exposing gold explanations to the QA model, it achieves 97.7% accuracy on E-KAR of both languages coincidentally. However, the QA model performs poorly when removing the explanations during inference (i.e., None). This is because the pipelined rationalization in training makes the QA model rely heavily on the rationales (explanations) than the problem itself, and the removal of them causes severe performance degradation. When we switch the explanations to generated ones during inference, the accuracy gap (\u2206) between gold results slightly narrows, with the gain in EASY mode being more significant than in HARD mode. To conclude, current SOTA generative language models still fall short of rationalizing analogical reasoning, which would be a challenging but interesting future direction. Error Analysis for EG We also randomly select 100 sentences generated by a BART (large) for manual inspection by the authors. Aside from  the common errors in generation models such as repetition, we find that task-specific errors for generated explanations can be roughly categorized into three classes: 1) unable to generate negated facts to refute source structure; 2) generating factually incorrect statements; 3) biasing towards common patterns, e.g., \"term 1 and term 2 have similar meanings\" and \"term 1 is a term 2\". For example, in Table 4 , both generated E Q (only in HARD mode) and E A are factually incorrect, and the model fails to generate the negated fact that \"both are not exclusively made of one component.\" We dig further into the first class of errors (w.r.t. negation), which is important to refute a candidate, as mentioned in \u00a74.1. We find \u223c90% gold explanations of wrong candidates contain negated statements. Yet, the number drops to 14.9% (Zh) and 22.1% (En) in the generated ones in HARD mode, and 21.3% (Zh) and 38.6% (En) in EASY mode. An interesting conclusion can be drawn that current generative models do not seem to know how to generate a negated yet truthful fact, such as \"feeling can not guide psychological reaction.\" since feeling is a reaction. And exposing source structure to the model (EASY mode) seems to alleviate this problem. The fact also questions the astonishing QA performance by adding gold explanations (97.7%), as the model could be biased towards surface-level negation. To debias this, we conduct a simple ablation study by directly removing the clauses containing the negation word \"\u4e0d\" (not) from the gold explanations in the test set, and still achieve 92.5% in QA accuracy. This finding indicates that the QA model with correct rationales would not be very much biased towards negation in the explanation. Conclusion and Discussion In this work, we propose a first-of-its-kind benchmark E-KAR (in both Chinese and English) for explainable analogical reasoning, which sets a concrete playground and evaluation benchmark to boost the development of human-like analogical reasoning algorithms. The E-KAR benchmark is featured by its rich coverage in knowledge and welldesigned free-text explanations to rationalize the analogical reasoning process. Preliminary experiments show that this benchmark provides a rather difficult challenge for prevailing language models. However, there are still many open questions to be addressed. For example, humans solve the analogy problems in a trial-and-error manner, i.e., adjusting the abduced source structure and trying to find the most suited one for all candidate answers. However, the explanation annotation process in E-KAR (not the EG task) is mostly post-hoc and reflects only the result of reasoning. Such explanations cannot offer supervision for intermediate reasoning, though it is an interesting question whether an intelligent model should be deeply supervised at every step (Tafjord et al., 2021) . Furthermore, E-KAR only presents one feasible explanation for each problem, whereas there may be several. This benchmark also invites reasoning models that can effectively interact with extra knowledge. It remains to be a great challenge to generate and evaluate factually correct explanation text. Especially, how to generate negated facts is relatively under-explored in the research community but of much importance. Finally, whether the analogical QA system can correctly exploit explanations and background knowledge is also worth investigating, which may intersect with research on debiasing (Tang et al., 2020; Niu et al., 2021) . We hope this work to be a valuable supplement to future research on natural language reasoning, especially for research on analogical reasoning and explainable NLP. Ethical Considerations This paper proposes a new kind of analogical benchmark with explanations to rationalize models' predictions. The dataset is collected from Civil Service Exams of China, which is publicly available and has been used in other public datasets before, such as LogiQA (Liu et al., 2020a) . The annotated explanations for each problem in our dataset are crowd-sourced by working with ByteDance. The construction team remains anonymous to the authors, and the annotation quality is guaranteed by the double-check strategy as mentioned in \u00a74.2. We ensure that all annotators' privacy rights are respected in the annotation process. All annotators have been paid above local minimum wage and consented to use the datasets for research purposes covered in our paper. R4: Grammar 6.36% 6.72% 1) subject-predicate The originator of the action and the action itself. plane : take off 1.19% 1.25% 2) verb-object The action and the object on which the action acts. transfer : goods 3.14% 3.36% 3) head-modifier The preceding term modifies the other. affluence : living 0.87% 0.74% 4) subject-object The originator and receiver of an action. dairy farmer : milk 1.16% 1.37% R5: Association 6.08% 6.65% 1) result_of One term causes the other. lack of water : plants wither 2.99% 2.97% 2) follow The terms have a chronological or other sequential relationship, but one term does not cause the other. sign up : take the exam 1.91% 2.19% 3) sufficient_to One term is a sufficient condition for the other. raining : wet ground 0.0% 0.0% 4) necessary_to One term is a necessary condition for the other. admission : graduation 1.18% 1.49% Acknowledgement We thank the anonymous reviewers for their valuable suggestions. We also thank Ruxin Yu for the logo design. A Implementation Details The pre-trained word embeddings are provided by Li et al. (2018b) , and the checkpoints for PLMs are hosted in HuggingFace (Wolf et al., 2020) . Most of the parameters in the baseline models take the default values from HuggingFace's Transformers library, and we keep the best checkpoint on the validation set for testing. The Chinese version of BERT (whole word masking) and RoBERTa (whole word masking extended) are provided by Cui et al. (2020) , BART by Shao et al. (2021) and T5 by Zhang et al. (2021) . 3 Thus the EG results of T5 in E-KAR (zh) can be attributed to both Raffel et al. (2020) and Zhang et al. (2021) . A.1 Example Prompts in E-KAR We denote terms in a query Q or a candidate A * \u2208 {A, B, C, D} as t {1,2} Q/A * . The example prompts for the QA and EG tasks in E-KAR are: \u2022 A Prompt for the QA Task: \"(context: \u2022 A Prompt for the EG Task: D </s> generate the explanation of Q/A i :\". \u2022 A Prompt for the QA model in Acc \u2206: concatenating explanations to the query and each candidate answer, such as \"t 1 Q : t 2 Q </s> explanation: E Q \" and \"t 1 A : t 2 A </s> explanation: E A \". B Detailed Relation Definitions To design the relation taxonomy, we refer to a number of sources that categorize types of analogy tests, including MAT 4 , Fibonicci 5 , Offcn Education (in Chinese) 6 and Huatu Education (in Chinese) 7 , etc. The complete set of meta-relations and subrelations are presented in Table 5 .",
    "abstract": "The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-itskind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area. Project page of E-KAR can be found at https:// ekar-leaderboard.github.io.",
    "countries": [
        "China"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "{E}-{KAR}: A Benchmark for Rationalizing Natural Language Analogical Reasoning"
}