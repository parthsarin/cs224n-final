{
    "article": "We present a pilot study of word-sense annotation using multiple annotators, relatively polysemous words, and a heterogenous corpus. Annotators selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that annotators agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and annotators that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators. Introduction Our goal is to grapple seriously with the natural sense variation arising from individual differences in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995) , and that serve different functions (Kittredge et al., 1991) . Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001) , much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; Nunberg, 1979) . Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannotator agreement (IA) for reasons that in the long run should be explicitly modelled in order for Natural Language Processing (NLP) applications to handle usage differences more robustly. This pilot study is a step in that direction. We present related work in the next section, then describe the annotation task in the following one. In Section 4, we present examples of variation in agreement on a matched subset of words. In Section 5 we discuss why we believe the observed variation depends on the words and present three lexical use factors we hypothesize to lead to greater or lesser IA. In Section 6, we use association rules to mine our data for systematic differences among annotators, thus to explain the variations in IA. We conclude with a summary of our findings goals. Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf . (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005) ), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005) . Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al ). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006) , the number or granularity of senses (Ng et al., 1999) , merging of related senses (Snow et al., 2007) , sense similarity (Chugur et al., 2002) , sense perplexity (Diab, 2004) , entropy (Diab, 2004; Palmer et al., 2005) , and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006) . With respect to using multiple annotators, Snow et al. included disambiguation of the word president-a relatively non-polysemous word with three senses-in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken from the SemEval Word Sense Disambiguation Lexical Sample task. Majority voting among three annotators achieve 99% accuracy. The Annotation Task The Manually Annotated Sub-Corpus (MASC) project is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC). 1 The MASC corpus includes handvalidated or manually produced annotations for a variety of linguistic phenomena. One of the goals of the project is to support efforts to harmonize Word-Net (Miller et al., 1993) and FrameNet (Ruppenhofer et al., 2006) , in order to bring the sense distinctions each makes into better alignment. As a starting sample, we chose ten fairly frequent, moderately polysemous words for sense tagging, targeting in particular words that do not yet exist in FrameNet, as well as words with different numbers of senses in the two resources. The ten words with part of speech, number of senses, and occurrences in the OANC are shown in Table 1 . One thousand occurrences of each word , including all occurrences appearing in the MASC subset and others semi-randomly 2 chosen from the remainder of the 15 million word OANC, were annotated by at least one annotator of six undergraduate annotators at Vassar College and Columbia University. Fifty occurrences of each word in context were sense-tagged by all six annotators for the in-depth study of inter-annotator agreement (IA) reported here. We have just finished collecting annotations of fifty new occurrences. All annotations are pro-duced using the custom-built interface to WordNet shown in Figure 1 : the sentence context is at the top with the word in boldface (fair), a comment region below that allows the annotator to keep notes, and a scrollable area below that shows three of the ten WordNet senses for \"fair.\" 4 Observation: Varying Agreement, depending on Lexical Items We expected to find varying levels of interannotator agreement (IA) among all six annotators, depending on obvious grouping factors such as the part of speech, or the number of senses per word. We do find widely varying levels of agreement, but as described here, most of the variation does not depend on these a priori factors. Inherent usage properties of the words themselves, and systematic patterns of variation across annotators, seem to be the primary factors, with a secondary effect of part of speech. In previous work (Passonneau, 2004) , we have discussed why we use Krippendorff's \u03b1 (Krippendorff, 1980) , and for purposes of comparison we also report Cohen's \u03ba; note the similarity in values 3 . As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff (Krippendorff, 1980) considers values above 0.67 moderately good, and values above 0.80 as quite good. (cf. (Artstein and Poesio, 2008) for discussion of agreement measurement for computational linguistic tasks.) Table 2 shows IA for a pair of adjectives, nouns and verbs from our sample for which the IA scores are at the extremes (high and low) in each pair: the average delta is 0.24. Note that the agreement decreases as part-of-speech varies from adjectives to nouns to verbs, but for all three parts-of-speech, there is a wide spread of values. It is striking, given that the same annotators did all words, that one in each pair has relatively better agreement. The average of the agreement values shown in Table 2 (\u03b1=0.4164; \u03ba=0.4191) is somewhat higher than the average 0.317 found for 191 words annotated for WordNet senses in (Ng et al., 1999) , but lower than their recomputed \u03ba of 0.85 for verbs, after they reanalyzed the data to merge senses for 42 of the verbs. It is widely recognized that achieving high \u03ba scores (or percent agreement between annotators, cf. (Palmer et al., 2005) ) is difficult for word sense annotation. Given that the same annotators have higher IA on some words, and lower on others, we hypothesize that it is the word usages themselves that lead to the high deltas in IA for each part-of-speech pair. We discuss the impact of three factors on the observed variations in agreement: 1. Greater specificity in the contexts of use leads to higher agreement 2. More concrete senses give rise to higher agreement 3. A sense inventory with closely related senses (e.g., relatively lower average inter-sense similarity scores) gives rise to lower agreement Explanatory Factors First we list factors that can not explain the variation in Table 2 ) with an abbreviated gloss, followed by the number of annotators who chose it. Ruled Out Factors It appears that neither annotator expertise, a word's part of speech, the number of senses in WordNet, the number of senses annotators find in the corpus, nor the nature of the distribution across senses, can account for the variation in IA in Table 2 . All six annotators used the same annotation tool, the same guidelines, and had already become experienced in the word sense annotation task. The six annotators all exhibit roughly the same performance. We measure an individual annotator's performance by computing the average pairwise IA (IA 2 ). For every annotator A i , we first compute the pairwise agreement of A i with every other annotator, then average. This gives us a measure for comparing individual annotators with each other: annotators that have a higher IA 2 have more agreement, on average, with other annotators. Note that we get the same ranking of individuals when for each annotator, we calculate how much the agreement among the five remaining annotators improves over the agreement among all six annotators. If agreement improves relatively more when annotator A i is dropped, then A i agrees less well with the other five annotators. While both approaches give the same ranking among annotators, IA 2 also provides a number that has an interpretable value. On a word-by-word basis, some annotators do better than others. For example, for long, the best annotator (A) has IA 2 =0.79, and the worst (F) has 0.44. However, across ten words annotated by all six, the average of their IA 2 is 0.39 with a standard deviation of 0.037. F at 0.32 is an outlier; apart from F, annotators have similar IA across words. Table 2 lists the distribution of available senses in WordNet for the four words (column 4), and the number of senses used (column 5). The words work and tell have relatively fewer senses (seven and eigith) compared with nine through twelve for the other words. However, neither the number (or proportion) of senses used by annotators, nor the distribution across senses, has a significant correlation with IA, as given by Pearson's correlation test. Lexical Use Factors Underspecified contexts lead to ambiguous word meanings, a factor that has been recognized as be-ing associated with polysemous contexts (Palmer et al., 2005) . We find that the converse is also true: relatively specific contexts reduce ambiguity. The word long seems to engender the greatest IA primarily because the contexts are concrete and specific, with a secondary effect that adjectives have higher IA overall than the other parts of speech. Sentences such as (1.), where a specific unit of temporal or spatial measurement is mentioned (months), restrict the sense to extent in space or time. In the few cases where annotators disagree on long, the context is less specific or less concrete. In example (2.), long is predicated of the word chapter, which has non-concrete senses that exemplify a certain type of productive polysemy (Pustejovsky, 1991) . It can be taken to refer to a physical object (a specific set of pages in an actual book), or a conceptual object (the abstract literary work). The adjective inherits this polysemy. The three annotators who agree on sense two (spatial extent) might have the physical object sense in mind; the two who select sense one (temporal extent) possibly took the point of view of the reader who requires a long time to read the chapter. Several of the senses of work are concrete, and quite distinct: sense seven, \"an artist's or writer's output\"; sense three, \"the occupation you are paid for\"; sense five, \"unit of force in physics\"; sense six, \"the place where one works.\" These are the senses most often selected by a majority of annotators. Senses one and two, which are closely related, are the two senses most often selected by different annotators for the same instance. They also represent examples of productive polysemy, here between an activity sense (sense one) and a product-of-theactivity sense (sense two). Example (3) shows a sen-tence where the verb perform restricts the meaning to the activity sense, which all annotators selected. The work performed by Rustom and colleagues suggests that cell protrusions are a general mechanism for cell-to-cell communication and that information exchange is occurring through the direct membrane continuity of connected cells independently of exo-and endocytosis. WN S1.activity of making something [N=6 of 6] In sentence (4.), four annotators selected sense one (activity) and two selected sense two (result): For the word fair, if five or six annotators agree, often they have selected sense one-\"free of favoritism or bias\"-as in example (5). However, this sense is often selected along with sense two-\"not excessive or extreme\"as in example (6). Both senses are relatively abstract. Example (7) illustrates a case where all annotators agreed on a sense for land. The named entity India restricts the meaning to sense five, \"territory occupied by a nation.\" Apart from a few such cases of high consensus, land seems to have low agreement due to senses being so closely related they can be merged. Senses one and seven both have to do with property (cf. example ( 8 ))., senses three and five with geopolitical senses, and senses two and four with the earth's surface or soil. If these three pairs of senses are merged into three senses, the IA goes up from 0.2627 to 0.3677. Examples for tell and show exhibit the same trend in which agreement is greater when the sense is more specific or concrete, which we illustrate briefly with show. Example (9) describes a specific work of art, an El Greco painting, and agreement is universal among the six annotators on sense 5. In contrast, example (10) shows a fifty-fifty split among annotators for a sentence with a very specific context, an experiment regarding delivery of a DNA solution, but where the sense is abstract rather than concrete: the argument of show is an abstract proposition, namely a conclusion is drawn regarding what the experiment demonstrates, rather than a concrete result such as a specific measurement, or statistical outcome. Sense two in fact contains the word \"experiment\" that occurs in (9), which presumably biases the choice of sense two. Impressionistically, senses two and three appear to be quite similar. 9. El Greco shows St. Augustine and St. Stephen, in splendid ecclesiastical garb, lifting the count's body. WN S5.show in, or as in, a picture, N=6 of 6 10. These experiments show that low-volume jet injection specifically targeted delivery of a DNA solution to the skin and that the injection paths did not reach into the underlying tissue. WN S2.establish the validity of something, as by an example, explanation or experiment, N=3 of 6 WN S3.provide evidence for, N=3 of 6 By insisting that everything Quantifying Sense Similarity Application of an inter-sense similarity measure (ISM) proposed in (Ide, 2006) to the sense inventories for each of the six words supports the observation that words with very similar senses have lower IA scores. ISM is computed for each pair in a given word's sense inventory, using a variant of the lesk measure (Banerjee and Pedersen, 2002) . Agglomerative clustering may then be applied to the resulting similarity matrix to reveal the overall pattern of inter-sense relations. ISMs for senses pairs of long, fair, work, land, tell, and show range from 0 to 1.44. 4 We compute a confusion threshhold CT based on the ISMs for all 250 sense pairs as CT = \u00b5 A + 2\u03c3 A where A is the sum of the ISMs for the six words' 250 sense pairs. Table 3 shows the ISM statistics for the six words. The values show that the ISMs for work and long are significantly lower than for land and fair. The ISMs for the two verbs in the study, show and tell, are distributed across nearly the same range (0 -1.38 and 0 -1.22, respectively), despite substantially lower IA scores for show. However, the ISMs for three of show's sense pairs are well above CT , vs. one for tell, suggesting that in addition to the range of ISMs for a given word's senses, the number of sense pairs with high similarity contributes to low IA. Overall, the correlation between the percentage of ISMs above CT for the words in this study and their IA scores is .8, which supports this claim. the annotators who choose one sense versus those who choose another. Mining association rules to find strong relations has been studied in many domains (see for instance (Agrawal et al., 1993; Zaki et al., 1997; Salleb-Aouissi et al., 2007) ). Here we illustrate how association rules can be used to mine relations such as systematic differences in word sense choices across annotators. An association rule is an expression C 1 \u21d2 C 2 , where C 1 and C 2 express conditions on features describing the instances in a dataset. The strength of the rules is usually evaluated by means of measures such as Support (Supp) and Confidence (Conf). Where C, C 1 and C 2 express conditions on attributes: POS Word \u2022 Supp(C) is the fraction of instances satisfying C \u2022 Supp(C 1 \u21d2 C 2 ) = Supp(C 1 \u2227 C 2 ) \u2022 Conf(C 1 \u21d2 C 2 ) = Supp(C 1 \u2227 C 2 )/Supp(C 1 ) Given two thresholds MinSupp (for minimum support) and MinConf (for minimum confidence), a rule is strong when its support is greater than MinSupp and its confidence greater than MinConf. Discovering strong rules is usually a two-step process of retrieving instances above MinSupp, then from these retrieving instances above MinConf. The types of association rules to mine can include any attributes in either the left hand side or the right hand side of rules. In our data, the attributes consist of the word sense assigned by annotators, the annotators, and the instances (words). In order to find rules that relate annotators to each other, the dataset must be pre-processed to produce flat (two-dimensional) tables. Here we focus on annotators to get a flat table in which each line corresponds to an annotator/sense combination: Annotator Sense. We denote the six annotators as A1 through A6, and word senses by WordNet sense number. Here are 15 unique pairs of annotators, so one way to look at where agreements occur is to determine how many of these pairs choose the same sense with nonnegligible support and confidence. Tell has much better IA than show, but less than long and work. We would expect association rules among many pairs of annotators for some but not all of its senses. We find 11 pairs of rules of the form A i T ell:Sense1 \u2192 A j T ell:Sense1, A j T ell:Sense1 \u2192 A i T ell:Sense1, indicating a bi-directional relationship between pairs of annotators choosing the same sense, with support ranging from 14% to 44% and confidence ranging from 37% to 96%. This indicates good support and confidence for many possible pairs Our interest here is primarily in mining for systematic disagreements thus we now turn to pairs of rules where in one rule, an attribute Annotator Sense i occurs in the left hand side, and a distinct attribute Annotator Sense j occurs in the right. Again, we are especially interested in 4 shows some general classes of disagreement rules using a compact representation with a bidirectional arrow, along with a table of variables for the different pairs of annotators associated with different levels of support and confidence. For fair, Table 4 summarizes three pairs of rules with good support (16-20% of all instances) in which one annotator chooses sense 1 of fair and another chooses sense 2: A3 and A5 choose sense 1 where A6 chooses sense 2, and A1 chooses sense 1 where A2 chooses sense 2. The confidence varies for each rule, thus in 100% of cases where A6 selects sense 2 of fair, A3 selects sense 1, but in only 32.3% of cases is the converse true. Example (6) where half the annotators picked sense 1 of fair and half picked sense 2 falls into the set of instances covered by these rules. The rules indicate this is not isolated, but rather part of a systematic pattern of usage. The word land had the lowest interannotator agreement among the six annotators, with eight of eleven senses were used overall (cf. Table 2 ). Here we did not find pairs of rules in which distinct Annotator Sense attributes that occur in the left and right sides of one rule occur in the right and left sides of another rule. For show, Table 4 illustrates two systematic divisions among groups of annotators. With rather good support ranging from 12% to 32%, senses 2 and 3 exhibit a systematic difference: annotators A1, A4 and A5 select sense 2 where annotators A3, A3 and A6 select sense 3. Similarly, senses 5 and 10 exhibit a systematic difference: with a more modest support of 8% to 12%, annotators A1, A3, A4 and A5 select sense 5 where annotators A2 and A6 select sense 10. Conclusion We have performed a sense assignment experiment among multiple annotators for word occurrences drawn from a broad range of genres, rather than the domainspecific data utilized in many studies. The selected words were all moderately polysemous. Based on the results, we identify several factors that distinguish words with high vs. low interannotator agreement scores. We also show the use of association rules to mine the data for systematic annotator differences. Where relevant, the results can be used to merge senses, as done in much previous work, or to identify internal structure within a set of senses, such as a word-based sense-hierarchy. In our future work, we want to develop the use of association rules in several ways. First, we hope to fully automated the process of finding systematic patterns of difference across annotators. Second, we hope to extend their use to mining associations among the representations of instances in order to further investigate the lexical use factors discussed here. Acknowledgments This work was supported in part by National Science Foundation grant CRI-0708952.",
    "abstract": "We present a pilot study of word-sense annotation using multiple annotators, relatively polysemous words, and a heterogenous corpus. Annotators selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that annotators agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and annotators that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "23",
    "year": "2009",
    "month": "June",
    "title": "Making Sense of Word Sense Variation"
}