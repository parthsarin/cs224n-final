{
    "article": "We describe ud2rrg, a rule-based approach for converting UD trees to Role and Reference Grammar (RRG) structures. Our conversion method aims at facilitating the annotation of multilingual RRG treebanks. ud2rrg uses general and language-specific conversion rules. In order to evaluate ud2rrg, we approximate the subsequent annotation effort via measures of tree edit distance. Our evaluation, based on English, German, French, Russian, and Farsi, shows that the ud2rrg transformation of UD-parsed data constitutes a highly useful starting point for multilingual RRG treebanking. Once a sufficient amount of data has been annotated in this way, the automatic conversion can be replaced by a statistical parser trained on that data for an even better starting point. 2 A clausal tree is an elementary tree whose lexical anchor is the head of a clause. Its spine starts with SENTENCE, CLAUSE, CORE, or NUC, followed by nodes for the lower clausal layers. 3 By merging we mean attaching any children of the former under the latter. Introduction Role and Reference Grammar (RRG) (Van Valin Jr. and LaPolla, 1997; Van Valin Jr., 2005) is a grammar theory for natural language that shares with Universal Dependencies (Nivre et al., 2016; Nivre et al., 2020) the aim of being descriptively adequate across typologically diverse languages while reflecting their commonalities in its analyses. It also shares with UD a number of design characteristics, such as recognizing dissociated nuclei and the principle to \"annotate what is there\", eschewing the use of empty elements, cf. de Marneffe et al. (2021) . In addition, RRG's separation between constituent structure and operator structure (the latter reflecting the attachment of functional elements) offers an explanatory framework for certain word-order and semantic phenomena. In recent years, the computational linguistics community has become increasingly interested in RRG and has started to formalize RRG (Osswald and Kallmeyer, 2018) and to build resources and tools to support data-driven linguistic research within RRG (Bladier et al., 2018; Bladier et al., 2020; Chiarcos and F\u00e4th, 2019) . As illustrated in the examples in Figure 1 , an important feature of RRG is the layered structure of the clause. The nucleus (NUC) contains the (verbal) predicate, arguments attach at the core (CORE) layer, and extracted arguments at the clause (CLAUSE) layer. Each layer also has a periphery, where adjuncts attach (marked PERI). Operators (closed-class elements encoding tense, modality, aspect, negation, etc.) attach at the layer over which they take scope. They are assumed to be part of a separate projection, but we collapse both projections into a single tree structure for convenience. Elements like wh-words in English are placed in the pre-core slot (PrCS), and the pre-clausal elements like fronted prepositional or adverbial phrases are placed in the pre-detached position (PrDP) . In this paper we describe the ongoing effort to build RRGparbank 1 , a novel-length parallel RRG treebank for English, German, French, Russian, and Farsi, in a semi-automatic fashion. We focus on the automatic part. Exploiting an off-the-shelf UD parser, the text (George Orwell's novel 1984 and translations) is parsed into UD. Then, exploiting structural similarities between UD and RRG, the UD trees are automatically converted into RRG trees ( \u00a72). This conversion accelerates the process of manually annotating the corpus ( \u00a73). Once enough data has been collected in this way, we replace the rule-based conversion with a statistical RRG parser trained on the collected data. A series of experiments shows that the statistical RRG parser offers a better starting point for annotating once approximately 2 000 sentences are available for training ($4) . Finally, we give a qualitative comparison between our converter and that of Chiarcos and F\u00e4th (2019) , which targets a slightly different flavor of RRG ( \u00a75). UD to RRG Conversion Auxiliary Formalism We define a custom formalism, inspired by tree grammar formalisms such as LTAG (Joshi and Schabes, 1997) that allows us to treat RRG trees as being composed from lexically anchored elementary trees via a number of composition operations. Figure 2 and Figure 3 show examples: in the middle, there are a number of elementary trees and the operations with which they are combined (the derivation) and on the right there is the resulting RRG tree. The set of operations is RRG-specific: \u2022 NUC-SUB: presupposes that the host tree is clausal. 2 If the guest tree is clausal, attach its NUC node under the host tree's NUC node, and merge its CORE, CLAUSE and SENTENCE nodes (if any) into the corresponding nodes of the host tree. 3 If not, attach its root under the host tree's NUC node.  \u2022 CORE-SUB: presupposes that the host tree is clausal. If the guest tree is clausal, attach its CORE node under the host tree's CORE node, and merge the CLAUSE and SENTENCE nodes (if any). If not, attach its root under the host tree's CORE node. \u2022 CORE-COSUB: presupposes that both trees are clausal. Create a CORE node above the host tree's CORE node (if it doesn't exist yet) and attach the guest tree's CORE node to that. Merge the CLAUSE and SENTENCE nodes. \u2022 CORE-COORD: presupposes that both trees are clausal. Attach the guest tree's CORE node under the host tree's CLAUSE node. Merge the CLAUSE and SENTENCE nodes. \u2022 CLAUSE-SUB: presupposes that the host tree is clausal. If the guest tree is clausal, attach its CLAUSE node under the host tree's CLAUSE node, and merge the SENTENCE nodes (if any). If not, attach its root under the host tree's CLAUSE node. \u2022 PRCS-SUB: create a left PrCS daughter of the host tree's CLAUSE node (if it doesn't exist yet) and attach the guest tree's root under it. \u2022 PRDP-SUB: create a left PrDP daughter of the host tree's SENTENCE node (if it doesn't exist yet) and attach the guest tree's root under it. \u2022 WRAP: attach the host under a designated node (marked * ) of the guest (used for attaching preposition complements under PPs). General Conversion Rules If we view the derivations, as exemplified in Figures 2 and 3 , as derivation trees where nodes are labeled with elementary trees and edges are labeled with operations, then this derivation tree is isomorphic to a corresponding UD tree. What remains to do to convert UD trees to RRG trees is to specify a set of rules that relabel nodes in UD trees with RRG elementary trees, and edges with operations. We try to keep these rules as local as possible, ideally looking only at one UD node and its incoming edge at a time, so a simple recursive traversal of the UD tree suffices. However, as we will see, in some cases we need to take a little more context into account. Table 1 shows the rules used in the example conversions. UD's content-word-centric approach is a good fit for the conversion to RRG regarding, e.g., copulas, modal, tense, and aspect operators, which RRG treats not as heads of verb phrases but as additional Overall, RRG's approach can be characterized as more content-word-centric than function-word-centric. This and the ready availability of UD resources made UD a more natural starting point for our conversion project than more function-word-centric variants such as SUD (Gerdes et al., 2018) . Special Conversion Rules Rules can also make reference to lexical and other language-specific knowledge. ). This is illustrated in Figure 3 . We have so far implemented this rule for English and for German. For English, we determine the verb class by lookup in the VerbNet lexical database (Kipper-Schuler, 2005) . For German, as far as we are aware, similar resources such as Ger-maNet (Hamp and Feldweg, 1997) , provide sets of verb classes which are less fine-grained. Therefore, the equivalent conversion rule for German uses a handwritten set of verbs instead of a lexical database. We have also defined rules to recognize xcomp instances encoding the raising construction and trigger core coordination by its associated lemma seem in English or scheinen in German (Rule 13). Due to the low frequency of relevant phenomena and the inevitable brittleness of rules, we have left these specialized conversion rules as a proof-of-concept and not aimed for more extensive coverage, relying on statistical predictions (see below) and annotators instead for the purpose of building RRGparbank. Implementation and Workflow The text basis for RRGparbank is provided by George Orwell's novel 1984 as well as translations to German, French, Russian and Farsi. The English and Farsi texts, their segmentation into sentences and tokens as well as POS tags and lemmas are taken from the MULTEXT-East dataset (Erjavec, 2017) , which also provides the (non-annotated) Russian text. The French and German data was built using the Orwell (1972) and Orwell (2003) editions, respectively. A large part of the German data was annotated by hand following the guidelines of the MULTEXT-East dataset. We used UDpipe2 (Straka, 2018) for segmentation, tagging, and lemmatization of the Russian, French and the non-annotated German data. UD parses for all languages are also provided by UDPipe2.  In the next step, we use a script called ud2rrg 5 , which we developed based on the formalism described above, to convert the UD trees to RRG. It performs a traversal of each UD tree and at each node applies the matching rule, thereby gradually building up an RRG tree. In the rare cases where conversion fails for a node because there is no matching rule (e.g., with rare combinations of POS and grammatical relation), conversion fails and a dummy tree is generated where all tokens are attached to the root. For the results reported in this paper, 13 annotators with training in RRG annotated 5 453 English, 5 723 German, 2 177 French, 4 675 Russian, and 1 110 Farsi sentences over a time period of 21 months. 6  They were provided the output of ud2rrg and corrected the trees using a graphical interface. The graphical interface and the annotation guidelines were based on RRGbank (Bladier et al., 2018) . Development of ud2rrg was ongoing during this period and informed by manual inspection of sentences that failed to convert and of changes annotators made to the ud2rrg output. Annotation on different languages started at different times. We used the same ud2rrg for all languages, but each new language typically brings with it a number of POS tags and constructions that have not or not much been seen in the data so far, meaning that rules have to be refined and added before ud2rrg performs as well on the new as on the old languages. As an example, consider the case of Russian. Table 2 shows the performance of ud2rrg on Russian at different points in time after its introduction as a new language. We measure the performance in nBURP (smaller is better), LF1 (larger is better), and number of failed sentences (smaller is better) -details are given below in Section 3. Timestamp #1 corresponds to the introduction of the Russian data in the annotation interface. Between #1 and #2, ongoing development of ud2rrg took into account, among other data, Russian gold data produced by annotators. The first rules making specific reference to Russian lexemes were added between #2 and #3 (7 new rules and 2 extensions of existing rules), leading to significantly better performance. Timestamp #4 is a week after #3, while #5 is the time of redaction of this article. The scores keep improving with time, as regular evaluations on the updated gold data indicates which transformation rules are missing, or need an update. Annotators are also encouraged to report sentences for which the transformation is problematic, or fails. As of this writing, ud2rrg contains about 278 rules, 4 of which depend on language-specific semantic lexical resources to select a juncture-nexus type. In addition, we have language-specific routines to determine finer-grained parts of speech for function words, such as negation particles, negative determiners, indefinite pronouns, demonstrative pronouns, clitic pronouns, or negative pronouns. Impact on Annotation Effort Correcting automatically pre-annotated data facilitates the annotation of the treebank because the data no longer need to be annotated from scratch. In this section we estimate the impact of pre-annotation on the human effort of creating treebank data. Specifically, we try to measure the mechanical effort it takes annotators to move, insert, delete, and relabel tree nodes in our graphical drag-and-drop annotation interface (Bladier et al., 2018) . For this study, we ignore the cognitive cost of annotation decisions, which is much harder to measure. Roughly speaking, the more similar a pre-annotated tree to the gold tree, the fewer drag-and-drop operations annotators will need. Established tree similarity measures include tree edit distance (TED) (Zhang and Shasha, 1989) and EVALB (Collins, 1997) . However, it is well known that these measures tend to over-penalize attachment errors (Bangalore et al., 1998; Emms, 2008) because constituents that have to be reattached do not incur a unitary cost but rather a cost proportional to their size or to the length of the path between the predicted and the correct attachment site. This contrasts with our graphical annotation interface where reattachment is a single drag-and-drop operation. As an example, consider Figure 4 . Here, we have to reattach one ADVP subtree and delete one NUC node in order to transform the ud2rrg tree into the gold tree. However, the tree edit distance is 6 because reattachment incurs a cost not only for the ADVP node but for all its descendants. Similarly, EVALB will count not 2 but 3 spans (NUC, NP, CORE N) as \"false positives\" in the ud2rrg tree. This effect gets worse with longer reattachments. We are not aware of a polynomial algorithm to compute optimal edit scripts between trees when reattachment is allowed as a single operation. Instead, we use an approximate but principled algorithm that counts the number of operations needed to turn the predicted tree into the gold tree when recreating the constituents of the gold tree by modifying the predicted tree in a strict bottom-up fashion, recreating smaller constituents first and then moving on to larger ones. This algorithm, called \"bottom-up replugging\" (BURP), is described in detail in Appendix B. In our example, BURP first recreates the CORE N subtree, for which the ADVP subtree needs to be moved down (cost 1). It then recreates the NP subtree and deletes the NUC node (cost 1). The trees are now identical, with total cost 2, which is exactly the number of operations intuitively needed. While not necessarily optimal, we conjecture that BURP approximates the strategies that human annotators use to edit trees, and that its scores are therefore a better predictor of human annotation effort than TED or EVALB. For our evaluation, we use all three measures. For TED and BURP, we normalize the score by the number of brackets in the gold RRG tree, since trees with a more complex internal structure require more editing than simpler trees. The results are given in Table 3 . Comparison with statistical parsing We compare the output of ud2rrg with parsing the sentences using the statistical neural parser ParTAGe (Bladier et al., 2020) , developed for RRG-based tree rewriting grammars. We evaluate how much training data is needed for the statistical parser to outperform the rule-based conversion approach. For the experiments, we did not distinguish between silver and gold data 7  Sentences that could not be converted/parsed are counted in the evaluation as flat dummy trees. We use our BURP measure (normalized by number of constituents in the gold tree) as well as tree edit distance (Zhang and Shasha, 1989) normalized in the same way, and EVALB LF1 (Collins, 1997) . All three measures show that statistical parsing starts to outperform ud2rrg at around 2 000 training sentences. 2 000 pre-annotated trees for training to surpass the rule-based conversion. We also evaluate the UD conversion on other languages (see Table 4 ). 8 In cases where ud2rrg could not convert UD parses to trees, we evaluate the scores as if the trees were annotated from scratch. Concretely, we measure the distance from flat dummy trees where each pre-terminal has a dummy POS tag and attaches directly to the root. The results show that about a fifth of the sentences are converted directly to the gold standard for different languages and in general the annotators' effort is reduced for the majority of sentences compared to annotation from scratch (represented as baseline in Table 4 ). These findings clearly show that using the rule-based UD conversion approach can be a good starting point for preannotation of a multilingual treebank. Related Work The availability of UD corpora for a big variety of languages makes them appealing to use for creating linguistic resources for different NLP tasks. Fancellu et al. (2017) and Reddy et al. (2017) describe algorithms for conversion of UD structures to logical forms enabling an almost language-independent transformation. Ranta and Kolachina (2017) develop an approach to convert UDs into abstract syntactic annotations to create treebanks based on the Grammatical Framework (GF) formalism for multilingual grammars (Ranta, 2011) . Closest to our work is that of Chiarcos and F\u00e4th (2019) who define a RDF/SPARQL-based converter to RRG, using as input not only UD but also semantic role annotation. The data for which both the input (partially manually corrected UD) and the output is publicly available 9 consists of 351 examples from the textbook of Van Valin Jr. and LaPolla (1997) . While their converter was developed on this kind of data, for us it presents a new domain. After normalizing away notational differences and ignoring operator attachment as well as POS tags (see below) but without any updates to ud2rrg, we obtained an nBURP of 0.16, an nTED of 0.18, and an F1 score of 85.75, with 15.38% exact matches. We then performed a qualitative comparison on 100 randomly chosen sentences to gain insights into types of mistakes and to inform future development. We summarize our findings here; the full results are provided in Appendix A. Notational conventions A large part of the differences are purely notational and can be automatically normalized away: C&F attach all punctuation at the root whereas we leave it attached at smaller phrases as in UD. We mark wh-phrases with -REL or -WH labels, C&F don't. C&F mark arguments with ARG nodes, which we don't have, and peripheries with PERIPHERY nodes, while we mark the children with -PERI instead. Some nonterminals have slightly different names, e.g., COREn vs. CORE N, LDP vs. PrDP, or ADJ vs. AP. We also ignore part-of-speech tags because C&F do not attempt to convert the input POS tags into the POS tags conventionally used in RRG analyses. Tense, modality, and aspect operator attachment In RRG, tense operators attach at the CLAUSE level, modal operators at the CORE level, and aspect operators at the NUC level. This means that, e.g., a tensed auxiliary verb as in she has seen or a tensed modal verb as in he can see attach at more than one level, namely at both CLAUSE and NUC, and at both CLAUSE and CORE, respectively. In C&F's annotation, this is so. By contrast, our guidelines limit annotations to trees for ease of processing and by convention only attach at one level, which is typically CLAUSE. We tried to ignore these differences in attachment by removing the non-CLAUSE additional edges from C&F's annotation. 10 differences due to operator attachment remain. Since auxiliary and modal verbs form a closed class, the multiple attachment would be easy to restore. Theoretical assumptions (51 instances) Some of the remaining differences can be explained by ud2rrg following conventions set down in our RRGbank-based annotation guidelines which differ from those followed in C&F's data. These are not mere notational differences but have potential theoretical significance because they reflect different assumptions about the internal structure of phrases, etc. These differences will be used to check and revise our annotation guidelines. For example, we annotate numerals using \"quantifier phrases\" (QPs), attach attributive APs at CORE N rather than NUC N level, assume a full AP rather than a simple nucleus for predicative adjectives, treat possessive pronouns like determiners and do not place them under NPIP, treat prepositions introducing adverbial clauses as clause linkage markers (CLM) rather than prepositions, do not distinguish CONJ from CLM, etc. Bugs (25 instances) Some differences are bugs in ud2rrg which can easily be fixed in future development, e.g., failure to convert prepositions marking clauses into PPs rather than CLM-marked clauses, failure to handle nmod:tmod dependents as adverbial modifiers, attachment of wh-PPs in PrDP rather than PrCS, or failure to recognize wh-movement when the subject is a passive subject. Limitations (84 instances) Telling the differences between an argument PP and a peripheral (adjunct) PP is hard and currently out of scope for ud2rrg. C&F use semantic role information to predict this. Relatedly, ud2rrg currently does not distinguish between PPs with and without internal layers (CORE P and NUC P). Clause linkage (20 instances) Similarly, mapping conj, ccomp, and xcomp dependencies to the appropriate juncture-nexus type for linking clauses together is complex. As described in Section 2, we have some rules to address this heuristically, but many cases are not yet covered and may also require semantic role or other lexical information to resolve. Bad input (12 instances) Incorrect UD input sometimes leads to incorrect ud2rrg output. For example, the input data contain a number of unspecific dep relations which are then not correctly handled. There are also instances of wrongly resolved PP attachment ambiguity and the occasional confusion of a relative clause with an adverbial clause, and vice versa. Error in gold standard (7 instances) Finally, we also discovered a handful of apparent errors in C&F's annotation. For example, the genitive suffix 's is always attached to the root instead of inside the NPIP, and some PrCS nodes appear to be spurious. Conclusions We have presented a rule-based algorithm for converting Universal Dependencies to RRG trees as a way to bootstrap RRG treebanks. By mapping UD nodes to RRG fragments and grammatical relations to operations that combine these fragments, it provides a principled mapping between the two formalisms. Language-independent at core, the algorithm can be extended with language-specific rules to incorporate lexical and other language-specific knowledge. We have shown that by basing RRG annotation on automatically converted trees, the number of tree manipulation operations that annotators have to perform is considerably reduced compared to annotating from scratch. We have also shown that for annotating English, a statistical parser trained on sentences annotated so far starts to produce more accurate trees than our rule-based conversion at around 2 000 training sentences. Finally, we have performed a detailed qualitative comparison with the output of another converter and pinned down the remaining issues for ours. In future work, we will consider applying the ud2rrg algorithm to the data from Parallel Universal Dependencies corpora (Zeman et al., 2020) . Moreover, ud2rrg allows bootstrapping of further RRG treebanks for different languages, based on existing UD treebanks. A Qualitative Comparison with Chiarcos and F\u00e4th (2019) The following table contains the results of the qualitative evaluation of our converter on 100 randomly selected example sentences from Van Valin Jr. and LaPolla (1997) as annotated by Chiarcos and F\u00e4th (2019) . The first column indicates the sentence number in their release, the second the type of difference (different operator attachment, theoretical assumption, bug, limitation, clause linkage, bad input (ud), or error in gold standard), and the third column contains a brief description of the difference. Empty second and third columns indicates sentences from our sample with no difference after normalization. sentence type description 5 op neg at CLAUSE vs. CORE 5 bug failure to attach fronted non-wh object in PrCS 8 ud dobj \u2192 dep 8 limit theo PP internal structure 9 bug fronted wh-PP attached to PrDP instead of PrCS 9 limit theo PP internal structure 12 limit failure to recognize PERI 17 limit theo PP internal structure 17 ud fronted wh-object attached with dep 18 ud nmod:tmod \u2192 dep 18 limit failure to recognize PERI 18 limit theo PP internal structure 24 26 limit theo PP internal structure 29 limit theo PP internal structure 30 limit theo PP internal structure 31 limit theo PP internal structure 31 limit theo PP internal structure 35 acoli \"that\" treated as NP when it is a determiner 48 acoli 's attached to root 48 theo AP-PERI always attaches at CORE N, not NUC N 48 limit theo PP internal structure 50 limit theo PP internal structure 54 limit theo PP internal structure 54 theo ADVP-PERI always attaches at CORE, not NUC 54 theo AP-PERI always attaches at CORE, not NUC 55 theo ADVP-PERI always attaches at CORE, not NUC 55 theo AP-PERI always attaches at CORE, not NUC 55 bug failure to recognize PoDP 55 limit theo PP internal structure 58 theo \"by\" passive subject treated as argument, not adjunct 58 limit theo PP internal structure 59 limit theo PP internal structure 62 67 theo \"by\" passive subject treated as argument, not adjunct 68 limit theo PP internal structure 71 limit failure to recognize PERI 74 ud compound \u2192 dobj 74 limit failure to recognize PERI 76 theo possessive pronoun treated as definiteness operators, not placed in NPIP 76 theo AP-PERI always attaches at CORE N, not NUC N 76 limit theo PP internal structure 89 91 99 bug raising construction where the subordinated predicate is an adjective is wrongly classified as dependency parsing error 103 ud acl:relcl \u2192 advcl 103 ud \"to whom\" is not a subtree 103 limit theo PP internal structure 104 limit theo PP internal structure 104 limit theo PP internal structure 104 bug failure to recognize PrCS when subject is marked nsubjpass 105 limit theo PP internal structure 105 limit theo PP internal structure 105 bug failure to recognize PrCS when subject is marked nsubjpass 111 112 120 limit failure to recognize PERI 120 limit theo PP internal structure 120 limit failure to recognize PERI 123 limit theo PP internal structure 123 limit failure to recognize PERI 123 limit failure to recognize PERI 125 limit failure to recognize PERI 125 limit failure to recognize PERI 127 132 theo to-infinitive that replaces a relative clause treated as CLAUSE-PERI and attached in NUC N instead of CORE attached at CORE N 132 limit theo PP internal structure 133 limit theo PP internal structure 134 ud NP with relcl instead of SENTENCE with fronted dobj 135 limit theo PP internal structure 139 140 limit theo PP internal structure 141 limit theo PP internal structure 146 limit theo PP internal structure 150 limit theo PP internal structure 153 theo AP-PERI always attaches at CORE N, not NUC N 153 limit theo PP internal structure 153 limit theo PP internal structure 153 limit theo PP internal structure 153 limit theo PP internal structure 153 limit theo PP internal structure 159 limit theo PP internal structure 159 limit theo PP internal structure 160 limit theo PP internal structure 160 limit theo PP internal structure 162 limit theo PP internal structure 168 limit theo PP internal structure 168 limit failure to recognize PERI 171 limit failure to recognize PERI 171 limit theo PP internal structure 172 limit theo PP internal structure We describe BURP (\"bottom-up replugging\"), an algorithm that computes an edit script between two trees with identical spans, such as two different natural-language constituent parse trees over the same sentence. Potential applications include evaluating the performance of constituent parsers and estimating the annotator effort in a semi-automatic annotation scenario. Similar metrics include tree-distance (Zhang and Shasha, 1989; Emms, 2008) , EVALB (Collins, 1997) , string-distance applied to tree linearizations (Roark, 2002) , and the leaf-ancestor metric (Sampson and Babarczy, 2003) . None of them explicitly models the possibility of re-attaching a subtree to a different node, and they thus tend to over-penalize attachment errors as every constituent containing a moved subtree is affected (Bangalore et al., 1998) . Although Roark (2002) and Emms (2008) propose strategies that mitigate this, subtree re-attachment is still handled as pair of delete and insert operations, thus its cost cannot be freely chosen but is necessarily the sum of the two. BURP differs from all these algorithms by trying to explicitly simulate the way human annotators using graphical annotation interface correct trees. We assume the following basic operations to be availalbe: relabeling a node, deleting an internal node (implicitly reattaching all its children to its parent), inserting a node below another node (so that children of the existing node become children of the new node), and moving a node (that has at least one sibling) to a different parent. Given these operations, one question to ask is what is the optimal set of operations to transform the source (or predicted) tree into the target (or gold) tree, given some cost for each operation (in the following, we assume that every operation has cost 1, but they can easily be weighted differently). This is an NP-complete problem. Another, and perhaps more interesting question is how many operations human annotators need. We conjecture that BURP mimicks the human annotation process to some degree, and thus gives script lengths that correlate better with human annotator effort than other measures. Sketch of the algorithm BURP transforms the source tree into the target tree in a bottom-up fashion, recreating smaller subtrees of the target tree in the source tree first and then moving on to larger ones until the root is reached and the whole tree transformed. To this end, the target tree is first divided up into maximal unary chains, as illustrated in Figure 5 . To simplify the description, we will often refer to a chain as if all its nodes have been contracted into one, i.e. we say that in the example, chain AB has two children CD and E. We will also occasionally refer to a subtree and its root as the same entity if the context makes it clear. BURP does a post-order traversal of the chains in the target tree and at each chain transforms a part of the source tree into the subtree under that chain. All children of the chain have at this point already been recreated, and they are re-used, even if this is not guaranteed to give an optimal edit script. This is how BURP cuts the NP-complete problem down to a polynomial one. The local decisions, viz., which part of the source tree to transform into the current target chain, however, are optimized for minimal local cost. Definitions The span (or yield) of a tree is the set of its leaves. Note that we do not assume spans to be contiguous. Inputs The inputs to BURP are a source tree T 1 and a target tree T 2 with identical spans. Data Structures and Initialization While transforming T 1 into T 2 , we will temporarily remove subtrees from T 1 and thus take it apart into multiple parts. We maintain a set P of these parts, which we initialize as P := {T 1 }. We say that a node is \"free\" if it is the root of a tree in P . The Traversal We do a post-order traversal of the chains in T 2 , recreating the subtrees under them in as subtrees of trees in P . Thus, when we visit a chain, all of its children have already been recreated. Let p 2 be the currently visited target chain and C its recreated children in the trees in P . In the example in Figure 6 , p 2 = BDE and C = {F, G, H, I, J}. We then pick an extended source chain or x-chain, i.e., a path p 1 in some tree in P such that p 1 = n 1 n 2 . . . n N with N \u2265 1, n N \u2208 C. In the example, p 1 = ABCF. The subtree under p 1 is then deterministically transformed into that under p 2 with the minimal number of operations and assuming that the subtrees under all c \u2208 C remain unchanged. 11 The transformation consists of the following steps: Acknowledgments We thank two anonymous reviewers for their insightful comments, as well as all people involved in the creation and annotation of RRGparbank (David Arps, Davy Baardink, Kata Balogh, Elif Benli, Sarah Fabian, Valeria Generalova, Zahra Ghane, Robin M\u00f6llemann, Natalia Moors, Rainer Osswald) for their help with collecting the data and fruitful discussions. We also thank Behrang QasemiZadeh, Jule Pohlmann and Roland Eibers for preparing the German data, Rainer Osswald for integrating the French data and completing the German data, Naima Grebe for helping with the implementation of special conversion rules for English phase verbs, and Robert D. Van Valin Jr. for contributing to annotation decisions. This work was carried out as a part of the research project TreeGraSP 10 funded by a Consolidator Grant of the European Research Council (ERC). Search For every visited target chain, we pick an x-chain that minimizes the local cost. The final edit chain and cost still depends on how ties between locally optimal x-chains are broken, and on the exact order of traversal. In our current implementation 14 , the leaves are assumed to be ordered (as the words are in natural language), and the postorder traversal proceeds from left to right. Ties between x-chains are currently broken by preferring chains that are in more recently freed subtrees, further to the right in the tree, and longer. A closer approximation to the optimal edit script could be achieved, e.g., by randomizing this and doing multiple restarts.",
    "abstract": "We describe ud2rrg, a rule-based approach for converting UD trees to Role and Reference Grammar (RRG) structures. Our conversion method aims at facilitating the annotation of multilingual RRG treebanks. ud2rrg uses general and language-specific conversion rules. In order to evaluate ud2rrg, we approximate the subsequent annotation effort via measures of tree edit distance. Our evaluation, based on English, German, French, Russian, and Farsi, shows that the ud2rrg transformation of UD-parsed data constitutes a highly useful starting point for multilingual RRG treebanking. Once a sufficient amount of data has been annotated in this way, the automatic conversion can be replaced by a statistical parser trained on that data for an even better starting point. 2 A clausal tree is an elementary tree whose lexical anchor is the head of a clause. Its spine starts with SENTENCE, CLAUSE, CORE, or NUC, followed by nodes for the lower clausal layers. 3 By merging we mean attaching any children of the former under the latter.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English",
        "French",
        "German",
        "Russian"
    ],
    "numcitedby": "1",
    "year": "2021",
    "month": "December",
    "title": "Bootstrapping Role and Reference Grammar Treebanks via {U}niversal {D}ependencies"
}