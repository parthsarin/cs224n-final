{
    "article": "The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks. Introduction Syntactic parsing is an important technique in the research area of natural language processing. The evaluation-driven methodology is a good way to spur the its development. Two main parts of the method are a benchmark database and several well-designed evaluation metrics. Its feasibility has been proven in the English language. After the release of the Penn Treebank (PTB) (Marcus et al., 1993) and the PARSEVAL metrics (Black et al., 1991) , some new corpusbased syntactic parsing techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003) , Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007) . By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007) , many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007) , MSTParser (McDonald et al., 2005) , Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007) . Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar anno-tation scheme of PTB, these parsing techniques were also transferred to the Chinese language. (Levy and Manning, 2003) explored the feasibility of applying lexicalized PCFG in Chinese. (Li et al., 2010) proposed a joint syntactic and semantic model for parsing Chinese. But till now, there is not a good Chinese parser whose performance can approach the state-of-art English parser. It is still an open challenge for parsing Chinese sentences due to some special characteristics of the Chinese language. We need to find a suitable benchmark database and evaluation metrics for the Chinese language. Last year, we organized the first Chinese syntactic parsing evaluation ---CIPS- ParsEval-2009 (Zhou and Zhu, 2009) . Five Chinese parsing tasks were designed as follows: Task 1: Part-of-speech (POS) tagging; Task 2: Base chunk (BC) parsing Task 3: Functional chunk (FC) parsing Task 4: Event description clause (EDC) recognition Task 5: Constituent parsing in EDCs They cover different levels of Chinese syntactic parsing, including POS tagging (Task 1), shallow parsing (Task 2 & 3), complex sentence splitting (Task 4) and constituent tree parsing (Task 5). The news and academic articles annotated in the Tsinghua Chinese Treebank (TCT ver1.0) were used to build different goldstandard data for them. Some detailed information about CIPS-ParsEval-2009 can be found in (Zhou and Li, 2009) . This evaluation found the following difficult points for Chinese syntactic parsing. 1) There are two difficulties in Chinese POS tagging. One is the nominal verbs. The POS accuracy of them is about 17% lower than the overall accuracy. The other is the unknown words. The POS accuracy of them is about 40-10% lower than the overall accuracy. 2) The chunks with complex internal structures show poor performance in two chunking tasks. How to recognize them correctly needs more lexical semantic knowledge. 3) The joint recognition of constituent tag and head position show poor performance in the constituent parsing task of EDCs. Therefore, the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks was proposed to deal with these problems. Some new designing ideas are as follows: 1) We use the segments sentences as the input of the syntactic parser to test the effects of POS tagging for Chinese parsing. 2) We design a new metric to evaluate performance of event construction recognition in a constituent parser of EDCs. 3) We try to evaluate the performance of event relation recognition in Chinese complex sentence. In the following sections, we will introduce the task designing ideas, data preparation methods, evaluation metrics and results of the evaluation. Task description For the syntactic parsing task (Task 2) of the CIPS-Bakeoff-2010, we designed two sub-tasks: Task 2-1: Parsing the syntactic trees in Chinese event description clauses Task 2-2: Parsing the syntactic trees in Chinese sentences. Each subtask is separated as close and open track. In the close track, only the provided training data can be used to build the parsing model. In the open track, other outside language resources can be freely used. We will give two examples to show the detailed goals of these two sub-tasks: 1) Task 2-1 Input: a Chinese event description clause with correct word segmentation annotations \u2022 \u6cbf\u9014 \uff0c\u6211\u4eec \u4e0d\u65f6 \u89c1\u5230 \u56e0 \u66f4\u65b0 \u800c \u4f10 \u5012 \u7684 \u6811\u6728 \uff0c \u56e0 \u4fee \u8def \u9700 \u4f10\u5012 \u7684 \u6811\u6728 Ouput: a syntactic parsing tree of the EDC with appropriate constitutent tag, head position and POS tag annotations. \u2022 [dj-2 \u6cbf\u9014/s \uff0c/wP [dj-1 \u6211\u4eec/rNP [vp- 1 \u4e0d\u65f6/d [vp-0 \u89c1\u5230/v [np-0-2 [np-2 [vp-1 [pp-1 \u56e0/p \u66f4\u65b0/v ] [vp-1 \u800c/cC \u4f10\u5012/v ] ] \u7684/uJDE \u6811\u6728/n ] \uff0c/wP [np-2 [vp-1 [pp-1 \u56e0/p [vp-0 \u4fee/v \u8def /n ] ] [vp-1 \u9700/vM \u4f10\u5012/v ] ] \u7684/uJDE \u6811\u6728/n ] ] ] ] ] ] 1 2) Task 2-2 Input: a Chinese sentence with correct word segmentation annotations \u2022 \u6cbf\u9014 \uff0c \u6211\u4eec \u4e0d\u65f6 \u89c1\u5230 \u56e0 \u66f4\u65b0 \u800c \u4f10 \u5012 \u7684 \u6811\u6728 \uff0c \u56e0 \u4fee \u8def \u9700 \u4f10\u5012 \u7684 \u6811 \u6728 \uff0c \u90fd \u662f \u6709\u7528 \u4e4b \u6750 \uff1b \u8fd0\u9001 \u6811\u6728 \u7684 \u8d27\u8f66 \u3001 \u62d6\u62c9\u673a \uff0c \u5357\u6765\u5317\u5f80 \u3002 Output: a syntactic parsing tree of the sentence with appropriate constitute tag and POS tag annotations. \u2022 [zj [fj [fj [dj \u6cbf\u9014/s \uff0c/wP [dj \u6211\u4eec/rNP [vp \u4e0d\u65f6/d [vp \u89c1\u5230/v [np [np [vp [pp \u56e0/p \u66f4\u65b0/v ] [vp \u800c/cC \u4f10\u5012/v ] ] \u7684 /uJDE \u6811\u6728/n ] \uff0c/wP [np [vp [pp \u56e0/p [vp \u4fee/v \u8def/n ] ] [vp \u9700/vM \u4f10\u5012/v ] ] \u7684/uJDE \u6811\u6728/n ] ] ] ] ]] \uff0c/wP [vp \u90fd /d [vp \u662f/v [np \u6709\u7528/a \u4e4b/uJDE \u6750 /n ] ] ] ] \uff1b/wP [dj [np [vp \u8fd0\u9001/v \u6811\u6728 /n ] \u7684/uJDE [np \u8d27\u8f66/n \u3001/wD \u62d6\u62c9 \u673a/n ] ] \uff0c/wP \u5357\u6765\u5317\u5f80/v ] ]\u3002/wE ] We define a Chinese sentence as the Chinese word serials ending with period, question mark or exclamation mark in the Chinese text. Usually, a Chinese sentence can describe a complex situation with several inter-related events. It consists of several clauses separated by commas or semicolons to describe one or more detailed event content. We call these clauses as event description clauses. We use the following example to explain the relationship between a Chinese sentence and The sentence gives us several sequential situations through the vision changing along the author's journey way: Firstly, we see the trees that have been cut down. They are useful building material. Then, we see several trucks and tractors to carry away these trees. They are going south and north busily. All the above situations are described through three EDCs annotated with bracket pairs in the sentence. Interestingly, in the corresponding English translation, the same situation is described through three English sentences with complete subject and predicate structures. They show difference event description characteristics of these two languages. The Chinese author tends to describe a complex situation through a sentence. Many complex event relations are implicit in the structural sequences or semantic connections among the EDCs of the sentence. So many subjects or objects of an EDC can be easily omitted based on the adjacent contexts. The English author tends to describe a complex situation through several sentences. Each sentence can give a complete description of an event through the subject and predicate structure. The event relations are directly set through the paragraph structures and conjunctions. The distinction between Chinese sentence and EDC can make us focus on different evaluation emphasis in the CIPS-Bakeoff-2010 section. For an EDC, we can focus on the parsing performance of event content recognition. So we design a special metric to evaluate the recall of the event recognition based on the syntactic parsing results. For a sentence, we can focus on the parsing performance of event relation recognition. So we separate the simple and complex sentence constitutes and give different evaluation metrics for them. Some detailed designations of the evaluation metrics can be found in section 4. Data preparation The evaluation data were extracted from Tsinghua Chinese Treebank (TCT) and PKU Chinese Treebank (PKU-CTB). TCT (Zhou, 2004) adopted a new annotation scheme for Chinese Treebank. Under this scheme, every Chinese sentence will be annotated with a complete parse tree, where each non-terminal constituent is assigned with two tags. One is the syntactic constituent tag, such as noun phrase(np), verb phrase(vp), simple sentence(dj), complex sentence(fj), etc., which describes basic syntactic characteristics of a constituent in the parse tree. The other is the grammatical relation tag, which describes the internal structural relation of its sub-components, including the grammatical relations among different phrases and the event relations among different clauses. These two tag sets consist of 16 and 27 tags respectively. Now we have two Chinese treebanks annotated under above scheme: (1) TCT version 1.0, which is a 1M words Chinese treebank covering a balanced collection of journalistic, literary, academic, and other documents; (2) TCT-2010, which consists of 100 journalistic annotated articles. The following is an annotated sentence under TCT scheme: \u2022 [zj-XX [fj-LS [dj-ZW \u6211\u4eec/rN [vp-PO \u95ee/v [dj-ZW [np-DZ \u4ed6/rN \u81ea\u5df1/rN ] [vp-PO \u4e70 /v \u591a\u5c11/m ] ] ] ] \uff0c/\uff0c [dj-ZW \u4ed6/rN [vp- LW [vp-PO \u51d1\u8fd1/v [sp-DZ \u8bb0\u8005/n \u9762\u524d /s ] ] [vp-PO \u4f38\u51fa/v [np-DZ [mp-DZ \uff14/m \u4e2a/qN ] \u6307\u5934/n ] ] ] ] ] \u3002/\u3002 ] 2 (2) PKU-CTB (Zhan et al., 2006) adopted a traditional syntactic annotation scheme. They annotated Chinese sentences with syntactic constitu-ent and head position tags in a complete parse tree. The tag set consists of 22 constituent tags. Because every content word is directly annotated with suitable constituent tag, there are many unary phrases in PKU-CTB annotated sentences. Its current annotation scale is 881,771 Chinese words, 55264 sentences. The following is an annotated sentence under PKU-CTB scheme: \u2022 ( zj ( !fj ( !fj ( !dj ( np ( vp ( !v ( \u5efa\u7b51 ) ) !np ( !n ( \u516c\u53f8 ) ) ) !vp ( !vp ( !v ( \u8fdb ) ) np ( !n ( \u533a ) ) ) ) wco ( \uff0c ) dj ( np ( ap ( !b ( \u6709 \u5173 ) ) !np ( !n ( \u90e8\u95e8 ) ) ) !vp ( dp ( !d ( \u5148 ) ) !vp ( !vp ( !vp ( !v ( \u9001 ) ) v ( \u4e0a ) ) np ( qp ( mp ( !rm ( \u8fd9 ) ) !q ( \u4e9b ) ) !np ( np ( !n ( \u6cd5\u89c4\u6027 ) ) !np ( !n ( \u6587\u4ef6 ) ) ) ) ) ) ) ) wco ( \uff0c ) vp ( c ( \u7136\u540e ) !vp ( !v ( \u6709 ) np ( ap ( !b ( \u4e13\u95e8 ) ) !np ( !n ( \u961f\u4f0d ) ) ) vp ( !vp ( !v ( \u8fdb\u884c ) ) vp ( !vp ( !v ( \u76d1\u7763 ) ) vp ( !v ( \u68c0\u67e5 ) ) ) ) ) ) ) wfs ( \u3002 ) ) ) 3 (3) Due to the different annotation schemes and formats used in these two treebanks, we proposed the following strategies to build the goldstandard data set for Task 2-1 and Task 2-2: 1) Unify POS tag set The PKU-CTB has 97 POS tags, and TCT has 70 POS tags. After analyzing these POS tags, we found most of them have same meanings. So we designed a unified POS tag set with 58 intersected tags. All the POS tags used in PKU-CTB and TCT can be automatically mapped to this unified tag set. 2) Transform PKU-CTB annotations Firstly, we mapped the POS tags into the unified tag set, and transformed the word and POS tag format into TCT's format. Then, we deleted all unary constituents in PKU-CTB parse trees and transferred the constituent structures and tags into TCT's constituent tags. Finally, we manually proofread the transformed parse trees to modify some constituent structures that are inconsistent with TCT annotation scheme. About 5% constituents are modified. 3 The PKU-CTB uses the similar POS and constituent tags with TCT scheme. The exclamation symbol '!' is used to annotate the head of each constituent in the parse tree. 3) Extract EDCs and event annotations from TCT Based on the detailed grammatical relation tags annotated in TCT, we can easily extract each EDC for a TCT sentence (Zhou and Zhu, 2009) . Then, we proposed an algorithm to extract different event constructions in each EDC and build a large scale Chinese event bank. It can be used as a gold-standard data to evaluation the event recognition performance of an automatic syntactic parser in Task 2-1. An event construction is an event chunk serial controlled by an event target verb. It is a basic unit to describe event content. For example, for the first EDC extracted from the above sentence (1), we can obtain the follow four event constructions for the event target verb '\u89c1\u5230', '\u4f10 \u5012', '\u4fee', and '\u4f10\u5012' . \u2022 [D-sp \u6cbf\u9014/s-@] \uff0c/wP [S-np \u6211\u4eec/rNP- @ ] [D-dp \u4e0d\u65f6/d-@ ] [P-vp-Tgt \u89c1\u5230/v- @ ] [O-np \u56e0/p \u66f4\u65b0/v \u800c/cC \u4f10\u5012/v \u7684 /uJDE \u6811\u6728/n-@ \uff0c/wP \u56e0/p \u4fee/v \u8def/n \u9700/vM \u4f10\u5012/v \u7684/uJDE \u6811\u6728/n-@ ] 4 \u2022 [D-pp \u56e0/p \u66f4\u65b0/v-@ ] [P-vp-Tgt \u9700/vM \u4f10\u5012/v-@ ] \u7684/uJDE [H-np \u6811\u6728/n-@ ] \u2026 \u2022 \u2026 \u56e0/p [P-vp-Tgt \u4fee/v-@ ] [O-np \u8def/n- @ ] \u9700/vM \u4f10\u5012/v \u7684/uJDE \u6811\u6728/n \u2022 \u2026 [D-pp \u56e0/p \u4fee/v-@ \u8def/n ] [P-vp-Tgt \u9700 /vM \u4f10\u5012/v-@ ]\u7684/uJDE [H-np \u6811\u6728/n-@ ] 4) Obtain TCT constituent structure trees We can easily select all syntactic constituent tags annotated in TCT sentences to build the gold-standard parsing trees for Task 2-2. We mainly used the journalistic and academic texts annotated in TCT and PKU-CTB to build different training and test set for task 2-1 and 2-2. We selected all news and academic texts annotated in TCT ver1.0 to form the training set of Task 2-1 and 2-2. 1000 EDCs extracted from TCT-2010 were selected as the test set of Task 2-1. These sentences are extracted from the People's Daily corpus with the same source of TCT ver1.0. 1000 sentences extracted from PKU-CTB were selected as the test set of Task 2-2. Most of them are extracted from the technical reports or popular science articles. They have much more technical terms than the encyclopedic articles used in TCT ver1.0. Evaluation metrics For Task 2-1, we designed three kinds of evaluation metrics: 1) POS accuracy (POS-A) This metri is used to evaluate the performance of automatic POS tagging. Its computation formula is as follows: \u2022 POS accuracy = (sum of words with correct POS tags) / (sum of words in goldstandard sentences) * 100% The correctness criteria of POS tagging is as follows: The automatically assigned POS tag is same with the gold-standard one. 2) Constituent parsing evaluation We selected three commonly-used metrics to evaluation the performance of constituent parsing: labeled precision, recall, and F1-score. Their computation formulas are as follows: \u2022 Precision = (sum of correctly labeled constituents ) / (sum of parsed constituents) * 100% \u2022 Recall = (sum of correctly labeled constituents) / (sum of gold-standard constituents) *100% \u2022 F1-score = 2*P*R / (P+R) Two correctness criteria are used for constituent parsing evaluation: 'B+C' criteria: the boundaries and syntactic tags of the automatically parsed constituents must be same with the goldstandard ones. 'B+C+H' criteria: the boundaries, syntactic tags and head positions of the automatically parsed constituents must be same with the gold-standard ones. 3) Event recognition evaluation We only considered the recognition recall of each event construction annotated in the event bank, due to the current parsing status of Task 2-1 output. For each event target verb annotated in the event bank, we computed their Micro and Macro average recognition recall. The computation formulas are as follows: \u2022 Micro Recall = (sum of all correctly recognized event constructions) / (sum of all gold standard event constructions) * 100% \u2022 Macro Recall = (sum of Micro-R of each event target verb ) / (sum of event target verbs in gold-standard set ) The correctness criteria of event recognition should consider following two matching conditions: Condition 1: Each event chunk in a goldstandard event construction should have a corresponding constituent in the automatic parse tree. For the single-word chunk, the automatically assigned POS tag should be same with the gold standard one. For the multiword chunk, the boundary, syntactic tag and head positions of the automatically parsed constituent should be same with the gold-standard ones. Meanwhile, the corresponding constituents should have the same layout sequences with the gold standard event construction. Condition 2: All event-chunk-corresponding constituents should have a common ancestor node in the parse tree. One of the left and right boundaries of the ancestor node should be same with the left and right boundaries of the corresponding event construction. For Task 2-2, we design two kinds of evaluation metrics: 1) POS accuracy (POS-A) This index is used to evaluate the performance of automatic POS tagging. Its formula and correctness criteria are same with the above definitions of Task 2-1. 2) Constituent parsing evaluation To evaluate the parsing performance of event relation recognition in complex Chinese sen-tences, we firstly divided all parsed constituents into following two parts: \u2022 Constituent of complex sentence (C_S), whose tag is 'fj'; \u2022 Constituents in simple sentence (S_S), whose tags are belong to the tag set {dj, vp, ap, np, sp, tp, mp, mbar, dp, pp, bp}. Then we computed the labeled precision, recall and F1-socre of these two parts and obtain the arithmetic mean of these two F1-score as the final ranking index. Their computation formulas of each part are as follows: \u2022 Precision = (sum of correctly labeled constituents in one part) / (sum of parsed constituents in the part) * 100% 3 Result submission data of all participants in Task 2. (TPI=Take Part In) Evaluation results The Task 2 of CIPS-Bakeoff-2010 attracted 13 participants. Almost all of them took part in the two subtasks: Task 2-1 and 2-2. Only one participant took part in the Task 2-2 subtask alone. Among them, 9 participants submitted parsing results. In Task 2-1, we received 16 parsing results, including 13 close track systems and 3 open track systems. In Task 2-2, we received 15 parsing results, including 9 close track systems and 6 open track systems. Table 3 shows the submission information of all participants of Task 2. Task 2-1 analysis We evaluated the parsing performance of EDC on the constituent and event level respectively. The constituent parsing evaluation only considers the parsing performance of one single constituent. The event recognition evaluation will consider the recognition performance of a complete event construction. So it can provide more useful reference information for event extraction application. Table 5 and Table 6 show the evaluation results of constituent parsing in the close and open tracks respectively. In the close track, the best F1-score under 'B+C' criteria is 85.39%, while the best F1 score under 'B+C+H' criteria is 83.66%. Compared with the evaluation results of the task 5 in CIPS-ParEval-2009 under the similar training and test conditions (Zhou and Li, 2009) , the performance of head identification is improved about 2%. Task 2-2 analysis Table 9 and Table 10 show the evaluation results of constituent parsing in the close and open tracks of Task 2-2 respectively. In each track, the F1-score of the complex sentence recognition is about 5-6% lower than that of the constituents in simple sentences. It indicates the difficultness of event relation recognition in real world Chinese sentences. Some new features need to be explored for them. Almost all the parsing performances of the systems in the open track are better than that ones in the close track. It indicates some outside language resources may useful for parsing performance improvement. Compared with the commonly-used English Treebank PTB with about 1M words, our current annotated data may be not enough to train a good Chinese parser. We may need to collect more useful treebank data in the future evaluation tasks. The F1-scores of constituent parsing in simple sentences of Task 2-2 are still about 5-6% lower than that of EDC constituents under 'B+C' criteria in Task 2-1. It indicates some lower level errors may be propagated to up-level constituents during complex sentence parsing. How to restrict the error propagation chains is an interesting issue need to be explored. POS tagging analysis The best POS accuracy in Task 2-1 is 93.96%, approaching to the state-of-art performance of the Task 1 in CIPS-ParsEval-2009, under similar training and test conditions. But the POS accuracy in Task 2-2 is about 3-4% lower than it. A possible reason is that there are lots of unknown words in the test data of Task 2-2. Most of them are technical terms outside the training data lexicon. How to deal with the unknown words is still an open challenge for POS tagging. Conclusions The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation jointed with SIGHAN Bakeoff tasks. Some new contributions of the evaluation are as follows: 1) Set a new metric to evaluate the event construction recognition performance in the constituent parsing tree; 2) Set a separated metric to evaluate the event relation recognition performance in complex Chinese sentence. Through this evaluation, we found: 1) The event construction recognition in a Chinese EDC is still a challenge. Some new techniques and machine learning models need to be explored for this task. 2) Compared with about 90% F1-score of the state-of-art English parser, the 75% F1-score of current Chinese parser is still on its primitive stage. There is a long way to go in the future. 3) The event relation recognition in real world complex Chinese sentences is a difficult problem. Some new features and methods need to be explored for it. They lay good foundations for the new task designation in the future evaluation round. Acknowledgements Thanks Li Yemei for her hard work to organize the evaluation. Thanks Li Yanjiao and Li Yumei for their hard work to prepare the test data for the evaluation. Thanks Zhu Muhua for making the evaluation tools and processing all the submitted data. Thanks all participants of the evaluation. The work was also supported by the research projects of National Science Foundation of China (Grant No. 60573185, 60873173)  and National 863 High-Tech research projects (Grant No.  2007AA01Z173).",
    "abstract": "The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": "7",
    "year": "2010",
    "month": "",
    "title": "{C}hinese Syntactic Parsing Evaluation"
}