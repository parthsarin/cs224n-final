{
    "article": "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction. Introduction In the fourteenth century the words boy and girl referred respectively to a male servant and a young person of either sex (Oxford English Dictionary). By the fifteenth century a narrower usage had emerged for girl, designating exclusively female individuals, whereas by the sixteenth century boy had lost its servile connotation and was more broadly used to refer to any male child, becoming the masculine counterpart of girl (Bybee, 2015) . Word meaning is indeed in constant mutation and, since correct understanding of the meaning of individual words underpins general machine reading comprehension, it has become increasingly relevant for computational linguists to detect and characterise lexical semantic change-e.g., in the form of laws of semantic change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016) -with the aid of quantitative and reproducible evaluation procedures (Schlechtweg et al., 2018) . Most recent studies have focused on shift detection, the task of deciding whether and to what extent the concept evoked by a word has changed between time periods (e.g., Gulordava and Baroni, 2011; Kim et al., 2014; Kulkarni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018) . This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991) . This limitation has motivated recent work on word sense induction across time periods (Lau et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al., 2019) . Word senses, however, have shortcomings themselves as they are a discretisation of word meaning, which is continuous in nature and modulated by context to convey ad-hoc interpretations (Brugman, 1988; Kilgarriff, 1997; Paradis, 2011) . In this work, we propose a usage-based approach to lexical semantic change, where sentential context modulates lexical meaning \"on the fly\" (Ludlow, 2014) . We present a novel method that (1) exploits a pre-trained neural language model (BERT; Devlin et al., 2019) to obtain contextualised representations for every occurrence of a word of interest, (2) clusters these representations into usage types, and (3) measures change along time. More concretely, we make the following contributions: \u2022 We present the first unsupervised approach to lexical semantic change that makes use of stateof-the-art contextualised word representations. \u2022 We propose several metrics to measure semantic change with this type of representation. Our code is available at https://github.com/ glnmario/cwr4lsc. \u2022 We create a new evaluation dataset of human sim-ilarity judgements on more than 3K word usage pairs across different time periods, available at https://doi.org/10.5281/zenodo.3773250. \u2022 We show that both the model representations and the detected semantic shifts are positively correlated with human intuitions. \u2022 Through in-depth qualitative analysis, we show that the proposed approach captures synchronic phenomena such as word senses and syntactic functions, literal and metaphorical usage, as well as diachronic linguistic processes related to narrowing and broadening of meaning across time. Overall, our study demonstrates the potential of using contextualised word representations for modelling and analysing lexical semantic change and opens the door to further work in this direction. Related Work Semantic change modelling Lexical semantic change models build on the assumption that meaning change results in the modification of a word's linguistic distribution. In particular, with the exception of a few methods based on word frequencies and parts of speech (Michel et al., 2011; Kulkarni et al., 2015) , lexical semantic change detection has been addressed following two main approaches: form-based and sense-based (for an overview, see Kutuzov et al., 2018; Tang, 2018) . In form-based approaches independent models are trained on the time intervals of a diachronic corpus and the distance between representations of the same word in different intervals is used as a semantic change score (Gulordava and Baroni, 2011; Kulkarni et al., 2015) . Representational coherence between word vectors across different periods can be guaranteed by incremental training procedures (Kim et al., 2014) as well as by post hoc alignment of semantic spaces (Hamilton et al., 2016) . More recent methods capture diachronic word usage by learning dynamic word embeddings that vary as a function of time (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018) . Form-based models depend on a strong simplification: that a single representation is sufficient to model the different usages of a word. Time-dependent representations are also created in sense-based approaches: in this case word meaning is encoded as a distribution over word senses. Several Bayesian models of sense change have been proposed (Wijaya and Yeniterzi, 2011; Lau et al., 2012 Lau et al., , 2014;; Cook et al., 2014) . Among these is the recent SCAN model (Frermann and Lapata, 2016) , which represents (1) the meaning of a word in a time interval as a multinomial distribution over word senses and (2) word senses as probability distributions over the vocabulary. The main limitation of sense-based models is that they rely on a bag-of-words representation of context. Furthermore, many of these models keep the number of senses constant across time intervals and require this number to be manually set in advance. Unsupervised approaches have been proposed that do not rely on a fixed number of senses. For example, the method for novel sense identification by Mitra et al. (2015) represents senses as clusters of short dependency-labelled contexts. Like ours, this method analyses word forms within the grammatical structures they appear. However, it requires syntactically parsed diachronic corpora and focuses exclusively on nouns. None of these restrictions limit our proposed approach, which leverages neural contextualised word representations. Contextualised word representations Several approaches to context-sensitive word representations have been proposed in the past. Sch\u00fctze (1998) introduced a clustering-based disambiguation algorithm for word usage vectors, Erk and Pad\u00f3 (2008) proposed creating multiple vectors for the same word and Erk and Pad\u00f3 (2010) proposed to directly learn usage-specific representations based on the set of exemplary contexts within which the target word occurs. Recently, neural contextualised word representations have gained widespread use in NLP, thanks to deep learning models which learn usage-dependent representations while optimising tasks such as machine translation (CoVe; McCann et al., 2017) and language modelling (Dai and Le, 2015, ULMFiT; Howard and Ruder, 2018, ELMo; Peters et al., 2018, GPT; Radford et al., 2018 , 2019, BERT; Devlin et al., 2019) . State-of-the-art language models typically use stacked attention layers (Vaswani et al., 2017) , they are pre-trained on a very large amount of textual data, and they can be fine-tuned for specific downstream tasks (Howard and Ruder, 2018; Radford et al., 2019; Devlin et al., 2019) . Contextualised representations have been shown to encode lexical meaning dynamically, reaching high accuracy on, e.g., the binary usage similarity judgements of the WiC evaluation set (Pilehvar and Camacho-Collados, 2019) , performing on a par with state-of-the-art word sense disambiguation models (Wiedemann et al., 2019) , and proving useful for the supervised derivation of time-specific sense representation (Hu et al., 2019) . In this work, we investigate the potential of contextualised word representations to detect and analyse lexical semantic change, without any lexicographic supervision. 3 Method: A Usage-based Approach to Lexical Semantic Change We introduce a usage-based approach to lexical semantic change analysis which relies on contextualised representations of unique word occurrences (usage representations). First, given a diachronic corpus and a list of words of interest, we use the BERT language model (Devlin et al., 2019) to compute usage representations for each occurrence of these words. Then, we cluster all the usage representations collected for a given word into an automatically determined number of partitions (usage types) and organise them along the temporal axis. Finally, we propose three metrics to quantify the degree of change undergone by a word. Language Model We produce usage representations using the BERT language model (Devlin et al., 2019) , a multilayer bidirectional Transformer encoder trained on masked token prediction and next sentence prediction, on the BooksCorpus (800M words) (Zhu et al., 2015) and on English text passages extracted from Wikipedia (2,500M words). There are two versions of BERT. For space and time efficiency, we use the smaller base-uncased version, with 12 layers, 768 hidden dimensions, and 110M parameters. 1 Usage Representations Given a word of interest w and a context of occurrence s = (v 1 , ..., v i , ..., v n ) with w = v i , we extract the activations of all of BERT's hidden layers for sentence position i and sum them dimensionwise. We use addition because neither concatenation nor selecting a subset of the layers produced notable differences in the relative geometric distance between word representations. The set of N usage representations for w in a given corpus can be expressed as the usage matrix U w = (w 1 , . . . , w N ). For each usage representation in the usage matrix U w , we store the context of 1 We rely on Hugging Face's implementation of BERT (available at https://github.com/huggingface/ transformers). 1910 1920 1930 1940 1950 1960 1970 1980 1990 Figure 1 : Usage representations and usage type distributions generated with occurrences of the word atom in COHA (Davies, 2012) . Colours encode usage types. occurrence (a 128-token window around the target word) as well as a temporal label t w indicating the time interval of the usage. Usage Types Once we have obtained a word-specific matrix of usage vectors U w , we standardise it and cluster its entries using K-Means. 2 This step partitions usage representations into clusters of similar usages of the same word, or usage types (see Figure 1a ), and thus it is directly related to automatic word sense discrimination (Sch\u00fctze, 1998; Pantel and Lin, 2002; Manandhar et al., 2010; Navigli and Vannella, 2013, among others) . For each word independently, we automatically select the number of clusters K that maximises the silhouette score (Rousseeuw, 1987) , a metric of cluster quality which favours intra-cluster coherence and penalises inter-cluster similarity, without the need for gold labels. For each value of K, we execute 10 iterations of Expectation Maximization to alleviate the influence of different initialisation values (Arthur and Vassilvitskii, 2007) . The final clustering for a given K is the one that yields the minimal distortion value across the 10 runs, i.e., the minimal sum of squared distances of each data point from its closest centroid. We experiment with K \u2208 [2, 10]. We choose the range [2, 10] heuristically: we forgo K = 1 as K-Means and the silhouette score are ill-defined for this case, while keeping the number of possible clusters manageable computationally. This excludes the possibility that a word has a single usage type. Alternatively, we could use a measure of intra-cluster dispersion for K = 1, and consider a word monosemous if its dispersion value is below a threshold d (if the dispersion is higher than d, we would discard K = 1 and use the silhouette score to find the best K \u2265 2). There also exist clustering methods that select the optimal K automatically, e.g. DBSCAN or Affinity Propagation (Martinc et al., 2020) . They nevertheless require method-specific parameter choices which indirectly determine the number of clusters. By counting the number of occurrences of each usage type k in a given time interval t (we refer to this count as freq(k, t)), we obtain frequency distributions f t w for each interval under scrutiny: f t w \u2208 N Kw : f t w [k] = freq(k, t) k \u2208 [1, K w ] (1) When normalised, frequency distributions can be interpreted as probability distributions over usage types u t w : 1b illustrates the result of this process. u t w [k] = 1 Nt f t w [k]. Figure Quantifying Semantic Change We propose three metrics for the automatic quantification of lexical semantic change using contextualised word representations. The first two (entropy difference and Jensen-Shannon divergence) are known metrics for comparing probability distributions. In our approach, we apply them to measure variations in the relative prominence of coexisting usage types. We conjecture that these kinds of metric can help detect semantic change processes that, e.g., lead to broadening or narrowing (i.e., to increase or decrease, respectively, in the number or relative distribution of usage types). The third metric (average pairwise distance) only requires a usage matrix U w and the temporal labels t w (Section 3.2). Since it does not rely on usage type distributions, it is not sensitive to possible errors stemming from the clustering process. Entropy difference (ED) We propose measuring the uncertainty (e.g., due to polysemy) in the interpretation of a word w in interval t using the normalised entropy of its usage distribution u t w : \u03b7(u t w ) = log Kw Kw k=1 u t w [k] \u2212u t w [k] (2) To quantify how uncertainty over possible interpretations varies across time intervals, we compute the difference in entropy between the two usage type distributions in these intervals: ED(u t w , u t w ) = \u03b7(u t w ) \u2212 \u03b7(u t w ). We expect high ED values to signal the broadening of a word's interpretation and negative values to indicate narrowing. Jensen-Shannon divergence (JSD) The second metric takes into account not only variations in the size of usage type clusters but also which clusters have grown or shrunk. It is the Jensen-Shannon divergence (Lin, 1991) between usage type distributions: JSD(u t w , u t w ) = H 1 2 u t w + u t w \u2212 1 2 H u t w \u2212 H u t w ( 3 ) where H is the Boltzmann-Gibbs-Shannon entropy. Very dissimilar usage distributions yield high JSD whereas low JSD values indicate that the proportions of usage types barely change across periods. Average pairwise distance (APD) While the previous two metrics rely on usage type distributions, it is also possible to quantify change bypassing the clustering step into usage types, e.g. by calculating the average pairwise distance between usage representations in different periods t and t : APD(U t w , U t w ) = 1 N t \u2022 N t x i \u2208U t w , x j \u2208U t w d(x i , x j ) (4) where U t w is a usage matrix constructed with occurrences of w only in interval t. We experiment with cosine, Euclidean, and Canberra distance. Generalisation to multiple time intervals The presented metrics quantify semantic change across pairs of temporal intervals (t, t ). When more than two intervals are available, we measure change across all contiguous intervals (m(U t w , U t+1 w ), where m is one of the metrics), and collect these values into vectors. We then transform each vector into a scalar change score by computing the vector's mean and maximum values. 3 Whereas the mean is indicative of semantic change across the entire period under consideration, the max pinpoints the pair of successive intervals where the strongest shift has occurred. 3 The Jensen-Shannon divergence can also be measured with respect to T > 2 probability distributions (R\u00e9 and Azad, 2014 ): JSD u 1 w , . . . , u T w = H 1 T T i=1 u i w \u2212 1 T T i=1 H u i w . However, this definition of the JSD is insensitive to the order of the temporal intervals and yields lower correlation with human semantic change ratings (cfr. Section 5.2) than the pairwise metrics. Data We examine word usages in a large diachronic corpus of English, the Corpus of Historical American English (COHA, Davies, 2012) , which covers two centuries (1810-2009) of language use and includes a variety of genres, from fiction to newspapers and popular magazines, among others. In this study, we focus on texts written between 1910 and 2009, for which a minimum of 21M words per decade is available, and discard previous decades, where data are less balanced per decade. We use the 100 words annotated with semantic shift scores by Gulordava and Baroni (2011) as our target words. These scores are human judgements collected by asking five annotators to quantify the degree of semantic change undertaken by each word (shown out of context) from the 1960's to the 1990's. We exclude extracellular as in COHA this word only appears in three decades; all other words appear in at least 8 decades, with a minimum and maximum frequency of 191 and 108,796, respectively. We refer to the resulting set of 99 words and corresponding shift scores as the 'GEMS dataset' or the 'GEMS words', as appropriate. We collect a contextualised representation for each occurrence of these words in the second century of COHA, using BERT as described in Section 3.2. This results in a large set of usage representations, \u223c1.3M in total, which we cluster into usage types using K-Means and silhouette coefficients (Section 3.3). We use these usage representations and usage types in the evaluation and the analyses offered in the remaining of the paper. Correlation with Human Judgements Before using our proposed method to analyse language change, we assess how its key components compare with human judgements. We test whether the clustering into usage types reflects human similarity judgements (Section 5.1) and to what extent the degree of change computed with our metrics correlates with shift scores provided by humans (Section 5.2). Evaluation of Usage Types The clustering of contextualised representations into usage types is one of the main steps in our method (see Section 3.3). It relies on the similarity values between pairs of usage representations created by the language model. To quantitatively evaluate the quality of these similarity values (and thus, by extension, the quality of usage representations and usage types), we compare them to similarity judgements by human raters. New dataset of similarity judgements We create a new evaluation dataset, following the annotation approach of Erk et al. (2009 Erk et al. ( , 2013) ) for rating pairs of usages of the same word. Since we need to collect human judgements for pairs of usages, annotating the entire GEMS dataset would be extremely costly and time consuming. Therefore, to limit the scope of the annotation, we select a subset of words. For each shift score value s in the GEMS dataset, we sample a word uniformly at random from the words annotated with s. This results in 16 words. To ensure that our selection of usages is sufficiently varied, for each of these words, we sample five usages from each of their usage types (the number of usage types is word-specific) along different time intervals, one usage per 20-year period over the century. All possible pairwise combinations are generated for each target word, resulting in a total of 3,285 usage pairs. We use the crowdsourcing platform Figure Eight 4 to collect five similarity judgements for each of these usage pairs. Annotators are shown pairs of usages of the same word: each usage shows the target word in its sentence, together with the previous and the following sentences (67 tokens on average). Annotators are asked to assign a similarity score on a 4-point scale, ranging from unrelated to identical, as defined by Brown (2008) and used e.g., by Schlechtweg et al. (2018) . 5 A total of 380 annotators participated in the task. The interrater agreement, measured as the average pairwise Spearman's correlation between common annotation subsets, is 0.59. This is in line with previous approaches such as Schlechtweg et al. (2018) , who report agreement scores between 0.57 and 0.68. Results To obtain a single human similarity judgement per usage pair, we average the scores given by five annotators. We encode all averaged human similarity judgements for a given word in a square matrix. We then compute similarity scores over pairs of usage vectors output by BERT 6 to obtain analogous matrices per word and measure Spearman's rank correlation between the humanand the machine-generated matrices using the Mantel test (Mantel, 1967) . We observe a significant (p < 0.05) positive correlation for 10 out of 16 words, with \u03c1 coefficients ranging from 0.13 to 0.45. 7 This is an encouraging result, which indicates that BERT's word representations and similarity scores (as well as our clustering methods which build on them) correlate, to a substantial extent, with human similarity judgements. We take this to provide a promising empirical basis for our approach. Evaluation of Semantic Change Scores We now quantitatively assess the semantic change scores yielded by the metrics described in Section 3.4 when applied to BERT usage representations and the usage types created with our approach. We do so by comparing them to the human shift scores in the GEMS dataset. For consistency with this dataset, which quantifies change from the 1960's to the 1990's as explained in Section 4, we only consider these four decades when calculating our scores. Using each of the metrics on representations from these time intervals, we assign a semantic change score to all the GEMS words. We then compute Spearman's rank correlation between the automatically generated change scores and the gold standard shift values. Results Table 1 shows the Spearman's correlation coefficients obtained using our metrics, together with a frequency baseline (the difference between the normalised frequency of a word in the 1960's and in the 1990's). The three proposed metrics yield significant positive correlations. This is again a very encouraging result regarding the potential of contextualised word representations for capturing lexical semantic change. As a reference, we report the correlation coefficients with respect to GEMS shift scores documented by the authors of two alternative approaches: the count-based model by Gulordava and Baroni (2011) For all our metrics, the max across the four time intervals-i.e., identifying the pair of successive intervals where the strongest shift has occurred (cfr. end of Section 3.4)-is the best performing aggregation strategy. Table 1 only shows values obtained with max and Euclidean distance for APD, as they are the best-performing options. It is interesting to observe that APD can prove as informative as JSD and ED, although it does not depend on the clustering of word occurrences into usage types. Yet, computing usage types offers a powerful tool for analysing lexical change, as we will see in the next section. Analysis In this section, we provide an in-depth qualitative analysis of the linguistic properties that define usage types and the kinds of lexical semantic change we observe. More quantitative methods (such as taking the top n words with highest JSD, APD and ED and checking, e.g., how many cases of broadening each metric captures) are difficult to operationalise (Tang et al., 2016) because there exist no well-established formal notions of semantic change types in the linguistic literature. To carry out this analysis, for each GEMS word, we identify the most representative usages in a given usage type cluster by selecting the five closest vectors to the cluster centroid, and take the five corresponding sentences as usage examples. What do Usage Types Capture? We first leave the temporal variable aside and present a synchronic analysis of usage types. Our However, to allow for direct comparison, Frermann and Lapata (2016) computed Spearman correlation for that work (see their footnote 7), which is the value we report. goal is to assess the interpretability and internal coherence of the obtained usage clusters. We observe that usage types can discriminate between underlying senses of polysemous (and homonymous) words, between literal and figurative usages, and between usages that fulfil different syntactic roles; plus they can single out phrasal collocations as well as named entities. Polysemy and homonymy Distinctions often occur between underlying senses of polysemous and homonymous words. For example, the vectors collected for the polysemous word curious are grouped together into two usage types, depending on whether curious is used to describe something that excites attention as odd, novel, or unexpected ('a wonderful and curious and unbelievable story') or rather to describe someone who is marked by a desire to investigate and learn ('curious and amazed and innocent'). The same happens for the homonymous usages of the word coach, for instance, which can denote vehicles as well as instructors (see Figure 2a for a diachronic view of the usage types). Metaphor and metonymy In several cases, literal and metaphorical usages are also separated. For example, occurrences of curtain are clustered into four usage types (Figure 2c ): two of these correspond to a literal interpretation of the word as a hanging piece of cloth ('curtainless windows', 'pulled the curtain closed') whereas the other two indicate metaphorical interpretations of curtain as any barrier that excludes the free exchange of information or communication ('the curtain on the legal war is being raised'). Similarly, we obtain two usage types for sphere: one for literal usages that denote a round solid figure ('the sphere of the moon'), and the other for metaphorical interpretations of the word as an area of knowledge or activity ('a certain sphere of autonomy') as well as metonymical usages that refer to the planet Earth ('land and peoples on the top half of the sphere'). Syntactic roles and argument structure Further distinctions are observed between word usages that fulfil a different syntactic functionality: not only is part-of-speech ambiguity detected (e.g., 'the cost-tapered average tariff' vs. 'cost less to make') but contextualised representations also capture regularities in syntactic argument structures. For example, usages of refuse are clustered into nominal usages ('society's emotional refuse', 'the amount of refuse'), verbal transitive and intransi-tive usages ('fall, give up, refuse, kick'), as well as verbal usages with infinitive complementation ('refuse to go', 'refuse for the present to sign a treaty'). Collocations and named entities Specific clusters are also assigned to lexical items that are parts of phrasal collocations (e.g., 'iron curtain') or of named entities ('alexander graham bell' vs. 'belllike whistle'). Other distinctions Some distinctions are interpretable but unexpected. As an example, the word doubt does not show the default noun-verb separation but rather a distinction between usages in affirmative contexts ('there is still doubt', 'the benefit of the doubt') and in negative contexts ('there is not a bit of doubt', 'beyond a reasonable doubt'). Observed errors For some words, we find that usages which appear to be identical are separated into different usage types. In a handful of cases, this seems due to the setup we have used for experimentation, which sets the minimum number of clusters to 2 (see Section 3.3). This leads to distinct usage types for words such as maybe, for which a single type is expected. In other cases, a given interpretation is not identified as an independent type, and its usages appear in different clusters. This holds, for example, for the word tenure, whose usages in phrases such as 'tenure-track faculty position' are present in two distinct usage types (see Figure 2b ). Finally, we see that in some cases a usage type ends up including two interpretations which arguably should have been distinguished. For example, two of the usage types identified for address are interpretable and coherent: one includes usages in the sense of formal speech and the other one includes verbal usages. The third usage type, however, includes a mix of nominal usages of the word as in 'disrespectful manners or address' as well as in 'network address'. What Kinds of Change are Observed? Here we consider usage types diachronically. Different kinds of change, driven by cultural and technological innovation as well as by historical events, emerge from a qualitative inspection of usage distributions along the temporal dimension. We describe the most prominent kinds-narrowing and broadening, including metaphorisation-and discuss the extent to which our metrics are able to detect them. 1910 1920 1930 1940 1950 1960 1970 1980 1990 1910 1920 1930 1940 1950 1960 1970 1980 1990 1910 1920 1930 1940 1950 1960 1970 1980 1990 1910 1920 1930 1940 1950 1960 1970 1980 1990 (Davies, 2012) . The legends show sample usages per identified usage type. Narrowing Examination of the dynamics of usage distributions allows us to see that for a few words certain usage types disappear or become less common over time (i.e., the interpretation of the word becomes 'narrower', less varied). This is the case, for example, for coach, where the frequency decrease of a usage type is gradual and caused by technological evolution (see Figure 2a ). Negative mean ED (see Section 3.4) reliably indicates this kind of narrowing. Indeed coach is assigned one of the lowest ED score among the GEMS words. In contrast, ED fails to detect the obsolescence of a usage type when new usage types emerge simultaneously (since this may lead to no entropy reduction). This is the case, e.g., of tenure. The usage type capturing tenure of a landed property becomes obsolete; however, we obtain a positive mean ED caused by the appearance of a new usage type (the third type in Figure 2b ). Broadening For a substantial amount of words, we observe the emergence of new usage types (i.e., a 'broadening' of their use). This may be due to technological advances as well as to specific historical events. As an example, Figure 2d shows how, starting from the 1950's and as a result of technological innovation, the word disk starts to be used to denote also optical disks while beforehand it referred only to generic flat circular objects. A special kind of broadening is metaphorisation. As mentioned in Section 6.1, the usage types for the word curtain include metaphorical interpretations. Figure 2c allows us to see when the metaphorical meaning related to the historically charged expression iron curtain is acquired. This novel usage type is related to a specific historical period: it emerges between the 1930's and the 1940's, reaches its peak in the 1950's, and remains stably low in frequency starting from the 1970's. The metrics that best capture broadening are JSD and APD-e.g., disk is assigned a high semantic change score by both metrics. Yet, sometimes these metrics generate different score rankings. For example, curtain yields a rather low APD score due to the low relative frequency of the novel usage (Figure 2c ). In contrast, even though the novel us-age type is not very prominent in some decades, JSD can still discriminate it and measure its development. On the other hand, the word address, for which we also observe broadening, is assigned a low score by JSD due to the errors in its usage type assignments pointed out in Section 6.1. As APD does not rely on usage types, it is not affected by this issue and does indeed assign a high change score to the word. Finally, although our metrics help us identify the broadening of a word's meaning, they cannot capture the type of broadening (i.e., the nature of the emerging interpretations). Detecting metaphorisation, for example, may require inter-cluster comparisons to identify a metaphor's source and target usage types, which we leave to future work. Conclusion We have introduced a novel approach to the analysis of lexical semantic change. To our knowledge, this is the first work that tackles this problem using neural contextualised word representations and no lexicographic supervision. We have shown that the representations and the detected semantic shifts are aligned to human interpretation, and presented a new dataset of human similarity judgements which can be used to measure said alignment. Finally, through extensive qualitative analysis, we have demonstrated that our method allows us to capture a variety of synchronic and diachronic linguistic phenomena. Our approach offers several advantages over previous methods: (1) it does not rely on a fixed number of word senses, (2) it captures morphosyntactic properties of word usage, and (3) it offers a more effective interpretation of lexical meaning by enabling the inspection of particular example sentences. In recent work, we have experimented with alternative ways of obtaining usage representations (using a different language model, fine-tuning, and various layer selection strategies) and we have obtained very promising results in detecting semantic change across four languages (Kutuzov and Giulianelli, 2020) . In the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics, e.g., to automatically discriminate between different types of meaning change. We expect our work to inspire further analyses of variation and change which exploit the expressiveness of contextualised word representations. Acknowledgments This paper builds upon the preliminary work presented by Giulianelli (2019) . We would like to thank Lisa Beinborn for providing useful feedback as well as the three anonymous ACL reviewers for their helpful comments. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 819455). A Appendix This appendix includes supplementary materials related to Section 5.1. A.1 New Dataset of Similarity Judgements Obtaining usage pairs For each of our 16 target words, we sample five usages from each of their usage types, one for every 20-year period in the last century of COHA. When a usage type does not occur in a time interval, we uniformly sample an interval from those that do contain occurrences of that usage type. All possible pairwise combinations (without replacement) are generated for each target word, resulting in a total of 3,285 usage pairs. Crowdsourced annotation We use the crowdsourcing platform Figure Eight (since then acquired by Appen 9 ) to collect five similarity judgements for each of these usage pairs. To control the quality of the similarity judgements, we select Figure Eight workers from the pool of most experienced contributors, we require them to be native English 9 https://appen.com speakers and to have completed a test quiz consisting of 10 similarity judgements. For this purpose, 170 usage pairs were manually annotated by the first author with 1 to 3 acceptable labels. The compensation scheme for the raters is based on an average wage of 10 USD per hour. Figures 4 and 5 (on the next pages) show the full instructions given to the annotators and Figure 3 illustrates a single annotation item. A.2 Correlation Results We measure Spearman's rank correlation between human-and machine-generated usage similarity matrices using the Mantel test and observe a significant positive correlation for 10 out of 16 words.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions that the project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 819455). The ERC is a public body for scientific and technological research funded by the European Union, which classifies it as a research agency. There is no mention of funding from defense, corporate entities, foundations, or any indication that there were no other sources of funding.",
    "abstract": "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.",
    "countries": [
        "Netherlands"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": 82,
    "year": 2020,
    "month": "July",
    "title": "Analysing Lexical Semantic Change with Contextualised Word Representations"
}