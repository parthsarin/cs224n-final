{
    "article": "We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with BERT base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performance. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures. Introduction Computing sentence semantic representations traditionally calls for a recursive compositional function whose structure is tree-shaped. There is a strong intuition in natural language processing that language has a recursive structure (Chomsky, 1956; Shen et al., 2019) . Tree-based models should thus mimic the compositional effect of language and enable better generalization and abstraction. Yet, tree-based models need carefully handannotated data to be trained. Alternative methods such as recurrent neural network (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or BERT (Devlin et al., 2019) have gained increased popularity as they only require raw text as input. On the other hand, as many results suggest (Linzen et al., 2016; Jawahar et al., 2019; Clark et al., 2019) , these new models acquire some sort of tree structure. Another line of work, called latent tree learning, induces trees from raw text and computes semantic representations along with the inferred structure (Socher et al., 2011; Bowman et al., 2016; Dyer et al., 2016; Maillard et al., 2019; Yogatama et al., 2017; Kim et al., 2019) . Such methods preserve explicit recursive computation and produce intelligible tree structures. Moreover, the parser and composition function are learned jointly and are specific to a given task or domain. Choi et al. (2018) propose the closest approach to ours by composing a tree using the Gumbel-Softmax estimator. The method is fully differentiable, produces a discrete tree, and does not require training the parser using an auxiliary task. However, Williams et al. (2018) show the method does not produce meaningful syntactic representations and that trees are inconsistent across initializations. Moreover, Choi et al. (2018) produces trees by selecting and merging adjacent nodes. Therefore, it cannot directly use architectures designed for standard parsing formalisms such as dependency structures. We propose a unified architecture, which infers an explicit tree structure and recursively trains a sentence embedding model. Our method is fully differentiable and relies on existing and wellknown components. We use a standard dependency parsing structure, obtained using a graph-based biaffine dependency parser (Dozat and Manning, 2017) . However, our model is not limited to a particular parser architecture as long as it is differentiable. We organize our paper as follows: we present our model in section 2. In section 3, we evaluate our model on textual entailment and semantic similarity tasks. We then conduct an ablation study and analyze the impact of the parser initialization. We compare the learned structures across initializations and with interpretable annotations (4.1) and we study how latent structures impact performance on downstream tasks (4.2). Model Our model jointly performs sentence parsing and the prediction of a sentence embedding. The sentence embedding is predicted by a weighted TREE-LSTM whose tree structure is provided by a dependency parser. The TREE-LSTM recursive composition function crucially uses a weighted sum of the child representations whose weights are provided by the parser edges, hence linking the parser outputs to the TREE-LSTM recursion. Figure 1 illustrates the architecture detailed in Eq. 1 to 10. Parsing model The parser is a standard graph based biaffine dependency parser 1 (Dozat and Manning, 2017) . It is formalized in two steps. First, Eq. 1 and 2 compute a weight matrix that is interpreted as a weighted directed graph whose nodes are the sentence tokens: a (dep) k = MLP(h k ), a (head) j = MLP(h j ) (1) s (arc) kj = (a (dep) k \u2295 1) \u22a4 U (b) a (head) j + b (b) (2) With h k \u2208 R d the hidden state associated with the word at index k in the input sentence and in Eq. 2, U (b) \u2208 R (d+1)\u00d7d and b (b) \u2208 R. The symbol \u2295 denotes vector concatenation and MLP in Eq. 1 are single-layer perceptron networks. The second step performs parsing by computing a maximum spanning tree from the graph. As in Dozat and Manning (2017) , we use the Max Spanning Tree (MST) algorithm to ensure the wellformedness of the tree (Chu, 1965; Edmonds et al., 1967) : \u03b1 kj = 1 mst(s (arc) kj ) s (arc) kj (3) Where \u03b1 kj is the probability of the edge linking node j to node k. For a given node k, there is at most one non-zero edge leading to its governor j. Compositionally weighted tree LSTM Given a predicted tree structure, we recursively encode the sentence using a variant of the Child Sum Tree model from Tai et al. (2015) . The recursion follows the predicted structure: from the leaves to the root. At each step j, the transition function takes as input the word vector representation x j of the 1 We give hyper-parameter details for the biaffine parser in Appendix A.3. Figure 1 : We illustrate the architecture detailed in Eq. 1 to 10. The Biaffine parser provides the sentence structure from which the TREE-LSTM computes sentence embeddings. The full pipeline is differentiable as the TREE-LSTM weights are given by the parser. head node j and the previously computed hidden states h k from all its children. hj = k\u2208C(j) \u03b1 kj h k , (4) i j = \u03c3 W (i) x j + U (i) hj + b (i) , (5) o j = \u03c3 W (o) x j + U (o) hj + b (o) , (6) u j = tanh W (u) x j + U (u) hj + b (u) , (7) f jk = \u03c3 W (f ) x j + U (f ) h k + b (f ) , (8) c j = i j \u2299 u j + k\u2208C(j) f jk \u2299 c k , (9) h j = o j \u2299 tanh(c j ), (10) Where x j and h j are respectively the word vector representation and hidden state associated with the head node j. In Eq. 4, C(j) denotes the set of children of node j. \u03c3 denotes the logistic sigmoid function and \u2299 denotes elementwise multiplication. Crucially, in our case, Eq. 4 is a weighted sum rather than a standard sum and the weights are those \u03b1 kj provided by the parser. We use the embedding computed by the weighted TREE-LSTM at the root of the tree as the sentence embedding. The tree shape and the edge weights are given by the best prediction of a graph parser. The parsing model is linked to the TREE-LSTM by the weights \u03b1 kj . This architecture allows us to jointly update the parser and the TREE-LSTM weights using only the downstream task loss. The supervision comes only from the objective of the downstream task, and no intermediate structure target is required. Our model is fully differentiable and preserves the discreteness of the tree composition process. It relies on a dependency parsing formalism and could accommodate any differentiable parser. Evaluation Our architecture primarily aims to produce relevant embeddings for downstream tasks. To this end, we compare our setup with other models from the literature on various tasks. For this comparison, we first pre-train the parsing submodel on human-annotated sentences from the Penn Tree Bank (PTB) (Marcus et al., 1993) converted to Stanford dependencies. We then fine-tune the parser's parameters on the task while training the full model 2 . Semantic textual similarity (STS) We first evaluate our model on the SICK-R downstream task (Marelli et al., 2014) , which is dedicated to assessing models' compositional properties. The dataset comprises 9,927 sentence pairs, distributed in a 4,500/500/4,927 train/dev/test split, annotated for semantic similarity on a 1 to 5 real range. It includes specific examples of variations on passive and active forms, quantifier and modifier switches, or negations 3 . We use a similar training procedure as in Tai et al. (2015) . We transform the target y from the SICK-R task into the distribution p defined by: p i = \uf8f1 \uf8f2 \uf8f3 y \u2212 \u230ay\u230b, i = \u230ay\u230b + 1 \u230ay\u230b \u2212 y + 1, i = \u230ay\u230b 0 otherwise We use a dedicated architecture to predict the similarity distribution from a pair of sentences. The 2 In this configuration, we observe pre-training the parser may cause weights \u03b1 to become too large in Eq. 3. This leads to poor downstream performance. We correct this with a multiplicative parameter \u03c4 whose value is estimated during training. It means we replace Eq. 3 with: \u03b1 kj = \u03c4 \u2022 1 mst(s (arc) kj ) s (arc) kj for tree weights computation. 3 Appendix A.1 details the hyper-parameters and training infrastructure. similarity module takes as input a pair of sentence vectors h L and h R and computes their componentwise product h L \u2299 h R and their absolute difference |h L \u2212 h R |. Given these features, we compute the probability distribution p\u03b8 using a two-layer perceptron network (MLP): h \u00d7 = h L \u2299 h R , h + = |h L \u2212 h R |, h s = \u03c3(W (\u00d7) h \u00d7 + W (+) h + + b (h) ), p\u03b8 = softmax(W (p) h s + b (p) ), (11) We use the KL-divergence between the prediction p\u03b8 and the ground truth p as training objective: J(\u03b8) = 1 N N k=1 KL(p (k) ||p (k) \u03b8 ) + \u03bb||\u03b8|| 2 2 (12) Finally during inference, the similarity score \u0177 is computed as \u0177 = r \u22a4 p\u03b8 with r \u22a4 = [1, . . . , 5]. Encoder r BOW \u2020 78.2 (1,1) LSTM \u2020 84.6 (0.4) Bidirectional LSTM \u2020 85.1 (0.4) N-ary TREE-LSTM \u2020 (Tai et al., 2015) 85.3 (0.7) Childsum TREE-LSTM \u2020 (Tai et al., 2015) 86.5 (0.4) BERT-base (Devlin et al., 2019) 87.3 (0.9) Unified TREE-LSTM \u2020 (Our model) 87.0 (0.3) Table 1 : Evaluation on the SICK-R task: we pre-train our parsing module on the PTB and continue to update the full model on the SICK-R task. We compare with BERT and models relying on sequential and tree structures. We report Pearson correlation on the test set, by convention as r \u00d7 100 (avg. and std. from 5 runs). \u2020 indicates models that we trained. All models are trained following the same procedure detailed in Appendix A.1. Table 1 reports the results from the test set. As expected, structured models perform better than models using weaker underlying structures. We also observe that our model is competitive with a BERT-base upper-line. It is essential to note that BERT models are heavily pre-trained on vast corpora, whereas our structured models are trained only on the SICK-R and PTB data. Textual entailment We also test our model on the Stanford Natural Language Inference (SNLI) task (Bowman et al., 2015) , which includes 570k pairs of sentences with the labels entailment, contradiction, and neutral, distributed in a 550k/10k/10k train/dev/test split 4 . We use a similar training procedure as in Choi et al. (2018) . A dedicated architecture is used to predict the similarity distribution from a pair of sentences. The similarity module takes as input a pair of sentence vectors h L and h R and computes their componentwise product h L \u2299 h R and their absolute difference |h L \u2212h R |. Given these features, we compute the probability distribution p\u03b8 using a three-layer perceptron network (MLP): h \u00d7 = h L \u2299 h R , h + = |h L \u2212 h R |, h s = h \u00d7 \u2295 h + \u2295 h L \u2295 h R h s = ReLU(W (1) h s + b (1) ), h s = ReLU(W (2) h s + b (2) ), p\u03b8 = softmax(W (p) h s + b (p) ), (13) We use the cross entropy loss between the prediction p\u03b8 and the ground truth p as training objective: J(\u03b8) = \u2212 1 N N k=1 p (k) log p(k) \u03b8 + \u03bb||\u03b8|| 2 2 (14) Encoder Test Acc. SPINN \\w Reinforce (Yogatama et al., 2017) 80.5 CYK and TREE-LSTM (Maillard et al., 2019) 81.6 SPINN (Bowman et al., 2016) 83.2 ST-Gumbel (Choi et al., 2018) 86.0 Structured Alignment (Liu et al., 2018) 86.3 BERT-base (Zhang et al., 2020) 90.7 Unified TREE-LSTM (Our model) 85.0 (0.2) Table 2 : Evaluation on the SNLI-R task: We pre-train our parsing module on the PTB and continue to update the full model on the SNLI task. We compare with BERT and latent tree learning models. We report the accuracy on the test set (avg. and std. from 2 runs). We report the results in Table 2 . Our results are close to Choi et al. (2018) , which also compute semantic representations along to discrete tree structures but relies on a distinct syntactic formalism. In models from Liu et al. (2018) and Zhang et al. (2020) sentences are encoded with direct interaction using an attention mechanism. These architectures relying on cross sentence attention outperform those without. We hypothesize that, on this textual entailment task, the final prediction cannot be directly deduced from both sentence embeddings. In this case, BERT and the structured alignment model have a clear advantage since they encode interactions between both sentences. Impact of the parser initialization Our framework primarily aims to be a structured sentence encoder. Accordingly, we have demonstrated in the previous section that our architecture is competitive with comparable approaches and might even be competitive with BERT-based models. We are also interested in interpreting the structures the model actually learns and how such structures impact downstream performance. In the previous section, we pre-trained the parser on human annotated data. However, the optimal structure might differ from the task. Moreover, for computational reasons, it might even differ significantly from linguistic insights. In this section we perform an ablation study to better understand how the initialization of the parser impacts the resulting structures (4.1) and the final downstream performance (4.2). We define two initialization scenarios below. In both, we either continue to update the parser when fine-tuning the model on downstream tasks or freeze the parser and only train the TREE-LSTM. These two configurations are indicated with respectively \u2713 and \u00d7 symbols. Linguistic annotations Tree-structured models traditionally rely on linguistic structures obtained by parsers (Tai et al., 2015) . For languages such as English, linguistic resources are available; it is technically possible to pre-train the parser. However, resources such as the PTB are not available in all languages. To better quantify the benefits of using linguistic annotations, we propose the following configurations, using various proportions of the PTB to initialize the parser: \u2022 In the PTB-All configuration, the parser is previously pre-trained on the PTB. This configuration is the same as in section 3. \u2022 In the PTB-\u2205 configuration, the parser parameters are randomly initialized \u2022 We also consider an initialization with only a small proportion of the PTB and train a parser by only using 100 randomly selected samples. This configuration is referred as PTB-100. Unsupervised structures Many lines of work investigate if attention matrices from large pretrained models reflect syntactic structures (Jawahar et al., 2019; Clark et al., 2019; Ravishankar et al., 2021) or if tree structures can be integrated into transformers (Wang et al., 2019; Bai et al., 2021) . Since our model is not specific to any parser architecture. It is possible to use the internal representations from BERT to infer sentence structure. BERT relies upon the self-attention mechanism. Inside each layer, tokens are computed as a weighted combination from each other. For each token x, a query and key vector are computed using a linear transformation detailed in Eq 15. Given these vector tuples, the attention weights s are computed following Eq 16 in which N refers to the dimension of the query and key vectors. q j , k j = W (q,k) x j + b (q,k) (15) s kj = softmax k k \u2022 q j \u221a N (16) We induce a tree structure following a procedure close from Ravishankar et al. ( 2021 ). We interpret the combination weights s as a weighted graph whose nodes are tokens. We then apply Eq 2 to induce a maximum spanning tree from the attention matrix as detailed in section 2. We make use of the last layer and induce a tree for each attention head taken separately 5 . Given the tree structure induced from BERT, we apply our TREE-LSTM model detailed in Eq. 4 to 10. In this configuration, we only use BERT as an unsupervised parser to infer a sentence structure. The semantic composition along with the structure to produce a sentence embedding is solely computed by the weighted TREE-LSTM. Impact on parses This section analyzes to which extent the structures generated by our model are comparable with meaningful linguistic annotations. We compare the parses generated by two distinct models differing by their initialization on the development set of both tasks. Our reference is the silver parses from the PTB-All configuration, where the parser is previously pre-trained on the full PTB and not updated during training. Table 3 measures the Unlabeled Attachment Score (UAS) between the two parsers, that is, the ratio from the number of common arcs between two parses by the total number of arcs 6 . Table 3 : Impact of the parser initialization on parses: we compare the parses from the SICK-R and SNLI development sets using different parser initializations. We obtained the PTB parses with the graph parser initialized on a given proportion of the PTB (section 2). Regarding BERT , we inferred the structures from the pattern learn by the pre-trained model (section 4). We either continue to update the parser (\u2713) when fine-tuning the model on downstream tasks or freeze the parser (\u00d7) and only train the TREE-LSTM. UAS corresponds to the mean pairwise comparison of two configurations between two runs (std. in parentheses). We observed distinct behaviors given both tasks. We believe this effect is due to the differences between training configurations. In particular, we use the Adagrad optimizer for the SICK-R task and Adam for the SNLI task. For the SICK-R task, the UAS between PTB-\u2205 and PTB-All are very low. This reveals that the parses obtained with only downstream task supervision have few in common with gold linguistic parses. In this regard, we share the observation from Williams et al. (2018) that latent trees obtained from sole downstream supervision are not meaningful in syntax. However, PTB-All and PTB-100 are remarkably close; only a few PTB samples are needed to obtain intelligible linguistic parses with our setup. Regarding the PTB-100 configuration, we note an evolution of the parses when finetuning on the downstream task. We hypothesize that the model can adapt to the dataset's specificity. For the SNLI task, fine-tuning the parser deeply impacts the shape of the parses. Depending from the initialization, parses will converge to distinct structures. Indeed, the UAS between all configura-tions is very low. Moreover, when using a random initialization (PTB-\u2205), the standard deviation between UAS from various runs is very high: without fixed initialization, parses become unstable. For the initialization with an unsupervised structure, we only evaluate our setup on the SNLI task, which has more training samples. We compare the structures obtained with BERT with the silver trees from the PTB-All-\u00d7 configuration. We present the mean UAS over the trees obtained for all attention heads. The standard deviation is relatively high, pointing underlying structures differ given the attention head. Nonetheless, self-supervised structures do not align well with linguistic insights. When updating BERT together with the TREE-LSTM, the UAS increases while the standard deviation decreases. As BERT is fine-tuned, structures tend to become more standard and present slightly more similarities with linguistic patterns. Impact on downstream tasks We observed in previous section 4.1 that the initialization and the training configuration of the parser component deeply impact the resulting parses. We now study the impact of the parser initialization on downstream performance. PTB sample size Parser fine-tuning SICK-R (r) SNLI (Acc.) Table 4 : Impact of the parser initialization on downstream task performance: We pre-train the parser module with a given sample size from the PTB. We either freeze (\u00d7) or update (\u2713) the parser during the finetuning. We report the average test score set from 5 runs for SICK-R and 2 runs for SNLI (the score from the development set are in parentheses). We report Pearson correlation by convention as r \u00d7 100. Table 4 compares the impact of the different initializations for both tasks. We report the Pearson correlation on the test set of the SICK-R task and the accuracy on the test set from the SNLI task. We either freeze the parser component or con-tinue to update it, given the downstream loss for each initialization. Fine-tuning the parser on the task generally leads to an improvement of the downstream results. In that regard, we share the observation from other latent tree learning methods (Maillard et al., 2019; Choi et al., 2018) ; models jointly learning the parsing and composition function outperform those with a fixed structure. Models using the full or partial annotated data outperform models relying on the sole downstream supervision (PTB-\u2205), in particular on the SICK-R task. We previously observed that fine-tuning the parser can lead to tree structure diverging from linguistic patterns. Nonetheless, regarding the downstream performance, human annotation appears as a good initialization for our model. Models relying on linguistic-driven structures seem to achieve better performance. Nonetheless, the difference is thin, and we present here an average score across trees obtained from all attention heads. Therefore some attention heads might present structures as efficient as linguistic patterns. Conclusion and future work We investigate the relevance of incorporating treelike structural bias in the context of sentence semantic inference. To this end, we formulate an original model for learning tree structure with distant downstream supervision. Our model is based on well-known components and could therefore accommodate a variety of parsing architectures such as graph parsers or attention matrices from BERT. We evaluate our model on textual entailment and semantic similarity tasks and outperform sequential models and tree-structured models relying on external parsers. Moreover, when initialized on human-annotated structures, our model improves inference close to BERT base performance on the semantic similarity task. We then conduct an ablation study to quantify the impact of the parser initialization on the resulting structures and downstream performance. We corroborate that the sole use of downstream supervision is insufficient to produce parses that are easy to interpret. To encourage convergence towards readable linguistic structures, we examine a number of initialization setups. Our structures often converge toward trivial branching patterns, which have few in common with gold linguistic parses. Yet, regarding downstream performance, linguistic insights appear as a relevant initialization. A Appendices A.1 SICK-R training configuration Hyper-parameters We set the hyperparameters given literature on the domain, in particular regarding choices made in Tai et al. (2015) . For all experiments detailed in the current section, the batch size is fixed to 25, weight decay to 1e \u22124 and gradient clipping set to 5.0. The learning rate is set to 0.025 for the TREE-LSTM parameters. When using a pre-training procedure for the parser, we set the learning rate to 5e \u22123 and use the following warm-up: for the first epoch, the parser is frozen. For the following epochs, all parameters are trained. At each epoch, the parser learning rate is divided by a factor of two. Without pre-training, the learning rate is set to 5e \u22124 for the parser. All model weights are initialized with a Xavier distribution. The hidden size of the similarity architecture is set to 50. The TREE-LSTM hidden size is set to 150. We use the Adagrad optimizer. We do not apply any dropout. We perform training for a maximum of 20 epochs and stop when no improvement was observed on the development set for 3 consecutive epochs. Regarding the vocabulary, we limit the size to 20,000 words and initialize the embeddings layer with 300-dimensional GloVe embeddings 7 . The embeddings are not updated during training. Training infrastructure We trained all models on a single 1080 Ti Nvidia GPU. Training time for each epoch is approximately 1 minute. The model counts 13.7M parameters. Data can be downloaded using the SentEval package 8 . A.2 SNLI training configuration Hyper-parameters We set the hyper-parameters given literature on the domain, in particular regarding choices made in Choi et al. (2018) . For all experiments detailed in section 3.2, the batch size is fixed to 128, weight decay to 0, and gradient clipping set to 5.0. The learning rate is set to 1e \u22123 for the TREE-LSTM and the parser. The hidden size of the similarity architecture is set to 1,024. The TREE-LSTM hidden size is set to 600. We use the Adam optimizer. We apply a 0.2 dropout within the similarity architecture. We perform training for a maximum of 20 epochs and stop when no improvement was observed on the development set for 3 7 https://nlp.stanford.edu/projects/ glove/ 8 https://github.com/facebookresearch/ SentEval consecutive epochs. Regarding the vocabulary, we limit the size to 100,000 words and initialize the embeddings layer with 300-dimensional GloVe embeddings. The embeddings are not updated during training. Training infrastructure We trained all models on a single 1080 Ti Nvidia GPU. Training time for each epoch is approximately 2h30 hours. The model counts 13.7M parameters. Data can be downloaded using the SentEval package 9 . A.3 Model Architecture Regarding the biaffine parser, all parameters are chosen given Dozat and Manning (2017) recommendations. We use a hidden size of 150 for the MLPs layers and 100 for the biaffine layer. The dropout rate is fixed to 0.33. We use an opensource implementation of the parser and replace the pos-tags features with character level features. Therefore we don't need pos-tags annotations to parse our corpus 10 . We encode words using 100dimensional GloVe embedding and a character embedding size of 50. Word vectors are then fed to a bidirectional LSTM with 3 layers of size 400. A.4 BERT unsupervised parsing When using BERT to perform unsupervised parsing, we use the implementation of BERT-base model from the Transformers library 11 . When fine-tuning the parser component, we set the learning rate to 2e \u22125 When fine-tuning BERT parser, each epoch takes around 5 hours on the SNLI. Without finetuning, this time is reduced to 90 minutes. A.5 Visualization of the parses We illustrate the effect summarize on Table 3 ). We either freeze (\u00d7) or update (\u2713) the parser during the fine tuning on the SNLI. We include the weights \u03b1 produced from the parser. We report the accuracy from a single run on the test set. samples while the one from the last row (2e and 2f) are initialized using unsupervised patterns. For the initialization with the PTB, we observe the fine-tuning makes the tree evolve to trivial structures and tend to connect every node to an arbitrary root. We hypothesize, such trivial structures present advantages from a computational point of view. As observed in Shi et al. (2018) , trivial trees without syntax properties might lead to surprisingly good results. Shi et al. (2018) hypothesize that trivial trees gain might benefit from shallow and balanced properties. For BERT parser initialization, we observe the fine-tuning produces rather sequential patterns, with words connected to direct neighbors. Some isolated groups of words also present inner connections.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 3.128162811005808e-07,
        "none": 0.9965103332895107
    },
    "reasoning": "Reasoning: The provided text does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or an absence of funding. Without explicit mention, it's not possible to determine the funding sources based on the provided information.",
    "abstract": "We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with BERT base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performance. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures.",
    "countries": [
        "France"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": 0,
    "year": 2022,
    "month": "July",
    "title": "Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations"
}