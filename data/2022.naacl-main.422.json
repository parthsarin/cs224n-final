{
    "article": "Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-todate information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) 1 that enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory. 2 * Corresponding author 1 We are also inspired by the French word \"La M\u00e9moire\", meaning \"the memory\". 2 Source code available at https://github.com/ thu-coai/LaMemo. Introduction Language modeling is an important task that tests the ability of modeling long-term dependencies by predicting the current token based on the previous context (Mikolov and Zweig, 2012; Merity et al., 2017) . Recently, Transformer-based language models achieved remarkable performance by enabling direct interaction between long-distance word pairs. However, as the computation overhead grows with the length of the input sequence, Transformers can only process a fixed length segment at a time. To allow long-term information flow across individual segments, existing approaches augment the model with a recurrence memory that stores hidden states computed in previous time steps (Dai et al., 2019) and their compressions (Rae et al., 2020; Martins et al., 2021) for the target tokens to attend to. One limitation of this approach is that the recurrence memory is only aware of older contexts since they are previously computed to predict the next word from left to right. As a result, distant memory states become outdated and less activated by the current context, as illustrated in Figure 1 . When humans read or write a document, they maintain a memory that records important information from the past and often refresh them under the current context to keep it up-to-date. In this paper, we propose Look-Ahead Memory (LaMemo) where memory states \"look ahead\" to future time steps by attending to the token representations on their right side to provide up-to-date contextualization. 3 To maintain information from the long-term history, we propose memory interpolation to take both past and future tokens into consideration, which mimics the bi-directional attention. Note that, directly applying bi-directional attention to update the memory representations brings an additional complexity of O(M 2 ) (M is the mem-ory length). This is expensive when the memory is very long. LaMemo incrementally attends to the right and accumulate the weighted attention sum from previous segments to simulate the full attention in only O(M \u00d7 N ) complexity (N is the target sequence length), which does not increase the attention complexity of Transformer-XL, namely O(N 2 + M \u00d7 N ). We provide an illustration of this mechanism in Figure 3 . Another technique proved to be effective in language modeling is the relative positional encoding (Shaw et al., 2018; Huang et al., 2018; Dai et al., 2019) , which biases the pair-wise attention score purely based on the relative distance of the two tokens. However its ability to generalize to the attention of the future tokens remains unknown, since both the distance and the direction need to be taken into consideration. In preliminary experiments, we observed the unstability of directly applying the relative positional encoding of Dai et al. (2019) to this setting. We propose a simple yet effective modification based on Dai et al. (2019) that disentangles the bias of the relative distance and the attention direction which facilitates the training of LaMemo. We give both theoretical and empirical analysis to the unstability issue and demonstrate the effectiveness of the proposed disentangled relative positional encoding method. To sum up, our contributions are as follows: (1) We propose LaMemo, a memory mechanism that incrementally attends to the right-side tokens, and interpolates with the old memory, which enables bi-directional interaction with a complexity linear in memory length. (2) We propose disentangled relative positional encoding, a simple yet effective solution that disentangles the relative distance and the attention direction that can better generalize to the attention of the future tokens. (3) We conduct experiments on standard language modeling benchmarks and demonstrate LaMemo's superiority over various baselines equppied with different types of memory mechanisms, despite some having an access to longer contexts. Comprehensive comparisons show the benefits of learning memory representations contextualized with up-to-date information. Background Transformer for Language Modeling A Transformer (Vaswani et al., 2017) is composed of multiple layers of identical blocks, including a multi-head self-attention (Bahdanau et al., 2015) that calculates pair-wise token interaction and a feed-foward layer for position-wise projection with a non-linear activation. Both two modules are followed by residual connections (He et al., 2016) and layer normalization (Ba et al., 2016) to facilitate optimization. Given the input sequence representations of the current \u03c4 -th segment X \u03c4 = [x \u03c4 +1 , \u2022 \u2022 \u2022 , x \u03c4 +N ] \u2208 R N \u00d7d where N is the target sequence length and d is the hidden state size, they are first mapped into queries Q, keys K and values V by learned weight matrix to compute self-attention: Q \u03c4 = X \u03c4 W q , K \u03c4 = X \u03c4 W k , V \u03c4 = X \u03c4 W v , (1) where W q , W k , W v \u2208 R d\u00d7d are learnable projection matrices. To perform multi-head self-attention, Q, K, V are further split into H heads. For simplicity, we only consider the case of a single head. In language modeling, the attention map is always added by a causal mask to avoid information leakage from the future when predicting the next token: C \u2192 \u03c4 = Causal-Attn(Q \u03c4 , K \u03c4 , V \u03c4 ) = softmax Q \u03c4 K \u22a4 \u03c4 \u221a d V \u03c4 , (2) where softmax (\u2022) masks position j > i for the i-th row of the input matrix with \u2212\u221e before taking the softmax. The resulted context representations are concatenated and then projected to the final outputs O \u03c4 \u2208 R N \u00d7d with a learnable projection matrix W o \u2208 R d\u00d7d . Finally, the self-attention outputs O \u03c4 are added by the input representations X \u03c4 and fed to the following point-wise non-linear transformation, denoted as f (\u2022): f (x) = LN FFN LN(x) + LN(x) , (3) where LN(\u2022) is the layer normalization and FFN(\u2022) is the feed-forward layer, both of which are applied to each row vector individually. The final output of this Transformer layer is f (O \u03c4 + X \u03c4 ). Outputs of the final layer are projected to the vocabulary to predict Pr(w t |w 1 , \u2022 \u2022 \u2022 , w t\u22121 ). The joint probability of predicting the whole segment is the product of these conditional factors. The final objective is to maximize the following loglikelihood: log Pr(w) = N t=1 log Pr(w t |w 1 , \u2022 \u2022 \u2022 , w t\u22121 ). (4) Recurrence Memory Mechanism To enable the Transformer to consider more contextual information from previous segments, Dai et al. (2019) proposed to augment the Transformer with a recurrence memory which stores the hidden states of previous time steps as extended keys and values, as shown in Figure 2 . Concretely, let us consider a memory length of M and memory representations X \u03c4 \u22121 = [x \u03c4 \u2212M +1 , \u2022 \u2022 \u2022 , x \u03c4 ] \u2208 R M \u00d7d . The extended key and value matrices are obtained by prepend X \u03c4 \u22121 to X \u03c4 before projection: Xsg \u03c4 = [sg(X \u03c4 \u22121 ) \u2022 X \u03c4 ] \u2208 R (M +N )\u00d7d , (5 ) where sg(\u2022) stands for stop-gradient which disables gradient propagation to previous segments, and [\u2022 \u2022 \u2022] indicates concatenation of hidden states along the length dimension. Extended by the recurrence memory, each query vector can consider contexts even beyond the total context length of the attention M + N . As illustrated by Dai et al. (2019) , the effective context length grows linearly to the number of layers and the attention context length due to layer-wise reusing. Another technique necessary to the recurrence memory is the relative positional encodings. By considering only the relative distance between two tokens when computing the attention score, it avoids temporal confusion caused by indexing the same position across segments and injects useful relative bias. Transformer-XL uses the fixed sinusoidal encoding matrix (Vaswani et al., 2017) to provide relative distance bias and learns global bias terms shared across different layers, which can extrapolate to longer contexts with a great reduction of parameters compared to Shaw et al. (2018) : A xl i,j = X \u22a4 i W \u22a4 q W E k X j + X \u22a4 i W \u22a4 q W R k R i\u2212j + u \u22a4 W E k X j + v \u22a4 W R k R i\u2212j , ( 6 ) where R is the sinusoid encoding matrix, u, v are learnable weight vectors governing the global content and position bias, and W E k , W R k are separate key projection matrices for the content and position respectively. Method In this section, we describe our method in detail with our motivation to learn better representations for the memory. Look-Ahead Attention Human language is sequential with one word following another, but humans process information usually in a non-sequential way and recontextualize certain contents for several times. For example, when countering complicated contents during reading, humans usually first store them temporarily in the memory and continue to scan for relevant information if any, and revisit those old contents to refresh their meaning quite often. This dynamic memory refreshing mechanism enables us to thoroughly understand the passage under current contexts. Existing recurrence memory however, lacks this dynamic contextualization ability. As the representations in the recurrence memory are previously computed conditioned on their past, they are not aware of the current contexts which provide more relevant information for the current token prediction. To address this limitation, we propose a lookahead attention that allow the memory to attend to the contexts on their right. Formally, we reuse the notation X \u03c4 = [x \u03c4 +1 , \u2022 \u2022 \u2022 , x \u03c4 +N ] \u2208 R N \u00d7d for the representations of the current target sequence and X \u03c4 \u22121 = [x \u03c4 \u2212M +1 , \u2022 \u2022 \u2022 , x \u03c4 ] \u2208 R M \u00d7d for the representations of the memory. Let us consider the i-th position of the memory X \u03c4 \u22121 , x i can attend to position x j on its right (j > i) without causing information leakage as long as j \u2264 \u03c4 + 1. Though appealing, this na\u00efve approach requires to calculate an M by M attention map, which would become inefficient and redundant when M is significantly greater than N . Actually, since the target segment moves forward N positions at each iteration, we devise an incremental manner of look-ahead attention computation that only requires the newest N positions on the right as key-value pairs. X\u03c4\u22121 = [x \u03c4 \u2212N +2 , \u2022 \u2022 \u2022 , x \u03c4 +1 ] \u2208 R N \u00d7d . (7) Then the look-ahead attention results computed previously can be effectively reused and interpolated with the current ones ( \u00a73.2). Concretely, we formalize the look-ahead attention as follows: K\u03c4\u22121 = X\u03c4\u22121 W k , \u1e7c \u03c4 \u22121 = X\u03c4\u22121 W v , (8) C \u2190 \u03c4 \u22121 = LookAhead-Attn(Q \u03c4 \u22121 , K\u03c4\u22121 , \u1e7c \u03c4 \u22121 ) = softmax Q \u03c4 \u22121 K\u22a4 \u03c4 \u22121 \u221a d \u1e7c \u03c4 \u22121 , (9) where softmax (\u2022) masks position j \u2264 i for the ith row of the input matrix with \u2212\u221e before softmax. Q \u03c4 \u22121 is obtained by Eq. ( 1 ), and the projection matrices of query, key and value are all shared with the causal attention. We illustrate this in Figure 3 where the look-ahead attention (yello paths) increases the attention window of each memory state to M tokens on its right. Memory Interpolation To save computations for looking-ahead and effectively reuse the attention results of the past, we propose memory interpolation that smoothly interpolates attention results from both the future and the past to provide bi-directional contextualization. Recall that in the previous iteration, we have calculated the causal context representations C \u2192 \u03c4 \u22121 of X \u03c4 \u22121 using Eq. 2, where each row is a linear s h a 1 _ b a s e 6 4 = \" t v I e H X 8 / 8 8 A 1 r 2 6 9 x 4 / 5 k y B b X 8 b C 4 t L y y u r p b X y + s b m 1 r a 5 s 9 t S I p G E N o n g Q n Z 8 r C h n E W 0 C A 0 4 7 s a Q 4 9 D l t + 6 N 6 7 r r H 8 D G 3 + C 0 H a D l S p a P z j l X 9 9 7 j J 4 J r s O 1 v a 2 l 5 Causal Self-Attention K, V ; Q Look-Ahead Attention Q K, V Interpolation Point-wise Non-linear Transform Concatenation < l a t e x i t s h a 1 _ b a s e 6 4 = \" v B y L m X a Q 1 x z x b b g Y N 6 b l 5 Q b t a c g = \" > A A A C A H i c b V C 7 T s M w F H X K q 5 R X g I G B x a J C Y q F K E A j G C h b G I l F a q Y k i x 3 F a q 4 4 d 2 Q 5 S F W X h V 1 g Y Q I i V z 2 D j b 3 D a D N B y J M t H 5 9 y r e + 8 J U 0 a V d p x v q 7 a 0 v L K 6 V l 9 v b G x u b e / Y u 3 s P S m Q S k y 4 W T M h + i B R h l J O u p p q R f i o J S k J G e u H 4 p v R 7 j 0 Q q K v i 9 n q T E T 9 C Q 0 5 h i p I 0 U 2 A d e K F i k J o n 5 8 n 4 R 5 J 5 G 2 a l b B H b T a T l T w E X i V q Q J K n Q C + 8 u L B M 4 S w j V m S K m B 6 6 T a z 5 H U F D N S N L x M k R T h M R q S g a E c J U T 5 + f S A A h 4 b J Y K x k O Z x D a f q 7 4 4 c J a r c 0 V Q m S I / U v F e K / 3 m D T M d X f k 5 5 m m n C 8 W x Q n D G o B S z T g B G V B G s 2 M Q R h S c 2 u E I + Q R F i b z B o m B H f + 5 E X y c N Z y L 1 r O 3 X m z f V 3 F U Q e H 4 A i c A B d c g j a 4 B R 3 Q B R g U 4 B m 8 g j f r y X q x 3 q 2 P W W n N q n r 2 w R 9 Y n z 9 D Y J b T < / l a t e x i t > X \u2327 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" u 0 a Z s j C 2 4 2 G 2 X G 3 a t z Y 8 C a E W + N Y = \" > A A A B / n i c b V D N S 8 M w H E 3 n 1 5 x f V f H k J T g E T 6 M V R Y 9 D L x 4 n u A 9 Y S 0 n T d A t L k 5 K k w i g F / x U v H h T x 6 t / h z f / G d O t B N x + E P N 7 7 / c j L C 1 N G l X a c b 6 u 2 s r q 2 v l H f b G x t 7 + z u 2 f s H P S U y i U k X C y b k I E S K M M p J V 1 P N y C C V B C U h I / 1 w c l v 6 / U c i F R X 8 Q U 9 T 4 i d o x G l M M d J G C u w j L x Q s U t P E X P m g C H J P o 6 w I 7 K b T c m a A y 8 S t S B N U 6 A T 2 l x c J n C W E a 8 y Q U k P X S b W f I 6 k p Z q R o e J k i K c I T N C J D Q z l K i P L z W f w C n h o l g r G Q 5 n A N Z + r v j R w l q k x o J h O k x 2 r R K 8 X / v G G m 4 2 s / p z z N N O F 4 / l C c M a g F L L u A E Z U E a z Y 1 B G F J T V a I x 0 g i r E 1 j D V O C u / j l Z d I 7 b 7 m X L e f + o t m + q e q o g 2 N w A s 6 A C 6 5 A G 9 y B D u g C D H L w D F 7 B m / V k v V j v 1 s d 8 t G Z V O 4 f g D 6 z P H 1 k w l m E = < / l a t e x i t > X \u2327 < l a t e x i t x K z p T P 8 z Z x / m l s w = \" > A A A C D X i c b V C 7 T s M w F H V 4 l v I K M L J E F C Q W q g S B Y K z o w l g k + p C a U D m u 0 1 p 1 4 s i + A V V R f o C F X 2 F h A C F W d j b + B q f N A C 1 H s n x 0 z f v q V R M R L c w j q k X 4 k H E A k Y w a K l n H r q + 4 H 0 1 D v W X 1 r O 7 1 O U 0 A C y l e M h 6 q Q s 4 O X G y n l m x q / Y E 1 j x x C l J B B R o 9 8 8 v t C 5 K E N A L C s V J d x 4 7 B S 7 E E R j j N y m 6 i a I z J C A 9 o V 9 M I h 1 R 5 6 e S a z D r S S t 8 K h N Q v A m u i / u 5 I c a j y h X V l i G G o Z r 1 c / M / r J h B c e i m L 4 g R o R K a D g o R b I K w 8 G q v P J C X A x 5 p g I p n e 1 S J D L D E B H W B Z h + D M n j x P W q d V 5 7 x q 3 5 x V a l d F H C W 0 j w 7 Q M X L Q B a q h a 9 R A T U T Q I 3 p G r + j N e D J e j H f j Y 1 q 6 Y B Q 9 e + g P j M 8 f B t O c y A = = < / l a t e x i t > C \u2327 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 0 v q M v b Q g X j h N 1 U G p R 5 1 S q H R N k g w = \" > A A A C D n i c b V C 7 T s M w F H V 4 l v I q M L J E V J V Y q B I E g r G i C 2 O R 6 E N q Q u S 4 b m v V s S P 7 B l R F + Q I W f o W F A Y R Y m d n 4 G 5 y 2 A 7 Q c y f L R O f f q 3 n v C m D M N j v N t L S 2 v r K 6 t F z a K m 1 v b O 7 u l v f 2 W l o k i t E k k l 6 o T Y k 0 5 E 7 Q J D D j t x I r i K O S 0 H Y 7 q u d + + p 0 o z K W 5 h H F M / w g P B + o x g M F J Q q n i h 5 D 0 9 j s y X 1 r O 7 1 F N s M A S s l H z I g t Q D n J y 4 W V A q O 1 V n A n u R u D N S R j M 0 g t K X 1 5 M k i a g A w r H W X d e J w U + x A k Y 4 z Y p e o m m M y Q g P a N d Q g S O q / X R y T m Z X j N K z + 1 K Z J 8 C e q L 8 7 U h z p f G N T G W E Y 6 n k v F / / z u g n 0 L / 2 U i T g B K s h 0 U D / h N k g 7 z 8 b u M U U J 8 L E h m C h m d r X J E C t M w C R Y N C G 4 8 y c v k t Z p 1 T 2 v O j d n 5 d r V L I 4 C O k R H 6 B i 5 6 A L V 0 D V q o C Y i 6 B E 9 o 1 f 0 Z j 1 Z L 9 a 7 9 T E t X b J m P Q f o D 6 z P H + n t n U U = < / l a t e x i t > C ! \u2327 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" A M W y f i k y h f Q L f s N M Q n S C H v u M o 9 A = \" > A A A C D H i c b V C 7 T s M w F H V 4 l v I q M L J Y V E h M V Y J A M F Z 0 Y S w S f U h N q B z H a a 0 6 c W T f g K o o H 8 D C r 7 A w g B A Z X V t v b R R 3 t z a 3 t m t 7 O 2 3 t U w V Z S 0 q h V R d n 2 g m e M x a w E G w b q I Y i X z B O v 6 o U e i d e 6 Y 0 l / E t j B P m R W Q Q 8 5 B T A o b q V 6 q u L 0 W g x 5 H 5 s k Z + l 7 m K D 4 Z A l J I P e T 9 z g a S 5 c d k 1 e 1 J 4 E T g z U E W z a v Y r X 2 4 g a R q x G K g g W v c c O w E v I w o 4 F S w v u 6 l m C a E j M m A 9 A 2 M S M e 1 l k 2 N y f G y Y A I d S m R c D n r C / O z I S 6 W J f 4 4 w I D P W 8 V p D / a b 0 U w k s v 4 3 G S A o v p d F C Y C g w S F 8 n g g C t G Q Y w N I F R x s y u m Q 6 I I B Z N f 2 Y T g z J + 8 C N q n N e e 8 Z t + c V e t X s z h K 6 B A d o R P k o A t U R 9 e o i V q I o k f 0 j F 7 R m / V k v V j v 1 s f U u m T N e g 7 Q n 7 I + f w D z u Z z T < / l a t e x i t > C ! \u2327 Figure 4: The architecture of LaMemo with look-ahead attention and memory interpolation that refresh the memory dynamically with both the current contexts and the long-term history. combination of the weighted token representations of the previous tokens. In Sec. 3.1, we describe the look-ahead attention which enables X \u03c4 \u22121 to attend to the contexts on their right and computes C \u2190 \u03c4 \u22121 using Eq. 9. Here, we formulate the memory interpolation as the interpolation between the old representations C \u2192 \u03c4 \u22121 and the new ones C \u2190 \u03c4 \u22121 with a coefficient vector \u03b1 \u03c4 \u22121 \u2208 R M controlling the memorization of the past activations: C \u2194 \u03c4 \u22121 = Mem-Interp(C \u2192 \u03c4 \u22121 , C \u2190 \u03c4 \u22121 , \u03b1 \u03c4 \u22121 ) = \u03b1 \u03c4 \u22121 sg(C \u2192 \u03c4 \u22121 ) + (1 \u2212 \u03b1 \u03c4 \u22121 )C \u2190 \u03c4 \u22121 . ( 10 ) The resulted C \u2194 \u03c4 \u22121 which attend to contexts from both directions, are further fed to the non-linear transformation defined in Eq. 3 to update representations in higher layers. For \u03b1 \u03c4 \u22121 , we define it to be the sum of the normalized attention weights on the previous tokens when calculating C \u2192 \u03c4 \u22121 (Eq. 2): \u03b1 \u03c4 \u22121 = sg(s \u2192 \u03c4 \u22121 ) sg(s \u2192 \u03c4 \u22121 ) + s \u2190 \u03c4 \u22121 + \u03b5 , (11) where s \u2192 \u03c4 \u22121 is the sum of the unnormalized attention score of C \u2192 \u03c4 \u22121 , which is the denominator of the softmax in Eq. 2. Similarly, s \u2190 \u03c4 \u22121 is the denominator of the softmax in Eq. 9. \u03b5 is a small value to prevent zero division error in practice. Then Eq. 10 can be derived into a form that resembles the bi-directional attention with the queries attending to positions on both sides 4 (Appendix A). Figure 4 shows the architecture of LaMemo. Note that the difference between the hidden state reuse in the recurrence memory and our memory interpolation is that they simply reuse the static representations to extend the contexts for attention while we update the memory representations by aggregating weighted attention sum of the history without the need to recompute them. Disentangled Relative Positional Encodings As the look-ahead attention allows the memory to attend to future tokens on its right, we need a relative positional encoding scheme that can generalize to this setting. We start by considering the relative positional encoding in Transformer-XL, as described by Eq. 6. When the i-th query vector attending to a position However, this approach solely relies on the fixed sinusoid encodings to represent the relative distance and the attention direction. We argue that disentangling them is more effective in capturing these two types of temporal biases and also mitigates the numerical unstability issue. Specifically, we propose to learn two direction-aware global position biases to parameterize the sign and query R with the absolute value of the relative distance: j = i + \u2206 > i, we have R i\u2212j = R \u2212\u2206 . A dis i,j = X \u22a4 i W \u22a4 q W E k X j + X \u22a4 i W \u22a4 q W R k R |i\u2212j| + u \u22a4 W E k X j + v \u22a4 i\u2212j W R k R |i\u2212j| , (12) where v i\u2212j = v + if i \u2265 j else v \u2212 . The global positional bias now explicitly separates the contributions of sgn(i \u2212 j) and |i \u2212 j|, which can better generalize to long distance in both forward and backward directions. To illustrate the numerical unstability caused by adapting Eq. 6 to j > i, we derive the variance of the dot product x T R i\u2212j where x is a random vector. We show that the variance undergoes an oscillation and cannot be properly bounded everywhere when i shifts from i \u2265 j to i < j. Detailed analysis are presented in Appendix B. Experiments We evaluate LaMemo on both word-level and character-level language modeling tasks and compare with existing Transformer baselines augmented with different types of memory. Datasets and Metrics For word-level language modeling task, we consider Wikitext-103 (Merity et al., 2017) , which is the most widely used word-level language modeling benchmark. It contains 103 million tokens for training from 28 thousand wikipedia articles, with an average length of 3.6 thousand tokens per article and a vocabulary size around 260K. We report perplexity (ppl) on the dev and test set. We also evaluate on two character-level language modeling benchmarks enwik8 and text8 (Mahoney, 2011). Both datasets contain 100 million Wikipedia characters. While enwik8 is unprocessed, text8 is preprocessed by case lowering and filtering to include only 26 letters from a to z and space. On both datasets, we report bit per character (bpc) on the dev and test set. Baselines To directly compare with different types of memory, we consider Transformer-XL and its variations with the same model architecture but different memory mechanism. Transformer+RPE is the vanilla Transformer (Vaswani et al., 2017) that uses relative positional encodings from Dai et al. (2019) but does not extend the context with additional memory. Transformer-XL (Dai et al., 2019 ) is a Transformer model equipped with relative positional encodings and a recurrence memory comprised of hidden states computed in previous time steps to extend the context length of the attention. Compressive Transformer (Rae et al., 2020 ) extends Transformer-XL with an external compressive memory that stores compressed hidden states at the temporal level using convolutional networks. \u221e-former (Martins et al., 2021) uses continuous space attention to attend over the external memory which consists of continuous signals. They also updated the external memory with recent hidden states to enable unbounded memory capacity. Table 1 : Word-level language modeling results on Wikitext-103. We report ppl (perplexity) on dev and test set. We also report the number of parameters, memory size, external memory size, and the number of FLOPS (floating-point operations) for computing one step prediction on average. Implementation Details We follow the standard architecture of the Transformer-XL (Dai et al., 2019) that has different configurations for different tasks. Specifically, on Wikitext-103, we use a 16-layer Transformer with 10 attention heads and head dimension 41 equipped with adaptive embeddings (Baevski and Auli, 2019) . We control the target sequence length to be 150 and the memory length 150 for all models following the setting of Dai et al. (2019) . For the Compressive Transformer and \u221e-former, we additionally use an external memory of size 150 following the setting of Martins et al. (2021) . 5 On the text8 and enwik8 datasets, we use a 12-layer Transformer with 8 heads and head dimension 64. The length of the target sequence and the recurrence memory are both set to 512. In the main results we use the identical evaluation setting to the training phase on all datasets and do not use a longer memory. We use the Pytorch framework (Paszke et al., 2019) and Apex for mixed-precision training. In practice, we found that calculating the exponentials ( \u00a73.2) may lead to numerical overflow in mixed-precision mode, so we compute the logarithm of the exponential sum using logsumexp and logaddexp operator. Further details of the dataset and the hyperparameter settings are described in the Appendix C. Main Results We show the results of word-level language modeling benchmark Wikitext-103 in  compared to the compressive memory and the unbounded memory that take longer contexts into account, LaMemo still achieves lower perplexity. This indicates that the look-ahead memory allows the language model to exploit the recent contexts to gain performance, while simply increasing the context length yields marginal improvement. This is in accordance with previous findings of how language models utilize contexts (Khandelwal et al., 2018; Sun et al., 2021) . In terms of the parameters, LaMemo has the same number of parameters as the Transformer-XL while other baselines use additional parameters in CNN to compress or smooth the hidden states. Lastly, we show the number of FLOPS necessary for computing one step prediction. \u221e-former has the highest number of FLOPS for resampling enough points from the continuous signal to update the memory using smoothing techniques. LaMemo also incurs additional computations to re-contextualize the memory under the current context. Note that although the Compressive Transformer has lower number of FLOPS than LaMemo, it has an external memory that consumes more GPU memory. We also present the results of character-level language modeling on text8 and enwik8 datasets in Table 2 . We observe similar trends as the results on the word-level benchmark, where LaMemo outperforms Transformer-XL by 0.04 on text8 and 0.02 on enwik8 with the same context length. Additionally, we observe that all models exhibit overfitting on text8, which might be caused by the extremely small vocabulary size of the dataset. Ablation Study We conduct ablation studies on Wikitext-103 to examine the effects of the proposed techniques, i.e., look-ahead attention, memory interpolation, and disentangled relative positional encodings. We use the same model achitecture and the same target and memory length as the main results. We first study three configurations, including (1) using the Full model setting, (2) ablating the memory interpolation module (w/o mem interp), i.e., set the memorizing coeffecient \u03b1 \u03c4 \u22121 = 0, and (3) ablating the look-ahead attention (w/o look-ahead), i.e., only use the causal context representations C \u2192 \u03c4 \u22121 in each layer. As shown in the First three rows in Table 3 , both the memory interpolation and the look-ahead attention are indispensible for achieving the best performance. Additionaly, we found that cancelling out memory interpolation leads to a worse performance, which indicates that the distant past still provides additional information beyond the current context. The second study targets at studying different encoding schemes. We substitute our encodings with the RPE of Transformer-XL Dai et al. (2019) and run multiple experiments with 3 different random seeds, but all the models fail to converge. We plot the training curves using two encodings in Figure 8 in Appendix B, where we observe that our disentangled RPE is more stable during training and achieves lower perplexity. Extrapolating to Longer Contexts In this section, we extrapolate the models to longer contexts during inference to study the effect of dynamic contextualization to the distant past.  We fix the length of the target sequence to 64 and extrapolate the trained models to longer memory length 64 \u00d7 m during inference, where m = 1, \u2022 \u2022 \u2022 , 10. We compare the perplexity of LaMemo and Transformer-XL trained on Wikitext-103 when augmented by a memory with different length. As shown in Figure 5 , LaMemo consistently achieves lower perplexity than Transformer-XL when extraploating to longer contexts, while the performance of both models saturate when m is over 7. Additionally, we observe that the gap of perplexity between the two models increases when taking longer contexts into account. This demonstrates the effectiveness of dynamically refreshing the distant memory representations under the current context. Attention Analysis In this section, we analyze the attention distribution of LaMemo to validate the effectiveness of utilizing bi-directional contexts with look-ahead attention. We first visualize the memorizing coefficient \u03b1 which stands for the portion of the past activations in the current memory representations. As show in Figure 6 , we plot \u03b1 in different layers as a function of the memory index averaged on 100 text segments. 6 We observe that in lower layers the memory mainly attends to the past (\u03b1 \u2248 1.0). We conjecture that long-term bi-directionality is not necessary for low-level representations such as lexical features. In higher layers, the memory substantially utilizes the future contents to refresh the high-level representations, especially for the old memory state with a small memory index. Next, we visualize the attention weight distribution on the context tokens when predicting each target token in Figure 1 . For every token, we take the maximal attention weight in each interval of 5 tokens on its left and scale to a context length of 100. The result indicates that LaMemo learns better memory represetations by attending to the right-side tokens, which increases the memory utilization when predicting the target token. Case Study We present the generated texts of LaMemo and Transformer-XL trained on Wikitext-103 in Appendix D. Both models maintain a memory size of 512, and we seed them with the same context randomly sampled from the test set and generate 256 tokens using top-p sampling (Holtzman et al., 2020) with p = 0.95. Related Work The Transformer (Vaswani et al., 2017) , with its pair-wise modeling ability of the input, becomes prevailing for sequence modeling, especially long sequence processing tasks, such as long text generation (Tan et al., 2021; Ji and Huang, 2021) , long document QA (Beltagy et al., 2020; Ainslie et al., 2020) , language modeling (Dai et al., 2019; Rae et al., 2020) , video processing (Wu et al., 2019) , and etc. Specifically, language modeling (Merity et al., 2017) which requires processing documents with thousands of tokens has become a natural testbed for benchmarking this long-term processing ability. However, due to the quadratic time and space complexity of self-attention, scaling to inputs with thousands of tokens is computationally prohibitive. One line of work investigated the linear-time attention mechanism to mitigate the scability issue of Transformer. Linformer (Wang et al., 2020) projects the inputs to lower dimension in length and approximates the full attention with a low-rank factorization. Linear Transformer (Katharopoulos et al., 2020) regards the self-attention as a kernel function and uses a linear dot-product as a substitute. Choromanski et al. ( 2021 ) and Peng et al. (2021) proposed to approximate the softmax more precisely with the expectation of the dot-product of random features. Although achieving substantial improvements on benchmarks designated for long inputs (Tay et al., 2021) . These methods, however, focus on approximating the full attention with low-rank factorizations or kernel functions, which compromise the expressiveness and robustness of the original softmax attention, are reported to be inferior to the simple local attentions on real world language processing tasks (Xiong et al., 2021) . Our work falls in another line, which augments the Transformer with a parametrized memory to store critical history information. Memoryaugmented networks (Graves et al., 2014; Weston et al., 2015; Sukhbaatar et al., 2015) have been studied in the context of recurrent neural networks for a long time, but are mostly restricted to small and synthetic datasets. With the rapid development of Transformer, various works start to adapt memories to this architecture. Dai et al. (2019) first extended Transformer with a recurrence memory that caches hidden states computed in previous steps for the target tokens to attend to. Rae et al. (2020) further extended the context with an external memory that stores compressed hidden states at the temporal level. Martins et al. (2021) used continuous space attention to attend over the old history and updated the memory with recent hidden states to enable unbounded memory capacity. Wu et al. (2021) proposed to use the encoder-decoder architecture to encode the memory states with previous text segments and pass this memory to future time steps. Instead of using a fixed-size attention span for different layers, Sukhbaatar et al. (2019) and Correia et al. (2019) proposed to learn dynamic attention spans for dif-ferent attention heads, which greatly reduced the computations. These works focused on enabling the Transformer to access contents in long distance, but did not consider to learn better memory representations by refreshing the old memory under the current context. Our work is orthogonal to learning adaptive attention spans and can be combined with this technique to reduce the complexity. Conclusion We present LaMemo, a memory mechanism that allows the memory states to incrementally attend to the right-side tokens and interpolates with the old memory states on the left side, which enables the memory to interact with bi-directional contexts with a complexity linear in memory length. Experiments on three language modeling datasets demonstrate the superiority of LaMemo over baselines with various types of memory mechanisms. We also found that LaMemo increases the utilization of older memory states when predicting the target tokens, and yields a higher performance boost when extrapolating to longer memory length, which indicates the effectiveness of recontextualizing the memory under the current context. A Derivation of Memory Interpolation We derive Eq. 10 into the form of standard selfattention in the following: C \u2194 \u03c4 \u22121 = \u03b1 \u03c4 \u22121 sg(C \u2192 \u03c4 \u22121 ) + (1 \u2212 \u03b1 \u03c4 \u22121 )C \u2190 \u03c4 \u22121 . We consider the i-th row of C \u2194 \u03c4 \u22121 , denoted as c \u2194 i . We omit the stop-grad operation sg(\u2022) and substitute \u03b1 with the result from Eq. 11: c \u2194 i = \u03b1 i c \u2192 i + (1 \u2212 \u03b1 i )c \u2190 i = s \u2192 i s \u2192 i + s \u2190 i c \u2192 i + s \u2190 i s \u2192 i + s \u2190 i c \u2190 i , where s \u2192 i , s \u2190 i is the denominator of the softmax when computing c \u2192 i , c \u2190 i respectively: s \u2192 i = j\u2264i exp q \u2032 \u22a4 i k \u2032 j \u221a d = j\u2264i sim(q \u2032 i , k \u2032 j ), s \u2190 i = j>i exp q \u22a4 i k j \u221a d = j>i sim(q i , k j ), where (q \u2032 i , k \u2032 j ) and (q i , k j ) are two sets of querykey vectors computed in the previous and this text segment respectively for the same position pair (i, j) . Then we have: c \u2194 i = j\u2264i sim(q \u2032 i , k \u2032 j ) j\u2264i sim(q \u2032 i , k \u2032 j ) + j>i sim(q i , k j ) c \u2192 i + j>i sim(q i , k j ) j\u2264i sim(q \u2032 i , k \u2032 j ) + j>i sim(q i , k j ) c \u2190 i = j\u2264i sim(q \u2032 i , k \u2032 j )v \u2032 j + j>i sim(q i , k j )v j j\u2264i sim(q \u2032 i , k \u2032 j ) + j>i sim(q i , k j ) = j \u03b2 j \u1e7dj , where j \u03b2 j = 1. Finally, we derive c \u2194 i as the weighted sum of the value vectors \u1e7dj from both the past (j \u2264 i) and the future (j > i) of the position i. B Unstability Analysis of the RPE in Transformer-XL We conjecture that the unstability of Eq. 6 stems from the terms involving the dot-product of R i\u2212j and another vector. So we start by considering the variance of x \u22a4 R i\u2212j where x \u2208 R d is a random vector. Without loss of generality, we assume that x has zero mean and a variance of \u03c3: E(x k ) = 0, \u2200k \u2208 [1, \u2022 \u2022 \u2022 , d] Var(x k ) = \u03c3 k,k , \u2200k \u2208 [1, \u2022 \u2022 \u2022 , d] Cov(x k , x l ) = \u03c3 k,l , \u2200l \u0338 = k \u2208 [1, \u2022 \u2022 \u2022 , d] Let i \u2212 j = \u2206. According to Vaswani et al. (2017) , R \u2206 takes the following form: R \u2206 =[sin(\u03c9 1 \u2206), cos(\u03c9 1 \u2206), \u2022 \u2022 \u2022 , sin(\u03c9 d/2 \u2206), cos(\u03c9 d/2 \u2206)], where w k = 10000 \u22122k/d . Then the dot-product x \u22a4 R \u2206 can be derived into the linear combination of sine and cosine functions: x \u22a4 R \u2206 = d/2 k=1 x 2k\u22121 sin(\u03c9 k \u2206) + x 2k cos(\u03c9 k \u2206), where we can easily derive that E(x \u22a4 R \u2206 ) = 0. According to the variance-expectation formula: Var(x) = E[x 2 ] \u2212 E[x] 2 , we can simplify the variance Var(x \u22a4 R \u2206 ) in the following: Var(x \u22a4 R \u2206 ) = E d/2 k=1 x 2k\u22121 sin(\u03c9 k \u2206) + x 2k cos(\u03c9 k \u2206) 2 = d/2 k=1 E[x 2 2k\u22121 ] sin 2 (\u03c9 k \u2206) + E[x 2 2k ] cos 2 (\u03c9 k \u2206) + 2 d/2 k=1 d/2 l=1,l\u0338 =k E[x 2k\u22121 x 2l ] sin(\u03c9 k \u2206) cos(\u03c9 l \u2206). We further simplify the above equation by assuming that all the elements have the same variance \u03c3 s , and all pairs of distinct elements have the same covariance \u03c3 c : Var(x \u22a4 R \u2206 ) = d/2 k=1 \u03c3 s [sin 2 (\u03c9 k \u2206) + cos 2 (\u03c9 k \u2206)] + 2 d/2 k=1 d/2 l=1,l\u0338 =k \u03c3 c sin(\u03c9 k \u2206) cos(\u03c9 l \u2206) = d 2 \u03c3 s + 2\u03c3 c g(\u2206), where g(x) = d/2 k=1 d/2 l=1,l\u0338 =k sin(\u03c9 k x) cos(\u03c9 l x) is an odd function. We consider the value of g(x) when x \u2248 0. Figure 7 : The plot of g(x) when d = 64. We see that g(x) is symmetric with respect to the origin. The value of g(x) when x approaches zero from the left and right diverge greatly. Since sin(\u03c9 k x) \u2248 \u03c9 k x, cos(\u03c9 k x) \u2248 1, we have: g(x) \u2248 d/2 k=1 d/2 l=1 \u03c9 k x = d 2 d/2 k=1 w k x = xd 2 d/2 k=1 1 10000 2/d k \u2248 d 2((10 8 ) 1/d \u2212 1) \u2022 x = \u03b3 d \u2022 x. Since a x \u2248 1 + x ln a when x \u2248 0, we derive that \u03b3 d \u2248 d 2 2 ln 10 8 with the grow of d. This causes g(x) to have a very steep slope near 0. Since g(x) is an odd function, the value of g(\u2206) and g(\u2212\u2206) will have a huge gap (\u2206 is a small positive value). To validate this, we plot the function of g(x) when d = 64 in Figure 7 . Overall, the variance of x \u22a4 R \u2206 is composed of two terms, the first being \u03c3 s multiplied by a constant factor d/2, and the second being \u03c3 c multiplied by g(\u2206). Note that \u03c3 s is strictly positive, while \u03c3 c does not have this restriction. Due the asymptotic behavior of g(\u2206) near 0, i.e., O(d 2 \u2206), we cannot find a proper \u03c3 c that makes Var(x \u22a4 R \u2206 ) bounded by O(d\u03c3 s ) for every \u2206 that takes its value from both the positive and negative integers. Finally, we plot the training curves of the two models using the RPE in Transformer-XL (xl-rpe) and our disentangled RPE (dis-rpe) in Figure 8 where we observed that the xl-rpe suffers from numerical unstability during training.   text8 dataset contains the first 100 million bytes of the clean text of Wikipedia that retains only regular articles and image captions. All the letters are converted into lower case, and only letters in the 27 character alphabet, namely letters a-z and nonconsecutive spaces, are preserved. This dataset is licensed under the CC BY-SA License. The statistics of the three datasets is shown in Table 4 . C.2 Model Configurations We follow the base model configuration of Dai et al. (2019) . On Wikitext-103, we use the Transformer model with 16 layers, 10 attention heads with a head dimension of 41. The inner dimension size of the feedforward layer is 2100. We use a dropout rate of 0.1 and no attention dropout. To cope with the large vocabulary, we use the adaptive embeddings (Baevski and Auli, 2019) . We set the memory length to 150 and the target sequence length to 150 as well. On text8 and enwik8 datasets, we use the Transformer model with 12 layers, 8 attention heads with a head dimension of 64. The inner dimension size of the feedforward layer is 2048. We use a dropout rate of 0.1 and no attention dropout. We set the memory length to 512 and the target length to 512. Specifically, our LaMemo uses the disentangled relative positional encodings described in Sec. 3.3. The look-ahead attention shares the query, key and value projection matrices with those in the causal attention. C.3 Training Settings We trained the models using Adam (Kingma and Ba, 2015) optimizer, with no warmup. We used a learning rate of 2.5 \u00d7 10 \u22124 which decayed to 0 at the end of training with a cosine schedule. On Wikitext-103, we trained the model with 250K steps using a batch size of 64. On enwik8 and text8, we trained the model with 100K 7 steps using a batch size of 40. We conducted our experiments on 2 Tesla V100. C.4 Hyperparameters We present the hyperparameter search space in Table 5. The number of hyperparameter search trials was 10. We adopted a manual search to select the hyperparameters, and the selection criterion was ppl/bpc on the dev set. We did not use early stopping during training. D Generated Examples In this section, we present the examples generated by LaMemo and Transformer-XL trained on the Wikitext-103 dataset. Both models maintain a memory with a length of 512. We randomly select a piece of text from the test set as the context 7 We used a smaller number of training steps compared to Dai et al. (2019) and allow both models to generate 256 tokens following the context. We use top-p sampling with p = 0.95 and detokenize the context and the generated texts to facilitate reading. We present the exmples in Table 6 and 7 . We present our major findings below: \u2022 Both models are able to hallucinate imaginary contents fairly relevant to the limited contexts given as prompts. \u2022 Transformer-XL sometimes generates topicirrelevant contents without further elaboration (marked by underline), while LaMemo stays on topic more closely during the course of generation. \u2022 Transformer-XL suffers more sever repetition issues (marked in boldface) than LaMemo both lexically and semantically. Context: = Shackleton ( crater ) = Shackleton is an impact crater that lies at the south pole of the Moon. The peaks along the crater's rim are exposed to almost continual sunlight, while the interior is perpetually in shadow (a Crater of eternal darkness). The low-temperature interior of this crater functions as a cold trap that may capture and freeze volatiles shed during comet impacts on the Moon. Measurements by the Lunar Prospector spacecraft showed higher than normal amounts of hydrogen within the crater, which may indicate the presence of water ice. The crater is named after Antarctic explorer Ernest Shackleton. = = Description = = The rotational axis of the Moon lies within Shackleton, only a few kilometers from its center. The crater is 21 km in diameter and 4.2 km deep. From the Earth, it is viewed edge-on in a region of rough, cratered terrain. It is located within the South Pole-Aitken basin on a massif. The rim is slightly raised about the surrounding surface and it has an outer rampart that has been only lightly impacted. No significant craters intersect the rim, and it is sloped about 1.5 \u00b0toward the direction 50 -90 \u00b0from the Earth. The age of the crater is about 3.6 billion years and it has been in the proximity of the south lunar pole for at least the last two billion years. Because the orbit of the Moon is tilted only 5 \u00b0from the ecliptic, the interior of this crater lies in perpetual darkness. Estimates of the area in permanent shadow were obtained from Earth-based radar studies. Peaks along the rim of the crater are almost continually illuminated by sunlight , spending about 80 -90 % of each lunar orbit exposed to the Sun. Continuously illuminated mountains have been termed peaks of eternal light and have been predicted to exist since the 1900s. The shadowed portion of the crater was imaged with the Terrain Camera of the Japanese SELENE spacecraft using the illumination of sunlight reflected off the rim. The interior of the crater consists of a symmetrical 30 \u00b0slope that leads down to a 6.6 km diameter floor. The handful of craters along the interior span no more than a few LaMemo Generation: kilometres of plains. A slight rotation is observed in the crater that now bears significant elements like snow, ice, and water ice. In other areas of the crater there are indications that the nature of the alpine ice layer provides some protection to the craters by increasing their brightness. The craters seem to be cut by an ice thickness of 10 cm (4 in). Images show their prominence, varying from a visible plateau to a multi-ring topography, indicating that they are present (a major component) in the craters. Their interior has a considerable surface area of dry ice. As seen from Earth, the initial craters are marked by slow-moving metal-bearing mudstone-covered rock that has a surface of substantial amounts of coal, calcium, lead and sodium alloy, carbon monoxide, nitrogen oxides, and silicon dioxide. All are developed by processes such as heat transfer by the advancing radiation pressure. The least dense feature along the surface of the Moon is the large resurfacing event. There is no view that these rocks remain melting to about 90 \u00b0C (150 \u00b0F), but that their surface shine is dark and their outer layers are light, reflecting vertical oil. This late flank and early 1970s discovery was announced in 1982, as a substantial amount of ice. A parallel lava Transformer-XL Generation: kilometres. During the hundreds of years after its formation, the crater now has significant seismic activity in the vicinity. Most craters are dome-shaped and are usually constructed of sedimentary materials that have been deposited during alpine ice times. The upper surface of the crater is covered with higher layers of rock that are cut by an extensive thickness of debris. The debris reaches the top of a mountain and comes in contact with visible ground planes. People are often observed wearing headgear of degrading materials such as clothing and boots, their shoes or hats, or even working on the surface. Below the crater. As the crater faces the crater it has thick, thin pipes or scarps. A total of more than 200 caves have been excavated, down to some 40 m by 20 m. This exceeding the margin of the crater where it actually passes through is considered to be very high. Other geologic features by the advancing magnetic field have been reported from the crater. However, in 1992, scientists announced they would study this area again. The crater was once a common feature of the Post Lunar System. Its medieval boundaries were not fixed in the orbital plane of Mercury. An individual crater had been called \" Discovery crater \" and one referred to as \" Bear crater \", although it is likely that an additional crater was called Acknowledgments This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005. This work was also sponsored by Tsinghua-Toyota Joint Research Fund. Context: Nero was not expected to become Emperor because his maternal uncle, Caligula, had begun his reign at the age of 24 with enough time to produce his own heir. Nero 's mother, Agrippina, lost favour with Caligula and was exiled in 39 after her husband 's death. Caligula seized Nero 's inheritance and sent him to be brought up by his less wealthy aunt, Domitia <unk>, who was the mother of Valeria <unk>, Claudius 's third wife. Caligula, his wife <unk> and their infant daughter Julia Drusilla were murdered on 24 January 41. These events led Claudius, Caligula 's uncle, to become emperor. Claudius allowed Agrippina to return from exile. Claudius had married twice before marrying Valeria <unk>. His previous marriages produced three children including a son, Drusus, who died at a young age. He had two children with <unk> -Claudia Octavia (born 40) and Britannicus (born 41). <unk> was executed by Claudius in the year 48. In 49 AD , Claudius married a fourth time, to Nero 's mother Agrippina, despite her being his niece. To aid Claudius politically, young Nero was adopted in 50 and took the name Nero Claudius Caesar Drusus Germanicus (see adoption in Rome). Nero was older than his stepbrother Britannicus, and thus became heir to the throne. Nero was proclaimed an adult in 51 at the age of 14. He was appointed proconsul, entered and first addressed the Senate, made joint public appearances with Claudius, and was featured in coinage. In 53, he married his stepsister Claudia Octavia. = = Emperor (54 -68 AD) = = = = = Early rule = = = Claudius died in 54 and Nero, taking the name Nero Claudius Caesar Augustus Germanicus, was established as Emperor. Though accounts vary, many ancient historians state Agrippina poisoned Claudius. According to Pliny the Elder, she used poison mushrooms. It is not known how much Nero knew or if he was even involved in the death of Claudius. Suetonius wrote \" ... for even if he was not the instigator of the emperor 's death , he was at least privy to it, as he openly admitted; for he used afterwards to laud mushrooms, the vehicle in which the poison was administered to Claudius, as ' the food of the gods , ' as the Greek proverb has it. At any rate, after Claudius' death he vented on him every kind of insult, in act and word, charging him now with folly and now with cruelty; for it was a favourite joke of his to say that Claudius had ceased ' to play the fool ' among mortals, lengthening the first syllable of the word <unk>, and he disregarded many of his decrees and acts as the work of a madman and a <unk>. Finally, he neglected to enclose the place where his body was burned except with a low and mean wall. \" According to Suetonius; Nero became Emperor at the age of 17 when the news of Claudius' death was made known , making him the youngest emperor at that time Although, what Suetonius may have meant is that he was LaMemo Generation: killed as a result of the subsequent anti-Caligula propaganda. At some time after 47 AD, Nero retired to his villa at Baiae. The combination of a curse that led to the death of Agrippa, and a duel between Claudius and his own son Drusus, may have encouraged the young Emperor to fire his own stela. = = = = = Germanicus = = = = = After becoming emperor, Nero received letters from Suetonius, which finally followed him on 28 May 44 AD with a letter to Claudius, completing the Emperor 's plot. The letters were printed in Latin. Claudius' plot was offered to Caligula by Agrippina the Elder, a woman from Kent, but she declined. Further letters from Gaius and Nero are known from the reigns of Tiberius and Caligul , though none of them mention Caligula. Germanicus' conspiracy had developed into a protracted affair. His wife Antonia, his sister Agrippina 's sister and grandmother, had refused to accept him because she believed they were connected for communication, but the Via Flavia was left to her husband and Agrippina the Elder, whom she had defeated in her husband 's palace. Both had been sent through Gibraltar to compete for power. Gaius and Nero were outraged that Agrippina was supposed to be ready to support Caligula; Suetonius says that Nero insisted on making Transformer-XL Generation: killed as a result of poison and was deemed a good comedian. At now and then he had adopted Nero as his heir. Although Nero 's rule was as close as he was to his father to his daughter Agrippina, he was not to physically exhibit any ill degree of higher kingship but to deny it. The Augustan History describes him as having been strongly willed, possessing an excellent hand and often claiming the title \" Augustus Caesar \", and often referred to him as Caesar 's \" paternal heir \". The Augustan History, however, asserts that he was no longer in China, and therefore he was raised as a Roman Hercules rather than a Roman citizen. Claudius Claudius was added as a junior emperor in 53 AD; he was crowned emperor in 61 AD.",
    "abstract": "Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-todate information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) 1 that enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory. 2 * Corresponding author 1 We are also inspired by the French word \"La M\u00e9moire\", meaning \"the memory\". 2 Source code available at https://github.com/ thu-coai/LaMemo.",
    "countries": [
        "China"
    ],
    "languages": [
        "Latin",
        "French"
    ],
    "numcitedby": "1",
    "year": "2022",
    "month": "July",
    "title": "{L}a{M}emo: Language Modeling with Look-Ahead Memory"
}