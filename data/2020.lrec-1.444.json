{
    "article": "A large number of significant assets are available online in English, which is frequently translated into native languages to ease the information sharing among local people who are not much familiar with English. However, manual translation is a very tedious, costly, and time-taking process. To this end, machine translation is an effective approach to convert text to a different language without any human involvement. Neural machine translation (NMT) is one of the most proficient translation techniques amongst all existing machine translation systems. In this paper, we have applied NMT on two of the most morphological rich Indian languages, i.e. English-Tamil and English-Malayalam. We proposed a novel NMT model using Multihead self-attention along with pre-trained Byte-Pair-Encoded (BPE) and MultiBPE embeddings to develop an efficient translation system that overcomes the OOV (Out Of Vocabulary) problem for low resourced morphological rich Indian languages which do not have much translation available online. We also collected corpus from different sources, addressed the issues with these publicly available data and refined them for further uses. We used the BLEU score for evaluating our system performance. Experimental results and survey confirmed that our proposed translator (24.34 and 9.78 BLEU score) outperforms Google translator (9.40 and 5.94 BLEU score) respectively. Introduction Many populated countries such as India and China have several languages which change region by region. for example, India has 23 constitutionally recognized official languages (e.g., Hindi, Malayalam, Telugu, Tamil, and Punjabi) and numerous unofficial local languages. Not only big countries, even small countries also rich in language diversity. There are 851 languages spoken in Papua New Guinea, which is one of the smallest populated regions. In India, the population is about three billion, but only about 10% of them can speak English 1 . Some studies say that out of those 10% English speakers only 2% can talk, write, and examine English well, and rest 8% can merely recognize simple English and talk with a variety of accents. Thinking about a large number of valuable sources is available on the web in English and most people in India can not understand it well, it becomes important to translate such content into neighborhood languages to facilitate people. Sharing pieces of information between human beings is important not only for business purposes but also for sharing their emotions, reviews, and acts. For this, translation plays an essential role in minimizing the communication hole between different peoples. considering the vast amount of text, it is not viable to translate them manually. Hence, it becomes crucial to translate text from one language (say, English) to other languages (say, Tamil, Malayalam) automatically. This technique is also referred to as machine translation. English to Indian language translation poses the challenge of morphological and structural divergence. For instance, (i) the number of parallel corpora and (ii) differences between languages, mainly the morphological richness and variation in word order due to syntactical divergence. Indian languages (IL) suffers from both of these problems, 1 https://www.bbc.com/news/magazine-20500312 especially when they are being translated from English. Moreover, Indian languages such as Malayalam and Tamil differ not only in word order but are also more agglutinative as compared to English which is fusional. For instance, English has Subject-Verb-Object (SVO) whereas Tamil and Malayalam have Subject-Object-Verb (SOV). While syntactic differences contribute to difficulties of translation models, morphological differences contribute to data sparsity. We attempt to overcome both issues in this paper. There are various papers on machine translation, but apart from foreign languages most of the works on Indian languages are limited to Hindi and on conventional machine translation techniques such as (Patel et al., 2018) and (Raju and Raju, 2016) . Most of the previous work is focused on separating the words in suffix and prefix based on some rules and then applying translation techniques. We addressed this issue with BPE to make this whole process more efficient and reliable. Moreover, We observed that very less work is being done on low resourced Indian languages and techniques such as Bytepair-encoding (BPEmb), MultiBPEmb, word-embedding, and self-attention are still unexplored which have shown a significant improvement in Natural Language Processing. Though unsupervised machine translation (Artetxe et al., 2017) is also in the focus of many researchers, still it is not as precise as supervised learning. We, also addressed that there is no trustworthy Public data available for the translation of such languages. Thus, in this paper, we have applied a neural machine translation technique with Multihead-self attention along with word embeddings and Pre-Trained Byte-Pair-Encoding. We worked on English-Tamil and English-Malayalam language pairs as it is one of the most difficult languages pair (Zdenek \u017dabokrtsk\u1ef3, 2012) to translate due to morphological richness of Tamil and Malayalam language. A similar approach can be applied to other languages as well. We obtained the data from En-Tamv2.0, Opus and UMC005, preprocessed them and evaluated our result using the evaluation matric BLEU. We used OpenNMT-py for the implementation of our models 2 . Experimental results, as well as the survey by native peoples, confirms that our result is far better than conventional translation techniques on Indian languages. The Main contributions of our work are as follows: \u2022 This is the first work to apply pre-trained BPE and MultiBPE embeddings on Indian language pairs (English-Tamil, English-Malayalam) along with Multihead self-attention technique. \u2022 We achieved good accuracy with a relatively simpler model and in less training time rather than training on a complex neural network which requires much resources and time to train. \u2022 We have addressed the issues with data preprocessing of Indian languages and shown why it is a crucial step in neural machine translation. \u2022 We made our preprocessed data publicaly available, which by our knowledge contains the largest number of a parallel corpus for the languages (English-Tamil, English-Malayalam, English-Telugu, English-Bengali, English-Urdu) \u2022 Our model outperforms Google translator with a margin of 3.36 and an 18.07 BLEU score. The paper is organized as follows. Sections Background and Approach describe the related work and the method that we used for our translator, respectively. Section experiments and Results show data preprocessing and results and analysis of our model. Finally, Section 5. concludes the paper and future work. Background A large amount of work has been reported on machine translation (MT) in the last few decades, the first one in the 1950s (Booth, 1955) . Various approaches is used by researchers, such as rule-based (Ghosh et al., 2014) , corpusbased (Wong et al., 2006) , and hybrid-based approach (Salunkhe et al., 2016) . Each approach has its own flaws and strength. Rule-based machine translation (RBMT) is MT systems based on the linguistic information about the source and target languages which is retrieved from ( multilingual, bilingual or monolingual) dictionaries and grammars covering the main syntactic, semantic and morphological regularities. It is further divided into transfer-based approach (TBA) (Shilon, 2011) and inter-lingual based approach (IBA). In the Corpus-based approach, we use a large-sized parallel corpus as raw data. This raw data contains ground truth translation for the desired languages. These corpora are used to train the model for translation. A corpus-based approach further classified in (i) statistical machine translation (SMT) (Patel et al., 2018 ) and (ii) example-based machine translation (EBMT) (Somers, 2003) . SMT is the combination of decoding algorithms and basic statistical language models.EBMT, on the other hand, uses the translation examples and generates the new translation accordingly. It is done by finding the examples which are matching with the input. The alignment has to be performed after that to find out the parts of translation that can be reused. Hybrid-base machine translation combines any corpus-based approach and transfer approach in order to overcome their limitations. According to the recent research (Khan et al., 2017) the machine translation performance of Indian languages such as (e.g., Hindi, Bengali, Tamil, Punjabi, Gujarati, and Urdu) is of an average of 10% accuracy. This demands the necessity of building better translation systems for Indian languages. Unsupervised machine translation is further a new way of translation without using the parallel corpus, but the results are still not remarkable. On the other hand, NMT is an emerging technique and shown significant improvement in the translation results. In this paper (Hans and Milton, 2016) phrase-based hierarchical model is used and trained after morphological preprocessing. (Patel et al., 2017) trained their model after compound splitting and suffix separation. Many researchers also tried the same way and achieved a decent result on their respective datasets (Pathak and Pakray, ) . We observed that morphological pre-processing, compound splitting and suffix or prefix separation can be overcome by using Byte-Pair-Encoding and produce similar or even better translation results without making the model complex. Approach In this paper, we present a neural machine translation technique using Multihead self-attention and word-embedding along with pre-trained Byte-Pair-Encoding (BPE) on our preprocessed dataset of Indian languages. We developed an efficient translation system, that overcomes the OOV (Out Of Vocabulary) and morphological analysis problem for Indian languages which do not have many translations available on the web. first, we provide an overview of NMT, Multi-head self-attention, word embedding, and Byte Pair Encoding. Next, we describe the framework of our translation model. Neural Machine Translation Overview Neural Machine translation is a powerful algorithm based on neural networks and uses the conditional probability of translated sentences to predict the target sentences of given source language (Revanuru et al., 2017a) . When coupled with the power of attention mechanisms, this architecture can achieve impressive results with different variations. The following sub-sections provide an overview of basic sequence to sequence architecture, self-attention and other techniques that are used in our proposed translator. Sequence to sequence architecture Sequence to sequence architecture is used for response generation whereas in Machine Translation systems it is used to find the relations between two language pairs. It consists of two important parts, an encoder, and a decoder. The encoder takes the input from the source language and the Figure 1 : Seq2Seq architecture for English-Tamil decoder leads to the output based on hidden layers and previously generated vectors. Let A be the source and B be a target sentence. The encoding part converts the source sentence a 1 , a 2 , a 3 ..., a n into the vector of fixed dimensions and the decoder part gives the word by word output using conditional probability. Here, A 1 , A 2 , ..., A M in the equation are the fixed size encoding vectors. Using chain rule, the Eq. 1 is transformed to the Eq. 2. P (B/A) = P (B|A 1 , A 2 , A 3 , ..., A M ) (1) P (B|A) = P (b i |b 0 , b 1 , b 2 , ..., b i\u22121 ; a 1 , a 2 , a 3 , ..., a m (2) The decoder generates output using previously predicted word vectors and source sentence vectors in Eq. 1. Attention Model In a basic encoder-decoder architecture, encoder memorizes the whole sentence in terms of vector, and store it in the final activation layer, then the decoder uses that vector to generates the target sentence. This architecture works quite well for small sentences, but for larger sentences, maybe longer than 30 or 40 words, the performance degrades. To overcome this problem attention mechanisms play an important role. The basic idea behind this is that each time, when the model predicts an output word, it only uses the parts of input where the most relevant information is concentrated instead of the whole sentence. In other words, it only pays attention to some weighted words. Many types of attention mechanisms are used in order to improvise the translation accuracy, but the multi-head selfattention overcomes most of the problems. Self-attention In self-attention architecture (Vaswani et al., 2017) at every time step of an RNN, a weighted average of all the previous states will be used as an extra input to the function that computes the next state. With the selfattentive mechanism, the network can decide to attend to a state produced many time steps earlier. This means that the latest state does not need to store all the information. The mechanism also makes it easier for the gradient to flow more easily to all previous states, which can help against the vanishing gradient problem. Multi-Head Attention When we have multiple queries q, we can combine them in a matrix Q. If we compute alignment using dot-product attention, the set of equations that are used to calculate context vectors can be reduced as shown in figure 3 . Q, K, and V are mapped into lowerdimensional vector spaces using weight matrices and the results are used to compute attention (which we call a Head). Word Embedding Word embedding is a unique way of representing the word in a vector space such that we can capture the semantic similarity of each word. Each word is represented in hundreds of dimensions. Generally, pre-trained embeddings are used trained on the larger data sets, and with the help of transfer learning, we convert the words from vocabulary to vector. (Cho et al., 2014) . 3.1.4. Byte Pair Encoding BPE (Gage, 1994) is a data compression technique that replaces the most frequent pair of bytes in a sequence. We use this algorithm for word segmentation, and by merging frequent pairs of charters or character sequences we can get the vocabulary of desired size (Sennrich et al., 2015) . BPE helps in the suffix, prefix separation, and compound splitting which in our case used for creating new and complex words of Malayalam and Tamil language by interpreting them as sub-words units. We used BPE along with pre-trained fast-text word embeddings 3 (Heinzerling and Strube, 2018) for both the languages with the variation in the vocabulary size. In our model, we got the best results with vocabulary size 25000 and dimension 300. MultiBPEmb MultiBPEmb is a collection of multiple languages subword segmentation models and pre-trained subword embeddings trained on Wikipedia data similar to monolingual BPE. On the contrary, instead of training one segmentation model for each language, here we train a single model and a single embedding for all the languages. We can also create a vocabulary of only two languages, source, and target. It deals with the mixed language sentences (Native language along with English) which are being popular nowadays on social media. Since our sentences were Experimentation and Results Evaluation Metric BLEU score is a method to measure the difference between machine translation and human translation (Papineni et al., 2002) . The approach works by matching n-grams in result translation to n-grams in the reference text, where unigram is a unique token, bigram is a word pair and so on. A perfect match results in a score of 1.0 or 100%. Dataset We obtained the data from different resources such as EnTamV2.0 (Ramasamy et al., 2012) , Opus (Tiedemann, 2012) and UMC005 (Jawaid and Zeman, 2011) .The sentences are of domain news, cinema, bible and movie subtitles. We combined and preprocessed the data of Tamil, Malayalam, Telugu, Bengali, and Urdu. After preprocessing (as described below) and cleaning, the dataset is split into train, test, and validation. Our final dataset is described in table 1. In our knowledge this is the largest, clean and preprocessed public dataset 4 available on the web for general purpose uses. As there is no publicly available dataset to compare various approaches on Indian languages, our datasets can be used to set baseline results to compare with. Data Pre-processing In the Research works (Hans and Milton, 2016 ) (Ramesh and Sankaranarayanan, 2018) EnTamV2.0 dataset is used. Also, the Opus dataset is a much widely used parallel corpus resource in various researcher's works. However, we observed that in both of these well-known parallel resources there are many repeated sentences, which may results into the wrong results (can be higher or lower) after dividing into train, validation, and test sets, as many of the sentences, occur both in train and test sets. In most of the work, the focus relies on the models without interpreting the data which performs much better on our own test set rather than on general translated sentences. Thus, it is essential to analyses, correct and cleans the data before using it for the experiments. Researchers should also provide a detailed source of the corpus otherwise results can be misleading such as in paper (Revanuru et al., 2017b) . We observed the following four important issues in the online available corpus. \u2022 Sentence repetition with the same source and target. 4 https://github.com/himanshudce/Indian-Language-Dataset \u2022 Different translations by the same source. \u2022 Same translated sentences by different source sentences. \u2022 Indian language tokenization. To overcome the first issue, we took unique pairs from all the parallel sentences and removed the repeating ones. To tackle the second and third case we removed sentence pairs which were repeated more than twice and the difference between their length are in the window of 5 words. It is because for both of these cases we cannot identify that which source is correct for the same translation and which translated sentence is comes from the same source. We observed that there were some sentences, which were repeating even more than 20 times in the Opus dataset. This confuses the model to learn, identify and capture different features and overfits the model. Though data-augmentation (Fadaee et al., 2017) can improve the translation results, but in that case, the original data should be pre-processed, otherwise many augmented sentences may appear in both train and test data which leads to higher but wrong BLEU score as it will not work efficiently on new sentences. For the tokenization of the English language, there are many libraries and frameworks such as (e.g., Perl tokenizer) but these do not work well on the Indian languages, due to the difference between morphological symbols. The wordformation of Indian languages is quite different which we believed can only be handled by either special library for that particular language or by Byte-Pair-Encoding. In the case of BPE, we don't need to tokenize the words which generally leads to better translation results. After working on all these minor, but effective preprocessing we got our final dataset. While extracting the data from the web, we also removed sentences with a length greater than 50, known translated words in target sentences, noisy translations, and unwanted punctuations. For the reliability of data, we also took the help of native speakers of these languages. Translator We tried various new techniques as described above to get a better intuition of the effects on these two Indian language pairs. Our first model consists of 4 layer Bi-directional LSTM encoder and a decoder with 500 dimensions each along with a vocabulary size of 50,004 words for both source and target. First, we used Bahdanau's attention and Adam optimizer with the dropout (regularization) of 0.3 and the learning rate 0.001. Here we used the 300 dimensional Pre-trained fast text 5 word embeddings for both the languages. Secondly, we used Pre-trained fast text Byte-Pair-Encoding 6 with the same attention. In the third model, we changed the attention to multi-head with 8 heads and 6 encoding and decoding layers. It shows an improvement of 1.2 and 6.18 BLEU scores for Tamil and Malayalam respectively. For the final model we used Multilingual fast text pre-trained Byte-pair-Encoddings 7 and got our final best results of 9.67 and 25.36 respectively as shown in table 2 and table 3. Result Our results is shown in table 2 and table 3. For Google translate we used Python API to translate the English sentences and compared the results with our various models. From the test results, It is observed that our model overcomes the OOV (Out of Vocabulary) problem in some cases, and is handy enough to be used in day to day life and official work. English-Tamil translation models Analysis We conducted a survey with ten random sentences from our test data and accumulated the reviews of native Tamil speaking peoples. On comparing the reviews of Google translator and our translator, it was found, that our translation results were better in 60% cases than the Google translator. The visualization of an Attention can be seen in 6 of one of the sample sentences from our test data. Conclusion In this paper, we applied Neural Machine Translation (NMT) on two of the most difficult Indian language pairs (English-Tamil, English-Malayalam). We addressed the issues of data pre-processing and tokenization. To handle morphology and word complexities of Indian languages we The same approach can be ap-plied to other Indian languages as well. Since the accuracy of our model was fairly good, it can be used for creating English-Malayalam and English-Tamil translation applications that will be very useful in domains like tourism, education and corporate. In the future, we can also explore the possibility to improve the translation results for codeswitched languages using MultiBPE and other variations.",
    "abstract": "A large number of significant assets are available online in English, which is frequently translated into native languages to ease the information sharing among local people who are not much familiar with English. However, manual translation is a very tedious, costly, and time-taking process. To this end, machine translation is an effective approach to convert text to a different language without any human involvement. Neural machine translation (NMT) is one of the most proficient translation techniques amongst all existing machine translation systems. In this paper, we have applied NMT on two of the most morphological rich Indian languages, i.e. English-Tamil and English-Malayalam. We proposed a novel NMT model using Multihead self-attention along with pre-trained Byte-Pair-Encoded (BPE) and MultiBPE embeddings to develop an efficient translation system that overcomes the OOV (Out Of Vocabulary) problem for low resourced morphological rich Indian languages which do not have much translation available online. We also collected corpus from different sources, addressed the issues with these publicly available data and refined them for further uses. We used the BLEU score for evaluating our system performance. Experimental results and survey confirmed that our proposed translator (24.34 and 9.78 BLEU score) outperforms Google translator (9.40 and 5.94 BLEU score) respectively.",
    "countries": [
        "India"
    ],
    "languages": [
        "Malayalam",
        "English",
        "Tamil"
    ],
    "numcitedby": "3",
    "year": "2020",
    "month": "May",
    "title": "Neural Machine Translation for Low-Resourced {I}ndian Languages"
}