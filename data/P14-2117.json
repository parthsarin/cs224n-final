{
    "article": "One fundamental problem of distant supervision is the noisy training corpus problem. In this paper, we propose a new distant supervision method, called Semantic Consistency, which can identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region. Specifically, we propose a semantic consistency model, which first models the local subspace around an instance as a sparse linear combination of training instances, then estimate the semantic consistency by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method. Relation Instance Label S1: Jobs was the founder of Apple Founder-of, CEO-of S2: Jobs joins Apple Introduction Relation extraction aims to identify and categorize relations between pairs of entities in text. Due to the time-consuming annotation process, one critical challenge of relation extraction is the lack of training data. To address this limitation, a promising approach is distant supervision (DS), which can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base (Mintz et al., 2009) . The underlying assumption of distant supervision is that every sentence that mentions two entities is likely to express their relation in a knowledge base. Figure 1 . Labeled instances by distant supervision, using relations CEO-of(Steve Jobs, Apple Inc.) and Founder-of(Steve Jobs, Apple Inc.) The distant supervision assumption, unfortunately, can often fail and result in a noisy training corpus. For example, in Figure 1 DS assumption will wrongly label S1 as a CEO-of instance and S2 as instance of Founder-of and CEO-of. The noisy training corpus in turn will lead to noisy extractions that hurt extraction accuracy (Riedel et al., 2010) . Figure 2 . The regions the two instances in Figure 1 located , where: 1) S1 locates in a semantically consistent region; and 2) S2 locates in a semantically inconsistent region To resolve the noisy training corpus problem, this paper proposes a new distant supervision method, called Semantic Consistency, which can effectively identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region. Figure 2 shows two intuitive examples. We can see that, semantic consistency is an effective way to identify reliable instances. For example, in Figure 2 S1 is highly likely a reliable Founder-of instance because its neighbors are highly semantically consistent, i.e., most of them express the same relation type -Founder-of. On contrast S2 is highly likely a noisy instance because its neighbors are semantically inconsistent, i.e., they have a diverse relation types. The problem now is how to model the semantic consistency around an instance. To model the semantic consistency, this paper proposes a local subspace based method. Specifically, given sufficient training instances, our method first models each relation type as a linear subspace spanned by its training instances. Then, the local subspace around an instance is modeled and characterized by seeking the sparsest linear combination of training instances which can reconstruct the instance. Finally, we estimate the semantic consistency of an instance by exploiting the characteristics of its local subspace. + + + + + \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 S2 \u00d7 S1 \u00d7 +: CEO-of \u00d7: Founder-of + \u00d7 + \u00d7 \u00d7 \u00d7 + : Manager-of : CTO-of This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. Related Work This section briefly reviews the related work. Craven and Kumlien (1999) , Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007) , Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013) , Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. The Semantic Consistency Model for Relation Extraction In this section, we describe our semantic consistency model for relation extraction. We first model the subspaces of all relation types in the original feature space, then model and characterize the local subspace around an instance, finally estimate the semantic consistency of an instance and exploit it for relation extraction. Testing Instance as a Linear Combination of Training Instances In this paper, we assume that there exist k distinct relation types of interest and each relation type is represented with an integer index from 1 to k. For ith relation type, we assume that totally ni training instances V i = fv i;1 ; v i;2 ; :::; v i;n i g V i = fv i;1 ; v i;2 ; :::; v i;n i g have been collected using DS assumption. And each instance is represented as a weighted feature vector, such as the features used in (Mintz, 2009) or (Surdeanu et al., 2012) , with each feature is TFIDF weighted by taking each instance as an individual document. To model the subspace of ith relation type in the original feature space, a variety of models have been proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption that the instances of a single relation type can be represented as the linear combination of other instances of the same relation type. This assumption is well motived in relation extraction, because although there is nearly unlimited ways to express a specific relation, in many cases basic principles of economy of expression and/or conventions of genre will ensure that certain systematic ways will be used to express a specific relation (Wang et al., 2012) . For example, as shown in (Hearst, 1992) , the IS-A relation is usually expressed using several regular patterns, such as \"such NP as {NP ,}* {(or | and)} NP\" and \"NP {, NP}* {,} or other NP\". Based on the above assumption, we hold many instances for each relation type and directly use these instances to model the subspace of a relation type. Specifically, we represent an instance y of ith type as the linear combination of training instances associated with ith type: y = \u00ae i;1 v i;1 + \u00ae i;2 v i;2 + ::: + +\u00ae i;n i v i;n i y = \u00ae i;1 v i;1 + \u00ae i;2 v i;2 + ::: + +\u00ae i;n i v i;n i (1) for some scalars , with j = 1, 2, \u2026,ni. For example, we can represent the CEO-of instance \"Jobs was the CEO of Apple\" as the following linear combination of CEO-of instances: \uf09f 0.8: Steve Ballmer is the CEO of Microsoft \uf09f 0.2: Rometty was served as the CEO of IBM For simplicity, we arrange the given ni training instances of ith relation type as columns of a matrix A i = [v i;1 ; v i;2 ; :::; v i;n i ] A i = [v i;1 ; v i;2 ; :::; v i;n i ], then we can write the matrix form of Formula 1 as: y = A i x i y = A i x i (2) where x i = [\u00ae i;1 ; :::; \u00ae i;n i ] x i = [\u00ae i;1 ; :::; \u00ae i;n i ] is the coefficient vector. In this way, the subspace of a relation type is the linear subspace spanned by its training instances, and if we can find a valid xi, we can explain y as a valid instance of ith relation type. Local Subspace Modeling via Sparse Representation Based on the above model, the local subspace of an instance is modeled as the linear combination of training instances which can reconstruct the instance. Specifically, to model the local subspace, we first concatenate the n training instances of all k relation types: A = [A 1 ; A 2 ; :::; A k ] A = [A 1 ; A 2 ; :::; A k ] Then the local subspace around y is modeled by seeking the solution of the following formula: y = Ax y = Ax (3) However, because of the redundancy of training instances, Formula 3 usually has more than one solution. In this paper, following the idea in (Wright et al., 2009) for robust face recognition, we use the sparsest solution (i.e., how to reconstruct an instance using minimal training instances), which have been shown is both discriminant and robust to noisiness. Concretely, we seek the sparse linear combination of training instances to reconstruct y by solving: ::] is a coefficient vector which identifies the spanning instances of y's local subspace, i.e., the instances whose \ud835\udefc \ud835\udc56,\ud835\udc57 \u2260 0 . In practice, the training corpus may be too large to direct solve Formula 4. Therefore, this paper uses the K-Nearest-Neighbors (KNN) of y (1000 nearest neighbors in this paper) to construct the training instance matrix A for each y, and KNN can be searched very efficiently using specialized algorithms such as the LSH functions in (Andoni & Indyk, 2006) . (l 1 ) : x \u00a4 = arg min kxk 1 s.t. kAx \u00a1 yk 2 \u2022 \" (l 1 ) : x \u00a4 = arg min kxk 1 s.t. kAx \u00a1 yk 2 \u2022 \" ( Through the above semantic decomposition, we can see that, the entries of x can encode the underlying semantic information of instance y. For ith relation type, let be a new vector whose only nonzero entries are the entries in x that are associated with ith relation type, then we can compute the semantic component corresponding to ith relation type as . In this way a testing instance y will be decomposed into k semantic components, with each component corresponds to one relation type (with an additional noise component ):  Figure 3 shows an example of semantic decomposition. We can see that, the semantic decomposition can effectively summarize the semantic consistency information of y's local subspace: if the instances around an instance have diverse relation types (S2 for example), its information will be scattered on many different semantic components. On contrast if the instances around an instance have consistent relation types (S1 for example), most of its information will concentrate on the corresponding relation type. Semantic Consistency based Relation Extraction This section describes how to estimate and exploit the semantic consistency for relation extraction. Specifically, given y's semantic decomposition: ::: + y k + \u00b2 we observe that if instance y locates at a semantic consistent region, then all its information will concentrate on a specific component yi, with all other components equal to zero vector 0. However, modeling errors, expression ambiguity and noisy features will lead to small nonzero components. Based on the above discussion, we define the semantic consistency of an instance as the semantic concentration degree of its decomposition: Definition 1(Semantic Consistency). For an instance y, its semantic consistency with ith relation type is: y = Consistency(y; i) = ky i k 2 P i ky i k 2 + k\u00b2k 2 Consistency(y; i) = ky i k 2 P i ky i k 2 + k\u00b2k 2 where Consistency(y, i) and will be 1.0 if all information of y is consistent with ith relation type; on contrast it will be 0 if no information in y is consistent with ith relation type. Semantic Consistency based Relation Extraction. To get accurate extractions, we determine the relation type of y based on both: 1) How much information in y is related to ith type; and 2) its semantic consistency score with ith type, i.e., whether y is a reliable instance of ith type. To measure how much information in y is related to ith relation type, we compute the proportion of common information between y and yi: sim(y; y i ) = y \u00a2 y i y \u00a2 y sim(y; y i ) = y \u00a2 y i y \u00a2 y (6) Then the likelihood for a testing instance y expressing ith relation type is scored by summarizing both its information and semantic consistency: rel(y; i) = sim(y; y i ) \u00a3 Consistency(y; i) rel(y; i) = sim(y; y i ) \u00a3 Consistency(y; i) and y will be classified into ith relation type if its likelihood is larger than a threshold: rel(y; i) \u00b8\u00bfi rel(y; i) \u00b8\u00bfi (7) where is a relation type specific threshold learned from training dataset. Founder-of CEO-of Founder-of noise CTO-of Multi-Instance Evidence Combination. It is often that an entity pair will match more than one sentence. To exploit such redundancy for more confident extraction, this paper first combines the evidence from different instances by combing their underlying components. That is, given the matched m instances Y={y 1 , y 2 , \u2026, y m } for an entity pair (e1, e2), we first decompose each instance as y j = y j 1 + ::: + y j k + \u00b2 y j = y j 1 + ::: + y j k + \u00b2 , then the entity-pair level decomposition y = y 1 + ::: + y k + \u00b2 y = y 1 + ::: + y k + \u00b2 is ob- tained by summarizing semantic components of different instances: y i = P 1\u2022j\u2022m y j i y i = P 1\u2022j\u2022m y j i . Finally, the likelihood of an entity pair expressing ith relation type is scored as: rel(Y; i) = sim(y; y i )Consistency(y; i)log(m + 1) rel(Y; i) = sim(y; y i )Consistency(y; i)log(m + 1) where is a score used to encourage extractions with more matching instances. One further Issue for Distant Supervision: Training Instance Selection The above model further provides new insights into one issue for distant supervision: training instance selection. In this paper, we select informative training instances by seeking a most compact subset of instances which can span the whole subspace of a relation type. That is, all instances of ith type can be represented as a linear combination of these selected instances. However, finding the optimal subset of training instances is difficult, as there exist 2 N possible solutions for a relation type with N training instances. Therefore, this paper proposes an approximate training instance selection algorithm as follows: 1) Computing the centroid of ith relation type as v i = P 1\u2022j\u2022n i v i;j v i = P 1\u2022j\u2022n i v i;j 2) Finding the set of training instances which can most compactly span the centroid by solving: (l 1 ) : x i = arg min kxk 1 s.t. kA i x \u00a1 v i k 2 \u2022 \" (l 1 ) : x i = arg min kxk 1 s.t. kA i x \u00a1 v i k 2 \u2022 \" 3) Ranking all training instances according to their absolute coefficient weight value ; 4) Selecting top p percent ranked instances as final training instances. The above training instance selection has two benefits. First, it will select informative instances and remove redundant instances: an informative instance will receive a high value because many other instances can be represented using it; and if two instances are redundant, the sparse solution will only retain one of them. Second, most of the wrongly labeled training instances will be filtered, because these instances are usually not regular expressions of ith type, so they appear only a few times and will receive a small . Experiments In this section, we assess the performance of our method and compare it with other methods. Dataset. We assess our method using the KBP dataset developed by Surdeanu et al. (2012) . The KBP is constructed by aligning the relations from a subset of English Wikipedia infoboxes against a document collection that merges two distinct sources: (1) a 1.5 million documents collection provided by the KBP shared task (Ji et al., 2010; Ji et al., 2011) ; and (2) a complete snapshot of the June 2010 version of Wikipedia. Totally 183,062 training relations and 3,334 testing relations are collected. For tuning and testing, we used the same partition as Surdeanu et al. (2012) : 40 queries for development and 160 queries for formal evaluation. In this paper, each instance in KBP is represented as a feature vector using the features as the same as in (Surdeanu et al., 2012) . Baselines. We compare our method with four baselines as follows: \uf09f Mintz++. This is a traditional DS assumption based model proposed by Mintz et al.(2009) . \uf09f Hoffmann. This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al. (2011) . \uf09f MIML. This is a multi-instance multi-label model proposed by Surdeanu et al. (2012) . \uf09f KNN. This is a classical K-Nearest-Neighbor classifier baseline. Specifically, given an entity pair, we first classify each matching instance using the labels of its 5 (tuned on training corpus) nearest neighbors with cosine similarity, then all matching instances' classification results are added together. Evaluation. We use the same evaluation settings as Surdeanu et al. (2012) . That is, we use the official KBP scorer with two changes: (a) relation mentions are evaluated regardless of their support document; and (b) we score only on the subset of gold relations that have at least one mention in matched sentences. For evaluation, we use Mintz++, Hoffmann, and MIML implementation from Stanford's MIMLRE package (Surdeanu et al., 2012) and implement KNN by ourselves. Experimental Results Overall Results We conduct experiments using all baselines and our semantic consistency based method. For our method, we use top 10% weighted training instances. All features occur less than 5 times are filtered. All l 1 -minimization problems in this paper are solved using the augmented Lagrange multiplier algorithm (Yang et al., 2010) , which has been proven is accurate, efficient, and robust. To select the classification threshold for ith relation type, we use the value which can achieve the best F-measure on training dataset (with an additional restriction that precision should > 10%).  Figure 4 shows the precision/recall curves of different systems, and Table 1 shows their best F1-measures. From these results, we can see that: 1) The semantic consistency based method can achieve robust and competitive performance: in KBP dataset, our method correspondingly achieves 5.6%, 7%, 3.3% and 3.4% F1 improvements over the Mintz++, Hoffmann, MIML and KNN baselines. We believe this verifies that the semantic consistency around an instance is an effective way to identify reliable instances. 2) From Figure 4 we can see that our method achieves a consistent improvement on the high-recall region of the KBP curves (when recall > 0.1). We believe this is because by modeling the semantic consistency using the local subspace around each testing instance, our method can better solve the classification of long tail instances which are not expressed using salient patterns. 3) The local subspace around an instance can be effectively modeled as a linear subspace spanned by training instances. From Table 1 we can see that both our method and KNN baseline (where the local subspace is spanned using its k nearest neighbors) achieve competitive performance: even the simple KNN baseline can achieve a competitive performance (0.277 in F1). This result shows: a) the effectiveness of instance-based subspace modeling; and b) by partitioning subspace into many local subspaces, the subspace model is more adaptive and robust to model prior. 4) The sparse representation is an effective way to model the local subspace using training instances. Compared with KNN baseline, our method can achieve a 3.4% F1 improvement. We believe this is because: (1) the discriminative nature of sparse representation as shown in (Wright et al., 2009) ; and (2) the sparse representation globally seeks the combination of training instances to characterize the local subspace, on contrast KNN uses only its nearest neighbor in the training data, which is more easily affected by noisy training instances(e.g., false positives). Training Instance Selection Results To demonstrate the effect of training instance selection, Table 2 Conclusion and Future Work This paper proposes a semantic consistency method, which can identify reliable instances from noisy instances for distant supervised relation extraction. For future work, we want to design a more effective instance selection algorithm and embed it into our extraction framework. Acknowledgments",
    "abstract": "One fundamental problem of distant supervision is the noisy training corpus problem. In this paper, we propose a new distant supervision method, called Semantic Consistency, which can identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region. Specifically, we propose a semantic consistency model, which first models the local subspace around an instance as a sparse linear combination of training instances, then estimate the semantic consistency by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method. Relation Instance Label S1: Jobs was the founder of Apple Founder-of, CEO-of S2: Jobs joins Apple",
    "countries": [
        "China"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "10",
    "year": "2014",
    "month": "June",
    "title": "Semantic Consistency: A Local Subspace Based Method for Distant Supervised Relation Extraction"
}