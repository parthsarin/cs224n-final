{
    "article": "In this paper we describe VERBCROCEAN, a broad-coverage repository of fine-grained semantic relations between Croatian verbs. Adopting the methodology of Chklovski and Pantel (2004) used for acquiring the English VerbOcean, we first acquire semantically related verb pairs from a web corpus hrWaC by relying on distributional similarity of subject-verb-object paths in the dependency trees. We then classify the semantic relations between each pair of verbs as similarity, intensity, antonymy, or happens-before, using a number of manually-constructed lexico-syntatic patterns. We evaluate the quality of the resulting resource on a manually annotated sample of 1000 semantic verb relations. The evaluation revealed that the predictions are most accurate for the similarity relation, and least accurate for the intensity relation. We make available two variants of VERBCROCEAN: a coverage-oriented version, containing about 36k verb pairs at a precision of 41%, and a precision-oriented version containing about 5k verb pairs, at a precision of 56%. Introduction Lexico-semantic resources such as WordNet (Miller, 1995) play an important role in many natural language processing (NLP) tasks. In particular, for applications that involve processing of events and states, resources that model verb semantics are of great importance. A number of such resources have been developed, most notably verb lexica FrameNet (Baker et al., 1998) and VerbNet (Kipper-Schuler, 2005) , which model the predicate-argument relations. More recently, the interest in textual entailment (Dagan et al., 2013) has motivated the construction of largescale verb entailment resources (Pekar, 2006; Szpektor and Dagan, 2008; Hashimoto et al., 2009) . VerbOcean (Chklovski and Pantel, 2004 ) is a broadcoverage repository of fine-grained semantic relations between English verbs. VerbOcean models five semantic relations between verbs: similarity, strength, antonymy, enablement, and happens-before. The resource has been acquired semi-automatically from the web using lexicosyntactic patterns, and covers about 3,500 verbs and 30,000 semantic relations. VerbOcean has been used for many NLP tasks, including event extraction (Mani et al., 2006) , paraphrase detection, entailment recognition (Dagan et al., 2010; Mehdad et al., 2011) and contradiction detection (De Marneffe et al., 2008) . In this paper we apply the VerbOcean methodology to extract a broad-coverage repository of semantic relations between Croatian verbs. We essentially adopt the procedure of Chklovski and Pantel (2004) and evaluate the quality of the resulting resource on a manually annotated sample of semantic verb relations. Based on our insights, we create VERBCROCEAN, a freely available repository of finegrained semantic verb relations for Croatian. We make available two versions of the resource: a coverage-oriented version containing about 36k relations and a precisionoriented version containing about 5k relations. Related Work Only a handful of semantic verb resources for Croatian are publicly available. The Croatian WordNet (CroWN) (Raffaelli et al., 2008) contains, alongside nouns and adjectives, 2318 verbs arranged into hypernym hierarchies. CroDeriV ( \u0160ojat et al., 2013) is a comprehensive morphological lexicon containing almost 14,000 Croatian verbs. In a followup work, \u0160ojat and Sreba\u010di\u0107 (2014) studied the morphosemantic relations between Croatian verbs in CroWN. The only publicly available predicate semantics resource is the Croatian Verb Valence Lexicon (Preradovi\u0107 et al., 2009) , which lists 1,739 verbs with 5,118 valence frames, organized in 173 syntactic-semantic classes. Less verb-specific, \u0160najder et al. (2013) present a distributional memory for Croatian, which includes syntax-based distributional vectors for about 2M lemmas, including the verbs. Verb Relations As noted by Schulte im Walde and Melinger (2005) , different resources incorporate different semantic verb relations, and there seems to be no exhaustive study on the range of possible verb relations. Princeton WordNet identifies synonymy, antonymy, troponymy, hypernymy, entailment, and cause. On the other hand, VerbOcean identifies similarity, strength, antonymy, enabled, and happens-before. Following (Chklovski and Pantel, 2004) , we chose to extract the following four semantic verb relations. Similarity. Similarity includes synonymous and nearsynonymous verbs (e.g., get :: acquire), but also verbs that differ in the degree of action, intensity, or the manner in which the action is carried out. Examples of similar verbs extracted in VERBCROCEAN include informirati :: educirati (to inform :: to educate), snimiti :: fotografirati (to record :: to photograph), and pobolj\u0161ati :: unaprijediti (to ameliorate :: to advance). Intensity. Even though two verbs are similar, one can refer to a more intense and stronger action. A pair of verbs belongs to this relation if the first verb refers to the weaker than (but similar to) the action to which the second verb refers. Example relations from VERBCROCEAN are raniti :: ubiti (to wound :: to kill), komentirati :: kritizirati (to comment :: to criticize), misliti :: znati, (to think :: to know), and udvostru\u010diti :: utrostru\u010diti (to double :: to tripple). Note that intensity implies similarity. Antonymy. Antonymy holds between two verbs that express opposite meaning. Examples from VERBCRO-CEAN are potvrditi :: demantirati (to confirm :: to confute), kupiti :: prodati (to buy :: to sell), and pove\u0107ati :: smanjiti (increase :: decrease). Happens-before. Verbs in this relation refer to two temporally disjoint intervals or instances. The first verb describes an event that generally happens before the one described by the second verb. Examples of this relation in VERBCROCEAN are napasti :: udariti (to attack :: to hit) and dijagnosticirati :: lije\u010diti (to diagnoze :: to treat). A pair of verbs can be in multiple semantic relations. However, some relations are mutually exclusive (e.g., antonymy and similarity). As intensity implies similarity, antonyms cannot stand in the intensity relation. The happens-before relation can appear alongside any other relation. For instance, one action (e.g, to wound) can happen before an action that is similar to it (to torture), more intense than it (to kill), or its antonym (to heal). VerbCROcean Following Chklovski and Pantel (2004) , we acquire VerbCROcean in two steps: (1) the acquisition of semantically related verb pairs based on statistical co-occurrence and (2) verb pair relation classification of using lexicosyntactic patterns. Prior to this, we preprocess the corpus and extract the verbs. We next describe these steps in detail. Corpus and Preprocessing We extract VERBCROCEAN from the Croatian web-corpus hrWaC (Ljube\u0161i\u0107 and Erjavec, 2011) , totaling 1.2 billion words. The corpus has been lemmatized and tagged using the tools of Agi\u0107 et al. (2013) , and parsed using the tools of Agi\u0107 and Merkler (2013) . It is a well-known fact that web corpora suffer from a considerable amount of noise (e.g., misspellings, ungrammatical language, jargon). In addition to this, in our case lemmatization introduces a substantial amount of systematic errors. To not compromise the precision of VERBCROCEAN, we decided to compile a whitelist of verbs consisting of those verbs occurring at least 500 times in hrWaC. Additionally, we discarded all verbs not ending in the infinitive suffix -ti or -\u0107i, as well as modal verbs biti (to be), htjeti (will), morati (must), mo\u0107i (can), and trebati (shall). The final list contains 7,050 verbs, which is almost twice as much as the number of verbs in VerbOcean. We also compiled a whitelist of nouns as those that occur more than 100 times in the corpora, resulting in 80,218 nouns. The nouns are used in the acquisition of verb pairs, as described below. Acquisition of Verb Pairs Following Chklovski and Pantel (2004) , we acquire the semantically related verb pairs using the DIRT (Discovery of Inference Rules from Text) algorithm of Lin and Pantel (2001) . DIRT utilizes the distributional hypothesis (Harris, 1954) , which posits that the similarity of words correlates with the similarity of their contexts, with the difference that the similarities are computed between paths in dependecy trees rather than between words. DIRT was originally used for extracting paraphrases, but as we are interested only in verbs, we apply DIRT to paths of the form subjectX \u2190 verb \u2192 objectY. Subjects and objects fill slots X and Y of a single path. To reduce the number of spurious paths, we allow as slot fillers only the nouns from our whitelist of nouns. Essentially, two paths are considered similar if they tend to link the same set of words, i.e., if they share a large number of words in the corresponding slots. Intuitively, not all words are equally important. For example, X=person is less informative than X=lawyer. Therefore, similarity between two paths is computed based on the similarity of their slot fillers, weighted using pointwise mutual information. Following Lin and Pantel (2001) , let (p, s, w) denote that slot s (where s=subjectX or s=objectY) of path p is filled by word w. Mutual information between a path slot and its filler is computed as: w\u2208T (p1,s) mi (p 1 , s, w) + w\u2208T (p2,s) mi (p 2 , s, w) Finally, the similarity between two paths p 1 and p 2 is computed as the geometric mean of similarities of their subject and object slots, ssim(p 1 , p 2 , subjectX ) and ssim(p 1 , p 2 , objectY ), respectively. For reasons of efficiency, we do not compute the similarity between all path pairs. Instead, for each path, we first discard the paths that share with it less than 1% of words in the same-type slots. We then compute its similarity with the rest of the paths and keep up to 50 most similar paths. As a result, we extracted a total of 5,963 unique verbs that are semantically related to another verb in our dataset. On average, each verb is related to 39 other verbs, yielding a total of 183,662 unique verb pairs with a potential semantic relation. As an example, Table 1 lists top 20 verbs extracted as potentially related to the verb voziti (to drive). Relation Classification After acquiring the related verb pairs using DIRT, we classify the semantic relation of each pair by applying lexicosyntactic patterns on sentences from the corpus in which those pairs occur.  To this end, we selected from the output of DIRT a list of 15 verb pairs that stand in one of the four considered relations. We then sampled sentences from hrWaC in which both verbs co-occur, and manually examined the syntactic paths beetween them. If the path seemed general enough, we add them to the list of patterns. In this way we were able to extract 8 more patterns from the corpus. We complemented the corpus search with web search using Google, but this did not result in any new patterns. Relation classification. Given a verb pair extracted by DIRT, the classification of semantic relation is done as follows. For each sentence in the corpus that contains the given verb pair, we apply all of the above-listed 29 patterns as regular expressions, and then collect the frequencies of the verbs and the patterns. Based on these, we compute the strength of each relation using (1) the frequency of the relation, (2) pointwise mutual information (PMI), and (3) local mutual information (LMI). PMI is defined as follows: PMI (v 1 , rel , v 2 ) = ln P (v 1 , rel , v 2 ) P (v 1 ) P (rel ) P (v 2 ) (1) PMI is known to overestimate the strength of the association for rare words. To compensate for this, we also consider the frequency-weighted PMI, or local mutual information, proposed by Evert (2008) : LMI (v 1 , rel , v 2 ) = f (v 1 , rel , v 2 ) PMI (v 1 , rel , v 2 ) (2) Relation refinement. Our model quite often predicts several relations for a given verb pair, each with a different confidence score. To make the predictions consistent with the definitions of semantic relations from Section 3, we post-process the relation predictions as follows. For each acquired relation, we take the confidence score (frequency, PMI, or LMI) of the verb pair (v 1 , v 2 ). We then refine the relations by applying the following four rules: 1. For symmetric relations (antonymy and similarity), we chose the one with the highest score, thereby considering also the score of the reverse pair (v 2 , v 1 ); 2. If relation is classified as similarity, we also predict intensity, provided intensity was among the originally predicted relations; 3. If the relation is classified as intensity, but not as similarity, we also predict similarity with confidence equal to that of the intensity relation; 4. If the relation is happens-before, leave it as it is, because it can hold alongside any other relation. Evaluation In this section we describe a sample-based evaluation of VERBCROCEAN. Verb Pairs Sample We evaluate VERBCROCEAN on a sample of 1000 verb pairs manually annotated with semantic relations. We obtain the sample from the output of the relation classification step (but prior to the relation refinement step). We asked three annotators to independently annotate the sample by tagging each verb pair with one or more of the four semantic relations, or none if they think the verbs are not semantically related. The inter-annotator agreement, measured in terms of Cohen's kappa (Cohen, 1960) , is 0.47, which is considered a moderate level of agreement according to Landis and Koch (1977) . Note that we framed the problem as a multi-label task, and considered two annotations to be identical if and only if their sets of assigned labels are identical. As a consequence, the inter-annotator agreement estimate is probably somewhat pessimistic.  svladati (to vanquish) S \u0161iriti (to spread) ograni\u010davati (to limit) A udarati (to hit) razbiti (to break) BIS Table 3 : Excerpt from gold sample (A -antonymy, S -similarity, B -hapens-before, I -intensity, N -no-relation). For the gold sample, we retain only the verb pairs for which at least two annotators agreed on their relation. We acquired a total of 953 verb pairs, distributed across the relations as follows: 73 antonymy, 151 happens-before, 15 intensity, 494 no-relation, and 261 similarity relations. An excerpt from the gold sample is presented in Table 3 . We make the annotated sample freely available. 1 Results The acquisition procedure has to trade-off between precision and coverage. To acquire a resource of satisfactory precision, but at the same time with the coverage as broad as possible, we set a threshold on each relation confidence score and acquire only the verb pairs for which the confidence score is above that threshold. Fig. 1 shows the micro-averaged precision of the acquired semantic relations (measured on the manually annotated sample) with respect to the number of acquired verb pairs (determined by the confidence score threshold). Somewhat surprisingly, scoring the relations by frequency yields higher precision than scoring by PMI or LMI; we leave the investigation of this for future work. We also observe a general trend of precision decrease across the first 5,000 ranks, after which the precision stabilizes at about 0.4. Based on these insights, we use frequency-based ranking to compile two variants of VERBCROCEAN: (1) a precisionoriented version (VCO-Pre), containing 5,010 verb pairs, with semantic relations classified at 56% micro-precision, and (2) a coverage-oriented version (VCO-Cov), containing all acquired relations (36,778 relations), with semantic relations classified at 41% micro-precision. We make both resources publicly available. 1  In Table 4 we show per class and macro acurracies for both VERBCROCEAN variants, measured in terms of precision, recall, and the F1-score on our manually annotated sample. 1 http://takelab.fer.hr/verbcrocean The predictions are most accurate for the similarity relation, and least accurate for the intensity relation. This is is probably due to the fact that similarity is a more frequent relation, so many of its instances can be captured with the three patterns we used. At the same time, it is a rather broad relation, hence it can be acquired with a good precision (0.56). In contrast, intensity occurs rarely, and is also a more strict relation. Another relation that can be predicted with relatively good precision is antonymy. Overall, VCO-Pre has a macro-precision of 0.40 and a rather low macro-recall of 0.10. In contrast, VCO-Cov has well-balanced macroprecision and macro-recall of around 0.37. Note that, while we do report recall in Table 4 , we do not measure the overall recall. Measuring the overall recall would require the annotation of a random sample of verb pairs from corpus. As most of the verbs stand in no semantic relation, annotating a sample of a satisfactory size would be too demanding. Conclusion We described the acquisition of VERBCROCEAN, a broadcoverage repository of semantic relations between Croatian verbs. We considered four semantic relations: synonymy, antonymy, intensity, and happens-before. Evaluation on a manually annotated sample reveals that we can extract 5,000 verb pairs with 56% micro-precision of semantic relation classification, and over 36K verb pairs with a microprecision of 41%. There are a number of ways in which both precision and recall of the resource could be improved. Recall could be improved by including more patterns, which could be harvested in an automatic or semi-automatic manner. Precision could be improved by training a supervised model to give confidence-rated predictions for each verb pair, using a number of statistical and linguistic features. We intend to pursue this in future work. Acknowledgments This work has been supported by the Croatian Science Foundation under the project UIP-2014-09-7312. Bibliographical References",
    "abstract": "In this paper we describe VERBCROCEAN, a broad-coverage repository of fine-grained semantic relations between Croatian verbs. Adopting the methodology of Chklovski and Pantel (2004) used for acquiring the English VerbOcean, we first acquire semantically related verb pairs from a web corpus hrWaC by relying on distributional similarity of subject-verb-object paths in the dependency trees. We then classify the semantic relations between each pair of verbs as similarity, intensity, antonymy, or happens-before, using a number of manually-constructed lexico-syntatic patterns. We evaluate the quality of the resulting resource on a manually annotated sample of 1000 semantic verb relations. The evaluation revealed that the predictions are most accurate for the similarity relation, and least accurate for the intensity relation. We make available two variants of VERBCROCEAN: a coverage-oriented version, containing about 36k verb pairs at a precision of 41%, and a precision-oriented version containing about 5k verb pairs, at a precision of 56%.",
    "countries": [
        "Croatia"
    ],
    "languages": [
        "Croatian",
        "English"
    ],
    "numcitedby": "0",
    "year": "2016",
    "month": "May",
    "title": "{V}erb{CRO}cean: A Repository of Fine-Grained Semantic Verb Relations for {C}roatian"
}