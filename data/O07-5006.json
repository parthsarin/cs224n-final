{
    "article": "The Pinyin-to-Character Conversion task is the core process of the Chinese pinyin-based input method. Statistical language model techniques, especially ngram-based models, are mostly adopted to solve that task. However, the ngram model only focuses on the constraints between characters, ignoring the pinyin constraints in the input pinyin sequence. This paper improves the performance of the Pinyin-to-Character Conversion system through exploitation of the pinyin constraints. The MEMM framework is used to describe the pinyin constraints and the character constraints. A Class-based MEMM (C-MEMM) model is proposed to address the MEMM efficiency problem in the Pinyin-to-Character Conversion task. The C-MEMM probability functions are strictly deduced and well formulized according to the Bayes rule and the Markov property. Both the cases of hard class and soft class are well discussed. In the experiments, C-MEMM outperforms the traditional ngram model significantly by exploitation of the pinyin constraints in the Pinyin-to-Character Conversion task. In addition, C-MEMM can well utilize the syntax and semantic information in word class and further improve the system performance. Introduction The standard keyboard was initially designed for native English speakers. In Asia, such as China, Japan and Thailand, people cannot input their language through the standard keyboard directly. Asian text input becomes one of the challenges for computer users in Asia. Therefore, an Asian language input method is one of the most difficult problems in Asian language processing. a Class-Based Maximum Entropy Markov Model Approach efficiency and robustness. However, several drawbacks have also been found in the traditional ngram model. First, according to Zipf's law [Zipf 1935 ], there are a lot of words which rarely or never occur in the training corpus. The data sparseness problem is severe [Brown et al. 1992 ] in the ngram model. Second, long distance constraints are difficult to capture since the ngram model only focuses on local lexical constraints. Third, it 's hard to utilize the linguistic knowledge of the ngram model. Many techniques have been proposed to address the drawbacks of the traditional ngram model. To solve the data sparseness problem, various kinds of smoothing techniques have been proposed, such as additive smoothing [Jeffreys 1948 ], Katz smoothing [Katz 1987 ], linear interpolation smoothing [Jelinek and Mercer 1980] , semantic based smoothing [Xiao et al. 2005b; Xiao et al. 2006 ]. To utilize the linguistic knowledge, a set of linguistic rules are generated automatically and they are incorporated into the traditional ngram model by a hybrid ngram model [Wang et al. 2005] . Hsu [Hsu 1995] proposes the context sensitive model (CSM) in which the semantic patterns are captured by the templates. As much as 96% accuracy, which is the best result of the traditional Chinese input methods as far as we know, is reported for CSM on the Phoneme-to-Character Conversion task. Trigger techniques have been proposed [Zhou and Lua 1998 ] and word-pair techniques have been proposed [Tsai and Hsu 2002; Tsai et al. 2004; Tsai 2005; Tsai 2006 ]. The linguist knowledge can be effectively described by triggers and pairs; meanwhile, the long distance constraints can be well captured. Compared with the commercial input system (MS-IME 2003) , effective improvements have been achieved by these techniques [Tsai 2006 ]. Wang [Wang et al. 2004] utilizes the theory of rough set so as to discover the linguistic knowledge and incorporate it into the Pinyin-to-Character Conversion system. Compared with the traditional ngram model, Wang 's system achieves a higher accuracy with a smaller storage requirement. Xiao [Xiao et al. 2005a] incorporates the word positional information into the Pinyin-to-Character Conversion system and achieves encouraging results in experiments. Gao [Gao et al. 2005] proposes the Minimum Sample Risk (MSR) principle to estimate the parameters of the ngram model. Success has been achieved with this principle for a Japanese input method. What's more, some techniques have been proposed especially for Chinese text input method. A Pinyin-to-Character Conversion system with spelling-error correction was developed by Zhang [Zhang et al. 1997 ]. In the system, a rule-based model is designed to correct typing errors when the user inputs pinyin strings. Not only can the system accept the correct pinyin input, but it can also tolerate common typing errors. Similar work has been done by Chen [Chen and Lee 2000] . Chen constructs a statistical model to correct user typing errors. Moreover, Chen proposes a modeless input technique in which the user can input English using a Chinese input method, not requiring language mode switch. However, there is another drawback of the ngram model in the Pinyin-to-Character Conversion task, which has been ignored by most researchers. It takes no account of pinyin constraints on the input pinyin sequence while actually in the process of Pinyin-to-Character Conversion. This paper regards that the pinyin information from the pinyin sequence is helpful for selecting the correct character sequence in the Pinyin-to-Character Conversion task. First, the current input pinyin string is helpful for selecting the correct character which corresponds to that pinyin. For example, the input pinyin sequence is \"ta1 shi4 di2 shi4 you3?\" which should be converted into \"\u4ed6\u662f\u654c\u662f\u53cb?\" (\"Is he an enemy or friend?\"). Let's focus on the third pinyin string of \"di2\". There are two homonyms which correspond to it: \"\u654c\" and \"\u7684\". (There are actually many homonyms, but let 's only focus on \"\u654c\" and \"\u7684\" for simplification). \"\u7684\" is one of the most frequent Chinese characters and its frequency is usually much higher than \"\u654c\". According to the ngram model, the above pinyin sequence should be converted into \" \u4ed6\u662f\u7684\u662f\u53cb?\" which is a wrong conversion. However, \" \u7684 \" is a polyphone which corresponds to both \"di2\" and \"de5\". In Chinese, \"\u7684\" is usually pronounced as \"de5\" instead of \"di2\". (\"\u7684\" is pronounced as \"di2\" only in the word \"\u7684\u786e\" (certainly)). The frequency of \"\u7684\" mainly comes with its pronunciation \"de5\". If the pinyin information is considered in the above conversion, the co-occurrences of \"\u7684\" and \"di2\" are usually lower than that of \"\u654c\" and \"di2\". Then, the above pinyin sequence is correctly converted into \"\u4ed6\u662f\u654c\u662f\u53cb?\". Second, the contextual information, especially the future information, can be well exploited in the pinyin constraints. For example, there are two pinyin sequences. The first one is \"yi4 zhi1 ke3 ai4 de5 xiao3 hua1\" which should be converted into \"\u4e00\u679d\u53ef\u7231\u7684\u5c0f\u82b1\" (This is a lovely flower). The second pinyin \"zhi1\" should be converted into \"\u679d\" which is determined by its future character \"\u82b1\" (flower). The second pinyin sequence is \"yi4 zhi1 ke3 ai4 de5 xiao3 hua1 mao1\" which should be converted into \"\u4e00\u53ea\u53ef\u7231\u7684\u5c0f\u82b1\u732b\" (This is a lovely cat). The second pinyin \"zhi1\" should be converted into \"\u53ea\" which is determined by its future character \"\u732b\" (cat). However, according to the ngram model, the conversion of \"zhi1\" is only determined by its history information which is the character \"\u4e00\" in the above two cases. The characters of \"\u82b1\" and \"\u732b\" are both the further information that the ngram model can not exploit. Therefore, the same probabilities are assigned to both the characters of \"\u53ea\" and \"\u679d\". They can not be distinguished by the ngram model. In the above two conversions, at least one of them would be converted incorrectly. However, if the pinyin constraints are considered, the constraints of \"hua1\" and \"mao1\", which correspond to the characters of \"\u82b1\" and \"\u732b\", are exploited and imposed on the conversion of \"zhi1\". Then, the above two cases can be distinguished and the correct conversions can be obtained. Third, the long distance constraints can be exploited from the pinyin sequence. As for the ngram model, it has to construct a high-order model to capture the long distance constraints. However, high-order ngram models suffer from the curse of dimensionality which usually leads to a severe data sparseness problem. The current model order is usually 2 or 3. In the above example, in order to exploit a Class-Based Maximum Entropy Markov Model Approach the constraints of \"\u82b1\" and \"\u732b\" on the conversion of \"zhi1\", it has to build up at least a 7-order ngram model which suffers from a great data sparseness problem and cannot work well in reality. However, the pinyin constraints are collected as features and exploited under the Maximum Entropy (ME) framework in this paper. The context window size can be relatively large (i.e. 5 pinyin strings or 7 pinyin strings) without the curse of dimensionality. Then the constraints of \"\u82b1\" and \"\u732b\" can be imposed on the conversion of \"zhi1\" by exploitation of their pinyin information. This paper aims to improve the performance of the Pinyin-to-Character Conversion system by exploitation of the pinyin constraints from pinyin sequence. The pinyin constraints are described under the ME framework [Berge et al. 1996] , and the character constraints are modeled by the traditional ngram model. Combining these two models into a unified framework, the paper builds the Pinyin-to-Character Conversion system on a MEMM model [McCallum et al. 2000 ]. However, the label set on the Pinyin-to-Character Conversion task is the Chinese lexicon. The scale of Chinese lexicon is usually in the range of label can be obtained by some automatic algorithms [Li 1998; Chen and Huang 1999; Gao et al. 2001] or from some pre-defined thesauri [Mei et al. 1983] . The scale of class set is usually much smaller than that of target label, which makes it feasible to train C-MEMM under the Maximum Entropy principle. Then, these constraints are conveyed from the class sequence to the target label sequence. So, C-MEMM can efficiently exploit the pinyin constraints from pinyin sequence and get effective improvement in the Pinyin-to-Character Conversion task. The paper is organized as follows: the MEMM model is briefly reviewed in Section 2. In Section 3, the C-MEMM model is proposed and its probability functions are deduced according to the Bayes rule and the Markov property. Both the cases of hard class and soft class are discussed in detail. Experimental results and discussions are provided in Section 4. The related works are described in Section 5, and the conclusions are drawn in Section 6. Brief Review of MEMM MEMM is a powerful tool used to perform the sequence labeling task, which is to determine a state sequence according to the observation sequence. Different from the ngram model, MEMM not only makes use of the constraints between states but also utilizes the constraints from observations. MEMM integrates these two kinds of constraints into a uniform \u00ec == \u00ef = \u00ed \u00ef \u00ee (2) where h is the contextual information of state s and h * (or s * ) is the concrete instance of h (or s). The following constraints are imposed so that the expectation of each feature in the learned model should be consistent with its empirical value in the training corpus. More formally, the constraints can be expressed as: It results in the probability function of exponential form: ' ' 1 (|,)exp((,)) (,) ii i pssofhs Zhs l = \u00e5 ( 7 ) where l is the weight of the feature f i , and Z is the normalization factor. In the above formula, there is no easy solution to get the optimal value of l directly. Some iterative algorithms, i.e. the Generalized Iterative Scaling (GIS) algorithm [Darroch and Ratcliff 1972] and the Improved Iterative Scaling (IIS) algorithm [Pietra et al. 1997] , are usually adopted. However, the time complexity of the iterative algorithm is far beyond the complexity of the MLE method, and it becomes the bottleneck of the training process of MEMM. When the scale is large, it is infeasible to use the iterative algorithm to train the MEMM model because of the high complexity. Principle of Class-Based MEMM This paper involves the class of state in traditional MEMM so as to address its efficiency problem on a large scale of state set. A Class-based MEMM model is proposed and its probability functions are strictly deduced and well formulized both in the case of hard class and soft class. The section is structured as follows. First, it presents C-MEMM in the case of hard class. Second, it describes C-MEMM in the case of soft class. Third, it provides ways to get the class of the state. In the case of hard class, where the class sequence is completely determined by the state sequence, the following equation can be made: C-MEMM on Hard Class (|)(,|) PSOPSCO = . (9) Then, the probability function of C-MEMM can be deduced through the following process: (|)(,|)(|)(|,) BayesianRule PSOPSCOPCOPSCO ==. (10) According to the Bayes rule, the conditional probability of sequential data is decomposed into two conditional probabilities. The probability of (|) PCO can be further decomposed by the Bayes rule and the Markov property, exactly as the process of Formula (1). The ultimate formula is directly presented as below: 111111 22 (|)(|)(|,)(|)(|)(|) nn iiiiiMEii ii PCOpcopccopcopccpch -- == == \u00d5\u00d5 (11) In the above formula, 1 (|,) iii pccois further decomposed by Formula (8). For the probability of (|,) PSCO , the decomposing process is a little more complex and an additional independent assumption should be made. n iiiii i copscopss - = \u00b4\u1e4c (12) The fore part of the above deduction is exactly the same as the process in Formula (1). In the following part, such an assumption is made that the state transition probability is independent of the emission probability. The local conditional probability of 1 (|,,) iiii pscosis then decomposed into two probabilities: 1 1 1 (,) (|) () ii ii i Ccc pcc Cc - - - = (15) where () Cxis the occurrence times of x in the training corpus. 1 1 1 (,) (|) () ii ii i Css pss Cs - - - = (16) (,,) (|,) (,) iii iii ii Ccos psco Cco = (17) When applying C-MEMM in the Pinyin-to-Character Conversion tasks, the four kinds of local conditional probabilities are first estimated from the training corpus. Then, according to the input pinyin sequence, the probability of a character sequence candidate is calculated by Formula (13). Finally, the most probable character sequence is selected as the conversion results for the input pinyin sequence. Some dynamic programming algorithms can be utilized in the above process, i.e. the Viterbi algorithm. In the remaining part of this section, the probability dependency graph in C-MEMM is presented and an intuitional description is provided on the functions of the four local conditional probabilities. Moreover, since there is rich syntactic and semantic information in word class [Brown et al. 1992 ], C-MEMM used in the Pinyin-to-Character Conversion task can well utilize this additional information to realize further improvement. C-MEMM on Soft Class In the above section, the probability function of C-MEMM is deduced in the case of a hard class in which the state is restricted to only one class. However, in natural language processing tasks, i.e. the Pinyin-to-Character Conversion task, the state of C-MEMM is usually defined as word in the lexicon which usually belongs to multiple classes in nature. For example, part-of-speech (POS) can be taken as a natural hierarchy of word class. Most words possess more than one kind of POS tag. Each POS tag represents a certain syntactic and semantic property of the word. It is beneficial for C-MEMM to exploit all the properties of the word in natural language processing. The section studies C-MEMM in the case of soft class in which the state belongs to multiple classes. The probability function is re-deduced for C-MEMM. In the case of a soft class, there are many class sequences corresponding to one state sequence. In order to calculate the probability of the state sequence, the conditional probabilities of all the class sequences should be summarized. Therefore, it is more complex to deduce the probability function of C-MEMM in the case of soft class than hard class. Similar to the case of the hard class, this section begins the work from calculating the conditional probability of sequential data, presented as below: (|)(,|)(|)(|,) BayesianRule CC PSOPSCOPCOPSCO ==\u01fb \u00e5 . ( 18 ) The decompositions of (|) PCO and (|,) PSCO are exactly the same as those in the case of the hard class, which were presented in the above section. Then, the probability function in the case of soft class can be directly described as below: (|)(|,)(|)(|)(|)(|,)(|) i n iiMEiiiiiii cc i PSOpscopcopccpchpscopss -- = \u00bb\u01fb\u00e5 \u00d5 . ( 20 ) The Viterbi algorithm can be applied to Formula (20) and can find the optimal state sequence of S in a polynomial time complexity. The dependency relationship graph is then described as below: Figure 3. Probability Dependency Graph for the Simplified C-MEMM on Soft Class Hierarchy of State Class There are two ways to get the class of state. One is the statistical method, by which the state class is obtained by the clustering algorithm from the training corpus. However, according to Zip's law, there are always low-frequency or zero-frequency states in the training corpus. Their frequencies are not statistically significant, and they can not be properly clustered by the statistical methods. The other method is getting the class from the pre-defined thesaurus. The hierarchy of class is defined by linguists according to the syntax and semantic information of each state. It can be taken as the well-defined hierarchy of state class. This paper attains the hierarchy of state class in the second way. TongyiciCilin is adopted as the hierarchy of state class in the case of hard class and the set of POS tag is adopted in the case of soft class. The detailed information is presented in Section 4.1. EXPERIMENTS AND DISCUSSIONS This section evaluates C-MEMM in the Pinyin-to-Character Conversion task. First, the data set is described. Second, the experimental results are presented. The performances of C-MEMM evaluated both in the case of hard class and soft class. Third, the conclusion is drawn. Data Set Description This section describes the data set used in the experiments. First, information about the text corpus is presented. Then, the way to get pinyin corpus is described. Finally, the hierarchies of word class are presented. Text Corpus This paper chooses six months of the People's Daily corpus in 1998 as the text corpus in the experiments. The corpus has been annotated by Peking University with the POS tags and the name entities [Yu et al. 2003 ]. It has become the standard corpus in Chinese language processing in recent years [Emerson 2005 ]. There are 46 kinds of POS tag in the POS set. They are listed in Table 1 : Pinyin Corpus The pinyin corpus is necessary for evaluating C-MEMM in the Pinyin-to-Character Conversion task. When C-MEMM is evaluated, the pinyin corpus is first converted into the character corpus by C-MEMM. Then, the conversion results are compared with the standard text corpus and the error rate is calculated. The pinyin corpus is obtained from the text corpus by a conversion toolkit 1 which achieves 99.7% accuracy on a golden pinyin corpus. In the experiments, the errors in the pinyin corpus could lead to the conversion error of C-MEMM. Therefore, the actual error rate of C-MEMM is a little lower than the reported results in this paper. However, there are not many errors in the pinyin corpus because of the high precision of our conversion toolkit. Thereby, the experimental results can be regarded to be close enough to the actual performance of C-MEMM. Hierarchy of Word Class Moreover, word class is necessary for building up C-MEMM in the Pinyin-to-Character Conversion task. The paper gets the hierarchy of word class from the compiled thesaurus which contains the word class information. TongyiciCilin [Mei et al. 1983 ] is adopted as the hierarchy of word class in the experiments of hard class. TongyiciCilin was initially complied in 1982. There were initially 5.38\u00b410 4 words which were organized into a tree structure according to their syntax and semantic information. The structure is shown in Figure 4 . There are a total of four layers in the tree. The word is represented by the leaf node in the leaf layer. The word class is represented by the internal node in the internal layer. There is a road from each leaf node to the root node. On the road, there are several internal nodes of different layers which represent the classes of different scales that the leaf node belongs to. The node in the higher layer represents the class of bigger scale which usually corresponds to a more general concept of Chinese, and vice-versa. Each layer represents a pattern of word class, and the nodes in the same layer describe a way to cluster the words in TongyiciCilin. Moreover, the lower the layer is, the finer the word classes are, therefore the more syntactic and semantic information the layer contains. For example, the 3 rd layer contains more syntactic and semantic information than the 1 st layer does in Figure 4 . Figure 4. Hierarchy of Word Class in TongyiciCilin In recent years, an extended version [Liu et al. 2005 ] of the original TongyiciCilin has been complied. Some infrequent words have been deleted, while some new words have been added. The scale of the lexicon in the new version is up to 7.73 \u00b410 4 . The detailed information is described in Table 3 : This paper adopts the new version of TongyiciCilin in the experiments of hard class. In the experiments of soft cluster, the POS set is a natural choice for the hierarchy of word class. The information of the POS set has been provided in the beginning of this section. Experiments on the Hard Class This section investigates C-MEMM in the case of hard class in the Pinyin-to-Character Conversion task. TongyiciCilin is adopted as the hierarchy of word class. All the words in TongyiciCilin are adopted as the lexicon. The bigram model is taken as the baseline model. The additive smoothing technique is utilized. One order of C-MEMM is evaluated. The combination of Yin i+1 and Yin i+2 From the above feature types, two feature templates are constructed so as to investigate the effectiveness of different feature types in C-MEMM performances. In template one, the size of the context window is set to 3, based on which the model of C-MEMM-1 is constructed. In template two, the size of the context window is set to 5, based on which the model of C-MEMM-2 is constructed. The information is presented in Table 5 : As mentioned above, there are several ways to cluster a word in TongyiciCilin, each corresponding to an internal layer in the tree structure of TongyiciCilin. C-MEMM is built up based on each pattern of word class used separately for each internal layer in TongyiciCilin. The performance of C-MEMM is investigated and the error rates are presented in Table 6 : 6 , it can be found that the error rates of C-MEMM generally decrease from the 1 st layer to the 3 rd layer, which proves that C-MEMM can make good use of the syntactic and semantic information from the word classes and attain further improvement. To draw a conclusion, C-MEMM achieves significant error rate reductions from the ngram model in the Pinyin-to-Character Conversion task by exploitation of pinyin constraints. In addition, C-MEMM makes good use of the syntactic and semantic information in word class and sees further improvement. Experiments on the Soft Class This section evaluates C-MEMM in the case of soft class. The POS set of Peking University is taken as the hierarchy of word class. A word list compatible with the POS set is adopted as the lexicon. Other settings are the same as those in the case of hard class. The experimental results are presented in Table 7 : Table 7. Error Rate of C-MEMM in the case of Soft Class Baseline C-MEMM-1 C-MEMM-2 Error rate (%) 8.37% 6.00% 5.82% Reduction (%) ------28.15% 30.47% The experimental results are similar to the results in the case of hard class. In the remaining part of this section, the performance of the soft-class based MEMM is compared with the hard-class based MEMM. However, the experimental results in this section can not be compared directly with the results in Section 4.2, due to the fact that different lexica were used in the two sections. For fair comparison, a hierarchy of hard class is created from the hierarchy of soft class in this section. It restricts only one POS tag for each word in the lexicon. The most frequent POS tag of that word is adopted in the hierarchy of hard class. The experimental results are presented in Table 8 : Table 8. Comparison between Soft-class based MEMM and Hard-class based MEMM Baseline C-MEMM-1 C-MEMM-2 No class 8.37% ------ ------ Hard class ------ 6.21% 6.17% Soft class ------6.00% 5.82% As shown in Table 8 , the soft-class based MEMM performs better than the hard-class based MEMM to some extent, proving that the soft-class based MEMM can exploit the comprehensive properties of word to achieve better performance. Comparison with Class-based Ngram Model The   The experimental results are similar to those found in Table 9 . The class-based ngram models outperform the traditional ngram model by exploitation of the syntactic and semantic information in word class. However, they underperformed the soft-class based MEMM models because the latter could also make use of the pinyin constraints from pinyin sequence. In conclusion, both the C-MEMM model and the class-based ngram model can make good use of the syntactic and semantic information of word class so as to improve the performance in the Pinyin-to-Character Conversion task; however, the former outperforms the latter by additionally exploiting the pinyin constraints from the pinyin sequence. Related Works To the best of our knowledge, there is no literature that proposes a class expansion to the MEMM model. John Lafferty [Lafferty and Suhm 1996] proposes a cluster expansion to the GIS algorithm so as to train the ME language model efficiently. However, as Lafferty admits, Conclusions This paper aims to improve the performance of the Pinyin-to-Character Conversion system by exploitation of the pinyin constraints from the pinyin sequence. Acknowledgements Natural Language Processing and Speech in China (grant No.01307620). Especially, the authors thank the anonymous reviewers for their valuable suggestions and comments.",
    "abstract": "The Pinyin-to-Character Conversion task is the core process of the Chinese pinyin-based input method. Statistical language model techniques, especially ngram-based models, are mostly adopted to solve that task. However, the ngram model only focuses on the constraints between characters, ignoring the pinyin constraints in the input pinyin sequence. This paper improves the performance of the Pinyin-to-Character Conversion system through exploitation of the pinyin constraints. The MEMM framework is used to describe the pinyin constraints and the character constraints. A Class-based MEMM (C-MEMM) model is proposed to address the MEMM efficiency problem in the Pinyin-to-Character Conversion task. The C-MEMM probability functions are strictly deduced and well formulized according to the Bayes rule and the Markov property. Both the cases of hard class and soft class are well discussed. In the experiments, C-MEMM outperforms the traditional ngram model significantly by exploitation of the pinyin constraints in the Pinyin-to-Character Conversion task. In addition, C-MEMM can well utilize the syntax and semantic information in word class and further improve the system performance.",
    "countries": [
        "China"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "8",
    "year": "2007",
    "month": "September",
    "title": "Exploiting {P}inyin Constraints in {P}inyin-to-Character Conversion Task: a Class-Based Maximum Entropy {M}arkov Model Approach"
}