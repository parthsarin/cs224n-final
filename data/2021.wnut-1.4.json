{
    "framing_question": "If this paper introduces a task or benchmark, what benefits of the task or benchmark does it state, if any?",
    "response": "The paper introduces negative sampling to alleviate the misleading of the incomplete annotated problem in keyphrase extraction.\n\nBenefits of the task or benchmark include:\n- Robustness to incomplete annotated problem\n- Competitiveness with the state-of-the-art method",
    "article": "Extracting keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Supervised approaches to keyphrase extraction(KPE) are largely developed based on the assumption that the training data is fully annotated. However, due to the difficulty of keyphrase annotating, KPE models severely suffer from incomplete annotated problem in many scenarios. To this end, we propose a more robust training method that learns to mitigate the misguidance brought by unlabeled keyphrases. We introduce negative sampling to adjust training loss, and conduct experiments under different scenarios. Empirical studies on synthetic datasets and open domain dataset show that our model is robust to incomplete annotated problem and surpasses prior baselines. Extensive experiments on five scientific domain datasets of different scales demonstrate that our model is competitive with the state-of-the-art method. 1 Introduction Keyphrase extraction is the task of automatically extracting a set of phrases that provide a concise summary of a document content. An effective keyphrase extraction (KPE) system can benefit a wide range of natural language processing and information retrieval tasks (Hasan and Ng, 2014) , such as text categorization (Hulth and Megyesi) , opinion mining (Berend, 2011) and recommendation (Pudota et al., 2010) . Recent supervised neural methods typically treat keyphrase extraction as a classification problem (Augenstein and S\u00f8gaard, 2017; Sun et al., 2020; Xiong et al., 2019) , where given phrases are classified as keyphrases or nonkeyphrases. Although supervised methods perform well in this task, it requires a large amount of labeled data which is extremely expensive and timeconsuming to collect in many application scenarios (Liu et al., 2012) . The latest advances in neural KPE are mainly carried out on data sets in scientific domain datasets. (Augenstein et al., 2017; Meng et al., 2017) . Because scientific domain has sufficient and high-quality training data for neural methods: some training data are annotated by authors, and the author is the person who is most familiar with their articles, the keyphrases annotated by author are high-quality. However, due to the requirements for professional knowledge in scientific domain, some keyphrases annotated by readers are incomplete. Table 1 shows an example case with incomplete keyphrase annotation. Obviously, \"model validation\" is more suitable as a keyphrase than \"paper\", \"model\" and \"use\", but \"model validation\" isn't annotated as keyphrase. It has been suggested that keyphrase annotation is highly subjective (Sterckx et al., 2016) . In real world scenarios, most potential applications of KPE deal with diverse documents originating from sparse sources that are rather different from scientific papers (Xiong et al., 2019) . They often involve various domains whose contents target much wider audiences than scientists and require a large amount of high-quality labeled data which is extremely expensive to collect (Wang et al., 2018) . There are several prior works focused on data quality and insufficient labeled data issues, Sterckx et al. (Sterckx et al., 2016) treat supervised keyphrase extraction as Positive Unlabeled Learning (Ren et al., 2014) by reweighting importance of training samples. Wang et al.(Wang et al., 2018) used the idea of transfer learning and proposed Topic-based Adversarial Neural Network (TANN). They exploited unlabeled data in the target domain and data in the resource-rich source domain to alleviate incomplete annotated problem. However, Table 1 : An example case from Kp20K 2 with incomplete keyphrase annotations, which \"model validation\" is more suitable as a keyphrase than \"paper\", \"model\" and \"use\", but \"model validation\" isn't annotated as keyphrase. title The use of graphical models in model validation abstract The use of graphical models for model specification and in modelling is increasing rapidly. This paper discusses the use of these graphical models in model validation. annotated keyphrases paper,model,use,graphic model transfer learning needs to have a strong correlation between the source domain and the target domain. To overcome the impaction of incomplete annotated problem in keyphrase extraction, we introduced negative sampling to allow unlabeled keyphrases to have opportunities to be considered as keyphrase to participate in the training. Most previous model treated unlabeled keyphrases as negative samples, which will mislead the biased results of model training. In this study, we treat keyphrase extraction task as classification problem. First, we apply pre-trained model RoBERTa (Liu et al., 2019) Related Work In this section, we briefly review two classes of closely related works: keyphrase extraction approaches and learning with noisy data. Keyphrase Extraction Approaches In most traditional existing literature, keyphrase extraction has been formulated as a two-step process (Yuan et al., 2020) . First, lexical features such as part-of-speech tags are used to determine a list of phrase candidates. Second, a ranking algorithm, such as TextRank (Mihalcea and Tarau, 2004) , Multi-Layer perceptron and Support Vector Machine (Lopez and Romary, 2010) , is adopted to rank the candidate list and the top ranked candidates are selected as keyphrases. Because of the similarity with Named Entity Recognition(NER) task, keyphrase extraction is treated as a sequence labeling problem by using IOB tagging scheme (Alzaidy et al., 2019) . Each word in the sentence is labeled as B-tag if it is the beginning of a keyphrase, I-tag if it's inside but not the first one within the keyphrase, or O-tag otherwise. Similarly, span-based models which are popular in NER task, also are utilized in keyphrase extraction task (Mu et al., 2020; Sun et al., 2020) . They treat the spans, instead of single words, as the basic units for labeling. Sometimes keyphrases are absent from the source text, the simple extraction methods mentioned above can only extract present keyphrase. Meng et al.(Meng et al., 2017) first proposed the CopyRNN, a neural generative model that both generates words from vocabulary and points to words from the source text with an encoder-decoder framework. The supervised methods mentioned above have an assumption that the labeled data is completely credible, while the noise in annotated data is ignored. Zhu et al.(ZHU and WU, 2004) suggested that noise in labels tends to be more harmful than noise in features. Learning with noisy data will be introduced in next subsection. Learning with Noisy Data A number of approaches have been proposed to train DNNs with noisy data. One line of approaches formulate explicit or implicit noise layers to characterize the distribution of noisy and true labels (Hedderich and Klakow, 2018) . Another line of approaches use correction methods to reduce the influence of noisy data. Sterckx et al. (Sterckx et al., 2016) reduce the influence of noisy data by reweighting the importance of unlabeled candidate phrases in a two-stage Positive Unlabeled Learning setting. Recently, a few other methods have also been proposed that use noise tolerant loss functions to achieve robust learning, Li et al. (Li et al., 2021) use negative sampling to adjust train loss, avoid training NER models with unlabeled entities. In our work, we treat KPE as classification problem, and introduce negative sampling to alleviate incomplete annotated problem. 3 Problem Formulation Keyphrase Extraction According to the setting of Meng et al. (Meng et al., 2017) , we denote phrases that do not match any contiguous subsequence of a document as absent keyphrases, and the ones that fully match a part of the document as present keyphrases. In this work, we only focus on present keyphrase extraction and treat it as a classification problem. Given a document d = [w 1 , w 2 , \u2022 \u2022 \u2022, w n ], and the present annotated keyphrase set y = {y 1 , y 2 , \u2022 \u2022 \u2022, y m }, where n is the length of document and m is the amount of present keyphrases. For each keyphease y i in the keyphrase set, y i = {w pos i , \u2022 \u2022 \u2022, w pos i +len i \u22121 }, -gram = {{w 1 }, {w 2 }, \u2022 \u2022 \u2022, {w n }}, 2-gram = {{w 1 , w 2 }, {w 2 , w 3 }, \u2022 \u2022 \u2022, {w n\u22121 , w n }}, grams of other lengths are similar, let c N i present the i th N-gram, and then apply a classifier to classify grams into keyphrases or non-keyphrases. Incomplete Annotated problem As mentioned above, keyphrase annotation is highly subjective (Sterckx et al., 2016) and requires the professional knowledge of the annotator, it is difficult to annotate keyphrases. Due to the complexity of keyphrase annotation or the heedlessness of human annotators, some ground truth keyphrase \u0177 of document d are not covered by annotated keyphrase set y, \u0177 / \u2208 y. Methodology Inspired by the effectiveness of negative sampling in the field of NER (Li et al., 2021) , we introduce negative sampling to keyphrase extraction field to solve incomplete annotated problem. CNN-based keyphrase extraction model will be presented firstly, and then we will introduce how to use negative sampling to improve the training process to make the model more robust. CNN-based keyphrase extraction model As shown in fig 2, our model mainly includes two components:(1) A feature extractor for candidate grams via sliding CNN. (2) A binary classifier which can verify whether a gram is a keyphrase. Gram feature Extractor via sliding CNN Inspired by textCNN proposed by Kim (Kim, 2014) , we apply five filters with different kernel size to extract gram features. Let h i \u2208 R k be the kdimensional word embedding corresponding to the i th token in the document. In general, let h i:i+j refer to the concatenation of word vectors h i , h i+1 , \u2022 \u2022 \u2022, h i+j . A convolution operation in- volves a filter w i \u2208 R N k , which is applied to a window of N words to produce a new gram feature. The representation of the i th N-gram c N i is calculated as: g N i = CN N (h i:i+N \u22121 ) (1) Here h i:i+N \u22121 is the concatenation of i th N-gram c N i word embeddings. Five filters are applied to each possible window of words to produce n+(n\u2212 1) + (n \u2212 2) + (n \u2212 3) + (n \u2212 4) candidate grams. Binary Classifier After obtaining the candidate gram representations, we employ a non-linear classify layer to predict Sequence Encoder (RoBERTa) whether a gram is a keyphrase based on it's representation. p(c N i = y N i ) = sof tmax(tanh(W h * g N i + b)) (2) where g N i is the representation of i th N-gram, y N i is the label of whether the gram c N i (w i:i+n\u22121 ) is a keyphrase or not, W h and b are parameters to be learned. Training via Negative Sampling In this work, keyphrase extraction is viewed as a classification problem. Existing models optimize this classification problem by direct cross-entropy function while ignore incomplete annotated problem. Meanwhile, according to statistics(table 2), we can see that the positive and negative samples are extremely unbalanced, one document only has 3 keyphrases on average, but there are close 500 unlabeled grams as non-keyphrases. We introduce negative sampling to adjust training loss. On the one hand, negative sampling lets unlabeled keyphrases have opportunities to be considered as keyphrase to participate in the training. On the other hand, it makes positive and negative samples more balanced. Specifically, we randomly sample a small subset of unlabeled grams as the negative samples. Let C be the candidate grams set, the negative candidate grams set C can be denoted as: C = {c N i |c N i / \u2208 y, 1 \u2264 N \u2264 5, 0 \u2264 i \u2264 n \u2212 N }, (3) based on C, we sample a subset \u0108 from the whole negative candidate grams set. The number of grams in set \u0108 is \u03b1 * | C| , the sampling rate {\u03b1|0 < \u03b1 < 1} is optional in practice, where is the ceiling function, we guarantee that at least one negative candidate gram is sampled. Then we compute the cross-entropy loss with positive labeled keyphrases and the sampled negative candidate grams as flow: L = ( c N i \u2208y \u2212log(P (c N i = 1)))+ ( c N i \u2208 \u0108 \u2212log(P (c N i = 0))) (4) Negative sampling can reduce the risk of training a KPE model with incomplete annotated problem by incorporating some randomness into the training loss. Experiments Datasets We choose scientific domain datasets as wellannotated datasets, use the dataset Kp20k (Meng et al., 2017) for training 3 , which contains a large amount of high-quality scientific metadata in the computer science domain from various online digital libraries (Meng et al., 2016) . Each of which contains a title and an abstract of a scientific publication as source text, and author-assigned keywords as target keyphrases, we follow the original work's partition of training, development and testing sets. We further test the model trained with Kp20k on four widely-adopted keyphrase extraction data sets including Inspec(Hulth and Megyesi), NUS (Nguyen and Kan, 2007) , SemEval-2010 (Kim et al., 2010) and Krapivin (Krapivin et al., 2009) . In this paper, we focus on keyphrase extraction, therefore, only the document of at least one present keyphrase are used for training and evaluation. The statistics on the number of documents, the average number of keyphrases and the average number of word in keyphrase for each benchmark are summarized in Table 2 . Furthermore, in order to explore the performance of our model in real world scenario, we also choose OpenKP (Xiong et al., 2019) as an open domain dataset, OpenKP includes 134K open domain web pages of various topics from search engine Bing. And we randomly remove some keyphrases of training dataset Kp20k with different masking rate r mask to construct synthetic datasets, to simulate poorly-annotated datasets with different degrees of incomplete annotated problem. Baselines and Evaluation Metrics Because keyphrase generation(KPG) can also generate present keyphrase, we compare our model with both extraction and generation approaches. For extraction approaches, we choose two well-known unsupervised algorithms for keyphrase extraction including Tf-Idf (Jones, 1972) , TextRank (Mihalcea and Tarau, 2004) , and also choose SKE-Large-Cls (Mu et al., 2020) , RoBERTa-Chunk (Sun et al., 2020) which published recently. For generation approaches, we compare our models with four supervised algorithms: CopyRNN (Meng et al., 2017) , TG-net (Chen et al., 2019 ), Cat-Seq(Yuan et al., 2020 ), and SGG(Zhao et al., 2021) . Note that, the comparison methods mentioned above are mostly methods for scientific domain. For open domain dataset OpenKP (Xiong et al., 2019) , we choose CopyRNN (Meng et al., 2017) , BLING-KPE (Xiong et al., 2019) and RoBERTa-Chunk (Sun et al., 2020) as comparison models. Following CopyRNN (Meng et al., 2017) and RoBERTa-Chunk (Sun et al., 2020) , we adopt top-K macro-averaged precision, recall and F1 measures as our evaluation metrics for the present keyphrases respectively, K = 5, 10 when evaluating scientific domain datasets, and K = 1, 3, 5 when evaluating open domain datasets. Here, precision is defined as the number of correctly predicted keyphrases over the number of all predicted keyphrases, we apply Porter Stemmer (Porter, 1980) to both target keyphrases and predicted keyphrases when determining whether the predictions are correct; recall is computed as the number of correctly predicted keyphrases over the total number of data records, and F1 is the harmonic average of precision and recall. Implementation Details We set maximal length of source sequence as 510 and maximum N-gram length as five (N = 5). The dimension of word embedding is 768, which obtained by fine-tuning RoBERTa (Liu et al., 2019) . Five sliding CNNs with kernel size between 1 and 5 are used to extract the representation of grams. The dimension of CNN outputs and hidden state in classifier are 512. The sampling rate \u03b1 is set to \u03b1 = 0.1 when training Kp20k, and \u03b1 = 0.05 when training OpenKP. Our model are optimized using Adam with 5e-5 learning rate, 10% warmup proportion, 24 batch size. We implement our model using PyTorch on a Linux machine with a GPU device Tesla V100 32GB. Code and models are available at https://github.com/fredia/NS-KPE. Results And Analysis To reduce the performance randomness, the performance of our model is the average of 3 random runs, we set sampling rate \u03b1 = 0.1 when training on Kp20k, and set sampling rate \u03b1 = 0.05 when training on OpenKP. The best results are highlighted in bold. Kp20k Inspec NUS Krapivin Semeval F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 CopyRNN 0. Because most training data of scientific domain datasets are annotated by authors, and the author is the person who is most familiar with their articles, the issue of incomplete keyphrase annotation is not serious, we treat scientific domain datasets as wellannotated datasets. Table 3 show the F1@5 and F1@10 performance of our model and the other baseline methods across multiple scientific domain datasets, the results of Tf-Idf, TextRank and Copy-RNN are taken from (Meng et al., 2017) , and other reported results are from their corresponding original paper. We can see that for most cases (expect Tf-Idf and TextRank), the extraction approaches outperform all the generation approaches, since extraction is often easier than generation (Wiseman et al., 2017) . As shown in Table 3 , the F1@5 and F1@10 scores of our model are very close to current best extraction results, our model even upperforms RoBERTa-Chunk by even 1% in NUS, these results indicate that our model is still effective when applied to high-quality data. 4 show the F1@1, F1@3 and F1@5 score on an open domain dataset(OpenKP (Xiong et al., 2019) ), the results of CopyRNN, BLING-KPE and RoBERTa-Chunk are token from (Sun et al., 2020) . We can observe that our model has outperformed prior baselines. Compared with RoBERTa-Chunk, we achieve the improvements of 2.5% on F1@1 and 1% on F1@3, and our results is very close to RoBERTa-Chunk on F1@5, this is not surprising since the average number of keyphrases in OpenKP is 1.79. Results on open domain datasets confirm the effectiveness of our model. Results on Synthetic Datasets Results on synthetic datasets are shown in Fig   x axis represents the value of masking rate r mask , the larger r mask , the more serious incomplete annotated problem is. And y axis represents the F1@5 score and F1@10 score. The red line denote the results of sampling rate \u03b1 = 0.1 and the blue line denote no sampling. In sub-figure (c), x axis represents the value of sampling rate \u03b1 and y axis represents the F1@5 score on synthetic datasets, different styles of line denote different synthetic datasets with different masking rate. The sampling rate \u03b1 should not as small as possible, too small sampling rate will reduce the number of negative samples, leading to underfitting. Sampling Rate \u03b1 in Negative Sampling Conclusion and Future Work In this work, we introduce negative sampling to keyphrase extraction model, to alleviate the misleading of the incomplete annotated problem. According to the length characteristics of the keyphrase, we use sliding CNN to extract the representation of the candidate gram, turn the keyphrase extraction problem into binary classification. In different scenarios and different scales datasets, we have confirmed the effectiveness of our model. Our work can be extended in many directions. To begin with, currently binary classifier in our model treats each candidate gram individually, but there is usually a relationship between different grams. We could leverage attention mechanism to model the relationship between grams. Moreover, we would like to further explore utilizing smart gating units or designing a network layer to filter unlabeled keyphrases. Acknowledgements Xingdi Yuan, Tong Wang, Rui Meng, Khushboo Thaker, Peter Brusilovsky, Daqing He, and Adam Trischler. 2020 . One size does not fit all: Generating and evaluating variable number of keyphrases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Jing Zhao, Junwei Bao, Yifan Wang, Youzheng Wu, Xiaodong He, and Bowen Zhou. 2021. SGG: Learning to select, guide, and generate for keyphrase generation. In NAACL. XINGQUAN ZHU and XINDONG WU. 2004. Class noise vs. attribute noise: A quantitative study of their impacts. Artificial Intelligence Review, 22:177-210.",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 4.320199066265573e-07,
        "none": 0.9999876476902904
    }
}