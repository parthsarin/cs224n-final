{
    "article": "We present a phonological probabilistic contextfree grammar, which describes the word and syllable structure of German words. The grammar is trained on a large corpus by a simple supervised method, and evaluated on a syllabification task achieving 96.88% word accuracy on word tokens, and 90.33% on word types. We added rules for English phonemes to the grammar, and trained the enriched grammar on an English corpus. Both grammars are evaluated qualitatively showing that probabilistic context-free grammars can contribute linguistic knowledge to phonology. Our formal approach is multilingual, while the training data is language-dependent. Introduction In this paper, we present an approach to supervised learning and automatic detection of syllable structure. The primary goal of the paper is to show that probabilistic context-free grammars can be used to gain substantial phonological knowledge about syllable structure. Beyond an evaluation of the trained model on a real-world task documenting the performance of the model, we focus on an extensive qualitative evaluation. In contrast to other approaches which work with syllable structures extracted from a pronunciation dictionary, our approach focuses on the probability of use of certain syllable structures. Among other approaches that deal with syllable structure, there are example-based approaches (Hall (1992) , Wiese (1996) , F\u00e9ry (1995) , Kenstowicz (1994) , Morelli (1999) ), symbolic approaches (Belz, 2000) , connectionist phonotactic models (Stoianov and Nerbonne, 1998) , stochastic models describing partial structures (Pierrehumbert (1994) , Coleman and Pierrehumbert (1997) ), or application-based approaches for syllabification (Van den Bosch, 1997) or text-to-speech systems (Kiraz and M\u00f6bius, 1998) . Our method builds on two resources. The first one is a large written text corpus, which is looked-up in a pronunciation dictionary resulting in a large transcribed and syllabified corpus. The second resource is a manually written context-free grammar describing German and English syllable structure. We code the assumptions (similar to Goldsmith (1995) ) that the phonological material that can occur in the onsets or codas might differ depending on the syllable positions: word-initial, word-final, word-medial, versus monosyllabic words. We train the context-free grammar for German on the transcribed and syllabified training corpus with a simple supervised training method (M\u00fcller, 2001a) . The main idea of the training method is that after a grammar transformation step, the grammar together with a parser can predict syllable boundaries of unknown phoneme strings. The trained model is evaluated on a syllabification task showing a high precision on a test corpus. We exemplify that the method can be easily transferred to related languages (here English) by adding rules for missing phonemes to the grammar. In an qualitative evaluation, we compare German and English syllable structure by interpreting the probability weights of the preterminal grammar rules. In sum, we aim to show that our method (i) models all possible words of a language, (ii) models how likely certain structures are used (in comparison to pure dictionary-based approaches), (iii) yields good results in an application-oriented evaluation, (iv) is able to disambiguate competing structures, (v) can be easily applied to other languages, (vi) produces mathematically well-defined models. The paper is organized as follows. We present our method in Section 2, the experiments in Section 3, and our evaluation in Section 4. In Section 5, we discuss the results, and in Section 6, we conclude. Method We build on the novel approach of M\u00fcller (2001a) which aims to combine the advantages of treebank and bracketed corpora training. In general, this approach consists of four steps: (i) writing a (symbolic i.e. non-probabilistic) context-free phonological grammar with syllable boundaries, (ii) training this grammar on a large automatically transcribed and syllabified corpus, (iii) transforming the resulting probabilistic phonological grammar by dropping the syllable boundaries, and (iv) predicting syllable boundaries of unseen phoneme strings by choosing their most probable phonological tree according to the transformed probabilistic grammar. The advantages of this approach are, that simple and efficient supervised training on bracketed corpora can be used (the brackets guarantee that all syllabified words of the training corpus receive only one single analysis), and that raw phoneme strings can be parsed and syllabified after the grammar transformation. Preserving these advantages, our approach differs in several important details. First, we write a more advanced phonological grammar for German, yielding a more fine-grained probabilistic model of syllable structure. Second, it is easily possible to enrich our phonological grammar by adding grammar rules for missing phonemes to adapt our phonological grammar to other languages (here English). Third, in addition to an evaluation on a real-world task (syllabification for German), we qualitatively evaluate the resulting probabilistic versions of our phonological grammar for German and English. 1 : Syllable structure of the word \"Abfall\" ( ! \" ) according to our phonological grammar for German. \u00a3 \u00a3 \u00a4\u00a1 \u00a1 Rhyme.fin \u00a5 \u00a5 \u00a5 \u00a6 \u00a6 Syl.fin Syl ] \u00a7 \u00a7 \u00a7 \u00a7 \u00a7 \u00a7 \u00a7 \u00a7 \u00a7 \u00a7 \u00a9\u00a9 \u00a9 Word Figure Our phonological grammar divides a word into syllables, which are in turn rewritten by onset, nucleus, and coda. Furthermore, the phonological grammar differentiates between monosyllabic and polysyllabic words. In polysyllabic words, the syllables are divided into syllables appearing wordinitially, word-medially, and word-finally. Additionally, the grammar distinguishes between consonant clusters of different sizes (ranging from one to five consonants), as well as between consonants occurring in different positions within a cluster. Figure 1 displays the structure of the German word \"Abfall\" (waste) according to our phonological grammar. In the following sections, we especially focus on the rewriting rules involving phonemic terminal nodes: X# % $ # '& (# % ) 10 2 and Y# % $ 30 4 5# The rules of the first type bear three of the above mentioned features for a consonant 2 inside an onset or a coda (X=On, Cod), namely: the position of the syllable in the word ($ =ini, med, fin, one), the cluster size () 76 98 @# A# A# CB ), and the position of a consonant within a cluster (& D6 E8 @# A# A# FB ). Obviously, vowels or diphthongs 4 of a nucleus (Y=Nucleus) do not need the position and size features () and & ). The probabilities of these phonological rules (after supervised training) are exactly the basis for our description and evaluation of the syllable parts in Section 4. Experiments In the following, we describe two experiments. The first experiment investigates syllable structure for German. In the second experiment, we generalize and apply the method to another language (English). Experiment with German data. First, we manually write a phonological grammar for German consisting of 2,394 context-free rules. If compared to the most successful grammar constructed by M\u00fcller (2001a) , our grammar is enriched with an additional feature: the size of the onsets and codas. Second, we extracted a training corpus of 2,127,798 words (3,961,982 syllables) from a German newspaper, the Stuttgarter Zeitung, and an additional corpus of 242,047 words for testing. All words are looked up in the German part of the CELEX (Baayen et al., 1993) yielding transcribed and syllabified corpora. As phoneme set, we used the symbols from the English and German SAMPA alphabet (Wells, 1997) . In contrast to M\u00fcller (2001a) , we did not investigate smaller training corpora, since we are interested in maximal phonological knowledge about internal word structure. Third, we train the phonological context-free grammar on the training corpus using the supervised method presented in Section 2. Additionally, due to events not occurring in the training data, we use the implemented smoothing procedure of the LoPar system (Schmid, 2000) producing rules with positive probabilities. Experiment with English data. In this experiment, we show that our method can be easily applied to other languages. We create a second training corpus of the same size of 2,123,081 words from the British National Corpus. The words are looked-up in the English part of the CELEX. Furthermore, the context-free grammar is extended by rules for all possible English phonemes. This means, we add preterminal rules for phonemes not occurring in German words, e.g., rules for the apicodental phoneme G TH (appearing in the word this). This (semi-automatic) procedure yields an English phonological grammar consisting of 4,418 rules, which is trained on the new corpus. Evaluation First, we evaluate on a syllabification task for German. Second, we analyze linguistically the errors made by the German phonological parser on the evaluation corpus (word types). Third, and more important, we concentrate on a qualitative evaluation of syllable structure for German and English. Evaluation on Syllabification The resulting probabilistic phonological grammar of German (Section 3) is evaluated on a syllabification task by comparing the maximum-probability parses of all raw phoneme strings in the test corpus (242,047 word tokens, 24,735 word types) with their annotated bracketed variants. As evaluation measure, we used \"word accuracy\" which computes the rate of words with all predicted syllable brackets exactly matching the annotated syllable brackets. The evaluation shows that our phonological grammar for German achieves 96.88% word accuracy on word tokens, and 90.33% on word types. Error Analysis We analyze the results of the German phonological parser on the evaluation corpus consisting of word types. Out of 24,735 words types 2391 words contained wrongly predicted syllable boundaries. We analyze every tenth word of the incorrect words, which means we look at 239 items. There are 243 errors found in the analyzed words. Due to the fact that there can occur more than one error in one word, the number of errors is higher than the number of items. We find that 72.42% of the errors are made for consonants uncorrectly assigned to the onset, whereas only 27.57% errors are made for consonants wrongly assigned to the coda. The tendency that more errors occur in predicting the onset agrees with the findings of M\u00fcller (2001b) . The main errors appear when a G tH , G RH , or an G nH is predicted to be an onset consonant. The main errors found in predicting the coda consonants mainly occur with G kH , G sH , and G pH . If we investigate the errors made on the linguistic level, the most frequent error occurs with word boundaries (98 cases). However, a further error source is found with syllable boundaries occuring in conjunction with prefixes and suffixes. The most frequent error appears with syllable boundaries after prefixes like /ver-/, /er-/, and /un-/, whereas with suffixes the most frequent error appears with /-lich/. Foreign words, which are subject to different phonotactic constraints, seem to be a minor source of errors (20 cases). Thus, we can see that most of the errors are found in conjunction with morphological entities. This might point out that a further morphological level could help to disambiguate syllabification alternatives. Qualitative Evaluation The evaluation is carried out for both English and German. First, we compare the complexity of words, syllables, and syllable parts. Second, we analyze the probabilities of grammar rules involving phonemic terminal nodes. Unfortunately, due to space constraints and the large size of our derived probabilistic phonological grammars, only preliminary results can be presented. Table 1 displays the occurrence frequencies of onsets and codas (of different size and different syllable position), counted on the basis of the training corpora for English and German (see Section 3). The following three complexity analyses are carried out on the basis of this table. Word complexity. German words tend to be more complex than English words. The German training corpus comprises 48.7% 1 monosyllabic words, whereas 67.4% are found in the English training corpus. The high frequency of occurrence of monosyllabic words justifies the separate treatment of those words. Syllable complexity. German syllables usually consist of onset and rhyme. An onset is observed in initial syllables (73%) 2 , in medial syllables (94%), in final syllables (94%), and in monosyllabic words (73%). A coda is found in initial syllables (42%), in medial syllables (34%), in final syllables (78%), and  in monosyllabic words (83%). English syllables. An onset is observed in initial syllables (72.2%), in medial syllables (94%), in final syllables (95%), and in monosyllabic words (68.3%). A coda is found in initial syllables (29.7%), in medial syllables (21.2%), in final syllables (82.3%), and in monosyllabic words (69%). Onset and coda complexity. English and German syllables prefer simple onsets and codas. English onsets. For both initial and medial syllables, a single consonant is found (80%), two consonants (18%), and three consonants (less than 1%). For both final syllables and monosyllabic words, one consonant is observed (85%), two consonants (13%), and three consonants (less than 0.9%). German onsets. For both initial and medial syllables, one consonant is found (82%), two consonants (15%), and three consonants (1%). For both monosyllabic words and final syllables, a single consonant is found (90%), two consonants (7%), and three consonants (less than 1%). English codas. For both initial and medial syllables, one consonant is observed (95%), and two consonants (5%). For both final syllables and monosyllabic words, one consonant is found (77%), two consonants (20%), and three consonants (2%). German codas. For both initial and medial syllables, one consonant is observed (88%), two consonants (10%), and three consonants (about 1%). In final syllables, one consonant is found (82.8%), two consonants (15.2%), three consonants (1.7%), and four consonants (0.08%). In monosyllabic words, one consonant occurs (74.6%), two consonants (23.1% ), three consonants (1.8%), and four consonants (0.3%). Nuclei. Table 2 displays the nuclei found for English and German. The symbol \"-\" indicates that the phoneme has not been observed in the training corpus. However, the corresponding grammar rules receive a (very small) positive probability according to our smoothing procedure. Note, we do not display those phonemes in the tables which are marked with \"-\" for all positions . Moreover, the symbol \"Q SR T# UR VR W8 \" indicates a probability of less than 0.001 (which means a occurrence frequency of less than 0.1%). English nuclei. The most likely nuclei in initial syllables are G @, I, E, O, &, VH (16.7%, 16.6%, 12.6%, 7.2%, 7%, 6.9%), in medial syllables G I, @, E, eIH (28.2%, 26.8%, 10.5%, 7.7%), in final syllables G @, IH (38.6%, 36.7%), and in monosyllabic words G @, I,&, O, eI, u:H (22%, 17.1%, 9.5%, 8.2%, 6.9%, 6.4%). Furthermore, in monosyllabic words, 33.6% of the nuclei are long vowels/diphthongs, 29.1% in initial syllables, 23.6% in medial syllables, and 18% in final syllables. German nuclei. In initial syllables, the most probable nuclei are G E, a, aI, @, e:, a:, I, o:, i:, O,H (13.%, 10.7%, 10.5%, 8.2%, 7.5%, 6.6%, 6.4%, 6.1%, 5.4%, 5.1%), in medial syllables, G @, i:, I, a, E, a:, e:, o:H (18%, 13.5%, 12.9%,9.2%, 9.1%, 5.6%, 5.4%, 5.2%), in final syllables G @, I, U, aH (69.3%, 4.9%, 4.4%, 3.6%), and in monosyllabic words G e:, I, i:, a, U, E, aI, O, aUH (17.1%, 16.3%, 12.4%, 10.3%, 8.2%, 6.1%, 6%, 5.2%, 5.1%). Generally, we can observe that long vowels/diphthongs are more likely in monosyllabic words (52.6%) than in all other syllable positions (48.3% in initial syllables, 42% in medial syllables, and 13.4% in final syllables). Mono-consonantal onsets and codas.  , v, g, b,z, m, d, k, hH , (11.5%,   11.5%, 9.8%, 9.6%, 7.8%, 7.8%, 7.3%, 6.4%, 6.2% ), in medial syllables G l, t, g, n, R, d, zH (12.1%, 11.6%, 10.3%, 7.8%, 7.4%, 7.2%, 6.9%), in final syllables G t, n, d, R, lH (18.4%, 12%, 10.6%, 8.2%, 7.6%), and in monosyllabic words G d, z, f, n, mH (45.4%, 10.7%, 9.9%, 7%). English onsets. In initial syllables, the most probable consonants are G s, k, r, m, d, pH (11.4%, 11.3%, 9.8%, 8.7%, 8.4%, 8%), in medial syllables G t, s, l, n, r, vH (12.5%, 9.8%, 9.7%, 9.1%, 8%, 8%), in final syllables G t, l, d, S, s, rH (18.5%, 9.5%, 7.5%, 7.3%, 6.7%, 6.5%), and in monosyllabic words G D, t, w, b, hH (25%, 10%, 10%, 6%, 6%). German codas. In initial position, the most likely consonants are G R, nH (35.6%, 26.9%), in medial syllables G R, nH (31.5%, 28.7%), in final syllables G n, RH (50.1%, 18.3%), and in monosyllabic words G n, R, s, xH (27.3%, 27.1%, 15.6%, 9.6%). English codas. In initial syllables, the most dominant consonants are G n, k, mH (43.5%, 11.8%, 11.1%), in medial syllables G n, k, lH (42.6%, 17.6%, 10.5%), in final syllables G r*, n, N, l, z, dH (17,8%-10%), and in monosyllabic words G n, t, z, r*, fH (16.8%-11%). Onset and coda clusters. Clusters of 2-3 consonants are displayed and described in Table 5 and  Table 7 (onsets), and in Table 6 and Table 8 (codas). Due to space constraints, we omitted to display clusters of 4-5 consonants, but our analysis can be found elsewhere (M\u00fcller, to appear 2002) . Clusters with more than 5 consonants have not been found in our corpora. Furthermore, for German, no onsets comprising 4 consonants, and for English, no codas occur comprising 5 consonants. Last, for German, there is only one consonant cluster G RnstsH appearing in words like \"Ernsts\", the genitive case of the proper name \"Ernst\". Discussion As M\u00fcller (2001a) , we presented a method for prediction of syllable boundaries using phonological probabilistic context-free grammars for German. However, our approach performs slightly better (96.88% word accuracy on word tokens versus 96.49%). Van den Bosch (1997) reports a word error rate of 2.22% on English syllabification using inductive learning. Due to the feature \"cluster size\", which was not used by M\u00fcller (2001a) , we are able to give an extensive qualitative evalu-ation of syllable structure considering syllable positions, as well as the complexities of consonant clusters, and the position of a consonant within a cluster. Since our approach is multilingual (only training is language dependent), we evaluated two languages (German and English) showing that probabilistic context-free grammars add linguistic knowledge to phonology. In contrast to theoretical approaches, we focus on syllable structures that are preferred in a certain language. Theoretical phonotactic approaches like (Hall (1992) , Wiese (1996) , F\u00e9ry (1995) ) describe possible syllable structures or German, and Kenstowicz (1994) , Morelli (1999) for English. There are many more approaches dealing with syllable structure. For instance, Kiraz and M\u00f6bius (1998) develop multilingual syllabification models on the basis of a pronunciation dictionary. Partial English syllable structure is described by Pierrehumbert (1994) , who also used a dictionary. A more general model was introduced by M\u00fcller et al. (2000) , who used a clustering algorithm to induce English and German syllable classes. However, the approach treats the onsets and codas as one string. In our method, we describe in more detail the internal structure of onsets and codas. Our model has several advantages. (i) We believe that the syllable structure of all words occurring in a certain language can be described by an elaborated context-free grammar. (ii) Moreover, using probabilistic context-free grammars, alternative syllabifications of phoneme strings can be disambiguated. (iii) Our model is able to analyze unexpected phoneme strings. For instance, the onset G bRdH of the proper name \"Brdaric\" (G bRda:RItSH ) is not allowed according to German phonotactics. Table 7 correctly displays that neither G bH occurs as a first, nor G RH as a second, or G dH as a third consonant in initial triconsonantal German onset clusters. Due to the smoothing procedure, our model syllabifies this name as G bRda:H XG RItSH although the onset G bRdH has never been observed in the German training corpus. (iv) The syllable structure of nonsense words can be predicted. The model can be exploited in two ways: first, it predicts the most probable syllabification, second, it can be used to model the lexical decision task. An example of English words is mentioned by Pierrehumbert (1994) . She compared \"bistro\" (G bIstr@UH ) as a possible word, with a good word \"bimplo\" (G bImpl@UH ), and a bad word \"bilflo\" (G bIlfl@UH ). The four possible syllabifications are G bImH XG pl@UH , G bIH XG mpl@UH , G bImpH XG l@UH , and G bImplH XG @UH . The most probable syllable structure for \"bimplo\" is G bImH XG pl@UH (1.8403e-13). For the twosyllabic word \"bilflo\", the model assigns the highest probability to a syllable boundary between G lH and G flH . Out of the four possible syllabifications for the real word \"bistro\" G bIstrH XG @UH is the most probable syllable structure. Although the triconsonantal cluster should be rather an onset cluster than a coda cluster, the model prefers G strH as a cluster, whereas a syllable boundary is added between G mH and G plH , and G lH and G flH . A further example mentioned in the literature is G brIkH , G blIkH , and G bnIkH . The first one is a possible word of English G brIkH , which receives a probability of 1.16e-08, the second non-occurring word G blIkH (7.2391e-09), and the non-English word G bnIkH (4.3249e-09). The least probable one is the non-English word, followed by the non-occurring one, and the highest probability is assigned to the real word brick. Beside the good performance of the current models in applications, further improvements of the present approach can possibly be achieved by embedding more prior phonotactic knowledge. For example, it might be useful to model the distribution of a consonant dependent on the previous one (Menzel, 2001) . In future work, we will investigate this issue by using head-lexicalized probabilistic context-free grammars, like those suggested by Carroll and Rooth (1998) , where the consonant cluster G StRH would be analyzed as: X.r.1.3 Y acb S X.r.2.3 Y ed Fb t X.r.3.3 Y gf hb R Here, the lexical choice events express the desired phonotactic feature. Moreover, it would be interesting to incorporate the stress feature. Our linguistic evaluation of the errors point out that most errors of the phonological parser occur in conjunction with morphological phenomena like prefixes, suffixes and word boundaries. This might point out that a further morphological layer could improve word accuracy (see also Meng (2001) ). Conclusion We introduced a multilingual approach to probabilistic modeling of syllable structure using probabilistic context-free grammars. We exemplified our approach for two languages, German and English. Evaluation on a syllabification task shows a small improvement in word accuracy rate compared to other state-of-the-art systems for German. Additionally, we presented an extensive qualitative evaluation of German and English syllable, onset, and coda structure (section 4) showing which structures are preferably used. However, the presented work is a starting point of analyzing in detail the huge amount of data with regard to phonological regularities. For instance, we found evidence that in consonant clusters sonorous consonants (like G RH , G lH ) are more restricted to appear next to the nucleus than less sonorous consonants. Clearly, this is promising work for future investigations. We believe that our method can be easily transferred to a variety of other languages, and aim in future work at embedding more fine-grained phonotactic constraints to our grammar. , k, p, t, d (21.6%, 16.4%, 13.8%, 11.9%) , and the second consonants j, r, t, l, Z (25.8%, 24.9%, 11.6%, 9.7%, 8.8%). In final syllables, the most probable first consonants are d, t, s , (21.6%, 21.2%, 20.3%), and the most probable second consonants are r, t, Z, l, j, S (30.9%, 14.9%, 11.5%, 11.3%, 11.2%, 8.7%). Monosyllabic words favor on the first position h, s, f, d, t, p , (19.2%, 16.8%, 13.2%, 9.3%, 8.5%, 7%), and on the second position R, w, l, t (34.1%, 21.6%, 12.6%, 7.9%). The consonants which only appear as first consonants are S,b,d,g,k , and as second consonants R,Z,j,l,n . The consonants which are restricted to the first position are less sonorous than the consonants restricted to the second position. This observation corresponds with the assumption that the sonority increases towards the nucleus. German: In initial syllables, the first consonants are S, t, p, f, g, k (26.1%, 22.8%, 15.5%, 10%, 9.4%, 8.7% with descending probability), and the second consonants are R, s, t, l (42.8%, 19%, 14%, 12.6%). In medial syllables, the first consonants are t,S (45.3%, 22.9%), and the second consonant are s, R, t, l (36.7%, 25.7%, 16.9%, 13.8%). In final syllables, the first consonants are t, S, s (46.3%, 15.3%, 11.6%), and the second consonants are s, t, R, l (36.6%, 22%, 16.6%, 12.9%). In monosyllabic words, the most probable first consonants are i qp sr T (49.2%, 21.8%), and the second consonants are s, R, t, l (45.7%, 22.5%, 14.3%, 10.5%). Summarizing, the most probable first consonants are S, t, p, f, g, h , and the most probable second ones are s, R, t, l .  English: In initial syllables, the most likely first consonants are k, n, m (27%, 26.6%, 13.7%), and the second consonants are s, p, t, d, z (39.3%, 14%, 12.4%, 10.7%, 10%). In medial syllables, the most prominent first consonants are n, d (67,8%, 16.1%), and the second consonants are t, Z (58,4%, 22.4%). In final syllables, the most dominant first consonants are n, l, s (50%, 10%, 8%), and the most dominant second ones are t,z,d,s (37.9%, 22.3%, 17.2%, 14.9%). In monosyllabic words, the most likely first consonants are n, t, s, l (42.9%, 14.7%, 11%, 8.7%), and the second consonants are d, t, s, z, S (40%, 20%, 14%, 10%, 8.7%). Generally, n is the most dominant first consonant, whereas t is the dominant second consonant in medial, and final syllables. German: In initial syllables, the most likely first consonants are n, R, t (37.8%, 20.5%, 12.5%), and the second consonants are t, s (55.7%, 18.1%). In medial syllables, the first consonants are n, t, R, N , (33.6%, 15%, 14.5%, 13%), and the second consonants are t, s (48.7%, 35.7%). In final syllables, the first consonants are mainly R, n, l, x (29.2%, 20%, 14.7%, 12.9%), and the second ones are t, s, n (66%, 16.6%, 12.5%). In monosyllabic words, n, R, x, l, s are the most probable first consonants (40.5%, 13.1%, 12.5%, 11.8%, 11.8%), and the second consonants are t, s (80%,10.5%). In general, the first position is more variable than the second one. Despite this fact, the most dominant first consonant is n , whereas the second position is mainly restricted to t,s . For English, the consonants occurring as third consonants are more sonorous than all other ones ( r,j,l,w ), which also applies for German ( v,R ). The sonority decreases in the direction to the syllable edge, which corresponds to the sonority sequencing principle.  It is remarkable that in English initial and medial syllables, triconsonantal coda clusters are avoided. A similar tendency can be observed for German. Moreover, sonorous consonants rather occur next to the nucleus (for English n,N,l , for German R,n ).",
    "abstract": "We present a phonological probabilistic contextfree grammar, which describes the word and syllable structure of German words. The grammar is trained on a large corpus by a simple supervised method, and evaluated on a syllabification task achieving 96.88% word accuracy on word tokens, and 90.33% on word types. We added rules for English phonemes to the grammar, and trained the enriched grammar on an English corpus. Both grammars are evaluated qualitatively showing that probabilistic context-free grammars can contribute linguistic knowledge to phonology. Our formal approach is multilingual, while the training data is language-dependent.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German",
        "English"
    ],
    "numcitedby": "15",
    "year": "2002",
    "month": "July",
    "title": "Probabilistic Context-Free Grammars for Phonology"
}