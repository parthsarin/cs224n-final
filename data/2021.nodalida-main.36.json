{
    "framing_question": "If this paper introduces a task or benchmark, how does the paper propose that performance be measured, if at all?",
    "response": "The paper proposes to measure performance by comparing the absolute differences in word start and end boundaries between the aligner's results and the gold standard boundaries, and then calculating the percentage of differences that fall within various ranges (10, 25, 50, and 100 milliseconds).",
    "article": "Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are languagedependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments. Introduction Matching speech signal and its orthographic transcription is a necessary first step for many research questions in linguistics (Yuan et al., 2018; Olsen et al., 2017; DiCanio et al., 2013) . For wellresourced languages, manually aligned corpora exist, providing an easy starting point for linguistic research. For under-resourced languages such corpora are rare, and for all languages new corpora are continuously studied. In these situations, the researcher needs to complete this task before any actual research can begin. Forced alignment, i.e., automatically matching text to speech using automatic speech recognition (ASR), is widely used, and tools that can accomplish this automatically exist, such as FAVE (Rosenfelder et al., 2011) , Prosodylab-aligner (Gorman et al., 2011) , MAUS (Kisler et al., 2017) , and Montreal Forced aligner (MFA) (McAuliffe et al., 2017) . If the researcher is studying a language that is supported by an existing tool for forced alignment, learning to use it will be beneficial, since manual segmentation is much more arduous than transcription (Jarifi et al., 2008) . However, the effort for this necessary, but often uninteresting step increases tremendously if no suitable model exits. The reason may be that the target data is out-ofdomain of what the acoustic model was trained with, or the target language is under-resourced and there is no model available at all. Some aligning tools do not support retraining models. For others, such as FAVE and Prosodylab, the model has been trained with a known ASR framework, here HTK (Young et al., 2002) , and the researcher could use the framework to train their own models. However, at this point it would be more straightforward to use the ASR framework itself. In addition to all of this, the technical knowledge required to train an acoustic model with minimal or difficult data is formidable. MFA provides ample documentation, and has a user friendly wrapper over Kaldi (Povey et al., 2011) , a popular speech recognition framework. It gives users the option to retrain the model to fit their own data, and add new languages. Gonzalez et al. (2018) used MFA to experiment on iterative forced alignment, and how it compared to the traditional linear method. Even though they used a ready-made tool, the effort to try two alignment methods on an under-resourced language was enough to qualify as a research paper on its own right. For a linguist, who might not have technical expertise on ASR, this may be intimidating as the first step. An alternative solution to the task of training new models is cross-language forced alignment, in which an aligner trained with a different language than the speech and transcriptions to be aligned, is used. In this paper we introduce a new wordlevel forced alignment tool based on Kaldi. We show that this very simple command line tool can align closely related languages, is robust against speaker variability without any fine-tuning, and can even adequately align linguistically very dissimilar languages. This paper shows the first results for cross-language forced alignment involving Finnish. In addition, using the tool we force-aligned a Northern S\u00e1mi corpus without proper word alignments with very little expert knowledge of the language. 2 Related research Forced aligners In their paper (McAuliffe et al., 2017) , the designers of MFA compared their tool to FAVE and Prosodylab. The latter tools are based on monophone models, while MFA utilizes triphones, and adds speaker adaptation to the process. A central underlying difference is that, similar to us, MFA uses Kaldi as the speech recognition framework. However, MFA uses Gaussian mixture models (GMM), popular in speech recognition before deep neural networks (DNN), while our tool uses the modern machine learning methods trained with Kaldi's lattice-free maximum mutual information cost function (Hadian et al., 2018 ). Another Kaldi-based tool is Gentle 1 , which also uses DNNs. Munich AUtomatic Segmentation system (MAUS) is a popular aligner based on its own speech recognition framework, utilizing a statistical expert system of pronunciation. Cross-language forced alignment Forced alignment has also been successfully used across languages, e.g., when the target language does not have enough transcribed data. This task is called cross-language or cross-linguistic forced alignment (CLFA), sometimes untrained forced alignment. Kempton et al. (2011) used their own phonetic distance metric to evaluate the accuracy of three phoneme recognizers on isolated words from under-resourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013) , tools trained on English were used to align isolated words from Yolox\u00f3chitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012) , where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent 'sampa' version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014) . Later Jones et al. (2019) compared MAUS' language-independent and Italian versions for conversational speech in 1 https://github.com/lowerquality/gentle Kriol, finding that the Italian version surpassed the language-independent one. A unifying method was presented by Tang and Bennett (2019) , who combined a larger source language and the target language with MFA to train the aligner. Finally Johnson et al. ( 2018 ) reviewed previous CLFA research and experimented on the minimum amount of data necessary for language dependent forced alignment, achieving good results with an hour of transcribed speech. Experiments We evaluate our Kaldi-based aligner on related and unrelated languages, with a small amount of expert knowledge added to grapheme-to-phoneme mapping. We also experiment on speaker variation. This is the first time either has been done in CLFA literature. The code and tool used in this paper are publicly available. 2 Kaldi pipeline Our method uses Kaldi to force-align transcibed audio. As is customary in Kaldi when aligning speech with neural networks, we employ 39 dimension Mel-frequency cepstral coefficients (MFCCs) and Cepstral mean and variance normalization (CMVN). Kaldi's i-vectors are used for speaker adaptation. The original Finnish acoustic model and i-vector exctractor are the same as in (Mansikkaniemi et al., 2017) . After the feature generation we create a dataset-specific dictionary from all the words in the transcription. The orthography is assumed to be phonetic, so the words in the lexicon are composed of their graphemes, which are mapped to closest Finnish match manually by non-experts. Smit et al. (2021) show that with DNN-based acoustic models, the assumption of phonetic orthography works reasonably well even for a language like English. As a final preparation for alignment Kaldi uses the lexicon, acoustic model and transcripts to create dataset-specific finite state transducers. Datasets We first evaluate the model on Finnish data using manually annotated Finnish read speech from one male speaker (Vainio, 2001; Raitio et al., 2008) . We use Pympi (Lubbers and Torreira, 2013-2015) to prepare the data. Here the grapheme-tophoneme mapping is one to one due to Finnish being a phonetic language. For experimenting on speaker variability and CLFA, we align nine Estonian speakers with data gathered from the corpus of lecture speeches introduced in (Meister et al., 2012) . For each speaker we have little over 15 minutes of speech, much less than the recommended hour by Johnson et al. (2018) . We create a rough mapping between Estonian graphemes and Finnish phonemes, which is a straightforward task as the languages are closely related. We also evaluate our model on Northern S\u00e1mi, by forcealigning the Giellagas corpus (Kielipankki, 2014 (Kielipankki, -2017)) . Since there are no accurate word boundaries for the dataset, we use ELAN (Wittenburg et al., 2006) to manually annotate roughly 20 seconds of speech from 11 native speakers to compare to our automatically generated boundaries. The annotations should be considered only approximative, as the recorded speech has poor quality and the annotator did not know the S\u00e1mi language. For Northern S\u00e1mi, we use the grapheme-to-phoneme mapping introduced by Leinonen (2015) . While most of CLFA papers use closely related or otherwise similar languages, we also try to align English speech with our Finnish model using the clean test sets from Librispeech corpus (Panayotov et al., 2015) . For the lexicon we map the graphemes e, and y to Finnish i, and a to \u00e4, otherwise assuming one-to-one mapping. For all datasets, we follow McAuliffe et al. (2017) , and compare what percentage of absolute differences in word start and end boundaries are inside the ranges 10, 25, 50 and 100 milliseconds, when comparing the aligner's results to the gold standard boundaries. Since we do not have manual alignments for the English and Estonian datasets, we align the audio with languagedependent acoustic models and use the predicted boundaries as gold standards. For Estonian this is done with a dockerized Estonian aligner 3 . The Librispeech datasets were aligned with an acoustic model trained with Kaldi Librispeech recipe 4 . We use the final GMM-based model called tri6b to create the word boundaries. We also experiment with other triphone models trained with the Librispeech recipe, varying in the amounts of training data, and model complexity, to test what improve-ments the advances in triphone models bring, and how well our Finnish model compares to language dependent models. Table 1 Results The Finnish alignment results in et al., 2005) . This seems reasonable since both are using Kaldi. The different amounts of smaller boundary errors might be due to audio quality, speaking style or method of annotation. For instance the Finnish dataset was more focused on phoneme labels than word boundaries. Model Dataset <10 <25 <50 <100 Finnish Finnish 0.21 0.55 0.84 0.98 MFA Buckeye 0.33 0.68 0.88 0.97 Dataset <10 <25 <50 <100 Giellagas 0.12 0.26 0.45 0.62 Table 4 : Cross-language forced alignment for Northern S\u00e1mi: word boundary accuracy using a part of the Giellagas corpus. The results for Northern S\u00e1mi in Table 4 are not as good as for Estonian, with some of the possible reasons listed in Section 3.2. With closer inspection of the differences between manual and forced alignment, it could be argued that the automatic method is more accurate. It is definitely much faster, being seconds instead of taking hours. Dataset <10 <25 <50 <100 dev-clean 0.12 0.30 0.51 0.68 test-clean 0.12 0.30 0.51 0.67 Table 5 : Cross-language forced alignment for English: word boundary accuracy using Librispeech datasets. The results for English in Table 5 are weaker than for any other target language, with the largest 100ms range having the same results as 25ms range for Estonian. While any researcher who needs to align English speech naturally has language-dependent models, this demonstrates the worst case scenario for CLFA, with multiple wrong assumptions including rough grapheme-tophoneme mapping, and even using phonetic orthography. If there is very little target speech, using an unrelated source language might be more cost effective than trying to train a new model or manual alignment. Model <10 <25 <50 <100 tri1 0.55 0.87 0.97 1.00 tri2b 0.65 0.93 0.98 1.00 tri3b 0.72 0.95 0.99 1.00 tri4b 0.80 0.97 0.99 1.00 tri5b 0.88 0.99 1.00 1.00 Table 6 : Librispeech word boundary accuracy with different English HMM-GMM models trained with Librispeech recipe. Dataset is devclean, using tri6b as a gold standard. The authors of MFA hypothesize the effects of using different phone models, speaker adaptive training and other methods in (McAuliffe et al., 2017) . Also to give context to the Finnish-English results, we experimented on how simpler ASR models might perform at the task. Table 6 show that improving the basic model underneath does improve the results for the smallest ranges, and that a much simpler language-dependent model is much better than results with cross-language alignment. Future work Most of the papers in related research use some tool to automatically generate a phoneme-based lexicon for the target language. These lexicons do contain errors, so we have evaluated our results with word boundaries, since the words can be extracted as is from the transcription. However, automatic phoneme mapping would be an interesting next step, and allow better comparison with previous research effort in this multidisciplinary field. Conclusion We have demonstrated promising results for crosslanguage forced alignment using Finnish acoustic model for related and unrelated languages. We have shown that its results for Finnish in languagedependent use are comparable to state-of-the-art aligners for English data. In addition, we present promising results with related and unrelated languages. We also showed the effects of speaker variation in cross-language situations, demonstrating that retraining speaker dependent models is generally not necessary. We share our tool as an easy to use Docker image. Acknowledgments We acknowledge the computational resources provided by the Aalto Science-IT project. SV was supported by the FoTran project, funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement \u2116 771113).",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.0039458080606245005,
        "foundation": 0.9999791843696483,
        "none": 0.0
    }
}