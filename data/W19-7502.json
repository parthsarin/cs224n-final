{
    "article": "Sentence parser is an essential component in the mechanical analysis of natural language texts. Building a parser for Sanskrit text is a challenging task because of its free word order and the dominance of verse style in Sanskrit literature in comparison to prose style. In this paper, we describe our efforts to build a parser which parses both prose as well as verse texts. It employs an Edge-Centric Binary Join method using various constraints following traditional rules of verbal cognition. We also propose a Da\u1e47\u1e0da-anvaya-janaka which converts the parsed verse form to its canonical prose order. Introduction Parsing natural language sentences automatically to reveal the underlying semantics has attracted many researchers to this field in the past two decades. The parse of a sentence is useful for several applications ranging from machine translation, information retrieval to question answering. Parsing sentences with fixed word order is comparatively easier than parsing texts that show some flexibility in the word order. We come across such flexibility in poetry. The syntax and semantics of poems have been an area of serious studies. Delmonte (2018) studies the syntax and semantics of Italian poetry. He observes that the best parsers for Italian based on statistical probabilistic information fail to parse poetic structures while the rule based system performs well. Lee and Kong (2012) have noticed the importance of treebank for poems in order to use the statistical or machine learning models, and have developed a dependency treebank for Classical Chinese poems. The Stanford Dependency relations were extended in order to account for certain poetic constructs in Chinese. Krishna et al. (2019) proposed a model, called k\u0101vya guru, for the conversion of Sanskrit sentences in verse to prose form, which considers the task of conversion as a linearisation problem. It first uses-Dynamic Meta Embeddings (DME)-for training, where it forms a single meta embedding from multiple pretrained word embeddings of a given token. Then it uses a linearisation model-Self-Attention Based Word Ordering (SAWO)-which generates multiple permutations of words, which are then sent to a seq2seq model that produces the required prose order form. They compared the performance of their system with an LSTM based Linearisation Model, and seq2seq model with Beam Search Optimisation, and their system performs the best with a BLEU score of 55.26. Majority of Sanskrit literature is in verse form. These verses follow metrical patterns which make them easy to memorise. The metrical pattern also brings in deviation from the default word order found in the prose. This makes it difficult to understand the verse without any special training. Sanskrit being a flexional language, and also rich in derivational morphology, enjoys the flexibility in the word order. There is, as well, a natural tendency to have a kind of rhythm even in the normal speech in Sanskrit, which results in the deviation from normal word order. Gillon (1996) reports several cases of dislocations of arguments from their default order even in prose. This flexibility, however, makes parsing such texts a bit challenging. In this paper we describe a parser for Sanskrit that can parse both verse and prose. In the next section we describe the basic architecture of our parser that extracts a tree from a graph satisfying some local and global constraints. In the third section we provide the algorithm for constraint solver and illustrate it with an example. Next two sections describe an application of this parser to get the prose order (also termed da\u1e47\u1e0da-anvaya) of any verse. We conclude with the discussion on the performance of the parser stating its limitations and the areas where it needs further improvement. Design of a Parser We find two main approaches towards the design of a dependency-based parser. They are Grammar based and Data driven. The Link parser (Sleator and Temperley, 1993) based on Link grammar formalism and the Minipar (Lin, 1998) based on Chomsky's Minimalism are among the grammar based dependency parsers. Data-driven dependency parsers are the state-of-art parsers. They use supervised machine learning algorithms to train the machine on annotated corpus. These parsers need manually annotated corpus, called tree banks, for training. Among these parsers, we come across two dominating approaches. They are graph-based dependency parsing and transition-based dependency parsing. The graph-based approach creates a parser model that assigns scores to all possible dependency graphs and then uses maximum spanning tree methods from Graph theory for getting the highest-scoring dependency graph. The transition-based approach scores transitions between parser states based on the parse history and then follows a greedy approach and produces a single parse corresponding to the highest-scoring transition sequence that derives a complete dependency graph. Most of the natural language parsers call a part of speech(POS) tagger and a chunker before invoking a parser. These two modules reduce the ambiguity due to multiple morphological analyses. A POS tagger selects the best part of speech in the context, and a chunker groups all the auxiliary verbs with the main verbs, the post-positions with the noun, and multi-word expressions as one chunk. The head of such chunks is marked which relates to other words or heads of other chunks in a sentence. The POS taggers and chunkers ease the task of a parser, by reducing the ambiguities at the morphological level. However the disadvantage of calling these modules before a parser is that the errors may get cascaded. Our parser differs from the state-of-the-art parsers in three ways. First, in the absence of any annotated corpus, we follow the grammar based approach. Secondly, our parser is invoked right after the morphological analyser. The main reason behind this decision was the following. Indian literature on verbal import was found to be useful from parsing point of view since it has discussions on various factors that are instrumental in the process of verbal cognition. Our main goal is to build a parser modeling the theories of \u015b\u0101bdabodha. When we looked at various Indian literature related to the theories of verbal cognition, there was no discussion on any kind of POS tagger or chunker. Moreover, use of chunker also presupposes that dependencies relate the whole chunk and do not involve a sub-part of it. But in Sanskrit we come across instances of compounds termed as asamartha-sam\u0101sa (Joshi, 1968; Gillon, 1993) where the dependencies relate to the sub-part of a compound which need not necessarily be a head. Use of a chunker module before calling a parser would fail to parse such constructs. Finally, the state-of-art parsers typically produce a single parse. We decided to produce all possible parses. This is to ensure that we do not miss out the correct parse. The onus of choosing the correct parse, from among the parses produced, is on the reader. The challenge before us was to handle the free word order in Sanskrit both in prose as well as in verse. The basic algorithm we followed for parsing is given below. 1. Define one node each corresponding to each morphological analysis of every word in a sentence. 2. Establish directed edges between the nodes, if there is either a mutual or unilateral expectancy (\u0101k\u0101\u1e45k\u1e63\u0101) between the corresponding words and the word meanings are not mu-tually incongruous (yogyat\u0101). 3. Define constraints, both local on each node as well as global on the graph as a whole. One of these constraints corresponds to sannidhi (proximity). 4. Extract all possible trees from this graph that satisfy both local and global constraints. Produce all possible solutions to ensure that in case of sentences with multiple interpretations, 1 machine does not miss any interpretation. 5. Produce the most probable solution as the first solution by defining an appropriate cost function. The cost C associated with a solution tree is defined as C = \u2211 e d e \u00d7 r k , where e is an edge from a word w j to a word w i with label k, d e = |j \u2212 i|, r k is the rank 2 of the role with label k. Then the problem of parsing a sentence may be modeled as the task of finding a sub-graph T of G such that T is a Directed Tree. To start with, in order to get familiarity with the kind of problems due to ambiguity, we designed a parser (Kulkarni et al., 2010) that handles a text in formally defined canonical prose order. This parser was implemented as a constraint solver. This parser was found to be very inefficient due to the use of matrix data structure which resulted in sparse matrices for long sentences or sentences with heavily ambiguous words at morphological level, affecting the efficiency. This algorithm was later improved by using vertex-centric traversal using dynamic programming (Kulkarni, 2013) . The major disadvantage of this method is, being node-centric traversal, if the initial words have several incoming arrows, then the number of partial solutions in the beginning are many and as one traverses various paths, the possibilities grow exponentially. It also checks the compatibility of each new edge with all the edges on the path explored so far. This leads to some redundancy, since if a node falls on more than one path, it would be visited more than once, and during each such visit all the incoming edges are checked for compatibility with all other edges on the path traversed so far. In the worst case scenario the incompatibility between the nodes would be noticed only at the final node. Both these algorithms were designed for sentences that have a default SOV order. Now we present below an algorithm that is designed to handle both prose as well as verse order. This algorithm also overcomes the disadvantages of the earlier algorithm viz. the redundancy in compatibility checking. It has been observed that the arguments having mutual expectancy (utthita \u0101k\u0101\u1e45k\u1e63\u0101), such as the core arguments of a predicate, follow weak non-projectivity while the arguments having unilateral expectancy (utth\u0101pya \u0101k\u0101\u1e45k\u1e63\u0101) are exceptions to this rule (Kulkarni et al., 2015) . We use these constraints to extract a tree from the graph. Edge-centric Binary Join We modify the previous algorithm at three levels. 1. Any edge that is a part of the solution should be compatible with remaining n \u2212 2 edges in the solution tree, where n is the number of words in the sentence. This is to ensure that the solution has n \u2212 1 edges. Hence, all those edges that are not compatible with at least n \u2212 2 other edges are thrown away. 2. We define the compatibility of two sets of edges as a simple operation of set intersection. 3. We build the solutions recursively starting with the individual words bottom-up, each time joining two sets of compatible edges. In n \u2212 1 joins we get all possible directed acyclic graphs (DAG), where n is the number of words in a sentence. Join operation is defined as a set union. These DAGs are Directed Trees, since each DAG involves n nodes with n \u2212 1 edges with all the words connected. This algorithm is edge-centric. Before giving the detailed algorithm, we define a few terms. 1. Local constraints: (a) A morpheme corresponding to a suffix marks only one relation. That is, a node can have one and only one incoming edge. (b) Each k\u0101raka relation is marked by a single morpheme. There cannot be more than one outgoing edge with the same label from the same node, if the relation corresponds to a k\u0101raka relation, 3 i.e. there cannot be two words satisfying the same k\u0101raka role of the same verb. (c) A morpheme does not mark a relation to itself. A word cannot satisfy its own expectancy, i.e. a word cannot be linked to itself. (d) There can be only one valid analysis of every word per solution. Since a word has one node corresponding to each morphological analysis it has, there are further restrictions as below. i. If a word has both an incoming edge as well as an outgoing edge, they should be through the same node. ii. If there is more than one outgoing edge for a word, then all of them should be through the same node. iii. A vi\u015be\u1e63a\u1e47a cannot have a vi\u015be\u1e63a\u1e47a. 4  These conditions ensure that only one morphological analysis is chosen per word. Global Constraints: (a) Sannidhi: There are no crossing of edges. If all the nodes are plotted in a straight line, then the edges connecting them (drawn on the upper side of the line) should not intersect each other. Adjectival relation and the relation due to genitive suffix are exceptions to this rule. (b) Certain relations always occur in pairs. For example, a kart\u1e5bsam\u0101n\u0101dhikara\u1e47a (a predicative adjective, literally having same locus as that of kart\u1e5b) assumes that there is a relation of kart\u1e5b already established. Compatible edge: An edge e 1 is said to be compatible with another edge e 2 if they satisfy local constraints, and we set Compatible(e 1 , e 2 ) = 1. Compatible set of edges: Let R be a set with edges {r 1 ,r 2 ,\u2026,r n }, and S be a set with edges {s 1 ,s 2 ,\u2026,s m }. S is compatible with R iff \u2200 i \u2200 j Compatible(s i , r j ) = 1. Joinable sets: Let R 1 and R 2 be two sets of edges. Let S 1 and S 2 be the sets of edges that are compatible with R 1 and R 2 respectively. R 1 and R 2 are joinable provided R 1 \u2286 S 2 and R 2 \u2286 S 1 . For such joinable sets, the edges compatible with R 1 \u222a R 2 are defined as (S 1 \u2229 S 2 ) -(R 1 \u222a R 2 ). Now we give the detailed algorithm. 1. Let there be N edges. 2. For each edge, list down all other edges it is locally compatible with. An Example We illustrate the algorithm with the following simple sentence. San: gacchati r\u0101ma\u1e25 vanam. (1) gloss: Goes Ram forest{acc.}. Eng: Ram goes to the forest. In this sentence, each of the two words r\u0101ma\u1e25 (Ram) and vanam (forest) has two possible analyses, and the word gacchati (goes) has three possible analyses as shown below.  First we filter out edge b, since it is not compatible with any of other edges. We retain all other edges as they are compatible with at least 1 (= n \u2212 2) other edge. Next we start building the solutions recursively. We start with the incoming edges of the first word. There is only one incoming edge, marked as a. This forms our first set of edges R 1 . The set of compatible edges with R 1 , denoted by S 1 has only one edge d. For the second word there are four incoming edges, marked as c, d, e, and f . Each of these starts a new partial solution. We call them R 2 , R 3 , R 4 , and R 5 . For each of these edges, the compatible edges are shown in Table 1 . We call them S 2 , S 3 , S 4 , and S 5 respectively. Now we check which of these partial solutions are joinable with R 1 . We notice that only R 3 is joinable with R 1 . Joining these two partial solution sets, results in {a,d}. The set of edges compatible with this partial solution is given by (S 1 \u2229 S 3 ) -(R 1 \u222a R 3 ) = \u03d5. We carry earlier partial solutions viz. R 2 , R 3 , R 4 , and R 5 , as well, being potential partial solutions, since each of them has one edge, and we still have one more word to visit. Now we get the edges of the third word, and join them with the current partial solutions. Corresponding to the third word, we have g and h as two incoming edges. Checking compatibility with all the partial solutions in the previous stage, we get five possible solutions as shown in Figure 2 . In Table 2 , we show the invocation of the algorithm for this sentence. The result shows the step number followed by the list of possible relations at that step. In this trace, we have not shown the compatible edges at each stage for each partial dag. Finally, we check all these solutions for global compatibility. In this example only {a, d} satisfies the global compatibility. And thus we get a unique solution. This corresponds to the left most tree in Figure 2 . If there are more than one globally compatible solutions, we rank them with the same cost function defined earlier. In this algorithm, JoinDags is called n \u2212 1 times. If there are r i incoming edges for i th word, then in the worst case, there are \u220f i r i set union and set intersection operations. Another Example Figure 3 shows the parse of the first \u015bloka from \u015ai\u015bup\u0101lavadham by the poet M\u0101gha, which occupies a prominent place among the Mah\u0101k\u0101vyas. It has the three virtues of the best K\u0101vya, viz. upam\u0101 of K\u0101lid\u0101sa, arthagauravam of Bh\u0101ravi and padal\u0101lityam of Da\u1e47\u1e0di. We also tried to parse the da\u1e47\u1e0da-anvaya of the same \u015bloka, and Figure 4 shows the parse of the anvaya. The \u015bloka and its prose form are given below. \u015aloka: \u015briya\u1e25 pati\u1e25 \u015br\u012bmati \u015b\u0101situm\u0307jagat jagat-niv\u0101sa\u1e25 vasudeva-sadmani | vasan dadar\u015ba avatarantam\u0307ambar\u0101t hira\u1e47ya-garbha-a\u1e45ga-bhuvam\u0307munim\u0307hari\u1e25 || (2) Da\u1e47\u1e0da-anvaya: \u015briya\u1e25 pati\u1e25 jagat-niv\u0101sa\u1e25 hari\u1e25 jagat \u015b\u0101situm\u0307\u015br\u012bmati vasudeva-sadmani vasan ambar\u0101t avatarantam\u0307hira\u1e47ya-garbha-a\u1e45ga-bhuvam\u0307munim\u0307dadar\u015ba | (3) Eng: Lak\u1e63mi's consort,Vi\u1e63\u1e47u, who is the source of the world, who was residing in the house of Vasudeva to control the world, saw Brahma's son N\u0101rada, descending from the sky. As stated earlier, our parser produces all possible parses, and since the constraint of mutual compatibility (yogyat\u0101) is not yet implemented fully, 6 the number of parses is on higher side. The total number of parses produced by the machine, in the case of \u015bloka and prose are 98,658 and 10,804 respectively. And the correct parse was found at 47, 848 th and 1, 256 th position respectively. The explanation for almost 10 fold increase in the number of parses in the case of \u015bloka is as follows: In the case of prose, it is assumed that the head is to the right. So all the adjectives, and also the arguments of the predicate occur to the left of the head. But in the case of a \u015bloka this condition does not hold. The adjectives as well as the arguments of the predicates can occur on either side of the head. Further, the adjectives and the modifiers with genitive case have more flexibility over the predicate-argument relations. Since they can cross the clausal boundaries, and that we have not yet implemented the meaning compatibility check on these relations, the possible number of solutions grows rapidly. Thus we notice that this parser can be still improved at two levels: a) To reduce the number of solutions. Study of mutual congruity among the meanings would help pruning out non-solutions. However, the representation of meaning congruity useful from computational point of view is challenging. b) The number of parses grow exponentially with the \u015bloka order, and this is essentially because of the dislocation of adjectives and the genitives. More research is needed in order to understand the nature of dislocations and also syntactic constraints on such dislocations. Understanding Texts: Commentary Tradition In this section, we explain how the parsed structure can help us in understanding the original text in the same way as does the commentary tradition. Free word order in Sanskrit had a key role in the emergence of the poetic style, rather than prose, as a natural style for Sanskrit compositions. Authors who have written Sanskrit prose also have taken advantage of the free word order to present texts that are consistent with the intended meter or are interesting from the aesthetics point of view. But it is also true that it is difficult to understand poetry compared to prose. This is evident from the fact, we notice, that the commentators, especially commenting on the k\u0101vya (poetic) literature, first rewrite the verse in prose in some default word order, and then comment on it. This deviation from the normal word order adds an extra load on the part of the readers in understanding the poetry. In order to understand such texts, one needs special training for interpreting these texts. We come across commentaries on several of such Sanskrit poetic texts, which make their understanding easier. In the Indian tradition, we see two methods followed by commentators while dealing with sentence level analysis of \u015blokas (Tubb and Boose, 2007) . In both these approaches, the aim of the commentator is to unfold the encoded meaning. While doing so, the commentator takes clues from the theories of \u015b\u0101bdabodha. The two approaches are described below. \u2022 The first approach is known as Kha\u1e47\u1e0da-anvaya (also known as katham-bh\u016btin\u012b), where the commentator starts with the verb, and the expectancies associated with the verb, and goes on filling these slots with the nominal forms in the \u015bloka. Once the basic skeleton with all the expectancies is ready, then the commentator connects the vi\u015be\u1e63a\u1e47as (adjectives) to their vi\u015be\u1e63yas (headwords), providing flesh to the skeleton. The parse produced by the machine provides us the kha\u1e47\u1e0d\u0101nvaya. All the words that are directly related to the verb work as a backbone, or as a part of the sentence carrying core information. The adjectives attached to the nouns, the arguments of non-finite verbs, etc. typically occupy the second or higher level in the tree structure, and add the flesh to the structure. \u2022 The second approach is the Da\u1e47\u1e0da-anvaya (also known as anvaya-mukh\u012b). In this method, first the commentator arranges the words in the \u015bloka in a prose form, following a default word order typically encountered in prose. In the next section, we present an algorithm that produces the Da\u1e47\u1e0da-anvaya for a \u015bloka, from its parsed output. Da\u1e47\u1e0da-anvaya-janaka The dependency structure, produced by the parser described above, of the following \u015bloka from Bhagavadg\u012bt\u0101 is shown in Figure 5 . D\u1e5b\u1e63\u1e6dv\u0101 tu p\u0101\u1e47\u1e0dav\u0101n\u012bkam Vy\u016b\u1e0dham duryodhana\u1e25 tad\u0101 | \u0100c\u0101ryam upasa\u1e45gamya R\u0101j\u0101 vacanam abrav\u012bt || (BhG 1.2) At that time, after seeing the army of the P\u0101\u1e47\u1e0davas arranged in military phalanx, Duryodhana approached (his) teacher and spoke (these) words. Relation (r) Parent Word (x,y) d\u1e5b\u1e63\u1e6dv\u0101 (0,0) p\u016brvak\u0101la\u1e25 abrav\u012bt (10,0) tu (1,0) sambandha\u1e25 d\u1e5b\u1e63\u1e6dv\u0101 (0,0) p\u0101\u1e47\u1e0dav\u0101n\u012bkam (2,0) karma d\u1e5b\u1e63\u1e6dv\u0101 (0,0) vy\u016b\u1e0dham (3,0) vi\u015be\u1e63a\u1e47am p\u0101\u1e47\u1e0dav\u0101n\u012bkam (2,0) duryodhana\u1e25 (4,0) kart\u0101 abrav\u012bt (10,0) tad\u0101 (5,0) k\u0101l\u0101dhikara\u1e47am abrav\u012bt (10,0) \u0101c\u0101ryam (6,0) karma upasa\u1e45gamya (7,0) upasa\u1e45gamya (7,0) p\u016brvak\u0101la\u1e25 abrav\u012bt (10,0) r\u0101j\u0101 (8,0) abheda\u1e25 duryodhana\u1e25 (4,0) vacanam (9,1) mukhyakarma abrav\u012bt (10,0) Table 3 : Output of Samsaadhanii parser Table 3 shows the machine internal representation of the dependency graph shown in Figure 5 . The shared roles are marked by dotted lines. For the purpose of re-ordering the words in Da\u1e47\u1e0daanvaya order, these shared roles are not useful and hence ignored. Initializing Reordering Task Anvaya reordering tool is a simple script written in Python. It takes the set of quintuplets as input and creates a corresponding Python tree object. Since multiple morphological variants of a word cannot occur in a single set of dependency solution, variant information is not used presently but is preserved for proposed uses in the future. Graphical representation of the tree object created with the parsed information of the Bhagavadg\u012bt\u0101 verse is same as in Figure 5 , without the dotted lines. Deciding the Order We found the clues for anvaya-order in the Sam\u0101sacakra. The two relevant k\u0101rik\u0101s go like this. \u0100dau kart\u1e5bpadam v\u0101cyam dvit\u012by\u0101dipadam tata\u1e25 Ktv\u0101tumunlyap ca madhye tu kury\u0101d ante kriy\u0101padam (Sam\u0101sacakram k\u0101rik\u0101 4, (Bhagirath, 1901, p. 12 )) Starting with kart\u1e5b, followed by other words, placing the non-finite verbal forms such as ktv\u0101, tumun, lyap in between, place the main verb at the end. Kart\u1e5b-karma-kriy\u0101-yuktam etad anvaya-lak\u1e63a\u1e47am (Sam\u0101sacakram k\u0101rik\u0101 10, (Bhagirath, 1901, p. 13 )) Vi\u015be\u1e63a\u1e47am purask\u1e5btya vi\u015be\u1e63yam tadanantaram Starting with adjectives, targeting the headword, in the order of kart\u1e5b-karma-kriy\u0101 (subjectobject-verb), gives an anvaya (the natural order of words in a sentence). In recent studies, Aralikatti (1991) has shown that the unmarked word order in Sanskrit is SOV. That is, all the arguments of a verb are placed to the left of the verb starting with the kart\u1e5b, then karman followed by other arguments, the attributive adjectives are placed to the left of the noun they qualify, and the predicate is at the end of the sentence. The sub-ordinate clauses, if any, are before the predicate. Taking clue from these resources, we define a sentence to be in canonical word order if it satisfies the following criteria: All the modifiers are placed to the left of the word they modify. This is equivalent to the following. 1. The adjectives are to the left of the substantives they qualify. 2. All the arguments of a verb (either in finite form or in non-finite form) are to its left. 3. All the non-finite forms that modify the finite verb form are to its left. This implies that the main verb 7 is always the last word of a sentence. This canonical word order provides us the Da\u1e47\u1e0da-anvaya for \u015blokas. We assigned the priorities to the dependency relation labels following these clues. These priorities were further fine-tuned by studying the commentaries and prose orders of around 400 \u015blokas from literature including Bhagavadg\u012bt\u0101, N\u012bti\u015bataka, various subh\u0101\u1e63itas and about 50 poetic prose sentences from K\u0101dambar\u012b. Adjusted by various measures, currently, the relative positions of various arguments are fixed following the rules given below. 1. Sambodhya (vocative) comes at the initial position in the canonical order. 2. Kart\u1e5b comes after vocative. 3. K\u0101raka relations follow in reverse order i.e. adhikara\u1e47a, ap\u0101d\u0101na, samprad\u0101na, kara\u1e47a and karman. 4. Vi\u015be\u1e63anas, modifiers with genitive case markers, etc. are placed before their vi\u015be\u1e63ya. 5. Kriy\u0101vi\u015be\u1e63ana, prati\u1e63edha etc. are placed immediately before their corresponding verb. 6. Mukhyakriy\u0101 is positioned at the end of the sentence. 7. Avyaya particles such as tu and api are placed immediately after their parent word. 8. The non-finite verbal forms are placed before the karman. All the arguments of non-finite verb appear to their left. 9. The kart\u1e5b-sam\u0101n\u0101dhikara\u1e47a and karma-sam\u0101n\u0101dhikara\u1e47a are placed after the kat\u1e5b and karman respectively. Sorting the Tree The reordering tool traverses through the tree object using level-order-iteration and sort recursively at each node. Primary sorting is carried out based on the relation priorities. The indeclinables such as emphatic particles, and conjuncts are left out as their positions are fixed with respect to their parent node. If there are relations with equal priorities at any level, secondary sorting is done based on the word order (ID) in the original sentence. The reordered dependency tree of the example \u015bloka is represented in Figure 6 . Linearizing the Tree The sorted dependency tree is linearized to get the anvaya order. The tree is traversed using post-order-iteration and each node is added to the linear order pattern. The tree mentioned in Figure 6 is linearized in the order: R\u0101j\u0101 Duryodhana\u1e25 vy\u016b\u1e0dham p\u0101\u1e47\u1e0dav\u0101n\u012bkam d\u1e5b\u1e63\u1e6dv\u0101 tu \u0101c\u0101ryam upasa\u1e45gamya tad\u0101 8 vacanam abrav\u012bt. Performance This parser was tested on 195 instances and their canonical prose versions. The sample was taken from the corpus available at Heritage Platform 9 , which essentially corresponds to the citations in the dictionary entry and thus is a random selection from Sanskrit texts belonging to different branches of knowledge and different time period. We provided manually their canonical form. And both the canonical form as well as verse form was run through the parser. Out of 195, the parser could not parse 45 instances both in prose as well as in verse form. One major reason for the failure is out of vocabulary words. The average number of parses for verse order text and prose order text were 151 and 60 respectively. There were around 10 instances, where the number of parses was greater than 1000. This was mainly due to over-analysis with the genitive case markers, in the absence of proper handling of mutual congruity. The median for number of parses is 4, for both verse as well as prose. Some of the limitations of the current parser are-1. The parser is based on the Vaiy\u0101kara\u1e47a's theory of \u015b\u0101bdabodha. As such, it expects a verb in a sentence. Sanskrit has a tendency of eliding stative verbs meaning 'to be' like asti, bhavati etc. Parser shows poor performance dealing with such sentences. 2. The relation of kart\u1e5bsam\u0101n\u0101dhikara\u1e47a is established with a noun, only if it agrees with kart\u1e5b in gender, number, person and case suffix. There are exceptions in literature where sam\u0101n\u0101dhikara\u1e47as have semantic compatibility though they don't agree in gender, number etc. For example, \u2022 Chanda\u1e25 p\u0101dau tu vedasya (chanda\u1e25 and p\u0101dau do not agree in number). \u2022 M\u0101y\u0101 idam sarvam (Gender of m\u0101y\u0101 does not agree with that of idam and sarvam). Parser fails to establish relations among such words. 3. Parser performs poorly on some domain specific sentences. Here is an example from mathematical domain: catur\u0101dhikam \u015batama\u1e63\u1e6dagu\u1e47am dv\u0101\u1e63a\u1e63\u1e6distath\u0101 sahasr\u0101\u1e47\u0101m ayutadvayavi\u1e63kambhasy\u0101sanna\u1e25 v\u1e5bttapari\u1e47\u0101ha\u1e25. Conclusion The main purpose behind the development of an indegenous parser was to evaluate the usefulness of the theories of \u015b\u0101bdabodha for the mechanical parsing of Sanskrit sentences. The theories of \u015b\u0101bdabodha discuss in minute detail the flow of information, various means of encoding the information, the amount of information encoded, and so on. These theories were further supported by providing various conditions such as \u00e1k\u0101\u1e45k\u1e63\u0101, yogyat\u0101 and sannidhi, that help in the process of verbal cognition. So we decided to model these conditions computationally. In this paper we have presented an edge-centric algorithm that handles both prose as well as poetry. In this algorithm, the incompatibility between the edges is noticed at an early stage. And hence the non-solutions are thrown out at an early stage. The user interface allows the user to select the best suited segmentation and provide the canonical word order of such segmented text. We noticed that the performance of the algorithm when the input is in prose form is better than when it is in verse form. The relations contributing to the over-generation are the relation due to genitive case suffix and the adjectival relation. More research towards the nature of dislocation and syntactic constraints on dislocation, and also the semantic compatibility of the words related thus would help in rejecting the non-solutions mechanically.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not provide any specific information regarding funding from defense, corporate entities, research agencies, foundations, or any other sources. Without explicit mention of funding, it is not possible to determine the presence of any financial support from these entities.",
    "abstract": "Sentence parser is an essential component in the mechanical analysis of natural language texts. Building a parser for Sanskrit text is a challenging task because of its free word order and the dominance of verse style in Sanskrit literature in comparison to prose style. In this paper, we describe our efforts to build a parser which parses both prose as well as verse texts. It employs an Edge-Centric Binary Join method using various constraints following traditional rules of verbal cognition. We also propose a Da\u1e47\u1e0da-anvaya-janaka which converts the parsed verse form to its canonical prose order.",
    "countries": [
        "India"
    ],
    "languages": [
        "Sanskrit"
    ],
    "numcitedby": 4,
    "year": 2019,
    "month": "October",
    "title": "Dependency Parser for {S}anskrit Verses",
    "values": {
        "building on past work": " It employs an Edge-Centric Binary Join method using various constraints following traditional rules of verbal cognition.  In n \u2212 1 joins we get all possible directed acyclic graphs (DAG), where n is the number of words in a sentence. Join operation is defined as a set union. These DAGs are Directed Trees, since each DAG involves n nodes with n \u2212 1 edges with all the words connected.  This algorithm is edge-centric. Before giving the detailed algorithm, we define a few terms.   We illustrate the algorithm with the following simple sentence.  We carry earlier partial solutions viz.",
        "novelty": " Introduction Our parser differs from the state-of-the-art parsers in three ways. First, in the absence of any annotated corpus, we follow the grammar based approach. Secondly, our parser is invoked right after the morphological analyser. The main reason behind this decision was the following. Indian literature on verbal import was found to be useful from parsing point of view since it has discussions on various factors that are instrumental in the process of verbal cognition. Our main goal is to build a parser modeling the theories of \u015b\u0101bdabodha. When we looked at various Indian literature related to the theories of verbal cognition, there was no discussion on any kind of POS tagger or chunker. Moreover, use of chunker also presupposes that dependencies relate the whole chunk and do not involve a sub-part of it. But in Sanskrit we come across instances of compounds termed as asamartha-sam\u0101sa (Joshi, 1968; Gillon, 1993) where the dependencies relate to the sub-part of a compound which need not necessarily be a head. Use of a chunker module before calling a parser would fail to parse such constructs. Finally, the state-of-art parsers typically produce a single parse. We decided to produce all possible parses. This is to ensure that we do not miss out the correct parse. The onus of choosing the correct parse, from among the parses produced, is on the reader.  Design of a Parser The main reason behind this decision was the following. Indian literature on verbal import was found to be useful from parsing point of view since it has discussions on various factors that are instrumental in the process of verbal cognition. Our main goal is to build a parser modeling the theories of \u015b\u0101bdabodha. When we looked at various Indian literature related to the theories of verbal cognition, there was no discussion on any kind of POS tagger or chunker. Moreover, use of chunker also presupposes that dependencies relate the whole chunk and do not involve a sub-part of it. But in Sanskrit we come across instances of compounds termed as asamartha-sam\u0101sa (Joshi, 1968; Gillon, 1993) where the dependencies relate to the sub-part of a compound which need not necessarily be a head. Use of a chunker module before calling a parser would fail to parse such constructs. Finally, the state-of-art parsers typically produce a single parse. We decided to produce all possible parses. This is to ensure that we do not miss out the correct parse. The onus of choosing the correct parse, from among the parses produced, is on the reader.  Edge-centric Binary Join We modify the previous algorithm at three levels.  Any edge that is a part of the solution should be compatible with remaining n \u2212 2 edges in the solution tree, where n is the number of words in the sentence. This is to ensure that the solution has n \u2212 1 edges. Hence, all those edges that are not compatible with at least n \u2212 2 other edges are thrown away.  We define the compatibility of two sets of edges as a simple operation of set intersection.  We build the solutions recursively starting with the individual words bottom-up, each time joining two sets of compatible edges. In n \u2212 1 joins we get all possible directed acyclic graphs (DAG), where n is the number of words in a sentence. Join operation is defined as a set union.  Da\u1e47\u1e0da-anvaya-janaka The two approaches are described below. \u2022 The first approach is known as Kha\u1e47\u1e0da-anvaya (also known as katham-bh\u016btin\u012b), where the commentator starts with the verb, and the expectancies associated with the verb, and goes on filling these slots with the nominal forms in the \u015bloka. Once the basic skeleton with all the expectancies is ready, then the commentator connects the vi\u015be\u1e63a\u1e47as (adjectives) to their vi\u015be\u1e63yas (headwords), providing flesh to the skeleton. The parse produced by the machine provides us the kha\u1e47\u1e0d\u0101nvaya. All the words that are directly related to the verb work as a backbone, or as a part of the sentence carrying core information. The adjectives attached to the nouns, the arguments of non-finite verbs, etc. typically occupy the second or higher level in the tree structure, and add the flesh to the structure. \u2022 The second approach is the Da\u1e47\u1e0da-anvaya (also known as anvaya-mukh\u012b). In this method, first the commentator arranges the words in the \u015bloka in a prose form, following a default word order typically encountered in prose.",
        "performance": " This parser was tested on 195 instances and their canonical prose versions.  Out of 195, the parser could not parse 45 instances both in prose as well as in verse form.  The average number of parses for verse order text and prose order text were 151 and 60 respectively.  There were around 10 instances, where the number of parses was greater than 1000.  The median for number of parses is 4, for both verse as well as prose."
    }
}