{
    "article": "This paper studies the use of structural representations for learning relations between pairs of short texts (e.g., sentences or paragraphs) of the kind: the second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic relations between the constituents composing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of stateof-the-art models for this type of relational learning. Our findings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identification and Textual Entailment Recognition. Introduction Advanced NLP systems, e.g., IBM Watson system (Ferrucci et al., 2010) , are the result of effective use of syntactic/semantic information along with relational learning (RL) methods. This research area is rather vast including, extraction of syntactic relations, e.g., (Nastase et al., 2013) , predicate relations, e.g., Semantic Role Labeling (Carreras and M\u00e0rquez, 2005) or FrameNet parsing (Gildea and Jurafsky, 2002) and relation extraction between named entities, e.g., (Mintz et al., 2009) . Although extremely interesting, the above methods target relations only between text constituents whereas the final goal of an intelligent system would be to interpret the semantics of larger pieces of text, e.g., sentences or paragraphs. This line of research relates to three broad fields, namely, Question Answering (QA) (Voorhees and Tice, 1999) , Paraphrasing Identification (PI) (Dolan et al., 2004) and Recognition of Textual Entailments (RTE) (Giampiccolo et al., 2007) . More generally, RL from text can be denied as follows: given two text fragments, the main goal is to derive relations between them, e.g., either if the second fragment answers the question, or conveys exactly the same information or is implied by the first text fragment. For example, the following two sentences: -License revenue slid 21 percent, however, to $107.6 million. -License sales, a key measure of demand, fell 21 percent to $107.6 million. express exactly the same meaning, whereas the next one: -She was transferred again to Navy when the American Civil War began, 1861. implies: -The American Civil War started in 1861. Automatic learning a model for deriving the relations above is rather complex as any of the text constituents, e.g., License revenue, a key measure of demand, in the two sentences plays an important role. Therefore, a suitable approach should exploit representations that can structure the two sentences and put their constituents in relation. Since the dependencies between constituents can be an exponential number and representing structures in learning algorithms is rather challenging, automatic feature engineering through kernel methods (Shawe-Taylor and Cristianini, 2004; Moschitti, 2006) can be a promising direction. In particular, in (Zanzotto and Moschitti, 2006) , we represented the two evaluating sentences for the RTE task with syntactic structures and then applied tree kernels to them. The resulting system was very accurate but, unfortunately, it could not scale to large datasets as it is based on a compu-tationally exponential algorithm. This prevents its application to PI tasks, which typically require a large dataset to train the related systems. In this paper, we carry out an extensive experimentation using different kernels based on trees and graphs and their combinations with the aim of assessing the best model for relation learning between two entire sentences (or even paragraphs). More in detail, (i) we design many models for RL combining state-of-the-art tree kernels and graph kernels and apply them to innovative computational structures. These innovative combinations use for the fist time semantic/syntactic tree kernels and graph kernels for the tackled tasks. (ii) Our kernels provide effective and efficient solutions, which solve the previous scalability problem and, at the same time, exceed the state of the art on both RTE and PI. Finally, our study suggests research directions for designing effective graph kernels for RL. Related Work In this paper, we apply kernel methods, which enable an efficient comparison of structures in huge, possibly infinite, feature spaces. While for trees, a comparison using all possible subtrees is possible, designing kernel functions for graphs with such property is an NP-Hard problem (i.e., it shows the same complexity of the graph isomorphism problem) (Gartner et al., 2003) . Thus most kernels for graphs only associate specific types of substructures with features, such as paths (Borgwardt and Kriegel, 2005; Heinonen et al., 2012) , walks (Kashima et al., 2003; Vishwanathan et al., 2006) and tree structures (Cilia and Moschitti, 2007; Mah\u00e9 and Vert, 2008; Shervashidze et al., 2011; Da San Martino et al., 2012) . We exploit structural kernels for PI, whose task is to evaluate whether a given pair of sentences is in the paraphrase class or not, (see for example (Dolan et al., 2004) ). Paraphrases can be seen as a restatement of a text in another form that preserves the original meaning. This task has a primary importance in many other NLP and IR tasks such as Machine Translation, Plagiarism Detection and QA. Several approaches have been proposed, e.g., (Socher et al., 2011) apply a recursive auto encoder with dynamic pooling, and (Madnani et al., 2012) use eight machine translation metrics to achieve the state of the art. To our knowledge no previous model based on kernel methods has been applied before: with such methods, we outperform the state of the art in PI. A description of RTE can be found in (Giampiccolo et al., 2007) : it is defined as a directional relation extraction between two text fragments, called text and hypothesis. The implication is supposed to be detectable only based on the text content. Its applications are in QA, Information Extraction, Summarization and Machine translation. One of the most performing approaches of RTE 3 was (Iftene and Balahur-Dobrescu, 2007) , which largely relies on external resources (i.e., WordNet, Wikipedia, acronyms dictionaries) and a base of knowledge developed ad hoc for the dataset. In (Zanzotto and Moschitti, 2006) , we designed an interesting but computationally expensive model using simple syntactic tree kernels. In this paper, we develop models that do not use external resources but, at the same time, are efficient and approach the state of the art in RTE. Structural kernels Kernel Machines carry out learning and classification by only relying on the inner product between instances. This can be efficiently and implicitly computed by kernel functions by exploiting the following dual formulation of the model (hyperplane): i=1..l y i \u03b1 i \u03c6(o i ) \u2022 \u03c6(o) + b = 0, where y i are the example labels, \u03b1 i the support vector coefficients, o i and o are two objects, \u03c6 is a mapping from the objects to feature vectors x i and \u03c6(o i ) \u2022 \u03c6(o) = K(o i , o) is the kernel function implicitly defining such mapping. In case of structural kernels, K maps objects in substructures, thus determining their size and shape. Given two structures S 1 and S 2 , our general definition of structural kernels is the following: K(S 1 , S 2 ) = s 1 \u2286S 1 ,s 2 \u2286S 2 ,s i \u2208S k iso (s 1 , s 2 ), (1) where s i are substructures of S i , S is the set of admissible substructures, and k iso determines if the two substructures are isomorphic, i.e., it outputs 1 if s 1 and s 2 are isomorphic and 0 otherwise. In the following, we also provide a more computational-oriented definition of structural kernels to more easily describe those we use in our work: Let the set S = {s 1 , s 2 , . . . , s |S| } be the substructure space and \u03c7 i (n) be an indicator function, equal to 1 if the target s i is rooted at node n and equal to 0 otherwise. A structural-kernel function over S 1 and S 2 is K(S 1 , S 2 ) = n 1 \u2208N S 1 n 2 \u2208N S 2 \u2206(n 1 , n 2 ), (2) where N S 1 and N S 2 are the sets of the S 1 's and S 2 's nodes, respectively and \u2206(n 1 , n 2 ) = |S| i=1 \u03c7 i (n 1 )\u03c7 i (n 2 ). ( ) 3 The latter is equal to the number of common substructures rooted in the n 1 and n 2 nodes. In order to have a similarity score between 0 and 1, a normalization in the kernel space, i.e., K(S 1 ,S 2 ) \u221a K(S 1 ,S 1 )\u00d7K(S 2 ,S 2 ) is usually applied. From a practical computation viewpoint, it is convenient to divide structural kernels in two classes of algorithms working either on trees or graphs. 3.1 The Partial Tree Kernel (P T K) P T K (Moschitti, 2006) generalizes a large class of tree kernels as it computes one of the most general tree substructure spaces. Given two trees S 1 and S 2 , P T K considers any connected subset of nodes as possible feature of the substructure space, and counts how many of them are shared by S 1 and S 2 . Its computation is carried out by Eq. 2 using the following \u2206 P T K function: if the labels of n 1 and n 2 are different \u2206 P T K (n 1 , n 2 ) = 0; else \u2206 P T K (n 1 , n 2 ) = \u00b5 \u03bb 2 + I 1 , I 2 ,l( I 1 )=l( I 2 ) \u03bb d( I 1 )+d( I 2 ) l( I 1 ) j=1 \u2206 P T K (c n 1 ( I 1j ), c n 2 ( I 2j )) where \u00b5, \u03bb \u2208 [0, 1] are two decay factors, (Moschitti, 2006) , where p is the largest subsequence of children that we want to consider and \u03c1 is the maximal outdegree observed in the two trees. However the average running time tends to be linear for natural language syntactic trees (Moschitti, 2006) . I Smoothed Partial Tree Kernel (SP T K) Constraining the application of lexical similarity to words embedded in similar structures provides clear advantages over all-vs-all words similarity, which tends to semantically diverge. Indeed, syntax provides the necessary restrictions to compute an effective semantic similarity. SP T K (Croce et al., 2011) generalizes P T K by enabling node similarity during substructure matching. More formally, SP T K is computed by Eq. 2 using the following \u2206 SP T K (n 1 , n 2 ) = |S| i,j=1 \u03c7 i (n 1 )\u03c7 j (n 2 )\u03a3(s i , s j ), where \u03a3 is a similarity between structures 1 . The recursive definition of \u2206 SP T K is the following: 1. if n 1 and n 2 are leaves \u2206 SP T K (n 1 , n 2 ) = \u00b5\u03bb\u03c3(n 1 , n 2 ); 2. else \u2206 SP T K (n 1 , n 2 ) = \u00b5\u03c3(n 1 , n 2 ) \u00d7 \u03bb 2 + I 1 , I 2 ,l( I 1 )=l( I 2 ) \u03bb d( I 1 )+d( I 2 ) l( I 1 ) j=1 \u2206 \u03c3 (c n 1 ( I 1j ), c n 2 ( I 2j )) , where \u03c3 is any similarity between nodes, e.g., between their lexical labels, and the other variables are the same of P T K. The worst case complexity of SP T K is identical to PTK and in practice is not higher than O(|N S 1 ||N S 2 |). Neighborhood Subgraph Pairwise Distance Kernel (N SP DK) When general subgraphs are used as features in a kernel computation, eq. 1 and 2 become computationally intractable (Gartner et al., 2003) . To solve this problem, we need to restrict the set of considered substructures S. (Costa and De Grave, 2010) defined N SP DK such that the feature space is only constituted by pairs of subgraphs (substructures) that are (i) centered in two nodes n 1 and n 2 such that their distance is not more than D; and (ii) constituted by all nodes (and their edges) at an exact distance h from n 1 or n 2 , where the distance between two nodes is defined as the number of edges in the shortest path connecting them. More formally, let G, N G and E G be a graph and its set of nodes and edges, respectively, the substructure space S = S G (H, D) used by N SP DK in eqs 2 and 3 is: {(\u03b3 h (n), \u03b3 h (n )) : 1 \u2264 h \u2264 H, n, n \u2208 N G , d(n, n ) \u2264 D}, where \u03b3 h (n) returns the subgraph obtained by executing h steps of a breadth-first visit of G starting from node n and d(n, n ) is the distance between two nodes in the graph. Note that (i) any feature of the space is basically a pair of substructures; and (ii) there is currently no efficient (implicit) formulation for computing such kernel. In contrast, when H and D are limited, it is simple to compute the space S G (H, D) explicitly. In such case, the complexity of the kernel is given by the substructure extraction step, which is O(|N G | \u00d7 h\u03c1 log \u03c1). Kernel Combinations Previous sections have shown three different kernels. Among them, N SP DK is actually an explicit kernel, where the features are automatically extracted with a procedure. In NLP, features are often manually defined by domain experts, who know the linguistic phenomena involved in the task. When available, such features are important as they encode some of the background knowledge on the task. Therefore, combining different feature spaces is typically very useful. Fortunately, kernel methods enable an easy integration of different kernels or feature spaces, i.e., the kernel sum produces the joint feature space and it is still a valid kernel. In the next section, we show representations of text, i.e., structures and features, specific to PI and RTE. Representations for RL from text The kernels described in the previous section can be applied to generic trees and graphs. Automatic feature engineering using structural kernels requires the design of structures for representing data examples that are specific to the learning task we want to tackle. In our case, we focus on RL, which consists in deriving the semantic relation between two entire pieces of text. We focus on two well-understood relations, namely, paraphrasing and textual implications. The tasks are simply defined as: given two texts a 1 and a 2 , automatically classify if (i) a 1 is a paraphrase of a 2 and/or (ii) a 1 implies a 2 . Although the two tasks are linguistically and conceptually rather different, they can be modeled in a similar way from a shallow representation viewpoint. This is exactly the perspective we would like to keep for showing the advantage of using kernel methods. Therefore, in the following, we define sentence representations that can be suitably used for both tasks and then we rely on structural kernels and the adopted learning algorithm for exploring the substructures relevant to the different tasks. Tree Representations An intuitive understanding of our target tasks suggests that syntactic information is essential to achieve high accuracy. Therefore, we consider the syntactic parse trees of the pair of sentences involved in the evaluation. For example, Fig. 1 shows the syntactic constituency trees of the sentences reported in the introduction (these do not include the green label REL and the dashed edges). Given two pairs of sentences, p a = a 1 , a 2 and p b = b 1 , b 2 , an initial kernel for learning the tasks, can be the simple tree kernel sum, e.g., P T K(a 1 , b 1 ) + P T K(a 2 , b 2 ) as was defined in (Moschitti, 2008) . This kernel works in the space of the union of the sets of all subtrees from the upper and lower trees, e.g.: However, such features cannot capture the relations between the constituents (or semantic lexical units) from the two trees. In contrast, these are essential to learn the relation between the two entire sentences 2 . a 1 : [PP [TO [to::t]][NP [QP [$ [$::$]][QP [CD [107.6::c]]]]]], [PP [TO][NP [QP [$][QP [CD [107]]]]]], [PP [TO][NP [QP [QP [CD]]]]], [PP [NP [QP [QP]]]], ... To overcome this problem, in (Zanzotto and Moschitti, 2006) , we proposed the use of placeholders for RTE: the main idea was to annotate the matches between the constituents of the two sentences, e.g., 107.6 millions, on both trees. This way the tree fragments in the generated kernel space contained an index capturing the correspondences between a 1 and a 2 . The critical drawback of this approach is that other pairs, e.g., p b , will have in general different indices, making the representation very sparse. Alternatively, we experimented with models that select the best match between all possible placeholder assignments across the two pairs. Although we obtained a good improvement, such solution required an exponential computational time and the selection of the max  assignment made our similarity function a nonvalid kernel. Thus, for this paper, we prefer to rely on a more recent solution we proposed for passage reranking in the QA domain (Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b) , and for Answer Selection (Severyn and Moschitti, 2013) . It consists in simply labeling matching nodes with a special tag, e.g., REL, which indicates the correspondences between words. REL is attached to the father and grandfather nodes of the matching words. Fig. 1 shows several green REL tags attached to the usual POS-tag and constituent node labels of the parse trees. For example, the lemma license is matched by the two sentences, thus both its father, JJ, and its grandfather, NP, nodes are marked with REL. Thanks to such relational labeling the simple kernel, P T K(a 1 , b 1 ) + P T K(a 2 , b 2 ), can generate relational features from a 1 , e.g., It should be noted that we proposed more complex REL tagging policies for Passage Reranking, exploiting additional resources such as Linked Open Data or WordNet (Tymoshenko et al., 2014) . Another interesting application of this RL framework is the Machine Translation Evaluation (Guzm\u00e1n et al., 2014) . Finally, we used a similar model for translating questions to SQL queries in (Giordani and Moschitti, 2012) . Graph Representations The relational tree representation can capture relational features but the use of the same REL tag for any match between the two trees prevents to deterministically establish the correspondences between nodes. For exactly representing such matches (without incurring in non-valid kernels or sparsity problems), a graph representation is needed. If we connect matching nodes (or also nodes labelled as REL) in Fig. 1 (see dashed lines), we obtain a relational graph. Substructures of such graph clearly indicate how constituents, e.g., NPs, VPs, PPs, from one sentence map into the other sentence. If such mappings observed in a pair of paraphrase sentences are matched in another sentence pair, there may be evidence that also the second pair contains paraphrase sen-tences. Unfortunately, the kernel computing the space of all substructures of a graph (even if only considering connected nodes) is an intractable problem as mentioned in Sec. 3.3. Thus, we opt for the use of N SP DK, which generates specific pairs of structures. Intuitively, the latter can capture relational features between constituents of the two trees. Figure 1 Basic Features In addition to structural representations, we also use typical features for capturing the degrees of similarity between two sentences. In contrast, with the previous kernels these similarities are computed intra-pair, e.g., between a 1 and a 2 . Note that any similarity measure generates only one feature. Their description follows: -Syntactic similarities, which apply the cosine function to vectors of n-grams (with n = 1, 2, 3, 4) of word lemmas and part-of-speech tags. -Kernel similarities, which use P T K or SP T K applied to the sentences within the pair. We also used similarity features from the DKPro of the UKP Lab (B\u00e4r et al., 2012) , tested in the Semantic Textual (STS) task: -Longest common substring measure and Longest common subsequence measure, which determine the length of the longest substring shared by two text segments. -Running-Karp-Rabin Greedy String Tiling provides a similarity between two sentences by counting the number of shuffles in their subparts. -Resnik similarity based on the WordNet hierarchy. -Explicit Semantic Analysis (ESA) similarity (Gabrilovich and Markovitch, 2007) represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. -Lexical Substitution (Szarvas et al., 2013) : a supervised word sense disambiguation system is used to substitute a wide selection of highfrequency English nouns with generalizations, then Resnik and ESA features are computed on the transformed text. Combined representations As mentioned in Sec. 3.4, we can combine kernels for engineering new features. Let K be P T K or SP T K, given two pairs of sentences p a = a 1 , a 2 and p b = b 1 , b 2 , we build the following kernel combinations for the RTE task: (i) K + (p a , p b ) = K(a 1 , b 1 ) + K(a 2 , b 2 ), which simply sums the similarities between the first two sentences and the second two sentences whose implication has to be derived. (ii) An alternative kernel combines the two similarity scores above with the product: K \u00d7 (p a , p b ) = K(a 1 , b 1 ) \u2022 K(a 2 , b 2 ). (iii) The symmetry of the PI task requires different kernels. The most intuitive applies K between all member combinations and sum all contributions: all + K (p a , p b )=K(a 1 , b 1 ) + K(a 2 , b 2 ) + K(a 1 , b 2 ) + K(a 2 , b 1 ). (iv) It is also possible to combine pairs of corresponding kernels with the product: all \u00d7 K (p a , p b ) = K(a 1 , b 1 )K(a 2 , b 2 ) + K(a 1 , b 2 )K(a 2 , b 1 ). (v) An alternative kernel selects only the best between the two products above: M K (p a , p b ) = max(K(a 1 , b 1 )K(a 2 , b 2 ), K(a 1 , b 2 )K(a 2 , b 1 )). This is motivated by the observation that before measuring the similarity between two pairs, we need to establish which a i is more similar to b j . However, the max operator causes M K not to be a valid kernel function, thus we substitute it with a softmax function, which is a valid kernel, i.e., SM K (p a , p b ) = soft- max(K(a 1 , b 1 )K(a 2 , b 2 ), K(a 1 , b 2 )K(a 2 , b 1 )), where softmax(x 1 , x 2 ) = 1 c log(e cx 1 + e cx 2 ) (c=100 was accurate enough). The linear kernel (LK) over the basic features (described previously) and/or N SP DK can be of course added to all the above kernels. Experiments Setup MSR Paraphrasing: we used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) examples. These pairs were extracted from topically similar Web news articles, applying some heuristics that select potential paraphrases to be annotated by human experts. RTE-3. We adopted the RTE-3 dataset (Giampiccolo et al., 2007) , which is composed by 800 texthypothesis pairs in both the training and test sets, collected by human annotators. The distribution of the examples among the positive and negative classes is balanced. Models and Parameterization We train our classifiers with the C-SVM learning algorithm (Chang and Lin, 2011) within KeLP 3 , a Kernel-based Machine Learning platform that implements tree kernels. In both tasks, we applied the kernels described in Sec. 4, where the trees are generated with the Stanford parser 4 . SP T K uses a node similarity function \u03c3(n 1 , n 2 ) implemented as follows: if n 1 and n 2 are two identical syntactic nodes \u03c3 = 1. If n 1 and n 2 are two lexical nodes with the same POS tag, their similarity is evaluated computing the cosine similarity of their corresponding vectors in a wordspace. In all the other cases \u03c3 = 0. We generated two different wordspaces. The first is a co-occurrence LSA embedding as described in (Croce and Previtali, 2010) . The second space is derived by applying a skip-gram model (Mikolov et al., 2013) with the word2vec tool 5 . SP T K using the LSA will be referred to as SP T K LSA , while when adopting word2vec it will be indicated with SP T K W 2V . We used default parameters both for P T K and SP T K whereas we selected h and D parameters of N SP DK that obtained the best average accuracy using a 5-fold cross validation on the training set. Performance measures The two considered tasks are binary classification problems thus we used Accuracy, Precision, Recall and F1. The adopted corpora have a predefined split between training and test sets thus we tested our models according to such settings for exactly comparing with previous work. Additionally, to better assess our results, we performed a 5fold cross validation on the complete datasets. In case of PI, the same sentence can appear in multiple pairs thus we distributed the pairs such that the same sentence can only appear in one fold at a time. Results on PI The results are reported in Table 1 . The first column shows the use of the relational tag REL in the structures (discussed in Sec. 4.1). The second column indicates the kernel models described in sections 3 and 4 as well as the combination of the best models. Columns 3-6 report Accuracy, Precision, Recall and F1 derived on the fixed test set, whereas the remaining columns regard the results obtained with cross validation. We note that: First, when REL information is not used in the structures, the linear kernel (LK) on basic features outperforms all the structural kernels, which all perform similarly. The best structural kernel is the graph kernel, N SP DK (GK in short). This is not surprising as without REL, GK is the only kernel that can express relational features. Second, SP T K is only slightly better than P T K. The reason is mainly due to the approach used for building the dataset: potential paraphrases are retrieved applying some heuristics mostly based on the lexical overlap between sentences. Thus, in most cases, the lexical similarity used in SP T K is not needed as hard matches occur between the words of the sentences. Third, when REL is used on the structures, all kernels reach or outperform the F1 (official measure of the challenge) of LK. The relational structures seem to drastically reduce the inconsistent matching between positive and negative examples, reflecting in remarkable increasing in Precision. In particular, SM SP T K LSA achieves the state of the art 6 , i.e., 84.1 (Madnani et al., 2012) . Next, combining our best models produces a significant improvement of the state of the art, e.g., LK + GK + SM SP T K W 2V outperforms the result in (Madnani et al., 2012 ) by 1.7% in accuracy and 1.1 points in F1. Finally, the cross-validation experiments confirm the system behavior observed on the fixed test set. The Std. Dev. (specified after the \u00b1 sign) shows that in most cases the system differences are significant. Results on RTE We used the same experimental settings performed for PI to carry out the experiments on RTE. The results are shown in Table 2 structured in the same way as the previous table. We note that: (i) Findings similar to PI are obtained. (ii) Again the relational structures (using REL) provide a remarkable improvement in Accuracy (RTE challenge measure), allowing tree kernels to compete with the state of the art. This is an impressive result considering that our models do not use any external resource, e.g., as in (Iftene and Balahur-Dobrescu, 2007 ). (iii) This time, SP T K \u00d7 W 2V improves on P T K by 1 absolute percent point. (iv) The kernel combinations are not more effective than SP T K alone. Finally, the cross-fold validation experiments confirm the fixed-test set results. Discussion and Conclusions In this paper, we have engineered and studied several models for relation learning. We utilized state-of-the-art kernels for structures and created new ones by combining kernels together. Additionally, we provide a novel definition of effective and computationally feasible structural kernels. Most importantly, we have designed novel computational structures for trees and graphs, which are for the first time tested in NLP tasks. Our kernels are computationally efficient thus solving one of the most important problems of previous work. We empirically tested our kernels on two of the most representative tasks of RL from text, namely, PI and RTE. The extensive experimentation using many kernel models also combined with traditional feature vector approaches sheds some light on how engineering effective graph and tree kernels for learning from pairs of entire text fragments. In particular, our best models significantly outperform the state of the art in PI and the best kernel model for RTE 3, with Accuracy close to the one of the best system of RTE 3. It should be stressed that the design of previous state-of-the-art models involved the use of several resources, annotation and heavy manually engineering of specific rules and features: this makes the portability of such systems on other domains and tasks extremely difficult. Moreover the unavailability of the used resources and the opacity of the used rules have also made such systems very difficult to replicate. On the contrary, the models we propose enable researchers to: (i) build their system without the use of specific resources. We use a standard syntactic parser, and for some models we use wellknown and available corpora for automatically learning similarities with word embedding algorithms; and (ii) reuse our work for different (similar) tasks (see paraphrasing) and data. The simplicity and portability of our system is a significant contribution to a very complex research area such as RL from two entire pieces of text. Our study has indeed shown that our kernel models, which are very simple to be implemented, reach the state of the art and can be used with large datasets. Furthermore, it should be noted that our models outperform the best tree kernel approach of the RTE challenges (Zanzotto and Moschitti, 2006) and also its extension that we proposed in (Zanzotto et al., 2009) . These systems are also adaptable and easy to replicate, but they are subject to an exponential computational complexity and can thus only be used on very small datasets (e.g., they cannot be applied to the MSR Paraphrase corpus). In contrast, the model we proposed in this paper can be used on large datasets, because its kernel complexity is about linear (on average). We believe that disseminating these findings to the research community is very important, as it will foster research on RL, e.g., on RTE, using structural kernel methods. Such research has had a sudden stop as the RTE data in the latest challenges increased from 800 instances to several thousands and no tree kernel model has been enough accurate to replace our computational expensive models (Zanzotto et al., 2009) . In the future, it would be interesting defining graph kernels that can combine more than two substructures. Another possible extension regards the use of node similarity in graph kernels. Additionally, we would like to test our models on other RTE challenges and on several QA datasets, which for space constraints we could not do in this work. Acknowledgments This research is part of the Interactive sYstems for Answer Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Hamad Bin Khalifa University and Qatar Foundation.",
    "abstract": "This paper studies the use of structural representations for learning relations between pairs of short texts (e.g., sentences or paragraphs) of the kind: the second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic relations between the constituents composing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of stateof-the-art models for this type of relational learning. Our findings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identification and Textual Entailment Recognition.",
    "countries": [
        "Qatar"
    ],
    "languages": [
        "English",
        "Arabic"
    ],
    "numcitedby": "49",
    "year": "2015",
    "month": "July",
    "title": "Structural Representations for Learning Relations between Pairs of Texts"
}