{
    "article": "We present results that show that incorporating lexical and structural semantic information is effective for word sense disambiguation. We evaluated the method by using precise information from a large treebank and an ontology automatically created from dictionary sentences. Exploiting rich semantic and structural information improves precision 2-3%. The most gains are seen with verbs, with an improvement of 5.7% over a model using only bag of words and n-gram features. Introduction Recently, significant improvements have been made in combining symbolic and statistical approaches to various natural language processing tasks. In parsing, for example, symbolic grammars are being combined with stochastic models (Riezler et al., 2002; Oepen et al., 2002; Malouf and van Noord, 2004) . Statistical techniques have also been shown to be useful for word sense disambiguation (Stevenson, 2003) . However, to date, there have been few combinations of sense information together with symbolic grammars and statistical models. Klein and Manning (2003) show that much of the gain in statistical parsing using lexicalized models comes from the use of a small set of function words. Features based on general relations provide little improvement, presumably because the data is too sparse: in the Penn treebank normally used to train and test statistical parsers stocks and skyrocket never appear together. They note that this should motivate the use of similarity and/or class based approaches: the superordinate concepts capital (\u2283 stocks) and move upward (\u2283 sky rocket) frequently appear together. However, there has been little success in this area to date. For example, Xiong et al. (2005) use semantic knowledge to parse Chinese, but gain only a marginal improvement. Focusing on WSD, Stevenson (2003) and others have shown that the use of syntactic information (predicate-argument relations) improve the quality of word sense disambiguation (WSD). McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. However, there is still little work on combining WSD and parse selection. We hypothesize that one of the reasons for the lack of success is that there has been no resource annotated with both syntactic (or structural semantic information) and lexical semantic information. For English, there is the SemCor corpus (Fellbaum, 1998) which is annotated with parse trees and Word-Net senses, but it is fairly small, and does not explicitly include any structural semantic information. Therefore, we decided to construct and use a treebank with both syntactic information (e.g. HPSG parses) and lexical semantic information (e.g. sense tags): the Hinoki treebank (Bond et al., 2004) . This can be used to train word sense disambiguation and parse ranking models using both syntactic and lexical semantic features. In this paper, we discuss only word sense disambiguation. Parse ranking is discussed in Fujita et al. (2007) . The Hinoki Corpus The Hinoki corpus consists of the Lexeed Semantic Database of Japanese (Kasahara et al., 2004 ) and corpora annotated with syntactic and semantic infor-mation. Lexeed Lexeed is a database built from on a dictionary, which defines word senses used in the Hinoki corpus and has around 49,000 dictionary definition sentences and 46,000 example sentences which are syntactically and semantically annotated. Lexeed consists of all words with a familiarity greater than or equal to five on a scale of one to seven. This gives a fundamental vocabulary of 28,000 words, divided into 46,347 different senses. Each sense has a definition sentence and example sentence written using only these 28,000 familiar words (and some function words). Many senses have more than one sentence in the definition: there are 75,000 defining sentences in all. A (simplified) example of the entry for \u00cd\u00bf untenshu \"chauffeur\" is given in Figure 1 . Each word contains the word itself, its part of speech (POS) and lexical type(s) in the grammar, and the familiarity score. Each sense then contains definition and example sentences, links to other senses in the lexicon (such as hypernym), and links to other resources, such as the Goi-Taikei (Ikehara et al., 1997) and WordNet (Fellbaum, 1998) . Each content word in the definition and example sentences is annotated with sense tags from the same lexicon. Lexical Semantics Annotation The lexical semantic annotation uses the sense inventory from Lexeed. All words in the fundamental vocabulary are tagged with their sense. For example, the word \u00b2 ookii \"big\" (in ookiku naru \"grow up\") is tagged as sense 5 in the example sentence (Figure 1 ), with the meaning \"elder, older\". Each word was annotated by five annotators. We use the majority choice in case of disagreements (Tanaka et al., 2006) . Inter-annotator agreements among the five annotators range from 78.7% to 83.3%: the lowest agreement is for the Lexeed definition sentences and the highest is for Kyoto corpus (newspaper text). These agreements reflect the difficulties in disambiguating word sense over each corpus and can be considered as the upper bound of precision for WSD. Table 1 shows the distribution of word senses according to the word familiarity in Lexeed. Ontology The Hinoki corpus comes with an ontology semiautomatically constructed from the parse results of definitions in Lexeed (Nichols and Bond, 2005) . The ontology includes more than 80 thousand relationships between word senses, e.g. synonym, hypernym, abbreviation, etc. The hypernym relation for \u00cd\u00bf untenshu \"chauffeur\" is shown in Figure 1 . Hypernym or synonym relations exist for almost all content words. Thesaurus As part of the ontology verification, all nominal and most verbal word senses in Lexeed were linked to semantic classes in the Japanese thesaurus, Nihongo Goi-Taikei (Ikehara et al., 1997) . These were then hand verified. Goi-Taikei has about 400,000 words including proper nouns, most nouns are classified into about 2,700 semantic classes. These semantic classes are arranged in a hierarchical structure (11 levels). The Goi-Taikei Semantic Class for \u00cd\u00bf untenshu \"chauffeur\" is shown in Figure 1 : C292:driver at level 9 which is subordinate to C4:person . Syntactic and Structural Semantics Annotation Syntactic annotation is done by selecting the best parse (or parses) from the full analyses derived by a broad-coverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002) , which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. As the grammar is based on a monostratal theory of grammar (HPSG: Pollard and Sag, 1994) it is possible to simultaneously annotate syntactic and semantic structure without overburdening the annotator. Using a grammar enforces treebank consistency -all sentences annotated are guaranteed to have well- \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 INDEX \u00cd\u00bf untenshu POS noun LEX-TYPE noun-lex FAMILIARITY 6.2 [1-7] (\u2265 5) SENSE 1 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 DEFINITION 1 \u2104 1 \u00cd 1 \u00be \u00bc 4 a ) WORDNET motorman 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb Figure 1 : Dictionary Entry for \u00cd\u00bf 1 untenshu \"chauffeur\" formed parses. The flip side to this is that any sentences which the parser cannot parse remain unannotated, at least unless we were to fall back on full manual mark-up of their analyses. The actual annotation process uses the same tools as the Redwoods treebank of English (Oepen et al., 2002) . There were 4 parses for the definition sentence shown in Figure 1 . The correct parse, shown as a phrase structure tree, is shown in Figure 2 . The two sources of ambiguity are the conjunction and the relative clause. The parser also allows the conjunction to join to densha and \u00bc hito. In Japanese, relative clauses can have gapped and non-gapped readings. In the gapped reading (selected here), \u00bc hito is the subject of \u00cd unten \"drive\". In the nongapped reading there is some underspecified relation between the thing and the verb phrase. This is similar to the difference in the two readings of the day he knew in English: \"the day that he knew about\" (gapped) vs \"the day on which he knew (something)\" (non-gapped). Such semantic ambiguity is resolved by selecting the correct derivation tree that includes the applied rules in building the tree. The parse results can be automatically given by the HPSG parser PET (Callmeier, 2000) with the Japanese grammar JACY. The current parse ranking model has an accuracy of 70%: the correct tree is ranked first 70% of the time (for Lexeed definition sentences) (Fujita et al., 2007) . The full parse is an HPSG sign, containing both syntactic and semantic information. A view of the semantic information is given in Figure 3 1 . 1 The specific meaning representation language used in The semantic view shows some ambiguity has been resolved that is not visible in the purely syntactic view. UTTERANCE NP VP N PP V NP PP N CONJ N CASE-P V V \u2104 \u2104 \u2104 \u00cd \u00cd \u00cd \u00be \u00be \u00be \u00bc \u00bc \u00bc The semantic view can be further simplified into a dependency representation, further abstracting away from quantification, as shown in Figure 4 . One of the advantages of the HPSG sign is that it contains all this information, making it possible to extract the particular view needed. In order to make linking to other resources (such as the sense annotation) easier, predicates are labeled with pointers back to their position in the original surface string. For example, the predicate densha n 1 links to the surface characters between positions 0 and 3: . JACY is Minimal Recursion Semantics (Copestake et al., 2005) . \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 TEXT \u2104 \u00cd\u00be \u00bc TOP h1 RELS \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \uf8ee \uf8ef \uf8f0 proposition m rel LBL h1 ARG0 e2 MARG h3 \uf8f9 \uf8fa \uf8fb \uf8ee \uf8ef \uf8f0 unknown rel LBL h4 ARG0 e2 ARG x5 \uf8f9 \uf8fa \uf8fb \uf8ee \uf8f0 densha n LBL h6 ARG0 x7 \uf8f9 \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 udef rel LBL h8 ARG0 x7 RSTR h9 BODY h10 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 ya p LBL h11 ARG0 x13 L-INDEX x7 R-INDEX x12 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 udef rel LBL h15 ARG0 x12 RSTR h16 BODY h17 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8f0 jidousha n LBL h18 ARG0 x12 \uf8f9 \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 udef rel LBL h19 ARG0 x12 RSTR h20 BODY h21 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 unten s LBL h22 ARG0 e23 tense=present ARG1 x5 ARG2 x13 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8f0 hito n LBL h24 ARG0 x5 \uf8f9 \uf8fb \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 udef rel LBL h25 ARG0 x5 RSTR h26 BODY h27 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb \uf8ee \uf8ef \uf8f0 proposition m rel LBL h10001 ARG0 e23 tense=present MARG h28 \uf8f9 \uf8fa \uf8fb \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe HCONS {h3 qeq h4, h9 qeq h6, h16 qeq h11, h20 qeq h18, h26 qeq h24, h28 qeq h22} ING {h24 ing h10001} \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb Task We define the task in this paper as \"allocating the word sense tags for all content words included in Lexeed as headwords, in each input sentence\". This task is a kind of all-words task, however, a unique point is that we focus on fundamental vocabulary (basic words) in Lexeed and ignore other words. We use Lexeed as the sense inventory. There are two problems in resolving the task: how to build the model and how to assign the word sense by using the model for disambiguating the senses. We describe the word sense selection model we use in section 4 and the method of word sense assignment in section 5. Word Sense Selection Model All content words (i.e. basic words) in Lexeed are classified into six groups by part-of-speech: noun, verb, verbal noun, adjective, adverb, others. We treat the first five groups as targets of disambiguating senses. We build five words sense models corresponding to these groups. A model contains senses for various words, however, features for a word are discriminated from those for other words so that the senses irrelevant to a target word are not selected. For example, an n-gram feature following a target word \"has-a-tail\" for dog is distinct from that for cat. In the remainder of this section, we describe the features used in the word sense disambiguation. First we used simple n-gram collocations, then a bag of words of all words occurring in the sentence. This was then enhanced by using ontological information and predicate argument relations. Word Collocations Word collocations (WORD-Col) are basic and effective cues for WSD. They can be modelled by ngram and bag of words features, which are easily extracted from a corpus. We used all unigrams, bigrams and trigrams which precede and follow the target words (N-gram) and all content words in the sentences where the target words occur (BOW). 2 : Example semantic collocation features (SEM-Col) extracted from the word sense tagged corpus and the dictionary (Lexeed and GoiTaikei) and the ontology which have the word senses and the semantic classes linked to the semantic tags. The first column numbers the feature template corresponding to each example. Semantic Features We use the semantic information (sense tags and ontologies) in two ways. One is to enhance the collocations and the other is to enhance dependency relations. Semantic Collocations Word surface features like N-gram and BOW inevitably suffer from data sparseness, therefore, we generalize them to more abstract words or concepts and also consider words having the same meanings. We used the ontology described in Section 2.3 to get hypernyms and synonyms and the Goi-Taikei thesaurus to abstract the words to the semantic classes. The superordinate classes at level 3, 4 and 5 are also added in addition to the original semantic class. For example, densha \"train\" and jidousha \"automobile\" are both generalized to the semantic class C988:land vehicle (level 7). The superordinate classes are also used: C706:inanimate (level 3), C760:artifact (level 4) and C986:vehicle (level 5). Semantic Dependencies The semantic dependency features are based on a predicate and its arguments taken from the elementary dependencies. For example, consider the semantic dependency representation for densha ya jidousha-wo unten suru hito \"a person who drives a train or car\" given in Figure 4 . The predicate unten \"drive\", has two arguments: ARG1 hito \"person\" and ARG2 ya \"or\". The coordinate conjunction is expanded out into its children, giving ARG2 densha \"train\" and jidousha \"automobile\". From these, we produce several features, a sample of them are shown in Table 3 . One has all arguments and their labels (D11). We also produce various back offs, for example the predicate with only one argument at a time (D1-D3). Each combination of predicate and its related argument(s) becomes a feature. For the next class of features, we used the sense information from the corpus combined with the semantic classes in the dictionary to replace each pred-icate by its disambiguated sense, its hypernym, its synonym (if any) and its semantic class. The semantic classes for 1 and 1 are both 988:land vehicle , while \u00cd 1 is 2003:motion and \u00bc 4 is 4:human . We also expand 1 into its synonym \u00f5 ae 1 m\u014dt\u0101k\u0101 \"motor car\". The semantic class features provide a semantic smoothing, as words are binned into the 2,700 classes. The hypernym/synonym features provide even more smoothing. Both have the effect of making more training data available for the disambiguator. Domain Domain information is a simple and sometimes strong cue for disambiguating the target words (Gliozzo et al., 2005) . For instance, the sense of the word \"record\" is likey to be different in the musical context, which is recalled by domain-specific words like \"orchestra\", \"guitar\", than in the sporting context. We use 12 domain categories like \"culture/art\", \"sport\", etc. which are similar to ones used in directory search web sites. About 6,000 words are automatically classified into one of 12 domain categories by distributions in web sites (Hashimoto and Kurohashi, 2007) and 10% of them are manually checked. Polysemous words which belong to multiple domains and neutral words are not classified into any domain. Search Algorithm The conditional probability of the word sense for each word is given by the word sense selection model described in Section 4. In the initial state, some of the semantic features, e.g. semantic collocations (SEM-Col) and word sense extensions for semantic dependencies (SEM-Dep) are not available, since no word senses for polysemous words have been determined. It is not practical to count all combinations of word senses for target words, therefore, we first try to decide the sense for that word which is most plausible among all the ambiguous words, then, disambiguate the next word by using the sense. We use the beam search algorithm, which is similar to that used for decoder in statistical machine translation (Watanabe, 2004) , for finding the plausible combination of word sense tags. The algorithm is described as follows. For a polysemous word set in an input sentence {w 1 , . . . , w n }, t w i k is the k-th word sense of word w i , W is a set having words to be disambiguated, T is a list of resolved word senses. A search node N is defined as [W, T ] and a score of a node N, s(N) is defined as the probability that the word sense set T occurs in the context. The beam search can be done as follows (beam width is b): 1. Create an initial node N 0 = [T 0 ,W 0 ] (T 0 = {}, W 0 = {}) and insert the node into an initial queue Q 0 . 2. For each node N in the queue Q, do the following steps. \u2022 For each w i (\u2208 W ), create W \u2032 i by picking out w i from W \u2022 Create new lists T \u2032 1 , . . . , T \u2032 l by adding one of word sense candidates t w i 1 ,. . . ,t w i l for w i to T \u2022 Create new nodes [W \u2032 i , T \u2032 0 ], . . . ,[W \u2032 i , T \u2032 l ] Evaluation We trained and tested on the Lexeed Dictionary Definition (LXD-DEF) and Example sections (LXD-EX) of the Hinoki corpus (Bond et al., 2007) . These have about 75,000 definition and 46,000 example sentences respectively. Some 54,000 and 36,000 sentences of them are treebanked, i.e., they have the syntactic trees and structural semantic information. We used these sentences with the complete information and selected 1,000 sentences out of each sentence class as test sets (LXD-DEF test , LXD-EX test ), and the remainder is combined and used as a training set (LXD-ALL). We also tested 1,000 sentences from the Kyoto Corpus of newspaper text (KYOTO test ). These sentences have between 3.4 (LXD-EX test ) -5.2 (KYOTO test ) polysemous words per sentence on average. We use a maximum entropy / minimum divergence Results and Discussion Table 4 shows the precision as the results of the word sense disambiguation on the combination of LXD-DEF and LXD-EX (LXD-ALL). The baseline method selects the senses occurring most frequently in the training corpus. Each row indicates the results using the baseline, word collocation (WORD-Col), the combinations of WORD-Col and one of the semantic features (+SEM-Col, +SEM-Dep and +DOMAIN), e.g, +SEM-Col gives the results using WORD-Col and SEM-Col, and all features (FULL). There are significant improvements over the baseline and the other results on all corpora. Basic word collocation features (WORD-Col) give a vast improvement. Extending this by using the ontological information (+SEM-Col) gives a further improvement over the WORD-Col. Adding the predicate-argument relationships (+SEM-Dep) improves the results even more. Table 6 shows the statistics of the target corpora. The best result of LXD-DEF test (80.7%) surpasses the inter-annotator agreement (78.7%) in building the Hinoki Sensebank. However, there is a wide gap between the best results of KYOTO test (60.4%) and the inter-annotator agreement (83.3%), this suggests other information such as the semantic classes for named entities (including proper nouns and multiword expressions (MWE)) and broader contexts are required. However, a model built on dictionary sentences lacks these features. Even, so there is some improvement. The domain features (+DOMAIN) give small contribution to the precision, since only intra-sentence context is counted in this experiment. Unfortunately dictiory definition and example sentences do not really have a useful context. We expect broader context should make the domain features more effective for the newspaper text (e.g. as in Stevenson (2003) ), Table 5 shows comparison of results of different POSs. The semantic features (+SEM-Col and +SEM-Dep) are particularly effective for verb and also give moderate improvements on the results of the other POSs. Figure 5 shows the precisions of LXD-DEF test in changing the size of a training corpus, which is divided into five partitions. The precision is saturated in using four partitions (264,000 tokens). These results of the dictionary sentences are close to the best published results for the SENSEVAL-2 task (79.3% by Murata et al. (2003) using a combination of simple Bayes learners). However, we are using a different sense inventory (Lexeed not Iwanami (Nishio et al., 1994) ) and testing over a different corpus, so the results are not directly comparable. In future work, we will test over SENSEVAL-2 data so that we can compare directly. None of the SENSEVAL-2 systems used ontological information, despite the fact that the dictionary definition sentences were made available, and there are several algorithms describing how to extract such information from MRDs (Tsurumaru Acknowledgements We would like to thank the other members of the NTT Natural Language Research Group NTT Communication Science laboratories for their support. We would also like to express gratitude to the reviewers for their valuable comments and Professor Zeng Guangping, Wang Daliang and Shen Bin of the University of Science and Technology Beijing (USTB) for building the demo system. , 1991; Wilkes et al., 1996; Nichols et al., 2005) . We hypothesize that this is partly due to the way the task is presented: there was not enough time to extract and debug an ontology as well as build a disambiguation system, and there was no ontology distributed. The CRL system (Murata et al., 2003) used a syntactic dependency parser as one source of features (KNP: Kurohashi and Nagao ( 2003 )), removing it decreased performance by around 0.6%. Conclusions We used the Hinoki corpus to test the importance of lexical and structural information in word sense disambiguation. We found that basic n-gram features and collocations provided a great deal of useful information, but that better results could be gained by using ontological information and semantic dependencies.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 0.0,
        "none": 0.9875615199142485
    },
    "reasoning": "Reasoning: The article does not provide specific information regarding its funding sources. Without explicit mentions of support from defense, corporate entities, research agencies, foundations, or an indication of no funding, it is not possible to accurately determine the funding sources based on the provided text."
}