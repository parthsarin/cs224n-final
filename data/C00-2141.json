{
    "article": "In this paper, we proposed a shallow syntactic knowledge description: constituent boundary representation and its simple and efficient prediction algorithm, based on different local context templates learned from the annotated corpus. An open test on 2780 Chinese real text sentences showed the satisfying results: 94%(92%) precision for the words with multiple (single) boundary tag output. Introduction Research on syntactic parsing has been a focus in natural language processing for a long time. As the development of corpus linguistics, many statistics-based parsers were proposed, such as Magerman(1995) 's statistical decision tree parser, Collins(1996) 's bigram dependency model parser, Ratnaparkhi(1997) 's maximum entropy model parser. All of them tried to get the complete parse trees of the input sentences, based on the statistical data extracted from an annotated corpus. The best parsing accuracy of these parsers was about 87%. Realizing the difficulties of complete parsing, many researches turned to explore the partial parsing techniques. Church(1988) proposed a simple stochastic technique for recognizing the non-recursive base noun phrases in English. Voutilaimen(1993) designed an English noun phrase recognition tool ---NPTool. Abney(1997) applied both rule-based and statistics-based approaches for parsing chunks in English. Due to the advantages of simplicity and robustness, these systems can be acted as good preprocessors for the further complete parsing. In this paper, we will introduce our partial parsing approach for the Chinese language. We first proposed a shallow syntactic knowledge description: constituent boundary representation. It simplified the complex constituent levels in parse trees and only kept the boundary information of every word in different constituents. Then, we developed a simple and efficient constituent boundary prediction algorithm, based on different local context templates learned from the annotated corpus. An open test on 2780 Chinese real text sentences showed the satisfying results: 94%(92%) precision for the words with multiple (single) boundary tag output. Constituent boundary description The constituent boundary representation comes from the simplification of the complete parse trees of the sentences. It omits the constituent 1 levels in parse trees and only keeps the boundary information of every word in different constituents, i.e. it is at the left boundary, right boundary or middle position of a constituent. Evidently, if the input sentence has only one parse tree, i.e. without syntactic ambiguity, the constituent boundary position of every word in the sentence is clear and definite. In the sense, the constituent boundary tag indicates the basic syntactic structure information in the sentence. Separating them from the constituent structure tree and assigning them to every word in the sentence, we can form a special syntactic unit: word boundary block (WBB). Definition: A word boundary block is the combination of the word(including part-of-speech information) and its constituent boundary tag, i.e. wbb i =<w i , b i >, where w i is the ith word in the sentence, b i can value 0,1,2, which means w i is at the middle, left-most, or right-most position of a constituent respectively. In the view of syntactic description capability, the WBBs defined above, the chunks defined by Abney(1991) and the phrases(i.e. constituents) defined in a parse tree have the following realtions: WBBs < chunks < phrases Here is an example: z The input sentence (10 words): \u00e5 : \u00f3\u00f3 Z \u00aa \u00d4 : \u00c4 (My brother gives him a book.) z Its parse tree representation (7 phrases): [ P1 [ P2 [ P3 \u00e5 : \u00f3\u00f3 ] [ P4 [ P5 Z ] \u00aa [ P6 [ P7 \u00d4 ] : ]]] \u00c4] z Its chunk representation (5 chunks): The goal of the constituent boundary prediction is to assign a suitable boundary tag for every word in the sentence. It can provide basic information for further syntactic parsing research. The following lists some application examples: [ C1 \u00e5 : \u00f3\u00f3 ] [ C2 Z ] [ C3 \u00aa ] [ C4 \u00d4 : ] [ C5 \u00c4] z To develop a statistics-based Chinese parser (Zhou 1997) based on the bracket matching principle (Zhou and Huang,1997) . z To develop a Chinese maximum noun phrase identifier (Zhou,Sun and Huang, 1999) . z The automatic inference of Chinese probabilistic context-free grammar(PCFG) (Zhou and Huang 1998) . Local context templates The linguistic intuitions tell us that many local contexts may be useful for constituent boundary prediction. For example, many function words in Chinese have their certain constituent boundary position in the sentences, such as, most prepositions are at the left boundaries, and the aspectual particles (\"le\", \"zhe\", \"guo\") are at the right boundaries. Moreover, some content words also show their preferential constituent boundary positions in a special local context, such as most adjectives are at the right boundary in local context: \"adverb + adjective\". A tentative idea is how to use such simple local context information(including the part-ofspeech(POS) tags and the number of Chinese characters(CN)) to develop an efficient automatic boundary prediction algorithm. Therefore, we defined the following local context templates (LCTs): 1) Unigram POS template: t i , BPFL i 2) Bigram POS templates: z Left restriction: t i-1 t i , BPFL i z Right restriction: t i t i+1 , BPFL i 3) Trigram POS template: t i-1 t i t i+1 , BPFL i 4) Trigram POS+CN template: t i-1 +cn i-1 t i +cn i t i+1 +cn i+1 , BPFL i In the above LCTs, t i is the POS tag of the ith word in the sentence, cn i is its character number, and BPFL i is the frequency distribution list of its different BP(boundary prediction) value(0,1,2) under the local context restrictions(LCR)(the left and right word). Q Q X-'( A noun is prior to at the right boundary if its previous word is a noun and its next one is a partial(De). Table 1 shows some examples of LCTs. All these templates can be easily acquired from the Chinese treebanks or Chinese corpus annotated with constituent boundary tags. Among these templates, some special ones have the following properties: a) TF i = \u2211 BPFL i [bp i ] > \u03b1, b) \u2203bp i \u2208[0,2], P(bp i |LCR i )=BPFL i [bp i ] / TF i > \u03b2 where the total frequency threshold \u03b1 and the BP probability threshold \u03b2 are set to 3 and 0.95, respectively. They are called the projected templates (PTs) (i.e. the local context template with a projecting BP value). Based on the different PTs, we can design a three-stage training procedure to overcome the problem of data sparseness: Therefore, only the useful trigram templates can be learned. Automatic prediction algorithm After getting the LCTs, the automatic prediction algorithm becomes very simple: 1) to set the projecting BPs based on the projected LCTs, 2) to select the best BPs based on the nonprojected LCTs. Some detailed information will be discussed in the following sections. Set the projecting BPs In this stage, the reference sequence to the \u00be TF L + TF R = \u2211 BPFL L [j] +\u2211 BPFL L [j] > \u03b1 \u00be P(bp j | LCR i ) = (BPFL L [j] + BPFL R [j]) / (TF L + TF R ) > \u03b2 then return this combined projecting BP(bp j ). z If its trigram POS template is a PT, then return its projecting BP. z If its trigram POS+CN template is a PT, then return its projecting BP. Select the best BPs In this stage, the reference sequence to the LCTs is : trigram POS+CN AE trigram POS AE bigram AE unigram. It's a backing-off model (Katz,1987) , just like the approach of Collins and Brooks(1995) for the prepositional phrase attachment problem in English. The detailed algorithm is as follows: Input: the position of the ith word in the sentence. Background: the LCTs learned from corpus. Output: the best BP of the word. z For the kth matched unigram templates, if TF k > 0, then return SelectBestBP(BPFL k ). z Return 1(default is at the left boundary). The internal function SelectBestBP() tries to select the best BP based on the frequency distribution list of different BP value in LCTs. It has two output modes: 1) single-output mode: only output the best BP with the highest frequency in the LCT; 2) multiple-output mode: output the BPs satisfying the conditions: |P bpi -P best | < \u03b3, where \u03b3 = 0.2 Experimental results Training and test data The training data were extracted from two different parts of annotated Chinese corpus: 1) The small Chinese treebank developed in Peking University (Zhou, 1996b) , which consists of the sentences extracted from two parts of Chinese texts: (a) test set for Chinese-English machine translation systems, (b) Singapore primary school textbooks. 2) The test suite treebank being developed in Tsinghua University (Zhou and Sun,1999) , which consists of about 10,000 representative Chinese sentences extracted from a large-scale Chinese balanced corpus with about 2,000,000 Chinese characters. The test data were extracted from the articles of People's Daily and manually annotated with correct constituent boundary tags. It was also divided into two parts: 1) The ordinary sentences. 2) The sentences with keywords for conjunction structures (such as the conjunctions or special punctuation 'DunHao'). They can be used to test the performance of our prediction algorithm on complex conjunction structures. Table 2 shows some basic statistics of these training and test data. Only the sentences with more than one word were used for training and testing. The learned templates After the three-stage learning procedure, we got four kinds of local context templates. Table 3 shows their different distribution data, where the section 'Type' lists the distribution of different kinds of LCTs and the section 'Token' lists the distribution of total words(i.e. tokens) covered by the LCTs. In the column 'PTs' and 'Ratio', the slash '/' was used to separate the PTs with total frequency threshold 0 and 3. More than 66% words in the training corpus can be covered by the unigram and bigram POS projected templates. Then only about 1/3 tokens will be used for training the trigram templates. Although the type distribution of the trigram templates shows the tendency of data sparseness (more than 70% trigram projected templates with total frequency less than 3), the useful trigram templates (TF>3) still covers about 70% tokens learned. Therefore, we can expect that them can play an important role during constituent boundary prediction in open test set. Prediction results In order to evaluate the performance of the constituent boundary prediction algorithm, the following measures were used: 1) The cost time(CT) of the kernal functions(CPU: Celeron TM 366, RAM: 64M). 2) Prediction precision(PP) = number of words with correct BPs(CortBP) total word number (TWN) For the words with single BP output, the correct condition is: Annotated BP = Predicted BP For the words with multiple BP outputs, the correct condition is: Annotated BP \u2208 Predicted BP set The prediction results of the two test sets were shown in Table 4 and Table 5 , whose first columns list the different template combinations using in the algorithm. In the columns 'CortBP' and 'PP', the slash '/' was used to list the different results of the single and multiple BP outputs. After analyzing the experimental results, we found: 1) The POS information in local context is very important for constituent boundary prediction. After using the bigram and trigram POS templates, the prediction accuracy was increased by about 9% and 3% respectively. But the character number information shows lower boundary restriction capability. Their application only results in a slight increase of precision in single-output mode but a slight decrease in multiple-output mode. 2) Most of the prediction errors can be attributed to the special structures in the sentences, such as conjunction structures (CSs) or collocation structures. Due to the long distance dependencies among them, it's very difficult to assign the correct boundary tags to the words in these structures only according to the local context templates. The lower overall precision of the test set 2 (about 2% lower than test set 1) also indicates the boundary prediction difficulties of the conjunction structures, because there are more CSs in test set 2 than in test set 1. 3) The accuracy of the multiple output results is about 2% better than the single output results. But the words with multiple boundary tags constitute only about 10% of the total words predicted. Therefore, the multiple-output mode shows a good trade-off between precision and redundancy. It can be used as the best preprocessing data for the further syntactic parser. 4) The maximal ratio of the words set by projected templates can reach 80%. It guarantees the higher overall precision. 5) The algorithm shows high efficiency. It can process about 6,000 words per second (CPU: Celeron TM 366, RAM: 64M). Zhou(1996) proposed a constituent boundary prediction algorithm based on hidden Marcov model(HMM). The Viterbi algorithm was used to find the best boundary path B': Compare with other work \u220f = \u2212 = = \u2032 n i i i i b bi P b CT P B P B T W P B 1 1) | ( ) | ( max arg ) ( ) | , ( max arg where the local POS probability P(CT i | b i ) was computed by backing-off model and the bigram parameters: f(t i-1 , t i , b i ) and f(b i , t i , t i+1 ). To compare its performance with our algorithm, the trigram (POS and POS+CN) information was added up to its backing-off model. Table 6 and Table 7 show the prediction results of the HMM-based algorithm, based on the same parameters learned from training set 1 and 2. The performance of the LCT-based algorithm surpassed the HMM-based algorithm in accuracy(about 1%) and efficiency (about 10 times). Another similar work is Sun(1999) . The difference lies in the definition of the constituent boundary tags: he defined them between word pair: w i b i w i+1 , not for the word. By using the HMM and Viterbi model, his algorithm showed the similar performance with Zhou(1996) Conclusions The paper proposed a constituent boundary prediction algorithm based on local context templates. Its characteristics can be summarized as follows: z The simple definition of the local context templates made the training procedure very easy. z The three-stage training procedure guarantees that only the useful trigram templates can be learned. Thus, the data sparseness problem was partially overcome. z The high coverage of different types of projected templates assures a higher overall prediction accuracy. z The multiple output mode provides the possibility to describe different boundary ambiguities. z The algorithm runs very fast, surpasses the HMM-based algorithm in accuracy and efficiency. There are a few possible improvement which may raise performance further. Firstly, some lexical-based templates, such as prepositions as left restriction, may improve performance further -this needs to be investigated. The introduction of the automatic identifiers for some special structures, such as conjunction structures or collocation structures, may reduce the prediction errors due to the long distance dependency problem. Finally, more training data is almost certain to improve results. Acknowledgements The research was supported by National Natural Science Foundation of China (NSFC) (Grant No. 69903007).",
    "abstract": "In this paper, we proposed a shallow syntactic knowledge description: constituent boundary representation and its simple and efficient prediction algorithm, based on different local context templates learned from the annotated corpus. An open test on 2780 Chinese real text sentences showed the satisfying results: 94%(92%) precision for the words with multiple (single) boundary tag output.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": "1",
    "year": "2000",
    "month": "",
    "title": "Local context templates for {C}hinese constituent boundary prediction"
}