{
    "article": "INTRODUCTION I was most honoured by the invitation to give the opening paper at the 10th anniversary of Translating and the Computer. These conferences have become both a meeting place for old friends and an occasion for finding out about new developments. It is hard to say which of these functions is the more useful, but for those who have been regular attenders this conference anniversary was both a sad and a happy occasion. Sad because we remembered the friends who are no longer with us, happy because we continue to share common interests and were able to welcome the younger generation and share with them some developments in information technology that bring us together every year. My own contribution to the conference theme, the translation environment ten years on, is to look back over the last ten years since it was my privilege to open the very first of these conferences in 1978. My main interest, however, lies in the future. As a member of the older generation who has participated in the errors of the past, I shall use this perspective to present the lessons I believe we have learnt or can learn. The subtitle and theme of this paper is From FAHQT to realism: with it I want to indicate that the originally-stipulated goal of machine translation of the 1960s, represented by the unwieldy acronym FAHQT (Fully Automated High Quality Translation) was not only overambitious but itself unrealistic. Now that we know more about the capabilities of the computer and also understand better what is involved in translation we can even say that the goal was wrongly expressed. This goal should always have been 'maximum assistance in text processing, understanding and translation' or, if we want to match the earlier acronym, MATPUT. This goal is open-ended and we can confidently say that we are moving towards it. The structure of this paper is quite simple: I intend to survey the last ten years, and draw some conclusions about future development prospects. Space permits only a very broad brush technique with huge gaps and over-generalisations, perhaps inappropriate in an academic paper or a carefully-researched report. But let us make a virtue out of this necessity: since I cannot argue my case exhaustively, I have chosen to be provocative, to stimulate discussion and reflection throughout and after the end of the conference. I shall start with the summary of my conclusions, which at this stage are to be considered hypotheses that have to be proved. I base my claims on a regular involvement with these issues for many years and from many different points of view. ANTICIPATED CONCLUSIONS As an industrial process, MT is neither an isolated nor an isolatable activity. MT has to fit into a sophisticated text processing environment combined with electronic communication. Since, however, modern communications are themselves in a state of rapid transformation MT design cannot remain static. As new forms of communication evolve, such as electronic mail (e-mail), factual databases and expert systems, new forms of translation aids will have to be developed to meet the requirements of new text forms and new communicative situations. As an object of study, MT requires not only a linguistic but a pragmatic theory, because texts that require translation occur in a particular communicative environment which to some extent influences the linguistic forms chosen; we must understand the motivation for these linguistic choices since they differ from language to language. We must further acknowledge that not all information contained in a text is expressed linguistically; we must have means of identifying, understanding and conveying this information in the target language. This inclusion of pragmatics in computational linguistic research is just beginning; it will take some time before it can influence system design. The full implication of what MT can do, or more generally what natural language processing can achieve in order to increase efficient communication, has hardly been explored. MT system developers do not yet distinguish types of systems by their user or subject orientation. There are, however, at least four functionally different types of MT -as a tool for translators; for monolingual writers; for readers of scientific and technical literature; and for databases. We are short of trained people to work on MT design. As there are not enough trained computational linguists to carry out advanced research into machine translation, it follows that there are not enough teachers of the necessary skills for future researchers. In what follows I hope to develop these points in some greater detail. RETROSPECTIVE In 1978 the United Kingdom had its first practical encounter with machine translation. Ten years later we can say, that the translation profession has become computer literate, but only a small number of people use the relatively few commercially available translation systems. Why? In order to feel the full impact we have to wait until translators are also computer-wise. Translators I think the answer has to be sought in the complexity of the necessary technology; hence the delay in the training and familiarisation of translators. A paradoxical subsidiary answer can be found in the simultaneous introduction of word processing and with it the beginnings of a translator's workstation. Translators can now edit text and build dictionaries for particular applications. All these facilities contribute both to quality improvements and to greater economy of production. This new technology has absorbed most translators' capacity for innovation while having at the same time considerably increased their productivity. At any one time there is only a limited capacity for absorbing new ideas and techniques. To put it in other words, translation as a craft is intrinsically linked to its conventional tools and most translators are traditionalists by nature, preservers of balances, accuracies and niceties. Some very successful professional translators still insist on using only pen and paper. It takes a long time, therefore -probably a full generation -before translators will fully accept the new tools. I need not point out that it is by and large the younger generation of translators that have accepted experimentation with MT. Producers MT developers also are still learning how to sell their product. The initial hard-sell techniques had an alienating effect on users. A climate of suspicion about the danger to the workplace was met by excessive claims by manufacturers and designers. Towards the end of the decade, the original hostility and subsequent scepticism have now been replaced by an understanding of the limitations of MT and a sober appreciation of the real benefits to be derived from new technology in general. Manufacturers of MT systems have also moderated their excessive early claims of performance and applicability of benefits. Potential users can now get more reliable information about systems through various journals such as the late Language Monthly, now merged with Language Technology,* or the restyled Computers and Translation. Information rather than publicity is now available. Finally we have also established who should be involved in the design and construction of MT systems. In 1978 it was said that translators had a key role; yet few translators joined the ranks of computational linguists. So this was not the solution hoped for. Instead MT designers have learned to listen to translators and have found out in the process that translation is a more complex activity than they had thought. It is now widely accepted that translation systems have to be based on models of translation which themselves require a sound theoretical foundation. It will take a few more years before this realisation is fully reflected in operational systems. In retrospect, we can now see that in 1978 we were still in a climate of euphoria and ignorance not all that dissimilar from the early days of MT. I think we needed the enthusiasm because without it there would not have been any new development. Ignorance will always be with us so long as we do not fully understand the process of translation. CURRENT STATE OF THE ART What then is the current state of hardware and software in general use? Word processing is generally established but the multilingual facilities are far from adequate. Monolingual communication, especially across national boundaries, is still hindered because there are as yet no standards for operating systems, sizes of disc or keyboards which would make the transfer of text in machine-readable form as easy as or, as we should expect, easier than the transmission of paper. Fax facilities have overcome some difficulties of text transmission, but at the expense of having text in machine-readable form. There is some progress with OCR but, again, this will not replace the benefit of direct data transmission. * Ed. note: since this paper was delivered, Language Monthly has joined forces with John Benjamins, the Amsterdam-based publisher of BABEL, to publish Language International. Language Technology is published by Language Technology BV, Amsterdam. The greatest success has come from stand-alone facilities working independently of electronic data transmission. For example the Systran facility in Luxembourg has experienced untold protocol difficulties which have nothing to do with the quality of the MT. The recent experience with Systran on Minitel* in France, on the other hand, is not fully documented, so we cannot say how it works in practice. Some MT systems are now in general commercial use, though the volume is still small in relation to the potential of these machines. There are a number of reasons for this. On the one hand, users have not been able to identify enough suitable material for systems to process because translators have yet to learn how to categorise translations according to their suitability for a particular system. Considerable rethinking of traditional organisational patterns is required in order that basic decisions as to whether a text should be processed by machine or by humans can be made. On the other hand, systems are not constructed with attention to the specific nature of texts. Most system design is still based on a general theoretical description of a language and not on a coherent theory of sublanguages. The topical dictionaries of domain specific lexica boasted of by some systems only deal with some lexical aspects of sublanguages. The current compromise whereby the translator adequates the system to the texts through particular dictionary routines and complete phrasal entries from texts to be translated represents a counsel of despair and besides is only of limited scope. The areas where we can record the greatest user satisfaction are those where users have recognised the full limitation of MT -of any kind -and have turned this into a benefit. This needs a little further explanation: computers can only produce Computer English or Computer French, or, even more specifically, 'Systran French' or 'Logos German', because that is all they know. These system-specific target language forms are based on the designers' simplified conception of the source language. As long as a text is written in 'Systran English' it can be translated quite well into 'Systran French'. This is not a criticism of one particular system: it is simply a statement of the reality of MT in general. Therefore users need to know precisely what source language forms are acceptable to a system, and see to what extent texts can be written to the specifications of the system designers. The various forms of restricted language developed by particular users, for example, Rank Xerox's Customised English or Perkins' PACE English are simply those users' versions of Systran English, or Weidner English and French. * Ed. note: Minitel is the French equivalent of Prestel. That is, of course, only a half-way house because it should not be necessary for translators or their customers to adjust to a particular form of MT output language which has been designed as a general compromise that satisfies no one. System design has now reached a state of sophistication such that it can accommodate the specific requirements of major users and construct systems that deal with a particular house style. A development, unrelated to MT but of considerable consequence for the future success of MT, is the appearance of dictionary production and lookup tools. Translators have enthusiastically accepted the usefulness of specialised dictionaries for topics, types of document or customer. In addition to the growing number of commercial software packages -I deliberately refrain from listing any -translators are developing their own facilities which fit into a convenient text processing environment. Some of these are then offered to others. For example, a former student of mine, Chris Blowers, has developed a tool which is so useful that he now offers it commercially*. On the other hand, it is too early to say what impact the large CD-ROM technical dictionaries, now available from Canada and Sweden, for example, will have. Equally there are, as yet, no commercially-usable tools for dealing with the multilingual problems associated with computer-assisted electronic communication in the form of machine-readable texts that can go directly into machines for processing of one kind or another. Electronic mail, factual databases, electronic books and journals will never be translated by conventional means; all these forms offer new challenges to machine translation. So we can say that after ten years of experience, translators can now give a realistic assessment of present possibilities, because they have a much greater familiarity with the new tools. Suppliers seem to be locked into their particular orientation of research and development and find it difficult to do two things: -to co-operate on compatibility problems -to be imaginative about new translation support tools which exploit the positive side of the restrictions imposed by the computer on language production. FUTURE OUTLOOK The demand for MT will grow with the growth of machine-held texts. MT research is moving into applications and is now more solid because it is more interdisciplinary. There is also more supporting research, for example on automated dictionaries. *Superlex -see list of Exhibitors In parallel, there are research and development efforts which will bypass translation by developing multilingual document and information systems for limited domains and text types. A fair number of the ESPRIT (European (Communities) Strategic Programme for Research in Information Technology) projects have implications for MT. A new generation of computer users is likely to demand facilities which today we consider to lie at the periphery of our work. Translators will use machines much as we use calculators, but I am not sure how quickly we shall advance in paperless communications which will confront the translator with the need to work with machine readable input and output. It is, however, a question of how soon and not of when or whether. CONCLUSIONS At the beginning I said that translation can no longer be considered an isolated activity at the margin of text production. Modern communications are themselves in a state of rapid transformation which affect the role and nature of translation so that MT design cannot rely on a stable situation. Text processing is becoming more sophisticated every day and electronic mail will overtake snail-mail, as the conventional postal services are affectionately termed. In such an environment the delays of full human translation will not be acceptable. Human translation, however inexpensive, will simply be rejected because of its negative influence on the advantages created by electronic transmission. The simple conclusion is that translation has to be designed into the automated document production and transmission processes and can no longer be accepted as a hiatus in an otherwise paperless flow of information and documentation. I also said that MT needed a pragmatic theory in addition to the linguistic theory and that this is what distinguishes the evolutionary among the current systems. Texts that require translation occur in a particular communicative environment which to some extent influences the linguistic forms chosen; we must understand the motivation for these linguistic choices since they differ from language to language. Furthermore, not all information contained in a text is linguistically expressed; we must have means of identifying, understanding and conveying this information in the target language. As a supplementary issue, it is worth mentioning that communication engineers and planners of office systems are already providing us with facilities into which we have to fit machine translation. Standards are also being developed for easier transmission of data across countries; they require our active intervention in order to make them suit our ends. My third projected conclusion was that the particular advantages of MT are far from being realised and exploited. In the first place the diversification of MT system design has barely begun because everybody appears still to hope for the big breakthrough of an all-purpose system. Thus is the greed, gullibility or naivety of venture capital. Yet everyone who is concerned with the reality of translation knows that there are quite distinct user groups with different expectations, abilities and needs. The second unrealised potential of MT is more difficult to explain. All computer produced language is controlled by the fact that it is the result of deliberate human design. It is therefore an artificial or controlled language. Normally, artificial or controlled languages in science and technology are created for the purpose of improving the efficiency of technical communication. It is, therefore, possible to design computer translation output in such a way that it represents an improved and controlled language. The translation process can be used to filter out any extraneous meaning of a text and successful translation will require control over the input in the same spirit of efficiency of communication. A serious limitation of MT is not fully recognised. It should be selfevident that translation can only be practised on forms of communication that are well-established and consolidated. This is not yet the case with much of the new electronic communication. This is not a matter of words but of behaviour. Translation inevitably lags behind the evolution of text forms. However, at a time when text form innovations such as e-mail, factual database and expert systems are claiming to be international, there is an opportunity for translation theory to be in the forefront and to assist in the creation of multilingual textforms, rather than to wait for the evolution of forms which have then to be translated. This observation applies particularly to MT design. We are still short of trained talent to carry out the work of MT design and nobody seems to do anything about it. On the contrary, the few people who are capable of such work are tempted away by better pay and conditions to work abroad. Since English is always in demand as a source or target language, there is not even a language barrier to stop this brain drain. The current situation is a vicious circle. As there are not enough trained computational linguists to carry out advanced research into machine translation, it follows that there are not enough teachers of the necessary skills for future researchers. AUTHOR",
    "abstract": "",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "French",
        "English"
    ],
    "numcitedby": "2",
    "year": "1988",
    "month": "November 10-11",
    "title": "Ten years of machine translation design and application: From {FAHQT} to realism"
}