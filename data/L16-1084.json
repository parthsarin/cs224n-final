{
    "article": "This paper reports on an approach and experiments to automatically build a cross-lingual multi-word entity resource. Starting from a collection of millions of acronym/expansion pairs for 22 languages where expansion variants were grouped into monolingual clusters, we experiment with several aggregation strategies to link these clusters across languages. Aggregation strategies make use of string similarity distances and translation probabilities and they are based on vector space and graph representations. The accuracy of the approach is evaluated against Wikipedia's redirection and cross-lingual linking tables. The resulting multi-word entity resource contains 64,000 multi-word entities with unique identifiers and their 600,000 multilingual lexical variants. We intend to make this new resource publicly available. Introduction Named entities (NEs) such as persons, organisations, locations and events are major bearers of information in text as they provide answers to the text representation questions Who did What to Whom, Where and When? For this reason, work on Named Entity Recognition and Classification is abundant (e.g. Nadeau and Sekine (2007) ) and NEs have been linked to knowledge bases (Rao et al., 2013; McNamee and Dang, 2009) . Major challenges are homographic entity names belonging to different classes or within the same class, as well as the existence of variant spellings within the same or across different languages (Steinberger et al., 2011) . The situation gets even more complex for multi-word entity names because such names are usually composed of words from the common language (e.g. Economic Community of West African States abbreviated as ECOWAS). These common words are normally translated when referred to in different languages (e.g. in Portuguese Comunidade Econ\u00f3mica dos Estados da \u00c1frica Ocidental, abbreviated as CEDEAO) and authors frequently abbreviate or change the multi-word forms, either out of negligence or for space reasons (e.g. in English Economic Community of West Africa). While one could argue that such abbreviated or newly created names are wrong, they appear daily in real documents and an information-seeking individual or system would be interested in retrieving documents in which the intention was to refer to the entity of interest. In a multilingual large-scale media monitoring environment such as EMM (Steinberger et al., 2009; Steinberger et al., 2015) 1 , we observe an abundant number of spelling variants for entities, including spelling mistakes (e.g. 'United Nattions' when referring to United Nations), inflections (e.g. Birle\u015fmi\u015f Milletler'in where the inflection suffix 'in is added to the Turkish equivalence of 'United Nations') and variants in other scripts (e.g. Russian Cyrillic \u041e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0451\u043d\u043d\u044b\u0445 \u041d\u0430\u0446\u0438\u0439). One daily media monitoring task consists in recognising new names and in determining automatically whether they are a new name or whether they might be a spelling variant of a name encountered before. We aim at addressing this task by creating a daily updated resource containing multi-word entities, their acronyms and their variants. Ehrmann et al. (2013) developed a method handling variants at monolingual level, meaning that there were separate clusters and identifiers for each language. In this paper we address this task at the multilingual level. Additionally to the complexity of the monolingual task, we have to address expression translations, increasing acronym ambiguity, and larger numbers of expressions referring to the same entity. The ECOWAS/CEDEAO example mentioned previously shows how the same conceptual entity can both have different variants and different acronyms in different languages. Figure 1 shows that we can neither assume that entities across languages have the same acronym, nor can we assume that the same acronym (within the same or across languages) refers to only one entity. After the discussion of related work (Section 2.), we introduce the multilingual resource that forms the starting point of our experiments (Section 3.). Next, we detail the cross-lingual cluster aggregation approach, specifying cluster representations and aggregation strategies (Section 4.). We then present our experiments, discuss the results (Section 5.) and conclude with pointers to future work (Section 6.). Related work Work in the domain of abbreviation processing is abundant, but it mostly focuses on the biomedical domain and on the English language. Since the pioneer work of Taghva and Gilbreth (1999) , research has developed into three main directions, namely: acronym extraction and mapping to their expansions; acronym variant clustering; and, more recently, acronym disambiguation. While the extraction of acronym/expansion pairs corresponds to the primary stage of lexical unit acquisition, variant clustering resembles sense inventory organisation, which can eventually serve as reference for disambiguation. We report here on the first two aspects. With regard to acronym extraction, existing work almost exclusively focuses on English biomedical literature (Schwartz and Hearst, 2003; Okazaki and Ananiadou, 2006; James et al., 2001; Wren and Garner, 2002; Adar, 2004; Chang et al., 2002; David and Turney, 2005) . Results are good and the extraction-recognition step can be considered a mature technology for this combination of domain and language. However, there is very little work on other languages: Kokkinakis and Dann\u00e9lls (2006) investigate the specificity of Swedish, Sikl\u00f3si et al. (2014) carry out Hungarian abbreviation processing, both on medical texts. Kompara (2010) and Hahn et al. (2005) seem to be the only ones to work with acronyms across languages, with preliminary work on Slovene, English and Italian for the former, and acronym alignment across English, German, Portuguese and Spanish based on an inter-lingua for the latter. As mentioned previously, the variety and the number of acronyms is very large so that it is useful to organise the acronym dataset on a semantic basis by grouping related variants under the same acronym identifier. The aim is thus -for each set of expansions having the same acronym -to identify those which are conceptually related. Previous related work focused mainly, anew, on biomedical literature in English. Adar (2004) experimented with k-means clustering based on an ngram similarity measure and on a MeSH term similarity measure. Results showed that the n-gram based clustering performs actually better than that based on the MeSH resource. Okazaki et al. (2010) designed a more complex clustering approach, using a similarity metric based on a mixture of several features. Once the best feature setting has been acquired (by supervised machine learning), hierarchical clustering is used to induce the final variant grouping. The features used to build the similarity metric are themselves similarity measures, such as character and word n-gram similarity. The outcome of these experiments on English abbreviations showed that character and word n-gram features contribute the most to the final result. Work on monolingual clustering of acronym variants outside the biomedical domain and for altogether 22 different languages was carried out in (Ehrmann et al., 2013 ). Ehrmann's approach is based on hierarchical groupaverage clustering, where cluster homogeneity is set using an empirically determined threshold. The clustering depends on a pair-wise string similarity between expansions, using a normalised Levenshtein edit distance. To the best of our knowledge, no work has been carried out for acronym clustering across languages. What comes closest to this or, more exactly, to its result, are multilingual lexical resources such as BabelNet (Navigli and Ponzetto, 2012) or YAGO (Hoffart et al., 2013) . Automatically built based on the mapping between WordNet and Wikipedia (and other resources), these resources provide (among others) multilingual variants of expansions for specific acronyms. They are inherited from the many cross-lingual and cross-script links provided in Wikipedia. In contrast, the work presented here starts from raw data extracted from real-life texts. Starting point The starting point of our work is a large set of multiword entities and their corresponding acronyms in 22 Roman-script languages (Ehrmann et al., 2013) . These acronym/expansion pairs were extracted from the news stream analysed by the EMM processing chain by applying patterns similar to those proposed by Schwartz and Hearst (2003) . In a nutshell, the algorithm collects acronym/expansion pairs (such as expansion (acronym) and acronym (expansion)) by identifying short strings within parenthesis, along with candidate expansions in a side-window of a limited length. A filtering step is then applied, with the following main constraints: the first letter of the acronym must be upper-cased, and the length of the expansion must be smaller than (a) twice as many words as there are characters in the acronym, or (b) the number of characters in the acronym plus five words, whichever is the smaller (i.e. min(|A|+5, |A|* 2) words, with |A| being the number of characters of the acronym). We refer the reader to Schwartz and Hearst (2003) for more details. This process resulted in the extraction of 1.7 million expansions for 0.4 million different acronyms. Applied on news articles, this method identified acronym/expansion pairs referring mostly to organisation names (e.g. CP-Communist Party), but also events (WW2-World War II ), names of drugs or of vaccines (MMR-measles, mumps, rubella), organisation types (NGO-Non-governmental organisation), job titles (MEP-Member of Parliament), physical measurement units (kmh-kilometres per hour), and more. As one of the next steps, we will work on cate- To automatically determine which of the expansions are lexical variants of the same conceptual entity, a clustering step was carried out, on the basis of expansions having the same language and the same acronym. This monolingual clustering, based on a pair-wise string similarity, allowed to distinguish between sets of conceptually related expansions, such as those referring to the International Space Station and those referring to the Institute for Security of Studies, both clusters having the acronym ISS (cf. English part of Figure 1 ). Evaluated over the 10 mostcovered languages, this monolingual clustering has a micro-average precision of 95.2% (Jacquet et al., 2014) . Out of this monolingual clustering step, we selected only clusters having at least four expansions, resulting in 81,000 monolingual clusters with an average of 7.5 expansions per cluster, the biggest one having 232 expansions. Based on this data, the objective of the current work is to go a step further by identifying cross-lingual multi-word entity lexical variants. More specifically, the objective is to link multilingual expansions referring to the same entity across languages and regardless of their acronyms. To this end, we leverage the previously computed monolingual clusters and attempt to link them across languages. Considering the previous example with the entity International Space Station (cf. Figure 1 ), this results in aggregating the monolingual clusters SSI-Station spatiale internationale (French), ISS-International Space Station (English) and EEI-Estaci\u00f3n Espacial (Spanish). Additionally to linking expansions across languages and independently from their acronym, cross-lingual cluster aggregation can also revise monolingual clusters by aggregating those conceptually related but isolated because of their acronyms (both pairs IMF-International Monetary Fund and FMI-Fondo Monetario Internazionale occur in Italian texts). Approach Cluster aggregation can be cast as the problem of identifying connected components of a graph, where monolingual clusters represent vertices and where edges need to be computed. This section describes different crosslingual aggregation strategies that we tested to link sets of monolingual clusters across languages. Cluster aggregation based on common expansions The most straightforward solution to link related acronyms in different languages (hereafter ExpAgg) is to merge those clusters that have more than n expansion forms in common, independently of whether their acronyms are identical or not (in our experiments, n was set to 1). This aggregation has been applied both to improve monolingual clusters (cf. IMF vs FMI case mentioned at the end of section 3.) and to aggregate clusters across languages. Each token has its own importance to describe a cluster. In order to compare two clusters on the basis of their most relevant tokens, we consider the tf-idf value of each token t j where, in our context, each cluster c i is seen as a document and the whole set of clusters C as a corpus: Cluster aggregation based on tokens C(c i , t j ) = tf (t j , c i ) \u00d7 idf (t j , C) (1) Cluster aggregation based on similar tokens This aggregation (hereafter TokAgg) addresses cases where monolingual clusters do not have identical expansions across languages, but they have a significant amount of highly similar tokens. We compute the matrix (|T| \u00d7 |T|), hereafter InvEdit, which corresponds to the inverse of the normalized Levenshtein edit distance where t i : i = 1, ..., |T| and t j : j = 1, ..., |T| are tokens from all the addressed languages: InvEdit(t i , t j ) = 1 \u2212 Lev(t i , t j ) max(|t i |, |t j |) (2) Lev(t i , t j ) is the Levenshtein edit-distance between t i and t j , and |t i | and |t j | are respectively the length of the tokens t i and t j . We filter InvEdit using a threshold \u03b4 as follows: InvEdit(t i , t j , \u03b4) = { InvEdit(t i , t j ) : InvEdit(t i , t j ) \u2265 \u03b4 0 : InvEdit(t i , t j ) < \u03b4 In this case, if \u03b4 = 1, InvEdit only contains values for exact matching tokens. This matrix is then used to enrich the monolingual cluster representation. Given two languages l 1 and l 2 , the corresponding monolingual clusters C l1 and C l2 do not have common tokens since in T tokens are language-dependent. The InvEdit matrix is used to identify common or similar tokens. We convert the obtained matrix C_T ok l1 to a binary matrix: C_T ok l1 (c i , t j ) = { 1 : C l1 (c i , t j ) \u00d7 InvEdit(c i , t j , \u03b4) > 0 0 : otherwise This aggregation is particularly useful when comparing clusters from similar languages. Table 1 illustrates such cases, with the English-Italian tokens Party/Partito and Democratic/Democratico. This representation can also benefit from the fact that it is possible to find multi-word entities of a given language in texts in another language (especially with names of international organisations such as European Space Agency which can be found in German text). Cluster aggregation based on translated tokens However, many entities have different written forms across languages so that a string-based comparison of tokens is not successful. We therefore complement the cluster aggregation by using token translation probabilities (hereafter TransTokAgg). They are produced using statistical translation models trained on parallel corpora built from Wikipedia, by making use of redirection tables (i.e. several written forms redirecting to a specific page/entity) and of interlingual links between pages. (implementation details of translation models are provided in section 5.2.1.). In order to separate training and test data, any variant name from these Wikipedia tables matching with one of the 1.7 million expansions or 0.4 million acronyms is removed from the parallel corpora (See section 5.). Let T ransM od be the resulting (|T| \u00d7 |T|) translation model matrix where t i : i = 1, ..., |T| and t j : j = 1, ..., |T| are tokens. As for InvEdit matrix, we filter T ransM od using a threshold \u03b2: T ransM od(t i , t j , \u03b2) = { T ransM od(t i , t j ) : T ransM od(t i , t j ) \u2265 \u03b2 0 : T ransM od(t i , t j ) < \u03b2 This matrix is then used to enrich the monolingual cluster representation. Given a language l and its corresponding monolingual clusters C l , C_T ransT ok l corresponds to the binary extended matrix based on a given translation model: C_T ransT ok l (c i , t j ) = { 1 : C l (c i , t j ) \u00d7 T ransM od(c i , t j , \u03b2) > 0 0 : otherwise Table 2 illustrates a case of such cluster aggregation, thanks to a high score in the T ransM od matrix between tokens Science in English and Wissenschaften in German. Aggregation strategies We formulate cluster linking as the task of identifying connected components in a graph, where monolingual clusters are vertices and where edges represent links of related clusters across languages. Clusters are linked if their similarity is above a certain threshold \u03b1. During preliminary experiments, we had also tested 'pure' clustering algorithms, but it turned out that the graph approach was more efficient. For the last two cluster aggregation methods (Tok-Agg and TransTokAgg), we applied two similarity measures: cosine and ComMNZ. The latter is actually a data fusion algorithm (Fox and Shaw, 1994) which we assimilate, in this context, to a similarity measure. This algorithm aims at measuring the similarity between two objects having multiple comparison criteria. Specifically, the overall similarity score between two objects is better when those objects have reasonable similarity scores for all criteria than when they have a very good similarity score for one criterion, and less good or no value for the others. In our case, it would promote the similarity between two clusters c i and c j if they have many similar or translated tokens t k with a reasonable similarity score, and it would decrease the similarity between two clusters c i and c j if they have few similar or translated tokens t k with a very high similarity score: CombM N Z(c i , c j ) = \u2211 t k \u2208cj C(c i , t k ) \u2211 t l \u2208ci C(c i , t l ) \u00d7 \u2211 t k \u2208cj 1 {C(ci,t k )\u0338 =0} (3) Evaluation Evaluation dataset As described in Section 3., the starting point of our experiments is a set of 81,000 monolingual clusters with one acronym per cluster, an average of 7.5 expansions per cluster, many of them having few expansions, and the biggest 232 expansions. We evaluate cross-lingual cluster aggregation against Wikipedia data excluding the part used for the translations models (cf. previous section). The gold standard corresponds to a set of Wikipedia redirection tables and interlingual linking tables, where we consider Wikipedia entities/pages as cross-lingual classes. Each class contains all the expressions listed in the redirection tables in all the languages linked via the interlingual linking tables. Only classes having at least two expansions were selected, resulting in a gold standard of 10,000 classes. Considering Wikipedia information as a gold standard is disputable. The interlingual linkings should be reliable but this is less the case for the redirection tables. However, a manual evaluation of the redirection table quality shows that, in over 160 randomly extracted classes in 4 different languages (fr, en, de, it), 93.4% of the forms were correct (Jacquet et al., 2014) . Parameters Parameters have to be set with regards to, first, the thresholds \u03b4 and \u03b2 applied to filter out some similarity values in the token matrices (C_T ok l and C_T ransT ok l ) and, second, the threshold \u03b1 applied to the aggregation strategies, i.e. the one above which clusters are aggregated. With respect to cluster representations based on similar tokens C_T ok l , the threshold \u03b4 should be high in order to consider two tokens as similar only if they are close in terms of edit distance. Regarding representations based on translated tokens C_T ransT ok l , the threshold \u03b2 can be low since even a weak token similarity could be a relevant indicator at the cluster level. For our experiments, the values of \u03b4 and \u03b2 were fixed to 0.7 and 0.3 respectively. Cluster aggregation is allowed when the cluster similarity (cosine or CombMNZ) is above a certain threshold \u03b1. We experimented with different values for \u03b1, ranging from 0.7 to 1 (cf. Section 5.4.). This aggregation step is further regulated with the addition of the following constraints: two clusters c 1 and c 2 are linked if their similarity is above \u03b1 and if c 1 is in the k most similar clusters of c 2 or c 2 is in the k most similar clusters of c 1 . This additional constraints allow to rule out clusters having a high similarity with a lot of other clusters. This is the case for short and frequent expansions, e.g. Olympic Committee which is highly similar to a cluster containing expansions such as Olympic Organizing Committee or to another containing games organising committee, but as well to clusters containing more specific expansions such as Vancouver Olympic Committee. In our experiments, k equals 3. Translation models Cluster representations based on translated tokens correspond to lexical conditional translation probabilities computed for three language pairs, between English and French, German and Italian. The translation models were trained on parallel corpora built from Wikipedia, by making use of redirection tables (i.e. several written forms redirecting to a specific page/entity) and of interlingual links between pages. More specifically, given an entity/page p and two redirection tables rt 1 and rt 2 in languages l 1 and l 2 , each written form from rt 1 can be seen as a translation t of each written form from rt 2 . For a given language pair, the corresponding parallel corpus is the concatenation of all translations t from all the entities/pages p. These Wikipedia tables are also used for evaluation purposes (cf Section 5.1.). As a consequence, the 1.7 million expansions and 0.4 million acronyms on which the approach is applied were removed from the parallel corpora. There were about 300,000 training examples for German-English and French-English, and about 170,000 for Italian-English. Word alignments with manyto-one links were generated using the unsupervised fast_align tool (Dyer et al., 2013) in both directions and combined with the grow-diag-final-and symmetrization heuristic (Koehn et al., 2003) . Lexical translation tables for the three language pairs in both directions where extracted with a tool from the Moses translation toolkit (Koehn et al., 2007) . Tables contain maximum likelihood probability estimated for the conditional word translation probabilities p(En|{F r, De, It}) and p({F r, De, It}|En). Our T ransM od matrix is constructed based on the concatenation of these tables. Evaluation measures Clusters are evaluated against the gold standard using micro-average Precision and Recall, adopting the mapping between identified clusters and gold standard clusters which maximised the F 1 measure. Microaverage precision (MAV-P) and recall (MAV-R) are defined as follows: m \u2212 av \u2212 prec(C) = \u2211 c\u2208C EXP (c) true \u2211 c\u2208C EXP (c) true + \u2211 c\u2208C EXP (c) f alse (4) m \u2212 av \u2212 rec(C) = \u2211 c\u2208C EXP (c) true \u2211 c\u2208C EXP (c) true + \u2211 c\u2208C EXP (c) miss ( 5 ) where C is the set of produced clusters, EXP (c) true is the set of expansions in a cluster c which also appear in the corresponding class of the gold standard, and EXP (c) f alse is the set of expansions in a cluster c which do not appear in the gold standard 2 . Results and discussion Table 3 reports the results obtained for the three language pairs for which we have a translation model, and Table 4 reports on a global evaluation for 22 languages. In both cases, values were computed with the aggregation similarity threshold \u03b1 set to 0.9. We defined the baseline as the concatenation of all monolingual clusters from all languages under consideration. It has a high precision (97.7% and 98.2% in Table 3 and 4 resp.) and a poor recall (51.5% and 40.5%) since none of the clusters is cross-lingual. The challenge is thus to improve the recall without affecting too much the precision. In Tables 3 and 4 , monolingual ExpAgg corresponds to the expansion aggregation strategy applied at the monolingual level, and multilingual ExpAgg at the multilingual level. The T okAgg and T ransT okAgg  lines correspond to results with the corresponding token aggregation strategies using cosine similarity and CombMNZ fusion, and All aggregations to the ones obtained when using the four aggregation strategies in a joint way. It can be observed that each aggregation strategy contributes to improving the quality of cross-lingual cluster aggregation, with multilingualExpAgg providing the best improvement (+10.8 points for the 3 language pairs and +12.6 points for the 22 languages). The contribution of the T ransT okAgg aggregation is slightly disappointing; it improves the baseline in both language configurations, but not significantly. Nevertheless, when all the aggregations are applied (bold lines), results are better than the addition of each single aggregation. It could mean that the T ransT okAgg aggregation provides links between clusters which are not useful in isolation, but adds relevant bridges between sets of clusters when combined with other aggregations. Besides, one should notice that between the three language pairs and the 22 languages, improvements per aggregation strategy are comparable. Similarly, results obtained based on cosine similarity and CombMNZ fusion are comparable. This strengthens the reliability of the obtained results. Figure 2 shows the impact of the threshold \u03b1. When too low (0.7), the F1 measure can be below the baseline because too many links are established between clusters; when too high (1.0), aggregations based on similar and translated tokens are reduced to values close to zero. In between, it has a clear improvement impact. Overall, all aggregations strongly improve the baseline by increasing the recall (+19.7 and +23.4 points resp.) with a small loss in precision (-1.9 and -2.4 points resp.). Eventually, there are 64,000 crosslingual connected clusters across languages instead of 81,000 monolingual ones for the 22 languages. Translation model discussion The training data for the lexical translation probabilities was quite noisy. An addition of other parallel text data might help to make more general translation tables, but it might remove some of the specificity learned from the Wikipedia data. We tried the same experiments, using the Europarl dataset, Conclusion We described an approach to create a highly multilingual named entity resource consisting of acronyms and the various monolingual and multilingual spelling variants of their corresponding expansions. Thanks to different aggregation strategies, an initial set of monolingual clusters has been linked across 22 languages with a high precision (95.8%) and a reasonable recall (65.5%). The result is a resource of 64,000 unique entities with an average of 9.4 expansions (spelling variants) per cluster. Future work includes classifying the entity into types, extending the translation models to other language pairs, improving the translated token aggregation strategy, and publishing the resource as linked open data.",
    "abstract": "This paper reports on an approach and experiments to automatically build a cross-lingual multi-word entity resource. Starting from a collection of millions of acronym/expansion pairs for 22 languages where expansion variants were grouped into monolingual clusters, we experiment with several aggregation strategies to link these clusters across languages. Aggregation strategies make use of string similarity distances and translation probabilities and they are based on vector space and graph representations. The accuracy of the approach is evaluated against Wikipedia's redirection and cross-lingual linking tables. The resulting multi-word entity resource contains 64,000 multi-word entities with unique identifiers and their 600,000 multilingual lexical variants. We intend to make this new resource publicly available.",
    "countries": [
        "Switzerland",
        "Italy"
    ],
    "languages": [
        "German",
        "English",
        "Portuguese",
        "Italian",
        "Spanish",
        "French",
        "Slovene"
    ],
    "numcitedby": "5",
    "year": "2016",
    "month": "May",
    "title": "Cross-lingual Linking of Multi-word Entities and their corresponding Acronyms"
}