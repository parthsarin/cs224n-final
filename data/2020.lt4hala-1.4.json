{
    "article": "Classical Armenian, Old Georgian and Syriac are under-resourced digital languages. Even though a lot of printed critical editions or dictionaries are available, there is currently a lack of fully tagged corpora that could be reused for automatic text analysis. In this paper, we introduce an ongoing project of lemmatization and POS-tagging for these languages, relying on a recurrent neural network (RNN), specific morphological tags and dedicated datasets. For this paper, we have combine different corpora previously processed by automatic out-of-context lemmatization and POS-tagging, and manual proofreading by the collaborators of the GREgORI Project (UCLouvain, Louvain-la-Neuve, Belgium). We intend to compare a rule based approach and a RNN approach by using PIE specialized by Calfa (Paris, France). We introduce here first results. We reach a mean accuracy of 91,63% in lemmatization and of 92,56% in POS-tagging. The datasets, which were constituted and used for this project, are not yet representative of the different variations of these languages through centuries, but they are homogenous and allow reaching tangible results, paving the way for further analysis of wider corpora. Introduction Classical Armenian, Old Georgian and Syriac are still poorly digitally resourced. Some major corpora already exist, for instance the Digital Syriac Corpus (DSC) for Syriac; Digilib, Arak29, Calfa and Titus for Classical Armenian; and Titus and the Georgian Language Corpus for Georgian 1 . These corpora, when they are really specialized on the ancient state of these languages, are mainly composed of plain texts or texts analyzed out of context (all possible analyses are given for each token and polylexical 2 word-forms are not fully described). Accordingly, scholars are still waiting for corpora enhanced with complete and reliable linguistic tags. Concerning the modern state of these languages, the Universal Dependencies (UD) provide annotated corpora for Armenian and Georgian, with the same limitations as described above. Furthermore, the modern and the ancient states of each language are usually quite different, so that digital resources built for either are inadequate to process the other. Usual techniques for the lemmatization of these corpora rely on sets of rules and dictionaries. Such a method is unable to handle unknown tokens, or to readily process data in context. We have initiated experimentations to complete these operations using a neural network (RNN) and purpose-built corpora dedicated to this very task (Dereza, 2018) . The task is particularly complex for these aforenamed languages due to their wealth of polylexical forms. In this paper, we present experimental results achieved through the application of state-of-the-art technologies to these languages. This research depends on the data and tools developed by both the GREgORI (henceforth GP) 3 and Calfa 4 projects. The texts all derive from the database of the GP, which consists of texts written in the main languages of the Christian East and already published in the form of critical editions. The scope of this paper is limited to the three already quoted languages. The datasets described below have all previously undergone automatic out-of-context lemmatization, and manual proofreading (see infra 3. Data Structure). Datasets D-HYE: Classical Armenian is an Indo-European language. This dataset contains 66.812 tokens (16.417 of which are unique) originating from three different corpora: Gregory of Nazianzus (Coulie, 1994; Coulie and Sirinian, 1999; Sanspeur, 2007; Sirinian, 1999) (GRNA), the Geography of the Indian World (Boisson, 2014) (GMI), and the Acta Pauli et Theclae (Calzolari, 2017 ) (THECLA). GRNA gathers the text of the Armenian versions of Gregory of Nazianzus' Discourses, already published in the Corpus Christianorum series. Gregory of Nazianzus ( \u2020390 AD) is the author of 45 Discourses, more than 240 letters, as well as theological and historical works in verse. The Armenian version is anonymous and dates from 500-550 AD; its style has been qualified as pre-Hellenophile (Lafontaine and Coulie, 1983) . THECLA contains the Armenian version of a group of texts relating to the legend of Thecla and the martyrdom of Paul (5 th -14 th c. AD), while GMI is a very small text written around 1120 AD, enumerating cities and trading posts of the Indian world. GMI contains a lot of unique tokens, such as toponyms and personal names. D-HYE primarily covers texts of the Hellenophile tradition, which entails a large number of neologisms and idiosyncratic syntactic constructions. As such, for the time being, it is not entirely representative of the Classical Armenian language (see infra 5. Perspectives). D-KAT: Old Georgian is a Kartvelian language. It contains 150.869 tokens (30.313 unique) from one unique corpus, made up of the texts of the Georgian versions of Gregory of Nazianzus' Discourses already published in the Corpus Christianorum series (Coulie and M\u00e9tr\u00e9v\u00e9li, 2001; Coulie and M\u00e9tr\u00e9v\u00e9li, 2004; Coulie and M\u00e9tr\u00e9v\u00e9li, 2007; Coulie and M\u00e9tr\u00e9v\u00e9li, 2011; M\u00e9tr\u00e9v\u00e9li, 1998; M\u00e9tr\u00e9v\u00e9li, 2000) . Several translations from Greek into Georgian are known. The most important of which are those by Euthymius the Hagiorite (10 th c. AD) and Ephrem Mtsire (Black Mountain, near Antioch, 11 th c. AD) (Haelewyck, 2017b) . D-SYC: Syriac is a Semitic language. This dataset contains 46.859 tokens (10.612 unique). It is the most heterogenous dataset of this study, since the texts it contains relate to a variety of topics: biblical, hagiographic, and historical texts, homilies, hymns, moral sayings, translations of Greek philosophical works, etc. These texts have been lemmatized by the collaborators of the GP: the Syriac version of Discourses I and XIII by Gregory of Nazianzus, translated from Greek in the 6 th -7 th c. AD (Haelewyck, 2011; Haelewyck, 2017b; Schmidt, 2002; Sembiante, 2017) ; the Story of Zosimus, translated no later than the 4 th c. AD (Haelewyck, 2014; Haelewyck, 2015; Haelewyck, 2016; Haelewyck, 2017a) ; the Syriac Sayings of Greek Philosophers (6 th -9 th c. AD) (Arzhanov, 2018) ; the Life of John the Merciful (Venturini, 2019) ; and some other texts dating from the 4 th to the 9 th century, described on the GP's website. These datasets do not embrace the whole lexicon of these languages (as a reference, the Calfa dictionary contains around 65.000 entries for Classical Armenian). We discuss this shortcoming in parts 3. and 4. Data Structure The data have been prepared and analysed in the framework of the GP. For each corpus, the following processing steps were implemented: 1. Cleaning up the forms of the text (removal of uppercase, critical signs used by editors, etc.). These forms constitute the column \"cleaned form\" of the corpus (see figure 1 ); 2. Morpho-lexical tagging, i.e. identifying a lemma and a POS for every cleaned-up form (token) of the text. This task is conducted through automatic comparison of the clean forms of the texts to the linguistic resources of the GP: dictionaries of simple forms and rules for the analysis of polylexical forms (see infra); 3. Proofreading of the results, corrections and encoding of missing analyses; 4. Enrichment of the linguistic resources for future processing of other texts. Syriac, Classical Armenian and Old Georgian contain a large quantity of polylexical forms, combining words with different prefixes (preposition or binding particle) and/or suffixes (postposition or determiner). These forms are systematically (and automatically) segmented in order to identify explicitly each of its components. The different lexical elements are separated by an @ sign and divided into the following columns: lemma, POS and morph (see table 4 ; displaying a short sentence from the Inscription of the Regent Constantine of Pape\u1e59\u014dn (Ouzounian et al., 2012) ). The morpho-lexical tagging follows the rules laid out for each language by the collaborators of the GP (Coulie, 1996; Coulie et al., 2013; Coulie et al., 2020; Kindt, 2004; Haelewyck et al., 2018; Van Elverdinghe, 2018) . This automated analysis does not take the context into account. The resulting data are proofread manually and the proofreaders add the morphology according to the context (see table 4 , columns marked GP). Figure 1 : Raw output from the GP system Method and Experiments Up until now, the annotation has depended on a set of rules and dictionaries, and the result has been manually corrected. The main flaw of this approach lies in the fact that this analysis only concerns the forms attested in the corpus and already included in the lexical resources (< 40% for a representative corpus of Classical Armenian like the NBHL (Vidal-Gor\u00e8ne et al., 2019)) on the one hand, and that it does not provide answers in case of lexical ambiguity on the other hand. We have, hence, initiated experimentations to complete the task of lemmatization and POS-tagging with a neural network. At present, the choice has fallen on PIE (Manjavacas et al., 2019) , which offers a highly modular architecture (using (Egen et al., 2016) . Even though PIE allows simultaneous annotation of lemmata and POS, we have decided here to conduct the tasks independently. We use the default hyper parameters proposed by Manjavacas and applied on twenty different corpora from UD, without tailoring them in any way to the dataset under consideration 5 . For the lemmatization task, we have followed the default structure provided by PIE. We are working at the char level, and we include the sentence context. We use an attention encoder-decoder. For the POS-tagging task, we have compared the Conditional Random Field (CRF) provided by LEMMING (M\u00fcller et al., 2015) and the linear decoder implemented in PIE. We have divided D-HYE, D-KAT and D-SYC into three sets: Train (80% of data), Validation (10%) and Test (10%). The distribution was implemented automatically on a sentence basis. Results on lemmatization The results achieved are consistent with the representativeness and the size of the corpora studied, and the results provided by Manjavacas on similar datasets (see infra 5. Perspectives). D-HYE is the most homogenous dataset, despite the numerous unique toponyms. Thus, there is little variation regarding vocabulary and expressions, which is why we achieve a very good accuracy during training, almost as good as with D-KAT, but for a corpus twice as small. By contrast, D-SYC is more representative of all the language state of Syriac. The results on ambiguous and unknown tokens are quite low, however they make it possible to already process automatically a larger number of cases. The train set for Armenian contains 17% of unknown tokens, due to the high proportion of proper nouns from GMI, whereas the proportion of unknown tokens is 14% in Georgian and 20% in Syriac, the latter being penalized twice, by its size and this proportion of unknown tokens. The confusion matrix reveals that mistakes are concentrated on homographic lemmata (e.g. mayr (mother) and mayr (cedrus)). Besides, these languages exhibit numerous polylexical forms: these are similar in form but they differ in their analysis. We had identified the homographs beforehand, in order to disambiguate them (e.g. \u056b\u0582\u0580 (\u056b\u0582\u0580\u0578\u0581) and \u056b\u0582\u0580 (\u056b\u0582\u0580\u0565\u0561\u0576\u0581)), but the lack of data results in a more complex task for the network. Besides, 50% of mistakes are localized on polylexical forms, such as demonstrative pronouns or prepositions. This is made clear in table 4 , where no pronoun has been predicted. The same applies for the task of POS-tagging. Results on POS-tagging (crf / linear) The Linear Decoder achieves better results for the task of POS-tagging, except for the task of tagging ambiguous and unknown tokens during training. Nevertheless, the linear decoder remains better than the CRF decoder (LEM-MING) on the test datasets, except for unknow tokens in Old Georgian and Syriac. The issue of the ambiguous tokens is the same as for the task of lemmatization. The confusion matrix for D-HYE shows that mistakes are essentially concentrated on common nouns (21%, generally predicted as verbs) and verbs (12%, generally predicted as common nouns). Vocalic alternation in Classical Armenian appears to create ambiguities between declined and conjugated tokens. As regards D-KAT, mistakes are essentially concentrated on common nouns (30%) and V+Mas (12%) 6 , which are generally confused with each other. In D-SYC, mistakes are more diversified: adjectives (11%), tokens composed by a particle followed by a name 4 ). Perspectives The problems affecting our results are due to two challenges posed by the structure and the source of our data. Firstly, the amount of data remains too small to ensure representativeness of the described languages. Secondly, the large number of polylexical tokens makes processing more challenging. We intend to integrate the OCR developed by Calfa for Syriac, Old Georgian and Classical Armenian with our process, in order to increase drastically our datasets. These data will be manually proofread and pre-tagged by the previous models for training. As regards Classical Armenian, we intend to combine the data of the NBHL on Calfa -composed in particular of more than 1.3 million tokens (190.000 of which are unique) and representative of the Armenian literary production (compilation of several hundreds of classical and medieval sources) -and lemmatized forms from the Gospels. The NBHL has already been lemmatized and the proofreading is being finalized (Vidal-Gor\u00e8ne et al., 2019; Vidal-Gor\u00e8ne and Decours-Perez, 2020) . Calfa also offers a database of more than 65.000 headwords for Classical Armenian and has generated a very large number of verbal and noun forms that will be integrated into the training. Furthermore, the GP is now producing a digital corpus of all the Armenian, Georgian and Syriac texts published in the Corpus Scriptorum Christianorum Orientalium series. The results presented here are a first step in the development of a lemmatizer and a POS-tagger for these languages. In particular, we only provide the results of one single neural network, but we intend to conduct a comparison with state-of-the-art technologies and rule-based approches, and to include contextual tagging at the morphological level. We already reach a mean accuracy of 91,63% in lemmatization (84,28% for ambiguous tokens and 71,93% for unknown tokens), and of 92,56% in POS-tagging (88,71% for ambiguous tokens and 75,17% for unknown tokens). Nevertheless, these results are not robust on a wide variety of texts: resolving issue constitutes the chief objective of our upcoming experiments.",
    "abstract": "Classical Armenian, Old Georgian and Syriac are under-resourced digital languages. Even though a lot of printed critical editions or dictionaries are available, there is currently a lack of fully tagged corpora that could be reused for automatic text analysis. In this paper, we introduce an ongoing project of lemmatization and POS-tagging for these languages, relying on a recurrent neural network (RNN), specific morphological tags and dedicated datasets. For this paper, we have combine different corpora previously processed by automatic out-of-context lemmatization and POS-tagging, and manual proofreading by the collaborators of the GREgORI Project (UCLouvain, Louvain-la-Neuve, Belgium). We intend to compare a rule based approach and a RNN approach by using PIE specialized by Calfa (Paris, France). We introduce here first results. We reach a mean accuracy of 91,63% in lemmatization and of 92,56% in POS-tagging. The datasets, which were constituted and used for this project, are not yet representative of the different variations of these languages through centuries, but they are homogenous and allow reaching tangible results, paving the way for further analysis of wider corpora.",
    "countries": [
        "Belgium",
        "Europe"
    ],
    "languages": [
        "Georgian",
        "Armenian"
    ],
    "numcitedby": "1",
    "year": "2020",
    "month": "May",
    "title": "Lemmatization and {POS}-tagging process by using joint learning approach. Experimental results on {C}lassical {A}rmenian, {O}ld {G}eorgian, and {S}yriac"
}