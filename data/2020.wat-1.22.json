{
    "article": "Statistical machine translation (SMT) was the state-of-the-art in machine translation (MT) research for more than two decades, but has since been superseded by neural MT (NMT). Despite producing state-of-the-art results in many translation tasks, neural models underperform in resource-poor scenarios. Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of translation of many low-resource languages. In this work, we investigate the performance of phrasebased SMT (PB-SMT) and NMT on two rarelytested low-resource language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019) , unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018) , exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017) , multi-task learning (Niehues and Cho, 2017) , selection of hyperparameters (Sennrich and Zhang, 2019) , and pre-trained language model fine-tuning (Liu et al., 2020) . Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017) ; unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (S\u00f8gaard et al., 2018) and the same is applicable in the case of transfer learning too (Montoya et al., 2019) . To this end, we investigate the performance of PB-SMT and NMT systems on two rarely-tested under-resourced language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into account. In this context, in Ramesh et al. (2020) , we investigated the performance of PB-SMT, NMT and a commercial MT system (Google Translate (GT)) 1 on English-to-Tamil taking the software localisation data into account, i.e. the same data as the one used in this work. In particular, in Ramesh et al. (2020) , we produced rankings of the MT systems (PB-SMT, NMT and GT) via a social media platform-based human evaluation scheme, and demonstrate our findings in this lowresource domain-specific text translation task. The next section talks about some of the papers that compared PB-SMT and NMT on a variety of use-cases. The remainder of the paper is organized as follows. In Section 2, we discuss related work. Section 3 explains the experimental setup including the descriptions of our MT systems and details of the data sets used. Section 4 presents the results with discussions and analysis, while Section 5 concludes our work with avenues for future work. of this work is to study translations of the MT systems (PB-SMT and NMT) in under-resourced conditions, we provide a brief overview on some of the papers that compared PB-SMT and NMT on highresource settings too. Junczys-Dowmunt et al. (2016) compare PB-SMT and NMT on a range of translation-pairs and show that for all translation directions NMT is either on par with or surpasses PB-SMT. Bentivogli et al. (2016) analyse the output of MT systems in an Englishto-German translation task by considering different linguistic categories. Toral and S\u00e1nchez-Cartagena (2017) conduct an evaluation to compare NMT and PB-SMT outputs across broader aspects (e.g. fluency, reordering) for 9 language directions. Castilho et al. (2017) conduct an extensive qualitative and quantitative comparative evaluation of PB-SMT and NMT using automatic metrics and professional translators. Popovi\u0107 (2017) carries out an extensive comparison between NMT and PB-SMT languagerelated issues for the German-English language pair in both translation directions. These works (Bentivogli et al., 2016; Castilho et al., 2017; Popovi\u0107, 2017; Toral and S\u00e1nchez-Cartagena, 2017) show that NMT provides better translation quality than the previous state-of-the-art PB-SMT. This trend continues in other studies and use-cases: translation of literary text (Toral and Way, 2018) , MT post-editing setups (Specia et al., 2017) , industrial setups (Shterionov et al., 2017) , translation of patent documents (Long et al., 2016; Kinoshita et al., 2017) , less-explored language pairs (Klubi\u010dka et al., 2017 (Klubi\u010dka et al., , 2018)) , highly investigated \"easy\" translation pairs (Isabelle et al., 2017) , and translation of catalogues of technical tools (Beyer et al., 2017) . An opposite picture is also seen in the case of translation of the domain text; Nunez et al. (2019) showed PB-SMT outperforms NMT when translating user-generated content. The MT researchers have tested and compared PB-SMT and NMT in the resource-poor settings too. Koehn and Knowles (2017) , \u00d6stling and Tiedemann (2017), and Dowling et al. (2018) found that PB-SMT can provide better translations than NMT in low-resource scenarios. In contrast to these findings, however, many studies have demonstrated that NMT is better than PB-SMT in low-resource situations (Casas et al., 2019; Sennrich and Zhang, 2019) . Hence, the findings of this line of MT research have yielded indeed a mixed bag of results, where way ahead unclear. This work investigates translations of a software localisation text with two low-resource translation-pairs, Hindi-to-Tamil and English-to-Tamil, taking two MT paradigms, PB-SMT and NMT, into account. Experimental Setups The MT systems To build our PB-SMT systems we used the Moses toolkit (Koehn et al., 2007) . We used a 5-gram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) . Our PB-SMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011) , and (e) word-count and distortion penalties. The weights of the parameters are optimized using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the Open-NMT toolkit (Klein et al., 2017) . The NMT systems are Transformer models (Vaswani et al., 2017) . The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) . Recently, Sennrich and Zhang (2019) demonstrated that commonly used hyper-parameters configuration do not provide the best results in low-resource settings. Accordingly, we carried out a series of experiments in order to find the best hyperparameter configurations for Transformer in our low-resource settings. In particular, we played with some of the hyperparameters, and found that the following configuration lead to the best results in our low-resource translation settings: (i) the BPE vocabulary size: 8,000, (ii) the sizes of encoder and decoder layers: 4 and 6, respectively, (iii) learning-rate: 0.0005, (iv) batch size (token): 4,000, and (v) Transformer head size: 4. As for the remaining hyperparameters, we followed the recommended best set-up from Vaswani et al. (2017) . The validation on development set is performed using three cost functions: cross-entropy, perplexity and BLEU (Papineni et al., 2002) . The early stopping criteria is based on cross-entropy; however, the final NMT system is selected as per highest BLEU score on the validation set. The beam size for search is set to 12. Choice of Languages In order to test MT on low-resource scenarios, we chose English and two Indian languages: Hindi, and Tamil. English, Hindi, and Tamil are Germanic, Indo-Aryan and Dravidian languages, respectively, so the languages we selected for investigation are from different language families and morphologically divergent to each other. English is a less inflected language, whereas Hindi and Tamil are morphologically rich and highly inflected languages. Our first investigation is from a less inflected language to a highly inflected language (i.e. Englishto-Tamil), and the second one is between two morphologically complex and inflected languages (i.e. Hindi-to-Tamil). Thus, we compare translation in PB-SMT and NMT with two difficult translationpairs involving three morphologically divergent languages. Data Used This section presents our datasets. For experimentation we used data from three different sources: OPUS 2 (Tiedemann, 2012), WikiMatrix 3 (Schwenk et al., 2019) and PMIndia 4 (Haddow and Kirefu, 2020) . As mentioned above, we carried out experiments on two translation-pairs, English-to-Tamil and Hindi-to-Tamil, and study translation of a specialised domain data, i.e. software localisation. Corpus statistics are shown in Table 1 . We carried out experiments using two different setups: (i) in the first setup, the MT systems were built on a training set compiled from all data domains listed above; we call this setup MIXED, and (ii) in the second setup, the MT systems were built on a training set compiled only from different software localisation data from OPUS, viz. GNOME, KDE4 and Ubuntu; we call this setup IT. The development and test set sentences were randomly drawn from these localisation corpora. As can be seen from Table 1 , the number of training set sentences of the Hindi-to-Tamil task is less than half of that of the training set size of the English-to-Tamil task. In order to remove noise from the data sets, we adopted the following measures. We observed that the corpora of one language (say, Hindi) contains sentences of other languages (e.g. English), so we use a language identifier 5 in order to remove such noise. Then, we adopted a number of standard cleaning routines for removing noisy sentences, e.g. removing sentence-pairs that are too short, too long or which violate certain sentence-length ratios. In order to perform tokenisation for English, we used the standard tool in the Moses toolkit. For tokenising and normalising Hindi and Tamil sentences, we used the Indic NLP library. 6 Without a doubt, BPE is seen as the benchmark strategy for reducing data sparsity for NMT. We built our NMT engines on both word and subword-level training corpora in order to test BPE's effectiveness on low-resource translation tasks. Results and Discussion Automatic Evaluation We present the comparative performance of the PB-SMT and NMT systems in terms of the widely used automatic evaluation metric BLEU. Additionally, we performed statistical significance tests using bootstrap resampling methods (Koehn, 2004) . Sections 4.1.1 and 4.1.2 present the performance of the MT systems on the MIXED and IT setups, respectively. The MIXED Setup We show the BLEU scores on the test set in Table 2 . The first and second rows of the table represent the English-to-Tamil and Hindi-to-Tamil translation tasks, respectively. 7 The PB-SMT and NMT systems produce relatively low BLEU scores on the test set given the difficulty of the translation pairs. However, these BLEU scores underestimate the translation quality, given the relatively free word order in Tamil, and the fact that we have just a single reference translation set for evaluation. We see from Ta- ble 2 that PB-SMT surpassed NMT by a large margin in terms of BLEU in both the English-to-Tamil and Hindi-to-Tamil translation tasks, and found that the differences in the BLEU scores are statistically significant. The IT Setup This section presents the results obtained on the IT setup. The BLEU scores of the MT systems are reported in Table 3 . When we compare the BLEU scores of this table with those of Table 2 , we see a huge rise in terms of the BLEU scores for PB-SMT and NMT as far as English-to-Tamil translation is concerned, and the improvements are found to be statistically significant. As for the Hindi-to-Tamil translation, we see a substantial deterioration in BLEU (an absolute difference of 1.36 points, a 24.9% relative loss in terms of BLEU) for PB-SMT. We found that this loss is statistically significant too. We also see that in this task the BLEU score of the NMT system is nearly identical to the one in the MIXED setup (2.12 BLEU points versus 2.10 BLEU points). As far as the English-to-Tamil translation and the IT setup are concerned, the PB-SMT system outperforms the NMT system statistically significantly, and we see an improvement of an absolute of 6.33 points (corresponding to 69.3% relative) in terms of BLEU on the test set. The same trend is seen in the Hindi-to-Tamil translation task too. We have a number of observations from the results of the MIXED and IT setups. As discussed in Section 3.3, in the IT task, the MT systems were built exclusively on in-domain training data, and in the MIXED setup, the training data is composed of a variety of domains, i.e. religious, IT, political news. Use of in-domain data only in training does not have any positive impact on the Hindi-to-Tamil translation, and we even saw a significant deterioration in performance on BLEU for PB-SMT. We conjecture that the morphological complexity of the languages (Hindi and Tamil) involved in this translation could be one of the reasons why the NMT and PB-SMT systems performed so poorly when trained exclusively on small-sized specialised domain data. When we compare PB-SMT and NMT, we see that PB-SMT is always the leading system in both the following cases: (i) across the training data setups (MIXED and IT) and (ii) the translation-directions (English-to-Tamil and Hindi-to-Tamil). Reasons for very low BLEU Scores The BLEU scores reported in the sections above are very low. We looked at the translations of the test set sentences by the MT systems and compare them with the reference translations. We found that despite being good in quality, in many cases the translations were penalised heavily by the BLEU metric as a result of many n-gram mismatches with the corresponding reference translations. This happened mainly due to the nature of target language (Tamil) in question, i.e. Tamil is a free word order language. This is indeed responsible for the increase in nonoverlapping n-gram counts. We also found that translations contain lexical variations of Tamil words of the reference translation, again resulting in the increase of the non-overlapping n-gram counts. We show such translations from the Hindi-to-Tamil task in Table 4 . We also reported this phenomenon in Ramesh et al. (2020) and showed such translations from the English-to-Tamil task (cf. Table 3 ; Section 3.2 of Ramesh et al. ( 2020 )). Error Analysis We conducted a thorough error analysis of the English-to-Tamil and Hindi-to-Tamil NMT and PB-SMT systems built on the in-domain training data. For this, we randomly sampled 100 sentences from the respective test sets (English-to-Tamil and Hindi-  to-Tamil). The outcome of this analysis is presented in the following sections. Terminology Translation Terminology translation is arguably viewed as one of the most challenging problems in MT (Dinu et al., 2019; Haque et al., 2019; Exel et al., 2020) . Since this work focuses on studying translation of data from a specialised domain, we looked at this area of translation with a special focus. We first looked at the translations of OOV terms in order to see how they are translated into the target. We found that both the NMT systems (English-to-Tamil and Hindi-to-Tamil) either incorrectly translate the software terms or drop them during translation. This happened for almost all the OOV terms. Nonetheless, the NMT systems are able to correctly translate a handful of OOV terms; this phenomenon is also corroborated by Haque et al. (2019) while investigating translation of the judicial domain terms.  We show four examples in Table 5. In the first example, we show a source English sentence and its Tamil translation. We see from the translation that the NMT system drops the source-side terms 'ipod', 'iphone' and 'ipad' in the target translation. The SMT system translates the segment as 'most ipod, iphone'. In the second example, we see that a part ('Open') of a multiword term ('Open script') is cor-rectly translated into Tamil, and the NMT system omits its remaining part ('script') in translation. As for the SMT system, the source text is translated as 'opened script'. In the third example, we show another multiword English term ('Color set') and its Tamil translation (i.e. English equivalent 'set the color') by the NMT system, which is wrong. As for the SMT system, the source text is translated as 'set color'. Here, we see that both the MT systems made correct lexical choices for each word of the source term, although the meaning of the respective translation is different to that of the source term. This can be viewed as a cross-lingual disambiguation problem. In the fourth example, we show a single word source Hindi sentence ('Freecell') which is a term and name of a computer game. The Hindi-to-Tamil NMT system incorrectly translates this term into Tamil, and the English equivalent of the Tamil translation is in fact 'freebugs'. The translation of the fourth segment by the SMT system is its transliteration. Lexical Selection We observed that both NMT systems (English-to-Tamil and Hindi-to-Tamil) often make incorrect lexical selection for polysemous words, i.e. the NMT systems often produce a target translation of a word that has no connection with the underlying context of the source sentence in which the word appears. As an example, we show a Hindi sentence and its Tamil translation in Table 6 . The ambiguous word \u0939\u093e\u0932 ('haal') has three meanings in Hindi ('condition', 'recent' and 'hall') and their Tamil translations are different too. The Hindi-to-Tamil NMT system chooses the Tamil translation for the Hindi word \u0939\u093e\u0932 which is incorrect in the context of the source sentence. As for the SMT system, it translates the source text as \"names of games played recently\". It makes a correct lexical selection for the word in question. Wrong Word Order We observed that the NMT systems occasionally commit reordering errors in translation. In Haque et al. (2019) observed that NMT tends to omit more terms in translation than PB-SMT. We found that this is true in our case with non-term entities too as we observed that the NMT systems often omit words in the translations. As an example, in Table 8 , we show an English sentence, its Tamil translations and the English equivalents of the Tamil translations. We see from the table that the NMT system translates only the first word of the English sentence and drops the remainder of the sentence during translation, and the SMT system translates the first two words of the English sentence and drops the remainder of the sentence for translation. Word Omission Hindi \u0916\u095c\u093e \u090a\u092a\u0930 \u0938\u0947 \u0905\u0902 \u0926\u0930 [khada oopar se andar] NMT [Nil] SMT \u0b89 \u0bc7\u0bb3 \u0bb1 [ul \u0323l \u0323\u0113 nirkiratu] Hindi \u0930\u092a\u091f [rapat] NMT \u0ba8\u0bbe [N\u0101l \u0323] SMT \u0bc6\u0b9a [ ceyti] Hindi \u0928\u0939\u0940 [nahee] NMT \u0b87 \u0bc8\u0bb2 \u0b87 \u0bc8\u0bb2 \u0b87 \u0bc8\u0bb2 \u0b87 \u0bc8\u0bb2 [llai illai illai illai illai] SMT \u0b87 \u0bc8\u0bb2 [llai] Hindi \u0917\u0932\u0924 [galat] NMT \u0ba4\u0bb5 \u0ba4\u0bb5 \u0ba4\u0bb5 \u0ba4\u0bb5 [thavaru thavaru thavaru] SMT \u0ba4\u0bb5 [thavaru] Table 9 : Miscellaneous errors in translation. Miscellaneous Errors We report a few more erroneous translations by the Hindi-to-Tamil NMT system in Table 9 . The errors in these translations occur for a variety of reasons. The translations of the source sentences sometimes contain strange words that have no relation to the meaning of the source sentence. The top two example translations belong to this category. The translation of the first sentence by the SMT system is partially correct. As for the second example, the SMT system translates it as 'report' which is incorrect too. We also see that the translations occasionally contain repetitions of other translated words. This repetition of words is seen only for the NMT system. The bottom two translation examples of Table 9 belong to this category. These findings are corroborated by some of the studies that pursued this line of research (e.g. Farajian et al. ( 2017 )). Unsurprisingly, such erroneous translations are seen more with the Hindito-Tamil translation direction. As for SMT, the MT system translates the third and fourth sentences incorrectly and correctly, respectively. In both cases, unlike NMT, the translations do not contain any repetition of other translated words. We sometimes found the appearance of one or more unexpected words in the translation, which completely changes the meaning of the translation, as shown in Table 10 . However, the SMT system correctly translates the first two source sentences shown in Table 10 . In the case of the third sentence, it translates the source sentence as 'move to trash'. We also observed that the translation-equivalents of some words are in fact the transliterations of the words themselves.  We observed this happening only for the Englishto-Tamil direction. For example, the English word 'pixel' has a specific Tamil translation (i.e. \u0baa\u0b9f [pat \u0323attun \u0323ukku]). However, the NMT system produces a transliterated form of that word in the target translation. In practice, many English words, especially terms or product names, are often directly used in Tamil text. Accordingly, we found the presence of transliterated forms of some words in the Tamil text of the training data. This could be the reason why the NMT systems generates such translations. The BPE segmentation on the Hindi-to-Tamil translation We saw in Section 4.1 that the BPE-based segmentation negatively impacts the translation between the two morphologically rich and complex languages, i.e. Hindi-to-Tamil. Since this segmentation process does not follow any linguistic rules and can abruptly segment a word at any character position, this may result in syntactic and morphological disagreements between the source-target sentence-pair and aligned words, respectively. We also observed that this may violate the underlying semantic agreement between the source-target sentence-pairs. As an ple, we found that the BPE segmentation breaks the We show here another similar example, where the Hindi word \u0930\u0902 \u0917 [rangon] whose English equivalent is 'colors' is the translation of the Tamil word \u0bb5 \u0ba3 \u0b95 [van \u0323nankal \u0323]. However, when the BPE segmenter is applied to the target-side word \u0bb5 \u0ba3 \u0b95 [van \u0323nankal \u0323], it is split into three subwords \u0bb5 \u0ba3 \u0b95 [va n \u0323na nkal \u0323] whose English equivalent is 'do not forget' which has no relation to \u0bb5 \u0ba3 \u0b95 [van \u0323nankal \u0323] (English equivalent: 'colors'). Unlike European languages, the Indian languages are usually fully phonetic with compulsory encoding of vowels. In our case, Hindi and Tamil differ a lot in terms of orthographic properties (e.g. different phonology, no schwa deletion in Tamil). The grammatical structures of Hindi and Tamil are different too, and they are morphologically divergent and from different language families. We saw that the BPE-based segmentation can completely change the underlying semantic agreements of the source and target sentences, which, in turn, may provide the learner with wrong (reasoning) knowledge about the sentence-pairs. This could be one of the reasons why the BPE-based NMT model is found to be underperforming in this translation task. This finding is corroborated by Banerjee and Bhattacharyya (2018) who in their work found that the Morfessorbased segmentation can yield better translation quality than the BPE-based segmentation for linguistically distant language-pairs, and other way round for the close language-pairs. Conclusion In this paper, we investigated NMT and PB-SMT in resource-poor scenarios, choosing a specialised data domain (software localisation) for translation and two rarely-tested morphologically divergent language-pairs, Hindi-to-Tamil and English-to-Tamil. We studied translations on two setups, i.e. training data compiled from (i) freely available variety of data domains (e.g. political news, Wikipedia), and (ii) exclusively software localisation data domains. In addition to an automatic evaluation, we carried out a manual error analysis on the translations produced by our MT systems. Use of in-domain data only at training has a positive impact on translation from a less inflected language to a highly inflected language, i.e. Englishto-Tamil. However, it does not impact the Hindi-to-Tamil translation. We conjecture that the morphological complexity of the source and target languages (Hindi and Tamil) involved in translation could be one of the reasons why the MT systems performed reasonably poorly even when they were exclusively trained on specialised domain data. We looked at the translations produced by our MT systems and found that in many cases, the BLEU scores underestimate the translation quality mainly due to relatively free word order in Tamil. In this context, Shterionov et al. (2018) computed the degree of underestimation in quality of three most-widely used automatic MT evaluation metrics: BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) , showing that for NMT, this may be up to 50%. We refer the interested readers to Way (2018 Way ( , 2019) ) who also drew attention to this phenomenon. Our error analysis on the translations by the English-to-Tamil and Hindi-to-Tamil MT systems reveals many positive and negative sides of the two paradigms: PB-SMT and NMT: (i) NMT makes many mistakes when translating domain terms, and fails poorly when translating OOV terms, (ii) NMT often makes incorrect lexical selections for polysemous words and omits words and domain terms in translation, and occasionally commit reordering errors, and (iii) translations produced by the NMT systems occasionally contain repetitions of other translated words, strange translations and one or more unexpected words that have no connection with the source sentence. We observed that whenever the NMT system encounters a source sentence containing OOVs, it tends to produce one or more unexpected or repetitions of other translated words. As for SMT, unlike NMT, the MT systems usually do not make such mistakes, i.e. repetitions, strange, spurious or unexpected words in translation. We observed that the BPE-based segmentation can completely change the underlying semantic agreements of the source and target sentences of the languages with greater morphological complexity. This could be one of the reasons why the Hindito-Tamil NMT system's translation quality is poor when the system is trained on the sub-word-level training data in comparison to one that was trained on the word-level training data. We believe that the findings of this work provide significant contributions to this line of MT research. In future, we intend to consider more languages from different language families. We also plan to judge errors in translations using the multidimensional quality metrics error annotation framework (Lommel et al., 2014) which is a widely-used standard translation quality assessment toolkit in the translation industry and in MT research. The MT evaluation metrics such as chrF (Popovi\u0107, 2015) which operates at the character level and COMET (Rei et al., 2020) which achieved new state-of-the-art performance on the WMT 2019 Metrics Shared Task (Ma et al., 2019) obtained high levels of correlation with human judgements. We intend to consider these metrics (chrF and COMET) in our future investigation. As in Exel et al. (2020) who examined terminology translation in NMT in an industrial setup while using the terminology integration approaches presented in Dinu et al. (2019) , we intend to investigate terminology translation in NMT using the MT models of Dinu et al. (2019) on English-to-Tamil and Hindi-to-Tamil. Acknowledgments The ADAPT Centre for Digital Content Technology is funded under the Science Foundation Ireland (SFI) Research Centres Programme (Grant No. 13/RC/2106) and is co-funded under the European Regional Development Fund. This project has partially received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No. 713567, and the publication has emanated from research supported in part by a research grant from SFI under Grant Number 13/RC/2077.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.9998024393176823,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article mentions that the ADAPT Centre for Digital Content Technology is funded under the Science Foundation Ireland (SFI) Research Centres Programme and is co-funded under the European Regional Development Fund. It also mentions partial funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement, and a research grant from SFI. These are all indicative of research agency and foundation funding sources. There is no mention of defense or corporate funding.",
    "abstract": "Statistical machine translation (SMT) was the state-of-the-art in machine translation (MT) research for more than two decades, but has since been superseded by neural MT (NMT). Despite producing state-of-the-art results in many translation tasks, neural models underperform in resource-poor scenarios. Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of translation of many low-resource languages. In this work, we investigate the performance of phrasebased SMT (PB-SMT) and NMT on two rarelytested low-resource language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation.",
    "countries": [
        "Ireland"
    ],
    "languages": [
        "English",
        "Tamil",
        "Hindi"
    ],
    "numcitedby": 3,
    "year": 2020,
    "month": "December",
    "title": "An Error-based Investigation of Statistical and Neural Machine Translation Performance on {H}indi-to-{T}amil and {E}nglish-to-{T}amil",
    "values": {
        "building on past work": "Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of translation of many low-resource languages. In this context, in Ramesh et al. (2020) , we investigated the performance of PB-SMT, NMT and a commercial MT system (Google Translate (GT)) 1 on English-to-Tamil taking the software localisation data into account, i.e. the same data as the one used in this work. In particular, in Ramesh et al. (2020) , we produced rankings of the MT systems (PB-SMT, NMT and GT) via a social media platform-based human evaluation scheme, and demonstrate our findings in this lowresource domain-specific text translation task. of this work is to study translations of the MT systems (PB-SMT and NMT) in under-resourced conditions, we provide a brief overview on some of the papers that compared PB-SMT and NMT on highresource settings too. Junczys-Dowmunt et al. (2016) compare PB-SMT and NMT on a range of translation-pairs and show that for all translation directions NMT is either on par with or surpasses PB-SMT. Bentivogli et al. (2016) analyse the output of MT systems in an Englishto-German translation task by considering different linguistic categories. Toral and S\u00e1nchez-Cartagena (2017) conduct an evaluation to compare NMT and PB-SMT outputs across broader aspects (e.g. fluency, reordering) for 9 language directions. Castilho et al. (2017) conduct an extensive qualitative and quantitative comparative evaluation of PB-SMT and NMT using automatic metrics and professional translators. Popovi\u0107 (2017) carries out an extensive comparison between NMT and PB-SMT languagerelated issues for the German-English language pair in both translation directions.",
        "novelty": "This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. We observed that the BPE-based segmentation can completely change the underlying semantic agreements of the source and target sentences of the languages with greater morphological complexity. We believe that the findings of this work provide significant contributions to this line of MT research.",
        "performance": "Despite producing state-of-the-art results in many translation tasks, neural models underperform in resource-poor scenarios. As far as the English-to-Tamil translation and the IT setup are concerned, the PB-SMT system outperforms the NMT system statistically significantly, and we see an improvement of an absolute of 6.33 points (corresponding to 69.3% relative) in terms of BLEU on the test set. The same trend is seen in the Hindi-to-Tamil translation task too."
    }
}