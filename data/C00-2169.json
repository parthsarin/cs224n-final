{
    "article": "Speech repairs occur often in spontaneous spoken dialogues. The ability to detect and correct those repairs is necessary for any s p o k en language system. We present a framework to detect and correct speech repairs where all relevant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated. The basic idea is to reduce the search space for repairs as soon as possible by cascading lters that involve more and more features. At rst an acoustic module generates hypotheses about the existence of a repair. Second a stochastic model suggests a correction for every hypothesis. Well scored corrections are inserted as new paths in the word lattice. Finally a lattice parser decides on accepting the repair. Introduction Spontaneous speech is dis uent. In contrast to read speech the sentences aren't perfectly planned before they are uttered. Speakers often modify their plans while they speak. This results in pauses, word repetitions or changes, word fragments and restarts. Current automatic speech understanding systems perform very well in small domains with restricted speech but have great di culties to deal with such dis uencies. A system that copes with these self corrections =repairs must recognize the spoken words and identify the repair to get the intended meaning of an utterance. To c haracterize a repair it is commonly segmented into the following four parts cf. g.1: reparandum: the wrong\" part of the utterance interruption point IP: marker at the end of the reparandum editing term: special phrases, which indicate a repair like w ell\", I mean\" or lled pauses such as uhm\", uh\" reparans: the correction of the reparandum  It remains an open question whether the two terms should be deleted before a semantic analysis as suggested sometimes in the literature 1 . If both terms are marked it is a straightforward preprocessing step to delete reparandum and editing term. In the Verbmobil 2 corpus, a corpus dealing with appointment s c heduling and travel planning, nearly 21 of all turns contain at least one repair. As a consequence a speech understanding system that cannot handle repairs will lose performance on these turns. Even if repairs are de ned by syntactic and semantic well-formedness Levelt, 1983 we observe that most of them are local phenomena. At this point w e h a ve to di erentiate between restarts and other repairs 3 modi cation repairs. Modi cation repairs have a strong correspondence between reparandum and reparans, 1 In most cases a reparandum could be deleted without any loss of information. But, for example, if it introduces an object which is referred to later, a deletion is not appropriate. 2 This work is part of the VERBMOBIL project and was funded by the German Federal Ministry for Research and Technology BMBF in the framework of the Verbmobil Project under Grant BMBF 01 IV 701 V0. The responsibility for the contents of this study lies with the authors. 3 Often a third kind of repair is de ned: abridged repairs\". These repairs consist solely of an editing term and are not repairs in our sense. whereas restarts are less structured. In our believe there is no need for a complete syntactic analysis to detect and correct most modi cation repairs. Thus, in what follows, we will concentrate on this kind of repair. There are two major arguments to process repairs before parsing. Primarily spontaneous speech is not always syntactically well-formed even in the absence of self corrections. Second Meta-rules increase the parsers' search space. This is perhaps acceptable for transliterated speech but not for speech recognizers output like lattices because they represent millions of possible spoken utterances. In addition, systems which are not based on a deep syntactic and semantic analysis e.g. statistical dialog act prediction require a repair processing step to resolve contradictions like the one in g. 1. We propose an algorithm for word lattices that divides repair detection and correction in three steps cf. g. 2 First, a trigger indicates potential IPs. Second, a stochastic model tries to nd an appropriate repair for each I P b y guessing the most probable segmentation. To accomplish this, repair processing is seen as a statistical machine translation problem where the reparandum is a translation of the reparans. For every repair found, a path representing the speaker's intended word sequence is inserted into the lattice. In the last step, a lattice parser selects the best path. Because it is impossible for a real time speech system to check for every word whether it can be part of a repair, we use triggers which indicate the potential existence of a repair. These triggers must be immediately detectable for every word in the lattice. Currently we are using two di erent kind of triggers 4 : 1. Acoustic prosodic cues: Speakers mark the IP in many cases by prosodic signals like pauses, hesitations, etc. A prosodic classier 5 determines for every word the probability of an IP following. If it is above a certain threshold, the trigger becomes active. For a detailed description of the acoustic aspects see Batliner et al., 1998. 2 . Word fragments are a very strong repair indicator. Unfortunately, no speech recognizer is able to detect word fragments to date. But there are some interesting approaches to detect words which are not in the recognizers vocabulary Klakow et al., 1999 . A word fragment is normally an unknown word and we hope that it can be distinguished from unfragmented unknown words by the prosodic classi er. So, currently this is a hypothetical trigger. We will elaborate on it in the evaluation section cf. sect. 5 to show the impact of this trigger. If a trigger is active, a search for an acceptable segmentation into reparandum, editing term and reparans is initiated. Scope Detection As mentioned in the introduction repair segmentation is based mainly on a stochastic translation model. Before we explain it in detail we give a short introduction to statistical machine translation 6 . The fundamental idea is the assumption that a given sentence S in a source language e.g. English can be translated in any sentence T in a target language e.g. German. To e v ery pair S; T a probability is assigned which re ects the likelihood that a translator who sees S will produce T as the translation. The statistical machine translation problem is 4 Other triggers can be added as well. Stolcke et al., 1999 for example integrate prosodic cues and an extended language model in a speech recognizer to detect IPs. 5 The classi er is developed by the speech group of the IMMD 5. Special thanks to Anton Batliner, Richard Huber and Volker Warnke. 6 A more detailed introduction is given by Brown et al., 1990 formulated as: T = argmax T PTjS 1 This is reformulated by B a yes' law for a better search space reduction, but we are only interested in the conditional probability PTjS. For further processing steps we h a ve t o i n troduce the concept of alignment Brown et al., 1990. Let S be the word sequence S 1 ; S 2 : : : S l S l 1 and T = T 1 ; T 2 : : : T m T m 1 . We can link a word in T t o a w ord in S. This re ects the assumption that the word in T is translated from the word in S. F or example, if S is On Thursday\" and T is Am Donnerstag\" Am\" can be linked to On\" but also to Thursday\". If each w ord in T is linked to exactly one word in S these links can be described by a v ector a m 1 = a 1 : : : a m with a i 2 0 : : : l . If the word T j is linked to S i then a j = i. If it is not connected to any w ord in S then a j = 0. Such a v ector is called an alignment a. 1 ; T j,1 1 ; m ; S PT j ja j 1 ; T Parameter Estimation The conditional probabilities in equation 3 cannot be estimated reliably from any corpus of realistic size, because there are too many parameters. For example both P in the product depend on the complete reparans RS. Therefore we simplify the probabilities by assuming that m depends only on l, a j only on j,m and l and nally RD j on RS a j . So equation 3 be- comes PRD; ajRS = Pmjl m Y j=1 Pa j jj; m; l PRD j jRS a j 4 These probabilities can be directly trained from a manually annotated corpus, where all repairs are labeled with begin, end, IP and editing term and for each reparandum the words are linked to the corresponding words in the respective reparans. All distributions are smoothed by a simple back-o method Katz, 1987 to avoid zero probabilities with the exception that the word replacement probability PRD j jRS a j i s smoothed in a more sophisticated way. Smoothing Even if we reduce the number of parameters for the word replacement probability b y the simpli cations mentioned above there are a lot of parameters left. With a vocabulary size of 2500 words, 2500 2 parameters have to be estimated for PRD j jRS a j . The corpus 7 contains 3200 repairs from which w e extract about 5000 word links. So most of the possible word links never occur in the corpus. Some of them are more likely to occur in a repair than others. For example, the replacement of Thursday\" by F riday\" is supposed to be more likely than by eating\", even if both replacements are not in the training corpus. Of course, this is related to the fact that a repair is a syntactic and or semantic anomaly. W e make use of it by adding two additional knowledge sources to our model. Minimal syntactic information is given by partof-speech POS tags and POS sequences, semantic information is given by semantic word classes. Hence the input is not merely a sequence of words but a sequence of triples. Each triple has three slots word, POS tag, semantic class. In the next section we will describe how w e obtain these two information pieces for every word in the lattice. With this additional information, PRD j jRS a j probability could be smoothed by linear interpolation of word, POS and semantic class replacement probabilities. PRD j jRS a j = PWo r d RD j jW o r d RS a j + PSemClassRD j jSemClassRS a j + PPO S RD j jP O S RS a j 5 with + + = 1 . Wo r d RD j is the notation for the selector of the word slot of the triple at position j. Integration with Lattice Processing We can now detect and correct a repair, given a sentence annotated with POS tags and semantic classes. But how can we construct such a sequence from a word lattice? Integrating the model in a lattice algorithm requires three steps: mapping the word lattice to a tag lattice triggering IPs and extracting the possible reparandum reparans pairs introducing new paths to represent the plausible reparans The tag lattice construction is adapted from Samuelsson, 1997 . For every word edge and every denoted POS tag a corresponding tag edge is created and the resulting probability is determined. If a tag edge already exists, the probabilities of both edges are merged. The original words are stored together with their unique semantic class in a associated list. Paths through the tag graph are scored by a POS-trigram. If a trigger is active, all paths through the word before the IP need to be tested whether an acceptable repair segmentation exists. Since the scope model takes at most four words for reparandum and reparans in account it is su cient to expand only partial paths. Each of these partial paths is then processed by the scope model. To reduce the search space, paths with a low score can be pruned. Repair processing is integrated into the Verbmobil system as a lter process between speech recognition and syntactic analysis. This enforces a repair representation that can be integrated into a lattice. It is not possible to mark only the words with some additional information, because a repair is a phenomenon that depends on a path. Imagine that the system has detected a repair on a certain path in the lattice and marked all words by their repair function. Then a search process e.g. the parser selects a di erent path which shares only the words of the reparandum. But these words are no reparandum for this path. A solution is to introduce a new path in the lattice where reparandum and editing terms are deleted. As we said before, we do not want to delete these segments, so they are stored in a special slot of the rst word of the reparans. The original path can now be reconstruct if necessary. To ensure that these new paths are comparable to other paths we score the reparandum the same way the parser does, and add the resulting value to the rst word of the reparans. As a result, both the original path and the one with the repair get the same score except one word transition. The probably bad transition in the original path from the last word of the reparandum to the rst word of the reparans is replaced by a probably good transition from the reparandum's onset to the reparans. We take the lattice in g. 2 to give an example. The scope model has marked I cannot\" as the reparandum, no\" as an editing term, and I can\" as the reparans. We sum up the acoustic scores of I\", \"can\" and no\". Then we add the maximum language model scores for the transition to I\", to can\" given I\", and to no\" given I\" and can\". This score is added as an o set to the acoustic score of the second I\". Results and Further Work Due to the di erent trigger situations we performed two tests: One where we use only acoustic triggers and another where the existence of a perfect word fragment detector is assumed. The input were unsegmented transliterated utterance to exclude in uences a word recognizer. We restrict the processing time on a SUN ULTRA 300MHZ to 10 seconds. A direct comparison to other groups is rather di cult due to very di erent corpora, evaluation conditions and goals. Nakatani and Hirschberg, 1993 suggest a acoustic prosodic detector to identify IPs but don't discuss the problem of nding the correct segmentation in depth. Also their results are obtained on a corpus where every utterance contains at least one repair. Shriberg, 1994 also addresses the acoustic aspects of repairs. Parsing approaches like in Bear et al., 1992; Hindle, 1983; Core and Schubert, 1999 must be proved to work with lattices rather than transliterated text. An algorithm which is inherently capable of lattice processing is proposed by Heeman Heeman, 1997 . He rede nes the word recognition problem to identify the best sequence of words, corresponding POS tags and special repair tags. He reports a recall rate of 81 and a precision of 83 for detection and 78 80 for correction. The test settings are nearly the same as test 2. Unfortunately, nothing is said about the processing time of his module. We h a ve presented an approach to score potential reparandum reparans pairs with a relative simple scope model. Our results show that repair processing with statistical methods and without deep syntactic knowledge is a promising approach at least for modi cation repairs. Within this framework more sophisticated scope models can be evaluated. A system integration as a lter process is described. Mapping the word lattice to a POS tag lattice is not optimal, because word information is lost in the search for partial paths. We plan to implement a combined combined POS word tagger.",
    "abstract": "Speech repairs occur often in spontaneous spoken dialogues. The ability to detect and correct those repairs is necessary for any s p o k en language system. We present a framework to detect and correct speech repairs where all relevant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated. The basic idea is to reduce the search space for repairs as soon as possible by cascading lters that involve more and more features. At rst an acoustic module generates hypotheses about the existence of a repair. Second a stochastic model suggests a correction for every hypothesis. Well scored corrections are inserted as new paths in the word lattice. Finally a lattice parser decides on accepting the repair.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German"
    ],
    "numcitedby": "29",
    "year": "2000",
    "month": "",
    "title": "Processing Self Corrections in a speech to speech system"
}