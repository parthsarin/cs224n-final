{
    "article": "The large amount of data available in social media, forums and websites motivates researches in several areas of Natural Language Processing, such as sentiment analysis. The popularity of the area due to its subjective and semantic characteristics motivates research on novel methods and approaches for classification. Hence, there is a high demand for datasets on different domains and different languages. This paper introduces TweetSentBR, a sentiment corpus for Brazilian Portuguese manually annotated with 15.000 sentences on TV show domain. The sentences were labeled in three classes (positive, neutral and negative) by seven annotators, following literature guidelines for ensuring reliability on the annotation. We also ran baseline experiments on polarity classification using six machine learning classifiers, reaching 80.38% on F-Measure in binary classification and 64.87% when including the neutral class. We also performed experiments in similar datasets for polarity classification task in comparison to this corpus. Introduction Sentiment Analysis (SA) became a popular area of Natural Language Processing in the last decade. The classification of semantic orientation of documents is a challenge for artificial intelligence methods since it is based not only on the regular meaning of words, but also on their semantic role in the context and on the author's intention. Furthermore, the amount of data available in blogs, social media posts and forums has created a great opportunity for researchers to build datasets for evaluating methods and studying new linguistic phenomena. Websites on e-commerce, movie reviews and hotel reservations usually allow the user to provide an objective evaluation besides the written commentaries. This objective evaluation (binary recommendation, star score, 10-point scale) can be a good feature for automatic labeling large datasets on semantic orientation, thus improving the resources for researches over the past decades (Pang et al., 2002; Pang and Lee, 2005; Blitzer et al., 2007) . The limitation of this technique is the data available in this conditions. Social media, for example, is a large source of user opinions and evaluation (Pak and Paroubek, 2010) , but the lack of an objective score attached to the posts demands a manual annotation in order to data become useful for SA, even though the data is enriched by linguistic phenomena such as expressions, slangs and irony. Manual annotation ends up being more expensive and time consuming, since it demands several guarantees of accuracy, such as developing guidelines, training annotators and revising the annotation (Hovy and Lavid, 2010) . In this paper we introduce TweetSentBR (TTsBR), a corpus manually annotated with data extracted from Twitter. The section 2. presents some related work on SA and corpus annotation. Section 3. presents the corpus and its properties, such as the size, the annotation tags, the information on annotators and the process of data extraction. Section 4. presents data analysis and polarity classification experiments on the corpus. Section 5. is a brief discussion on the importance of the corpus and how it can be used in Brazil-ian Portuguese research on SA. Related Work Several works present new methods and approaches for tasks such as polarity classification (Turney, 2002; Pang and Lee, 2005) , detection of irony (Carvalho et al., 2009; Reyes et al., 2012) and aspect extraction in text (Hu and Liu, 2004 ). One of the major issues of this area is the building of datasets for evaluating methods and for training machine learning models. Turney (2002) , one of the first works on polarity classification, used product reviews labeled as \"recommended\" and \"not recommended\". The source of the data was a website called Epinions, where users could evaluate products and leave a five star score for each review. The authors considered any review with less than 3 stars as \"not recommended\". Pang et al. (2002) uses a similar score (star rating) in order to compile a corpus of movie reviews on three classes (positive, negative and neutral). The automatic approach worked very well for building large datasets, but the method limited research on domains where users input an objective score. Despite of the challenges of the manual annotation, researches began building new datasets by training annotators to label the data. Socher et al. (2013) introduces Stanford Sentiment Treebank, a relabeling of the previous IMDB corpus presented in (Pang and Lee, 2005) . SemEval, an important semantic evaluation event, also produces several datasets for English designed for SA tasks (Nakov et al., 2016) . Some authors even used distant supervision techniques for automatic labeling large datasets quickly using features such as emoticons (Go et al., 2009) . In Brazilian Portuguese, several works presented corpora for SA. Freitas et al. (2012) introduce ReLi, a sentiment corpus of book reviews manually annotated in three classes (positive, neutral and negative). The authors have chosen books from different genres in order to vary the linguistic phenomena in the corpus (from teenage books to literature classics). ReLi contains annotation of semantic ori-entation, part-of-speech tagging and aspect of opinion, and it was later used as resource for researches in SA (Balage et al., 2013; Brum et al., 2016) . One of the issues on this corpus observed on the literature is the unbalanced classes -the majority of sentences is neutral (72%), while the negative class represents only 4% of the data. On the product review domain, Hartmann et al. (2014) presented Buscape corpus, a large corpora in Brazilian Portuguese. The corpus contains 13.685 reviews labeled as positive and negative, using scores given by users on Buscape, a popular e-commerce website. A similar dataset is Mercado Livre corpus, introduced in Avanc \u00b8o (2015), containing 43.818 product reviews also labeled automatically and balanced between the two classes. Silva et al. (2011) collected a corpus from Twitter in Portuguese. The dataset was collected by searching two entities in the social network (Dilma and Serra, two running candidates at the time) and manually annotated as positive or negative. The corpus contains 76.358 documents balanced between positive and negative. The corpus was originally constructed for sentiment stream analysis meaning it contains several retweets and links, phenomena that may interfere on sentiment classification but is vital to maintain the stream for the former task. Also on binary polarity classification, Moraes et al. (2015) introduce the Corpus 7x1, a brazilian portuguese corpus on Twitter comments during the 2014 World Cup semi-finals. The corpus presents some interesting user behavior such as irony, sarcasm, cheering and angry due to the final match score. Corpus 7x1 contains 2.728 tweets labeled manually in three classes -the neutral class represents tweets that do not align with either positive or negative sentiments. Moraes et al. (2016) also uses Twitter as the source of data, but compile a corpus of computer products containing 2.317 tweets. The data is manually labeled in three classes and the authors also performed experiments on SA using lexical-based classifiers and SVM. A large Twitter corpus was compiled by (Correa Junior et al., 2017) using distant supervision. The authors labeled tweets in Brazilian Portuguese using emojis representing positive and negative sentiments following the work of Go et al. (2009) in English. The corpus contains 554.623 positive tweets and 425.444 negative. The approach is a fast way to label data, but the method can not guarantee the absence of noise data such as irony, sarcasm or incorrect labels. TweetSentBR TweetSentBr is composed of 15.000 tweets (17.166 tokens) extracted using Python-Twitter 1 , a wrapper for Twitter API. Due to the limitations of Twitter API, we developed a continuous crawler in order to obtain documents during the first semester of 2017. The final dataset is split in two documents -a training set with 12.999 documents labeled in positive (44%), neutral (26%) and negative (29%); and a test set composed of 2001 documents with similar distribution to the training set, 45%, 25% and 29% respectively. See Table 1 for the number of documents in each class. Data source Data was extracted from Twitter between January and July in 2017. We chose to focus on the TV show domain because of the large amount of user generated content on Twitter during the exhibition of the shows. Hashtags (#) are used on social media to group messages on topics and the TV shows usually ask for its audience to use a specific hashtag in order to get visibility in these social networks. Some of the program hashtags group hundreds of thousands of messages during the exhibition of a show and that content can represent suggestions, complaints, evaluations and questions to the entities related to the programs. We empirically defined nine programs from three major TV channels in Brazil based on their popularity and activeness in social media. Talk-shows, reality shows (gastronomy and music) and variety shows were chosen in order to diversify the phenomena in the corpus. The periodicity of the exhibitions are also different, some shows go live daily when others go live once or twice a week. Since we were looking for user generated content, we ignored documents generated by public entities, such as celebrities, companies, TV channels or any official user on Twitter. We also discarded retweets, which are the reposts of popular posts in the social network. Classes definition Following Hovy and Lavid (2010) recommendations, a codebook or manual was written to ensure the agreement between annotators. The codebook contains examples, definitions and tips for the annotation process. The rules and guidelines were formed by empirically observing the dataset crawled before the annotation update based on the feedback from the annotators during the early stages of annotation. The definitions were created based on the domain and the input received by the annotators after the first contact with the data. These are the guidelines for the annotation in TTsBR: Positive class: Positive sentences describe feelings of pleasure, satisfaction, compliment or recommendation. The target of the sentiment must be the TV show or any entity related to it (host, guests, audience, sketches, invited bands...). Positive comparisons, such as \"This show is better than the other\" are considered positive and emojis can be strong indicatives of positivity. Negative class: Negative sentences describe feelings of disagreement, disapprove, complaint or hate. The target of the sentiment must be the TV show or any entity related to it (hosts, guests, audience, sketches, invited bands...). Neg-ative sentences can be direct, as in \"Today's show is terrible...\" or implied in a suggestion, as in \"the host could improve its jokes, right?\". Factual information such as delays, abrupt cuts or technical failures are also considered negative as long as it refers for the show or any entity related to it. Emojis are also good indicatives of negativity. Neutral class: The neutral label must be used for any sentence the annotator could not identify as an opinion (positive or negative) direct or implied. Factual sentences that do not represent a hit or a flaw, such as \"Show X just started\", inaccurate semantic orientation (\"Don't know what to think about this\") and sentences the annotator can not completely comprehend were instructed to be annotated as neutral. Some tweets in the corpus were generated by social media robots (most of them on audience measurements) and the annotators labeled these as neutral as well. We also wanted to keep track of the sentences that most caused doubt in the annotators. The annotators had a check box to mark in case of doubt in the annotation, even though this option did not prevent the annotator from labeling the sentiment of the sentence. The annotators were instructed to mark the doubt option every time they felt divided between two or more classes or when they took more than the average time (2 minutes) in a sentence. The addition of a doubt option gives us new information on the data and also reduces the stress on annotators. In the first stages of annotation, only an average of 10% of sentences were marked. Annotation process For the annotation process we recruited seven native speakers of Brazilian Portuguese in three different areas -linguistics, journalism and computer science. The annotation process was based on Hovy and Lavid (2010) , following the eight steps of annotation in order to improve the reliability of the resource. We developed a user friendly interface for the annotators to label the tweets (Figure 1 ). The interface contains the codebook, the phases of annotation, a progress bar and a panel with tweets for labeling. The annotation panel shows to the annotator the three classes and a box to be checked when in doubt (even though every tweet must have a label chosen in order to proceed to the next phase) and a side box with quick tips, contact information and a link to the codebook. Each annotator received a set with one hundred tweets to be labeled. After this step we measured the average time, agreement (all received the same set) and we took notes of the questions about the codebook guidelines. Then we proceeded to rewrite the codebook, adding more examples and detailing the definitions based on the questions presented on the first annotation. The next step was a meeting with all annotators to receive the feedback of the annotators, when the tweets were revised by everyone and we presented the new version of the codebook. We then proceeded to the regular annotation. First the participants labeled 300 tweets in order to measure agreement. We used Krippendorf's Alpha (Kripendorff, 2004) to measure the agreement of the annotators. In this phase we obtained 52.9% on the nominal measure and 70% on the in- Two supervisors annotated a small portion of the corpus in order to obtain a test set revised. The goal was to form a 10% part of the dataset specially labeled for evaluating machine learning methods. Some of the data annotated in the agreement phase was also used to compose this set. For the release, we define the general sentiment for each document based on a major voting of the labels provided by each annotator. Some documents were only labeled by one annotator, while others were annotated by 3 or 7 annotators. 45 documents tied and have no sentiment label, even though they were kept in the dataset with a \"none\" label. Release and distribution The dataset is available in http://bitbucket.org/ HBrum/tweetsentbr/. Twitter has a Privacy Policy forbidding the redistribution of the data, so we managed to provide only the ids of the tweets in the corpus. Any user with a working identification can search for the tweets freely. We provide the dataset with the ids, the hashtags used in the search, full annotators labels, the count of how many annotators checked the doubt option and the general sentiment for each document, as well as a tool for downloading the dataset as long as having a valid credential (provided by Twitter itself). An example of the dataset is presented below: id hashtag labels h s split - -------------------------------------- - 86304477 #encontro [1,1,1] 0 1 train 86558371 #theNoite [1,0,0] 2 0 test 86506323 #encontro [1] 1 1 train 86466839 #masterChef [-1] 0 -1 test Comparison with Brazilian datasets Comparison between datasets is a general challenge in NLP since each task carries its own issues, needs and goals. Even in a specific field it can be hard to directly compare The data distribution between classes can be a issue for machine learning methods. A common approach for handling unbalanced corpora is using under-sample (removing samples from the majority classes until the corpus is equally distributed). In this scenario, TTsBR only loses 21% of its size (11.768 documents) when ReLi, 7x1 and Computer-BR end up with 1.788, 1.347 and 591 respectively. When comparing automatic labeled methods, by scorebased annotation (Buscape and Mercado Livre) or distant supervision (Pelesent), manual approaches suffer from time consumption, but also gain in reliability. The distant supervision approach, for example, performed in Pelesent demands the removal of emojis and emoticons from the final corpus. This information may be important for a linguistic approach of even semantic study of user behavior online. We believe TTsBR can be helpful for evaluating new polarity classification approaches or linguistic studies, since it relies on a popular topic, is publicly available, was mostly revised by more than one annotator and the annotation methodology is documented and easily open for replication. Experiments In order to investigate the properties of the corpora, we defined a series of experiments to determine word frequency, the class balance, and we also performed polarity classification using baseline methods for Portuguese. For the experiments, we performed a preprocessing of the data -we replaced numbers (dates, currency values) by a NUMBER token, we also replaced user names and links by the tokens USERNAME and URL, respectively. We trimmed repetition of characters (eg. \"looooove\" turns into \"love\") to a minimum of 3 repeat characters Corpus statistics In order to extract some information from the corpus, we measured the relevance of words on each class. We calculated the tf-idf value of each term ignoring hashtags, stop- words, emojis and punctuation. We chose to report only the polarity classes (positive and negative) since the neutral class groups several characteristics (facts, out-of-topic sentences, confusing content) that led the analysis to terms not expressive, such as the name of the shows, users nicknames and neutral verbs (present, watch,...). The terms indicated in Table 3 show a notable semantic orientation represented in the classes -the positive class shows the verb \"to love\" and positive adjectives, while the negative class shows adjectives (the word \"trash\" is popular used as adjective on Twitter) and the verb \"to remove\" that may indicate a request for removing a guest from a show or even a participant from a reality show. Polarity classification task In order to evaluate the corpus on the polarity classification task we used six machine learning classifiers -a linear SVM (C: 1), a Bernoulli Naive Bayes (alpha:0.1), Logistic Regression, a Multilayer Perceptron (2 layers, 200 neurons, learning-rate:00.1), a Decision Tree classifier and Random Forest approach with 200 estimators. For data representation we used a bag-of-words with occurrence of terms, presence of negation words (\"not\", \"never\",...), positive and negative emoticons, emojis, presence of positive and negative words and PoS tags. We used the lexicons presented in Avanc \u00b8o et al. ( 2016 ) for negation words, positive/negative emoticons and words. For PoS tagging we used NLPnet tagger (Fonseca et al., 2015) and Emoji Sentiment Ranking (Novak et al., 2015) for emoji polarity probability. Discussion and future work TweetSentBR is a manually annotated corpus designed for polarity classification. The corpus was formed using a novel domain for the Brazilian Portuguese language that can be exploited by new machine learning approaches such as deep learning architectures and ensembles. It also offers new resources for linguistic approaches on natural language by observing the expressions, social media behavior or hate speech detection. The doubt label, for example, can be used for a better evaluation of classifiers by comparing machine learning flaws with human uncertainty on labeling data. This corpus also differs from other approaches by including the neutral class. The addition of the neutral class approximates the corpora to popular applications, since the polarity classifiers available in the industry must find solutions for separating the opinions of users from noisy data. ReLi (Freitas et al., 2012) and Computer-BR (Moraes et al., 2016) are the only corpora we found in the literature that describes the use of a neutral class on the annotation. It also motivates new research on features for better describing the neutral polarity space in classification, since the results achieved when removing the class are improved in almost 20% using Multilayer Perceptron with features used in the literature. We believe this corpus can still be improved by labeling more data manually. This could improve classification by reducing the unbalance in class distribution, which could help classifiers to achieve better results. We are currently working on a semi-supervised approach for automatically expanding the corpus based on selftraining and co-training (da Silva et al., 2016) . We beleieve this can improve the size of the corpus with few human effort and could also be applied for different domains and tasks in the future. Acknowledgement We acknowledge financial support from CNPq and CAPES as well as the help of Amanda Carneiro, Fernando N\u00f3brega, Juliana Batista, Rafael Anchi\u00eata and Thales Bertaglia for the volunteer annotation task of TTsBR, as well as Marcos Treviso for the development of the annotation interface. Bibliographical References Avanc \u00b8o, L. V., Brum, H. B., and Nunes, M. (2016) . Improving opinion classifiers by combining different",
    "abstract": "The large amount of data available in social media, forums and websites motivates researches in several areas of Natural Language Processing, such as sentiment analysis. The popularity of the area due to its subjective and semantic characteristics motivates research on novel methods and approaches for classification. Hence, there is a high demand for datasets on different domains and different languages. This paper introduces TweetSentBR, a sentiment corpus for Brazilian Portuguese manually annotated with 15.000 sentences on TV show domain. The sentences were labeled in three classes (positive, neutral and negative) by seven annotators, following literature guidelines for ensuring reliability on the annotation. We also ran baseline experiments on polarity classification using six machine learning classifiers, reaching 80.38% on F-Measure in binary classification and 64.87% when including the neutral class. We also performed experiments in similar datasets for polarity classification task in comparison to this corpus.",
    "countries": [
        "Brazil"
    ],
    "languages": [
        "Portuguese",
        "English"
    ],
    "numcitedby": "19",
    "year": "2018",
    "month": "May",
    "title": "Building a Sentiment Corpus of Tweets in {B}razilian {P}ortuguese"
}