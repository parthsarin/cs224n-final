{
    "article": "It is necessary to have a (large) annotated corpus to build a statistical parser. Acquisition of such a corpus is costly and time-consuming. This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus. Sample selection for annotation is based upon \"representativeness\" and \"usefulness\". A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees. Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method. Introduction A prerequisite for building statistical parsers (Jelinek et al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction (Thompson et al., 1999) , text classification (McCallum and Nigam, 1998) and natural language parsing (Thompson et al., 1999; Hwa, 2000) , to name a few. The basic idea is to couple tightly knowledge acquisition, e.g., annotating sentences for parsing, with model-training, as opposed to treating them separately. In our setup, we assume that a small amount of annotated sentences is initially available, which is used to build a statistical parser. We also assume that there is a large corpus of unannotated sentences at our disposalthis corpus is called active training set. A batch of samples 1 is selected using algorithms developed here, and are annotated by human beings and are then added to training data to rebuild the model. The procedure is iterated until the model reaches a certain accuracy level. Our efforts are devoted to two aspects: first, we believe that the selected samples should reflect the underlying distribution of the training corpus. In other words, the selected samples need to be representative. To this end, a model-based structural distance is defined to quantify how \"far\" two sentences are apart, and with the help of this distance, the active training set is clustered so that we can define and compute the \"density\" of a sample; second, we propose and test several entropy-based measures to quantify the uncertainty of a sample in the active training set using an existing model, as it makes sense to ask human beings to annotate the portion of data for which the existing model is not doing well. Samples are selected from the clusters based on uncertainty scores. The rest of the paper is organized as follows. In Section 2, a structural distance is first defined based on the sequential representation of a parse tree. It is then straightforward to employ a k-means algorithm to cluster sentences in the active training set. Section 3 is devoted to confidence measures, where three uncertainty measures are proposed. Active learning results on the shallow semantic parser of an air travel dialog system are presented 1 A sample means a sentence in this paper. in Section 4. A summary of related work is given in Section 5. The paper closes with conclusions and future work. Sentence Distance and Clustering To characterize the \"representativeness\" of a sentence, we need to know how far two sentences are apart so that we can measure roughly how many similar sentences there are in the active training set. For our purpose, the distance ought to have the property that two sentences with similar structures have a small distance, even if they are lexically different. This leads us to define the distance between two sentences based on their parse trees, which are obtained by applying an existing model to the active training set. However, computing the distance of two parse trees requires a digression of how they are represented in our parser. Event Representation of Parse Trees A statistical parser computes \u00a2 \u00a4\u00a3 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 , the probability of a parse \u00a5 given a sentence \u00a9 . Since the space of the entire parses is too large and cannot be modeled directly, a parse tree \u00a5 is decomposed as a series of individual actions \"! #! \"! $ % '& . In the parser (Jelinek et al., 1994) we used in this study, this is accomplished through a bottomup-left-most (BULM) derivation. In the BULM derivation, there are three types of parse actions: tag, label and extension. There is a corresponding vocabulary for tag or label, and there are four extension directions: RIGHT, LEFT, UP and UNIQUE. If a child node is the only node under a label, the child node is said to extend UNIQUE to its parent node; if there are multiple children under a parent node, the left-most child is said to extend RIGHT to the parent node, the right-most child node is said to extend LEFT to the parent node, while all the other intermediate children are said to extend UP to their parent node. The BULM derivation can be best explained by an example in Figure 1 . (1, 3, 5, 7, 11, 13 ) -blue boxes, labels (9,15,17)-green underlines, extensions (2, 4, 6, 8, 10, 12, 14, 16 )-red parentheses. Numbers indicate the order of actions. The input sentence is fly from new york to boston. Numbers on its semantic parse tree indicate the order of parse actions while colors indicate types of actions: tags are numbered in blue boxes, extensions in red parentheses and labels in green underlines. For this example, the first action is tagging the first word fly given the sentence; the second action is extending the tag wd RIGHT, as the tag wd is the left-most child of the constituent S; and the third action is tagging the second word from given the sentence and the two proceeding actions, and so on and so forth. We define an event as a parse action together with its context. It is clear that the BULM derivation converts a parse tree into a unique sequence of parse events, and a valid event sequence corresponds to a unique parse tree. Therefore a parse tree can be equivalently represented by a sequence of events. Let ( $\u00a3 )\u00a9 0 be the set of tagging actions, 1 2\u00a3 )\u00a9 0 be the labeling actions and 3 4\u00a3 )\u00a9 be the extending actions of \u00a9 , and let 5 6\u00a3 be the sequence of actions ahead of the action , then \u00a2 \u00a4\u00a3 7\u00a5 \u00a8 \u00a7 \u00a9 0 can be rewritten as: \u00a2 \u00a4\u00a3 7\u00a5 \u00a8 \u00a7 \u00a9 0 98 % '& @ A CB \u00a2 \u00a4\u00a3 A \u00a7 \u00a9 ED A 7F HG 8 @ I P Q D CR G \u00a2 \u00a4\u00a3 \u00a7 \u00a9 5 6\u00a3 S @ T P U D CR G \u00a2 \u00a4\u00a3 )V W \u00a7 \u00a9 5 6\u00a3 )V \" S @ X YP `D aR G \u00a2 \u00a4\u00a3 )b \u00a7 \u00a9 5 6\u00a3 \u00a6b \" Y dc (1) Note that \u00a7 ( $\u00a3 e\u00a9 0 \" \u00a7 gf h \u00a7 1 i\u00a3 e\u00a9 0 \" \u00a7 pf h \u00a7 3 q\u00a3 )\u00a9 0 # \u00a7 r8 ts vu . The three models (1) can be trained using decision trees (Jelinek et al., 1994; Breiman et al., 1984) . Note that raw context space w yx \u00a9 5 6\u00a3 Y is too huge to store and manipulate efficiently. In our implementation, contexts are internally represented as bitstrings through a set of pre-designed questions. Answers of each question are represented as bitstrings. To support questions like \"what is the previous word (or tag, label, extension)?\", word, tag, label and extension vocabularies are all encoded as bitstrings. Words are encoded through an automatic clustering algorithm (Brown et al., 1992) while tags, labels and extensions are normally encoded using diagonal bits. An example can be found in (Luo et al., 2002) . In summary, a parse tree can be represented uniquely by a sequence of events, while each event can in turn be represented as a bitstring. With this in mind, we are now ready to define a structural distance for two sentences given an existing model. Sentence Distance Recall that it is assumed that there is a statistical parser trained with a small amount of annotated data. To infer structures of two sentences \u00a9 and \u00a9 , we use to decode \u00a9 and \u00a9 and get their most likely parse trees \u00a5 and \u00a5 . The distance between \u00a9 and \u00a9 , given , is defined as the distance between \u00a3 e\u00a9 \u00a5 and \u00a3 )\u00a9 \u00a5 , or: p \u00a3 )\u00a9 W \u00a9 08 x \u00a3 )\u00a9 \u00a5 \u00a3 e\u00a9 \u00a5 S 'c (2) To emphasize the dependency on , we denote the distance as \u00a3 )\u00a9 \u00a9 . Note that we assume here that \u00a9 and \u00a9 have similar \"true\" parses if they have similar structures under the current model . We have shown in Section 2.1 that a parse tree can be represented by a sequence of events, each of which can in turn be represented as bitstrings through answer- ing questions. Let 3 A 8 D SG A D G A \"! \"! #! D U G A be the sequence representation for \u00a3 e\u00a9 A \u00a5 A ( 8 ), where \" A 8 \u00a3 )5 D G A D G A , and 5 D G A is the context and D G A is the parsing action of the \u00a6d event of the parse tree \u00a5 A . We can define the distance between two sentences \u00a9 \u00a9 as p \u00a3 )\u00a9 W \u00a9 08 fe \u00a3 )\u00a9 \u00a5 \u00a3 )\u00a9 \u00a5 Sg 8 \u00a3 )3 3 (3) The distance between two sequences 3 and 3 is computed as the editing distance using dynamic programming (Rabiner and Juang, 1993) . We now describe the distance between two individual events. We take advantage of the fact that contexts w 5 D G A can be encoded as bitstrings, and define the distance between two contexts as the Hamming distance between their bitstring representations. We further define the distance between two parsing actions as follows: it is either h or a constant b if two parse actions are of the same type (recall there are three types of parsing actions: tag, label and extension), and infinity if different types. We choose b to be the number of bits in 5 D G A to emphasize the importance of parsing actions in distance computation. Formally, let i \u00a3 be the type of action , then \u00a3 ) D G D Cj G 08 lk m\u00a3 )5 D G 5 D Cj G 6f \u00a3 nD G nD Cj G (4) where k m\u00a3 )5 D G 5 D Cj G is the Hamming distance, and \u00a3 ED G Y nD Cj G o8 p q q q r q q q s h if D G 8 D Cj G b if Y( D G ) = Y( D Cj G ) s nD G ot 8 nD Cj G u if Y( D G t 8 Y( D Cj G ). (5) Computing the editing distance (3) requires dynamic programming and it is computationally extensive. To speed up computation, we can choose to ignore the difference in contexts, or in other words, (4) becomes \u00a3 \u00a6 D G D Cj G 08 vk w\u00a3 )5 D G 5 D Cj G vf \u00a3 nD G Y nD Cj G x \u00a3 ED G Y nD Cj G dc (6) The distance p \u00a3 ! C \"! makes it possible to characterize how dense a sentence is. Given a set of sentences y z8 w \u00a9 c Cc Cc \u00a9 { | , the density of sample \u00a9 A is defined as: } \u00a3 e\u00a9 A 08 B $A p \u00a3 )\u00a9 \u00a9 A c (7) That is, the sample density is defined as the inverse of its average distance to other samples. We also define the centroid 2 R of S as R 8 argmax R \u00a3 } \u00a3 )\u00a9 A Y dc (8) K-Means Clustering With the model-based distance measure defined above, we can use the K-means algorithm to cluster sentences. A sketch of the algorithm (Jelinek, 1997 ) is as follows. Let y 8 w \u00a9 \u00a9 c Cc c \u00a9 { be the set of sentences to be clustered. 1. Initialization. Partition w \u00a9 \u00a9 ' c Cc c \u00a9 { into k initial clusters ( \u00a88 c c Cc ). Let 08 h . 2. Find the centroid for each collection , that is: 8 argmin P ' | R P ' \u00a8 p \u00a3 e\u00a9 A 3. Re-partition w \u00a9 \u00a9 W c c Cc \u00a9 { into clusters 7 \u00a3 C 8 #! \"! \"! , where 7 8 zw \u00a9 A f p \u00a3 e\u00a9 A p \u00a3 e\u00a9 A d 5 t 8 m c 4. Let 08 f . Repeat Step 2 and Step 3 untill the algorithm converges (e.g., relative change of the total distortion is smaller than a threshold). For each iteration we need to compute: the distance between samples \u00a9 A and cluster centers , the pair-wise distances within each cluster. The basic operation here is to compute the distance between two sentences, which involves a dynamic programming process and is time-consuming. The complexity of this algorithm is, if we assume the N samples are uniformly distributed between the k clusters, approximately \u00a3 { r j f ~ , or \u00a3 { j when \u00a1 . In our experiments ~x \u00a3\u00a2 #h \u00a4 and x #h 'h , we need to call the dynamic programming routine \u00a3 H h \u00a5 \" times each iteration! To speed up, dynamic programming is constrained so that only the band surrounding the diagonal line (Rabiner and Juang, 1993) is allowed, and repeated sentences are stored as a unique copy with its count so that computation for the same sentence pair is never repeated. The latter is a quite effective for dialog systems as a sentence is often seen more than once in the training corpus. Uncertainty Measures Intuitively, we would like to select samples that the current model is not doing well. The current model's uncertainty about a sentence could be because similar sentences are under-represented in the (annotated) training set, or similar sentences are intrinsically difficult. We take advantage of the availability of parsing scores from the existing statistical parser and propose three entropybased uncertainty scores. Change of Entropy After decision trees are grown, we can compute the entropy of each leaf node \u00a6 as: k \u00a7 $8 A \u00a9\u00a8 \u00a7 S\u00a3 \u00a6 H p\u00aa \u00ab '\u00ac \u00a8 \u00a7 H\u00a3 \u00a6 H ( 10 ) where sums over either tag, label or extension vocabulary, and \u00a8 \u00a7 H\u00a3 7 H is simply { f D A G \u00ae { D G , where ~ \u00a7 H\u00a3 7 H is the count of in leaf node \u00a6 . The model entropy k is the weighted sum of k \u00af \u00a7 : k 8 \u00a7 ~ \u00a7 k \u00a7 (11) where ~ \u00a7 8 A ~ \u00a7 \u00a3 7 H . Note that k is the log probability of training events. After seeing an unlabeled sentence \u00a9 , we can decode it using the existing model and get its most probable parse \u00a5 . The tree \u00a5 can then be represented by a sequence of events, which can be \"poured\" down the grown trees, and the count ~ \u00a7 H\u00a3 7 H can be updated accordingly -denote the updated count as \u00b1\u00b0 \u00a7 \u00a3 \u00a6 H . A new model entropy k \u00b0can be computed based on \u00b1\u00b0 \u00a7 \u00a3 7 H , and the absolute difference, after it is normalized by the number of events s 6u in \u00a5 , is the change of entropy we are after: k \u00af\u00b2 \u00b38 \u00a7 k \u00b0E k \u00b3 \u00a7 s $u (12) It is worth pointing out that k \u00b2 is a \"local\" quantity in that the vast majority of ~\u00b0 \u00a7 \u00a3 7 H is equal to ~ \u00a7 H\u00a3 \u00a6 H , and thus we only have to visit leaf nodes where counts change. In other words, k \u00a4\u00b2 can be computed efficiently. k \u00b2 characterizes how a sentence \u00a9 \"surprises\" the existing model: if the addition of events due to \u00a9 changes a lot of w \u00a8 \u00a7 \u00a3 ! d , and consequently, k , the sentence is probably not well represented in the initial training set and k \u00b2 will be large. We would like to annotate these sentences. Sentence Entropy Now let us consider another measurement which seeks to address the intrinsic difficulty of a sentence. Intuitively, we can consider a sentence more difficult if there are potentially more parses. We calculate the entropy of the distribution over all candidate parses as the sentence entropy to measure the intrinsic ambiguity. Given a sentence \u00a9 , the existing model could generate the top \u00b4most likely parses w #\u00a5 A \u00b5 08 h \"! #! \"! y \u1e83 , each \u00a5 A having a probability \u00b6 A : \u00a9 \u00b8\u2022 \u00ba\u00b9 \u00bb\u00a5 A \u00b6 A f\u00bc \u00a7 \u00bd A CB (13) where \u00a5 A is the \u00a6d possible parse and \u00b6 A is its associated score. Without confusion, we drop \u00b6 A 's dependency on and define the sentence entropy as: k R 8 \u00bd A B \u00a8A \u00aa \u00ab '\u00ac \u00a8A (14) where: \u00a8A 8 \u00b6 A \u00bd B \u00b6 c (15) Word Entropy As we can imagine, a long sentence tends to have more possible parsing results not because it is difficult but simply because it is long. To counter this effect, we can normalize the sentence entropy by the length of sentence to calculate per word entropy of a sentence: k \u00af\u00be \u00bf8 k R 1 R (16) where 1 R is the number of words in \u00a9 .  This can be explained as follows: longer sentences tend to have more complex structures ( extension and labeling ) than shorter sentences. And the models for these complex structures are relatively less trained as compared with models for tagging. As a result, longer sentences would have higher change of entropy, in other words, larger impact on models. As explained above, longer sentences also have larger sentence entropy. After normalizing, this trend is reversed in word entropy. Experimental Results and Analysis All experiments are done with a shallow semantic parser (a.k.a. classer (Davies et al, 1999) ) of the natural language understanding part in DARPA Communicator (DARPA Communicator Website, 2000) . We built an initial model using 1000 sentences. We have 20951 unlabeled sentences for the active learner to select samples. An independent test set consists of 4254 sentences. A fixed batch size \u00c0 \u00c18 \u00c2 #h 'h is used through out our experiments. Exact match is used to compute the accuracy, i.e., the accuracy is the number of sentences whose decoding trees are exactly the same as human annotation divided by the number of sentences in the test set. The effectiveness of active learning is measured by comparing learning curves (i.e., test accuracy vs. number of training sentences ) of active learning and random selection. Sample Selection Schemes We experimented two basic sample selection algorithms. The first one is selecting samples based solely on uncertainty scores, while the second one clusters sentences, and then selects the most uncertain ones from each cluster. Uncertainty Only: at each active learning iteration, the most uncertain \u00c0 sentences are selected. The drawback of this selection method is that it risks selecting outliers because outliers are likely to get high uncertainty scores under the existing models. Figure 3 shows the test accuracy of this selection method against the number of samples selected from the active training set. Short sentences tends to have higher value of k \u00be while sentence-based uncertainty scores (in terms of k \u00b2 or k R ) are low. Since we use the sentences as the basic units, it is not surprising that k \u00be -based method performs poorly while the other two perform very well. Most Uncertain Per Cluster: In our implementation, we cluster the active training set so that  We expect that restricting sample selection to each cluster would fix the problem that k \u00be tends to be large for short sentences, as short sentences are likely to be in one cluster and long sentences will get a fair chance to be selected in other clusters. This is verified by the learning curves in Figure 4 . Indeed, k \u00be performs as well as k \u00a4\u00c3 most of the time. And all active learning algorithms perform better than random selection. Weighting Samples In the sample selection process we calculated the density of each sample. For those samples selected, we also have the knowledge of their correct annotations, which can be used to evalutate the model's performance on them. We exploit this knowledge and experiment two weighting schemes. Weight by Density: A sample with higher density should be assigned greater weights because the model can benefit more by learning from this sample as it has more neighbors. We calculate the density of a sample inside its cluster so we need to adjust the density by cluster size to avoid the unwanted bias toward small clusters. For cluster 8 \u00c4w \u00a9 A p \u00a7 % A B , the weight for sample \u00a9 j is proportional to \u00a7 h \u00a7 #\u00c5 } \u00a3 )\u00a9 j . Weight by Performance: The idea of weight by performance is to focus the model on its weakness when it knows about it. The model can test itself on its training set where the truth is known and assign greater weights to sentences it parses incorrectly. In our experiment, weights are updated as follows: the initial weight for a sentence is its count; and if the human annotation of a selected sentence differs from the current model output, its weight is multiplied by 'c AE . We did not experiment more complicated weighting scheme (like AdaBoost) since we only want to see if weighting has any effect on active learning result. Effect of Clustering Figure 7 compares the best learning curve using only uncertainty score(i.e., sentence entropy in Figure 3 ) to select samples with the best learning curve resulted from clustering and the word entropy k 4\u00be . It is clear that clustering results in a better learning curve. Summary Result Figure 8 shows the best active learning result compared with that of random selection. The learning curve for active learning is obtained using k \u00be as uncertainty measure and selected samples are weighted by density. Both active learning and random selection are run 40 times, each time selecting 100 samples. The horizontal line on the graph is the performance if all 20K sentences are used. It is remarkable to notice that active learning can use far less samples ( usually less than one third ) to achieve the same level of performance of random selection. And after only about 2800 sentences are selected, the active learning result becomes very close to the best possible accuracy. Previous Work While active learning has been studied extensively in the context of machine learning (Cohn et al., 1996 ; Freund et al., 1997) , and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995) , there are only a handful studies on natural language parsing (Thompson et al., 1999) and (Hwa, 2000; Hwa, 2001) . (Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but Hwa does not characterize the distribution of sample space. Knowing the distribution of sample space is important since uncertainty measure, if used alone for sample selection, will be likely to select outliers. (Stolcke, 1998 ) used an entropy-based criterion to reduce the size of backoff n-gram language models. The major contribution of this paper is that a modelbased distance measure is proposed and used in active learning. The distance measures structural difference of two sentences relative to an existing model. Similar idea is also exploited in (McCallum and Nigam, 1998) where authors use the divergence between the unigram word distributions of two documents to measure their difference. This distance enables us to cluster the active training set and a sample is then selected and weighted based on both its uncertainty score and its density. (Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. This is a different venue for reducing annotation work in that the current model output is directly used and no human annotation is assumed. (Luo et al., 1999; Luo, 2000) also aimed to making use of unlabeled data to improve statistical parsers by transforming model parameters. Conclusions and Future Work We have examined three entropy-based uncertainty scores to measure the \"usefulness\" of a sample to improving a statistical model. We also define a distance for sentences of natural languages. Based on this distance, we are able to quantify concepts such as sentence density and homogeneity of a corpus. Sentence clustering algorithms are also developed with the help of these concepts. Armed with uncertainty scores and sentence clusters, we have developed sample selection algorithms which has achieved significant savings in terms of labeling cost: we have shown that we can use one-third of training data of random selection and reach the same level of parsing accuracy. While we have shown the importance of both confidence score and modeling the distribution of sample space, it is not clear whether or not it is the best way to combine or reconcile the two. It would be nice to have a single number to rank candidate sentences. We also want to test the algorithms developed here on other domains (e.g., Wall Street Journal corpus). Improving speed of sentence clustering is also worthwhile. Acknowledgments We thank Kishore Papineni and Todd Ward for many useful discussions. The anonymous reviewer's suggestions to improve the paper is greatly appreciated. This work is partially supported by DARPA under SPAWAR contract number N66001-99-2-8916.",
    "abstract": "It is necessary to have a (large) annotated corpus to build a statistical parser. Acquisition of such a corpus is costly and time-consuming. This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus. Sample selection for annotation is based upon \"representativeness\" and \"usefulness\". A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees. Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Thompson",
        "Juang"
    ],
    "numcitedby": "187",
    "year": "2002",
    "month": "July",
    "title": "Active Learning for Statistical Natural Language Parsing"
}