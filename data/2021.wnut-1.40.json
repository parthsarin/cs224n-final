{
    "article": "We explore the application of state-of-the-art NER algorithms to ASR-generated call center transcripts. Previous work in this domain focused on the use of a BiLSTM-CRF model which relied on Flair embeddings; however, such a model is unwieldy in terms of latency and memory consumption. In a production environment, end users require low-latency models which can be readily integrated into existing pipelines. To that end, we present two different models which can be utilized based on the latency and accuracy requirements of the user. First, we propose a set of models which utilize state-of-the-art Transformer language models (RoBERTa) to develop a high-accuracy NER system trained on a custom annotated set of call center transcripts. We then use our bestperforming Transformer-based model to label a large number of transcripts, which we use to pretrain a BiLSTM-CRF model and further fine-tune on our annotated dataset. We show that this model, while not as accurate as its Transformer-based counterpart, is highly effective in identifying items which require redaction for privacy law compliance. Further, we propose a new general annotation scheme for NER in the call-center environment. Introduction The recent promulgation of new privacy laws, such as the European Union's General Data Protection Regulation (GDPR) and the California Privacy Rights Act has drawn additional attention to the need for businesses to effectively identify and redact sensitive personally identifiable information (PII) in their business records. For the call center industry, which frequently collects transcripts of customer calls, this task is particularly tricky as transcripts tend to be an extremely noisy data source. This noise results from several factors, including errors in automated speech recognition (ASR), the use of single-channel audio (recordings that do not differentiate between users), and speech disfluencies. In this paper, we explore the use of a Named Entity Recognition (NER) task to identify those items in transcripts which should be redacted for privacy law compliance. Most existing NER models are trained on well-formed written text. Due to the noisy data in call center transcripts and domain language differences the use of such models in our task is not ideal. Additionally, although our task is quite similar to traditional NER, such as the CoNLL 2003 task (Sang and De Meulder, 2003) , the set of entities which we need to identify for privacy law compliance is significantly larger than that used in many NER tasks. For example, traditional NER label sets do not contain tags for entities such as telephone numbers, account numbers, or email addresses. To address this issue, we propose a new NER label set designed to meet the requirements of privacy law compliance in the call center industry. Our NER model also lends itself to uses such as identifying product mentions for call routing. Given the recent dominance of large pretrained Transformer language models such as BERT (Devlin et al., 2019) in NLP, we explore the application of Transformer LMs to the task of NER on call center transcripts. We demonstrate that fine-tuning these models on unlabeled, in-domain data, then further fine-tuning on a custom annotated set of call center transcripts, results an effective model for entity identification. Further, we explore the use of these models to label a large number of transcripts, which we then use as training data for a smaller BiLSTM-CRF sequence prediction model. This model, while not as accurate as its transformerbased brethren, can allow for less computationally expensive inference. The key contributions of this paper are demonstrating the efficacy of Transformer LM-based NER models for use in the call center domain and the use of said models to generate training data for smaller models which may be better suited to production environments. We demonstrate that a BiLSTM-CRF using Flair (Akbik et al., 2018) contextual string embeddings is approximately 7 times slower than our best-performing BiLSTM-CRF model using fixed pretrained subword embeddings, with only marginal F1-score improvement. Additionally, the Flair-based LSTM model (0.73 micro-F1) is 3 times slower than our best-performing RoBERTabased model (0.82 micro-F1) and 7 times slower than a DistilRoBERTa model (0.81 micro-F1). Previous work NER has long been a popular task in the NLP community, and many public benchmark datasets exist to test the performance of various sequence labeling algorithms on the NER task. Initially developed as part of the Message Understanding Conferences (Grishman and Sundheim, 1996) , the task was further popularized by the CoNLL 2003 shared task on NER (Sang and De Meulder, 2003) . The CoNLL 2003 dataset and the OntoNotes corpus (Hovy et al., 2006) remain popular benchmarks for sequence labeling algorithms. The Conditional Random Field (Lafferty et al., 2001) was, and remains, a popular method for sequence labeling, including NER (Konkol and Konop\u00edk, 2013) . Huang et al. (2015) achieved success in the NER task by combining the CRF with the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997) . Similarly, Ma and Hovy (2016) achieved state-of-the-art NER results with a BiLSTM-CNN-CRF model. More recently, with the advent of pretrained Transformer language models such a BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) , attention has shifted to using these models for the NER task. Devlin et al. (2019) use an NER sequence labeling task as an example of the power of BERT. Additionally, Souza et al. (2019) combine BERT with a CRF layer to achieve impressive results for NER in Portuguese. In this paper, we utilize both Transformer-based NER and BiLSTM-CRF models for NER on a custom annotated dataset of call center transcripts. The use of automated methods for text anonymization has a long history in both NLP and the biomedical domain. For example, Chen and Kan (2013) In addition to our labeled data, we use a larger set of 99,796 unlabeled call transcripts for domain adaptation of our pretrained language models, as well as for generation of synthetically labeled training data. Annotation We employed two experienced annotators to annotate our data for entities based on a detailed annotation schema developed for this task (see Table 1 ). Annotation was completed using the BRAT annotation tool 1 . Annotation was an iterative process, in which we first had the two annotators label a small, identical set of transcripts, from which we then calculated inter-annotator agreement. We also reviewed the annotation guidelines with the annotators and updated the guidelines based on their comments. Once interannotator agreement reached a Krippendorff's \u03b1 of 0.80 (as suggested by Carletta ( 2008 )), we proceeded with annotation of the larger dataset. We had the annotators each label 711 unique transcripts, as well as an overlapping set of 50 transcripts, on which we calculated our final interannotator agreement score. Our final interannotator agreement was \u03b1 of 0.875, indicating a high level of agreement between our annotators and a well-defined annotation task. In addition to the NER entities required for redaction of PII, such as the set used by Kaplan (2020) To better understand which categories were particularly challenging for annotators, we calculated the F1 score between annotators on the agreement set, with one annotator being set as ground-truth (see Table 1 ). The reported F1 scores are based the evaluation scheme developed for the SemEval 2013 Task 9 (Segura Bedmar et al., 2013) . As mentioned in Kaplan (2020) , email addresses proved particularly challenging for our annotators, as the ASR language model struggles to intelligibly decipher email addresses; this results in strange forms such as \"XTD a ballgode dot com.\" 2 As a result, it can be challenging for annotators to identify which words belong to the email address, as demonstrated by the low F1 score reported in Table 1 . The fact that there is no difference between strict and partial-overlap F1 scores indicates that annotators are struggling not only to identify the span boundaries of email addresses, but also to identify where email addresses exist in the text. By contrast, for entities like Phone, ID, and Account, the annotators generally agree on the entity type to be assigned, but often disagree on 2 All potential PII in text and appendix have been replaced with fabricated data to replicate real examples the boundaries, resulting in a large difference between the strict and partial-match F1 scores. This disagreement may be due to the fact that our data is not speaker-differentiated. Models We propose two classes of model trained on our annotated dataset. The first class applies state-of-theart Transformer LMs to the NER task, while the second class uses a BiLSTM-CRF architecture. Our initial transformer model harnessed BERT, however early testing indicated that RoBERTa generally performed better for our target task. As our goal was to create an efficient model for production use, we chose to not explore model ensembling, though this is a potential direction for future work. Our Transformer-based models were built using Huggingface's Tranformers package 3 for PyTorch. Our best performing model uses a roberta-base pretrained model to encode input token sequences, which are then passed to a linear layer for dimension reduction, and finally to a CRF layer (implemented using PyTorch CRF 4 ) to make final label predictions. Our BiLSTM-CRF model, which is implemented in PyTorch, consists of a 4-layer bidirectional LSTM encoder with 300 hidden units per layer. As with our Transformer model described above, encoder output is passed to a linear layer prior be being passed to a CRF layer for final prediction. Input to the model consists of 100dimension pretrained byte-pair subword embeddings generated using BPEmb (Heinzerling and Strube, 2018) . We also tested a BiLSTM-CRF model which uses Flair embeddings (Akbik et al., 2018) as input, similar to the model proposed in Kaplan (2020) . Experiments To directly compare the performance of our various models, we train each model on our final annotated training dataset and evaluate on our held-out test set. Additionally, we analyze the perfomance of our BiLSTM-CRFs relative to the Transformer model, as it is significantly faster and requires fewer computational resources. We test two different pretrained models from the Transformers library: RoBERTa (Liu et al., 2019) , and DistilRoBERTa (Sanh et al., 2019) . All models are fine-tuned on our annotated training data for 6 epochs using the AdamW (Loshchilov and Hutter, 2018) optimizer, with an initial learning rate of 5e\u22125 and epsilon of 1e\u22128. We use linear warmup to reach the initial learning rate over the first training epoch. We select the model checkpoint with the best validation micro-F1 for further evaluation. We also train each model with and without a sliding context window of 99 tokens on either side of the target input, as described in Luoma and Pyysalo (2020) . The size of the context window was determined by testing various context window sizes; we found 99 tokens to provide the most improvement in model performance. Next, we test the performance of our BiLSTM model, which we train with two types of input token embeddings as described in Section 4. As with the Transformer models, we use the AdamW optmizer with an initial learning rate of 5e \u2212 5 and epsilon of 1e \u2212 8. We also experimented with the Adam optimizer with a LR of 0.001, with nearly identical results. We implement early stopping on validation F1 with a patience of 6. Our bestperforming model triggered early stopping after 17 epochs. Given that our LSTM models are significantly faster than our Transformer models on CPU, we wished to test an LSTM model trained on additional RoBERTa-labeled data. We therefore use our best-performing RoBERTa model to label 20,000 unlabeled transcripts, which we then use to train a BiLSTM-CRF model. improving, we further fine-tune the model on our annotated dataset. The resulting model gains 3 F1 points over our other LSTM models, and reaches an F1 within 5 points of our DistilRoBERTa model. All model training is conducted using an NVIDIA GeForce GTX 1080 GPU with 8 GB of memory. Inference is performed on both GPU and CPU to test inference latency. F1 and total test runtime of each of our best-performing models are shown in Table 2 . All F1 scores reported for model performance use \"strict\" evaluation in which both entity type and span boundaries must match. Additionally, in Figure 1 we provide a confusion matrix of token-level classification using our RoBERTa model. Discussion Our experiments demonstrate that pretrained Transformer language models, such as RoBERTa, can lead to significant performance improvements for NER labeling of call center transcripts when compared to previously published methods. While this is not terribly surprising, given the number of NLP tasks in which Transformer LMs have come to dominate, this paper is the first application of said models to the domain of call transcripts. The only work which has focused on NER for call center transcripts, Kaplan (2020) used a BiLSTM-CRF model only; while we are unable to replicate their work due to the unavailability of their data 5 , our experiments using a similar model design show that our proposed Transformer models achieve improved performance on this task. Our LSTM model uses pretrained, rather than custom, Flair embed- As previously mentioned, call center transcripts are a particularly noisy type of input data; noise sources include disfluencies in spoken language, poor audio quality, and ASR errors. Given this fact, it can be challenging for both annotators and a model to correctly identify entity boundaries. As previously mentioned, all F1 scores reported for model performance use \"strict\" evaluation. However, such precision is not necessarily required for a model to be effective for redaction of PII. For example, if only the majority of an Email is captured by the model, the resulting redaction may still be sufficient. Of note is the fact that our best performing model achieves an \"entity type\" F1 score of 0.90; this metric requires that entity types be labeled correctly, but that span boundaries need only overlap. This score indicates that the model is correctly labeling the large majority of spans, even if its predicted boundaries do not exactly match those of the annotator. Conclusion This paper presents the first application of pretrained Transformer language models to NER for the call transcript domain. This is a unique domain in many ways, particularly due to the large amounts of noise present in the data. We demonstrate that Transformer LMs, namely RoBERTa and Distil-RoBERTa, outperform the previous state of the art model for this domain on our custom dataset. Additionally, we show that these models are significantly faster to run than the previous state of the art model. Finally, we show that a Transformer LM model can be used to generate training data for a smaller, less computationally greedy LSTM model, resulting in substantial performance improvement for the smaller model. This finding provides additional choice to end-users wishing to develop an NER system for production use in a call center environment, and potential future directions for improving model efficiency and performance.  Well I'm t trying to see if they gave me the credit that they were supposed to give me the store. Okay and I know you're calling for you to actually funded your account today. A.4 Additional input data examples Example 1 For your convenience if you would prefer to prepay please have your credit card ready at the end of the call . They were calling police sugar . This is Melissa . I got a phone I'm burning In 2 9 8 0 that I got a call back . Thank you . This is for donor correct . Donna Yes ma'am . Donna thank you . What can I get started today . Three orders or three equals . I'll tell him in increments of three . All right . The Vietnamese chicken salad rolls are like the pork egg rolls . The pork barrel Yes we can do either . If you want three we can do three of them 4 7 and 9 I couldn't remember the increment oh . sorry . It's 1 8 7 2 4 or 9 Yes to . leave OK . . Already And . that's Anything it . else . Naomi I so just so just make sure we've got one order of the two count pork egg rolls Yes so ma'am . you can about ten minutes and then you're picking up over from the Wilsonville location Yes . Thank you wonderful . You're welcome . I'll see you soon . Bye . yes . Example 2 I think clean food pickup company here Lake Woods mines Ramona . Can I I get your hear last name your please voice is . breaking up . Can you hear me . I can hear you Okay . I'd like to order a large chicken noodle soup to go OK . I just need a first and last name for the order . Stacy is the first name and last name is Art VERONA Thank you Amanda . I get the last of your phone number 3 6 7 2 is that correct . Perfect . And you said you needed a large chicken noodle soup . OK . That's right . How soon could you have that right . OK . Well can I tempt you with some cheesy garlic bread today Not just this fine all right . So I have here for the order just the regular chicken noodle soup . Correct . Perfect . So your total is six dollars and sixty six cents and then we'll be ready in all ten right minutes here in Court Street . OK . okay great . Thank you . You're welcome Stacy you and Jules on have a great day . Example 3 Crowd services . My name is Leo . Your first and last name please . To blue . Thank you so much for the information Ms Blue . . And good evening to you ma'am . Can you verify your phone number on file please . 0 0 0 0 0 0 0 00. Thank you . Do you have your stamps Credit Card right now on blue screen . I do . Can you help me with the expiration date . Money . Thank you . And do you have a twelve user on the card ma'am aside from you . Are you on the credit card account . William Bell come . Do you remember how much was the last payment that you made on the credit card . Hold on let me look at my notes . Why am I going through this . I don't know but . This is verification process blue just to make sure that we're speaking with the correct person on the account . I paid a twenty dollars the confirmation numbers 0 0 0 0 0 0 0 0 . OK . Thank you . And do you still remember where did you last use your credit card man . Oh my God . I went to yesterday I went to a hair store . OK lastly you kindly verify your complete mailing address please . What do you say Jones . Street Akron Ohio 6 6 4 2 1 . OK . My dream is blue . The system is also Bront\u00eb me though we need to update her account for additional security . It's either we save your mother's maiden name or a five digit passcode as additional security question . Which six do you prefer . My mother's maiden name . Can you help me with your mom's maiden name . Clayton . Ma'am I need the three digits on the back of your card please . 0 0 0 . Okay thank you so much . Just give me two moment here . Thank you for patiently waiting . There you go information has been completely updated . Miss Blue many help you with your account . I was trying to find out the balance and didn't make a payment but it went straight to a payment and I wanted some clarity first . Mm hmm . It didn't Okay communicate . . Okay . We apologize for the inconvenience ma'am . If you wanted to check do you wanted to check the statement balance . Ma'am that is due on the current due date or the whole balance including recent charges . Whole balance . Member Current balance is four thousand two hundred twelve and 28 cents . four thousand . Say it again . four thousand two hundred twelve to 2 3 5 . And And twenty my credit and twenty eight . cents man . My credit limit was worth . Six thousand . Okay can I make a four hundred dollar payment . Yes of course ma'am . Since you're having issues with the automated system let me just help you with the payment . Just one time free of charge for tonight ma'am . Okay . Because customer service normally charge $10 for over the phone payment . Just give me one moment here . Are you going to use checking account for your payment then . Okay . And you did mention that you're paying eight fourteen dollars . Correct . eight . Okay . We got a checking account ending in 0 0 0 0 0 in 8 minutes . Just the one you will be using . Okay . Let me go . I made a $400 payment using your checking account ending insert a 0 0 0 . Again $10 if he has been waived . So there you go . Miss Blue you will are if you have questions or reached the modified as payment please call us at the number on your statement . This confirms that today March 9 two thousand twenty you authorize us to make a single that a Chinese debit of seven eighty Yeah dollars from your bank to pay your West Credit Card account on March 9 . two nineteen twenty . Is this correct . Thank you . Payment will successfully process and it will be dated today and it will post within 13 hours under your account for other payment options . You can pay at any local West store truly automated system Ensure . the website . All of them are for free . Do you need your confirmation number . Can you write this one down please . It's going to be 0 0 0 0 0 0 0 0 0 to . Okay . There you go . Would there be anything else that I could further check man . I just wanted to make sure they haven't missed any payments on my account of . He was a very My credit . with my account is in good standing . That's in good standing . No need to worry ma'am . That is correct . Thank you . You're welcome As . green . There are no more concerns ma'am . I'm glad I was able to assist And . thank you for calling card services . You ever get one ma'am . We value your business . ",
    "abstract": "We explore the application of state-of-the-art NER algorithms to ASR-generated call center transcripts. Previous work in this domain focused on the use of a BiLSTM-CRF model which relied on Flair embeddings; however, such a model is unwieldy in terms of latency and memory consumption. In a production environment, end users require low-latency models which can be readily integrated into existing pipelines. To that end, we present two different models which can be utilized based on the latency and accuracy requirements of the user. First, we propose a set of models which utilize state-of-the-art Transformer language models (RoBERTa) to develop a high-accuracy NER system trained on a custom annotated set of call center transcripts. We then use our bestperforming Transformer-based model to label a large number of transcripts, which we use to pretrain a BiLSTM-CRF model and further fine-tune on our annotated dataset. We show that this model, while not as accurate as its Transformer-based counterpart, is highly effective in identifying items which require redaction for privacy law compliance. Further, we propose a new general annotation scheme for NER in the call-center environment.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Portuguese"
    ],
    "numcitedby": "0",
    "year": "2021",
    "month": "November",
    "title": "Improved Named Entity Recognition for Noisy Call Center Transcripts"
}