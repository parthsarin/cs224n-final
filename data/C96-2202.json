{
    "article": "Unknown words are inevitable at any step of analysis in natural language processing. Wc propose a method to extract words from a corl)us and estimate the probability that each word belongs to given parts of speech (POSs), using a distributional analysis. Our experiments have shown that this method is etfective for inferring the POS of unknown words. Introduction Dictionaries are indispensable in NLP in order to determine the grammatical functions and meanings of words, but the coutinuous increase of new words and technical terms make unknown words an ongoing problem. A good <teal of research has been directed to finding efficient and effective ways of expanding the lexicon. With agglutinative languages like Japanese, tile problem is even greater, since even word boundaries are ambiguous. To solve these problems, we propose a method that uses distributional analysis to extract words from a corpus and estimate the probability distribution of their usc a.s ditferent parts of speech. Distributional analysis was originally proposed by Harris (1951) , a structural linguist, tLs a technique to uncover the structure of a language. Llarris intended it ~Ls a substitute for what he perceived ~ts unscientific information-gathering by linguists doing field work at that time. Thus, linguists determine whether two words belong to the same class by observing the environments in which the words occur. Recently, this technique has been mathematically refined and used to discover phrase structure from a corpus annotated with POS tags (Brill and Marcus, 1992; Mori and Nagao, 1995) . Schiltze (1995) used the technique to induce POSs. However, in these researches, the problem of catcgorial ambiguity (the fact that some words or POS sequences can belong to more titan one category), has been ignored. In this paper, we propose a method that assumes that a word may belong to more than one POS, and provides estimates of the relative i)rol)ability that it may belong to each of a number of POSs. Our method decomposes an observed probability distribution into a particular linear summation of a given set of model l)robability distributions. '1't1\u00a2.' resulting set of coefficients rcl)rcsents tim probability that the observed event belongs to each model event. The application d:lscussed here is word extraction fron] a Japanese corpus. First we calculate the model probability distribution of each POS by observing the context of each occurrence in a tagged corpus. Then, for each unknown word, we similarly calculate its environment by collecting all occurren('es from a raw corpus. Finally, wc coml)ute tile probability distribution of POSs for a word by comparing its observed environment with tile model environments represented by the set of POS distributions. 1,1 subsequent sections, first we discuss the hypothesis, secondly describe the algorithm, thirdly present results of the exl)eriments on the ED]{ corpus and journal articles, and finally conclude this rcsearch. Hypothesis In this section, first we define environment of a string occurring ix, a corpus. Next, we prol)ose a hypothesis which gives foundation to our word extraction method. Environment of a String in a Corpus We detine tile \"environment\" of a type (character string, group of morl)hemes , or as tile prol)ability distribution of the elements preceding and follow\u00b0 ing occurrences of that type in a corpus. The elements which precede tile type a.re described by the left probability distribution, and those which follow it, by the right probability distribution. For instance, Table 1 shows the one-character envio romnent of the string \".~-L\" in the I~DR corpus (Jap, 1993) . This string occurs 181 times, wittl 12 different characters appearing to its left and l0 to its right. In general, a probability distribution can be regarded a.s a vector, so the concatenatiori of two vectors is also a vector. Thus, the concatenation of the left and right probability distributions for a type is what we call the \"environment\" of that type, and we represent this by D in the subsequent part of this paper. Hypothesis Concerning Environment In general, if a string a is a word which belongs to a POS, it is expected that the environment D(a) of the string in a particular corpus will be similar to the environment D(pos) of that POS. Since a word can belong to more than one POS, it is expected that the environment of tire string will be similar to the summation across all POSs of the environment of each POS multiplied by tile probability that the string occurs ms that POS. Therefore, we obtain the following formula: D(et) ..~ Zp(posk[a)D(posk ) (1) k where p(poskla) is the probability that the string a belongs to posk, and D(posk) is tire environment of posk. In this formula, summation is calculated for the set of POSs in consideration. As an example, let us take the string \"~.1_.\", which is used in tile corpus only as a verb and an adjective. Ifp(Adjl'Z~l..) and p(Verbl~-b ) are the probabilities that a particular instance of the string is used as an adjective and a verb respectively, then the enviromnent of the string \"x~-I~\" is described by the tbllowing formula: D(-x~-U) p(Adjl~-L )D(Adj) + p( VerblX~ -L )D(Verb), In most cases, however, formula (1) cannot be solved as a linear equation, since the dimension of probability distribution vector D is greater than that of the independent variables. In addition, we need to minimize the effects of sample bias inherent in statistical estimates of this sort. We therefore reason that the question is to find the set of p(posk let) which minimizes the difference between both sides of formula (1) in terms of some measure. We use, as this measure, the square of Euclidean distance betwen vectors. Then it follows that the problem is formalized as an optimization problem (minimize). The decision variables are the elements of tile probability distribution vector p which expresses tile likelihood that the string is used as each POS: F(p) = ]D(et) -ZpkD(posk)l 2 (2) k where p = (Pl,P2,...,P,~), Pk = p(poskl o~) and n is tile number of POSs in consideration. Since each element ofp represents a probability, the feasible region V is given as follows: V={plO<_pk<_l,~p~=l} (3/ k The minimum value of F(p) will be relatively small when tile environment of the string can be decomposed into a linear summation of some POS environments, while it will be relatively large when such a decomposition does not exist. Since all true words must belong to one or more POSs, the minimum value of F(p) can be used to decide whether a string is a word or not. We call this value the \"word measure,\" and accept as words all strings with word measure Less than a certain threshold. Algorithm In this section we describe the algorithm used to calcnlate tile word rneasure of all arbitrary string and tire probabilities that the string belongs to each of a set of POSs. We used observations from tile EDI{ corpus, which is divided into words and tagged as to POS, to calculate tile POS environments, and then used a raw corpus (no indication of word or morpheme boundaries, and no POS tags) for calculating the string environments. Calculating POS Environments The environment of each POS is obtained by calculating statistics on all contexts that precede and follow the POS in a tagged corpus, as follows: 1. Let all elements of left and right probability vectors be 0. 2. For each occurrence of the POS in the corpus, iucrement the left vector elenmnt corresponding to the context preceding this occurrence of the POS, and increment the right vector element corresponding to the context following the POS. 3. Divide each vector element by the total number of o<:currences of the POS. Figure 1 shows a sample sentence from the EI)R corpus, and Table 2 shows the computation of the one-character environment of Noun in the tiny corpus consisting of this single sentence. In practice, instead of a single character, we used as contexts the preceding or following POStagged string (a morpheme or word). Thus the Calculating String Environments The cMculation of the enviromnent of an arbitrary string (possible word) in a corpus is basically identical to tire POS algorithm above, except that because Japanese has no blank space between words arr(t a raw (unsegmented) corpus is used, the extent of the environment is ambiguous. There are two ways to determine the extent of the left and right environment: one is to specify a fixed number of characters, and the other is to use a look-up-and-match procedure to identify specific morphenms. We adol)ted the second method, and used as a mort)henm lexicon the set of hash keys representing the POS envirouments. Where there was a conflict between two or more possible matches of a string context with tire POS hash keys, the longest match was selected. For instance, although a right context zi, ro 'kate' couht match either the postposition 'ka' or the postl)osilion 'kara', the longer match 'kara' would always be chosen. Optimization The environments for a string and for each POS which it represents become the parameters of the objective flmction defined I)y formula (2), and the optimization of this flmction then yields the probabilities that the string belongs to each l)OS. The. problem can be solved e~sily by the optimal gradient method because both the objective function and the feasible region are convex. Results We conducted two experiments, in each using a range of different thresholds for word measure. One experiment used the El)]{ corl)us a.s a raw corpus (ignoring the POS tags) in order to cal-('ulate recall and precision. The other experiment  POS environments were defined as one POStagged string (assumed to be one morpheme), and were limited to strings made up only of h*ragana characters plus comma and period. The aim of this limitation was to reduce computational time during inatehing, and it was ['ell, that morl)hemes using kanji and katakana characters are too infrequent ~s contexts to exert much intluence on the results. Candidate for unknown words were limited to strings of two or more characters appearing in the corpus at least ten times and not containing any symbols such as parentheses. Since there are very few unknown words which consist of only one character, this limitation will not have much effect on the recall. Experiment 1: Word Extraction For evaluation purposes, we conducted a word extraction ext)eriment using the El)l{. corpus as a raw corpus, and calculated recall and precision ['or each threshold value (see Table 3 ). First, we calculated f'mi,~and p for all character n-grams, Recall was computed as the percent, of all POStagged strings in the EDR corpus that were successfully identified by our algorithm as words and as belonging to the correct POS. In calculation of the recalls and the precisions, both POS and string is distinguished. Precision was calculated using the estimated frequency f((~,pos) = p(posl~ ) .f(tx) where f(,x)is the frequency of the string ~t in the corpus, and p(poslot) is the estimated probability that ct belongs to the pos. Judgement whether the string ~ belongs to pos or not was made by hand. The recalls are calculated for ones with the estimated probability more than or equal to 0.1. The reason for this is that the amount of the output is too enormous to check by hand. For the same reason we did not calculate the precisions for thresholds more than 0.25 in Table 3 . This table tells us that the lower the threshold is, the higher the precision is. This result is consistent with the result derived from the hypothesis that we described in section 2.2. Besides, there is a tendency that in proportion ,as the frequency increases the precision rises. Experiment 2: Improvement of Stochastic Tagging In order to test how much the accuracy of a tagger could be improved by adding extracted words to its dictionary, we developed a tagger based on a simple Markov model and analyzed one journal article 1. Using statistical parameters estimated from the EDR corpus, and an unknown word model based on character set heuristics (any kauji sequence is a noun, etc.), tagging accuracy was 95.9% (the percent of output morphemes which were correctly segmented and tagged). Next, we extracted words from the Japanese version of Scientific American (1990; 617,837 characters) using a threshold of 0.25. Unknown words were considered to he those which could not be divided into morphemes appearing in the learning corpus of the Markov model. Table 4 shows examples of extracted words, with unknown words a\"Progress in Gallium Arsenide Semiconductors\" (Scientific American; February, 1990) starred. Notice that some extracted words consist of more than one type of character, such as \"3~ :/ -'~'~ (protein).\" This is one of the advantages of our method over heuristics based on character type, which can never recognize mixed-character words. Another advantage is that our method is applicable to words belonging to more than one POS. For example, in Table 4 \"Et;~ (nature)\" is both a noun and the stem of a na-type adjective. We added the extracted unknown words to the dictionary of the stochastic tagger, where they are recorded with a frequency calculated by the following fo,'mula: (size~/size,)f(c~,pos), where size~ and size, are the size of the EDR corpus and the size of the Scientific A merican corpus respectively. Using this expanded dictionary, the tagger's accuracy improved to 98.2%. This result tells us that our method is useful as a preprocessor for a tagger. Conclusion We have described a new method to extract words from a corpus and estimate their POSs using distributional analysis. Our method is based on the hypothesis that sets of strings preceding or following two arbitrary words belonging to the same POS are similar to each other. We have proposed a mathematically well-founded method to compute probability distrii)ution in which a string belongs to given POSs. The results of word extraction experiments attested the correctness of our hypothesis. Adding extracted words to the dictionary, the accuracy of a morphological analyzer augmented considerably.",
    "abstract": "Unknown words are inevitable at any step of analysis in natural language processing. Wc propose a method to extract words from a corl)us and estimate the probability that each word belongs to given parts of speech (POSs), using a distributional analysis. Our experiments have shown that this method is etfective for inferring the POS of unknown words.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "32",
    "year": "1996",
    "month": "",
    "title": "Word Extraction from Corpora and Its Part-of-Speech Estimation Using Distributional Analysis"
}