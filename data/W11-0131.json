{
    "article": "Distributed models of semantics assume that word meanings can be discovered from \"the company they keep.\" Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a document. In contrast, this paper proposes a structured vectorial semantic framework, in which semantic vectors are defined and composed in syntactic context. As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse. Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy. Introduction Distributed semantic representations like Latent Semantic Analysis (Deerwester et al., 1990) , probabilistic LSA (Hofmann, 2001) , Latent Dirichlet Allocation (Blei et al., 2003) , or relational clustering (Taskar et al., 2001) have garnered widespread interest because of their ability to quantitatively capture 'gist' semantic content. Two modeling assumptions underlie most of these models. First, the typical assumption is that words in the same document are an unstructured bag of words. This means that word order and syntactic structure are ignored in the resulting vectorial representations of meaning, and the only relevant relationship between words is the 'same-document' relationship. Second, these semantic models are not compositional in and of themselves. They require some external process to aggregate the meaning representations of words to form phrasal or sentential meaning; at best, they can jointly represent whole strings of words without the internal relationships. This paper introduces structured vectorial semantics (SVS) as a principled response to these weaknesses of vector space models. In this framework, the syntax-semantics interface is fully interactive: semantic vectors exist in syntactic context, and any composition of semantic vectors necessarily produces a hypothetical syntactic parse. Since semantic information is used in syntactic disambiguation (MacDonald et al., 1994) , we would expect practical improvements in parsing accuracy by accounting for the interactive interpretation process. Others have incorporated syntactic information with vector-space semantics, challenging the bagof-words assumption. Syntax and semantics may be jointly generated with Bayesian methods (Griffiths et al., 2005) ; syntactic structure may be coupled to the basis elements of a semantic space (Pad\u00f3 and Lapata, 2007) ; clustered semantics may be used as a pre-processing step (Koo et al., 2008) ; or, semantics may be learned in some defined syntactic context (Lin, 1998) . These techniques are interactive, but their semantic models are not syntactically compositional (Frege, 1892) . SVS is a generative model of sentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but is inherently compositional. Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f (u, v, R, K) (1) where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor). In this initial work of theirs, they leave out any notion of syntactic context, focusing on additive and multiplicative vector composition (with some variations): Add: p[i] = u[i] + v[i] Mult: p[i] = u[i] \u22c5 v[i] (2) Since the structured vectorial semantics proposed here may be viewed within this framework, our discussion will begin from their definition in Section 2.1. Erk and Pad\u00f3's (2008) model also fits inside Mitchell and Lapata's framework, and like SVS, it includes syntactic context. Their semantic vectors use syntactic information as relations between multiple vectors in arriving at a final meaning representation. The emphasis, however, is on selectional preferences of individual words; resulting representations are similar to word-sense disambiguation output, and do not construct phrase-level meaning from word meaning. Mitchell and Lapata's more recent work (2009) combines syntactic parses with distributional semantics; but the underlying compositional model requires (as other existing models would) an interpolation of the vector composition results with a separate parser. It is thus not fully interactive. Though the proposed structured vectorial semantics may be defined within Equation 1 , the end output necessarily includes not only a semantic vector, but a full parse hypothesis. This slightly shifts the focus from the semantically-centered Equation 1 to an accounting of meaning that is necessarily interactive (between syntax and semantics); vector composition and parsing are then twin lenses by which the process may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation is composed during decoding. Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing. The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006; Gesmundo et al., 2009) ). Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (Klein and Manning, 2003) , syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracyit will only show parsing gains where semantic information does not overlap with distributional syntactic information. Evaluations show that interactively considering semantic information with syntax has the predicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity. The remainder of this paper is organized as follows: Section 2 describes SVS as both vector composition and parsing; Section 3 shows how relational-clustering SVS subsumes PCFG-LAs; and Section 4 evaluates modeling assumptions and empirical performance. Structured Vectorial Semantics Vector Composition We begin with some notation. This paper will use boldfaced uppercase letters to indicate matrices (e.g., L), boldfaced lowercase letters to indicate vectors (e.g., e), and no boldface to indicate any singlevalued variable (e.g. i). Indices of vectors and matrices will be associated with semantic concepts (e.g., i 1 , i 2 , . . .); variables over those indices are single-value (scalar) variables (e.g., i); the contents of vectors and matrices can be accessed by index (e.g., e[i 1 ] for a constant, e[i] for a variable). We will also define an operation d (\u22c5), which lists the elements of a column vector on the diagonal of a diagonal matrix, i.e., d (e)[i, i]=e [i] . Often, these variables will technically be functions with arguments written in parentheses, producing vectors or matrices (e.g., L(l) produces a matrix based on the value of l). As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now. We can rewrite their equation (Equation 1 ) in SVS notation by following several conventions. All semantic vectors have a fixed dimensionality and are denoted e; source vectors and the a) e\u01eb (l MOD )S e0 (l MOD )NP e00 (l MOD )DT the e01 (l ID )NN engineers pulled off ... b) e00 = headwords i u i t i p truth \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 1 .1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 e01 = headwords i u i t i p truth \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 0 1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 a T 0 = .1 .1 .8 L0\u00d700(l MOD ) = parent 0 i u i t i p child 00 i p i t i u \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 .2 .8 0 0 1 .1 .4 .5 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 M(l MOD \u2236NP \u2192 l MOD \u2236DT l ID \u2236NN) = parent 0 i u i t i p parent 0 i p i t i u \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 .2 0 0 0 .1 0 0 0 .4 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 Figure 1 : a) Syntax and semantics on a tree during decoding. Semantic vectors e are subscripted with the node's address. Relations l and syntactic categories c are constants for the example. b) Example vectors and matrices needed for the composition of a vector at address 0 (Section 2.2.1). target vector are differentiated by subscript; instead of context variables R and K we will use M and L: e \u03b3 = f (e \u03b1 , e \u03b2 , M, L) (3) Syntactic context is in the form of grammar rules M that are aware of semantic concepts; semantic knowledge is in the form of labeled dependency relationships between semantic concepts, L. Both of these are present and explicitly modeled as matrices in SVS's canonical form of vector composition: e \u03b3 = M \u22c5 d (L \u03b3\u00d7\u03b1 \u22c5 e \u03b1 ) \u22c5 d (L \u03b3\u00d7\u03b2 \u22c5 e \u03b2 ) \u22c5 1 (4) Here, M is a diagonal matrix that encapsulates probabilistic syntactic information, where the syntactic probabilities depend on the semantic concept being considered. The L matrices are linear transformations that capture how semantically relevant source vectors are to the resulting vector (e.g., L \u03b3\u00d7\u03b1 defines the the relevance of e \u03b1 to e \u03b3 ), with the intuition that two 1D vectors are under consideration and require a 2D matrix to relate them. 1 is a vector of ones -this takes a diagonal matrix and returns a column vector corresponding to the diagonal elements. Of note in this definition of f (\u22c5) is the presence of matrices that operate on distributed semantic vectors. While it is widely understood that matrices can represent transformations, relatively few have used matrices to represent the distributed, dynamic nature of meaning composition (see Rudolph and Giesbrecht (2010) for a counterexample). Syntax-Semantics Interface This section aims to more thoroughly define the way in which the syntax and semantics interact during structured vectorial semantic composition. SVS will specify this interface such that the composition of semantic vectors is probabilistically consistent and subsumes parsing under various frameworks. Parsing has at times added semantic annotations that unwittingly carry some semantic value: headwords (Collins, 1997) are one-word concepts that subsume the words below them; latent annotations (Matsuzaki et al., 2005) are clustered concepts that touch on both syntactic and semantic information at a node. Of course, other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics. In this light, semantic concepts (vector indices i) and relation labels (matrix arguments l) may also be seen as annotations on grammar trees. Let us introduce notation to make the connection with parsing and syntax explicit. This paper will denote syntactic categories as c and string yields as x. The location of these variables in phrase structure will be identified using subscripts that describe the path from the root to the constituent. 1 Paths consist of left and/or right branches (indicated by '0's and '1's, respectively, as in Figure 1a ). Variables \u03b1, \u03b2, and \u03b9 stand for whole paths; \u03b3 is the path of a composed vector; and \u01eb is the empty path at the root. The yield x \u03b3 is the observed (sub)string that eventually results from the progeny of c \u03b3 . Multiple trees \u03c4 \u03b3 can be constructed at \u03b3 by stringing together grammar rules that are consistent with observed text. Lexicalized Parsing To illustrate the definitions and operations presented in this section, we start with the concrete 'semantic' space of headwords (i.e., bilexical parsing) before moving on to a formal definition. Our example here corresponds to the best parse of the first two words in Figure 1a . In this example domain, assume that the semantic space of concept headwords is {i pulled , i the , i unk }, abbreviated as {i p , i t , i u } where the last concept is a constant for infrequently-observed words. This semantic space becomes the indices of semantic vectors; complete vectors e at each node of Figure 1a are shown in Figure 1b . The tree in Figure 1a contains complete concept vectors e at each node, with corresponding indices i. Values in these vectors (see Figure 1b ) are probabilities, indicating the likelihood that a particular concept summarizes the meaning below a node. For example, consider e 00 : i t produces the yield below address 00 ('the') with probability 1, and i u may also produce 'the' with probability 0.1. Not shown on the tree are the matrices in Figure 1b . In the parametrized matrix M(l MOD \u2236NP \u2192 l MOD \u2236DT l ID \u2236NN), each diagonal element corresponds to the hypothesized grammar rule's probability, given a headword. Similarly, the matrix L 0\u00d700 (l MOD ) is parametrized by the semantic context l MOD -here, l MOD represents a generalized 'modifier' semantic role. For the semantic concept i p at address 0, the left-child modifier (address 00) could be semantic concept i t with probability 0.2, or concept i u with probability 0.8. Finally, by adding an identity matrix for L 0\u00d701 (l ID ) (a 'head' semantic role) to the quantities in Figure 1b , we would have all the components to construct the vector at address 0: e 0 = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 .2 0 0 0 .1 0 0 0 .4 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 M \u22c5d \u239b \u239d \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 .2 .8 0 0 1 .1 .4 .5 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 L 0\u00d701 \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 1 .1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 e 00 \u239e \u23a0 \u22c5 d \u239b \u239d \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 1 0 0 0 1 0 0 0 1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 L 0\u00d701 \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 0 1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 e 01 \u239e \u23a0 \u22c5 \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 1 1 1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 1 = i u i t i p truth \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 0 0 0.036 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 e 0 Since the vector was constructed in syntactic and semantic context, the tree structure shown (including semantic relationships l) is implied by the context. Probabilities in vectors and matrices Formally defining the probabilities in Figure 1 , SVS populates vectors and matrices by means of 5 probability models (models are denoted by \u03b8), along with the process of composition: Syntactic model M(lc \u03b3 \u2192lc \u03b1 lc \u03b2 )[i \u03b3 , i \u03b3 ] =P \u03b8 M (lci \u03b3 \u2192 lc \u03b1 lc \u03b2 ) Semantic model L \u03b3\u00d7\u03b9 (l \u03b9 )[i \u03b3 , i \u03b9 ] =P \u03b8 L (i \u03b9 i \u03b3 , l \u03b9 ) Preterminal model e \u03b3 [i \u03b3 ] =P \u03b8 P-Vit(G) (x \u03b3 lci \u03b3 ), for preterm \u03b3 (5) Root const. model a T \u01eb [i \u01eb ] =P \u03c0 G\u01eb (lci \u01eb ) Any const. model a T \u03b3 [i \u03b3 ] =P \u03c0 G (lci \u03b3 ) These probabilities are encapsulated into vectors and matrices using a convention: column indices of vectors or matrices represent conditioned semantic variables, row indices represent modeled variables. As an example, from Figure 1b , elements of L 0\u00d700 (l MOD ) represent the probability P \u03b8 L (i 00 i 0 , l 00 ). Thus, the conditioned variable i 00 is shown in the figure as column indices, and the modeled i 0 as row indices. This convention applies to the M matrix as well. Recall that M is a diagonal matrix -its rows and columns model the same variable. Thus, we could rewrite P \u03b8 M (lci \u03b3 \u2192 lc \u03b1 lc \u03b2 ) as P \u03b8 M (lci \u03b3 \u2192 lc \u03b1 lc \u03b2 , i \u03b3 ) to make a consistent probabilistic interpretation. We have intentionally left out the probabilistic definition of normal (non-preterminal) nonterminals P \u03b8 Vit(G) , and the rationale for a T vectors. These are both best understood in the dual problem of parsing. Vector Composition for Parsing The vector composition of Equation 4 can be rewritten with all arguments and syntactic information as: e \u03b3 = M(lc \u03b3 \u2192 lc \u03b1 lc \u03b2 ) \u22c5 d (L \u03b3\u00d7\u03b1 (l \u03b1 ) \u22c5 e \u03b1 ) \u22c5 d (L \u03b3\u00d7\u03b2 (l \u03b2 ) \u22c5 e \u03b2 ) \u22c5 1 (4 \u2032 ) a compact representation that masks the underlying consistent probability operations. This section will expand the vector composition equation to show its equivalence to standard statistical parsing methods. Let us say that e \u03b3 [i \u03b3 ] = P(x \u03b3 lci \u03b3 ), the probability of giving a particular yield given the present distributed semantics. Recall that in matrix multiplication, there is a summation over the inner dimensions of the multiplied objects; replacing matrices and vectors with their probabilistic interpretations and summing in the appropriate places, each element of e \u03b3 is then: e \u03b3 [i \u03b3 ] = P \u03b8 M (lci \u03b3 \u2192 lc \u03b1 lc \u03b2 ) \u22c5 i\u03b1 P \u03b8 L (i \u03b1 i \u03b3 , l \u03b1 ) \u22c5 P \u03b8 Vit(G) (x \u03b1 lci \u03b1 ) \u22c5 i \u03b2 P \u03b8 L (i \u03b2 i \u03b3 , l \u03b2 ) \u22c5 P \u03b8 Vit(G) (x \u03b2 lci \u03b2 ) (6) This can be loosely considered the multiplication of the syntax (\u03b8 M term), left-child semantics (first sum), and right-child semantics (second sum). The only summations are between L and e, since all other multiplications are between diagonal matrices (similar to pointwise multiplication). We can simplify this probability expression by grouping \u03b8 M and \u03b8 L into a grammar rule P \u03b8 G (lci \u03b3 \u2192lci \u03b1 lci \u03b2 ) def = P \u03b8 M (lci \u03b3 \u2192lc \u03b1 lc \u03b2 ) \u22c5 P \u03b8 L (i \u03b1 i \u03b3 , l \u03b1 ) \u22c5 P \u03b8 L (i \u03b2 i \u03b3 , l \u03b2 ) , since they deal with everything except the yield of the two child nodes. The summations are then pushed to the front: e \u03b3 [i \u03b3 ] = i\u03b1,i \u03b2 P \u03b8 G (lci \u03b3 \u2192 lci \u03b1 lci \u03b2 )\u22c5P \u03b8 Vit(G) (x \u03b1 lci \u03b1 ) \u22c5 P \u03b8 Vit(G) (x \u03b2 lci \u03b2 ) (7) Thus, we have a standard chart-parsing probability P(x \u03b3 lci \u03b3 ) -with distributed semantic conceptsin each vector element. The use of grammar rules necessarily builds a hypothetical subtree \u03c4 \u03b3 . In a typical CKY algorithm, the tree corresponding to the highest probability would be chosen; however, we have not defined how to make this choice for vectorial semantics. We will choose the best tree with probability 1.0, so we define a deterministic Viterbi probability over candidate vectors (not concepts) and context variables: P \u03b8 Vit(G) (x \u03b3 lce \u03b3 ) def = e \u03b3 = arg max lce\u03b9 a T \u03b9 e \u03b9 \u22c5 P \u03c0 G (lca T \u03b9 ) \u22c5 P \u03b8 Vit(G) (x lce \u03b9 ) ( 8 ) where \u22c5 is an indicator function such that \u03c6 =1 if \u03c6 is true, 0 otherwise. Intuitively, the process is as follows: we construct the vector e \u03b9 at a node, according to Eqn. 4 \u2032 ; we then weight this vector against prior knowledge about the context a T \u03b9 ; the best vector in context will be chosen (the argmax). Also, the vector at a node comes with assumptions of what structure produced it. Thus, the last two terms in the parentheses are deterministic models ensuring that the best subtree \u03c4 \u03b9 is indeed the one generated. Determining the root constituent of the Viterbi tree is the same process as choosing any other Viterbi constituent, except that prior contextual knowledge gets its own probability model in a T \u01eb . As before, the most likely tree \u03c4\u01eb is the tree that maximizes the probability at the root, and can be constructed recursively from the best child trees. Importantly, \u03c4\u01eb has an associated, sentential semantic vector which may be construed as the composed semantic information for the whole parsed sentence. Similar phrasal semantic vectors can be obtained anywhere on the parse chart. These equations complete the linear algebraic definition of structured vectorial semantics. SVS with Relational Clusters Inducing Relational Clusters Unlike many vector space models that are based on the frequencies of terms in documents, we may consider frequencies of terms that occur in similar semantic relations (e.g., head l ID or modifier l MOD ). Reducing the dimensionality of terms in a term-context matrix will result in relationally-clustered concepts. From a parsing perspective, this amounts to latent annotations (Matsuzaki et al., 2005) in l-context. Let us re-notate the headword-lexicalized version of SVS (the example in Section 2.2.1) using h for headword semantics, and reserve i for relationally-clustered concepts. Treebank trees can be deterministically annotated with headwords h and relations l by using head rules (Magerman, 1995) . The 5 SVS models \u03b8 M , \u03b8 L , \u03b8 P-Vit(G) , \u03c0 G\u01eb , and \u03c0 G can thus be obtained by counting instances and normalizing. Empirical probabilities of this kind are denoted with a tilde, whereas estimated models have a hat. Concepts i in a distributed semantic representation, however, cannot be found from annotated trees (see example concepts in Figure 2 ). Therefore, we use Expectation Maximization (EM) in a variant of the inside-outside algorithm (Baker, 1979) to learn distributed-concept behavior. In the M-step, the datainformed result of the E-step is used to update the estimates of \u03b8 M , \u03b8 L , and \u03b8 H (where \u03b8 H is a generlization of \u03b8 P-Vit(G) to any nonterminal). These updated estimates are then plugged back in to the next E-step. The two steps continually alternate until convergence or a maximum number of iterations. E-step: P(i \u03b3 , i \u03b1 , i \u03b2 lc \u03b3 , lc \u03b1 , lc \u03b2 ) = P\u03b8 Out (lci \u03b3 , lch \u01eb \u2212lch \u03b3 ) \u22c5 P\u03b8 Ins (lch \u03b3 lci \u03b3 ) P(lch \u01eb ) (9) E(lci \u03b3 ,lci \u03b1 ,lci \u03b2 ) = P(i \u03b3 ,i \u03b1 ,i \u03b2 lc \u03b3 ,lc \u03b1 ,lc \u03b2 ) \u22c5 P(lc \u03b3 ,lc \u03b1 ,lc \u03b2 ) M-step: P\u03b8 M (lci \u03b3 lc \u03b1 , lc \u03b2 ) = \u2211 i\u03b1,i \u03b2 E(lci \u03b3 , lci \u03b1 , lci \u03b2 ) \u2211 lci\u03b1,lci \u03b2 E(lci \u03b3 , lci \u03b1 , lci \u03b2 ) P\u03b8 L (i \u03b1 i \u03b3 ; l \u03b1 ) = \u2211 lc\u03b3 ,c\u03b1,lci \u03b2 E(lci \u03b3 , lci \u03b1 , lci \u03b2 ) \u2211 lc\u03b3 ,ci\u03b1,lci \u03b2 E(lci \u03b3 , lci \u03b1 , lci \u03b2 ) (10) P\u03b8 H (h \u03b3 lci \u03b3 ) = E(lci \u03b3 , \u2212, \u2212) \u2211 h\u03b3 E(lci \u03b3 , \u2212, \u2212) Inside probabilities can be recursively calculated on training trees from the bottom up. These are simply probability sums of all subsumed subtrees (Viterbi probabilities with sums instead of maxes). Outside probabilities can also be recursively calculated from training trees, here from parent probabilities. For a left child (the right-child case is similar): P\u03b8 Out (lci \u03b1 , lch \u01eb \u2212lch \u03b1 ) = P\u03b8 Out (lci \u03b3 , lch \u01eb \u2212lch \u03b3 ) \u22c5 P\u03b8 M (lci \u03b3 lc \u03b1 , lc \u03b2 ) \u22c5 i \u03b2 P\u03b8 L (i \u03b2 i \u03b3 , l \u03b2 ) \u22c5 P\u03b8 Ins (lch \u03b2 lci \u03b2 ) \u22c5 P\u03b8 L (i \u03b1 i \u03b3 , l \u03b1 ) (11) Since outside probabilities signify everything but what is subsumed by the node, they carry a complementary set of information to inside probabilities. Thus, inside and outside probabilities together are a natural way to produce parent and child clustered concepts. Relational Semantic Clusters in Parsing Section 2.2.2 listed the five probability models necessary for SVS. To define SVS with relational clusters, the estimates in Equation 10 can be used for \u03b8 M and \u03b8 L . The preterminal model is based on \u03b8 H , but it also includes some backoff for words that have not been used as headwords. The other two models also fall out nicely from the algorithm, though they are not explicitly estimated in EM. The prior probability at the root is just the base case for outside probabilities: P\u03c0 G\u01eb (lci \u01eb ) def = P\u03b8 Out (lci \u01eb , lch \u01eb \u2212lch \u01eb ) (12) Prior probabilities at non-root constituents are estimated from the empirically-weighted joint probability. P\u03c0 G (lci \u03b3 ) def = lci\u03b1,lci \u03b2 P(lci \u03b3 , lci \u03b1 , lci \u03b2 ) (13) With these models, a relationally-clustered SVS parser is now defined. Evaluation Sections 02-21 of the Wall Street Journal (WSJ) corpus were used as training data; Section 23 was used as test data with reported parsing results on sentences greater than length 40. Punctuation was left in for all reported evaluations. Trees were binarized, and syntactic states were thoroughly split into subcategorization classes. As previously discussed, unlike tests on state-of-the-art automatically statesplitting parsers, this isolates the contribution of semantics. The baseline 83.57 F-measure is comparable to Klein and Manning (2003) before the inclusion of head annotations. Subsequently, each branch was annotated with a head relation l ID or a modifier relation l MOD according to a binarized version of headword percolation rules (Magerman, 1995; Collins, 1997) , and the headword was propagated up from its head constituent. The most frequent headwords (e.g., h 1 , . . . , h 50 ) were stored, and the rest were assigned a constant, 'unk' headword category. From counts on the binary rules of these annotated trees, the \u03b8 M , \u03b8 L , \u03b8 P-Vit(G) , \u03c0 G\u01eb , and \u03c0 G probabilities for headword-lexicalization SVS were obtained. Modifier relations l MOD were deterministically augmented with their syntactic context; both c and l symbols appearing fewer than 10 times in the whole corpus were assigned 'unknown' categories. These lexicalized models served as a baseline, but the augmented trees from which they were derived were also inputs to the EM algorithm in Section 3.1. Each parameter in the model or training algorithm was examined, with I = {1, 5, 10, 15, 20} clusters, random initialization from reproducible seeds, and a varying numbers of EM iterations. The implemented parser had few adjustments from a plain CKY parser other than these vectors. No approximate inference was used, with no beam for candidate parses and no re-ranking. Interpretable relational clusters Figure 2 shows example clusters for one of the headword models used, where EM clustered 1,000 headwords into 10 concepts in 10 iterations. The lists are parts of the P\u03b8 H (h \u03b3 lci \u03b3 ) model. As such, each of the 10 clusters will only produce headwords in light of some syntactic constituent. The figure shows how distributed concepts produce headwords for transitive past-tense verbs. Note that the probability distributions for different headwords are quite uneven, again confirming that some clusters are more specific, and others are more general. Each cluster has been given a heading of its approximate meaning -i 5 , for example, mostly picks verbs that are 'change in value' events. With 10 clusters, we might not expect such fine-grained clusters, since pLSA-related approaches typically use several hundred for such tasks. The syntactic context of transitive (and therefore state-split) past-tense verbs allows for much finer-grained distinctions, which are then predominantly semantic in nature. Engineering considerations We should note that relationally-clustered SVS is feasible with respect to random initialization and speed. Four relationally-clustered SVS models (with 500 headwords clustered into 5 concepts) were trained, each having a different random initialization. We found that the parsing F-score had a mean of 83.98 and a standard deviation of 0.21 across different initializations of the model. This indicates that though there are significant difference between the models, they still outperform models without SVS (see next section). Also, it may seem slow to consider the set of semantic concepts and relations alongside syntax, at least with respect to normal parsing. The definition of SVS in terms of vectors actually mitigates this effect on WSJ Section 23, according to Figure 3 . Since SVS is probabilistically consistent, the parser could be defined without vectors, but this would have the 'non-vectorized' speed curve. The contiguous storage and access of information in the 'vectorized' version leads to an efficient implementation. Comparison to Lexicalization One important comparison to draw here is between the effectiveness of semantic clusters versus headword-lexicalization. For fair head-to-head comparison on WSJ Section 23, both models were vectorized and included no smoothing or backoff. Neither relational clusters nor lexicalization were optimized with backoff or smoothing. Table 1a shows precision, recall, and F-score for lexicalized models and for clustered semantic models. First, note that the 10-cluster model (in bold) improves on a syntax-only parser (top line), showing that the semantic model is contributing useful information to the parsing task. Next, compare the 50-headword, 10-cluster model (in bold) to the line above it. It is natural to compare this model to the headword-lexicalized model with 50 headwords, since the same information from the trees is available to both models. The relationally-clustered model outperforms the headwordlexicalized model, showing that clustering the headwords actually improves their usefulness, despite the fact that fewer referents are used in the actual vectors. It is also interesting, then, to compare this 50-headword, 10-cluster model to a headword-lexicalized model with 10 headwords. In this case, the possible size of the grammar is equal. Again, the relationallyclustered model outperforms plain lexicalization. This indicates that the 10 clustered referents are much more meaningful than 10 headword referents for the disambiguating of syntax. Effect of Number of clusters The final experiment on relational-clustering SVS was to determine whether performance would vary with the number of clusters. Table 1b compares average performance (over different random initializations) for numbers of clusters from 1 (a syntax-equivalent case) to 20. First, it should be noted that all of the relationally clustered models improved on the baseline. Random initializations did not vary enough for these models to do worse than syntax alone. For each vector/domain size, in fact, the gains over syntax-only are substantial. In addition, the table shows that average performance increases with the number of clusters. This loosely positive slope means that EM is still finding useful parts of the semantic space to explore and cluster, so that the clusters remain meaningful. However, the increase in performance with number of clusters is likely to eventually plateau. Maximum-accuracy models were also evaluated, since each model is a full-fledged parser. The best 20-referent model obtained an F score of 84.60%, beating the syntactic baseline by almost a full absolute point. Thus, finding relationally-clustered semantic output also contributes to some significant parsing benefit. Perplexity Finally, per-word perplexities were calculated for a syntactic model and for a 5-concept relationallyclustered model. Specific to this evaluation, following Mitchell and Lapata (2009) Table 2 shows that adding semantic information greatly reduces perplexity. Since as much syntactic information as possible (such as argument structure) has been pre-annotated onto trees, the isolated contribution of interactive semantics improves on a syntax-only model model. Conclusion This paper has introduced a structured vectorial semantic (SVS) framework in which vector composition and syntactic parsing are a single, interactive process. The framework thus fully integrates distributional semantics with traditional syntactic models of language. Two standard parsing techniques were defined within SVS and evaluated: headword-lexicalization SVS (bilexical parsing) and relational-clustering SVS (latent annotations). It was found that relationallyclustered SVS outperformed the simpler lexicalized model and syntax-only models, and that additional clusters had a mildly positive effect. Additionally, perplexity results showed that the integration of distributed semantics in relationally-clustered SVS improved the model over a non-interactive baseline. It is hoped that this flexible framework will enable new generations of interactive interpretation models that deal with the syntax-semantics interface in a plausible manner.",
    "abstract": "Distributed models of semantics assume that word meanings can be discovered from \"the company they keep.\" Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a document. In contrast, this paper proposes a structured vectorial semantic framework, in which semantic vectors are defined and composed in syntactic context. As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse. Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "10",
    "year": "2011",
    "month": "",
    "title": "Structured Composition of Semantic Vectors"
}