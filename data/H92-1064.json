{
    "article": "On what basis are the input processing capabilities of Natural Language software judged? That is, what are the capabilities to be described and measured, and what are the standards against which we measure them? Rome Laboratory is currently supporting an effort to develop a concise terminology for describing the linguistic processing capabilities of Natural Language Systems, and a uniform methodology for appropriately applying the terminology. This methodology is meant to produce quantitative, objective profiles of NL system capabilities without requiring system adaptation to a new test domain or text corpus. The effort proposes to develop a repeatable procedure that produces consistent results for independent evaluators. INTRODUCTION An appreciable drawback to current corpus-based (eg., [BBN; 1988] , [Flickinger, et al; 1987] , [Hendrix, et al; 1976] , [Malhotra; 1975] ) and task-based (eg., [\"Proceedings\"; 1991] ) methodologies for evaluating Natural Language Processing Systems is the requirement for transportation of the system to a test domain. The expense and time consumption are sizable and, as the port may be minimal or incomplete, the evaluation may be based on a demonstration of less than the full potential of the system. Further, current evaluation methodologies do not fully elucidate NLP system capabilities for possible future applications. Under contract to Rome Laboratory, Dr. Jeannette Neal (Calspan Corporation) and Dr. Christine Montgomery (Language Systems Incorporated) are in the final months of developing an NLP system evaluation methodology that produces descriptive, objective profiles of system linguistic capabilities without a requirement for system adaptation to a new domain. The evaluation methodology is meant to produce consistent results for varied haman users. Evaluation Methodology Description Within the Neal-Montgomery NLP System Evaluation Methodology each identified linguistic (lexical, syntactic, semantic, or discourse) feature is first carefully defined and explained in order to establish a standard delimitation of the feature. Illustrative language patterns and sample sentences then guide the human evaluator to the formulation of an input that tests the feature on the NLP system within the system's native domain. Based on clear and specific evaluation criteria for test item inputs, NLP system responses are scored as follows: S: The system successfully met the stated criteria and demonstrated understanding with respect to the feature under test. C: The system responded in a way that was correct (that is, correctly answered the question posed), but the criteria were not met. P: The system responded in a way that was only partially correct. F: The system responded in a way that was incorrect, failing to meet the criteria. N: The system was unable to accept the input or form a response (for example, the system vocabulary lacks appropriate words to complete a test inpu0. Each linguistic feature is tested by more than one methodology item to make sure that results are not based on spurious responses, and each item examines only one asyet-untested capability, or one as-yet-untested combination of capabilities. Test inputs that are dependent on capabilities previously shown to be unsuccessful are avoided. Scores are then aggregated into percentages for hierarchically-structured classes of linguistic capabilities which produce descriptive profiles of NLP systems. The profiles can be viewed at varying levels of granularity. Figure 1 shows a sample system profile from the top level of the hierarchy. Note that the scoring nomenclature (above) has been refined and expanded since project experiments produced the profiles and results presented in this paper. In Figures 1 and 2 , \"Unable to Compose Input\" is equivalent to an 'N' in the newer nomenclature. A score of \"Indeterminate\" earlier meant the human evaluator could not determine if the NLP 1 : A Top Level Evaluation Profile of an NLP System system correctly processed the test input. The new system of scores will be applied for the final project selfassessment activities. The columns at the far fight of Figure 1 display the total time (in hours and minutes) the user required to complete that section of the evaluation, and the average time per item (hours:minutes:seconds) for the section. Figure 2 displays part of the evaluation to the methodology's most detailed level of granularity. PROJECT SELF-ASSESSMENT In March and September of 1991 rigorous project assessments provided valuable feedback into the design of the Neal-Montgomery NLP System Evaluation Methodology. For each assessment, three people applied the methodology to each of three NLP systems, for a total of eighteen applications. Assessment personnel, knowledgeable with respect to interface technology but not trained linguists, were distinct from the methodology development team. The consistency of system profiles resulting from these applications, the examination of test inputs composed during the assessments, records of oral commentary by evaluators, and responses to a post-evaluation questionnaire have been used as measures of the accuracy of methodology results. For the September assessment phase, Figure 3 shows, for each section of the methodology, the percentage of items for which the assessment team gave the same score to each system. For example: the data points for the adverbial section indicate that all three people gave the same assessment of System 2's skills for adverbials (they agreed in every instance), they agreed 60% of the time on System l's adverbial skills, and they agreed only 20% of the time for System 3's adverbial skills. The inconsistency of scores in this section has prompted the development team to refine the methodology's adverbial section. NLP systems used for assessments to date have included three NL database query systems and two MUC-3 systems. Focusing on reliability rather than feedback into methodology design, four people will apply the Neal-Montgomery NLP Evaluation Methodology to each of two systems for the third (and final) project self-assessment in April 1992. TOWARD THE FUTURE Evaluation \"standards\" are not developed and adopted without a period of review, rumination, and tweaking by the relevant user community. It is our hope therefore, in distributing the Neal-Montgomery NLP System Evaluation Methodology to the technical community, to stir interest that may lead to the eventual consideration of the methodology as the basis for a standard evaluation tool for NLP system capabilities. The Neal-Montgomery NLP System Evaluation Methodology is due for completion and delivery to Rome Laboratory in May of 1992. It will become immediately available at that time to all interested parties. Requests should be made to the author of this paper. Reviewer comment, critique, and suggestions for the methodology are invited. I. Basic . . Malhotra, A., \"Design Criteria for a Knowledge-6. Based Language System for Management: An Experimental Analysis\", MIT/LCS/ TR-146, 1975. Neal, J. G., Feit, E.L., and Montgomery, C.A., 7 . \"An Application-Independent Approach to Natural Language Evaluation\", submitted to ACL-92.",
    "abstract": "On what basis are the input processing capabilities of Natural Language software judged? That is, what are the capabilities to be described and measured, and what are the standards against which we measure them? Rome Laboratory is currently supporting an effort to develop a concise terminology for describing the linguistic processing capabilities of Natural Language Systems, and a uniform methodology for appropriately applying the terminology. This methodology is meant to produce quantitative, objective profiles of NL system capabilities without requiring system adaptation to a new test domain or text corpus. The effort proposes to develop a repeatable procedure that produces consistent results for independent evaluators.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "4",
    "year": "1992",
    "month": "",
    "title": "{N}eal-{M}ontgomery {NLP} System Evaluation Methodology"
}