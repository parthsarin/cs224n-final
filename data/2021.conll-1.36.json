{
    "article": "It is often posited that more predictable parts of a speaker's meaning tend to be made less explicit, for instance using shorter, less informative words. Studying these dynamics in the domain of referring expressions has proven difficult, with existing studies, both psycholinguistic and corpus-based, providing contradictory results. We test the hypothesis that speakers produce less informative referring expressions (e.g., pronouns vs. full noun phrases) when the context is more informative about the referent, using novel computational estimates of referent predictability. We obtain these estimates training an existing coreference resolution system for English on a new task, masked coreference resolution, giving us a probability distribution over referents that is conditioned on the context but not the referring expression. The resulting system retains standard coreference resolution performance while yielding a better estimate of human-derived referent predictability than previous attempts. A statistical analysis of the relationship between model output and mention form supports the hypothesis that predictability affects the form of a mention, both its morphosyntactic type and its length. Introduction A long-standing hypothesis in linguistics relates predictability to linguistic form: the more predictable parts of a speaker's meaning tend to be communicated with shorter, less informative words and be pronounced more quickly and flatly (Ferrer i Cancho and Sol\u00e9, 2003; Aylett and Turk, 2004; Levy and Jaeger, 2007; Piantadosi et al., 2011) . This is posited to result from a trade-off between clarity and cost: when the context already clearly conveys certain information, making it explicit would be inefficient. In the domain of referring expressions, or mentions, the prediction is that pronouns such as \"they\" and \"her\" are used for more \"Kamala Harris\" or full noun phrases such as \"my child's teacher\" are required for less predictable referents (Tily and Piantadosi, 2009) . The aim of this paper is to test this prediction using a novel, computational estimate of referent predictability. Existing work on the relation between predictability and mention form paints a mixed picture (Section 2). A set of studies in psycholinguistics used controlled stimuli designed to manipulate predictability (Arnold, 2001; Rohde and Kehler, 2014 ). An alternative approach uses naturally occurring corpus data to elicit judgments from human subjects (Tily and Piantadosi, 2009; Modi et al., 2017b) . Neither approach, however, scales well. We instead obtain predictability estimates from a coreference system (trained on English data); if successful, this strategy would allow to test the hypothesis in a wider set of contexts than in psycholinguistic experiments. This paper follows the long tradition of using computational models trained on large amounts of data as proxies for different aspects of human cognition; in particular, it extends to the referential level previous research that uses computational models to obtain predictability scores (Hale, 2001; Smith and Levy, 2013) . Standard coreference models, however, cannot be directly used to model predictability, because they are trained with access to both the context of a referring expression and the expression itself. Instead, we aim at obtaining predictability scores that are only conditioned on the context, following the definition of predictability used in psycholinguistics. To this end, we minimally modify a state-of-the-art coreference system (Joshi et al., 2020) to also carry out what we call masked coreference resolution (Figure 1 ): computing referent probabilities without observing the target mention. We show that the resulting model retains standard coreference resolution performance, while yielding a better estimate of human-derived referent predictability than previous attempts. Statistical analysis of the relationship between model scores and mention form suggests that predictability is indeed linked to referential form. Low referent surprisal tends to be associated with pronouns (as opposed to proper names and full noun phrases) and in general shorter expressions. When controlling for shallow cues that modulate the salience of an entity (e.g., recency, frequency), surprisal no longer differentiates pronouns from proper names (while still favoring longer noun phrases). This may be read as supporting the hypothesis that it is primarily mention length that is at stake, rather than morphosyntactic category, but it may also be due to stylistic factors in the data we use (OntoNotes, Weischedel et al. 2013 ). 1 Related work Predictability and referential form. Traditional approaches to discourse anaphora posit that the salience of referents determines the choice of referring expression (a.o., Ariel, 1990; Gundel et al., 1993) . Salience, however, is notoriously hard to characterize and operationalize. Later work has therefore focused on predictability, that is, how expected a specific referent is at a certain point in the discourse (see Arnold and Zerkle 2019 for an overview). This notion can be more easily operationalized to gather judgements from human subjects. Previous work has done so using cloze tasks and discourse continuation experiments. Some of this work found that more pronouns were produced for more predictable referents, suggesting an addressee-oriented strategy for pronoun production (e.g., Arnold, 2001) . Other work instead reported the same pronominalization rates in contexts that favored different referents, and argued for a mismatch between speaker and listener strategies (e.g., Rohde and Kehler, 2014; Mayol, 2018) . While the aforementioned psycholinguistic studies used tightly controlled stimuli, researchers also reported contradictory results when looking at cloze task responses in corpus data. Tily and Piantadosi (2009) considered newspaper texts from the OntoNotes corpus and found that pronouns and proper names were favoured over full NPs when subjects were able to guess the upcoming referent. By contrast, Modi et al. (2017b) did not find this ef- 1 We make the code used to carry out this study available at https://github.com/amore-upf/ masked-coreference. fect on narrative texts in the InScript corpus (Modi et al., 2017a) . Computational estimates of predictability. Probabilistic language models have been commonly adopted to study expectation-based human language comprehension (Armeni et al., 2017) . Predictability scores obtained from language models have been shown to correlate with measures of cognitive cost at the lexical and the syntactic levels (Smith and Levy, 2013; Frank et al., 2013) . In this work, predictability is typically measured with solely the preceding context in computational psycholinguistics (e.g., Levy, 2008) . However, more recent work has also looked at predictability calculated based on both the previous and following contexts using bidirectional networks, like we do: Pimentel et al. (2020) used surprisal calculated from a cloze language model which takes both left and right pieces of context, as the operationalization of word predictability. They studied the trade-off between clarity and cost and reported a tendency for ambiguous words to appear in highly informative contexts. Previous work also used computational estimates of referent predictability. In Orita et al. (2015) , they were used to explain referential choice as the result of an addressee-oriented strategy. Their measures of predictability, however, were based on simple features like frequency or recency. Modi et al. (2017b) built upcoming referent prediction models combining shallow linguistic features and script knowledge. This approach allowed them to disentangle the role of linguistic and common-sense knowledge, respectively, on human referent predictions. More recent research assessed the ability of autoregressive language models to mimick referential expectation biases that humans have shown in the psycholinguistic literature (e.g., Upadhye et al., 2020) . Davis and van Schijndel (2021) extended this assessment to non-autoregressive models, like the ones we use here, and reported results consistent with prior work in autoregressive models, showing that these models exhibited biases in line with existing evidence on human behavior, at least for English. Automatic coreference resolution. The goal of a standard coreference resolution system is to group mentions in a text according to the real-world entity they refer to (Pradhan et al., 2012) . Several deep learning approaches have been proposed in Meg congratulated the colleague. She was happy because [MASK] got promoted. the NLP literature, such as cluster-ranking (Clark and Manning, 2016) or span-ranking architectures (Lee et al., 2017) . We focus on span-ranking models, which output, for each mention, a probability distribution over its potential antecedents. We rely on an existing state-of-the-art 'end-toend' coreference resolution system based on the SpanBERT language model (Joshi et al., 2020) , henceforth SpanBERT-coref. It builds directly on the coreference systems of Joshi et al. (2019) and Lee et al. (2018) , the main innovation being its reliance on SpanBERT in place of (respectively) BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) . We give more details about the system in Section 3. SpanBERT and BERT are transformer encoders pretrained on masked language modeling (Devlin et al., 2018) : a percentage of tokens is masked -substituted with a special token [MASK] ; the model has to predict these tokens based on their surrounding context. For training SpanBERT, random contiguous sets of tokens -spans-are masked, and an additional Span Boundary Objective encourages meaningful span representations at the span's boundary tokens. 2 Methods The SpanBERT-coref system. We use the best system from Joshi et al. (2020) , which is based on SpanBERT-base (12 hidden layers of 768 units, with 12 attention heads). We make no changes to its architecture, only to the data on which it is trained, by masking some mentions. In SpanBERT-coref, each span of text is represented by a fixed-length vector computed from SpanBERT token representations, obtained considering a surrounding context window of maximum 384 tokens. 3 From span representations, a mention score is computed for each span (s m : how likely it is to be a mention), and a compatibility score is computed for each pair of spans (s a : how likely it is that they corefer). These scores are aggregated into a score s for each pair of spans (Eq. 1). For each mention, a probability distribution over its candidate antecedents (previous mentions and 'none') is then derived, determining coreference links (Eq. 2). The complete, end-to-end system is trained (and the underlying language model finetuned) on the English portion of the coreference-annotated OntoNotes 5.0 dataset (Weischedel et al., 2013) , which we also use. s(x, y) = s m (x) + s m (y) + s a (x, y) (1) P (antecedent x = y) = e s(x,y) i\u2208candidatex e s(x,i) (2) Training on masked coreference. We model entity predictability in terms of a probability distribution over entities given a masked mention. The probability that a mention x refers to the entity e -P (E x = e) -can be computed as the sum of the antecedent probabilities of the mentions of e in the previous discourse (M e ): P (E x = e) = i\u2208Me P (antecedent x = i) (3) However, in SpanBERT-coref this probability is conditioned both on the mention and its context (the model has observed both to compute a prediction), whereas we need a distribution that is only conditioned on the context. To achieve this, we introduce masked coreference resolution, the task of determining the antecedent of a mention that has been replaced by an uninformative token [MASK] . The task, inspired on masked language modeling (Devlin et al., 2018) , is illustrated in Figure 1 . Note that SpanBERTcoref can be directly used for masked coreference, token representation, end token representation, and a weighted sum (attention) over all of its token representations. since its vocabulary already includes the [MASK] token. However, since the system was not trained in this setup, its predictions are not optimal, as we show in Section 4. Therefore, we train a new instance of SpanBERT-coref adapted to our purposes -M m . To train M m , we mask a random sample of mentions in each document by replacing them by a single [MASK] token. 4 The percentage of mentions masked is a hyperparameter (we test 5%-40%). Note that, this way, the model is optimized to identify the correct antecedents of both masked and unmasked mentions, such that it retains standard coreference capabilities (see Section 4). Evaluation. To evaluate general coreference performance, following the CoNLL-2012 shared task (Pradhan et al., 2012) , we report the averages of the MUC, B 3 and CEAF metrics in precision, recall and F1, respectively. These metrics focus on the quality of the induced clusters of mentions (i.e., coreference chains) compared to the gold ones. When using M m to model predictability, however, we only care about antecedent assignments, not overall clustering structure. Therefore, we also evaluate the models on the task of antecedent prediction. We compute antecedent precision, recall and F1, where a model's prediction for a mention is correct if it assigns the largest probability to a true antecedent (an antecedent belonging to the correct mention cluster), or to none if it is the first mention of an entity. We use F1 on antecedent prediction as the criterion for model selection during hyperparameter tuning. To obtain predictions at test time, given a masked mention, we derive a probability distribution over its unmasked antecedents (Figure 1 ). Thus, when computing masked predictions we must avoid interference between masked mentions. At the same time, for computational efficiency, we want to avoid masking only one mention per document at a time. We solve this by computing, for each document, a partition of the mentions, where each subset is maskable if, for each mention, none of its antecedents nor surrounding tokens (50 on either side) are masked. We generate one version of each document for each subset of masked mentions. We compute predictions for each document version separately, and collect antecedent assignments for the masked mentions, thereby obtaining masked predictions for each mention in the document. 5   Using gold mention boundaries. As mentioned earlier, SpanBERT-coref is jointly trained to detect mentions and to identify coreference links between them. We are interested only in the latter task, for which the challenge of mention detection only adds noise. Therefore, for analysis (not during training) we use gold mentions as the only candidate spans to consider; mention scores are set to 0, nullifying their contribution to model predictions (i.e., s(i, j) = s a (i, j); Eq. 1). Context. Our setup differs from that of Tily and Piantadosi (2009) and Modi et al. (2017b) in that their human participants were given only the context preceding the mention, while our model is given context on both sides, in line with the bidirectional nature of BERT, SpanBERT and the state of the art in coreference resolution. Thus, the notion of referent predictability we model is to be understood not in the sense of anticipation in time, but in informational terms (in line with Levy and Jaeger 2007) : how much information the context provides vs. how much information the mention needs to provide for its intended referent to be understood. This is not necessarily less cognitively plausible than a left-context-only setting: Humans take right-hand context into account when interpreting referring expressions (Deemter, 1990; Song and Kaiser, 2020) , Indeed, this could provide crucial disambiguating information, as shown in the following example: (1) Ann scolded her daughter , because a) she was not behaving. b) she was not happy with her behavior. We leave the exploration of different kinds of context to future work. Evaluation Table 1 reports the results of evaluation on OntoNotes test data for both M u (the standard SpanBERT-coref coreference system) and our variant M m , trained with 15% of mentions masked in each document. 6 The .74 Table 1 : Results on OntoNotes test data (English): document-level coreference resolution (only with unmasked mentions; CoNLL scores) and antecedent prediction (both unmasked and masked mentions); P, R, F1 = precision, recall, F1 scores (when using gold mention boundaries on antecedent prediction, P = R = F1). BUC, M 3 and CEAF scores are reported in Appendix B. model-predicted and gold mention boundaries; the latter are always higher, as expected. For unmasked mentions, we provide results both for standard coreference resolution and per-mention antecedent predictions (ANTECEDENT); for masked mentions, only the latter is applicable (see Section 3). On unmasked mentions, the two models perform basically the same. This means that masking 15% of mentions during training, which was done only for M m , does not interfere with the ability of the system on ordinary, unmasked coreference resolution. On masked mentions, both models perform worse, which is expected because it is a more difficult task: Without lexical information about the mention, the models have less information to base their prediction on. Still, both models provide correct predictions in a non-trivial portion of cases. M u , which did not observe any masked mentions during training, achieves .5 F1 for gold mentions. A random baseline gets only .08, and selecting always the immediately previous mention or always \"no antecedent\" obtain .23 and .26, respectively. Thus, it seems that M u retains some ability to compute meaningful representations for masked tokens from pretraining, despite not seeing [MASK] during training for coreference resolution. Nevertheless, in line with our expectations, training on masked coreference is beneficial: M m improves substantially over the results of M u , with .74 F1. This means that even without lexical information from the mention itself, 74% of referents are correctly identified, i.e., predictable on the basis of context alone. Results by mention type. Figure 2 breaks down the antecedent precision scores of M m by mention type. From now on we look only at the setup with gold mention boundaries, though the trends are the same for predicted mentions (reported in Appendix B). We distinguish between proper names (e.g., \"Kamala Harris\"), full noun phrases (NPs; e.g., \"the tall tree\") and pronouns (e.g., \"she\", \"my\", \"that\"). For completeness, in Appendix B we report the results considering a more fine-grained distinction. The figure shows that for predicting the antecedent of masked mentions, pronouns are the easiest (.81), followed by proper names (.71), and full NPs (.66) are the hardest. Put differently, pronouns are used in places where the referent is the most predictable, full NPs when the referent is the least predictable. Table 2 shows examples of predictions on masked mentions with different mention types. For unmasked mentions, instead, proper names are the easiest (.96; names are typically very informative of the intended referent), and full NPs (.89) are only slightly more difficult than pronouns (.92). Hence, the pattern we see for masked mentions cannot be a mere side-effect of pronouns being easier to resolve in general (also when unmasked), which does not seem to be the case. Instead, it provides initial evidence for the expected relation between referent predictability and mention choice, which we will investigate more in the next section. Comparison to human predictions. We assess how human-like our model M m behaves by comparing its outputs to human guesses in the clozetask data from Modi et al. (2017b) . Subjects were asked to guess the antecedent of a masked mention in narrative stories while seeing only the left context (182 stories, \u223c3K mentions with 20 guesses each). To evaluate the model's estimates, we follow Modi et al.'s approach, and compute Jensen-Shannon divergence to measure the dissimilarity between a model's output and the human distribution over guessed referents (the lower the better). M m achieves a divergence of .46, better than Modi et al.'s best model (.50), indicating that our system better approximates human expectations. Appendix B provides further results and details. Predictability and mention form The previous section assessed the effect of our masked training method on model quality. We believe that the model predictions are of high quality enough that we can use them to test the main hypothesis regarding the relation between predictability and mention choice. Following previous work (see Section 2), we define predictability in terms of the information-theoretic notion of surprisal: the more predictable an event, the lower our surprisal when it actually occurs. Given a masked mention x with its true referent e true , surprisal is computed from the model's output probability distribution over entities E x (Eq. 3), given the context c x : surprisal(x) := \u2212 log 2 P (E x = e true | c x ) Surprisal ranges from 0 (if the probability assigned to the correct entity equals 1) to infinity (if this probability equals 0). Surprisal depends only on the probability assigned to the correct entity, regardless of the level of uncertainty between the competitors. As Tily and Piantadosi (2009) note, uncertainty between competitors is expected to be relevant for mention choice, e.g., a pronoun may be safely used if no competitors are likely, but risks being ambiguous if a second entity is somewhat likely. Tily and Piantadosi (2009) and, following them, Modi et al. (2017b) took this uncertainty into account in terms of entropy, i.e., expected surprisal. We report our analyses using entropy in Appendix C, for reasons of space and because they support essentially the same conclusions as the analyses using just surprisal. We check whether surprisal predicts mention type (Section 5.1) and whether it predicts mention length (number of tokens; Section 5.2). All analyses in this section use the probabilities computed by M m with gold mention boundaries. Surprisal as a predictor of mention type For this analysis, in line with previous studies, we consider only third person pronouns, proper names and full NPs with an antecedent (i.e., not the first mention of an entity). For the OntoNotes test data this amounts to 9758 datapoints (4281 pronouns, 2213 proper names and 3264 full NPs). Figure 3 visualizes surprisal of masked mentions grouped by type, showing that despite much within-type variation, full NPs tend to have higher surprisal (be less predictable) than pronouns and proper names. To quantify the effect of predictability on mention type, we use multinomial logistic regression, using as the dependent variable the three-way referential choice with pronoun as the base level, and surprisal as independent variable. 7 The results of this surprisal-only regression are given in the top left segment of Table 3 . The coefficients show that greater surprisal is associated with a higher probability assigned to proper names (\u03b2 = .31) and even more so full NPs (\u03b2 = .47); hence pronouns are used for more predictable referents. Since surprisal was standardized, we can interpret the coefficients (from logits to probabilities): e.g., adding one standard deviation from mean surprisal increases the predicted probability of a proper name from .23 to .25, and of a full NP from .33 to .42, decreasing the probability of a pronoun from .43 to .34. 7 We use the multinom procedure from the library nnet (Venables and Ripley, 2002) Next, following Tily and Piantadosi 2009; Modi et al. 2017b , we test whether predictability has any effect over and above shallower linguistic features from the literature that have been hypothesized to affect mention choice. We fit a new regression model including the following features as independent variables alongside surprisal: 8 distance (num. sentences between target mention and its closest antecedent); frequency (num. mentions of the target mention's referent so far); closest antecedent is previous subject (i.e., of the previous clause); target mention is subject; closest antecedent type (pronoun, proper name, or full NP). The results are shown in the bottom left segment of Table 3 . 9  We verified that the incorporation of each predictor improved goodness-of-fit, using the Likelihood Ratio (LR) chi-squared test (with standard .05 alpha level; see Appendix C for specific results). Surprisal improved goodness-of-fit (p \u03c7 2 << 0.001): it contributes relevant information not captured by the shallow features alone. At the same time, however, now surprisal is not anymore predictive of the distinction between pronouns and proper names, as found by Tily and Piantadosi (2009) -only of the distinction between pronouns and full NPs (see significance values of the predictor 'surprisal' for the two left columns of Table 3 ). If we conceive of the shallow features as possi- 8 The result of this simultaneous regression as regards the predictor surprisal will be identical to what the result would be of a hierarchical regression where surprisal is the last added predictor (Wurm and Fisicaro, 2014) . 9 Because the features themselves may capture aspects of predictability, and are indeed correlated with predictability (though all r < .35), we cannot in this case interpret the coefficient for surprisal as directly indicative of the magnitude of the effect of predictability on mention choice. We visualize the comparison of observed to predicted types using a ternary plot, see Figure 6 in Appendix C. ble confounds, our results shows that predictability still affects mention choice when controlling for these. Alternatively, we can take the shallow features to themselves capture aspects of predictability (e.g., grammatical subjects tend to be used for topical referents, which are therefore expected to be mentioned again), in which case the results show that these features do not capture all aspects. As for the shallow features themselves, we find that pronouns are favoured over proper names and full NPs when the referent has been mentioned recently, in line with the idea that the use of pronouns is sensitive to the local salience of a referent. Moreover, pronominalization is more likely if the previous mention of the referent was itself a pronoun. There is also a strong tendency to reuse proper names, perhaps due to stylistic features of the texts in OntoNotes: in news texts, proper names are often repeatedly used, plausibly to avoid confusion, as news articles often introduce many entities in a short span; in the Bible, the use of repeated proper names is especially common for the protagonists (e.g. Jesus). Lastly, we find the well-known subject bias for pronouns: pronouns are more likely than full NPs or proper names when the referent's previous mention occurred in subject position. Overall, the results corroborate the finding in Tily and Piantadosi (2009) that full NPs are favoured, and pronouns and proper names disfavored, when surprisal is higher; and extend their finding, based on newspaper texts only, to a larger amount of data and more diverse genres of text (news, magazine articles, weblogs, religious texts, broadcast and telephone conversation). Surprisal as a predictor of mention length If pronouns are favoured for more predictable referents due to a trade-off between information content and cost, one would expect to find similar patterns using graded measures of utterance cost, instead of flattening it to coarse-grained distinctions across mention types. In this subsection we use the number of tokens as such a measure (Orita et al., 2015) . The average number of tokens per mention in our dataset is (of course) 1 for pronouns, 1.67 for proper names and 3.16 for full NPs. We fit linear regression models with mention length in number of tokens as the dependent variable (or number of characters, in Appendix C), and, again, surprisal with and without shallow linguistic features as independent variables. The right segment of Table 3 presents the results, indeed showing an effect of mention length. In the surprisal-only model, moving up by one standard deviation increases the predicted mention length by .25 tokens (or 1.40 characters, see Table 5 in Appendix C). Grammatical function and type of the antecedent are still strong predictors, with surprisal again making a contribution on top of that: mentions that refer to a more surprising referent tend to have more words. Figure 4 visualizes this trend between surprisal and predicted mention length. Single-token pronouns dominate the lower end of the output range, raising the question of whether predictability still makes a difference if we exclude them, i.e., fit regression models only on the nonpronominal mentions. Our results support an affirmative answer (see Table 4 and 6 in Appendix C): the more surprising a referent, the longer the proper name or full NP tends to be. Discussion and Conclusion In this work, we studied the relationship between referent predictability and the choice of referring expression using computational estimates of the former. To derive these, we adapted an existing coreference resolution system to operate in a setup resembling those of cloze tasks employed in psycholinguistics. Using computational estimates of semantic expectations allowed us to scale and expedite analyses on a large dataset, spanning different genres and domains. Our results point to a trade-off between clarity and cost, whereby shorter and possibly more ambiguous expressions are used in contexts where the referent is more predictable. We found this both when grouping mentions by their morphosyntactic type and by their length. Referent predictability seems to play a partially overlapping but complementary role on referential choice with features affecting the salience of an entity, such as its recency, frequency or whether it was last mentioned as a subject. This points to the open question as to whether salience can actually be reduced to predictability (Arnold, 2001; Zarcone et al., 2016) . Our bidirectional setup is not directly comparable to that of some of the related work as to the amount and scope of context given for prediction. Referents are predicted with only the preceding context in previous work, both in psycholinguistic and computational approaches, while our model gives predictions based on both the preceding and following contexts. If one important hypothesis both previous studies and our study aim at testing is that speakers tend to avoid the redundancies between the informativeness of context and that of referring expression, our results then point to an issue that merits more attention: What kind of context influences referential choice? Is it only the preceding one, or the following How much (on either side)? Leventhal (1973) raised a similar question concerning word intelligibility in sentences and found that participants delayed the decision about a test word presented in noise until the entire sentence was encoded, and that the context after the target word was more facilitating to its intelligibility. Song and Kaiser (2020) also showed that comprenhenders actively utilized postpronominal information in pronoun resolution. The use of a computational model provides flexibility to compare predictions using different amounts of context, and could shed light on how the previous and following context affect mention choice. Future work could also use unidirectional models, which allow for a setup more like the one adopted by prior work for ease of comparison, if requirements on the quality of performance can be met. We hope that our work will foster the use of computational models in the study of referential choice. Our methodology can be applied to more languages besides English (provided the availability of coreference resources; for instance, Arabic and Chinese are included in OntoNotes 5.0) and the study of phenomena beyond those considered here. Relevant future venues are more fine-grained classifications of NPs (such as indefinite vs. definite), the effect of referent predictability on processing (Mc-Donald and MacWhinney, 1995) Appendices A Method: details For simplicity, both in training and evaluation, we never mask mentions which are embedded in another mention (e.g., \"the bride\" in \"the mother of the bride\"), since that would cover information relevant to the larger mention. In case we mask a mention that includes another mention, we discard the latter from the set of mentions for which to compute a prediction. For evaluation on development data, to find the best models across training epochs and hyperparameters, we use a quicker but more coarse-grained method than that used for evaluation on test data to assess performances on masked mentions. We mask a random sample (10%; independently of the percentage used during training) of mentions in each document, compute evaluation scores and get the average of these across 5 iterations (i.e., with different samples of mentions masked). Although in this setup masks could potentially interfere with each other, and we will not have masked predictions for all mentions, overall this method will give us a good enough representation of the model's performances on masked mentions, while being quick to compute. When evaluating antecedent prediction, we skip the first mention in a document as this is a trivial prediction (no antecedent). B Evaluation B.1 Complete results on OntoNotes Table 4 reports MUC, B 3 and CEAF scores (precision, recall and F1), for the M u and M m . The results are overall comparable between the two systems across all metrics. Figures 5 and 6 report the antecedent prediction results, using gold mention boundaries, of the M m and M u considering a fine-grained distinctions across mention types than what reported in Figure 2 of the paper. Concretely, we divide pronouns, into first-, second-and third-person pronouns, as well as treating demonstratives (e.g., \"that\") as a separate category (DEM). We subdivide pronouns in this way because they are quite heterogeneous: first-and second-person pronouns are comparatively rigid (typically referring to the speaker and addressee), and are used oftentimes within a quotation (e.g. Asked why . . . . . . . . . senators were giving up so much, New Mexico Sen. Pete Dominici, [...] said, \"[We]'re looking like idiots [...]\"); and demonstrative pronouns tend to be more difficult cases in OntoNotes, for instance referring to the head of verbal phrases (e.g. [...] their material life will no doubt be a lot less . . . . . . taken of when compared to the usual both parents or one parent at home situation. [This] is indisputable). Overall, for masked mentions, precision is high across pronouns, and highest among pronoun types for third-person pronouns. For unmasked mentions, the hardest cases are demonstrative pronouns. We also report these results looking at predictions with predicted (i.e., identified by the system) mention boundaries. These are displayed in Figure 9 and 8 for M u and M m , respectively. While results are generally better with gold mention boundaries, the trends stay the same across the two setups for both masked and unmasked mentions. Finally, in Figure 7 we report the results looking at a variant of M m where instead of substituting mentions with one [MASK] token we use a sequence of three. This is to verify whether the use of a single token biases the system to be better on one-token mentions. The results show that this is not the case, as the trends found with the one-token masking are the same as those with the three-tokens masking: In particular, when a thirdperson pronoun is used the antecedent is still easier to predict than when a proper name is used, and even less than a full NP. B.2 Comparison to human predictions To elicit human judgments of referent predictability, Modi et al. (2017b) relied on mention heads rather than the complete mention (e.g., \"supermarket\" in \"the supermarket\"). For one, they constructed the cloze task by cutting a text right before the head .84 .83 .84 .76 .75 .76 .75 .71 .73 .78 .77 .77 gold .95 .91 .93 .87 .86 .86 .92 .77 .84 .91 .85 .88 M m predicted .84 .83 .83 .76 .75 .75 .74 .71 .73 .78 .76 .77 gold .95 .93 .94 .86 .88 .87 .92 .78 .85 .91 .86 .88 Table 4 : Results on OntoNotes test data (English) in document-level coreference resolution (only with unmasked mentions); P, R, F1 = precision, recall, F1 scores.  of the target mention (e.g., before \"supermarket\"), thus leaving part of the mention visible (e.g., \"the\" in this case). Moreover, they indicated candidate antecedent mentions for the human participants to consider, by listing again only the mention heads. model mentions MUC B 3 CEAF Average of metrics P R F1 P R F1 P R F1 P R F1 M u predicted To make this task suitable for standard coreference resolution we need to identify the full mention boundaries belonging to each head (not given in the original annotations). To that end we rely on 'noun chunks' identified by the spaCy library, amended by a number of heuristics, for an estimated 91% accuracy (estimated by manually checking a sample of 200 mentions for correctness). We use the identified mention boundaries as gold mention boundaries exactly as in our OntoNotes setup (Section 3). However, different from our OntoNotes setup, we mask only the head of the target mention, exactly as in the human cloze task. Table 5 reports the results of M m on the data by Modi et al. (2017b) . We deploy the system in two setups: 1) Using just the left context of the target mention, mimicking the setup used to elicit the human judgments, and 2) Using both the left and right context of the mention. In both cases,  our results improve over those reported by Modi et al. (2017b) for their best model, indicating that through our method we obtain better proxies for human discourse expectations. M m 's predictions are more aligned to those of humans when accessing both sides of the context than with only the left context, in spite of the second setup more closely resembling that used for the human data collection. Since information in the following context could not influence the human judgements (it was not available), we take this result to indicate that M m works generally better when deployed in a setup that is closer to that used during its training (recall that in training it never observed texts cropped after a masked mention), leading to suboptimal predictions when only the left context is used. We plan to explore this further in future work, by experimenting with variants to the training setup or different architectures (e.g., auto-regressive) that may improve the model's ability to resolve mentions based only on their previous contexts. C Predictability and mention form C.1 Regression with both surprisal and entropy In addition to surprisal, Tily and Piantadosi (2009) and Modi et al. (2017b) also consider the uncertainty over competitors as a feature that captures some aspect of predictability. This uncertainty, more precisely entropy, is defined as expected sur-prisal: entropy(x) := e\u2208Ex P (E x = e | c x ) \u2022 surprisal(x) Entropy will be low if the probability mass centers on one or a few entities, and high if the probability is more evenly distributed over many entities, regardless of which entity is the correct one. In principle, entropy and surprisal capture genuinely different aspects of predictability; for instance, when the model is confidently incorrect, surprisal is high while entropy is low. However, in our data, entropy and surprisal are highly correlated (r s = .87, p < .001). We did not fit regression models with both by residualising entropy to eliminate the collinearity, as our precedents did, because of the shortcomings of treating residualisation as remedy for collinearity (Wurm and Fisicaro, 2014) . Instead, we define predictability primarily by surprisal (Uniform Information Density, Levy and Jaeger 2007) in our main analysis, and report the regression with both surprisal and the non-residualised entropy as a supplementary analysis. Note that we do not intend to interpret the coefficient of surprisal or entropy in this analysis (this is not possible because they are collinear), but rather to test whether surprisal and entropy still improve goodness-of-fit to the data on top of many other shallow linguistic features. Again, the shallow features themselves may capture aspects of entropy, and are indeed correlated with entropy (though all r < .50). Table 5 : Evaluation of M m against human guesses using different amounts of context, in terms of average relative accuracy with respect to human top guess, as well as average Jensen-Shannon divergence (smaller is better) between the probability distribution of human predictions and model predictions. still matter for mention choice when controlling for the other factors, even though their statistical significance might be undermined due to the collinearity between them. Compared to the model with predictability primarily formulated as surprisal, similar effect patterns are found with entropy added, except that entropy seems to be better at distinguishing between pronoun vs. non-pronouns, and as the contexts become more uncertain, proper names and full NPs are roughly equally favored (z = \u2212.88, p = .38) over pronouns after controlling for other variables. C.2 More analyses results Figure 10 displays the predictions of mention type from the multinomial regression model, based on shallow features as well as surprisal. Each point represents a division of probability between the three levels of mention type. The corners of the triangle correspond to probability 1 for one outcome level and 0 for the other two, and the centre corresponds to probability 1/3 for each. Our model clusters most of the true pronouns (red) in the bottom left, and true full NPs (blue) in the bottom right, true proper names (green) at the top. Besides, many datapoints obtain similar division of probability, suggesting that some of them share similar pattern of features (recency, frequency etc.). Table 7 shows two linear regression models predicting mention length quantified in terms of number of tokens for each non-pronominal mention (proper name, full NP). In the first model, we regress mention length (num. of tokens) on surprisal alone. In the second one, the coefficient of surprisal decreases a bit with other shallow linguistic features added. F-tests are carried out to test if each predictor improves the fits to the data. In the fuller model, \"frequency\" and \"antecedent type\" are tested to significantly improve the model fit, above and over which surprisal still matters for mention type: longer non-pronominal expressions are favoured with surprisal increasing. We show similar effect pattern with two linear regression models predicting mention length alternatively measured in terms of characters, in Table 9 . Table 8 displays two linear regression models predicting mention length measured in terms of number of characters for each masked mention (including pronominal mentions). Compared to models for non-pronominal mentions, features like \"distance\", \"antecedent type\" matter more when predicting the mention length with pronouns included, suggesting that these features better identify the distinction between pronouns vs. non-pronouns, but probably not between shorter and longer nonpronominal expressions. Table 10 and 11 add results from likelihood-ratio chi-square tests and F-tests to Table 3 in the main text. All variables are tested to significantly improve goodness-of-fit to the data, except the feature \"target mention is subject\" in predicting mention length (num. of tokens). 11 : Two Linear regression models predicting mention length (number of tokens) of the masked mention, based on 1) surprisal alone and 2) shallow linguistic features + surprisal. F-test compares the fits of nested models. All predictors were tested to improve goodness-of-fit to the data except \"target mention is subject\". * : p < 0.001 Acknowledgements We thank Thomas Brochhagen, Andreas M\u00e4debach and Laia Mayol for their valuable comments. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 715154). This paper reflects the authors' view only, and the EU is not responsible for any use that may be made of the information it contains. Matthijs Westera also received funding from Leiden University (LUCL, SAILS). We are grateful to the NVIDIA Corporation for the donation of GPUs used for this research.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section mentions that the project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme, which is a research agency. It also mentions that Matthijs Westera received funding from Leiden University (LUCL, SAILS), which is an educational institution and does not fit into the specified categories. There is no mention of funding from defense, corporate, or foundation sources."
}