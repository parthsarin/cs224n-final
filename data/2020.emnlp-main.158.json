{
    "article": "Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickrshift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work 1 . Introduction Modern NLP systems, including ones that build on pre-trained language models (Devlin et al., 2019; Radford et al., 2019) , excel on a wide variety of tasks. These systems rely on offline (batch) training and have drawn recent criticism due to their inability to adapt to new contexts (Linzen, 2020) . In contrast, humans acquire language from evolving environments, require a small memory footprint (Mc-Clelland et al., 1995) , and can generalize their knowledge to newer tasks (Sprouse et al., 2013) . It has been suggested that humans ground perceptual 1 Code and data: https://github.com/INK-USC/ VisCOLL experience to semantically interpret symbols (Bisk et al., 2020; Harnad, 1990; Vigliocco et al., 2014) . Model model the challenge, we propose Vis-COLL, a Visually-grounded ContinuaL Learning setup, to acquire compositional phrases from streaming visual-linguistic data. Models receive a stream of paired image-caption data which has a shifting object distribution. As the end task, we employ masked token prediction of captions given the associated image, as illustrated in Fig. 1(a) . This evaluates a model's learned knowledge on composing phrases with the given context. VisCOLL captures two inter-related challenges. First, unlike previous continual learning works on image classification (Kirkpatrick et al., 2017; Zenke et al., 2017) , VisCOLL requires predicting, for example, a noun with a verb or an adjectivewhich results in a significantly large search space. As a result of this increased search space, memory based continual methods (Robins, 1995; Aljundi et al., 2019a) cannot expect to store prototypes of each visited compositions. Second, the increased search space makes it infeasible to view all possible combinations of atomic words at train time. Therefore, to succeed on VisCOLL, models should generalize to novel compositions at test time (also called composition generalization) (Lake and Baroni, 2017; Keysers et al., 2020) . In this work, we extensively study the challenges associated with VisCOLL. To facilitate the research, we construct a continuously shifting data distribution to closely resemble real-word datastream and contribute COCO-shift and Flickr-shift. We benchmark these datasets using multi-modal language modeling architectures (Tan and Bansal, 2019; Su et al., 2020) which achieve state-of-art performance on multiple vision-language tasks. In particular, we don't use any pre-training, instead train randomly initialized models on streaming data using continual learning algorithms (Robins, 1995; Rolnick et al., 2019; Aljundi et al., 2019a) and evaluate their resistance to forgetting and compositional generalization. We quantify the performance and forgetfulness of trained models and evaluate on a novel test split to measure compositional generalization, as shown in Fig. 1 (b). Our proposed VisCOLL benchmark reveals that the gains observed in image classification tasks from state-of-art continual learning algorithms fail to transfer to VisCOLL even with increased memory. On the other hand, composition generalization remains challenging even for offline-training. To summarize, our contributions are: (i) we propose VisCOLL, a task aimed at continual learning of compositional semantics from visually grounded text inputs (ii) we contribute two datasets COCOshift and Flickr-shift to study VisCOLL and benchmark them with multi-modal language models (iii) we show that existing continual learning algorithms fail at learning compositional phrases and provide potential future research direction. Related Works Continual Learning. A major challenge in continual learning is to alleviate catastrophic forgetting (Robins, 1995) . Several recent works (Greco et al., 2019; Wang et al., 2019; de Masson d'Autume et al., 2019) study the challenge in the context of NLP. Existing continual learning algorithms can be broadly classified into memorybased approaches (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2019b) , pseudo-replay based ap-proaches (Shin et al., 2017) , regularization based approaches (Kirkpatrick et al., 2017; Zenke et al., 2017; Nguyen et al., 2018) and architecture based approaches (Rusu et al., 2016) . However, these works are mainly designed for image classification tasks where the training data has \"clear\" task boundaries-i.e., training stream are partitioned into disjoint subsequences. In contrast, task boundaries in VisCOLL are unknown and \"smooth\" (i.e., with gradual transitions between tasks)-a setting that is closer to real-world situations. Thus, VisCOLL rules out many continual learning algorithms which require explicit task identity and boundary (Kirkpatrick et al., 2017; Rusu et al., 2016) . Modeling Language Compositionality. Capturing compositionality in language has been a long challenge (Fodor et al., 1988) for neural networks. Recent works explore the problem with compositional generalization on synthetic instruction following (Lake and Baroni, 2017), text-based games (Yuan et al., 2019) , visual question answering (Bahdanau et al., 2019) , and visually grounded masked word prediction (Sur\u00eds et al., 2019) . In particular, Li et al. (2020) study a closely related task of continual learning of sequence prediction for synthetic instruction following. However, their techniques for separating semantics and syntax is restricted to text-only case. Nguyen et al. (2019) investigate continual learning of image captioning, but make strong assumptions on the structure of the data stream, and do not evaluate compositionality. Different from these, our work focuses on learning compositional language (e.g., phrases) in a continual learning setup. We create realistic training streams to simulate shifting data distribution, with systematic evaluation of compositionality learned in the models. We aim at improving model's ability of acquiring language from real-world streaming data with a low-memory footprint. The VisCOLL Task Overview. There are two design considerations for VisCOLL: compositionality and continual learning. To test compositionality, we choose visually grounded masked language modeling where the model needs to compose atomic words to describe complex and novel visual scenes. To simulate a realistic continual learning setup, we construct a dynamic environment where the training data comes in as a non-stationary data stream without clear \"task\" boundaries. Since the goal is to simulate lan-  At train time, the model receives a stream of masked captions (highlighted in text) with their associated image. We use the noun appearing in the masked token as the \"task\" subsequently used to create a continuously shifting data distribution. We further evaluate the model's performance every fixed time interval to quantify \"forgetting\". At test time, the models receives a seen composition or a novel composition of seen words.  In most of existing continual learning settings, task identities are obtainable and can be used to define task boundaries. In contrast, models in Vis-COLL is agnostic to task identities (task-free), and fed/trained by a gradually shifting data distribution. guage acquisition from scractch, VisCOLL models shouldn't initialize weights from a pre-trained network. We introduce details of our task setup in the rest of the section. Prediction End-Task. We employ visually grounded masked language modeling as the prediction task in VisCOLL. An input instance to this task consists of an image x img , its object bounding boxes x bbox (without object labels), and the caption text x text , where a contiguous text span in x text is masked with MASK tokens which the model needs to fill. The masked text span x label always includes a noun and optionally includes verbs or adjectives. We define each noun, verb, and adjective as an atom, and evaluate the model in both \"seen\" and \"novel\" composition setting of nouns and verbs/adjectives. For instance, we may test whether the model successfully predicts \"red apples\" (adjective+noun) when the model has seen examples that involve \"red\" and \"apples\" separately (see Figure 2 for an illustration). Training Data Stream. Unlike traditional offline training setups where a model visits the training examples for multiple passes, we study an online continual learning setup, where the model visits a non-stationary stream of data. We assume the data distribution changes gradually: for example, the model may see more \"apples\" in the beginning, and see less of them later. Unlike prior continual learning benchmarks, we do not assume strict task boundaries, i.e., sudden distribution changes. We illustrate this distinction in Figure 3 . Formally, at each time step t, the model receives a small mini-batch of stream examples {X i } B\u22121 i=0 where X i =(x i img , x i bbox , x i text , x i label ) whose distribution changes over time, i.e., P (X i , t i ) = P (X i , t j ) where the time step t i = t j . Note that our formulation rules out continual learning algorithms that make use of information about task boundaries. Section 4 formally introduces the data stream construction process. Evaluation. In addition to the final performance, we also evaluate the model performance every fixed time interval and compute its forgetting as the performance loss over time. For compositional generalization, a novel composition split is used. Details will be covered in the following Sections 4 and 6. Dataset Construction We construct our data streams using two popular vision-language datasets: COCO-captions (Chen et al., 2015) and Flickr30k Entities (Plummer et al., 2015) which provide multiple captions for each image in MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014) respectively. We call the resulting datasets COCO-shift and Flickr-shift (see Table 1 for dataset statistics). Constructing a dataset for VisCOLL involves two key steps: (i) identify the phrase to be masked which involves a noun and associated verbs and adjectives (ii) create a non-stationary data-stream. Masking Tokens. First, we append part-of-speech tags (POS) to each caption using Stanza (Qi et al., 2020) . For Flickr30k Entities, we use the annotated noun-phrases as mask tokens. For COCO-captions, we identify text spans with a regular expression chunker with the following regular expression. CHUNK: <DT>?<JJ|VBG|VBN> * <NN|NNS>+ <VB|VBD|VBG|VBN|VBP|VPZ> * The resulting text span always includes a noun, and optionally include a determinator and an adjective and verb before or after the noun. To construct a data-stream, we define a \"task\" as the object being referred to in the textual input data. For Flickr30k Entities, this is simply the lemmatized noun in the masked span. For COCOcaptions, we further map the lemmatized nouns to the 80 object categories defined in MSCOCO using a synonym table provided in (Lu et al., 2018) . Non-Stationary Data-Stream. With a set of \"tasks\" defined as T , we let each task T i \u2208 T gradually introduced in the stream, then gradually fade out. We generate a random permutation of all K tasks (T 1 , T 2 , T 3 , ..., T K ) as the order in which the centroid (mode) of each task distribution arrives in the stream. Each task proposes a task distribution for itself, which is a gaussian with \u00b5 i = |D i |/2 + k<i |D k | and \u03c3 i = |D i |/2 , where D i is the set of training instances for task i. \u00b5 i and \u03c3 i roughly determines the centroid and the spread of the distribution of each task. Finally, the algorithm greedily tries to put the proposed number of instances into each time interval to construct the stream. As a result, the constructed data stream has a gradually shifting task distribution without strict boundaries. Figure 4 illustrates the task distribution in our constructed data streams. For COCO-shift, we separate out instances related to 5,000 images from the official validation set as our test set, and the rest as the test set. For Flickr-shift, we use the official train, validation and the test split. Note that the \"task\" is only used as an identifier of data distribution for constructing the dataset; the task identities are not revealed to models and the way we construct the data streams ensures there are no clear task boundaries. Test Split of Novel Compositions. We measure compositional generalization by evaluating on a disjoint set of noun-adjective or noun-verb compositions. We use the compositional test split of COCO dataset by Nikolaus et al. (2019) and remove images related to predefined 24 concept pairs (e.g., black cat, standing child) from the training, validation and the regular test set. The test split is referred to as the compositional test set, and the rest is referred to as the regular test set. Methods To benchmark on VisCOLL and study the challenges it poses on model learning, we establish several continual learning baselines. We use visuallanguage encoder models (Sec. 5.1) for masked token predictions. These models are trained from scratch (i.e., randomly initialized) with continual learning algorithms (Sec. 5.2) to dissuade catas- For the input image, we first extract image-level and object-level features using a FasterRCNN. These features along with the masked caption are passed to a cross-modal transformer (in our case LXMERT and VLBERT) to predict the masked tokens. The model is randomly initialized without pre-trained weights, and trained end-to-end with cross-entropy-loss. trophic forgetting. Architectures of Visual Language Model Recall that our end-task is masked token prediction where the input is an image and a caption with masked out tokens. Since the task of masked token prediction is used as a pre-training method in almost all multi-modal masked language models, we choose two such model architectures, VLBERT (Su et al., 2020) and LXMERT (Tan and Bansal, 2019), as encoder models but expect similar conclusions with other models like ViLBERT (Lu et al., 2019) or UNITER (Chen et al., 2019) . Since we seek to simulate the language acquisition process, we remove the pre-trained weights from the models and randomly initialize the model weights for both the visual and language transformers. For both base models, we first extract image and object features in ground truth bounding boxes with a Faster-RCNN (Ren et al., 2015) model with Resnet-101 backbone (He et al., 2015) pretrained on Visual Genome (Krishna et al., 2017) dataset using only object-detection (without attributes). The ground-truth object labels are not provided to the feature extractor model. The extracted features are then passed to the base models along with the caption with masked text span replaced with [MASK] tokens. Finally we compute the cross entropy loss with model predictions. We illustrate our base model in Figure 5 . Compared Methods Non-continual Learning Comparators. The most common way of training a deep learning model is to perform gradient updates on a minibatch of independently and identically distributed samples. Since the model has access to the complete data (offline mode), the process is repeated multiple times (multiple epochs); we call this offline training. To decouple the effect of training for multiple epochs, we report offline (one pass) where we restrict to a single pass over the data. Due to the absence of catastrohpic forgetting, the results are generally better than continual learning scenarios. Note that these two methods do not address the VisCOLL task and potentially indicate and upper-bound of performance. Continual Learning Methods. In a continual learning setup like VisCOLL, the distribution of training examples is non-stationary, and due to limited memory only a part of the data can be stored. In general, simply performing gradient updates after receiving a mini-batch (also called Vanilla Online Gradient Descent), leads to catastrophic forgetting (Robins, 1995) . For VisCOLL, we focus on memory-based continual learning algorithms. These can be easily adapted to our setup as they don't require any task identifiers (not available in VisCOLL). We leave the exploration of regularization based approaches (Hsu et al., 2018) to future work. (1) Experience Replay (ER) (Robins, 1995) randomly stores visited examples in a fix-sized memory called the replay memory, and these stored examples can later be randomly sampled for retraining. Similar techniques have been used in reinforcement learning (Schaul et al., 2016; Rolnick et al., 2019) . We apply reservoir sampling (Vitter, 1985) to decide examples to store and replace from the memory. The algorithm ensures each visited example has the same probability to be stored in the memory. ( 2 ) Average Gradient Episodic Memory (AGEM) (Chaudhry et al., 2019a) . Unlike ER, AGEM projects the gradients to a direction of non-increasing average loss computed on a random subset in the memory to alleviate forgetting. ( 3 ) ER with Maximally Interfering Retrieval (ER-MIR) (Aljundi et al., 2019a) extends ER by selecting examples that are most likely to be forgotten in the next one update. We further implement a method, ER-MIR max as a variation of ER-MIR specific to our compositional prediction setting; which, instead of selecting the most likely forgotten phrase, selects the phrases containing the most likely forgotten words. It prevents the importance of an example get underestimated when the example contains mostly easy words and a few forgettable words. Experiments With the VisCOLL task formulation in place, we study: (i) performance of continual learning algorithms on VisCOLL. (ii) effect of the large search space on memory-based continual learning algorithms. (iii) performance on generalizing to novel compositions. (iv) effect of replay memory management strategies. We first describe our implementation details and metrics, and present our results with findings. Implementation Details. For both VLBERT and LXMERT, we use a transformer with 6 layers, with 384 hidden units and 6 attention heads each. Note that all the parameters are learned from scratch without using pretrained weights. For all continual learning algorithms, we use a memory size of 1k and 10k, corresponding to nearly 0.2% and 2% of data for the two datasets. We report average over 3 runs with the fixed stream and the same set of random seeds. See Appendix for more details. General Evaluation Metrics. We employ Perplexity (PPL) as our primary metrics (lower is better) (Mikolov et al., 2011) . Given a masked text span W =[w 1 , ..., w N ] and the model's prediction probability output P (W ), the log-perplexity is defined as, P P L(W ) = \u2212 1 N log P (W ). We report the perplexity in the log scale. Besides, we also use sentence-level BLEU (Papineni et al., 2002) . Study of Continual Learning To analyze the continual learning ability of our model, we use two metrics: (i) final perplexity and BLEU: the test set perplexity and BLEU scores when the training ends (ii) forgetting metric: the averaged perplexity increase over all tasks when the training ends compared to the checkpoints (almost) all of training data of a given task is observed. Mathematically, the forgetting metric is calculated as f avg = 1 |T | k\u2208T P P L T (D k ) \u2212 P P L c k (D k ), where c k = arg min c i \u2208C P P L c i (D k ). T is the set of all tasks, and C is the set of all checkpoints; P P L c k (D k ) is the averaged perplexity over all test examples of task k at the checkpoint c k , and T is the time step when the training ends. We expect a well-performing method to achieve low final perplexity, high BLEU scores, and low forgetting.   Tables 2 compares base-models with various continual strategies on the corresponding regular test splits of COCO-shift and Flickr-shift. We discuss our findings below. Non-stationary vs i.i.d Data Distribution. Across both datasets, it is evident that models trained with the non-stationary distribution (closer to what is observed in the real-world) largely under-perform compared to their i.i.d offline training counterpart at the single epoch setting (20-40 points difference in BLEU scores). This emphasizes that catastrophic forgetting is prevalent in our constructed non-stationary data stream. Performance of Continual Learning Methods. Despite its simplicity, ER achieves extremely competitive results, scoring within 1-2 PPL of the best performing model. While AGEM achieves very appealing final perplexity on Flickr-shift dataset at the 1k memory setting (almost 0.5 points better than alternatives), we find the corresponding BLEU is worse. Given that perplexity evaluates over output probabilities, it is likely that AGEM makes less confident wrong predictions. We also find ER-MIR and its variant ER-MIR max occasionally outperforms ER, but the improvements are inconsistent over base-models and datasets. This is in stark contrast to continual learning benchmarks on image classification where algorithms like AGEM and ER-MIR achieve SoTA performance. In Fig. 8 (a)(b), we illustrate the change of perplexity over time for selected time steps in different methods. We notice that with a memory size of 10k, on average the forgetting metric for ER is close to or less than zero most of the time. This implies the performance of each task remains constant or improves over what was initially learned.  Replay Memory Requirements Compared to Existing Benchmarks. It should be noted that even with a memory of 10k examples, the performance of continual learning algorithms are far from the i.i.d setting. In contrast to the popular continual learning benchmarks (Kirkpatrick et al., 2017; Zenke et al., 2017) , where storing only a few examples for each class is believed to be sufficient for a good performance (Chaudhry et al., 2019b) . Effect of Multi-Modal Data. To decouple the gains obtained from visual and textual modality, we construct a text-only baseline by zeroing out the image-inputs in our base models and train using ER with memory size 10k. From Table 2 , we find across all cases, text-only baseline is outperformed by its multi-modal counterpart (5 points on BLEU) suggesting information from both image and captions is necessary to perform well on VisCOLL. Our findings underscore the challenges imposed by VisCOLL and encourages closer examination towards existing continual learning benchmarks. Study of Compositional Generalization To measure compositionality captured by models, in addition to a regular test set, we evaluate on the compositional test set which consists of novel noun-adjective and noun-verb pairs. We compare the performance with seen compositions sharing the same set of atomic words in the regular test set. For a fair comparison with novel splits, we compare the performance on held-out novel pairs with a subset of the regular test-set sharing the same set of atomic words. Overall Compositional Generalization Results. We plot the results in Figure 6 . We note that the performance on novel compositions drops across all cases implying composition generalization is very difficult for visual language transformers. Interestingly, offline (one pass) outperforms offline training on novel compositions, suggesting the latter is over-fitting to the \"seen\" case. Analyzing Performance on Old Compositions. In an online setting, we further probe the model's predictive performance on old seen compositions. Interestingly, we find that the performance is largely dependent on the atomic words used in the compositions. For instance, the performance drop on predicting \"black\" in the composition \"black dog\" is relative small (Figure 8 (c)) compared to predicting \"big\" in \"big dog\" (Figure 8(d) ). Study of Memory Management Strategies We further study two memory scheduling strategies to account for a limited memory but large search space. Recall that the reservoir sampling applied our main experiments keeps each visited example has the same probability to be stored in the memory. We study two methods targeting storing more useful examples, aiming at: (i) storing diverse compositions, and (ii) prioritizing likely forgotten words. We first propose target word distributions p tgt in the memory. For (i), the target probability of each word is set proportional to the square root of its frequency in the visited stream. Thus, highly frequent words would take a smaller portion compared to reservoir sampling where the word distribution in the memory is linear to its frequency in the visited stream, leaving space for storing more diverse examples. We call this strategy Balanced-sqrt. For (ii), the target probability is proportional to its frequency in the stream multiplied by its forgetting estimated during training (i.e., loss increase). We call this strategy Balanced-forget. For both strategies, given the word distribution in the memory p mem and target word distributions p tgt , we minimize the KL-divergence KL(p mem ||p tgt ). Thus, each time an example is received from the stream, we choose the memory example which if replaced causes the largest decrease in KL-divergence. If there are no such memory examples that let KL-divergence decrease, we discard the example. The results are compared in Figure 9 . We find that (i) diversifying storage improves performance at the early stage of the stream but not in the later stages; (ii) prioritizing words likely to be forgotten does not improve performance. Thus, future works should find a balance between storing more diverse or important examples and respecting original data distribution. Conclusion We propose VisCOLL, a novel continual learning setup for visually grounded language acquisition. VisCOLL presents two main challenges: continual learning and compositionality generalization. To facilitate study on VisCOLL, we generate continuously shifting data-stream to construct two datasets, namely COCO-shift and Flickr-shift, and establish evaluation protocols. We benchmark our proposed datasets with extensive analysis using state-of-theart continual learning methods. Experiments reveal that continual learning algorithms struggle at composing phrases which have a very large search space, and show very limited generalization to novel compositions. Future works include looking into models and continual learning algorithms to better address the challenge. Acknowledgements A Details of Dataset Construction Heldout Phrases. We put the complete list of 24 noun-verb and noun-adjective compositions used as novel compositions in Table 4 , which are provided in (Nikolaus et al., 2019) . B Hyperparameter Settings Since the complete stream should not be assumed known apriori in the online learning setting, following prior work (Chaudhry et al., 2019a) , we use a small portion (10%) of the training data and the validation set to perform hyperparameter search. We use AdamW optimizer (Loshchilov and Hutter, 2019) throughout the experiements. We tune the learning rate based on the validation performance on the Vanilla method averaged over 3 runs. For a fair comparison in the online learning setup, we use the same learning rates for all methods. The learning rate is selected from {2e \u22124 , 1e \u22124 , 5e \u22125 } and decided as 1e \u22124 . We set the batch size to 32 throughout the experiments. For ER, ER-MIR and ER-MIR max , at each training iteration, we replay the same number of examples from the memory as the examples received from the stream (i.e., training batch size), following the convention in recent works (Aljundi et al., 2019a; Chaudhry et al., 2019b) . AGEM, unlike ER, requires a larger set of memory examples to compute regularizations. We set the numbers to 80 and 64 respectively for COCO- shift and Flickr-shift. While larger numbers can be preferable, it introduces significant time and resource consumption overhead in our problem setup, which is much larger in scale compared to existing continual learning benchmarks. Similarly, ER-MIR and ER-MIR max introduce a hyperparameter for the size of the \"candidate set\" to retrieve examples that are most likely to be forgotten. We set the hyperparameters as 80 and 64 for COCO-shift and Flickr-shift respectively. C Effect of Data Order Data order has been known to affect performance in continual learning (Greco et al., 2019) . To illustrate this point, we conduct a simple experiment where the task centroids are sorted in the ascending or descending order. We show the log perplexity metrics in Table 5 . The results show a significant log-perplexity gap, which verifies that data order may significantly affect performance. We leave more in-depth analysis as future works. Note that throughout our main experiments, the task order is fixed as a random order. D Infrastructures and Statistics We use PyTorch 1.0.0 (Paszke et al., 2019) with CUDA toolkit version 10.1. We train our models with NVIDIA 2080 Ti GPUs. Our VLBERT models have 23,564,040 trainable parameters and LXMERT models have 58,614,794 trainable parameters. We report the average training time over a single pass of the stream in table 3.",
    "abstract": "Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickrshift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work 1 .",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "5",
    "year": "2020",
    "month": "November",
    "title": "Visually Grounded Continual Learning of Compositional Phrases"
}