{
    "article": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014) , named entity recognition (Guo et al., 2014) , and sentiment analysis (Socher et al., 2013) . The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010) . Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995) . Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014) . Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors ( \u00a72). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015) , the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an \"overcomplete\" representation (Olshausen and Field, 1997) . Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) , and to increase stability in the presence of noise (Donoho et al., 2006) . Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia) . Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014) . The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex-ical semantic theories. Using a number of stateof-the-art word vectors as input, we find consistent benefits of our method on a suite of standard benchmark evaluation tasks ( \u00a73). We also evaluate our word vectors in a word intrusion experiment with humans (Chang et al., 2009) and find that our sparse vectors are more interpretable than the original vectors ( \u00a74). We anticipate that sparse, binary vectors can play an important role as features in statistical NLP models, which still rely predominantly on discrete, sparse features whose interpretability enables error analysis and continued development. We have made an implementation of our method publicly available. 1 Sparse Overcomplete Word Vectors We consider methods for transforming dense word vectors to sparse, binary overcomplete word vectors. Fig. 1 shows two approaches. The one on the top, method A, converts dense vectors to sparse overcomplete vectors ( \u00a72.1). The one beneath, method B, converts dense vectors to sparse and binary overcomplete vectors ( \u00a72.2 and \u00a72.4). Let V be the vocabulary size. In the following, X \u2208 R L\u00d7V is the matrix constructed by stacking V non-sparse \"input\" word vectors of length L (produced by an arbitrary word vector estimator). We will refer to these as initializing vectors. A \u2208 R K\u00d7V contains V sparse overcomplete word vectors of length K. \"Overcomplete\" representation learning implies that K > L. Sparse Coding In sparse coding (Lee et al., 2006) , the goal is to represent each input vector x i as a sparse linear combination of basis vectors, a i . Our experiments consider four initializing methods for these vectors, discussed in Appendix A. Given X, we seek to solve arg min D,A X \u2212 DA 2 2 + \u03bb\u2126(A) + \u03c4 D 2 2 , ( 1 ) where D \u2208 R L\u00d7K is the dictionary of basis vectors. \u03bb is a regularization hyperparameter, and \u2126 is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009) . To obtain sparse word representations we will impose an 1 penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel ( \u00a72.3): arg min D,A V i=1 x i \u2212 Da i 2 2 + \u03bb a i 1 + \u03c4 D 2 2 (2) where m i denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015) . To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002) . Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and a i are nonnegative. Here, we apply this constraint only to the representation vectors {a i }. Thus, the new objective for nonnegative sparse vectors becomes: arg min D\u2208R L\u00d7K \u22650 ,A\u2208R K\u00d7V \u22650 V i=1 x i \u2212Da i 2 2 +\u03bb a i 1 +\u03c4 D 2 2 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as explained next. Optimization We use online adaptive gradient descent (Ada-Grad; Duchi et al., 2010) for solving the optimization problems in Eqs. 2-3 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014) . However, directly applying stochastic subgradient descent to an 1 -regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009) , which keeps track of the online average gradient at time t with respect to a i . We define : \u1e21t = 1 X L V K x V D A K K x V D V K B Sparse \u03b3 = \u2212sign(\u1e21 t,i,j ) \u03b7t G t,i,j (|\u1e21 t,i,j | \u2212 \u03bb), where G t,i,j = t t =1 g 2 t ,i,j . Now, using the average gradient, the 1 -regularized objective is optimized as follows: a t+1,i,j = 0, if |\u1e21 t,i,j | \u2264 \u03bb \u03b3, otherwise (4) where, a t+1,i,j is the jth element of sparse vector a i at the tth update and \u1e21t,i,j is the corresponding average gradient. For obtaining nonnegative sparse vectors we take projection of the updated a i onto R K \u22650 by choosing the closest point in R K \u22650 according to Euclidean distance (which corresponds to zeroing out the negative elements): a t+1,i,j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0, if |\u1e21 t,i,j | \u2264 \u03bb 0, if \u03b3 < 0 \u03b3, otherwise (5) Binarizing Transformation Our aim with method B is to obtain word representations that can emulate the binary-feature 1975, 1976, 1968, 1970, 1977, 1969 dress, shirt, ivory, shirts, pants upscale, affluent, catering, clientele space designed for various NLP tasks. We could state this as an optimization problem: X L \u03bb \u03c4 K % Sparse arg min D\u2208R L\u00d7K B\u2208{0,1} K\u00d7V V i=1 x i \u2212 Db i 2 2 + \u03bb b i 1 1 + \u03c4 D 2 2 (6) where B denotes the binary (and also sparse) representation. This is an mixed integer bilinear program, which is NP-hard (Al-Khayyal and Falk, 1983) . Unfortunately, the number of variables in the problem is \u2248 KV which reaches 100 million when V = 100, 000 and K = 1, 000, which is intractable to solve using standard techniques. A more tractable relaxation to this hard problem is to first constrain the continuous representation A to be nonnegative (i.e, a i \u2208 R K \u22650 ; \u00a72.2). Then, in order to avoid an expensive computation, we take the nonnegative word vectors obtained using Eq. 3 and project nonzero values to 1, preserving the 0 values. Table 2 shows a random set of word clusters obtained by (i) applying our method to Glove initial vectors and (ii) applying k-means clustering (k = 100). In \u00a73 we will find that these vectors perform well quantitatively. Hyperparameter Tuning Methods A and B have three hyperparameters: the 1 -regularization penalty \u03bb, the 2 -regularization penalty \u03c4 , and the length of the overcomplete word vector representation K. We perform a grid search on \u03bb \u2208 {0.1, 0.5, 1.0} and K \u2208 {10L, 20L}, selecting values that maximizes performance on one \"development\" word similarity task (WS-353, discussed in \u00a7B) while achieving at least 90% sparsity in overcomplete vectors. \u03c4 was tuned on one collection of initializing vectors (Glove, discussed in \u00a7A) so that the vectors in D are near unit norm. The four vector representations and their corresponding hyperparameters selected by this procedure are summarized in Table 1 . There hyperparameters were chosen for method A and retained for method B. Experiments Using methods A and B, we constructed sparse overcomplete vector representations A, starting from four initial vector representations X; these are explained in Appendix A. We used one benchmark evaluation (WS-353) to tune hyperparameters, resulting in the settings shown in Table 1 ; seven other tasks were used to evaluate the quality of the sparse overcomplete representations. The first of these is a word similarity task, where the score is correlation with human judgments, and the others are classification accuracies of an 2regularized logistic regression model trained using the word vectors. These tasks are described in detail in Appendix B. Effects of Transforming Vectors First, we quantify the effects of our transformations by comparing their output to the initial (X) vectors. Table 3 shows consistent improvements of sparsifying vectors (method A). The exceptions are on the SimLex task, where our sparse vectors are worse than the skip-gram initializer and on par with the multilingual initializer. Sparsification is beneficial across all of the text classification tasks, for all initial vector representations. On average across all vector types and all tasks, sparse overcomplete vectors outperform their corresponding initializers by 4.2 points. 2  Binarized vectors (from method B) are also usually better than the initial vectors (also shown in Table 3 ), and tend to outperform the sparsified variants, except when initializing with Glove. On average across all vector types and all tasks, binarized overcomplete vectors outperform their corresponding initializers by 4.8 points and the continuous, sparse intermediate vectors by 0.6 points. From here on, we explore more deeply the sparse overcomplete vectors from method A (denoted by A), leaving binarization and method B aside. Effect of Vector Length How does the length of the overcomplete vector (K) affect performance? We focus here on the Glove vectors, where L = 300, and report average performance across all tasks. We consider K = \u03b1L where \u03b1 \u2208 {2, 3, 5, 10, 15, 20}. Figure 2 plots the average performance across tasks against \u03b1. The earlier selection of K = 3, 000 (\u03b1 = 10) gives the best result; gains are monotonic in \u03b1 to that point and then begin to diminish. Alternative Transformations We consider two alternative transformations. The first preserves the original vector length but  achieves a binary, sparse vector (B) by applying: b i,j = 1 if x i,j > 0 0 otherwise (7) The second transformation was proposed by Guo et al. (2014) . Here, the original vector length is also preserved, but sparsity is achieved through: a i,j = \uf8f1 \uf8f2 \uf8f3 1 if x i,j \u2265 M + \u22121 if x i,j \u2264 M \u2212 0 otherwise (8) where M + (M \u2212 ) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012) , we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability, focusing on individual dimensions. Word Intrusion Word intrusion experiments seek to quantify the extent to which dimensions of a learned word representation are coherent to humans. In one instance of the experiment, a human judge is presented with five words in random order and asked to select the \"intruder.\" The words are selected by the experimenter by choosing one dimension j of the learned representation, then ranking the words on that dimension alone. The dimensions are chosen in decreasing order of the variance of their values across the vocabulary. Four of the words are the top-ranked words according to j, and the \"true\" intruder is a word from the bottom half of the list, chosen to be a word that appears in the top 10% of some other dimension. An example of an instance is: Table 5 : Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss' \u03ba (Davies and Fleiss, 1982) . (The last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different initializers X. We selected the 25 dimensions d in each case. Each of the 100 instances per condition (initial vs. sparse overcomplete) was given to three judges. Results in Table 5 confirm that the sparse overcomplete vectors are more interpretable than the dense vectors. The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss' \u03ba increases from \"fair\" to \"moderate\" agreement (Landis and Koch, 1977) . Qualitative Evaluation of Interpretability If a vector dimension is interpretable, the topranking words for that dimension should display semantic or syntactic groupings. To verify this qualitatively, we select five dimensions with the highest variance of values in initial and sparsified GC vectors. We compare top-ranked words in the dimensions extracted from the two representations. The words are listed in Table 6 , a dimension per row. Subjectively, we find the semantic groupings better in the sparse vectors than in the initial vectors. Figure 3 visualizes the sparsified GC vectors for six words. The dimensions are sorted by the average value across the three \"animal\" vectors. The animal-related words use many of the same dimensions (102 common active dimensions out of 500 total); in constrast, the three city names use combat, guard, honor, bow, trim, naval 'll, could, faced, lacking, X Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006) . Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009) . Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004) , like POS tagging (Ganchev et al., 2009) , dependency parsing (Martins et al., 2011) , text classification (Yogatama and Smith, 2014) , and representation learning (Bengio et al., 2013) . Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007) , and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012) . V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V466 V465 V128 V11 V413 V98 V131 V445 V199 V475 V208 V431 V299 V357 V149 V80 V247 V231 V42 V44 V376 V152 V74 V254 V141 V341 V349 V234 V55 V477 V272 V217 V457 V57 V159 V223 V310 V436 V325 V211 V117 V360 V483 V363 V439 V403 V119 V329 V83 V371 V424 V179 V214 V268 V38 V102 V93 V89 V12 V172 V173 V285 V344 V78 V227 V426 V430 V241 V384 V460 V347 V171 V289 V380 V8 V2 V3 V5 V6 V7 V10 V14 V15 V16 V17 V18 V19 V20 V21 V22 V25 V26 V28 V29 V30 V31 V32 V33 V35 V36 V37 V39 V40 V41 V43 V45 V47 V49 V50 V51 V52 V54 V56 V58 V59 V60 V63 V64 V65 V67 V68 V69 V70 V72 V75 V77 V81 V87 V90 V92 V94 V99 V101 V103 V105 V106 V108 V110 V111 V116 V118 V122 V123 V125 V130 V132 V133 V136 V137 V138 V139 V140 V143 V144 V147 V148 V150 V155 V158 V160 V162 V165 V166 V167 V168 V169 V170 V174 V175 V178 V180 V181 V182 V183 V185 V188 V189 V190 V191 V193 V194 V195 V196 V202 V203 V204 V205 V212 V213 V215 V218 V220 V224 V226 V228 V232 V233 V235 V236 V238 V239 V240 V242 V243 V244 V248 V249 V250 V251 V252 V253 V255 V258 V259 V260 V261 V262 V263 V264 V265 V266 V271 V273 V274 V278 V282 V284 V287 V288 V290 V292 V293 V294 V296 V300 V302 V304 V307 V308 V311 V312 V313 V314 V316 V317 V318 V319 V320 V321 V322 V323 V327 V330 V331 V333 V334 V336 V338 V340 V343 V345 V346 V352 V356 V361 V362 V366 V368 V369 V370 V372 V373 V375 V377 V378 V381 V382 V383 V385 V386 V387 V388 V389 V390 V391 V392 V394 V395 V396 V398 V399 V400 V401 V402 V404 V405 V406 V407 V408 V409 V410 V412 V414 V415 V416 V417 V418 V419 V420 V422 V423 V425 V427 V428 V429 V433 V434 V435 V437 V441 V442 V444 V446 V449 V450 V451 V452 V453 V455 V456 V458 V459 V461 V462 V463 V464 V467 V468 V469 V471 V472 V478 V479 V480 V481 V482 V484 V485 V486 V488 V489 V490 V491 V492 V493 V494 V495 V497 V499 V500 V501 V487 V200 V326 V4 V121 V267 V230 V438 V134 V97 V104 V351 V219 V13 V88 V129 V286 V229 V350 V96 V107 V153 V145 V154 V34 V301 V374 V109 V397 V156 V161 V297 V115 V151 V245 V447 V53 V337 V79 V448 V283 V443 V201 V393 V365 V48 V126 V257 V246 V295 V120 V367 V27 V184 V209 V306 V269 V124 V470 V112 V187 V62 V474 V354 V454 V279 V146 V275 V221 V207 V71 V335 V73 V85 V440 V95 V23 V225 V411 V328 V305 V198 V163 V9 V135 V315 V142 V498 V291 V86 V476 V210 V359 V84 V100 V309 V176 V216 V432 V206 V421 V276 V237 V61 V157 V364 V127 V66 V256 V280 V113 V298 V197 V496 Conclusion We have presented a method that converts word vectors obtained using any state-of-the-art word vector model into sparse and optionally binary word vectors. These transformed vectors appear to come closer to features used in NLP tasks and outperform the original vectors from which they are derived on a suite of semantics and syntactic evaluation benchmarks. We also find that the sparse vectors are more interpretable than the dense vectors by humans according to a word intrusion detection test. Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word's Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300. 4   Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012) . These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50. 5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48. 6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001) , which contains 353 pairs of English words that have been assigned similarity ratings by humans. This dataset is used to tune sparse vector learning hyperparameters ( \u00a72.5), while the remaining of the tasks discussed in this section are completely held out. A more recent dataset, SimLex-999 (Hill et al., 2014) , has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman's rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002) . The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider binary categorization tasks from the 20 Newsgroups dataset. 7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014) : (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validation accuracy is reported on the remaining nine folds. Acknowledgments We thank Alona Fyshe for discussions on vector interpretability and three anonymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300. 3",
    "abstract": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "156",
    "year": "2015",
    "month": "July",
    "title": "Sparse Overcomplete Word Vector Representations"
}