{
    "article": "Behavioural testing-verifying system capabilities by validating human-designed inputoutput pairs-is an alternative evaluation method of natural language processing systems proposed to address the shortcomings of the standard approach: computing metrics on held-out data. While behavioural tests capture human prior knowledge and insights, there has been little exploration on how to leverage them for model training and development. With this in mind, we explore behaviour-aware learning by examining several fine-tuning schemes using HATECHECK, a suite of functional tests for hate speech detection systems. To address potential pitfalls of training on data originally intended for evaluation, we train and evaluate models on different configurations of HATECHECK by holding out categories of test cases, which enables us to estimate performance on potentially overlooked system properties. The fine-tuning procedure led to improvements in the classification accuracy of held-out functionalities and identity groups, suggesting that models can potentially generalise to overlooked functionalities. However, performance on held-out functionality classes and i.i.d. hate speech detection data decreased, which indicates that generalisation occurs mostly across functionalities from the same class and that the procedure led to overfitting to the HATECHECK data distribution. Introduction The standard method for evaluating natural language processing (NLP) systems-computing metrics on held-out data-may be a good indicator of model correctness, but tends to overestimate performance in the wild (Ribeiro et al., 2020) , does not indicate possible sources of models failure (Wu et al., 2019) and overlooks potential dataset biases (Niven and Kao, 2019; McCoy et al., 2019; Zellers et al., 2019) . Behavioural testing of NLP models (R\u00f6ttger et al., 2021; Ribeiro et al., 2020) has been proposed as an additional evaluation methodology, where system functionalities are validated by checking specific input-output behaviour of the system. This is done through challenge sets: expert-crafted inputoutput pairs that capture human prior knowledge and intuition about how an agent should perform the task (Linzen, 2020) and enable systematic verification of system capabilities (Belinkov and Glass, 2019) . For the purposes of this paper, we consider a behavioural test suite to be a collection of test cases, input-output pairs that describe an expected behaviour. Each case assesses a specific functionality, which are grouped into functionality classes. For example, test cases in HATECHECK (R\u00f6ttger et al., 2021) , a test suite for hate speech detection, include (\" [IDENTITY] belong in a zoo.\", hateful), (\"No [IDENTITY] deserves to die.\", non-hateful) and (\"I had this queer feeling we were being watched\", non-hateful). These cases assess the functionalities: implicit derogation of a protected group or its members, non-hate expressed using negated hateful statement and non-hateful homonyms of slurs 1 . These functionalities are grouped into the derogation, slur usage and negation classes. A test suite may also contain aspects, relevant properties of test cases that are orthogonal to the functionalities. An example of aspect in HATECHECK is the set of possible targeted identity groups. While behavioural testing has been designed as a diagnostics tool, whether and how to leverage it for model training and development has seen little exploration, even though the human insights encoded in the test cases could potentially lead to more robust and trustworthy models. However, naively using behavioural testing for both training and evaluation is a risky affair-giving models access to the test cases could clue them into spurious correlations and lead to overestimation of model performance (Linzen, 2020) . We view these risks as strong motivation to explore such settings, in order to gain insights into the vulnerability of behavioural tests to gaming and over-optimisation. We explore three questions regarding behaviouraware learning: Q1: Do models generalise across test cases from the same functionality? This is a sanity check: test cases from the same functionality share similar patterns-sometimes generated by the same template-so we expect that behaviour-aware learning leads to better performance on test cases from functionalities seen during training. Q2: Do models generalise from covered functionalities to held-out ones? By examining how behaviour-aware learning affects performance on held-out functionalities, we can estimate the robustness of the approach to potentially overlooked phenomena. Equivalently, performance decrease is an indicator of overfitting to functionalities covered during training. Q3: Do models generalise from test cases to the target task? Improvements in the target task performance, as measured by independent and identically distributed (i.i.d.) data, would indicate that a model was able to extract the knowledge encoded in the behavioural tests. Conversely, a decrease in target task performance would signal overfitting to the behavioural test distribution. In this paper, we explore behaviour-aware learning by fine-tuning pre-trained BERT (Devlin et al., 2019) models on HATECHECK 2 . We experiment with several splitting methods and evaluate on different sets of held-out data: test cases for covered functionalities (Q1), test cases for held-out functionalities (Q2), and hate speech detection i.i.d. data (Q3). In addition to HATECHECK's functionalities, we consider performance on held-out functionality classes and identity groups. By investigating our research questions, we address potential pitfalls and identify promising approaches for behaviour-aware learning 3 . Related work Traditional NLP benchmarks are created from text corpora assembled to reflect the naturally-occurring data distribution, which may fail to sufficiently capture important phenomena. Challenge sets were created as an additional evaluation framework, characterised by greater control over data that enables testing for specific linguistic phenomena (Belinkov and Glass, 2019) 2019 ) created HANS, a challenge set for natural language inference (NLI) designed to contradict classification heuristics that exploit spurious correlations in NLI datasets. They used the HANS templates to augment NLI training data, which helped prevent models from adopting such heuristics, though the improvement on held-out cases was inconsistent. Liu et al. (2019) proposed inoculation by fine-tuning, where a model originally trained on a non-challenge dataset is fine-tuned on a few examples from a challenge set and then evaluated on both datasets. They do not assess generalisation from covered to held-out functionalities, as they use samples from the same functionality for training and testing. To the best of our knowledge, we are the first to examine cross-functional behaviour-aware learning by fine-tuning models on different configurations of test suite and task data and evaluating performance across multiple generalisation axes. of the resulting models on both data distributions to assess the impact of behaviour-aware learning considering both task and challenge data. Test suites have limited coverage: the included functionalities, functionality classes and aspects are only subsets of the phenomena of interest. For example, HATECHECK covers seven protected groups, which are particular samples of the full set of communities targeted by hate speech. Therefore, naive evaluation of models fine-tuned using test suite data can lead to overestimating their performance: models can overfit to the covered phenomena and pass the tests, but fail cases from uncovered phenomena (e.g., hate targeted at an uncovered identity group). Since we cannot directly evaluate performance on uncovered cases, we use performance on held-out sets of functionality, functionality classes and aspects as a proxy for generalisation across those three axes, as described in sections 3.2 and 3.4. Task data We use two hate speech detection datasets (Davidson et al., 2017; Founta et al., 2018) as source of task data. Both are composed of tweets annotated by crowdsourced workers. The Davidson et al. (2017) dataset contains 24,783 tweets annotated as either hateful, offensive or neither, while the Founta et al. (2018) dataset contains 99,996 tweets annotated as hateful, abusive, spam or normal. We use the versions of the datasets made available 4 by R\u00f6ttger et al. (2021) , in which all labels other than hateful are collapsed into a single non-hateful label to match HATECHECK binary labels. The data is imbalanced: hateful cases comprise 5.8% and 5.0% of the datasets, respectively. We follow (R\u00f6ttger et al., 2021) and use a 80%-10%-10% train-validation-test split for each of them. Test suite data We use HATECHECK (R\u00f6ttger et al., 2021) 2021 ) define hate speech as \"abuse that is targeted at a protected group or at its members for being a part of that group\", while protected groups are defined based on \"age, disability, gender identity, familial status, pregnancy, race, national or ethnic origins, religion, sex or sexual orientation\". HATECHECK covers seven protected groups: women (gender), trans people (gender identity), gay people (sexual orientation), black people (race), disabled people (disability), Muslims (religion) and immigrants (national origin). In addition to the gold label (hateful or non-hateful), each test is labelled with the targeted group. When fine-tuning on test suite data, we use one of several splitting methods, as illustrated in Figure 1 : All A random 50%-25%-25% train-validationtest split. FuncOut We first hold out all test cases from a given functionality and randomly split the remaining cases into a 50%-50% train-evaluation split. We divide the union of held-out and evaluation split cases into a 50%-50% validation-test split. The process is repeated for each functionality, resulting in 29 split configurations. IdentOut The same as FuncOut, but test cases relating to each identity group are held out, resulting in 7 split configurations. ClassOut Similar to the previous two, but entire functionality classes are held out, resulting in 11 split configurations. Training configurations We consider the following training configurations: Task-only Models are fine-tuned only on the task data. We denote the task-only configurations as Davidson and Founta, depending on which dataset was used for training. Test suite-only Models are fine-tuned only on test suite data. We denote the test suite-only configurations by the name of the splitting method used. Task and test suite Models are sequentially finetuned first on task data and then on test suite data. We denote these configurations as [Task data]-[Test suite split]. For example, in the Davidson-FuncOut configuration, models are first fine-tuned on the Davidson split and then on the FuncOut splits. Evaluation We evaluate the models that result from each training configuration on both task and test suite data. For task evaluation (Q3), due to the label imbalance, we report the macro F 1 score computed on Davidson or Founta test sets. For test suite evaluation, we follow R\u00f6ttger et al. ( 2021 ), and use the accuracy as the classification metric. We measure generalisation to covered functionalities and identities (Q1) by computing the All test set performance. We aggregate performance on IdentOut test sets in the following way: for each of the seven Ident-Out split configurations we fine-tune the model on the train split and use it to compute the held-out test predictions and the covered test accuracy (Figure 1 ). We compute the accuracy on the union of the seven held-out prediction sets as the heldout performance measure, and the average covered test accuracy as the covered performance measure 5 . 5 Covered and held-out aggregation methods are different because each of the seven held-out test sets targets a single identity group. Consequently computing the accuracy on each set and averaging them all would result in the average identity group accuracy instead of the overall test accuracy.  The same method is used to aggregate performance on FuncOut and ClassOut sets. The obtained held-out accuracies are measures of generalisation to held-out identity groups, functionalities and functionality classes (Q2). Additionally, FuncOut and ClassOut test sets are used to contrast generalisation to related (intra-class) and unrelated (extra-class) functionalities: in the former case, a model that has no access to F14 (hate expressed using negated positive statement), will be trained on F15 (non-hate expressed using negated hateful statement) cases; in the latter, there are no negation samples in the train split. Experimental setting All models start from a pre-trained uncased BERTbase model 6 . When fine-tuning, we follow R\u00f6ttger et al. ( 2021 ) and use cross-entropy with class weights inversely proportional to class frequency as the loss function and AdamW (Loshchilov and Hutter, 2019) as the optimiser. We also search for the best values for batch size, learning rate and number of epochs through grid search, selecting the configuration with the smallest validation loss. Results and discussion Covered functionalities performance (Q1) Figure 2 exhibits performance on HATECHECK All split. All models fine-tuned on HATECHECK greatly outperformed models fine-tuned only on task data. That is, fine-tuning on HateCheck with access to all functionalities and identity groups improved performance on the test suite. Prior finetuning on task data did not make a relevant dif-  Scores also decrease when models are evaluated on the task dataset thsey were not fine-tuned on (domain gap). ference: Davidson-All, Founta-All and All performance differences were not statistically significant 7 . Held-out functionalities performance (Q2) Figure 3 contrasts covered and held-out average accuracies in the FuncOut, IdentOut and ClassOut test sets. Unsurprisingly, scores are higher for cov-ered phenomena. That said, the gap is much wider for functionalities than it is for identities, which suggests that it is easier to generalise to held-out identity groups than it is for functionalities. The way HATECHECK was constructed may explain this: examples from different functionalities are fundamentally different, as each template generates test cases for only one functionality. Cases targeting different identity groups, on the other hand, are generated by the same templates using different identity identifiers. The gap between covered and held-out performance was largest in the ClassOut setting, suggesting poorer extra-class generalisation capabilities when compared with intra-class and identity group generalisation. Figure 4 shows the impact of fine-tuning on HATECHECK by contrasting performance before and after the procedure. Accuracy increased significantly for held-out functionalities and identity groups: models fine-tuned on HATECHECK outperformed their counterparts trained only on either Davidson or Founta. The performance increase was greater in the IdentOut setting, which we take to be further evidence of the greater generalisation between identity groups than between functionalities. While the fine-tuning procedure contributed positively to performance in the FuncOut setting, the Sample (top: Davidson et al. (2017) same did not happen in the ClassOut scenario. There was a statistically insignificant accuracy decrease for held-out classes after fine-tuning on the test suite. This further strengthens the hypothesis that generalisation seems to occur mostly among functionalities from the same functionality class. Task data performance (Q3) Figure 5 compares model performance on the task test sets 8 . Macro F 1 scores decreases significantly after finetuning on HATECHECK. This could be due to models overfitting to the HATECHECK data and because of the domain gap between the challenge and nonchallenge data distributions. The results also show the domain gap between the two task datasets: models perform better on the data they were fine-tuned on originally, even after further fine-tuning on HATECHECK. Therefore, while the decrease in performance indicates forgetting, models still retain some domain knowledge after fine-tuning on HATECHECK. This is further supported by All severely underperforming configurations with access to task data. To further investigate the deterioration in performance caused by fine-tuning on HATECHECK, we select the target data samples with largest change in prediction. That is, given a sample s and the gold label probabilities p before (s) and p after (s) predicted before and after fine-tuning on HATECHECK, we calculate for each sample the change in prediction: \u2206 p (s) = p after (s) \u2212 p before (s). Then, for each hate speech detection dataset, we argmax s \u2206 p (s), s \u2208 H c . Where H and H c are the sets of samples labeled as hateful and non-hateful, respectively. Table 1 presents the results of this procedure. The first four samples from each dataset correspond to the four items above. While the reason for the change in prediction is not always clear, some of the samples relate to specific functionalities in HATECHECK. The second sample from Davidson et al. (2017) contains threatening language (F5 and F6). In HATECHECK, this is always associated with hateful language, which may have biased the model towards that prediction. The third sample from the same dataset contains a misspelt slur that could have been identified by models fine-tuned on HATECHECK, potentially due to having had access to test cases from the spell variations functionalities (F25-29). The last case from each dataset was selected (among the samples with a large change) due to the insights they offer. The fifth sample from Davidson et al. (2017) , although clearly non-hateful, was predicted as hateful after model fine-tuning on HATECHECK. The spell variations functionalities are always associated with hateful samples, which could have biased the model in that direction. Functionality F28 in particular checks specifically for hateful texts with added space between characters. It would be interesting to examine if leveraging other types of tests (e.g. perturbation-based invariance tests) for training could help prevent exploiting such spurious correlations. The fifth sample from (Founta et al., 2018) is interesting because the model was able to correct the previously wrong prediction even though the identity \"men\" is not covered by HATECHECK, further evidence of generalisation to other identity groups. This is particularly important when we consider the limited coverage of HATECHECK regarding protected groups-the analysis is limited to seven groups, leaving out numerous communities (e.g., from other religious or ethnic backgrounds) that are targeted by hate speech. Conclusion We have presented a cross-functional analysis of behaviour-aware learning for hate speech detection. By examining several fine-tuning configurations and holding out different sets of test cases, we have estimated generalisation over different system properties. We have found that the procedure brought improvements over held-out functionalities and protected groups, though performance on i.i.d. task data and held-out functionality classes decreased. Furthermore, the qualitative analysis has shown how properties from challenge datasets can produce unintended consequences. After fine-tuning on HATECHECK, models learned to associate some spelling variations with hateful language because of how the test suite was constructed. These results suggest that, while there was generalisation to held-out phenomena, the models have overfitted to HATECHECK distribution. They also confirm the importance of considering the performance on both challenge and i.i.d. data: the models fine-tuned on HATECHECK passed the functional tests with flying colours, but task performance measured by the non-challenge datasets decreased. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics. A HATECHECK functionalities Table 2 exhibits the functionalities and functionality classes covered by HATECHECK. B P-values of performed tests Acknowledgements This research was funded by the WWTF through the project \"Knowledge-infused Deep Learning for Natural Language Processing\" (WWTF Vienna Research Group VRG19-008). ",
    "abstract": "Behavioural testing-verifying system capabilities by validating human-designed inputoutput pairs-is an alternative evaluation method of natural language processing systems proposed to address the shortcomings of the standard approach: computing metrics on held-out data. While behavioural tests capture human prior knowledge and insights, there has been little exploration on how to leverage them for model training and development. With this in mind, we explore behaviour-aware learning by examining several fine-tuning schemes using HATECHECK, a suite of functional tests for hate speech detection systems. To address potential pitfalls of training on data originally intended for evaluation, we train and evaluate models on different configurations of HATECHECK by holding out categories of test cases, which enables us to estimate performance on potentially overlooked system properties. The fine-tuning procedure led to improvements in the classification accuracy of held-out functionalities and identity groups, suggesting that models can potentially generalise to overlooked functionalities. However, performance on held-out functionality classes and i.i.d. hate speech detection data decreased, which indicates that generalisation occurs mostly across functionalities from the same class and that the procedure led to overfitting to the HATECHECK data distribution.",
    "countries": [
        "Austria"
    ],
    "languages": [],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Checking {H}ate{C}heck: a cross-functional analysis of behaviour-aware learning for hate speech detection"
}