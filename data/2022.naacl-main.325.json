{
    "article": "We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed improves perplexity and other metrics. Moreover, this architecture provides a new way to compare different frameworks of linguistic representation. In our oracle graph setup, training and evaluating on English WSJ, semantic constituency structures prove most useful to language modeling performance-outpacing syntactic constituency structures as well as syntactic and semantic dependency structures. Introduction Linguistic theories posit that humans can take advantage of hierarchical structure related to some notion of compositionality to produce and comprehend utterances with complex meanings. Yet explicit representations of this kind of structure are harder to come by than raw text, and large-scale pretrained neural language models (e.g., Devlin et al., 2019; Radford et al., 2019) have managed to perform strikingly well at contextually encoding and predicting words from distributional evidence alone. At the same time, there are good reasons to doubt that these models can be said to understand language in any meaningful way (Trott et al., 2020; Bender and Koller, 2020; Merrill et al., 2021) . To address this conundrum, people have started to explore probing pretrained models (Liu et al., 2019; Tenney et al., 2019a, inter alia) and supplementing training data with linguistic structure guidance (Strubell et al., 2018; Swayamdipta et al., 2018; Peng et al., 2019; Wu et al., 2021, inter alia) . A question that has received less attention is which kind of symbolic linguistic representation (SLR) is most conducive to guiding neural language models (LMs). Numerous domain-general candidates exist (Abend and Rappoport, 2017; Oepen et al., 2019 Oepen et al., , 2020;; \u017dabokrtsk\u00fd et al., 2020; M\u00fcller, 2020) : some are focused on syntactic structure, others on semantics ( \u00a72; big grey example graphs in the left panels of figure 1 ). Frameworks vary along several dimensions, with different label inventories and treatments of specific constructions. Formal differences include the type of structure (dependency or constituency, one or multiple parents, projectivity) and its relation to the input string. In general, different design choices may aim to capture different kinds of generalizations or facilitate different kinds of processing, and may make parsing raw text easier or harder. It is often not obvious which framework should be chosen for best results on an external task-or indeed, how to even perform a controlled comparison across frameworks. In this paper we investigate whether structurally guided language modeling can serve as a benchmark task for directly comparing linguistic representations. Specifically, we evaluate on next-word prediction-a relatively neutral task in that it does not rely on any artificial test suite, nor does it target a specific downstream application where one linguistic framework may have an advantage. 1  We devise a method for selecting and encoding partial views of linguistic graphs over the preceding context relevant to predicting the next token ( \u00a73 and \u00a74). 2 We call these views slices (small per-token graphs and dashed lines in figure 1 ). Our neuro-symbolic encoder statically allocates distinct vector dimensions for different structural 1 Our findings are limited to a particular language (English) and domain (financial news) in which gold graphs from multiple frameworks are available for the same sentences, but such annotations could be obtained for other samples in the future. 2 Our code is available to the research community at https: //github.com/jakpra/LinguisticStructureLM. relations within each slice, which in the incremental setting is much faster and more flexible than computation-intensive deep graph encoders. Using this encoding, we compare 7 SLR formalisms by virtue of their incremental language modeling capability in an controlled experimental setup ( \u00a75) on jointly annotated ground-truth data (Oepen et al., 2019 (Oepen et al., , 2020)) . The results ( \u00a76) suggest that linguistic graphs are indeed informative for next-word prediction, complementing what is learned in pretraining. This invites future research quantifying different formalisms' design choices ( \u00a77). Background: Symbolic Linguistic Representation Following a long tradition in formal linguistics, graph-structured representations of language qualitatively describe grammatical and logical relations among words. The SLR paradigm has recently seen a revival in the form of larger-scale treebanking and sembanking for training neural parsers. Formally, an SLR instance is a directed acyclic graph (DAG) = , , , with vertices , labeled edges , and an anchoring function that maps each vertex to a (potentially empty) subset of tokens in the sentence. We broadly distinguish SLR frameworks along two dimensions: 3 Scope. A main goal of syntactic representations is to explain distributional patterns in word order; they tend to be rooted trees with often projective anchoring functions. Semantic formalisms are meaning-oriented, aiming to capture the higherlevel logic expressed in a sentence; thus, they may have more complex structures, including reentrant edges and discontiguous anchors. Structure. SLRs can further be subdivided into dependency and constituency structures. The former are relatively shallow, while the latter contain abstract nodes with no or multiple word anchors. 3 Overview: Language Modeling with Linguistic Graphs Our main goal is to quantify the predictive power of different SLRs by combining them with a pretrained language model and measuring how this affects next-token generation performance. A language model (LM) assigns probabilities to sentences and can be used to both process existing sentences and generate new ones. As is standard practice, we treat sentences as length-sequences of word tokens, = 0 , 1 , \u2026 , 1 . An incremental LM factorizes the joint probability of the sentence in terms of the probability of each word conditioned on previous tokens < ; eq. ( 1 ). Here we describe at a high level how we process (oracle) SLR graphs for use in this language modeling scenario, i.e., to obtain context-conditional vocabulary distributions from them. In contrast to sequential LMs, contexts are now graph-structured, and which context tokens to select as well as in what way they are related to the target token is determined by the underlying SLR graph ; eq. ( 2 ). ( ) = 1 =0 ( < ) (1) ( ) = ( ) (2) This general idea is closely related to syntactic language modeling (Pauls and Klein, 2012; Gubbins and Vlachos, 2013, inter alia) . We extend this line of work to arbitrarily complex syntactic and semantic DAG structures and, in doing so, take particular care to restrict conditioning contexts from accessing not only future words but also future subgraphs, so effectively top-down and left-to-right. Our procedure is as follows: First, we select for each token position to be predicted a subgraph , called the token's slice. Slices are both admissible in the language modeling setting, i.e., they do not violate the left-to-right conditioning order, and relevant to the token prediction according to some criteria-here we consider criteria based on structural relationships generally, without relying on formalism-specific labels ( \u00a74.1). Consider the small colored subgraphs for each token in figure 1 : the EDS-slice for the target 'reported', for example, starts at node 3, and extends to the ARG2-child 2, ARG1-coparent 1, and BVcoparent 0, which are anchored, respectively, in the spans 'injuries', 'Numerous', and 'Numerous injuries'). Recall from \u00a72 that context words < are contained in , to the extent that they are anchored in a node reachable from . Inspired by Markov assumptions of independence in generative modeling and Markov blankets in causal networks, SLR graph slicing thus allows us to factorize ( ) as ( ) = 1 =0 ( ). (3) Next, we encode each graph slice as a fixedsized vector. Prior approaches to encoding linguistic graphs for neural modeling have involved serialization, e.g., as parser transition sequences (Qian et al., 2021, inter alia) , recursive auto-encoders (Tai et al., 2015; Roth and Lapata, 2016) , and graphconvolutional networks (GCNs; Yang and Deng, 2020; Wu et al., 2021) . However, transition sequences for non-tree graphs are subject to spurious ambiguity; and we find that graph-structured neural networks are impractical in the incremental setting ( \u00a76.5). Instead, we propose a computationally inexpensive method for statically and deterministically projecting slices into a high-dimensional space by vector concatenation ( \u00a74.2). Finally, we compute output distributions ( ) from the vector representations ( \u00a74.3). 4 Modeling Details Slicing Graphs A slice is a connected subgraph of that captures 's linguistically structured context, masking itself (or else estimating ( ) would be trivial). always minimally consists of 's direct anchor node = Select({ ( )}). Starting from , we traverse the graph and add vertices and edges that are connected to via paths of a few specific relative types, REL. Here we settle on 6 types: parents, siblings, grandparents, parents' siblings, children, and coparents. The vertices and edges for slice = , , consist then of the union of these sets. 4  To prevent information leakage from future tokens, we discard from all nodes { ( ) = , > } which are only anchored in tokens following . E.g., in figure 1 , the UD-slice for the token 'were' does not contain the parent node 3 because that is anchored only in the following token 'reported' (and thus the sibling 1 cannot be accessed either). If a node's anchors contain or overlap with (i.e., the node is a non-terminal above ), we retain the node and its edges but remove its token anchors. Vectorizing Graph Slices Because slices can be large, we partition each slice's nodes by structural relative type, in order to aggregate them into a fixed-length summary vector. Specifically, we allocate capacities for each relative type: rel = 2 for parents, siblings, aunts, and children, and 1 for grandparents and coparents. Up to 1, relative nodes rel are added 'with high resolution', maintaining their identity and order; beyond the capacity, relatives are aggregated 'with low resolution'; eq. ( 4 ). Within each relative type, precedence is given to relatives whose token anchors are sequentially closer to . HiRes ,rel = rel, < rel LoRes ,rel = { rel, rel } (4) Next we look up the relatives' edge label and word vector encodings 5 and and collate them into a single vector ,rel per relative type. Highresolution vectors are concatenated and lowresolution vectors are averaged; eq. ( 5 ). Finally, we concatenate all of these (zero-padded) relativevectors to obtain the final vector representation of the whole slice, ; eq. ( 6 ). At a high level, this vector essentially specifies a deterministic, structured, typed, discrete self-attention over the token history. Predicting Emission Distributions We compute model posteriors for next-token predictions as ( = context , ) = Sof tMax(logits , )[ ], where is either a pure SLR model or LM, or an ensemble of the two (bottom right of figure 1 ). SLR only. As described above, we define context , as , which is encoded as . We obtain by letting the slice-vectors serve as inputs to a -multilayer perceptron (MLP) with a final softmax layer over the vocabulary, which yields the estimated token emission distributions. logits , = MLP ( ) MLP ( ) = ( ) \u2026 (1) ( ) Emb , where Emb is an embedding matrix. LM + SLR. Since we want to measure whether and how much the information contained in the SLR can contribute to state-of-the-art language models, our primary experimental condition is a combined setup Ensemble , where logits obtained from slice-encodings are added to a base neural LM's logits before taking the softmax: logits ,Ensemble = logits , + logits , , with logits , = LM( < ). LM only. , i.e., the bare LM without any exposure to SLR graphs, serves as a baseline. Experimental Setup All models are implemented in PyTorch and experiments are run on 1 NVIDIA Tesla T4 GPU. Model hyperparameters are reported in appendix A.5. Data Our dataset consists of the intersection of Wall Street Journal (WSJ; English financial news) sentences that have been annotated with syntactic trees in the Penn Treebank (PTB; Marcus et al., 1993; Hovy et al., 2006) 6 as well as a range of semantic representation formalisms for the MRP 2019 & 2020 shared tasks (Oepen et al., 2019 (Oepen et al., , 2020)) . Summary statistics are shown in table 1. Our preprocessing steps are described in appendix B. SLR Formalisms The 7 (versions of) linguistic representation frameworks examined in this study are listed in table 2, along with their classifications along the scope and structure dimensions. We draw the structural dependencies vs. constituencies distinction (described at a high level in \u00a72) based on specific properties of the MRP shared task data: a framework is considered a dependency framework if all edges are only between pairs of individual word anchors at a time; if there are any unanchored 7 nodes or nodes anchored in more than one linguistic word token, it is considered a constituency framework. 8 Below we give a brief description of each framework. PTB trees specify hierarchically nested syntactic constituents. We consider two labeling variants: basic phrase structure (-phr) and phrase types refined with functional specifications (-fxn). Universal Dependencies (UD; Nivre et al., 2016 Nivre et al., , 2020;; de Marneffe et al., 2021) is a syntactic dependency representation with coarse, crosslinguistically applicable edge labels. DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) and Elementary Dependency Structures (EDS; Oepen and L\u00f8nning, 2006) are derived from underspecified logical forms computed by the English Resource Grammar (Flickinger, 2000; Copestake et al., 2005) . Prague Semantic Dependencies (PSD; Haji\u010d et al., 2012) and Prague Tectogrammatical Graphs (PTG) are syntactico-semantic predicate-argument structures converted from the Prague Functional Generative Description (Sgall et al., 1986; B\u00f6hmov\u00e1 et al., 2003; Haji\u010d et al., 2012) . Language Model The base language model we use in all our experiments is GPT-2 (Radford et al., 2019 , as distributed in the huggingface-transformers PyTorch library). GPT-2 is a Transformer model (Vaswani et al., 2017) pretrained on a diverse collection of web texts. In contrast to other widely-used Transformers like BERT (Devlin et al., 2019) , which optimize bidirectional masked language modeling, GPT-2 is incremental, i.e., next-word decisions only take into account the preceding context. Training We train all models for 10 epochs with the AdamW optimizer (Loshchilov and Hutter, 2019) , minimizing cross-entropy between the model posterior and the ground truth at each token position. We perform early stopping with the last 10% of the original training corpus set aside for development scoring after each epoch. 9 We keep the model state that achieves the best perplexity on the dev set. Peak development performance is reached after 3 epochs for SLR models, whereas finetuning GPT-2 by itself takes between 7 and 9 epochs. Evaluation We compute model perplexity (PPL) as the most standard language modeling evaluation measure, as well as accuracy (Acc) and confidence (Conf) of a model's top-ranked guess, mean reciprocal rank of the correct answer (MRR), and entropy of the model's token prediction posterior (H). All metrics are reported as microaverages over the evaluation data at the BPE token level. 10 6 Findings Main Results The most striking observation in terms of overall model performance (table 2 ) is that ground-truth linguistic graphs of all investigated linguistic formalisms improve vanilla GPT-2 by a large margin, in all metrics. This improvement holds up when compared to a version of GPT-2 that is exposed to the raw WSJ text without the graphs; with this condition we control for mere domain differences between our evaluation data and the data GPT-2 was trained on originally ('+Domain' in table 2 ). The large performance gap suggests that at least a subset of the oracle knowledge about linguistic structure is not yet encoded in the base language model, which learns from only raw text. We observed that if we keep training for the entirety of 10 epochs, rather than early stopping based on development performance, we somewhat overfit to the training set. While accuracy itself is not affected very much by this, the models become increasingly overconfident ( overall confidence overall accuracy , which gets up to 8-12%, compared to 4% with the vanilla GPT-2 model and in most cases even slightly less than that with the early-stopped SLR models). This leads to overall worse perplexity. Differences between Formalisms Comparing across rows in table 2, we find a considerable performance spread. The general trend, which is relatively consistent in all metrics, 11 is indicated by the order of rows, with UD having the smallest (though still respectable) improvement over the baseline, and PTG and EDS the largest. Interestingly, there are two marked separations: a primary one between dependency and constituency formalisms, and a secondary one between syntactic (i.e., more surface-oriented) and semantic (more abstract) formalisms. This is summarized 10 We compute average PPL over all sentences by exponentiating last: ). We report each quality metric as mean \u00b1 stdev over 3 random seeds. We also report model size in #parameters (all non-baseline models as absolute difference to baseline) and training speed in sentences per second as measures of efficiency. Statistical significance of the PPL and Acc differences to the next-best model (always adjacent rows) is reported as *** < .0001 / ** < .001 / * < .005 / not significant (approximate randomization test as described in Riezler and Maxwell (2005) , with =10,000 shuffles). We only consider a difference significant if < for all three random model initialization seeds. Best results in each column are bolded. For confidence, 'best' means best-calibrated, i.e., the smallest relative difference to accuracy. Table 3 : Model perplexity (lower is better) summarized in terms of two SLR dimensions: Scope (syntax vs. semantics) and structure (dependency vs. constituency). exp 1 w w 1 =0 log = context , Model Scope/Struct #Labels Speed \u2191 Size \u2193 PPL \u2193 H [nats] \u2193 Acc [%] \u2191 Conf [%] \u2191 MRR \u2191 GPT- \u00b1 ( ) over frameworks per condition. Statistical significance of the difference between the two closest SLRs of each pair of conditions is reported as *** < .0001 / ** < .001 / * < .005 / not significant (approximate randomization test with =10,000 shuffles). in table 3. A limiting factor for dependency representations in the incremental LM setting is that relations between the target token and subsequent tokens are entirely ignored, whereas constituency graphs can back off to higher-level structures. Further, the syntactic graphs we use are always trees, so they never populate the coparent capacity in the slices. Semantic constituency representations, with their abstract and meaning-oriented labeling and structure schemes, jump out as being especially predictive of the underlying text, as compared to both syntax and shallow semantics. We note that the function-enhanced PTB label set has a slight advantage over the basic phrasestructure labels; and that, among the two closely related pairs of formalisms (DM/EDS and PSD/ PTG, which each are dependency and constituency versions converted from the same underlying grammars), the constituency versions always work better than the dependency versions in our setting. There is, however, no consistent ranking between DM/ EDS on one hand and PSD/PTG on the other. In terms of perplexity, EDS works better than PTG, and PSD better than DM, but these differences are not significant for accuracy. Differences between Word Classes To better understand where particular strengths and weaknesses of the baseline LM and linguistically enhanced models lie, we analyze subsets of tokens by part-of-speech (POS) tag (table 4, see appendix C for more details). Across all models there is a clear and expected separation between rather predictable function words, more perplexing content words, and numbers, punctuation, and miscellaneous tokens somewhere in the middle. Average perplexity of the tested SLR models is better than baseline GPT-2 in all POS classes but one. The one exception is the noun class, where both the SLR macro-average and UD in particular do not raise performance. Only EDS and DM show perplexity improvements on nouns; PTB even has a noticeable negative impact. We conjecture that this may have to do with relatively deep NP nesting in PTB (compared to the other formalisms), such that the current slicing hyperparameters (relative types and capacities) are too strict and hide informative signals like modifiers and verb attachment. Some formalisms seem to be particularly wellsuited for the prediction of certain POS: UD for verbs; PTB and PTG for adpositions and subordinating conjunctions; EDS for pronouns, determiners, and numbers; PTG, PSD, and EDS for coordinating conjunctions. The advantage of EDS and DM on nouns, pronouns, determiners, and numbers can likely be attributed to their explicit representation of variable binding/quantification. Similarly, PTG and PSD have detailed categories for coordination, distinguishing, e.g., con-and disjunction. For nouns and modifiers, the spread across formalisms is particularly wide, which suggests that SLRs diverge quite a bit on these types of words (e.g., whether adjectives and certain nouns can count as predicates) and that this diversity has a strong effect on utility for language modeling. Model Ablations The linguistically enriched models consist of a substantial number of newly learned parametersaround 50-60M each, an additional 50% the size of vanilla GPT-2. Although model size does not seem to be correlated with performance among the SLR-enriched models, it could still be that the additional capacity allows the models to store more information about the words' distributions than the baseline GPT-2 model, without ever truly using the concrete linguistic structures. We check this by randomly shuffling ( ) two core graph properties: (i) the assignment of edge labels, and (ii) the anchoring mapping between graph nodes and word tokens in each graph. If the models are largely independent of the correct label and structure assignments, these changes should have a very small effect on performance (Dubossarsky et al., 2018; Hewitt and Liang, 2019) . But on the contrary, we find that performance worsens considerably in the ablated settings compared to the full combined models of each formalism ( confirms that the models really do acquire-and are quite sensitive to-the graph-encoded linguistic signals, relying to a large part on this new information in making their predictions. Shuffling only edge labels while leaving the rest of the graphs unchanged has a smaller effect than changing how tokens are anchored in the graph structure. This suggests that the linguistic graphs' entire structural arrangement of labels and attention-like selection of context words play a crucial role-more so than knowing the type of each individual (correctly attached) grammatical relations. Note that the Anchors setting, too, changes which edge labels are used in the predic- tion of a given token, resulting in a smaller difference between Anchors and Both. If a model has learned to rely on correct labels and structure during training, then perturbing these properties at test time has a highly adverse effect, confusing the model and leading to a drastic decrease in performance-even worse than not consulting SLR graphs at all! Given previous findings that syntactic structure is to some extent already learned in pretraining (Linzen et al., 2016; Tenney et al., 2019b) , we conjecture that this representational capacity gets offloaded to the graphs at training time, and thus test-time permutations fool the PTB model to a much greater extent than DM. As expected, exposing models to shuffled graphs at training time renders the additional model parameters practically neutral, resulting in similar perplexity as the base LM. In this case, it also does not matter whether test-time graphs are correct or random (training vs. both in column 2)-either way, the model learns to mostly disregard the random structure as noise. Comparison with R-GCN Encoding As an additional strong baseline, we compare our concatenation-based slice vector encoding to a graph neural network from the literature. We choose relational graph-convolutional networks (R-GCN; Schlichtkrull et al., 2018; Kipf and Welling, 2017) as a suitable representative of this type of model, which has been used successfully by Wu et al. (2021) to encode DM graphs. Results are shown in table 6. Contrasting with table 2, there is a big difference in training speed: our simple encoder is on average roughly twice as fast as the computation-heavy alternative, whose time and space complexity is dominated by the number of labels. 13  We observe at best similar LM quality as with our concatenation method (EDS and DM), but for most formalisms performance degrades. We follow Schlichtkrull et al. and Wu et al. in using 2 R-GCN layers with basis matrix regularization. Possible disadvantages of this for encoding linguistic graphs are the fixed path length (2 layers exclude parent's siblings; but 3 layers would include a lot of irrelevant information) and that many of the trained parameters are shared between different relations. In contrast, our concatenation encoding forces the MLP input layer to learn distinct parameters for each structural relative type and edge label. Discussion Related Work Researchers have long been interested in scaffolding sequential language models with linguisticstructure-based inductive biases. Syntactic language modeling dates back to the pre-neural era, when Pauls and Klein (2012) and Gubbins and Vlachos (2013) generalized Markov assumptions from word n-grams to syntactic subtrees. These ideas have since been adapted to recurrent neural network (RNN) LMs (Mirowski and Vlachos, 2015) and expanded on (Dyer et al., 2016; Choe and Charniak, 2016; Shen et al., 2018 Shen et al., , 2019)) . Ek et al. (2019) condition RNN-LMs on predicted syntactic and semantic (unstructured) tags, interestingly finding less or sometimes no benefit, especially on the semantic side. They hypothesize this might be due to tagging errors-an issue our oracle setup avoids. In the era of attention-based neural modeling of language dominated by pretrained Transformers, models are often finetuned for and evaluated on specific NLP tasks-like semantic role labeling, machine translation, natural language inference, graph-to-text generation, or the GLUE benchmark (Wang et al., 2019) -rather than language modeling in its own right, which makes it difficult to compare them directly to our findings. There have been two main directions: One group of approaches continues the old syntactic language modeling tradition by incrementally generating words and SLRs with either joint (Peng et al., 2019; Qian et al., 2021; Sartran et al., 2022) or iteratively-coupled LM and parser models (Choshen and Abend, 2021) . The second group assumes parsed input sentences, which are then used to guide the model, e.g. by directly optimizing Transformers' attention weights to reflect linguistic graph structures (Strubell et al., 2018; Bai et al., 2021; Slobodkin et al., 2021) . Rather than controlling the existing sequential attention, Hajdik et al. (2019) process serialized graphs directly with a sequence-to-sequence model, and Wu et al. (2021) extend a pretrained Transformer with an additional graph encoder. Notably, Wu et al. (2021) and Slobodkin et al. (2021) experiment with a few different semantic and syntactic SLRs, while all other studies we have looked at are limited to either syntax or very shallow semantics. Another relevant line of work employs probing tasks in investigating to what extent grammar and meaning are already encoded in neural language models trained predominantly on raw text with little to no linguistic supervision (Linzen et al., 2016; Tenney et al., 2019a,b; Hewitt and Manning, 2019; Liu et al., 2019; Kim et al., 2019; Wu et al., 2020; Geiger et al., 2021, inter alia) . Among the probing literature, the works of Kuznetsov and Gurevych (2020) and Kulmizev et al. (2020) are noteworthy in that they investigate subtle differences between different (versions of) frameworks roughly covering the same representational scope, namely, semantic roles and syntactic dependencies, respectively. Orthogonal approaches to comparing SLR designs have involved measuring how well different frameworks complement each other for joint parsing or can be merged or converted into one another (Prange et al., 2019a; Hershcovich et al., 2020) . Limitations and Future Work While the use of oracle graphs has both theoretical advantages (measuring an upper bound without needing to account for potential errors or uncertainties) and practical ones (saving the computational overhead from training and running a parser), ground-truth SLR graphs are a very limited resource and generally assumed to only be available at training time. There is no guarantee our results translate to the non-oracle setting. For instance, it could be that the most helpful abstract semantic information is also the hardest to predict. And despite segmenting the existing sentence-level graph into token-level slices, the human annotator who created the graph in the first place has seen and analyzed the whole sentence, thus already resolving crucial ambiguities and simplifying the task based on knowledge 'from the future'. In subsequent work, we plan to parse graph slices incrementally, which will both relax the conditional modeling assumption into a more broadly interpretable joint model and enable test-time use of the full system on datasets without linguistic annotations. We also only test formalisms that are explicitly anchored in linguistic units, roughly corresponding to LM (sub-)word tokens. This prevents us from applying the same paradigm to some other widelyused unanchored formalisms like AMR (Banarescu et al., 2013) without some changes to the setup. Broader Impact Our experiments yield evidence which-at least in the case of encoding contexts for next-word prediction-supports the thesis of Bender and Koller (2020), Trott et al. (2020) , and others that linguistic meaning goes beyond form. Computational models of language that exclusively learn from even very large amounts of raw text are thus generally expected to hit a ceiling 14 which can only be overcome with access to higher-level structures and mechanisms of understanding. It further seems to matter in which manner and shape linguistic graph structure is drawn. Assuming a perfect incremental parser, deeper structure and semantic categorization seems to be particularly beneficial for integration with a standard language model. This is in line with previous findings by, e.g., Tenney et al. (2019b) that while pretrained LMs tend to encode shallow syntactic structure, abstract relations are more difficult to probe for. We thus see a promising research direction in moving towards linguistic scaffolding of language models with representations that are more complex than tags or dependencies and that capture meaningful relations beyond surface structure. Conclusion We have presented evidence that symbolic linguistic representations of various frameworks have the potential to aid a pretrained incremental Transformer in task-neutral next-word prediction. To this end, we have proposed a framework-agnostic neural encoding scheme for linguistic graphs and applied it to an English dataset jointly annotated with 7 different formalisms. The results highlight the importance of appreciating complex linguistic structure and handling its computational representation with nuance. A.5 Model Hyperparameters We report our model and training hyperparameters in table 8. We did not perform explicit hyperparameter tuning, besides some manual testing early in development on a subset of the MRP shared task data. Those data are annotated with SLR frameworks other than the ones we compare here, and we ended up excluding them from our experiments for lack of overlap with most of the other frameworks' annotations. A.6 Efficient Batching for R-GCN In our incremental setting we need to apply the R-GCN to each token-level slice, which would lead to multiple days 15 of training for each model if done naively. We achieve a considerable speedup by exploiting the oracle graphs at training and evaluation time to pre-compute slices and running the R-GCN only once per sentence batch. B Data Preprocessing B.1 Sentence Filtering To establish a common ground for comparison, we take the intersection of sentences occurring in the annotated datasets of all linguistic formalisms. In a first step, we discard two sentences whose linguistic graph in at least one formalism is empty. 18 We then select only those 35,513 traindev / 1,401 eval sentences that appear in both the MRP 2019 and 2020 datasets (the 2019 corpus contains 143/1,958 more in train-dev/eval). 19 Next, 15 Projected timeline based on a few iterations, which is confirmed by Yang and Deng (2020) . 16 For label set . The factor 16 arises from the capacities chosen (table 7 ), and the extra embedding allocation is for averaged preceding unanalyzable/within-anchor tokens. 17 For bidirectional label set , which is twice as big as . 18 The sentence \"It is.\" in DM and a 'sentence' consisting of the @-symbol in PTG. 19 'train-dev' refers to the data split that was used as training data in both the MRP and 2019 tasks, and which we split 90%/ 10% into our training and development data. 'eval' refers to the data that was used as evaluation data in MRP 2019 and as development data in MRP 2020, and which we evaluate our 4387 PTG (constituencies) PTG has an abstract PRED node as well as a multiword anchor where PSD does not, which results in diverging slice representations for the last two tokens. we take the intersection of these sentences and OntoNotes 5.0, which contains the gold PTB syntax annotations. 26,719/929 sentences remain in the train-dev/eval set. The MRP graph format operates on raw-text character offsets, while PTB and UD trees operate on word tokens. We are able reconstruct offset-based text anchors for PTB and UD from the raw text strings used in the MRP data for all but 394 train-dev / 8 eval sentences, which leaves us with the final 26,325 train-dev and 921 eval sentences. In a few cases, where the linguistic graph has no edges, we add an artificial edge with a dummy label. B.2 Tokenization We follow the sentence segmentation of the Penn Treebank corpus. Within sentences, we obtain token boundaries from GPT-2's pretrained bytelevel byte-pair encoding (BBPE) tokenizer. The BBPE tokens are then aligned with the formalismdependent SLR node anchors via raw-text character offsets. Tokens that are continuations of multiword anchors in the graph (' reported' in PTG, figure 1 ); subword tokens of a single graph anchor ('N-umerous'); or are unanchored in the graph (' were' in EDS), are treated as unanalyzable, i.e., their slice consists of a copy of the preceding tomodels on. ken's slice, plus the preceding within-anchor tokens. B.3 UD Conversion Quasi-gold UD 2.0 trees are obtained from the UD converter released with the Java Stanford Parser v4.2.0 (https://nlp.stanford.edu/ software/lex-parser.html) on the PTB trees. B.4 PTB Labels By convention, phrasal and functional labels in PTB are node labels. To match the labeled-edgesunlabeled-nodes format of the other formalisms, we losslessly convert them to edge labels (namely, on each node's single incoming edge), discarding the preterminal nodes' POS labels. In preliminary experiments we saw that including the POS tags is much more beneficial than phrase structure only; but since we do not include word-level tags in any of the other conditions, this would be an unfair comparison. We focus here on sentence-level structure and leave studies of word-level tags to future work. B.5 Data Splits We split the corpus into training/development and evaluation data following the MRP task setup. Specifically, we evaluate on the data split that was used as evaluation data in MRP 2019 and as development data in 2020, as only for this data gold annotations in all formalisms have been released. We do not perform empirical hyperparameter tuning. In early development, a small subset of the data was used. C Detailed Results We report detailed results without early stopping (table 9 ), breakdowns by POS-class (table 10 and appendix C.1), as well as ablation experiments (table 11 ) for all SLR formalisms. In tables 4 and 10 and figure 3 we merge the POS tags {NOUN, PROPN} into 'noun', {ADJ, ADV} into 'mod', and {INTJ, SYM, X} into 'misc'. C.1 Lexico-Semantic or Syntactic Knowledge? In \u00a76.3 we have found part-of-speech-specific patterns of model performance. But whenever, for a certain syntactic word class , a formalism is more conducive to next-word prediction than a formalism , it is not clear whether this is the case because the choices get narrowed down to itself or whether it is caused by either complementary or completely independent signals, perhaps at the lexical or semantic-structure levels. We investigate this by rerunning the experiment with each token's UPOS tag as an additional input. If this is more or less the same information as is gained-to different extents-from the SLRs, then the results should be similar to before, and SLRconditional differences should disappear. A few particularly interesting POS are shown in figure 3. We discuss them in order. Among content words, nouns and verbs are similar both in terms of baseline performance and in how much easier it becomes to select the correct lexical item if the part-of-speech is known. At the same time, the individual SLR formalisms differ quite a lot in how much information they contribute about the POS class itself and about lexical choice within the part-of-speech. The respective best formalisms (EDS for nouns, PTB and UD for verbs) approximate oracle POS knowledge by themselves and still contribute substantial complementary information when the actual POS tag is revealed. In contrast, PTB does not seem to provide any useful signal about nouns to the incremental LM-neither independently nor in conjunction with the POS. Modifiers (adjectives and adverbs) display a rather interesting behavior: the fact that a word of this type is coming next is very hard to predict from just the preceding raw context, which makes sense since they tend to add optional meaning on top of the (obligatory) logical and grammatical content. However, once the decision to modify has been made, the contextual choice becomes much easier than that for nouns or verbs. In both cases, all SLRs are quite helpful, with UD on the lower end and EDS leading the field. We find similar tendencies among auxiliaries ( function verbs) and pronouns ( function nouns) as with (content) verbs and (content) nouns, but naturally at a much smaller scale. Despite their functional-grammatical distribution and behavior, the semantic frameworks EDS and PTG consistently outperform the syntactic ones UD and PTB even on these 'small' words. A possible explanation for this interaction with auxiliaries in particular could be that EDS and PTG do not analyze them separately at all, but rather group them, respectively, with the preceding context 20 or their main predicate. The models might be able to leverage this to focus on things like subject-verb agreement, local cohesion, or anticipating the main predicate. More explicit syntactic analyses of auxiliaries (incrementally inaccessible forward-pointing dependencies in UD; VP-nesting in PTB), in contrast, may restrict the model from directly making these connections. Adding POS information in the input decreases SLR-dependent differences. For 'subordinators' in the broad sense, i.e., subordinating conjunctions at the clausal level and adpositions for nominal complements, PTB and PTG are particularly well-suited. By themselves they are already at least as informative as POS, and they still add a small but noticeable complementary signal when the POS is revealed. Determiners and coordinating conjunctions, which both already show extremely low perplexity with some SLR models (namely, EDS, PSD, and PTG), entirely lose any reliance on particular SLRs when their POS is known. Acknowledgements We would like to thank Katrin Erk and Chris Dyer; members of the Georgetown NERT/GUCL and HKU NLP labs; the organizers, reviewers, and audience of MASC-SLL 2022; as well as the anonymous ARR reviewers for their extremely insightful feedback and suggestions. A Additional Modeling Details A.1 Selecting Anchor Nodes In case there are multiple anchoring options (see, e.g., EDS nodes 0 vs. 1 for first token in figure 1 ), we use the following tie-breaker heuristics: Select the anchor node with the most parents and children; if still a tie, select the anchor with the highest node ID (tends to be hierarchically lower, i.e., vertically closer to the token anchor). A.2 Relative Types = , , , , , , namely, parents , siblings , , grandparents , , aunts , (all indexed by parent ), children , and coparents , (indexed by child ). This is the anchor node's Markov blanket, plus siblings, grandparents, and aunts. We chose this set of relations based on general notions of linguistic hierarchy (predicate-argument, head-dependent) and preliminary experiments, but without tuning for specific formalisms. Precise definitions are given in table 7. Relative nodes are permanently associated with the label of the edge that got them selected. A.3 Representing Tokens and Labels We use GPT-2's pretrained global embeddings (from the lowest layer, before any local contextualization) to obtain embeddings for relative token anchors in the slice-vector. When a token anchor in a linguistic graph consists of multiple BBPE tokens, we average their embeddings. We reuse the transpose of the same embedding matrix again to project the last hidden state of the token-emission MLP into the vocabulary. SLR edge labels are encoded as one-hot vectors in the slice vectors, which lowers the potential for unnecessary random initialization variance of fromscratch embeddings. A.4 Distinguishing Dependencies from Constituencies While this distinction-as defined in \u00a75.2 in terms of the anchoring mapping between graph nodes and word tokens-can be subtle for individual sentences, it nonetheless affects slice encoding. In PSD, for example, auxiliaries are unanchored, whereas in PTG they are grouped with their main predicate (figure 2 ). .527 \u00b1 .8e-3 Table 9 : Main results without early stopping: performance of language models combined with 7 SLR formalisms of different scope, structure, and label set (each corresponding to a Ensemble in \u00a74.3), compared to vanilla GPT-2 and a version of GPT-2 that has been domain-finetuned on the raw text of the SLR training corpus ( ). We report each quality metric as mean \u00b1 stdev over 5 random seeds. We also report model size in #parameters and training speed in sentences per second as measures of efficiency. Best results in each column are bolded. For confidence, 'best' means best-calibrated, i.e., the smallest relative difference to accuracy. Figure 3 : Model perplexity (lower is better) with UPOS as additional input. Top left: nouns, verbs, and modifiers; top right: auxiliaries and pronouns; bottom left: adpositions and subordinating conjunctions; bottom right: determiners and coordinating conjunctions. Big gray squares mark baseline (finetuned GPT-2) performance without (dark) and with (light) POS inputs and SLR-specific data points without/with POS inputs follow below the squares in each respective column. Mind the different y-axis scales, and in particular the log scale in the top-left plot, which makes it easier to read very big and slightly smaller (but still big) differences at the same time.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 0.9919298241339868
    },
    "reasoning": "Reasoning: The provided text does not mention any specific funding sources for the research conducted or the writing of the article. Without explicit mention of funding from defense, corporate entities, research agencies, foundations, or an indication of no funding, it is not possible to accurately determine the funding sources based on the provided information."
}