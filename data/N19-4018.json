{
    "framing_question": "How does the work address cost, productivity, efficiency, and related topics, if at all?",
    "response": "The provided paper does not contain information about cost, productivity, efficiency, and related topics. Thus I am unable to answer your question.",
    "article": "We present SkillBot that takes the first step to enable end users to teach new skills in personal assistants (PA). Unlike existing PA products that need software developers to build new skills via IDE tools, an end user can use SkillBot to build new skills just by naturally demonstrating the task on device screen. SkillBot automatically develops a natural language understanding (NLU) engine and implements the action without the need of coding. On both benchmark and inhouse datasets, we validate the competitive performance of SkillBot automatically built NLU. We also observe that it only takes a few minutes for an end user to build a new skill using SkillBot. Introduction Artificially intelligent voice-enabled personal assistants (PA) have been emerging in our daily life, such as Alexa, Google Assistant, Siri, Bixby, etc. Existing off-the-shelf personal assistants serve different domains, in which each domain has a large number of capabilities, called skills. A skill refers to the understanding of various utterances about one intent/task and the execution of this intent/task. Existing industrial PA products completely rely on software developers to build new skills by manually developing NLU engine and implementing action fulfillment. On one hand, while recent work CRUISE (Shen et al., 2018a) and SliQA-I (Shen et al., 2019) have introduced automatic training utterance and question generation approaches with lightweight human workload, they still require the involvement of software developers for NLU development through IDE tools. Another line of research is to personalize NLU engine in existing skills (Azaria et al., 2016; Ray et al., 2018; Shen et al., 2018b; Wang et al., 2018a ), yet they cannot support building new skills. On the other hand, developers need to write a significant amount of code in order to fulfill a sequence of operations to carry out the task (DialogFlow, 2018; Alexa, 2018) . However, in practice, it is infeasible for software developers to pre-build all possible skills that satisfy all users' needs. First, the pre-collected training corpus usually cannot exhaustively cover all possible varieties of utterances. Second, due to the heavy workload on fulfillment implementation, many actions are not implemented to be supported in PA. Thus, it is critically desirable to design an easy to use system that can facilitate end users to quickly build high quality new skills. Unlike existing IDE for skill development, such system needs to be more friendly and natural to ordinary end users without any complex interfaces. In this paper, we take the first step to present SkillBot that enables end users to initialize building PA skills. An ordinary user, without either natural language expertise or software development background, only needs to demonstrate the task on his daily device screen. SkillBot, without a complex IDE interface, automatically learns the action by tracking user operations and develops the NLU engine by automatically generating training utterances based on pre-built skills. Since users intend to use spoken language to interact with PA, we follow most industrial products (DialogFlow, 2018; Alexa, 2018) to use spoken language understanding (SLU) as our NLU engine, which understands user query by detecting its intent (skill) and extracting semantic slots (slot filling) (Liu and Lane, 2016; Kim et al., 2017; Wang et al., 2018b) . Even though our fully automated SkillBot only aims to satisfy each user's personal needs rather than understand all various expressions User: \"Find a five star hotel near San Jose\" Agent: \"Please teach me\" [User Selection of Each Input Description] Input description: Review rating Review numbers Amenities Input description: Location name Distance type Amenities (a) Learning via Demonstration: When PA asks for teaching, the user operates step by step accordingly in Yelp app. For each user input, SkillBot prompts to ask user to select the most relevant description of this input. User: \"Find a hotel with good rating\" Agent: \"What is the location name? \" User: \"San Francisco\" (b) Automatic Execution: After parsing the utterance using NLU, PA executes the learned action step by step. When finding a missing input, PA prompts for user input. 2 SkillBot System Overview Our Settings To satisfy user personalized needs in PA, SkillBot aims to enable end users to build their own (longtail) skills only by demonstration on screen. That said, an end user only naturally uses their device as usual to build a new skill. In this first work, we assume that there exist a set of pre-built popular (head) skills in the ecosystem and these skills contain both annotated training utterances and a text description for each slot. In addition, we target on teaching and executing actions on the same mobile apps. Figure 1 shows a running example in which  SkillBot helps an end user to build the new \"find hotel\" skill in Yelp. For an end user, he uses PA as usual via voice utterances. SkillBot prompts the user to teach when PA cannot understand and execute the user utterance correctly. As in Figure 1a , all the end user needs to do is to demonstrate on screen step by step how he wants PA to execute. After each user input, SkillBot identifies possible slot descriptions and asks user to select the most relevant one. SkillBot then automatically builds the new skill and outputs a well-trained NLU engine and an action executable file. A user could teach a new skill multiple times where each skill is considered to have the same on-screen operation sequence. Next time, in Figure 1b , PA can correctly understand this user's different expressions of this intent and execute the right action. System Design Recall that our target is to facilitate the end users who have neither natural language expertise nor software development knowledge. Thus, SkillBot is designed to support these two automation respectively. Specifically, SkillBot consists of two main components, automatic action fulfillment  3 Learning via Demonstration Action Learning Action learning module has two main threads, as shown in the left part of Figure 3 . Following user demonstration, at each screen, screen UI element extractor collects all UI elements in the format of a DOM tree on the current screen that the user is operating. In the meanwhile, event extractor collects all events from user operates in this screen. Since there are typically a lot of system services running on device, the extracted events usually include many irrelevant ones that is not from user demonstrated actions, such as 'windows change', 'window state change', 'system or other app notifications', etc. In order to filter the irrelevant events, we first prune all events out of the current front end app based on their event package name. To further ensure some unexpected events within app (e.g., location permission request in Yelp app), we allow user to teach again at any point when user sees any unusual popups or notifications. At last, a bytecode is outputted including the event sequence in which each event contains its UI element information and the required slot value input based on the identified slot in Section 3.2.1. Automatic NLU Development The key idea of training utterance generation is to identify the slots in the user utterance and then use them to generate more training utterances based on the training utterances in existing pre-built skills. Slot Identification This module is invoked after each user input during demonstration. As shown in the middle part (blue) of Figure 3 , after user inputs \"San Jose\" in the example of Figure 1a , it receives the user utterance and the optional values (e.g., \"New York\", \"LA\", etc.) extracted from dropdown list of the user input box (Yelp location textbox) during action learning. Taking the above input, we first construct the set of natural language utterances by replacing the input values with other optional values. For each utterance, zero-shot slot tagging module extracts its semantic slots based on each slot description using the zero-shot model in (Bapna et al., 2017) trained on pre-built skills. Slot ensemble module performs a joint slot detection across all constructed utterances by combining the likelihood scores of each slot. The descriptions of identified top ranked slots are sent to the user to select the most relevant one. Utterance Generation In this first work, we assume that each training utterance in pre-built skills has been decomposed into segments by human expert or our proposed CRUISE approach (Shen et al., 2018a) . Each segment contains a slot tag (two examples are shown in the right side of Figure 4 ). We only use the subset of segments associated with the aforementioned identified slots. We generate the utterance by combining identified segments into long utterances by concatenating them together (middle part in Figure 4 ). To do so, we first use the off-theshelf Stanford parser to identify the verb and main object in user utterance. In the sample utterance of Figure 1 , \"find\" and \"hotel\" are marked as verb and object based on the parser tree. As Automatic Execution Figure 5 shows the flow of automatic execution of a skill. NLU first parses the utterance and outputs its intent and slots. The intent class is used to query the action/skill database to retrieve the bytecode of the corresponding action. SkillBot executes the events one by one following the sequence. For each event, the execution consists of the following two threads: One thread determines if this event requires a slot input based on the saved meta data during learning. If so, we extract on the slot results parsed by NLU. If this information is missing in utterance or NLU fails to parse, we prompt the follow-up question to ask user. The other thread first locates the UI element based on its saved coordinates. It then inputs slot values and simulates the user operation by using gesture control based on the element coordinates (e.g., MotionEvent in Android devices). Experiments In this section, we focus on evaluating SkillBot built NLU engine given that auto action learning and execution are restricted to the same app. We test on both benchmark and in-house datasets/domains: (1) ATIS (4978 training, 893 test) with 17 intent labels and 79 slot labels (Hemphill et al., 1990) ; (2) In-house Yelp (1968 training, 911 test) with 5 intents and 10 slots. To evaluate both datasets which are not generated by CRUISE, we segment each utterance using dependency parser and slot annotations (each segment ends with a slot). The noisy segments are further removed by human experts. In the experiment, we assume that user always select the correct slot description after each input to ensure the correct slot identification. We use the (Liu and Lane, 2016) . In each dataset (with k intents), for each intent I, we assume the user teaches this intent I given the remaining k \u2212 1 intents as pre-built skills. Let T I be the subset of training data w.r. Table 1 presents the results. For ATIS, we show the results for top 8 intents, which contain at least 0.9% of the overall training utterances (the third column displays this fraction). SkillBot NLU achieves a large gain in intent accuracy over the baseline in both datasets. Moreover, we observe that the accuracy gain is also roughly correlated to the fraction of all test utterances in each intent. This indicates that the SkillBot NLU can correctly learn most of the test utterances in the newly added intent. SkillBot NLU also improves the slot filling F1 score in most intents. In cases when SkillBot NLU completely misses a slot, it can directly ask a follow-up question to allow user provide the slot value before action fulfillment. Therefore, SkillBot with follow-up user clarification further improves the F1 score and obtains performance gain in last column. Discussion & Future Work We present the first SkillBot that enables end users to build skills in PA. SkillBot automates both action fulfillment and NLU development. In Acknowledgments The authors would like to thank Abhishek Patel and Xiangyu Zeng for useful discussion and their help of demo implementation and video shooting.",
    "funding": {
        "military": 0.0,
        "corporate": 3.547726766683912e-05,
        "research agency": 5.51223498068687e-07,
        "foundation": 1.9361263126072004e-07,
        "none": 1.0
    }
}