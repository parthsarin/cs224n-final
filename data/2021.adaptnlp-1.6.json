{
    "article": "While high performance has been obtained for dependency parsing of high-resource languages, performance for low-resource languages lags behind. In this paper we focus on the parsing of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-gram features. The best single source treebank (NL ALPINO) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data. Additional experiments consisted of removing diacritics from our Frisian data, creating more similar training data by cropping sentences and running our best model using XLM-R. These experiments did not lead to a better performance. Introduction As parsers are improving (with currently scores higher than 96 (Mrini et al., 2020) ), parsing scores for low-resource languages lag behind. In recent years there has been an increase in interest in ways to parse and annotate these languages. This paper will focus on parsing the low-resource language Frisian (Germanic language spoken in the north-western part of the Netherlands, approximately 612.000 speakers 1 ). We will use spontaneous speech data containing code-switches from the FAME! corpus created by Yilmaz et al. (2016) . We annotate a small portion of this data with UPOS tags and Universal Dependencies (Zeman et al., 2020) for evaluation purposes. An example of such an utterance can be seen in Figure 1 . Because it is unclear which (parts of) datasets are likely to be good candidates for training a parser for spoken Frisian, we propose to use instance-based selection from a variety of sources. This leads to our main research question: Can automatic data selection on the instance level outperform selection on the treebank level for training a parser for a new target domain/language? After finding our best model, we will try to improve our results by removing diacritics in our Frisian data and by making our train data more similar to spoken Frisian. All data and code are available on GitHub 2 . Related Work Previous work on dependency parsing for spoken data created English annotations for conversational agents (Davidson et al., 2019) and Slovenian data (Dobrovoljc and Martinc, 2018) . Partanen et al. (2018) create a treebank for spoken Komi-Zyrian with code-switching to Russian. Contemporary to our treebank, C \u00b8etinoglu and C \u00b8\u00f6ltekin (2019) created a treebank for spoken code-switched Turkish-German. They adapt the guidelines to deal with the spoken and code-switch nature of the data. Seddah et al. (2020) create a treebank for an Arabic dialect that contains a high amount of code-switching. Meechan-Maddon and Nivre (2019) and Blodgett et al. (2018) show that annotating a small amount of target language and target domain data outperforms a cross-lingual setup, but we focus on a pure zero-shot scenario instead. Previous work on zero-shot parsing has mainly focused on annotation projection using parallel data (Barry et al., 2019) and selecting transfer treebanks (Meechan-Maddon and Nivre, 2019) . Previous work exploring more fine-grained selection methods are either focusing only on 3 domains (and multiple topics within the news) and 3 corpora (Plank and Van Noord, 2011) , or focus on parser selection during test time (Litschko et al., 2020) . Data & Annotations Data The creators of the FAME! corpus (Yilmaz et al., 2016) had already transcribed and segmented the data and annotated the code-switches. We copied their word-level language labels to our miscellaneous column. The data consists of broadcasts from Omrop Frysl\u00e2n (Frisian radio broadcaster) and mainly contains spontaneous interviews. The manually annotated radio broadcasts contain approximately 18.5 hours of speech and contains 3837 word-and sentence-level code-switches (Yilmaz et al., 2016) . The majority of these switches are from Frisian speakers who switch to Dutch, because as Yilmaz et al. (2016) also mention it is not common for Dutch speakers to switch to Frisian. 67.8% of the words are Frisian and 26.1% are Dutch. The remainder of the words are annotated as being Frisian-Dutch, are hesitations (like \"eh\") or are of a different language. From this corpus we randomly selected and annotated 400 utterances. Each utterance contains at least one switch from Frisian to Dutch or the other way around. In our selection there are 144 different speakers, 135 of them Frisian speakers (the other 9 Dutch). Most of the speakers only occur one, two or three times. In the corpus 3,067 tokens are Frisian, 625 Dutch and 37 are other languages or annotated as Frisian-Dutch. The distributions over the languages for POS tags can be found in Appendix A. Annotations As a starting point, 150 utterances where annotated by two annotators (the authors of the paper, who have backgrounds in NLP, parsing and linguistics), in three batches of 50 utterances. After each batch, disagreements where discussed and resolved. Afterwards, 250 more utterances where annotated by one of the annotators and checked by the other. Table 1 shows the inter-annotator agreement between the three initial batches of annotation. We report accuracy over Universal Part-of-speech tags, Unlabelled Attachment Score (UAS) and Labelled Attachment Score (LAS) (Zeman et al., 2018) . Most of our disagreements were due to difficulties with non-standard constructions, sentence segmentation, ambiguities of utterances and the fact that the guidelines had to be applied consistently, and in some cases had to be adapted. Adaptations to the guidelines can be found in Appendix B. For a more detailed discussion regarding the annotations we refer to Braggaar and van der Goot (2021). Analysis of disagreements To find trends in the disagreements, we calculated confusions over all labels (POS tags and Universal Dependency relations). The most occurring confusion for the POS tags that is solved in the third round is between ADV-AUX. This is due to shortcuts in the annotation tool ConlluEditor (Heinecke, 2019) . Another confusion that is mostly solved in the third round is the confusion between AUX and VERB. For the Universal Dependency relations, the most common disagreements included amodadvmod confusions and differences in selecting the root. Experiments Data selection First, we selected 24 treebanks of languages close to Frisian, which contained code-switching or consisted of transcribed speech. The languages included are Dutch, English, German, French, Naija, Afrikaans, Danish and Hindi-English 3 . As a baseline, 24 parsers were trained, each on a single treebank. The eight best scoring treebanks (based on LAS score) were used for further experiments. 3 The names of all treebanks can be found in Appendix C The eight best scoring treebanks were: Dutch Alpino ( Van der Beek et al., 2002) , Dutch LassySmall (Van Noord et al., 2013) , German GSD, German PUD, English GUM (Zeldes, 2017) , English EWT (Silveira et al., 2014) , English ParTUT (Sanguinetti and Bosco, 2015) and the Tweebank by Liu et al. (2018) . We chose to use these eight because we observed a drop in performance beyond the lowest scoring treebank in this selection. Latent Dirichlet Allocation For the data selection, we use Latent Dirichlet Allocation (LDA) with character 1-5 grams and word 1-2 grams as features. We choose LDA, because it is unsupervised, efficient and returns the probability of instances belonging to each clusters. Traditionally LDA is commonly used for topic classification, but when given data from multiple languages, we hypothesize that it will cluster based on language similarity instead. In LDA, one has to define the number of classes; we started with eight topics (because we used eight treebanks) and multiplied the number by two for the successive runs (8, 16, 32, 64 and 128). Experimental setup We use the full training data from all eight treebanks. For the treebanks without train, we use the test data, as we do not perform any experiments on the source data. For the LDA target data, we sampled sentences from the train split of the FAME! (Yilmaz et al., 2016) corpus. Note that we did not use the samples that are in our annotated data set. After the clustering, we select similar sentences based on the euclidean distance to the mean of the Frisian input. We ran experiments with 8, 16, 32, 64 and 128 LDA components. From the results we selected 1000, 2000 and 4000 sentences for training to see if the number of sentences in training was also influential. We did a total of 75 runs (8, 16, 32, 64, 128 components with each 1000, 2000 and 4000 sentences, 5 seeds). Training was done using a deep biaffine parser as implemented by MaChAmp 0.2 (van der Goot et al., 2020) with default parameters and mBERT embeddings (Devlin et al., 2019) . Five random seeds were used for training. Results Overall, our best model consisted of 128 components and used 2000 sentences. the effect of changing the number of components when using 2000 sentences for training. While 128 components is the best for LAS and UAS, 8 components gave the best result for POS. When using the best performing number of components ( 128 ) and varying the amount of training data (Table 3 ), we can also see that the best model only needs 2000 sentences while using more sentences seemed to improve only the POS score. Table 4 shows the results of our best model (128 components, 2000 sentences) compared to the best single treebank parser (NL ALPINO) and training on the eight treebanks simultaneously. The scores for test are for LAS and UAS slightly lower than in the development phase. Surprisingly the POS scores are a bit higher. Our model outperforms the baselines on the test scores for LAS and UAS (but not for POS). We tested significance with random Bootstrapping as done by Udapi (Popel et al., 2017) compared to both baselines. Unfortunately, none of the results have proven to be significant at an alpha of 0.05. A closer look at the data selection of our model (Table 5 ) shows that the training set consists of mainly Dutch data. Only few sentences are selected that are not in the Dutch treebanks. This comes as no surprise, as we have seen that in the total corpus 26.1% of the words are Dutch, and the Frisian language is related to Dutch. This also agrees with the fact that the Dutch treebanks perform best in single treebank training. Error Analysis We perform an error analysis of our best model and the baseline (NL ALPINO) on the development data. Table 6 shows the top five confusions on POS tags. The baseline is often unable to correctly predict interjections and our model does not seem to solve this confusion. Some of the difficulties that we have come across during annotation also arise here (e.g. ADV-ADJ and AUX-VERB). The confusions on dependency labels are similar for both models (Table 7 ). Our model is slightly better on discourse-parataxis, but other than that it shows no big improvements. This is not surprising as our data consists mainly out of Alpino sentences. The most occurring confusion (discourse-parataxis) was also a cause of disagreement in the annotation process. 7 What did not work? In an attempt to obtain better results we have tried three different approaches to improve our best model. Muller et al. (2020) have shown that transliteration to a script of a related high source language on which the language model is trained leads to better results. Even though Frisian is not in a different script as the training data, it does contain a high amount of diacritics, which results in both many unseen wordpieces during testing, as well as differences in tokenization 4 . We attempt to overcome this mismatch by removing the diacritics from the characters in our Frisian data. In our development set, which consists of 150 utterances, there are 44 utterances with at least one diacritic and there are a total of 53 tokens with diacritics. The second approach tries to make our training data more similar to our Frisian spoken data. A similar approach is taken by Blodgett et al. ( 2018 ), they create synthetic data following Internet-specific conventions and syntactic features of African-American English. This synthetic data proved to be helpful for performance. Vania et al. (2019) also try similar methods of data augmentation and found that when no source treebank is available, data augmentation can be very helpful. In our case, the utterances are not always \"full sentences\". An example from our development set is \"d\u00ear kinnen we in hele hoop oer sizze mar\" (\"we can say a lot about that but\"). Normally the sentence would continue after \"mar\" (but) and \"mar\" would connect to this next part, but in this case the utterance stops at this point. We chose to connect this \"mar\" with an orphan relation to the root. These kind of \"non-standard\" constructions occur relatively often in our target data compared to the non-spoken treebanks in our training data. We tried to make our training data more similar by cropping sentences and adding an orphan relation. We did this for approximately the same percentage of utterances that have this construction in our development data (approximately 9 %). As a final experiment we replaced mBERT with XLM-R. The results of all three experiments can be found in Table 8 . As can be seen, our diacritics and orphan experiments do not increase our scores. Only the UAS increases slightly when cropping some sentences, but unfortunately the LAS drops in both cases. The results with XLM-R are highly similar to the mBERT scores. Only the POS scores increase slightly but the difference in scores for UAS and LAS is very small. Conclusion In this paper we explored parsing and annotating the low-resource language Frisian and we have shown that selecting more similar training data (by using LDA) can lead to improvements in scores. B Annotation guidelines We tried to follow the universal dependency guidelines as much as possible. In cases we could not follow them precisely we discussed and made our own guidelines: \u2022 To determine if something is an ADJ or ADV we use a dictionary. If the word is not in the dictionary we use the frequencies of the Alpino treebank. For the decision between advmod and amod, we used amod only for nominals. \u2022 If \"toen/want/tot/dan\" are at the beginning of utterances we tag them as mark and not as conj. \"en\" is cconj or cc. \u2022 Discourse is always attached to the highest node without any projectivity. Figure 3 shows an utterance with a discourse relation. C List of treebanks",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 4.320199066265573e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not provide any specific information regarding funding from defense, corporate entities, research agencies, foundations, or any other sources. Without explicit mention of funding, it is not possible to accurately determine the sources of financial support for the research."
}