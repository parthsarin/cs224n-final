{
    "framing_question": "If this paper introduces a dataset, how is data filtered out during its creation, if at all?",
    "response": "The provided paper does not introduce a dataset, so I cannot answer this question from the provided context.",
    "article": "Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, simplification, and style transfer. These tasks share a common trait -they exhibit a large amount of textual overlap between the source and target texts. Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs. This tutorial 1 provides a comprehensive overview of text-editing models and current state-of-the-art approaches, and analyzes their pros and cons. We discuss challenges related to productionization and how these models can be used to mitigate hallucination and bias, both pressing challenges in the field of text generation. Introduction After revolutionizing the field of machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) , sequence-to-sequence (seq2seq) methods have quickly become the standard approach for not only multilingual but also for monolingual sequence transduction / text generation tasks, such as text summarization, style transfer, and grammatical error correction. While delivering significant quality gains, these models, however, are prone to hallucinations (Maynez et al., 2020; Pagnoni et al., 2021) . The seq2seq task setup (where targets are generated from scratch word by word) overlooks the fact that in many monolingual tasks the source and target sequences have a considerable overlap, hence targets could be reconstructed from the source inputs by applying a set of edit operations. Text-editing models attempt to address some of the limitations of seq2seq approaches and there has been recently a surge of interest in applying them to a variety of monolingual tasks including text simplification (Dong et al., 2019; Mallinson et al., 2020; Agrawal et al., 2021) , grammatical error correction (Awasthi et al., 2019; Omelianchuk et al., 2020; Malmi et al., 2019; Stahlberg and Kumar, 2020; Rothe et al., 2021; Chen et al., 2020; Hinson et al., 2020; Gao et al., 2021) , sentence fusion (Malmi et al., 2019; Mallinson et al., 2020) (see an example in Figure 1 ), MT automatic post-editing (Gu et al., 2019; Zietkiewicz, 2020; Mallinson et al., 2020) , text style transfer (Reid and Zhong, 2021; Malmi et al., 2020) , data-to-text generation (Kasner and Du\u0161ek, 2020) , and utterance rewriting (Liu et al., 2020; Voskarides et al., 2020; Jin et al., 2022) . Text-editing approaches claim to be more accurate or on-par with seq2seq baselines especially in low resource settings, less prone to hallucinations and faster at inference time. These advantages have generated a substantial and continued level of interest in text-editing research. The goal of this tutorial is to provide the first comprehensive overview of the family of text-editing approaches and to offer practical guidelines for applying them to a variety of text-generation tasks. Target Audience and Prerequisites The tutorial is intended for researchers and practitioners who are familiar with generic seq2seq textgeneration methods, such as Transformer (Vaswani et al., 2017) and pre-trained language models like BERT (Devlin et al., 2019) . However, prior experience with text-editing models is not required to be able to follow the tutorial. We expect the topic to attract people in both academia and industry. The high-sample efficiency and low-computational requirements of text-editing models (Malmi et al., 2019; Mallinson et al., 2020) makes them an attractive baseline, e.g., for researchers developing new text-generation tasks for which large training sets do not yet exist. Moreover, the high-inference speed of text-editing methods, owing to their often non-autoregressive architecture (Awasthi et al., 2019; Mallinson et al., 2020) , makes them suitable for building real-time applications. Tutorial Outline The structure of the tutorial with duration estimates for different sections are shown in Table 1 . Below we provide brief descriptions for each section. Introduction. We first define the family of textediting methods: Text-editing models are sequencetransduction methods that produce the output text by predicting edit operations which are applied to the inputs. In contrast, the traditional seq2seq methods produce the output from scratch, token by token. We summarize the main pros and cons of these two approaches and provide guidelines for choosing which approach is more suitable for a given task. Model Design. The similarities and differences of a set of popular text-editing methods will be analyzed in terms of the types of edit operations they employ, their tagging architecture, and whether they are auto-regressive or feedforward. We also discuss methods for converting target texts into target edit sequences, a task which often does not have a unique solution. Table 2 provides a summary of the similarities and differences between the methods covered in the tutorial. Applications. A key criterion for determining whether text-editing models are a good fit for a given application is the average degree of overlap between source and target texts. The higher the overlap, the more input tokens can be reused to generate the target, thus resulting in a simpler edit sequence. We give an overview of applications with a high degree of overlap to which text-editing methods have been applied to. Then we do a deep dive in to the following applications: grammatical error correction, text simplification, unsupervised style transfer, and incomplete utterance rewriting. Controllable Generation. Text-editing models with a restricted vocabulary of phrases to insert (Malmi et al., 2019; Jin et al., 2022) or with linguistically informed suffix-transformation operations (Awasthi et al., 2019; Omelianchuk et al., 2020) are less prone to different types of hallucination since the models cannot produce arbitrary outputs. Moreover, the restricted vocabulary makes it feasible to manually refine the list of phrases that the model can insert. Another route through which the decomposition of the generation task into explicit edit operations can improve controllability is via biasing of certain types of edits to control how often the model will insert new text (Dong et al., 2019; Omelianchuk et al., 2020) . Controllable generation with editing models can be useful for generating large synthetic datasets with a desired distribution of errors, which yields improvements in tasks such (Dong et al., 2019) Simplification Felix (Mallinson et al., 2020) multiple GECToR (Omelianchuk et al., 2020) ( ) GEC HCT (Jin et al., 2022) Utterance Rewriting LaserTagger (Malmi et al., 2019) multiple LevT (Gu et al., 2019) ( ) multiple LEWIS (Reid and Zhong, 2021) Style Transfer Masker (Malmi et al., 2020) multiple PIE (Awasthi et al., 2019) GEC Seq2Edits (Stahlberg and Kumar, 2020) ( ) multiple SL (Alva-Manchego et al., 2017) Simplification Table 2 : Overview of selected text-editing methods. as grammatical error correction (Stahlberg and Kumar, 2021 ). We will provide concrete examples of the aforementioned control measures and their effects. Multilingual Text Editing. Most text-editing models, like text-generation models in general, are evaluated on English, but there are also methods evaluated or specifically developed for other languages, including Chinese (Hinson et al., 2020; Liu et al., 2020) , Czech (N\u00e1plava and Straka, 2019) , German (Mallinson et al., 2020) , Russian (Stahlberg and Kumar, 2020) , and Ukrainian (Syvokon and Nahorna, 2021) . Apart from general tokenization-related challenges discussed in (Mielke et al., 2021) , an additional challenge with applying text-editing methods to morphologically rich languages is a potential mismatch between the subword tokens, on which the underlying sequence labeling model operates, and the morphemes or affixes, on which the edits should happen. Possible solutions to this challenge include developing custom inflection operations (Awasthi et al., 2019; Omelianchuk et al., 2020) or learning them from the data (Straka et al., 2021) , and using more fine-grained edit operations, such as character-level edits (Gao et al., 2021 ). An additional challenge when building a truly multilingual model-as opposed to one model per language-is to ensure that it is not skewed towards a particular language or a set of languages (Chung et al., 2020) while being computationally efficient. Productionization. We discuss how casting a text-generation problem as a text-editing task often allows the use of significantly faster and more dataefficient model architectures, without sacrificing output quality. We make use of the TensorFlow Profiler 2 to compare latencies of text-editing and non-text-editing solutions for an example problem, and illustrate where the time savings come from. Recommendations and Future Directions. We provide practical guidelines for when to use (and when not to use) text-editing methods (see Figure 2 for a summary). We also outline possible future directions which include: (i) learned edit operations, (ii) studying the effects of different subword segmentation methods since these typically determine the granularity at which the edit operations are applied, (iii) text-editing-specific pre-training methods, (iv) sampling strategies for text-editing methods, and (v) studying the effects of scaling up text-editing methods, a strategy that has been found to be very effective for many other text-generation methods (Brown et al., 2020; Chowdhery et al., 2022) . Diversity Considerations A significant portion of the tutorial is devoted to discussing multilingual text-editing, including applying text-editing models to morphologically rich languages which presents specific challenges related to larger vocabularies and the need to edit word affixes. The presenters come from both academia and industry, are native speakers of 8 languages based in 4 different countries (Switzerland, Germany, Canada, USA), and are of different seniority levels from a PhD student to a Senior Staff Research Scientist. Reading List Before the tutorial, we expect the audience to read (Vaswani et al., 2017) and (Devlin et al., 2019) . For references to text-editing works that will be discussed in the tutorial, see Table 2 . Breadth. 50% of the methods that will be discussed in the tutorial (cf. Ethical Considerations Text-generation methods have the potential to generate non-factual (Maynez et al., 2020; Pagnoni et al., 2021; Kreps et al., 2020) and offensive content (Gehman et al., 2020) . Furthermore, training these models on uncurated data can lead to the models replicating harmful views presented in the training data (Bender et al., 2021) . Text-editing models are also susceptible to these issues, but they have been shown to mitigate some of them. Specifically, they reduce the likelihood of different types of hallucination (Malmi et al., 2019) and their higher sample efficiency (Malmi et al., 2019; Mallinson et al., 2020) enables more careful curation of the training data. The tutorial will discuss the ethical issues related to text generation and provide concrete examples on how text-editing models can help mitigate them.",
    "funding": {
        "military": 0.0,
        "corporate": 1.9361263126072004e-07,
        "research agency": 0.0,
        "foundation": 1.9361263126072004e-07,
        "none": 0.9999989719621284
    }
}