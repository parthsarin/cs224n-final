{
    "article": "In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar. Introduction It is well-known that fully lexicalised grammar formalisms such as LTAG (Joshi and Schabes, 1991) are difficult to parse with efficiently. Each word in the parser's input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (Vijay-Shanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate tile processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vijay-Shanker (1997) addresses this problem from two different perspectives. Evans and Weir (1997) outline a technique for compiling LTAG grammars into automata which are then merged to introduce some sharing of structure. Chen and Vijay-Shanker (1997) use underspecified tree descriptions to represent sets of trees during parsing. Tile present paper takes the former approach, but extends our previous work by: \u2022 showing how merged automata can be minirnised, so that they share as much structure as possible; \u2022 showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; \u2022 providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of it 1 . 2 Automaton-based parsing Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; Vijay-Shanker and Weir, 1993 ) maintain a parse table, a set of items corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for parser actions. Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or 1However, due to lack of space, no proofs and only minimal informal descriptions are given in this paper. right); 2. extending a constituent by adjoining a surrounding complete auxiliary constituent; 3. predicting the span of the foot node of an auxiliary constituent (to the left or right). Parsing is complete when all possible parser actions have been executed. In a completed parse table it is possible to trace the sequence of items corresponding to the recognition of an elementary tree from its lexical anchor upwards. Each item in the sequence corresponds to a node in the tree (with the sequence as a whole corresponding to a complete traversal of the tree), and each step corresponds to the parser action that licensed the next item, given the current one. From this perspective, parser actions can be restated relative to the items in such a sequence as: 1. substitute a complete subconstituent (on the left or right); 2. adjoin a surrounding complete auxiliary constituent; 3. predict the span of the tree's foot node (to the left or right). The recognition of the tree can thus be viewed as the computation of a finite state atttomaton, whose states correspond to a traversal of the tree and whose input symbols are these relativised parser actions. This perspective suggests a re-casting of the conventional LTAG parser in terms of such automata 2. For this automaton-based parser, the grammar structures are not trees, but automata corresponding to tree traversals whose inputs are strings of relativised parser actions. Items in the parse table reference automaton states instead of tree addresses, and if the automaton state is final, the item represents a complete constituent. Parser actions arise as before, but are executed by relativising them with respect to the incomplete item participating in the action, and passing this relativised parser action as the next input symbol for the automaton referenced by that item. The resulting state of that automaton is then used as the referent of the newly licensed item. On a first pass, this re-casting is exactly that: it does nothing new or different from the original ~Evans and Weir (1997) provides a longer informal introduction to this approach. parser on the original grammar. However there are a number of subtle differences3: \u2022 the automata are more abstract than the trees: the only grammatical information they contain are the input symbols and the root node labels, indicating the category of the constituent the automaton reeognises; ,, automata for several trees can be merged together and optimised using standard well-studied techniques, resulting in a single automaton that recognises many trees at once, sharing as many of the common parser actions as possible. It is this final point which is the focus of this paper. By representing trees as automata, we can merge trees together and apply standard optinfisation techniques to share their common structure. The parser will remain unchanged, but will operate more efficiently where structure has been shared. Additionally, because the automata are more abstract than the trees, capturing precisely the parser's view of the trees, sharing may occur between trees which are structurally quite different, but which happen to have common parser actions associated with them. 3 Merging and minimising automata Combining the automata for several trees can be achieved using a variety of standard algorithms (Huffman, 1954; Moore, 1956) . However any transformations must respect one important feature: once the parser reaches a final state it needs to know what tree it has just recognised 4. When automata for trees with different root categories are merged, the resulting automaton needs to somehow indicate to the parser what trees are associated with its final states. In Evans and Weir (1997) , we combined automata by introducing a new initial state with e-transitions to each of the original initial states, 3A further difference is that the traversal encoded in the automaton captures part of the parser's control strategy. However for simplicity we assume here a fixed parser control strategy (bottom-up, anchor-out) and do not pursue this point further - Evans and Weir (1997) offers some discussion. 4For recognition alone it only needs to know the root category of the tree, but to recover the parse it needs to identify the tree itself. and then determinising the resulting automaton to induce some sharing of structure. To recover trees, final automaton states were annotated with the number of the tree the final state is associated with, which the parser can then readily access. However, the drawback of this approach is that differently annotated final states can never be merged, which restricts the scope for structure sharing (minirnisation, for example, is not possible since all the final states are distinct). To overcome this, we propose an alternative approach as follows: \u2022 each automaton transition is annotated with the set of trees which pass through it: when transitions are merged in automaton optimisation, their annotations are unioned; \u2022 the parser maintains for each item in the table the set of trees that are valid for the item: initially this is all the valid trees for the automaton, but gets intersected with the annotation of any transition followed; also if two paths through the automaton meet (i.e., an item is about to be added for a second time), their annotations get unioned. This approach supports arbitrary merging of states, including merging all the final states into one. The parser maintains a dynamic record of which trees are valid for states (in particular final states) in the parse table. This means that we can minimise our automata as well as determinising them, and so share more structure (for example, common processing at the end of the recognition process as well as the beginning). Recognition and parse recovery We noted above that a parsing algorithm needs to be able to access the tree that an automaton has recognised. The algorithm we describe below actually needs rather more information than this, because it uses a two-phase recognition/parse-recovery approach. The recognition phase only needs to know, for each complete item, what the root label of the tree recognised is. This can be recovered from the ~valid tree' annotation of the complete item itself (there may be more than one valid tree, corresponding to a phrase which has more than one parse which happen to have been merged together). Parse recovery, however, involves running the recogniser 'backwards' over the completed parse table, identifying for each item, the items and actions which licensed it. A complication arises because the automata, especially the merged automata, do not directly correspond to tree structure. The recogniser returns the tree recognised, and a search of the parse table reveals the parser action which completed its recognition, but that information in itself may not be enough to locate exactly where in the tree the action took place. However, the additional information required is static, and so can be pre-compiled as the automata themselves are built up. For each action transition (the action, plus the start and finish states) we record the tree address that the transition reaches (we catl this the action-site, or just a-site for short). During parse recovery, when the parse table indicates an action that licensed an item, we look up the relevant transition to discover where in the tree (or trees, if we are traversing several simultaneously) the present item must be, so that we can correctly construct a derivation tree. 5 Technical details 5.1 Constructing the automata We identify each node in an elementary tree 7 with an elementary address 7/i. The root of 7 has the address 7/e where e is the empty string. Given a node v/i, its n children are addressed from left to right with the addresses 7/il,...v/in, respectively. For convenience, let anchor (7) and foot (7) denote the elementary address of the node that is the anchor and footnode (if it has one) of 7, respectively; and label (7/i) and parent (7/i) denote the label of 7/i and the address of the parent of v/i, respectively. In this paper we make the following assumuptions about elementary trees. Each tree has a single anchor node and therefore a single spine 5. In the algorithms below we assume that nodes not on the spine have no children. In practice, not all elementary LTAG trees meet these conditions, and we discuss how the approach described here might be extended to the more gen-5The path from the root to the anchor node. eral case in Section 6. Let \"y/i be an elementary address of a node on the spine of 7 with n children ~f/il,...,',/ilk,...,~/in for n > 1, where k is such that \"~ilk dominates anchor (\"/). 7/ik+l ifj=l&n>k 7~i j-1 if2_<j_<k next(3'/ij)= 3\"/ij+l ifk<j <n 7/i otherwise next defines a function that traverses a spine, starting at the anchor. Traversal of an elementary tree during recognition yields a sequence of parser actions, which we annotate as follows: the two actions A and ~ indicate a substitution of a tree rooted with A to the left or right, respectively; __A, and ~_A_ indicate the presence of the foot node, a node labelled A, to the left or right, respectively; Finally A indicates an adjunetion of a tree with root and foot labelled A. These actions constitute the input language of the automaton that traverses the tree. This automaton is defined as follows (note that we use e-transitions between nodes to ease the construction -we assume these are removed using a standard algorithm). Define as follows the finite state automaton M = (Q, E, _l_ [anchor ('y)], 6, F). Q is the set of states, E is the input alphabet, q0 is the initial state, 5 is the transition relation, and F is the set of final states. Q = { T['Ui ], \u00b1[3\"/i] [3\"/i is /i) = A } { (_1_[3'/i], ~, T[3'/i D I ?/i adjunction node } { (T[7/i], ~, T[3\"/i]) I 3\"/i adjunction node, label (3\"/i) = d } In order to recover derivation trees, we also define the partial function a-site(q,a,q ~) for (q, a, q') E 5 which provides information about the site within the elementary tree of actions occurring in the automaton. q,) = ~ 3\"/i ifa\u00a2(~; q' = T[~//i] a-site(q, a~ undefined otherwise ( Combining Automata Suppose we have a set of trees F = {3'1,...,7n }. Let M.n,... ,M.Y~ be the e-free automata that are built from members of the set F using the above construction, where for 1 < k < n, Mk = (Qk,Ek,qk,bk, Fk) . Construction of a single automaton for F is a two step process. First we build an automaton that accepts all elementary computations for trees in F; then we apply the standard automaton determinization and minimization algorithms to produce an equivalent, compact automaton. The first step is achieved simply by introducing a new initial state with e-transitions to each of the qk: Let M = (Q, E, q0, 5, F) where Q = ( q0 } u Ul<k< Oi; Z = I.Jl<k<, Ek F = Ul<k<,~ F~ 5 = Ul<~<,~(q0, e, qk) U U~<k<~ 5k. We determinize and then minimize M using the standard set-of-states constructions to produce Mr = (Q', E, Q0,5',F'). Whenever two states are merged in either the determinizing or minimizing algorithms the resulting state is named by the union of the states from which it is formed. For each transition (QI,a, Q2) E 5' we define the function a-sites(Q1, a, Q2) to be a set of elementary nodes as follows: a-sites(Q1, a, Q2) = [.Jq~ eQ~,q2eQ2 a-site(ql, a, q2) Given a transition in Mr, this function returns all the nodes in all merged trees which that tran-sition reaches. Finally, we define: cross(Q1, a, Q2) = { 3' 17/i E a-sites(Q1, a, Q2) } This gives that subset of those trees whose elementary computations take the Mr through state Q1 to Q2. These are the transition annotations referred to above, used to constrain the parser's set of valid trees. The Recognition Phase This section illustrates a simple bottom-up parsing algorithm that makes use of minimized automata produced from sets of trees that anchor the same input symbol. The input to the parser takes the form of a sequence of minimized automata, one for each of the symbols in the input. Let the input string be w = al...an and the associated automata be MI,... Mn where Mk = (Qk, Ek, qk, 5k, Fk) for 1 < k < n. Let treesof(Mk) = Fk where Fk is a set of the names of those elementary trees that were used to construct the automata Mk. During the recognition phase of the algorithm, a set I of items are created. An item has the form (T, q, [l, r, l', r']) where T is a set of elementary tree names, q is a automata state and l,r,l',r' E {0,... ,n,--} such that either l<l'<r'<rorl<randl'=r'=-. The indices l, l', #, r are positions between input symbols (position 0 is before the first input symbols and position n is after the final input symbol) and we use Wp,p, to denote that substring of the input w between positions p and p'. I can be viewed as a four dimensional array, each entry of which contains a set of pairs comprising of a set of nonterminals and an automata state. Roughly speaking, an item (T, q, [I, r, l', r]) is ineluded in I when for every 7 E T, anchored by some ak (where I < k _< r and if l' #then k < l' or r' _< k); q is a state in Qk, such that some elementary subcomputation reaching q from the initial state, qk, of Mk is an initial substring of the elementary computation for 7 that reaches the elementary address 7/i, the subtree rooted at 7/i spans Wt,r, and ifT/i dominates a foot node then that foot node spans We,r,, otherwise l' = r' = -The input is accepted if an item (T, qf,[O,n, , ] ) is added to I where T contains some initial tree rooted in the start symbol S and ql E Fk for some k. When adding items to I we use the procedure add(T, q, [l, r, l', r']) which is defined such that if there is already an entry (T', q, [l, r, l', r']) E I for some T' then replace this with the entry (T U T', q, [l, r,l', rq)6; otherwise add the new entry (T, q, [/, r, l', r']) to I. I is initialized as follows. For each k E { 1,... ,n } call add(T, qk,[k-1,k,-,-]) where T = treesof(Mk) and qk is the initial state of the automata Mk. We now present the rules with which the complete set I is built. These rules correspond closely to the familiar steps in existing bottomup LTAG parser, in particular, the way that we use the four indices is exactly the same as in other approaches (Vijay-Shanker and Joshi, 1985) . As a result a standard control strategy can be used to control the order in which these rules are applied to existing entries of I. 1. If (T,q,[l,r,l',r'] ), (T',qs,[r,r\", -, -] )El, qf E Fk for some k, (q,A,q~) E 5k, for some U, label (~,l/e) = A from some ~/i E T ~ ~z T\" = T VI cross(q,A,q~) then call add(T\", q', [1, r\", l', r']). 2. If <T, q, [l, r, l', r']), iT', q$, [l\", l, -, -]> e I, qf E Fk for some k, (q,A,q') E 5k, for some k', label(7'/e) = A from some 71 E T' gz T\" = T A cross(q,a,q') then call add(T\", q', [l\", r, l', r']>. 3. If(T,q,[1, r,-,-]) EI, (q, A,q,) eSk for some k & T' = T A cross(q,_A,,q') then for each r ~ such that r < r / < n call add(T', q', [/, r', r, r']). 4. If (T, q, [l, r, -, -]) E I, (q,\u00f7A__,q') E ~k for some k & T' = T M cross(q,\u00f7A,q~) then for each l' such that 0 < l' < l call add(T', q', [/', r, l', l]). 5. If (T,q,[l,r,l',r'] ), (T',qf,[l\",r\",l,r] ) G I, qf E Fk for some k, (q,A,q') E 5k' for some k', label (~//e) = A from some ~/ E T' & T\" = T V~ eross(q,~,q') then call add(T\", q', [l\", r\", l', r']). \u00b0This replacement is treated as a new entry in the table. If the old entry has already licenced other entries, this may result in some duplicate processing. This could be eliminated by a more sophisticated treatment of tree sets. The running time of this algorithm is O(rfi) since the last rule must be embedded within six loops each of which varies with n. Note that although the third and fourth rules both take O(n) steps, they need only be embedded within the 1 and r loops. Recovering Parse Trees Once the set of items I has been completed, the final task of the parser is to a recover derivation tree 7. This involves retracing the steps of the recognition process in reverse. At each point, we look for a rule that would have caused the inclusion of item in I. Each of these rules involves some transition (q, a, q') ~ 5k for some k where a is one of the parser actions, and from this transition we consult the set of elementary addresses in a-sites(q, a, q') to establish how to build the derivation tree. We eventually reach items added during the initialization phase and the process ends. Given the way our parser has been designed, some search will be needed to find the items we need. As usual, the need for such search can be reduced through the inclusion of pointers in items, though this is at the cost of increasing parsing time. There are various points in the following description where nondeterminism exists. By exploring all possible paths, it would be straightforward to produce an AND/OR derivation tree that encodes all derivation trees for the input string. We use the procedure der((T, q, [l, r, l', r']), r) which completes the partial derivation tree r by backing up through the moves of the automata in which q is a state. A derivation tree for the input is returned by the call der (<T, qf, [O, n, -, -] ), ~-) where (T, qf , [O, n, -, -] ) E I such that T contains some initial tree 7 rooted with the start nonterminal S and qI is the final state of some automata Mk, 1 < k <_ n. r is a derivation tree containing just one node labelled with name 7. In general, on a call to der(<T, q, [l, r, l', r']), T) we examine I to find a rule that has caused this item to be included in I. There are six rules to consider, corresponding to the five recogniser rules, plus lexical introduction, as follows: 1. If (T',q',[l,r\",I',r'] >, (T\",qf,[r\",r,-,-] l e TDerivation trees are labelled with tree names and edges are labelled with tree addresses. I, qI E Fk for some k, (q',A,q) E 6k, for some k', 7 is the label of the root of % 3' E T', label (-'//e) = A from some 7' E T\" & ~//i E a-sites(q', A,q), then let z' be the derivation tree containing a single node labelled 7', and let T\" be the result of attaching der((T\", q/, [r\", r, -,-]), ~-') under the root of T with an edge labelled the tree address i. We then complete the derivation tree by calling der((T', q', [l, r\", l', r']}, T\"). 2. If (T', q', [r\", r, l', r']>, <T\", qf, [l, r\", -, -]) e I, qf E Fk for some k, (q',A,q) ~ @ for some U 7 is the label of the root of y, 7 E T', label (7'/e) = A from some 7' E T\" & 7/i E a-sites(q', +__a, q), then let r' be the derivation tree containing a single node labelled 7', and let r\" be the result of attaching der((T\", qs, [l, r\", -, -]), r') under the root of r with an edge labelled the tree address i. We then complete the derivation tree by calling der((T', q', [r\", r, l', r']>, r\"). 3. If r = r', <T',q',[l,l',-,-] ) ~ I and (q,,_A,,q) 6 c~ k for some k, 7 is the label of the root of T, 7 E T' and foot (7) E a-sites(q',_A,, q) then make the call der((T', q', [l, l', -, -]), r). 4. If l = l', <T',q',[r',r,-,-] ) E I and (q,,\u00f7Aq,) E 6k tbr some k, 3' is the label of the root of % 7 E T' and foot (3') E a-sites(q', \u00f7A q) then make the call der(<T', q', [r', r, -, -]>, ~-). 5. If <T',q',[l\",r\",l',r'] ), <T\",qf,[l,r,l\",r\"] ) E I, qs E Fk for some k, (q',A,q) E @ for some k', 7 is the label of the root of \"r, 3' E T', label (7'/e) = A from some 7' E T\" and \"y/i E a-sites(q', A,q), then let T' be the derivation tree containing a single node labelled 7% and let \"r\" be the result of attaching der(<T\",qi, [1, r,l\",r\"]), r') under the root of -r with an edge labelled the tree address i. We then complete the derivation tree by calling der(<T', q', [/\", r\", l', r']), ~-\"). 6. If I + 1 = r, r' = l' = -q is the initial state of M~, 3' is the label of the root ofT-, 7 E T, then return the final derivation tree 7-. Discussion The approach described here offers empirical rather than formal improvements in performance. In the worst case, none of the trees in the grammar share any structure so no optimisation is possible. However, in the typical case, there is scope for substantial structure sharing among closely related trees. Carroll et al. (1998) report preliminary results using this technique on a wide-coverage DTG (a variant of LTAG) grammar. Table 1 gives statistics for three common verbs in the grammar: the total number of trees, the size of the merged automaton (before any optimisation has occurred) and the size of the minimised automaton. The final column gives the average of the number of trees that share each state in the automaton. These figures show substantial optimisation is possible, both in the space requirements of the grammar and in the sharing of processing state between trees during parsing. As mentioned earlier, the algorithms we have presented assume that elementary trees have one anchor and one spine. Some trees, however, have secondary anchors (for example, a subcategorised preposition). One possible way of including such cases would be to construct automata from secondary anchors up the secondary spine to the main spine. The automata for both the primary and secondary anchors associated with a lexieal item could then be merged, minimized and used for parsing as above. Using automata for parsing has a long history dating back to transition networks (Woods, 1970) . More recent uses include Alshawi (1996) and Eisner (1997) . These approaches differ from the present paper in their use of automata as part of the grammar formalism itself. Here, automata are used purely as a stepping-stone to parser optimisation: we make no linguistic claims about them. Indeed one view of this work is that it frees the linguistic descriptions from overt computational considerations. This work has perhaps more in common with the technology of LR parsing as a parser optimisation technique, and it would be interesting to compare our approach with a direct application of LR ideas to LTAGs.",
    "abstract": "In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [],
    "numcitedby": "0",
    "year": "1998",
    "month": "",
    "title": "A structure-sharing parser for lexicalized grammars"
}