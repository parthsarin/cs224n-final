{
    "article": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domainindependent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005) , text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007) , and even psychiatric diagnosis (Elvev\u00e5g et al., 2007; Bedi et al., 2015) . Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991 ) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts. The entity grid model (Barzilay and Lapata, 2008) and its extensions 1 capture the referential coherence of entities moving in and out of focus across a text. Each captures only a single aspect of coherence, and all focus on scoring existing sentences, rather than on generating coherent discourse for tasks like abstractive summarization. Here we introduce two classes of neural models for discourse coherence. Our discriminative models induce coherence by treating human generated texts as coherent examples and texts with random sentence replacements as negative examples, feeding LSTM sentence embeddings of pairs of consecutive sentences to a classifier. These achieve stateof-the-art (96% accuracy) on the standard domainspecific sentence-pair-ordering dataset (Barzilay and Lapata, 2008) , but suffer in a larger opendomain setting due to the small semantic space that negative sampling is able to cover. Our generative models are based on augumenting encoder-decoder models with latent variables to model discourse relationships across sentences, including (1) a model that incorporates an HMM-LDA topic model into the generative model and (2) an end-to-end model that introduces a Markovstructured neural latent variable, inspired by recent work on training latent-variable recurrent nets (Bowman et al., 2015; Serban et al., 2016b) . These generative models obtain the best result on a large open-domain setting, including on the difficult task of reconstructing the order of every sentence in a paragraph, and our latent variable generative model significantly improves the coherence of text generated by the model. Our work marks an initial step in building endto-end systems to evaluate open-domain discourse coherence, and more importantly, generating coherent texts given discourse contexts. The Discriminative Model The discriminative model treats cliques (sets of sentences surrounding a center sentence) taken from the original articles as coherent positive examples and cliques with random replacements of the center sentence as negative examples. The discriminative model can be viewed as an extended version of Li and Hovy's (2014) model but is practical at large scale 2 . We thus make this section succinct. Notations Let C denote a sequence of coherent texts taken from original articles generated by humans. C is comprised of a sequence of sentences C = {s n\u2212L , ..., s n\u22121 , s n , s n+1 , ..., s n+L } where L denotes the half size of the context window. Suppose each sentence s n consists of a sequence of words w n1 , ..., w nt , ..., w nM , where M is the number of tokens in s n . Each word w is associated with a K dimensional vector h w and each sentence is associated with a K dimensional vector x s . Each C contains 2L + 1 sentences, and is associated with a (2L + 1) \u00d7 K dimensional vector obtained by concatenating the representations of its constituent sentences. The sentence representation is obtained from LSTMs. After word compositions, we use the representation output from the final time step to represent the entire sentence. Another neural network model with a sigmoid function on the very top layer is employed to map the concatenation of representations of its constituent sentences to a scalar, indicating the probability of the current clique being a coherent one or an incoherent one. Weakness Two problems with the discriminative model stand out: First, it relies on negative sampling to generate negative examples. Since the sentence-level semantic space in the open-domain setting is huge, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don't cover the space of possible meanings. As we will show in the experiments section, the discriminative model performs competitively in specific domains, but not in the open domain setting. Secondly and more importantly, discriminative models are only able to tell whether an already-given chunk of text is coherent or not. While they can thus be used in tasks like extractive summarization for sentence re-ordering, they cannot be used for coherent text generation in tasks like dialogue generation or abstractive text summarization. The Generative Model We therefore introduce three neural generative models of discourse coherence. Model 1: the SEQ2SEQ Model and its Variations In a coherent context, a machine should be able to guess the next utterance given the preceding ones. A straightforward way to do that is to train a SEQ2SEQ model to predict a sentence given its contexts (Sutskever et al., 2014) . Generating sentences based on neighboring sentences resembles skip-thought models (Kiros et al., 2015) , which build an encoder-decoder model by predicting tokens in neighboring sentences. As shown in Figure 1a , given two consecutive sentences [s i , s i+1 ], one can measure the coherence by the likelihood of generating s i+1 given its preceding sentence s i (denoted by uni). This likelihood is scaled by the number of words in s i+1 (denoted by N i+1 ) to avoid favoring short sequences. L(s i , s i+1 ) = 1 N i+1 log p(s i+1 |s i ) (1) The probability can be directly computed using a pretrained SEQ2SEQ model (Sutskever et al., 2014) or an attention-based model (Bahdanau et al., 2015; Luong et al., 2015) . In a coherent context, a machine should not only be able to guess the next utterance given the preceding ones, but also the preceding one given the following ones. This gives rise to the coherence model (denoted by bi) that measures the bidirectional dependency between the two consecutive sentences: L(s i , s i+1 ) = 1 N i log p B (s i |s i+1 ) + log 1 N i+1 p F (s i+1 |s i ) (2) We One problem with the described uni and bi models is that sentences with higher language model probability (e.g., sentences without rare words) also tend to have higher conditional probability given their preceding or succeeding sentences. We are interested in measuring the informational gain from the contexts rather than how fluent the current sentence is. We thus propose eliminating the influence of the language model, which yields the following coherence score: L(s i , s i+1 ) = 1 N i [log p B (s i |s i+1 ) \u2212 log p L (s i )] + 1 N i+1 [log p B (s i+1 |s i ) \u2212 log p L (s i+1 )] (3) where p L (s) is the language model probability for generating sentence s. We train an LSTM language model, which can be thought of as a SEQ2SEQ model with an empty source. A closer look at Eq. 3 shows that it is of the same form as the mutual information between s i+1 and s i , namely log[p(s i+1 , s i )/p(s i+1 )p(s i )]. Generation The scoring functions in Eqs. 1, 2, and 3 are discriminative, generating coherence scores for an already-given chunk of text. Eqs. 2 and 3 can not be directly used for generation purposes, since they requires the completion of s i+1 before the score can be computed. A normal strategy is to generate a big N-best list using Eq. 1 and then rerank the N-best list using Eq. 2 or 3 (Li et al., 2015a) . The N-best list can be generated using standard beam search, or other algorithmic variations that promote diversity, coherence, etc. (Shao et al., 2017) . Weakness (1) The SEQ2SEQ model generates words sequentially based on an evolving hidden vector, which is updated by combining the current word representation with previously built hidden vectors. The generation process is thus not exposed to more global features of the discourse like topics. As the hidden vector evolves, the influence from contexts gradually diminishes, with language models quickly dominating. (2) By predicting a sentence conditioning only on its left or right neighbor, the model lacks the ability to handle the longerterm discourse dependencies across the sentences of a text. To tackle these two issues, we need a model that is able to constantly remind the decoder about the global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007) , and an end-to-end generative model with variational latent variables. HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012) . The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMM-LDA-GM in Fig. 1b ), in which an HMM-LDA model provides the SEQ2SEQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007) . Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sampled from a distribution based on the topic from previous sentence. Let t n denote the distribution of topics for the current sentence, where t n \u2208 R 1\u00d7T . We also associate each LDA topic with a K dimensional vector, representing the semantics embedded in this topic. The topicrepresentation matrix is denoted by V \u2208 R T \u00d7K , where T is the pre-specified number of topics in LDA. V is learned in the word predicting process when training encoder-decoder models. (2) Training encoder-decoder models: For the current sentence s n , given its topic distribution t n , we first compute the topic representation z n for s n using the weighted sum of LDA topic vectors: z n = t n \u00d7 V (4) z n can be thought of as a discourse state vector that stores the information the current sentence needs to convey in the discourse, and is used to guide every step of word generation in s n . We run the encoderdecoder model, which subsequently predicts tokens in s n given s n\u22121 . This process is the same as the vanilla version of SEQ2SEQ models, the only difference being that z n is incorporated into each step of decoding for hidden vector updates: p(s n |z n , s n\u22121 ) = M t=1 p(w t |h t\u22121 , z n ) (5) V is updated along with parameters in the encoderdecoder model. z n influences each time step of decoding, and thus addresses the problem that vanilla SEQ2SEQ models gradually lose global information as the hidden representations evolve. z n is computed based on the topic distribution t n , which is obtained from the HMM-LDA model, thus modeling the global Markov discourse dependency between sentences of the text. 3 The model can be adapted to the bi-directional setting, in which we separately train two models to handle the forward probability log p(t n |s n\u22121 , ...) and the backward one log p(t n |s n+1 ). The bi-directional (bi) strategy described in Eq. 3 can also be incorporated to remove the influence of language models. Weakness Topic models (either vanilla or HMM versions) focus on word co-occurrences at the document-level and are thus very lexicon-based. Furthermore, given the diversity of topics in a dataset like Wikipedia but the small number of topic clusters, the LDA model usually produces very coarse-grained topics (politics, sports, history, etc.), assigning very similar topic distributions to consecutive sentences. These topics thus capture topical coherence but are too coarse-grained to capture all the more fine-grained aspects of discourse coherence relationships. Variational Latent Variable Generative Models (VLV-GM) We therefore propose instead to train an end-to-end system, in which the meaning transitions between sentences can be naturally learned from the data. Inspired by recent work on generating sentences from a latent space (Serban et al., 2016b; Bowman et al., 2015; Chung et al., 2015) , we propose the VSV-GM model in Fig. 1c . Each sentence s n is again associated with a hidden vector representation z n \u2208 R K which stores the global information that the current sentence needs to talk about, but instead of obtaining z n from an upstream model like LDA, z n is learned from the training data. z n is a stochastic latent variable conditioned on all previous sentences and z n\u22121 : p(z n |z n\u22121 , s n\u22121 , s n\u22122 , ...) = N (\u00b5 true zn , \u03a3 true zn ) \u00b5 true zn = f (z n\u22121 , s n\u22121 , s n\u22122 , ...) \u03a3 true zn = g(z n\u22121 , s n\u22121 , s n\u22122 , ...) (6 ) where N (\u00b5, \u03a3) is a multivariate normal distribution with mean \u00b5 \u2208 R K and covariance matrix \u03a3 \u2208 R K\u00d7K . \u03a3 is a diagonal matrix. As can be seen, the global information z n for the current sentence depends on the information z n\u22121 for its previous sentence as well as the text of the context sentences. This forms a Markov chain across all sentences. f and g are neural network models that take previous sentences and z n\u22121 , and map them to a real-valued representation using hierarchical LSTMs (Li et al., 2015b) 4 . Each word w nt from s n is predicted using the concatenation of the representation previously build by the LSTMs (the same vector used in word prediction in vanilla SEQ2SEQ models) and z n , as shown in Eq.5. We are interested in the posterior distribution p(z n |s 1 , s 2 , ..., s n\u22121 ), namely, the information that the current sentence needs to convey given the preceding ones. Unfortunately, a highly non-linear mapping from z n to tokens in s n results in in-tractable inference of the posterior. A common solution is to use variational inference to learn another distribution, denoted by q(z n |s 1 , s 2 , ..., s N ), to approximate the true posterior p(z n |s 1 , s 2 , ..., s n\u22121 ). The model's latent variables are obtained by maximizing the variational lower-bound of observing the dataset: log p(s 1 , .., s N ) \u2264 N t=1 \u2212D KL (q(z n |s n , s n\u22121 , ...)||p(z n |s n\u22121 , s n\u22122 , ...)) + E q(zn|sn,s n\u22121 ,...) log p(s n |z n , s n\u22121 , s n\u22122 , ...) (7) This objective to optimize consists of two parts; the first is the KL divergence between the approximate distribution q and the true posterior p(s n |z n , s n\u22121 , s n\u22122 , ...), in which we want to approximate the true posterior using q. The second part E q(zn|sn,s n\u22121 ,...) log p(s n |z n , s n\u22121 , s n\u22122 , ...), predicts tokens in s n in the same way as in SEQ2SEQ models with the difference that it considers the global information z n . The approximate posterior distribution q(z n |s n , s n\u22121 , ...) takes a form similar to p(z n |s n\u22121 , s n\u22122 , ...): q(z n |s n , s n\u22121 , ...) = N (\u00b5 approx zn , \u03a3 approx zn ) \u00b5 approx zn = f q (z n\u22121 , s n , s n\u22121 , ...) \u03a3 approx zn = g q (z n\u22121 , s n , s n\u22121 , ...) (8) f q and g q are of similar structures to f and g, using a hierarchical neural network model to map context tokens to vector representations. Learning and Testing At training time, the approximate posterior q(z n |z n\u22121 , s n , s n\u22121 , ...), the true distribution p(z n |z n\u22121 , s n\u22121 , s n\u22122 , ...), and the generative probability p(s n |z n , s n\u22121 , s n\u22122 , ...) are trained jointly by maximizing the variational lower bound with respect to their parameters: a sample z n is first drawn from the posterior distribution q, namely N (\u00b5 approx zn , \u03a3 approx zn ). This sample is used to approximate the expectation E q log p(s n |z n , s n\u22121 , s n\u22122 , ...). Using z n , we can update the encoder-decoder model using SGD in a way similar to the standard SEQ2SEQ model, the only difference being that the current token to predict not only depends on the LSTM output h t , but also z n . Given the sampled z n , the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix). The proposed VLV-GM model can be adapted to the bi-directional setting and the bi setting similarly to the way LDA-based models are adapted. The proposed model is closely related to many recent attempts in training variational autoencoders (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) , variational or latent-variable recurrent nets (Bowman et al., 2015; Chung et al., 2015; Ji et al., 2016; Bayer and Osendorfer, 2014) , hierarchical latent variable encoder-decoder models (Serban et al., 2016b,a) . Experimental Results In this section, we describe experimental results. We first evaluate the proposed models on discriminative tasks such as sentence-pair ordering and full paragraph ordering reconstruction. Then we look at the task of coherent text generation. Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008) . A detailed description of this commonly used dataset and training/testing are found in the Appendix. We report the performance of the following baselines widely used in the coherence literature. (1) Entity Grid Model: The grid model presented in Barzilay and Lapata (2008) . Results are directly taken from Barzilay and Lapata's (2008) paper. We also consider variations of entity grid models, such as Louis and Nenkova (2012) which models the cluster transition probability and the Graph Based Approach which uses a graph to represent the entity transitions needed for local coherence computation (Guinaudeau and Strube, 2013) . (2) Li and Hovy (2014) : A recursive neural model computes sentence representations based on parse trees. Negative sampling is used to construct negative incoherent examples. Results are from their papers. (3) Foltz et al. (1998) computes the semantic relatedness of two text units as the cosine similarity between their LSA vectors. The coherence of a discourse is the average of the cosine of adjacent sentences. We used this intuition, but with more modern embedding models: (1) 300-dimensional Glove word vectors (Pennington et al., 2014) , embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al., 2003) with 300 topics, trained on the Wikipedia dataset. Results are reported in Table 2 . The extended version of the discriminative model described in this work significantly outperforms the parse-tree based recursive models presented in Li and Hovy (2014) as well as all non-neural baselines. It achieves almost perfect accuracy on the earthquake dataset and 93% on the accident dataset, marking a significant advancement in the benchmark. Generative models (both vanilla SEQ2SEQ and the proposed variational model) do not perform competitively on this dataset. We conjecture that this is due to the small size of the dataset, leading the generative model to overfit. Evaluating Ordering on Open-domain Since the dataset presented in Barzilay and Lapata (2008) is quite domain-specific, we propose testing coherence with a much larger, open-domain dataset: Wikipedia. We created a test set by randomly selecting 984 paragraphs from Wikipedia dump 2014, each paragraph consisting of at least 16 sentences. The training set is 30 million sentences not overlapping with the test set. Binary Permutation Classification We adopt the same strategy as in Barzilay and Lapata (2008) , in which we generate pairs of sentence permutations from the original Wikipedia paragraphs. We follow the protocols described in the subsection and each pair whose original paragraph's score is higher than its permutation is treated as being correctly classified, else incorrectly classified. Models are evaluated using accuracy. We implement the Entity Grid Model (Barzilay and Lapata, 2008) using the Wikipedia training set as a baseline, the detail of which is presented in the Appendix. Other baselines consist of the Glove and LDA updates of the lexical coherence baselines (Foltz et al., 1998) . Results Table 2 presents results on the binary classification task. Contrary to the findings on the domain specific dataset in the previous subsection, the discriminative model does not yield compelling results, performing only slightly better than the entity grid model. We believe the poor performance is due to the sentence-level negative sampling used by the discriminative model. Due to the huge semantic space in the open-domain setting, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don't cover the space of possible meanings. By contrast the dataset in Barzilay and Lapata (2008) is very domain-specific, and the semantic space is thus relatively small. By treating all other sentences in the document as negative, the discriminative strategy's negative samples form a much larger proportion of the semantic space, leading to good performance. Generative models perform significantly better than all other baselines. Compared with the dataset in Barzilay and Lapata (2008) , overfitting is not an issue here due to the great amount of training data. In line with our expectation, the MMI model outperforms the bidirectional model, which in turn outperforms the unidirectional model across all three generative model settings. We thus only report MMI results for experiments below. The VLV-GM model outperforms that the LDA-HMM-GM model, which is slightly better than the vanila SEQ2SEQ models. Paragraph Reconstruction The accuracy of our models on the binary task of detecting the original sentence ordering is very high, on both the prior small task and our large open-domain version. We therefore believe it is time for the community to move to a more difficult task for measuring coherence. We suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003) . More formally, given a set of permuted sentences s 1 , s 2 , ..., s N (N the number of sentences in the original document), our goal is return the original (presumably most coherent) ordering of s. Because the discriminative model calculates the coherence of a sentence given the known previous and following sentences, it cannot be applied to this task since we don't know the surrounding context. Hence, we only use the generative model. The first sentence of a paragraph is given: for each step, we compute the coherence score of placing each remaining candidate sentence to the right of the partially constructed document. We use beam search with beam size 10. We use the Entity Grid model as a baseline for both the settings. Evaluating the absolute positions of sentences would be too harsh, penalizing orderings that maintain relative position between sentences through which local coherence can be manifested. We therefore use Kendall's \u03c4 (Lapata, 2003 (Lapata, , 2006)) , a metric of rank correlation for evaluation. See the Appendix for details of Kendall's \u03c4 computation. We observe a pattern similar to the results on the binary classification task, where the VLV-GM model performs the best. Adversarial evaluation on Text Generation Quality Both the tasks above are discriminative ones. We also want to evaluate different models' ability to generate coherent text chunks. The experiment is set up as follow: each encoder-decoder model is first given a set of context sentences (3 sentences). The model then generates a succeeding sentence using beam-search given the contexts. For the unidirectional setting, we directly take the most probable sequence and for the bi-directional and MMI, we rerank the N-best list using the backward probability and language model probability. We conduct experiments on multi-sentence generation, in which we repeat the generative process described above for N times, where N =1,2,3. At the end of each turn, the context is updated by adding in the newly generated sequence, and this sequence is used as the source input to the encoderdecoder model for next sequence generation. For example, when N is set to 2, given the three context sentences context-a, context-b and context-c, we first generate sen-d given the three context sentences and then generate sen-e given the sen-d, context-a, context-b and context-c. For evaluation, standard word overlap metrics such as BLEU or ROUGE are not suited for our task, and we use adversarial evaluation Bowman et al. (2015) ; Anjuli and Vinyals (2016) . In adversarial evaluation, we train a binary discriminant function to classify a sequence as machine generated or human generated, in an attempt to evaluate the model's sentence generation capability. The evaluator takes as input the concatenation of the contexts and the generated sentences (i.e., context-a, context-b and context-c, sen-d , sen-e in the example described above), 5 and outputs a scalar, indicating the probability of the current text chunk being human-generated. Training/dev/test sets are held-out sets from the one on which generative models are trained. They respectively contain 128,000/12,800/12,800 instances. Since discriminative models cannot generate sentences, and thus cannot be used for adversarial evaluation, they are skipped in this section. We report Adversarial Success (AdverSuc for short), which is the fraction of instances in which a model is capable of fooling the evaluator. Adver- Suc is the difference between 1 and the accuracy achieved by the evaluator. Higher values of Ad-verSuc for a dialogue generation model are better. AdverSuc-N denotes the adversarial accuracy value on machine-generated texts with N turns. Table 4 show AdverSuc numbers for different models. As can be seen, the latent variable model VLV-GM is able to generate chunk of texts that are most indistinguishable from coherent texts from humans. This is due to its ability to handle the dependency between neighboring sentences. Performance declines as the number of turns increases due to the accumulation of errors and current models' inability to model long-term sentence-level dependency. All models perform poorly on the adver-3 evaluation metric, with the best adversarial success value being 0.081 (the trained evaluator is able to distinguish between human-generated and machine generated dialogues with greater than 90 percent accuracy for all models). Qualitative Analysis With the aim of guiding future investigations, we also briefly explore our model qualitatively, examining the coherence scores assigned to some artificial miniature discourses that exhibit various kinds of coherence. The examples suggest that the model handles lexical coherence, correctly favoring the 1st over the 2nd, and the 3rd over the 4th examples. Note that the coherence score for the final example is negative, which means conditioning on the first sentence actually decreases the likelihood of generating the second one. Cases 2 and 3 suggest the model may, at least in these simple cases, be capable of addressing the much more complex task of dealing with temporal and causal relationships. Presumably this is because the model is exposed in training to the general preference of natural text for temporal order, and even for the more subtle causal links. Case 4: Centering/Referential Coherence Mary ate some apples. She likes apples. 3.06 She ate some apples. Mary likes apples. 2.41 The model seems to deal with simple cases of referential coherence. Example3: 2.40 John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could finally buy a piano. He arrived just as the store was closing for the day. Example4: 1.62 John went to his favorite music store to buy a piano. It was a store John had frequented for many years He was excited that he could finally buy a piano.. It was closing just as John arrived. In these final examples from Miltsakaki and Kukich (2004) , the model successfully captures the fact that the second text is less coherent due to rough shifts. This suggests that the discourse embedding space may be able to capture a representation of entity focus. Of course all of these these qualitative evaluations are only suggestive, and a deeper understanding of what the discourse embedding space is capturing will likely require more sophisticated visualizations. Conclusion We investigate the problem of open-domain discourse coherence, training discriminative models that treating natural texts as coherent and permutations as non-coherent, and Markov generative models that can predict sentences given their neighbors. Our work shows that the traditional evaluation metric (ordering pairs of sentences in small domains) is completely solvable by our discriminative models, and we therefore suggest the community move to the harder task of open-domain full-paragraph sentence ordering. The proposed models also offer an initial step in generating coherent texts given contexts, which has the potential to benefit a wide range of generation tasks in NLP. Our latent variable neural models, by offering a new way to learn latent discourse-level features of a text, also suggest new directions in discourse representation that may bring benefits to any discourse-aware NLP task. Supplemental Material Details for the domain specific dataset (Barzilay and Lapata, 2008) The corpus consists of 200 articles each from two domains: NTSB airplane accident reports (V=4758, 10.6 sentences/document) and AP earthquake reports (V=3287, 11.5 sentences/document), split into training and testing. For each document, pairs of permutations are generated 6 . Each pair contains the original document order and a random permutation of the sentences from the same document. Training/Testing details for models on the domain specific dataset We use reduced versions of both generative and discriminative models to allow fair comparison with baselines. For the discriminative model, we generate noise negative examples from random replacements in the training set, with the only difference that random replacements only come from the same document. We use 300 dimensional embeddings borrowed from GLOVE (Pennington et al., 2014) to initialize word embeddings. Word embeddings are kept fixed during training and we update LSTM parameters using AdaGrad (Duchi et al., 2011) . For the generative model, due to the small size of the dataset, we train a one layer SEQ2SEQ model with word dimensionality and number of hidden neurons set to 100. The model is trained using SGD with AdaGrad (Zeiler, 2012) . The task requires a coherence score for the whole document, which is comprised of multiple cliques. We adopt the strategy described in Li and Hovy (2014) by breaking the document into a series of cliques which is comprised of a sequence of consecutive sentences. The document-level coherence score is attained by averaging its constituent cliques. We say a document is more coherent if it achieves a higher average score within its constituent cliques. Implementation of Entity Grid Model For each noun in a sentence, we extract its syntactic role (subject, object or other). We use a wikipedia dump parsed using the Fanse Parser (Tratz and Hovy, 2011) . Subjects and objects are extracted based on nsubj and dobj relations in the dependency trees. (Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution tens of millions of Wikipedia sentences, we follow other researchers in using Barzilay and Lapata's simpler  method (Feng and Hirst, 2012; Burstein et al., 2010;  Barzilay and Lapata, 2008). 7   Kendall's \u03c4 Kendall's \u03c4 is computed based on the number of inversions in the rankings as follows: \u03c4 = 1 \u2212 2# of inversions N \u00d7 (N \u2212 1) (9) where N denotes the number of sentences in the original document and inversions denote the number of interchanges of consecutive elements needed to reconstruct the original document. Kendall's \u03c4 can be efficiently computed by counting the number of intersections of lines when aligning the original document and the generated document. We refer the readers to Lapata (2003) for more details. Derivation for Variation Inference For simplicity, we use \u00b5 post and \u03a3 approx to denote \u00b5 approx (z n ) and \u03a3 approx (z n ), \u00b5 true and \u03a3 true to denote \u00b5 true (z n ) and \u03a3 true (z n ). The KLdivergence between the approximate distribution q(z n |z n\u22121 , s n , s n\u22121 , ...) and the true distribution p(z n |z n\u22121 , s n\u22121 , s n\u22122 , ...) in the variational inference is given by: where k denotes the dimensionality of the vector. Since z n has already been sampled and thus known, \u00b5 approx , \u03a3 approx , \u00b5 true , \u03a3 true and consequently Eq10 can be readily computed. The gradient with respect to \u00b5 approx , \u03a3 approx , \u00b5 true , \u03a3 true can be respectively computed, and the error is then backpropagated to the hierarchical neural models that are used to compute them. We refer the readers to Doersch (2016) for more details about how a general VAE model can be trained. Our generate models offer a powerful way to represent the latent discourse structure in a complex embedding space, but one that is hard to visualize. To help understand what the model is doing, we examine some relevant examples, annotated with the (log-likelihood) coherence score from the MMI generative model, with the goal of seeing (qualitatively) the kinds of coherence the model seems to be representing. (The MMI can be viewed as the informational gain from conditioning the generation of the current sentence on its neighbors.) Acknowledgements The authors thank Will Monroe, Sida Wang, Kelvin Guu and the other members of the Stanford NLP Group for helpful discussions and comments. Jiwei Li is supported by a Facebook Fellowship, which we gratefully acknowledge. This work is also partially supported by the NSF under award IIS-1514268, and the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, the NSF, or Facebook.",
    "funding": {
        "defense": 1.0,
        "corporate": 1.0,
        "research agency": 1.0,
        "foundation": 1.9361263126072004e-07,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section of the article mentions support from a Facebook Fellowship, the NSF (National Science Foundation) under award IIS-1514268, and the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462. Facebook is a for-profit corporation, the NSF is a government-funded research agency, and DARPA is a branch of the United States Department of Defense. There is no mention of funding from a foundation.",
    "abstract": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domainindependent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": 109,
    "year": 2017,
    "month": "September",
    "title": "Neural Net Models of Open-domain Discourse Coherence",
    "values": {
        "building on past work": "Our work marks an initial step in building endto-end systems to evaluate open-domain discourse coherence, and more importantly, generating coherent texts given discourse contexts. The proposed model is closely related to many recent attempts in training variational autoencoders (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) , variational or latent-variable recurrent nets (Bowman et al., 2015; Chung et al., 2015; Ji et al., 2016; Bayer and Osendorfer, 2014) , hierarchical latent variable encoder-decoder models (Serban et al., 2016b,a) .",
        "novelty": "Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. Here we introduce two classes of neural models for discourse coherence. Our discriminative models induce coherence by treating human generated texts as coherent examples and texts with random sentence replacements as negative examples, feeding LSTM sentence embeddings of pairs of consecutive sentences to a classifier. Our generative models are based on augumenting encoder-decoder models with latent variables to model discourse relationships across sentences, including (1) a model that incorporates an HMM-LDA topic model into the generative model and (2) an end-to-end model that introduces a Markovstructured neural latent variable, inspired by recent work on training latent-variable recurrent nets (Bowman et al., 2015; Serban et al., 2016b) .",
        "performance": "The proposed models also offer an initial step in generating coherent texts given contexts, which has the potential to benefit a wide range of generation tasks in NLP. Our latent variable neural models, by offering a new way to learn latent discourse-level features of a text, also suggest new directions in discourse representation that may bring benefits to any discourse-aware NLP task."
    }
}