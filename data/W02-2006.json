{
    "article": "This paper presents a method for bootstrapping a fine-grained, broad-coverage part-of-speech (POS) tagger in a new language using only one personday of data acquisition effort. It requires only three resources, which are currently readily available in 60-100 world languages: (1) an online or hard-copy pocket-sized bilingual dictionary, (2) a basic library reference grammar, and (3) access to an existing monolingual text corpus in the language. The algorithm begins by inducing initial lexical POS distributions from English translations in a bilingual dictionary without POS tags. It handles irregular, regular and semi-regular morphology through a robust generative model using weighted Levenshtein alignments. Unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort. Introduction Previous work in minimally supervised language learning has defined minimal using several different criteria. Some have assumed only partially tagged training corpora (Merialdo, 1994) , while others have begin with small tagged seed wordlists (such as Collins and Singer (1999) and Cucerzan and Yarowsky (1999) for named-entity tagging). Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in Yarowsky et al. (2001) , requiring no direct supervision in the foreign language). Ngai and Yarowsky (2000) observed that an often more practical measure of the degree of supervision is not simply the quantity of annotated words, but the total weighted human labor and resource costs of different modes of supervision (allowing manual rule writing to be compared directly with active learning on a common cost-performance learning curve). In this paper we observe that another useful measure of (minimal) supervision is the additional cost of obtaining a desired functionality from existing commonly available knowledge sources. In particular, we note that for a remarkably wide range of languages, academic libraries, many booksellers and websites offer a foundation of linguistic wisdom in reference grammars and dictionaries. Thus starting from this baseline, what is the marginal cost of distilling from and augmenting this existing knowledge to achieve a desired new task functionality? Inducing POS Tag Candidates from Unlabeled Bilingual Dictionaries A substantial percentage of foreign language dictionaries that are available on line or in smaller paperback format are simple bilingual word or phrase translation lists which fail to specify part of speech. 1  Thus one component question of this work is how can one extract preliminary part-of-speech distributions from untagged monolingual translation lists. Figure 1 illustrates such a bilingual dictionary, also specifying the true part of speech for each possible translation, which we do not assume to be generally available. One approach is to take an unweighted mixture of the prior part-of-speech distributions for the English words given in the translation list (TL) as illustrated in Figure 2 . These probabilities may be estimated from a large and preferably balanced, corpus. In this work, we used statistics from the Brown and WSJ corpora combined.  However, when a translation candidate is phrasal (e.g. mandat \u00b0money order), one can model the more general probability of the foreign word's part of speech tag (\u00cc ) given the part of speech sequence of the English phrasal translation (\u00cc \u00bd \u00cc \u00d2 ). For example, one could model P(T money order) via P(T AE AE \u00b5 and P(T manifest itself) via P(T \u00ce \u00c8 \u00d6 \u00d3 \u00b5. However, because English words often have multiple parts of speech (e.g. order may be a verb), one may weight phrasal POS sequence probabilities (making an independence assumption) as: \u00c8 \u00b4AE \u00d1\u00d3\u00d2 \u00dd \u00d3\u00d6 \u00d6\u00b5 \u00c8 \u00b4AE AE AE \u00b5 \u00a1 \u00c8 \u00b4AE \u00d1\u00d3\u00d2 \u00dd\u00b5 \u00a1 \u00c8 \u00b4AE \u00d3\u00d6 \u00d6\u00b5\u2022 \u00c8 \u00b4AE AE \u00ce \u00b5 \u00a1 \u00c8 \u00b4AE \u00d1\u00d3\u00d2 \u00dd\u00b5 \u00a1 \u00c8 \u00b4\u00ce \u00d3\u00d6 \u00d6\u00b5\u2022 \u00c8 \u00b4AE \u00ce AE \u00b5 \u00a1 \u00c8 \u00b4\u00ce \u00d1\u00d3\u00d2 \u00dd\u00b5 \u00a1 \u00c8 \u00b4AE \u00d3\u00d6 \u00d6\u00b5\u2022 \u00c8 \u00b4AE \u00ce \u00ce \u00b5 \u00a1 \u00c8 \u00b4\u00ce \u00d1\u00d3\u00d2 \u00dd\u00b5 \u00a1 \u00c8 \u00b4\u00ce \u00d3\u00d6 \u00d6\u00b5\u2022 And in general: \u00c8 \u00b4\u00cc \u00db \u00bd \u00db \u00be \u00b5 \u00cc \u00bd \u00cc \u00be \u00c8 \u00b4\u00cc \u00cc \u00bd \u00cc \u00be \u00b5 \u00a1 \u00c8 \u00b4\u00cc \u00bd \u00db \u00bd \u00b5 \u00a1 \u00c8 \u00b4\u00cc \u00be \u00db \u00be \u00b5 where \u00c8 \u00b4\u00cc \u00db \u00b5 is estimated from the dictionary as above. Without an independence assumption: \u00c8 \u00b4\u00cc \u00db \u00bd \u00db \u00d2 \u00b5 \u00c8 \u00b4\u00cc \u00cc \u00bd \u00cc \u00d2 \u00b5 \u00a1 \u00c8 \u00b4\u00cc \u00bd \u00cc \u00d2 \u00db \u00bd \u00db \u00d2 \u00b5 There are two major options via which one can estimate \u00c8 \u00b4\u00cc \u00cc \u00bd \u00cc \u00d2 \u00b5. The first is to assume that the part-of-speech usage of phrasal (English) translations is generally consistent across dictionaries (e.g. \u00c8 \u00b4AE AE \u00bd AE \u00be \u00b5 remains high regardless of publisher or language). Hence one could use any foreign-English bilingual dictionary that also includes the true foreign word part of speech in addition to its translations to train these probabilities. Alternately, one could do a first-pass assignment of foreign-word part of speech based on only single word translations as in Figure 2 , and use this to train \u00c8 \u00b4\u00cc \u00cc \u00bd \u00cc \u00d2 \u00b5 for those foreign words having both phrasal and single-word definitions (such as mandat). The advantage of this approach is that it may benefit dictionaries with different phrasal translation styles from the training dictionary (e.g. use or omission of the word 'to' in verb definitions). However, given the assumption of relatively consistent dictionary formatting styles (which was unfortunately not the case for Kurdish), we evaluated this work based on supervised phrasal training from a single independent third language dictionary. Table 1 measures the POS induction performance on three languages, where the true POS tags were given in the dictionary (as in Figure 1 ), but ignored except for evaluation. The accuracy values in this table are based on exact matches between a word's dictionary-provided POS and the most probable tag in its induced distribution. For our target application of part-of-speech tagging, what matters is to have a robust tag probability distribution that includes the true candidate with sufficiently large probability to seed further training. By setting this baseline threshold to 0.1 and deleting lower ranked candidates, up to 98% of the true POS were found to be above this threshold and hence were considered in future training. The Mean Probability of Truth, as shown in Table 1, is another measure of the quality of the POS predictions made by the algorithm, representing the probability mass associated with the true POS tag averaged over all words. In some cases the algorithm could not predict a POS tag, primarily due to English translations for which no POS distribution was known (often an obscure word, proper name or OCR error). This oc- casional omission is measured by the coverage column. Most of the observed errors are due to differences in phrasal definitional conventions in the training and testing dictionaries, long phrasal idioms, singleword definitions with ambiguous English parts-ofspeech and OCR errors. The Kurdish dictionary was particularly hindered by frequent long phrasal translations which often included an explanation or definition in their translation. Because all dictionary entries are equally weighted, errors on rare words such as mythological characters or kinship terms can substantially downgrade performance. But for the purposes of providing seed POS distributions to context-sensitive taggers, performance is quite adequate for this follow-on task. Inducing Morphological Analyses There has been extensive previous work in the supervised and minimally supervised induction of both affix paradigms (e.g. Goldsmith, 2000; Snover and Brent, 2001) and diverse models of regular and irregular concatenative and non-concatenative morphology (e.g. Schone and Jurafsky, 2000; van den Bosch and Daelemans, 1999; Yarowsky and Wicentowski, 2000) . While such approaches are important from the perspective of learning theory or broad coverage handling of irregular forms, another possible paradigm for minimal supervision is to begin with whatever knowledge can be efficiently manually entered from the grammar book in several hours work. We defined such grammar-based \"supervision\" as entry of regular inflectional affix changes and their associated part of speech in standardized ordering of fine-grained attributes, as in Table 2 for Spanish and Romanian. The full tables have approximately 200 lines each and required roughly 1.5-2 person-hours for entry. Given a dictionary marked with core parts of speech, it is trivial to generate hypothesized inflected forms following the regular paradigms, as shown in the left size of Figure 3 . However, due to irregularities and semi-regularities such as stem- Root Inflected Affix Affix Part-of-speech Tag Spanish: o$ o$ Adj-masc-sing o$ os$ Adj-masc-plur o$ a$ Adj-fem-sing o$ as$ Adj-fem-plur e$ e$ Adj-masc,fem-sing e$ es$ Adj-masc,fem-plur ar$ o$ Verb-Indic_Pres-p1-sing ar$ as$ Verb-Indic_Pres-p2-sing ar$ a$ Verb-Indic_Pres-p3-sing ar$ amos$ Verb-Indic_Pres-p1-plur ar$ \u00e1is$ Verb-Indic_Pres-p2-plur ar$ an$ Verb-Indic_Pres-p3-plur Romanian: \u0101  changes, such generation will clearly have substantial inaccuracies and overgenerations. $ e$ Noun-Nomin-p3-fem-plur-indef e$ i$ Noun-Nomin-p3-fem-plur-indef ea$ ele$ Noun-Nomin-p3-fem-plur-indef i$ ile$ Noun-Nomin-p3-fem-plur-indef a$ ale$ Noun-Nomin-p3-fem-plur-indef $ $ Adj-masc,neut-sing $ \u0101$ Adj-fem-sing $ i$ Adj-masc, However, through weighted-Levenshtein-based iterative alignment models, such as described in Yarowsky and Wicentowski (2000) , one can perform a probabilistic string match from all lexical tokens actually observed in a monolingual corpus, as For example, when looking for a potential analysis path for the Spanish irregular inflection destrocen, the closest string match is the regular hypothesis destrozar/V \u00b0destrozen/V-pres_subj-3pl. z -> c destrozan destroc\u00e9 destroz\u00e9 V-pres-3pl V-pret-1sg V-subj-3pl destrozen destrocen destrozan destrozar/V z -> c destruo destru\u00ed destruen V-pres-1sg V-pres-1sg V-pret-1sg destrue destru\u00ed destruyo destruye destruyen destruir/V V-pres-3sg ->y -> y \u03c6 \u03c6 -> y V-pres-3pl V-pret-3pl doler/V dormir/V V-pres-1sg dormen V-pres-3pl Likewise, the closest string match for destruyen is destruir/V \u00b0destruen/V-pres_indic-3pl. The differences between these regular hypotheses and observed inflected forms are the relatively productive stem changes \u00dd and \u00de , neither of which was listed in the inflectional supervision table, and yet they were correctly handled. Note that a traditional \u00c8 \u00b4POS suffix) model would fail to handle this case given that the common inflection suffix -en corresponds to two different parts of speech here (present indicative or subjunctive depending on -ir or -ar paradigm). Also note that the irregular stem change processes such as dormir duermen have a correct best-fit analysis, despite the absence of any internal stem change exemplars (e.g. o ue) in the humangenerated inflectional supervision table. For further robustness, the consensus model of \u00c8 \u00b4\u00c8 \u00d3 \u00d7 \u00cf \u00b5 is estimated as a weighted mixture of the part-of-speech tags of the most closely aligned 2 For processing efficiency, one additional constraint is that potential hypothesized\u00b0observed string pair candidates must exactly match in both initial consonant cluster and suffix of the generated hypothesis. pseudo-regular generated inflections. The inflections of closed-class words (such as pronouns, determiners and auxiliary verbs) are not well handled by this generative-alignment model, both due to their often very high irregularity (e.g. the Spanish verb ser (to be)) and/or their typical shortness (e.g. the pronominal inflections of mi, tu, su). Thus as one final amount of supervision, lists of closed-class words, paired with their inflections and fine-grained part-ofspeech tags were entered manually from the grammar book (e.g. aquellas#(aquel)Adj_Demfem-plur-p3). This final source of supervision utilized an average of 400 lines and 3 person-hours per language. POS Model Induction The non-traditional supervision methodology in Sections 2 and 3 yields a noisy but broad-coverage candidate space of parts of speech with little human effort. We then perform a noise-robust combination of model estimation and re-estimation techniques for the syntagmatic trigram models \u00c8 \u00b4\u00d4\u00d3\u00d7 \u00be \u00d4\u00d3\u00d7 \u00bd \u00d4\u00d3\u00d7 \u00bc \u00b5 and lexical priors \u00c8 \u00b4\u00db \u00d4\u00d3\u00d7 \u00b5 using the word cooccurrence information from a raw corpus. \u00afA suffix-based part-of-speech probability model \u00c8 \u00b4\u00d4\u00d3\u00d7 suffix\u00b4\u00db \u00b5\u00b5 using hierarchically smoothed tries is trained on the raw initial tag distributions, yielding coverage to unseen words and smoothing of low-confidence initial tag assignments. \u00afParadigmatic cross-context tag modeling is performed as in Cucerzan and Yarowsky (2000) when sufficiently large unannotated corpora are available. \u00afSub-part-of-speech contextual agreement for features such as gender is performed as described in Section 4.1. \u00afThe part-of-speech tag sequence models \u00c8 \u00b4\u00d4\u00d3\u00d7 \u00be \u00d4\u00d3\u00d7 \u00bd \u00d4 \u00d3 \u00d7 \u00bc \u00b5 utilize a weighted backoff between fine-grained and coarse-grained tags. \u00afBoth the tag-sequence and lexical prior models are iteratively retrained using these additional evidence sources and first-pass probability distributions. The success of this model is based on the assumption that (a) words of the same part of speech tend to have similar tag sequence behavior, and (b) there are sufficient instances of each POS tag labeled by either the morphology models or closedclass entries described in Section 3. One example where these assumptions do not hold is for the Romanian word a, which has 5 possible POS tags, including Infinitive_Marker (corresponding to the English word to). But because the Infini-tive_Marker tag has no other word instances in Romanian, no other filial supervision exists to resolve the ambiguity of a if no context-sensitive tagging is provided (such as the preference for a to be labeled Infinitive_Marker when followed by a Verb-Infinitive). Thus one avenue of potential improvement to these models would be to include limited tagged contexts for ambiguous small class (or singleton class) words, although such supervision is less readily extractable from grammar books by non-native speakers, and was not employed here. Contextual-agreement models for part-of-speech subtags Traditional part-of-speech models assume a strict Markovian sequential dependency. However, Adj-Noun, Det-Noun and Noun-Verb agreement at the subtag-level (e.g. for person, number, case and gender) often do not require direct adjacency, and are based on the selective matching of isolated subfeatures. This is particularly important for grammatical gender, where the lack of gender features projected from English rootwords in a bilingual dictionary (as in Section 2) require contextual agreement to assign gender to many inflected and root forms. However, given the assumptions of minimal supervision, it is not reasonable to require a parser or dependency model to identify non-adjacent agreeing pairs explicitly. Rather, we utilize a much more general tendency for words exhibiting a property such as grammatical gender to co-occur in a relatively narrow window with other words of the same gender (etc.) with a probability greater than chance. Empirically, we observe this in Figures 4-5 , which show the gender-agreement ratio between a target noun/adjective and other gender marked words appearing in context at relative position \u00a6 . Adjectives in Romanian exhibit a stronger agreement tendency with words to their left (5/1 ratio), while for nouns the agreement ratio is quite closely balanced between -1 (primarily determiners) and +1 (primarily adjectives), although weaker (2.4/1 ratio), perhaps due to a greater relative tendency for nouns to juxtapose directly with other independent clauses of different gender. Also, both parts of speech con- verge on the agreement ratio expected by chance (0.82) relatively quickly. Thus while any individual context may suggest incorrect gender based on agreement, if one aggregates over all occurrences of a word in a corpus, a consensus gender preference emerges, with the true gender agreement signal exceeding nearby spurious gender noise. Formally, we can model this window-weighted global feature consensus as: \u00c8 \u00b4 \u00d2 \u00db\u00b5 \u00bd AE \u00be\u00d0\u00d3 \u00b4\u00db\u00b5 \u2022\u00bf \u00bf \u00c8 \u00b4 \u00d2 \u00db \u2022 \u00b5\u00cf \u00d8 \u00b4 \u00b5 The \u00a6\u00bf window-size parameter was selected prior to the studies shown in Figures 4-5 , but is supported by them. Beyond this window the agreement/disagreement ratio approaches chance, but with a smaller window the probability of finding any gender-marked word in the window drops below the 80% coverage observed for \u00a6\u00bf, trading lower coverage for increased accuracy. If one makes the assumption that the overwhelming majority of nouns have a single grammatical gender independent of context, we perform smoothing to force nouns with sufficient global context frequency towards their single most likely gender. Finally, the trie-based suffix model noted in Section 3 can be utilized here to further generalize gender affixal tendencies for use in smoothing poorly represented single words. Through this approach we successfully discover a wide space of lowentropy gender affix tendencies, including the common -a, -dad and -ci\u00f3n feminine affixes in Spanish, without any human or dictionary supervision of nominal gender. But even those words without gender-distinguishing affixes (e.g. parte, cabal) can be successfully learned via global context maximization. Evaluation of the Full Part-of-speech Tagger One problem with minimally supervised learning of foreign languages is that annotated evaluation data are often not available for the features being induced, or are otherwise difficult to obtain. Thus we have used for initial test languages two languages familiar to the authors (Romanian and Spanish) for which sufficient evaluation resources could be obtained. However, the monolingual corpora utilized for bootstrapping were quite small (123 thousand words of the book 1984 for Romanian and 3.2 million words of newswire for Spanish), which are easily comparable to the sizes that can be accessed online for 60-100 world languages. The seed dictionaries were located online (for Spanish -42k entries) and via OCR (for Romanian -7k entries), and small grammar references were obtained at a local bookstore. 1000 words of test data were annotated with a standardized, finely detailed part-of-speech tag inventory including the full complex distinctions for gender, person, number, case, detailed tense and nominal definiteness (an inventory of 259 and 230 fine-grained tags were used for Spanish and Romanian respectively). The minimal supervision in this study consisted of an average total of 4 person-hours per language for manually entering the inflectional paradigms and associated parts of speech from a grammar as in Section 3, and an additional average of 3 personhours per language for dictionary extraction and entry parsing. OCR itself on our high-speed 2-sided scanner with OmniPage Pro took under 30 minutes). As would be expected given that data entry was done by computer scientists which were not native speakers of the test languages, significant analysis errors or gaps were introduced when rather blindly transferring from the reference grammar. Thus to test the relative contributions of limited native speaker help when available, for roughly 4 additional total person hours in a second test condition for Romanian a native speaker corrected and augmented gaps in the patterns previously entered from the grammar book, focusing almost exclusively on the complex inflections of closed-class words. A summary of the results for these three supervision modes is given in Table 3 . Performance is broken down by fine-grained part of speech. Exactmatch accuracy is measured over both the full finegrained (up to 5-feature) part-of-speech space, as well as the 12-class core POS tag (noun and proper noun, pronoun, verb, adjective, adverb, numeral, determiner, conjunction, preposition, interjection, particle, punctuation). The feature of grammatical gender was specifically isolated because it is rarely salient for cross-language applications such as machine translation (where grammatical gender rarely transfers), and because its induction algorithm in Section 4.1 depends heavily on the size of the monolingual corpus (which is small in these experiments, suggesting size-dependent potential for significant further improvement here). Finally, a post-hoc analysis of the system vs. test data discrepancies showed that a significant number were simply arbitrary differences in annotation convention between the grammar-book analyses and the test data tagging policy. For example, one such \"error\"/discrepancy is the rather arbitrary distinction of whether the Romanian word oricare (meaning any) should be considered an adjective (as listed in a standard bilingual dictionary) or a determiner. Another difference is whether proper-name citations of common nouns (e.g. Casa Blanca) should be annotated for gender/number etc. or not. Yet regardless of exactly how many system-test discrepancies are just policy differences rather than errors, even the raw accuracy here is very promising given the very fined-grained part-of-speech inventory and small monolingual data size used for bootstrapping. And ultimately the performance is quite remarkable given that it is the result of less than 1 total person day of data collection and supervision, in contrast to the thousands of hours and $100,000-$1,000,000 spent on some annotated training data in a much more limited tagset inventories. Thus in terms of cost-benefit analysis, the supervision paradigm and associated bootstrapping models presented here offer quite a good value of new functionality per labor invested. Conclusion This paper has presented an alternative to traditional corpus annotation-based supervision of partof-speech taggers. Given that even obscure languages have reference grammars and dictionaries available in large bookstores, libraries or even online, the focus of this work is on using human supervision for efficient structured entry of this seed knowledge (in the form of regular and semi-regular inflectional paradigms and often irregular closedclass part-of-speech entries). Minimally supervised bootstrapping procedures then used corpus-derived distributional data to induce lexical tag probabilities from dictionaries, irregular morphological analyses via weighted Levenshtein-based alignment models, tag sequence probability induction and grammatical gender agreement modeling. Experiments show high accuracy coarse and fine-grained ( 250 tag) part-of-speech analyses using only one person day of new human supervision based on readily available linguistic resources. Acknowledgements This work was partially supported by NSF grant IIS-9985033 and ONR/MURI contract N00014-01-1-0685.",
    "abstract": "This paper presents a method for bootstrapping a fine-grained, broad-coverage part-of-speech (POS) tagger in a new language using only one personday of data acquisition effort. It requires only three resources, which are currently readily available in 60-100 world languages: (1) an online or hard-copy pocket-sized bilingual dictionary, (2) a basic library reference grammar, and (3) access to an existing monolingual text corpus in the language. The algorithm begins by inducing initial lexical POS distributions from English translations in a bilingual dictionary without POS tags. It handles irregular, regular and semi-regular morphology through a robust generative model using weighted Levenshtein alignments. Unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English",
        "Romanian"
    ],
    "numcitedby": "56",
    "year": "2002",
    "month": "",
    "title": "Bootstrapping a Multilingual Part-of-speech Tagger in One Person-day"
}