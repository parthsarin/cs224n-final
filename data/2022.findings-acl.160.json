{
    "article": "Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero-and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora). We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg. Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora. LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT); it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model's cross-lingual capabilities. Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages. The gains are observed in zero-shot, few-shot, and even in full-data scenarios. The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together. Introduction The aim of Natural Language Understanding (NLU) in task-oriented dialogue systems is to identify the user's need from their utterance (Xu et al., 2020) . This comprises the following crucial information: 1) intents, what the user intends to do, and 2) (typically predefined) slots, associated arguments of the intent (Tur et al., 2010; Tur and De Mori, 2011) which need to be filled with specific values. Intent detection is often framed as a standard sentence classification task, where every sentence maps to one or more intent classes; slot labelling is typically cast as a sequence labelling task, where each word is labelled with a BIO-style slot tag (Bunk et al., 2020) , see Figure 1 . The supervised models for NLU in English are plentiful and achieve extremely high accuracy (Louvan and Magnini, 2020a; Qin et al., 2021) . At the same time, porting an NLU system to any new domain and language requires collecting a large indomain dataset, and training a model for the target language (Xu et al., 2020) . Such in-domain annotations in multiple languages are extremely expensive and time-consuming (Rastogi et al., 2020) , also reflected in the fact that large enough dialogue NLU datasets for other languages are still few and far between (Razumovskaia et al., 2021) . This in turn creates the demand for strong multilingual and crosslingual methods which generalise well and learn effectively in zero-shot and few-shot scenarios. In this work, we propose two methods to this end: 1) Multi-SentAugment, a weakly supervised data augmentation method which improves the capability of current state-of-the-art (SotA) dialogue NLU in few-shot scenarios via self-training; 2) Layer-Agg learns to effectively leverage and combine the knowledge stored across different layers of a pretrained multilingual Transformer (e.g., mBERT). The main goal of Multi-SentAugment is to reduce the required amount of labelled data and manual annotation labour by harvesting the large pool of unannotated data, and carefully selecting relevant in-domain examples which can then be automatically labelled (Du et al., 2021) . In a nutshell, domain-relevant unannotated sentences are first retrieved from a large multilingual sentence bank. The synthetic labels for the data are then generated by a teacher model, previously trained with available annotated data. A final student model is then trained on the combination of synthetically labeled and annotated data. To the best of our knowledge, our work is the first to mine large unannotated monolingual resources in multiple languages to augment data for multilingual dialogue NLU. The goal of LayerAgg is to leverage useful lexical and other semantic information scattered across layers (Tenney et al., 2019; Vuli\u0107 et al., 2020) of a pretrained multilingual Transformer. Moving away from the standard fine-tuning practice of using only the representations from the top layer, we hypothesise that the model's cross-lingual capabilities can be increased by forcing it (i) to propagate semantic information from lower layers, as well as (ii) to aggregate/combine semantic information from all its layers. In a nutshell, we propose to use a multilingual encoder with cross-layer Transformer, which selects and combines the knowledge from all layers of a pretrained model during fine-tuning. Our experiments show that Multi-SentAugment gives consistent improvements in few-shot and fulldata scenarios on the two available multilingual dialogue NLU datasets: MultiATIS++ (Xu et al., 2020) and xSID (van der Goot et al., 2021) . The results further indicate that LayerAgg improves zero-shot performance on the same datasets. Finally, since the two methods can be independently applied to SotA NLU models, we demonstrate that they yield a synergistic effect: the highest scores on average are achieved with their combination. Contributions. 1) Multi-SentAugment is a simple yet effective data augmentation approach which leverages unannotated data from large Web-scale corpora to boost multilingual dialogue NLU. 2) LayerAgg is a novel cross-layer attention method which learns to effectively combine useful semantic information from multiple layers of a multilingual Transformer. 3) The two methods applied with SotA NLU models obtain consistent gains across two standard multilingual NLU datasets in zeroshot, and 8 languages in few-shot, and full-data setups, boosting the capability of cross-lingual dialogue in resource-lean scenarios. Related Work and Background Multilingual NLU for Dialogue Systems is usually divided into two tasks: intent detection and slot labelling (Tur et al., 2010; Xu et al., 2020) . In \"pre-Transformer\" times, the methods for training multilingual NLU systems were based on static multilingual word vectors (Mrk\u0161i\u0107 et al., 2017; Upadhyay et al., 2018; Schuster et al., 2019) , lexicon alignment (Liu et al., 2019b,a) , and model or annotation projection via parallel data (Kulshreshtha et al., 2020; L\u00f3pez de Lacalle et al., 2020) . Transfer learning with large pretrained multilingual Transformer-based language models (LMs) such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a) has demonstrated currently unmatched performance in many NLU tasks (Liang et al., 2020; Hu et al., 2020; Ponti et al., 2020; Ruder et al., 2021) , including intent classification and slot labelling (Zhang et al., 2019; Liu et al., 2020) . Fine-tuning a large multilingual LM has become a standard for multilingual NLU (Zhang et al., 2019; Xu et al., 2019; Kulshreshtha et al., 2020) . However, the excessively high data annotation costs for multiple domains and languages still hinder progress in multilingual dialogue (Razumovskaia et al., 2021) . In this paper, unlike prior work, we propose to use external unannotated data to mine and automatically label in-domain in-language examples which aid learning in low-data regimes across multiple languages. Data Augmentation in Multilingual NLU, as well as data augmentation methods in NLP in general, aim to produce additional training data automatically, without the need to manually label it. In monolingual English-only settings, English NLU data has been augmented by generating additional data with a large monolingual language model (Peng et al., 2020) such as BERT (Devlin et al., 2019) or GPT-2 (Radford et al., 2019) , or from atomic templates (Zhao et al., 2019) . In multilingual settings, data augmentation methods for NLU include simple text span substitution and syntactic structure manipulation (Louvan and Magnini, 2020c,b) . Recently, code switching (Krishnan et al., 2021) and generating translations through a pivot language (Kaliamoorthi et al., 2021) have also been proposed as data augmentation methods. The previous work relies on (i) additional components such as syntactic parsers or POS taggers, or (ii) parallel and code-switched data. However, they might be unavailable or of low-quality for many (low-resource) languages. In contrast, Multi-SentAugment relies on the cheapest and largest resource available: monolingual Web-crawled data; it disposes of any dependency parsers and taggers, which makes it more widely applicable. Mining knowledge from Web-scale data was shown effective in various (non-dialogue) text classification tasks (Du et al., 2021) and in MT (Wu et al., 2019) . 1   Layer Aggregation in Pretrained LMs. A standard practice is to use the output of the final/top layer of a pretrained LM as input into task-specific classifiers (Devlin et al., 2019; Sun et al., 2019) . At the same time, prior work shows that most of (decontextualised) lexical information (Ethayarajh, 2019; Vuli\u0107 et al., 2020) and word-order information (Lin et al., 2019) is localised in lower layers of BERT. Middle layers usually encode syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019) while (contextual) semantic information is spread across all the layers of a pretrained LM (Tenney et al., 2019) , with higher layers capturing increasingly abstract language phenomena (Lin et al., 2019; Rogers et al., 2020; Tenney et al., 2019) . Kondratyuk and Straka (2019) showed that using a weighted combination of all layers works well in cross-lingual settings for a syntactic task of dependency parsing. In addition, they proposed to use layer dropout to redistribute how the information is localised in a fine-tuned BERT model. In order to 'unlock' additional semantic knowledge from other layers, we propose an additional Transformer encoder with cross-layer attention as a layer aggregation mechanism. We hypothesise that relying only on the representations from the top layer dilutes mBERT's lexical and semantic information. Moreover, we expect lexically and semantically richer representations to be especially useful for zero-shot settings: aggregated (contextualised) semantic information from lower layers could help correctly identify the intent of the sentence, while lexical information could help identify the slot tag for different languages. 2 Methodology We assume a standard state-of-the-art approach to dialogue NLU in multiple languages (Xu et al., 2020) , based on fine-tuning pretrained multilingual LMs on the tasks of intent detection and slot labelling. Following Xu et al. (2020) , we fine-tune the pretrained LM in a standard supervised fashion, with task-specific linear layers stacked on top. Separate NLU Models. The multilingual encoder for each NLU task is fine-tuned separately, and there is no knowledge exchange (but also no noise or destructive inference) between the two tasks. We adopt a standard task-specific fine-tuning setup (Xu et al., 2020; Siddhant et al., 2020) . Joint NLU Model. Another line of recent work pursued joint modelling of the two tasks, motivated by the intuitive correlation between them. 3  In this work, we follow a standard joint modelling procedure (Xu et al., 2020; Hardalov et al., 2020; Krishnan et al., 2021) , where the model consists of a shared multilingual encoder followed by taskspecific linear layers for intent classification and slot labelling. The loss is then simply a sum of two task-dedicated losses. In our experiments, we use mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a) as the encoder. Multi-SentAugment ( \u00a73.1) and LayerAgg ( \u00a73.2) are then applied to the joint NLU model, while we also provide detailed comparisons to the separate NLU models as baselines in zero-shot setups. Multi-SentAugment Large Web-crawled datasets have been proven useful for extracting additional data for classification tasks in English (Du et al., 2021) . We adapt the approach of Du et al. (2021) to multilingual dialogue NLU, that is, we propose to use large Web-crawled corpora to obtain additional in-domain data for dialogue NLU tasks in multiple languages. For each language l we are given: 1) some annotated training data D l which consists of |D l | sentences x 1 , ..., x |D l | , each labelled with intent class and slot labels (see Figure 1 ); 2) a large Webcrawled corpus U l consisting of |U l | sentences s 1 , ..., s |U l | ; 3) off-the-shelf multilingual sentence encoder F fine-tuned towards semantic sentence similarity, that is, to produce semantic embeddings of input sentences (Reimers and Gurevych, 2020) . The data augmentation process then consists of 1) unsupervised data retrieval and 2) self-training. The aim of unsupervised data retrieval is to construct an in-domain unannotated set of sentences by filtering the sentences from U l . The process is formulated by the following equations: X = F(x 1 , . . . , x |D l | ); U = F(s 1 , ..., s |U l | ); \u03c3 = UX U X > \u03b8; \u03b8 is a similarity threshold for sentence filtering: a sentence s i will be added into the in-domain dataset if there is an annotated sentence x j \u2208 D l such that \u03c3 i,j > \u03b8. As a result of data retrieval, we obtain a set of in-domain unannotated sentences which are similar to annotated training data D l . At self-training, we first fine-tune a joint NLU model on annotated D l data. We then use this model to annotate the retrieved in-domain sentences. As our final NLU model, we fine-tune a new joint NLU model on the full dataset, combining the D l set and filtered and annotated sentences. LayerAgg To ensure the propagation and use of lexical and semantic information from lower layers, we propose a simple layer aggregation technique based on cross-layer attention (Vaswani et al., 2017) , illustrated in Figure 2 . In short, let w ij be a representation of a word (or WordPiece; Devlin et al. ( 2019 )) at position i at layer j, j = 1, . . . , N l , where N l is the number of layers in the pretrained LM (e.g., N l = 12 for mBERT). Layer-aggregated representation w i of the input w i is computed as follows: w i = T(w i,1 :N l ), (1) where w i,1 :N l is a sequence comprising all (ordered) w ij per-layer representations, and T is a cross-layer Transformer encoder. In essence, T effectively always operates over a sequence of length N l : it outputs the representations from all layers, but which have now been self-attended. We then feed the last item (i.e., N l -th item) of the sequence representation output by the Transformer T into the task-specific classifiers. Relying on the N l -th output representation, the model is forced to incorporate the information from all layers into the final representation of the input token w i . The parameters of T are also updated during fine-tuning. Large (Multilingual) Sentence Banks. We use the CC-100 dataset (Conneau et al., 2020a; Wenzek et al., 2020) , which comprises monolingual CommonCrawl data in 116 languages. For computational tractability with resources at our disposal, we rely on the smaller CC-100-100M dataset, a random sample from the full CC-100 4 spanning 100M sentences in each language. CC-100 covers multiple domains, language styles and variations. Experimental Setup Multi-SentAugment: Setup. Unless noted otherwise, we use the LASER multilingual sentence encoder (Artetxe and Schwenk, 2019) , pretrained on 93 languages with a sentence similarity objective on parallel data. The similarity threshold \u03b8 is set to 0.8. Besides the basic setup, (i) we also analyse the impact of the sentence encoder by running experiments with another SotA multilingual encoder: LaBSE (Feng et al., 2020; Litschko et al., 2021) ; (ii) we apply an additional filtering step based on the intent confidence of the teacher model, retaining only high-confidence examples. 5 LayerAgg. The aggregator Transformer T contains a single 512-dimensional layer with 4 attention heads. Here, we remind the reader that the N l -th item of T's output sequence is fed to the taskspecific layers; see again \u00a73.2). LayerAgg adds up to 2 million additional parameters, which is \u2248 1% of the total number of trainable parameters in the baseline model. In addition, we present an extensive comparison with a standard layer aggregation method of Kondratyuk (2019) , which is based on cross-layer attention. Results and Discussion Joint vs Separate NLU. We first establish the performance of joint versus separate baseline NLU models. The main results, provided in Tables 2  and 3 , indicate that joint NLU training performs better on intent classification while separate taskspecific NLU models are more beneficial on slot labelling. Our results corroborate the findings from prior work (Schuster et al., 2019; He et al., 2020; Weld et al., 2021) . We suspect that joint training works better for intent classification as sentencelevel representations are enriched with lexical information through the additional slot-labelling loss. At the same time, separate training attains stronger performance in slot labelling as it retains more taskspecific representations for each token. Impact of LayerAgg. The motivation behind Lay-erAgg is to combine the strengths of both joint and separate training, that is, having sentence-level representations enriched with lexical information while keeping token representations specified. The benefits of LayerAgg in both tasks in zero-shot setups are indicated by the results in Tables 2-3 . We observe large improvements with LayerAgg, both on average and for the large number of individual target languages. It is worth noting that LayerAgg provides gains also with both underlying multilingual encoders. Besides that, adding LayerAgg also yields more stable performance of the joint model in general (e.g., compare the scores on Japanese and Turkish slot labelling without and with Lay-erAgg). The gains with LayerAgg also persist in few-shot and full-data setups, as shown in Figure 3 . +LayerAgg versus +Attn. Impact of Multi-SentAugment. The results in Figure 3 suggest that Multi-SentAugment is indeed useful as data augmentation for the two NLU tasks, both in few-shot and full-data scenarios, and for different target languages. 6 Achieving slight gains in full-data scenarios implies that mining additional monolingual data is beneficial even when a large in-domain dataset in the target language is avail-   able. Notably, we observe larger gains for Turkish and Hindi in Figure 3d : it is expected due to the fact that MutiATIS++ contains a smaller number of sentences for tr and hi than for the other target languages. Finally, the impact of filtering by teacher confidence (see \u00a73.1) is inconsistent for intent classification (i.e., it seems to be target language-dependent) while it improves the results for slot labelling on average. Encouraged by these insights, we will investigate more sophisticated indomain sentence mining methods in future work. Combining Multi-SentAugment and LayerAgg results in a synergistic effect, based on the additional slight gains observed in Figure 3 (the full results are available in the Appendix C, including the MultiSentAugment results in 5-shot and 20shot setups in the Appendix D). This is expected as the two methods offer distinct enhancements of the base joint NLU model: (i) Multi-SentAugment includes more diverse sentences and lexical information into the training data (i.e., enhancement at the input level), while (ii) LayerAgg aims to select and combine semantic information spread across mBERT's layers (i.e., feature-level enhancement). Zero-Shot vs Few-Shot. As discussed before, using Multi-SentAugment and LayerAgg seems to benefit the base NLU model both in low-data and full-data setups; we observe gains also in 5-shot and 20-shot setups (see Appendix D). Similar to other NLP tasks (e.g., named entity recognition, parsing, QA) (Lauscher et al., 2020) , few-shot setups (e.g., even having only 5 examples per intent or \u224880 annotated sentences in total) yield huge benefits over zero-shot setups (see Table 4 ; compare the results in Table 2 and Figure 3 ). Our results provide another empirical proof calling for more modelling effort in more realistic few-shot cross-lingual transfer setups (Lauscher et al., 2020; Zhao et al., 2021) in future work. We also observe that the results in 10-shot setups when both Multi-SentAugment and LayerAgg are used are mostly on par with the results in 20-shot setups with the base NLU model. In general, this finding validates that the proposed methods can indeed reduce the manual annotation effort. Analysis and Further Discussion Target Language Analysis. While both Multi-SentAugment and LayerAgg are language-agnostic techniques per se, the actual transfer results also depend on the linguistic properties of the source and Table 6 : F 1 scores in a lexical probe of detecting the 1,000 most frequent words on MultiATIS++. target languages. We thus aim to answer the following question: Which languages benefit most from Multi-SentAugment and LayerAgg? To this end, we study the correlations between zero-shot and fewshot transfer performance (i.e., gains over the joint baseline when using the two methods) and sourceto-target language distance, which is based on the language vectors obtained from the URIEL typological database (Littell et al., 2017) . Following Lauscher et al. (2020) , we consider the following linguistic features: syntax (SYN), encoding syntactic properties; language family memberships (FAM) and geographic locations (GEO). The results are shown in Table 5 . SYN similarity has the highest correlation with zero-shot performance gains in both NLU tasks. We suspect that this might stem from LayerAgg's prop-erty to selectively aggregate information from multiple layers, which is easier to learn if the input sequences have similar syntactic structures. In simple words, LayerAgg might benefit more if similar information is found at similar places in the input sentences. FAM and GEO similarities are more correlated with gains in few-shot settings. This might be due to the fact that languages which are similar genealogically (FAM) and geographically (GEO) have more common lexical stems. It means that Multi-SentAugment extracts sentences with lexically similar words which unlock the generalisation abilities of the model. Does LayerAgg Enrich Semantic Content? While the task results seem to suggest this, we design a probing experiment which aims to answer the following question: Do the representations obtained with LayerAgg really capture more semantic information? To this end, we first obtain representations of the 1,000 most frequent words (Conneau et al., 2018; Mehri and Eric, 2021) in Multi-ATIS++ 7 in each sentence using a frozen mBERT task-tuned on English, with and without LayerAgg. We then aim to identify which word was encoded by training a simple linear classifier. The rationale is that by storing more lexical information in the representations, similar words will obtain similar representations: consequently, the classifier should more easily identify the correct word. The micro-averaged F 1 scores are shown in Table 6. The same positive trend with large gains in the classification score is observed in all languages, confirming our hypothesis. We note that the large gains are reported not only for English (which was used for task fine-tuning), but also in other languages, suggesting the benefits of Layer-Agg in boosting cross-lingual lexical capabilities of multilingual encoders in transfer scenarios. Cross-lingual Similarity in LayerAgg. We now assess how LayerAgg captures cross-lingual representation similarity by comparing self-attention maps for different languages emerging from Transformer T. We analyse the similarity of representations of the source language (en) with each target language in MultiATIS++ and xSID using linear Centered Kernel Alignment (l-CKA, Kornblith et al. 2019) , a standard tool for such analyses in Transformer-based models (Conneau et al., 2020b; Glava\u0161 and Vuli\u0107, 2021) . Linear CKA is a repre- sentation similarity metric for representations obtained from neural networks. L-CKA is invariant to orthogonal transformation and isotopic scaling (Glava\u0161 and Vuli\u0107, 2021) . More formally, it is defined as follows: CKA(X, Y ) = ||Y T X|| 2 F ||X T X|| F ||Y T Y || F where X, Y are input matrices. We measure 1) cross-lingual correspondence for slots where l-CKA is computed between the representations of the same slot 8 in different languages; 2) the correlation between the l-CKA scores and transfer performance. The l-CKA scores for MultiATIS++ in Figure 4 reveal high similarities between self-attention maps for similar languages. For instance, the scores are high between Romance languages in Multi-ATIS++ and Germanic languages in XSID. At the same time, the scores are low between ja and Romance languages and between tr and all other, non-Turkic languages. Spearman's \u03c1 correlation scores between the l-CKA scores and zero-shot transfer performance are also very strong. For MultiATIS++, \u03c1 = 0.95 (intent classification) and \u03c1 = 0.92 (slot labelling), while for xSID: \u03c1 = 0.77 (intent classification) and \u03c1 = 0.59 (slot labelling). Another Multilingual Sentence Encoder? Intuitively, the effectiveness of Multi-SentAugment depends on the underlying multilingual sentence encoder F. We now analyse how much performance differs if we replace one state-of-the-art encoder (i.e., LASER) with another: LaBSE (Feng et al., 2020) , running Multi-SentAugment with LaBSE in 3 languages from 3 different language families that also use different scripts -Turkish, Hindi and Japanese. The results in Table 7 do indicate some performance variance across tasks and languages: LaBSE is slightly better in full-data scenarios while LASER performs better in few-shot scenarios. In future work on Multi-SentAugment, we will investigate encoder ensembles, and we plan to make the mining process more scalable and quicker. Conclusion and Future Work We presented 1) LayerAgg, a layer aggregation method which learns to effectively combine useful semantic information from multiple layers of a pretrained multilingual Transformer, and 2) Multi-SentAugment, a data augmentation approach that leverages unannotated Web-scale monolingual corpora to reduce manual annotation efforts. Our results suggest that both methods, applied with stateof-the-art multilingual dialogue NLU models, yield performance benefits both for intent classification and for slot labelling. The methods obtain consistent gains in zero-shot, few-shot and full-data setups on 2 multilingual NLU datasets spanning 16 languages. In future work, we will investigate further applications of Multi-SentAugment in cross-lingual settings (e.g., by mining sentences in languages from the same language family). We will also extend the methods towards truly low-resource languages. Acknowledgements We thank the anonymous reviewers for their helpful comments and suggestions. This work is supported by the ERC PoC Grant MultiCon-vAI:: Enabling Multilingual Conversational AI (no. 957356), and a Huawei research donation. (Feng et al., 2020) in full-data and few-shot scenarios for intent classification and slot labelling, for the LayerAgg model variant. F l-CKA Similarities on xSID",
    "abstract": "Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero-and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora). We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg. Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora. LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT); it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model's cross-lingual capabilities. Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages. The gains are observed in zero-shot, few-shot, and even in full-data scenarios. The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "Turkish",
        "English",
        "Hindi"
    ],
    "numcitedby": "2",
    "year": "2022",
    "month": "May",
    "title": "Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue"
}