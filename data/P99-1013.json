{
    "article": "In what sense is a grammar the union of its rules? This paper adapts the notion of composition, well developed in the context of programming languages, to the domain of linguistic formalisms. We study alternative definitions for the semantics of such formalisms, suggesting a denotational semantics that we show to be compositional and fully-abstract. This facilitates a clear, mathematically sound way for defining grammar modularity. Introduction Developing large scale grammars for natural languages is a complicated task, and the problems grammar engineers face when designing broad-coverage grammars are reminiscent of those tackled by software engineering (Erbach and Uszkoreit, 1990) . Viewing contemporary linguistic formalisms as very high level declarative programming languages, a grammar for a natural language can be viewed as a program. It is therefore possible to adapt methods and techniques of software engineering to the domain of natural language formalisms. We believe that any advances in grammar engineering must be preceded by a more theoretical work, concentrating on the semantics of grammars. This view reflects the situation in logic programming, where developments in alternative definitions for predicate logic semantics led to implementations of various program composition operators (Bugliesi et al., 1994) . This paper suggests a denotational semantics tbr unification-based linguistic formalisms and shows that it is compositional and fully-*I am grateful to Nissim Francez for commenting on an em'lier version of this paper. This work was supported by an IRCS Fellowship and NSF grant SBR 8920230. abstract. This facilitates a clear, mathematically sound way for defining grammar modularity. While most of the results we report on are probably not surprising, we believe that it is important to derive them directly for linguistic formalisms for two reasons. First, practitioners of linguistic formMisms usually do not view them as instances of a general logic programming framework, but rather as first-class programming environments which deserve independent study. Second, there are some crucial differences between contemporary linguistic formalisms and, say, Prolog: the basic elements --typed feature-structures --are more general than first-order terms, the notion of unification is different, and computations amount to parsing, rather than SLD-resolution. The fact that we can derive similar results in this new domain is encouraging, and should not be considered trivial. Analogously to logic programming languages, the denotation of grammars can be defined using various techniques. We review alternative approaches, operational and denotational, to the semantics of linguistic formalisms in section 2 and show that they are \"too crude\" to support grammar composition. Section 3 presents an alternative semantics, shown to be compositional (with respect to grammar union, a simple syntactic combination operation on grammars). However, this definition is \"too fine\": in section 4 we present an adequate, compositional and fully-abstract semantics for linguistic formalisms. For lack of space, some proofs are omitted; an extended version is available as a technical report (Wintner, 1999) . Grammar semantics Viewing grammars as formal entities that share many features with computer programs, it is natural to consider the notion of semantics of ratification-based formalisms. We review in this se(:tion the operational definition of Shieber et a,1. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206) . We show that these definitions are equivalent and that none of them supports compositionality. 2.1 Basic notions W(, assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992) . Grammars are defined over typed featwre .structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991) . TFSs are partially ordered by subsumption, with \u00b1 the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) ()r Wintner and Francez (1999) ) is a sequence of TFSs, with possible reentrancies among diffi;rent elements in the sequence. Meta-variables A,/3 range over TFSs and a, p -over MRSs. MRSs are partially ordered by subsumption, den()ted '__', with a least upper bound operation ()f 'an'llfication, denoted 'U', and a greatest lowest t)(mnd denoted 'W. We assume the existence of a. fixed, finite set WORDS of words. A lexicon associates with every word a set of TFSs, its category. Meta-variable a ranges over WORDS and .w --over strings of words (elements of WORDS*). Grammars are defined over a signature of types and features, assumed to be fixed below. Definition 1. A rule is an MRS of length greater than or equal to 1 with a designated (fir'st) element, the head o.f the rule. The rest of the elements .form the rule's body (which may be em, pty, in which case the rule is depicted a.s' a TFS). A lexicon is a total .function .from WORDS to .finite, possibly empty sets o.f TFSs. A grammar G = (T\u00a2,/:, A s} is a .finite set of ,rules TO, a lexicon \u00a3. and a start symbol A s that is a TFS. The definition of unification is lifted to MRSs: let a,p be two MRSs of the same length; the 'Grammars are displayed using a simple description language, where ':' denotes feature values. 2Assmne that in all the example grammars, the types s, n, v and vp are maximal and (pairwise) inconsistent.  If f, g, are flmctions over the same (set) domain, .f + g is )~I..f(I) U .q(I). Let ITEMS = { [w,i,A,j] [ w E WORDS*, A is a TFS and i,j E {0,1,2,3,...}}. Let Z = 2 ITEMS. Metavariables x, y range over items and I -over sets of items. When 27 is ordered by set inclusion it forms a complete lattice with set union as a least upper bound (lub) operation. A flmction T : 27 -+ 27 is monotone if whenever 11 C_/2, also T(I1) C_ T(I2). It is continuous iftbr every chain I1 C_ /2 C_ ..., T(Uj< ~/.i) = Uj<~T(Ij) . If a function T is monotone it has a least fixpoint (Tarski-Knaster theorem); if T is also continuous, the fixpoint can be obtained by iterative application of T to the empty set (Kleene theorem): lfp(T) = TSw, where TI\" 0 = 0 and T t n = T(T t (n-1)) when 'n is a successor ordinal and (_Jk<n(T i\" n) when n is a limit ordinal. When the semantics of programming languages are concerned, a notion of observables is called for: Ob is a flmction associating a set of objects, the observables, with every program. The choice of semantics induces a natural equivalence operator on grammars: given a semantics 'H', G1 ~ G2 iff ~GI~ = ~G2~. An essential requirement of any semantic equivalence is that it 97' be correct (observables-preserving): if G1 -G2, then Ob(G1) = Ob(G2). Let 'U' be a composition operation on grammars and '\u2022' a combination operator on denorations. A (correct) semantics 'H' is compo-.s'itional (Gaifinan and Shapiro, 1989) if whenever ~1~ : ~G2~ and ~G3] --~G4], also ~G, U G3~ = [G2 U G4]. A semantics is commutative (Brogi et al., 1992) if ~G1 UG2] = ~G,~ \u2022 [G2~. This is a stronger notion than (:ompositionality: if a semantics is commutative with respect to some operator then it is compositional. An operational semantics As Van Emden and Kowalski (1976) note, \"to define an operational semantics for a programruing language is to define an implementational independent interpreter for it. For predicate logic the proof procedure behaves as such an interpreter.\" Shieber et al. (1995) view parsing as a. deductive process that proves claims about the grammatical status of strings from assumptions derived from the grammar. We follow their insight and notation and list a deductive system for parsing unification-based grammars. Definition 3. The deductive parsing system associated with a grammar G = (7~,F.,AS} is defined over ITEMS and is characterized by: Axioms: [a, i, A, i + 1] i.f B E Z. (a) and B K A; [e, i, A, i] if B is an e-rule in T~ and B K_ A Goals: [w, 0, A, [w]] where A ~ A s Inference rules: [wx , i l , A1, ill, ..., [Wk, ik, Ak , Jk ] [Wl \" \" \" Wk, i, A, j] if .'h = i1,+1 .for 1 <_ l < k and i = il and J = Jk and (A1,...,Ak) =>a A When an item [w,i,A,j] can be deduced, applying k times the inference rules associ-z~ted with a grammar G, we write F-~[w, i, A, j]. When the number of inference steps is irrelevant it is omitted. Notice that the domain of items is infinite, and in particular that the number of axioms is infinite. Also, notice that the goal is to deduce a TFS which is subsumed by the start symbol, and when TFSs can be cyclic, there can be infinitely many such TFSs (and, hence, goals) -see Wintner and Francez (1999) . G2 iy ]C1 o, = G2Bo , We use the operational semantics to define the language generated by a grammar G: L(G) = {(w,A} [ [w,O,A,l',,[] E [G]o,}. Notice that a language is not merely a set of strings; rather, each string is associated with a TFS through the deduction procedure. Note also that the start symbol A ' does not play a role in this definition; this is equivalent to assuming that the start symbol is always the most general TFS, _k. The most natural observable for a grammar would be its language, either as a set of strings or augmented by TFSs. Thus we take Ob(G) to be L(G) and by definition, the operational semantics '~.] op' preserves observables. Denotational semantics In this section we consider denotational semantics through a fixpoint of a transformational operator associated with grammars. -This is essentially similar to the definition of Pereira and Shieber (1984) and Carpenter (1992, pp. 204-206) . We then show that the denotational semantics is equivalent to the operational one. Associate with a grammar G an operator 7~ that, analogously to the immediate consequence operator of logic programming, can be thought of as a \"parsing step\" operator in the context of grammatical formalisms. For the following discussion fix a particular grammar G = (n,E,A~). For every grammar G, To., is monotone and continuous, and hence its least fixpoint exists and l.fp(TG) = TG $ w. Following the paradigm of logic programming languages, define a fixpoint semantics for unification-based grammars by taking the least fixpoint of the parsing step operator as the denotation of a grammar. Definition 6. The fixpoint denotation of a grammar G is ~G [.fp = l.fp(Ta) . G1 =--.fp G2 iff ~ti,( T<; ~ ) = l fp(Ta~). The denotational definition is equivalent to the operational one: Theorem 1. For x E ITEMS, X E lfp(TG) iff ~-(? x. The proof is that [w,i,A,j] E Ta $ n iff F-7;,[w, i, A, j], by induction on n. Corollary 2. The relation '=fp' is correct: whenever G1 =.fp G2, also Ob(G1) = Ob(a2). Compositionality While the operational and the denotational semantics defined above are standard for complete grammars, they are too coarse to serve as a model when the composition of grammars is concerned. When the denotation of a grammar is taken to be ~G]op, important characteristics of the internal structure of the grammar are lost. To demonstrate the problem, we introduce a natural composition operator on grammars, namely union of the sets of rules (and the lexicons) in the composed grammars.   The implication of the above proposition is that while grammar union might be a natural, well defined syntactic operation on grammars, the standard semantics of grannnars is too coarse to support it. Intuitively, this is because when a grammar G1 includes a particular rule p that is inapplicable for reduction, this rule contributes nothing to the denotation of the grammar. But when G1 is combined with some other grammar, G2, p might be used for reduction in G1 U G2, where it can interact with the rules of G2. We suggest an alternative, fixpoint based semantics for unification based grammars that naturally supports compositionality. (cat : vp) -~ (co, t : v) (cat: vp) --+ (cat: v) (cat: n) /:(John) = {(cat: n)} \u00a3(sleeps) = \u00a3(loves) = {(cat: v)} G1UGa : A s = (cat : s) (cat: s) --+ (cat: n) (cat: vp) C(John) = {(cat: ',,,)} \u00a3(loves) = {(cat: v)} GI U G4 : A s = (cat : s) (co A compositional semantics To overcome the problems delineated above, we follow Mancarella and Pedreschi (1988) in considering the grammar transformation operator itself (rather than its fixpoint) as the denota-tion of a grammar. Definition 8. The algebraic denotation o.f G is ffGffa I = Ta. G1 -at G2 iff Tal = TG2. Not only is the algebraic semantics composi-tionM, it is also commutative with respect to grammar union. To show that, a composition operation on denotations has to be defined, and we tbllow Mancarella and Pedreschi (1988) in its definition: Tc;~ \u2022 To;., = ),LTc, (~) u Ta2 ( 5 Theorem 4. The semantics '==-at ' is commutative with respect to grammar union and '\u2022': for e, vcry two grammars G1, G2, [alffat\" ~G2ffal = :G I [-J G 2 ff (tl . Proof. It has to be shown that, for every set of items L Tca~a., (I) = Ta, (I)u Ta.,(I). \u2022 if x E TG1 (I) U TG~, (I) then either x G Tch (I) or x E Ta., (I) . From the definition of grammar union, x E TG1uG2(I) in any case. \u2022 if z E Ta~ua.,(I) then x can be added by either of the three clauses in the definition of Ta. if x is added by the first clause then there is a rule p G 7~1 U T~2 that licenses the derivation through which z is added. Then either p E 7~1 or p G T~2, but in any case p would have licensed the same derivation, so either ~ Ta~ (I) or \u2022 ~ Ta~ (I). if x is added by the second clause then there is an e-rule in G1 U G2 due to which x is added, and by the same rationale either x C TG~(I) or x E TG~(I). if x is added by the third clause then there exists a lexical category in \u00a31 U \u00a32 due to which x is added, hence this category exists in either \u00a31 or \u00a32, and therefore x C TG~ (I) U TG2 (I). [] Since '==-at' is commutative, it is also compositional with respect to grammar union. Intuitively, since TG captures only one step of the computation, it cannot capture interactions among different rules in the (unioned) grammar, and hence taking To: to be the denotation of G yields a compositional semantics. The Ta operator reflects the structure of the grammar better than its fixpoint. In other words, the equivalence relation induced by TG is finer than the relation induced by lfp(Tc). The question is, how fine is the '-al' relation? To make sure that a semantics is not too fine, one usually checks the reverse direction. \u2022 for all G, Ob(G U G~) = Ob(G [3 G2). The only difference between GUG1 and GUG2 is the presence of the rule (cat : up) -+ (cat : up) in the former. This rule can contribute nothing to a deduction procedure, since any item it licenses must already be deducible. Therefore, any item deducible with G U G1 is also deducible with G U G2 and hence Definition 9. A fully-abstract equivalence relation '-' is such that G1 =-G'2 'i,.[.-f .for all G, Ob(G1 U G) = Ob(G.e U G). Ob(G U G1) ----Ob(G U G,2). [] A better attempt would have been to consider, instead of TG, the fbllowing operator as the denotation of G: [G]i d = AI.Ta(I) U I. In other words, the semantics is Ta + Id, where Id is the identity operator. Unfortunately, this does not solve the problem, as '~']id' is still not fully-abstract. A fully abstract semantics We have shown so far that 'Hfp' is not compositional, and that 'Hid' is compositional but not fully abstract. The \"right\" semantics, therefore, lies somewhere in between: since the choice of semantics induces a natural equivalence on grammars, we seek an equivalence that is cruder thzm 'Hid' but finer than 'H.fp'. In this section we adapt results from Lassez and Maher (1984) a.nd Maher (1988) to the domain of unification-b~Lsed linguistic formalisms. Consider the following semantics for logic programs: rather than taking the operator assodated with the entire program, look only at the rules (excluding the facts), and take the meaning of a program to be the function that is obtained by an infinite applications of the operator associated with the rules. In our framework, this would amount to associating the following operator with a grammar: Definition 10. Let RG : Z -~ Z be a transformation on sets o.f items, where .for every [ C ITEMS, [w,i,A,j] E RG(I) iff there exist Yl,...,Yk E I such that yl = [wz,it,Al,jd .for 1 _ < l _ < k and il+t = jl .for 1 < l < k and i, = 1 and.jk = J and (A1,...,Ak) ~ A and \"~1) ~ 'tl) 1 \u2022 \u2022 \u2022 ?U k. Th, e functional denotation of a grammar G is /[G~.f,,, = (Re + Id) ~ = End-0 (RG + Id) n. Notice that R w is not RG \"[ w: the former is a function \"d from sets of items to set of items; the latter is a .set of items. Observe that Rc is defined similarly to Ta (definition 5), ignoring the items added (by Ta) due to e-rules and lexical items. If we define the set of items I'nitc to be those items that are a.dded by TG independently of the argument it operates on, then for every grammar G and every set of items I, Ta(I) = Ra(I) U Inita. Relating the functional semantics to the fixpoint one, we tbllow Lassez and Maher (1984) in proving that the fixpoint of the grammar transformation operator can be computed by applying the fimctional semantics to the set InitG. [] The choice of 'Hfl~' as the semantics calls for a different notion of' observables. The denotation of a grammar is now a flmction which reflects an infinite number of' applications of the grammar's rules, but completely ignores the erules and the lexical entries. If we took the observables of a grammar G to be L(G) we could in general have ~G1].f,. = ~G2]fl~. but Ob(G1) 7 ~ Ob(G2) (due to different lexicons), that is, the semantics would not be correct. However, when the lexical entries in a grammar (including the erules, which can be viewed as empty categories, or the lexical entries of traces) are taken as input, a natural notion of observables preservation is obtained. To guarantee correctness, we define the observables of a grammar G with respect to a given input. The above definition corresponds to the previous one in a natural way: when the input is taken to be Inita, the observables of a grammar are its language. Theorem 8. For all G, L(G) = Obinita(G). Theorem 9. fiG1 U G2~fn = ~Gl]fn \" ~G2~.fn. The proof is basically similar to the case of logic programming (Lassez and Maher, 1984) and is detailed in Wintner (1999) . Theorem 10. The semantics '~'[fn' is fully abstract: ,for every two grammars G1 and G2, 'llf .for\" every grammar G and set of items I, Obr(G1 U G) = ObI(G2 U G), then G1 =fn G2. The proof is constructive: assuming that G t ~f;~ G2, we show a grammar G (which det)ends on G1 and G2) such that Obt(G1 U G) \u00a2 Obr(G2 U G). For the details, see Wintner (1999) . Conclusions This paper discusses alternative definitions for the semantics of unification-based linguistic formalisms, culminating in one that is both compositional and fully-abstract (with respect to grammar union, a simple syntactic combination operations on grammars). This is mostly an adaptation of well-known results from h)gic programming to the ti'amework of unification-based linguistic tbrmalisms, and it is encouraging to see that the same choice of semantics which is compositional and fiflly-abstra(:t for Prolog turned out to have the same desirable properties in our domain. The functional semantics '~.].f,' defined here assigns to a grammar a fimction which reflects the (possibly infinite) successive application of grammar rules, viewing the lexicon as input to the parsing process. We, believe that this is a key to modularity in grammar design. A grammar module has to define a set of items that it \"exports\", and a set of items that can be \"imported\", in a similar way to the declaration of interfaces in programming languages. We are currently working out the details of such a definition. An immediate application will facilitate the implementation of grammar development systems that support modularity in a clear, mathematically sound way. The results reported here can be extended in various directions. First, we are only concerned in this work with one composition operator, grammar union. But alternative operators are possible, too. In particular, it would be interesting to define an operator which combines the information encoded in two grammar rules, for example by unifying the rules. Such an operator would facilitate a separate development of grammars along a different axis: one module can define the syntactic component of a grammar while another module would account for the semantics. The composition operator will unify each rule of one module with an associated rule in the other. It remains to be seen whether the grammar semantics we define here is compositional and fully abstract with respect to such an operator. A different extension of these results should provide for a distribution of the type hierarchy among several grammar modules. While we assume in this work that all grammars are defined over a given signature, it is more realistic to assume separate, interacting signatures. We hope to be able to explore these directions in the future.",
    "abstract": "In what sense is a grammar the union of its rules? This paper adapts the notion of composition, well developed in the context of programming languages, to the domain of linguistic formalisms. We study alternative definitions for the semantics of such formalisms, suggesting a denotational semantics that we show to be compositional and fully-abstract. This facilitates a clear, mathematically sound way for defining grammar modularity.",
    "countries": [
        "Argentina",
        "United States"
    ],
    "languages": [],
    "numcitedby": "14",
    "year": "1999",
    "month": "June",
    "title": "Compositional Semantics for Linguistic Formalisms"
}