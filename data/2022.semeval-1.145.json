{
    "article": "In this paper, we describe our submissions to SemEval-2022 contest. We tackled subtask 6-A -\"iSarcasmEval: Intended Sarcasm Detection In English and Arabic -Binary Classification\". We developed different models for two languages: English and Arabic. We applied 4 supervised machine learning methods, 6 preprocessing methods for English and 3 for Arabic, and 3 oversampling methods. Our best submitted model for the English test dataset was an SVC model that balanced the dataset using SMOTE and removed stop words. For the Arabic test dataset, our best submitted model was an SVC model that preprocessed removed longation. Introduction The rapid development of various types of social networks allows the development of increasingly offensive language in general and sarcasm in particular. Sarcasm is the use of words that mean the opposite of what you want to say especially to insult someone, show irritation, or be funny 1 . Sarcastic language can harm individuals or groups of people and may cause harmful effects on society. Thus, it is important to develop highquality systems capable of detecting sarcastic expressions automatically. Sarcasm detection is a difficult task not only for a computer but even for a human being. Highquality performance of this task requires understanding the context of the situation, the relevant culture, and in some cases the specific issue or people involved in this situation (Maynard and Greenwood, 2014) . Moreover, the noisy nature of social media texts especially Twitter messages make the detection task even harder. Therefore, it is an interesting and challenging task to detect sarcasm using supervised machine learning (ML) methods and natural language processing (NLP) tools. Furthermore, Rosenthal et al. (2014) show a significant drop in sentiment polarity classification performance when processing sarcastic tweets, compared to non-sarcastic ones. In this paper, we describe our research and participation in subtask 6-A for sarcasm detection in tweets written in two languages: English and Arabic. The full description of task 6 in general and 6-A in particular including the datasets and the participating teams is given in Abu Farha et al. (2022) . The structure of the rest of the paper is as follows. Section 2 introduces a background concerning sarcasm detection, text preprocessing, and text classification with imbalanced classes. Section 3 describes subtask 6-A and its datasets. In Section 4, we present the submitted models and their experimental results. Section 5 summarizes and suggests ideas for future research. Related Research Most prior textual sarcasm detection datasets have been annotated by using either manual labeling or a weak supervision method. In the first approach, sarcasm labels are provided by human annotators (e.g., Filatova, 2012; Riloff et al., 2013; Abercrombie and Hovy, 2016) . However, such labels represent annotator perception, which may differ from the author intention, as further pointed out by Oprea and Magdy (2020) . In the second approach, texts are labeled as sarcastic if they meet predefined criteria, e.g., including specific tags (e.g. #irony, #sarcastic, #sarcasm) (Pt\u00e1\u010dek et al., 2014; Khodak et al., 2018) . However, this method can lead to noisy labels because of various reasons (Oprea and Magdy, 2020) . To overcome these disadvantages, Oprea and Magdy (2019) introduced another method. In their method, the sarcasm labels for texts are provided by the authors themselves. Most of the sarcasm detection studies are in the English language (Campbell and Katz, 2012; Riloff et al., 2013; Bamman and Smith, 2015; Rajadesingan et al., 2015; Wallace et al., 2015; Amir et al., 2016; Joshi et al., 2016; Hazarika et al., 2018; Oprea and Magdy, 2019) . There are also some studies in Arabic (Karoui et al., 2017; Ghanem et al.,2019; Abbes et al., 2020; Abu-Farha and Magdy, 2020) . For more information about various issues concerning sarcasm detection please refer to survey papers such as Joshi et al. (2017 ), Sarsam et al. (2020) , Verma et al. (2021) , and Moores and Mago (2022) . Text preprocessing Text preprocessing is an important step in many NLP domains such as ML, sentiment analysis, text mining, and text classification (TC). In text documents in general and in social text documents in particular, it is common to find various types of noise, e.g., typos, emojis, slang, HTML tags, spelling mistakes, and repetitive letters. Analysis of text that has not been carefully cleaned or preprocessed might lead to misleading results. Not all of the preprocessing types are considered effective for TC tasks. For instance, HaCohen-Kerner et al. (2008) demonstrated that the use of word unigrams including stop words leads to improved results compared to the results obtained using word unigrams excluding stop words. HaCohen-Kerner et al. ( 2019 ) investigated the impact of all possible combinations of six preprocessing methods (spelling correction, HTML tag removal, converting uppercase letters into lowercase letters, punctuation mark removal, reduction of repeated characters, and stopword removal) on TC in three benchmark mental disorder datasets. In another study, HaCohen-Kerner et al. (2020) explored the influence of various combinations of the same six basic preprocessing methods mentioned in the previous paragraph on TC in four general benchmark text corpora using a bag-of-words representation. The general conclusion was that it is always advisable to perform an extensive and systematic variety of preprocessing methods, combined with TC experiments because this contributes to improving TC accuracy. Text classification with imbalanced classes The An additional frequent method is to generate synthetic samples which means randomly sampling the attributes from instances in the minority class. There are several algorithms that support the generation of synthetic samples. The most popular is called the Synthetic Minority Oversampling Technique (SMOTE) (Chawla, 2002) . This method is an oversampling method that creates synthetic samples from the minor class instead of creating copies. This method selects two or more similar instances and perturbs an instance one attribute at a time by a random amount within the difference to the similar instances. We used also two other oversampling methods BorderlineSMOTE and ADASYN that work similarly. Readers interested in expanding and deepening the topic of solutions to TC with imbalanced classes are referred to the following articles (Chawla et al., 2002; He and Ma, 2013; Krawczyk, 2016; Brownlee, 2020 , Shaikh et al., 2021) . Task and Datasets Description Tables 1-4 present various statistical details about the training and test sets for English and Arabic. The analysis of the details presented in Tables 1 and 2 show that the training and test sets for English are highly imbalanced, with a ratio of about 25:75 and 14:86 (non-sarcastic:sarcastic), respectively. We tried to balance the dataset in our experiments using the oversampling methods that we have mentioned above. The analysis of the details presented in Tables 3 and 4 show a similar picture. The training and test sets for Arabic are highly imbalanced, with a ratio of about 39:61 and 14:86 (nonsarcastic:sarcastic), respectively. Also, for Arabic, we tried to balance the dataset in our experiments using the oversampling methods that we have mentioned above. Both for English and Arabic, the test datasets are even more imbalanced than the compatible training datasets. The Submitted Models and Experimental Results We applied 4 supervised ML methods on the training datasets: Random Forest (RF), Support Vector Classifier (SVC), Logistic regression (LR), and Decision Tree (DT). RF is an ensemble learning method for classification and regression (Breiman, 2001) . Ensemble methods use multiple learning algorithms to obtain improved predictive performance compared to what can be obtained from any of the constituent learning algorithms. RF operates by constructing a multitude of decision trees at training time and outputting classification for the case at hand. RF combines Breiman's \"bagging\" (Bootstrap aggregating) idea in Breiman (1996) and a random selection of features introduced by Ho (1995) to construct a forest of decision trees. SVC is a variant of the support vector machine (SVM) ML method (Cortes and Vapnik, 1995) implemented in SciKit-Learn. SVC uses LibSVM (Chang & Lin, 2011) , which is a fast implementation of the SVM method. SVM classifies vectors in a feature space into one of two sets, given training data. It operates by constructing the optimal hyperplane dividing the two sets, either in the original feature space or in higher dimensional kernel space. LR (Cox, 1958; Hosmer et al., 2013) is a linear classification model. It is known also as maximum entropy regression (MaxEnt), logit regression, and the log-linear classifier. In this model, the probabilities describing the possible outcome of a single trial are modeled using a logistic function. DT (Song and Ying, 2015) is a flowchart-like structure method in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). These ML methods were applied using the following tools and information sources: \u2022 The Python 3.7.3 programming language 2 . \u2022 Scikit-learna Python library for ML methods 3 . \u2022 Numpya Python library that provides fast algebraic calculous processing, especially for multidimensional objects 4 . We applied six preprocessing methods for English: 1. change to lower-case 2. stop words removal 3. numbers removal 4. emojis removal 5. HTML tags removal 6. punctuations removal 2 https://www.python.org/downloads/release/python-373/ 3 https://scikit-learn.org/stable/index.html 4 https://numpy.org For Arabic we applied three preprocessing methods: 1. punctuations removal 2. Tashkeel removal 5 3. longation removal 6,7 We applied three oversampling methods to balance the data: 4. SMOTE 5. BorderlineSMOTE 6. ADASYN Tables 5 and 6 present the F1-scores over the PCL class of our models for English and Arabic, respectively. The analysis of the details presented in Tables 5 and 6 show that there is a impact on the longation in the Arabic language, and removing it improved the models. In the English language, we do not see a specific preprocess method that is the most effective method (mark in boldthe three highest results in each language for different number of word unigrams). Summary and Future Research In this paper, we describe our models and submissions to subtask 6-A of SemEval-2022: sarcasm detection in Twitter messages written in English and Arabic using preprocessing methods and word n-grams. We applied 4 supervised ML methods, 6 preprocessing methods for English and 3 for Arabic, and 3 oversampling methods. Our best submitted model for the English test dataset was an SVC model that balanced the dataset using SMOTE and removed stop words. For the Arabic test dataset, our best submitted model was an SVC model that preprocessed longation removal. There are various ideas for future research that are connected to the nature of Twitter messages as follows: (1) the use of skip character ngrams because they serve as generalized ngrams that allow us to overcome problems such as noise and sparse data (HaCohen-Kerner et al., 2017) , which are common to Twitter messages and (2) Many Twitter messages contain acronyms. Acronym disambiguation might enable better classification (HaCohen-Kerner et al., 2010A) . Another idea that may lead to better classification is to use additional feature sets such as stylistic feature sets (HaCohen-Kerner et al., 2010B) .",
    "abstract": "In this paper, we describe our submissions to SemEval-2022 contest. We tackled subtask 6-A -\"iSarcasmEval: Intended Sarcasm Detection In English and Arabic -Binary Classification\". We developed different models for two languages: English and Arabic. We applied 4 supervised machine learning methods, 6 preprocessing methods for English and 3 for Arabic, and 3 oversampling methods. Our best submitted model for the English test dataset was an SVC model that balanced the dataset using SMOTE and removed stop words. For the Arabic test dataset, our best submitted model was an SVC model that preprocessed removed longation.",
    "countries": [
        "Israel"
    ],
    "languages": [
        "English",
        "Arabic"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "{JCT} at {S}em{E}val-2022 Task 6-A: Sarcasm Detection in Tweets Written in {E}nglish and {A}rabic using Preprocessing Methods and Word N-grams"
}