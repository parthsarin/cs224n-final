{
    "article": "Discourse planning systems developed to date apply local considerations in order to generate an initial presentation that achieves a given communicative goal. However, they lack a global criterion for selecting among alternative presentations. In this paper, we cast the problem of planning discourse as an optimization problem, which allows the definition of a global optimization criterion. In particular, we consider two such criteria: (1) generating the most concise discourse, and (2) generating the 'shallowest' discourse, i.e., discourse that requires the least prerequisite information. These criteria are embodied in a discourse planning mechanism which considers the following factors: (1) the effect of a user's inferences from planned utterances on his/her beliefs, (2) the amount of prerequisite information a user requires to understand an utterance, and (3) the amount of information that must be included in referring expressions which identify the concepts mentioned in an utterance. This mechanism is part of a discourse planning system called WISHFUL-II which generates explanations about concepts in technical domains. Introduction Schema-based Natural Language Generation (NLG) systems, e.g., [Weiner, 1980; McKeown, 1985; Paris, 1988] , determine the information to be presented based on common patterns of discourse. Goal-based planners, e.g., [Moore and Swartout, 1989; Cawsey, 1990] , select a discourse operator if its prescribed effect matches a given communicative goal. If there is more than one such operator, the operator whose prerequisite information is believed by the user is preferred. However, if all the candidate operators require the generation of discourse that conveys some prerequisite information, the selection process is either random or the system designer determines in advance which operators should be preferred. In this paper, we cast the problem of planning discourse that achieves a given communicative goal as an application of an optimization algorithm. This approach supports the definition of different optimization objectives, such as generating (1) the most concise discourse; (2) the 'shallowest' discourse, i.e., discourse that requires the least amount of prerequisite information; or (3) the most concrete discourse, i.e., discourse with the most examples. The resulting mechanism is part of a discourse planning system called WISHFUL-II, which is a descendant of the WISHFUL system described in [gukerman and McConachy, 1993a] . Table 1 illustrates the discourse generated by our system using the concise and the shallow optimization objectives. These texts were generated in order to convey the attributes type, habitat, body parts, height and transportation mode of the concept Wallaby to a user who owns a toy wallaby calhd Wally, and knows something about kangaroos, but is not familiar with the term pouch. The concise discourse conveys most of the intended information by means of a Simile between wallabies and kangaroos. The Simile also yields the erroneous inference that wallabies are the same height as kangaroos. To contradict this inference, the system asserts that wallabies are 3 ft. tall. Since the user does not know that kangaroos have a pouch, this is asserted, and since the user does not know what a pouch is, information which evokes this concept is presented. The shallow discourse, on the other hand, uses Wally (the toy wa.llaby) to convey the body parts of a wallaby without naming them explicitly. This information is complemented by Assertions about a wallaby's type, habitat, height and transportation mode. In the next section, we present an overview of WISHFUL-II. In Section 3, we describe the discourse plaaming mechanism. We then discuss the results we have obtained, and present concluding remarks. 2 Overview of the System WISHFUL-II receives as input a conceptto be conveyed, e.g., Wallaby, a list of aspects that must be conveyed about this concept, e.g., habitat and body parts, and a desired level of expertise the user should attain as a result of the presentation. WISHFUL-II was used to generate descriptive discourse in various technical domains, such as chemistry, high-school algebra, animal taxonomy and Lisp. It produces multi-sentence paragraphs of connected English text. The discourse planning mechanism, which is the focus of this paper, generates a set of Rhetorical Devices (RDs), where each RD is a rhetorical action, such as Assert, Negate or Instantiate, applied to a proposition. This set of RDs is optimal with respect to a given optimization criterion, e.g., conciseness or depth. The following steps axe performed by WISHFUL-II. 1. Content Selection -WISHFUL-II consults a model of the user's beliefs in order to determine which propositions must be presented to convey the given aspects of a concept. This step selects propositions about which the user has misconceptions, propositions that axe unknown to the user, and propositions that are not believed by the user to the extent demanded by the desired level of expertise. 2. Generation of the Optiraal Set of RDs -WISHFUL-II searches for a set of RDs that conveys the propositions generated in the previous step while satisfying a given optimization objective. The process of generating candidate sets of RDs considers the following factors: (1) the effect of inferences from an RD on a user's beliefs; (2) the amount of prerequisite information required by the user to understand the concepts in an RD; and (3) the amount of information to be included in referring expressions which identify the concepts in an RD. 3. Discourse Structuring-A discourse structuring mechanism extracts discourse rdations and constraints from the set of RDs generated in Step 2. It then generates an ordered sequence of the RDs in this set, where the strongest relations between the RDs are represented and no constraints are violated. Where necessary, the RDs in this sequence are interleaved with conjunctive expressions that signal the relationship between them [Zukerman and Mc-Conachy, 1993b ]. 4. Generation of Anaphorlc Referring Expressions-Anaphoric referring expressions are generated for RDs that refer to a concept in focus. This process follows the organization of the discourse, since the appropriate use of anaphora depends on the structure of the discourse. 5. Discourse Realization -The resulting sequence of RDs is realized in English by means of the Functional Unification Grammar described in [Elhadad, 1992] . 3 Generating the Optimal Set of RDs The main stage of the optimization procedure consists of generating alternative sets of RDs that can convey a set of propositions. The first step in this stage consists of generating candidate RDs that can convey each proposition separately. To this effect, WISHFUL-II reasons from the propositions determined in the content selection step to the RDs that may be used to convey these propositions. This reasoning mechanism has been widely used in NLG systems, e.g., [Moore and Swartout, 1989; Cawsey, 1990] . The process of generating a set of RDs that can convey a set of propositions is not a straightforward extension of the process of generating candidate RDs that can convey each proposition separately. This is due to the foUowing reasons: (1) an inference from an RD generated to convey a proposition pl may undermine the effect of an RD generated to convey a proposition pj; and (2) an RD generated to convey a proposition pl may be made obsolete by an RD which was generated to convey another proposition, but from which the user can infer pi. Further, it is not sufficient to propose a single set of RDs that can convey a set of propositions, because a set of RDs that initially appears to be promising may require a large number of RDs to convey its prerequisite information or to identify the concepts mentioned in it. Thus, after generating the RDs that can convey each of the intended propositions separately, the optimization procedure must consider concurrently the following inter-related factors in order to generate candidate sets of RDs that can convey the intended propositions: (1) the effect of the RDs in a set on a user's beliefs, (2) the prerequisite information that the user must know in order to understand these RDs, and (3) the referring expressions required to identify the concepts mentioned in these RDs. Owing to the interactions between the RDs in a set, the problems of generating the most concise set of RDs and generating the shallowest set of RDs are NP-hard 1. Since this level of complexity is likely to be maintained for other optimization objectives, we have chosen a weak search procedure for the implementation of the optimization process. In the following sections, we describe the optimization process as an application of the Graphsearch algorithm [Nilsson, 1980] , and discuss the implementation of the main steps of this algorithm. The Basic Optimization Procedure Our optimization procedure, Optimize-RDs, receives as input the set of propositions generated in the content selection step of WISHFUL-II. It implements a simplitied version of the Graphsearch algorithm [Nilsson, 1980] to generate a set of RDs that conveys these propositions and satisfies a given optimization criterion. The discourse planning considerations are incorporated during the expansion stage and the evaluation stage of Graphsearch. The expansion stage of our procedure activates algorithm Expanding Sets of RDs Procedure Expand-sets-of-RDs receives as input a node to be expanded, and returns all the minimally sufficient sets of RDs that convey the set of propositions in this node, accompanied by their respective prerequisite propositions and referring expressions. We compute all the minimally sufficient sets of RDs, rather than just the minimal set of RDs, because a set of RDs that initially appears to be promising may require a large number of RDs in order to convey its prerequisite information or to identify the concepts in it. Algorithm Expand-sets-of-RDs(n) 1. Determine RDs that increase a user's belief in each proposition in node n. (Not all the RDs generated in this step axe capable of conveying an intended proposition by themselves, but they may be able to do so in combination with other RDs.) (Section 3.2.1) 2. Use these RDs to construct minimally sufficient sets of RDs that convey all the propositions in n jointly. Put these sets of I~Ds in {A47~D}. (Section 3.2.2) 3. Determine the prerequisite propositions required by each set of RDs in {A4gD} so that the user can understand it. (Section 3.2.3) 4. Determine referring expressions which evoke the concepts in each set of RDs in {.MT~D}. (Section 3.2.4) The output of Ezpand.sets-of.RDs takes the form of a set of RD-Graphs. An RD-Graph is a directed graph that contains the following components: (1) the set of propositions to be conveyed (pl,.--,p,~ in Figure 1 ); (2) a minimally sufficient set of RDs (RD~,..., RDm); (3) the effect of the inferences from the RDs in this set on the intended propositions, and possibly on other (unintended) propositions (labelled wid); (4) the prerequisite propositions that enable these RDs to succeed (p~ .... ,p~); (5) the relations between the prerequisite propositions and the main RDs (in thick lines); (6) the sets of RDs that evoke concepts in the main RDs ({RD m+l } .... , {RD m+t }); and (7) the relations between the evocative sets of RDs and the main RDs (labelled vm+id)-The main set of RDs and the relations between the RDs in this set and the propositions to be conveyed are generated in Step 2 above. The weight wid contains information about the effect of RDi on the user's belief in proposition pj. The prerequisite propositions are generated in Step 3, and the evocative sets of RDs and their corresponding links are produced in Step 4. 3.L1 Determining RDs Given a list of propositions to be conveyed, {p}, in this step we propose RDs that can increase a user's belief in each of these propositions. To this effect, for each proposition pi E {p} we first consider the following RDs: Assertion (A), Negation (N), Instantiation (I) and Simile (S), Return( RD-list) To illustrate this process, let us consider a situation where we want to convey to the user the following propositions: From the above Assertions the user may also infer incorrectly that wombats hop. In this case, a proposition which negates this incorrect conclusion, i.e., [Wombat -~hop] , is assigned to {newp}. The RDs that can convey this proposition in our example axe Negate[Wombat hop] and Instantiate [Wombat -~hop] , where the Instantiation is performed with respect to a wombat called Wimpy that is known to the user. These RDs in turn may yield the incorrect inferences that neither wallabies nor kangaroos hop, which contradict the intended propositions. However, since the propositions that contradict these inferences already exist in {oldp}, the process stops. Constructing Minimally Sufficient Sets of RDs In this step, we generate all the minimally sufficient sets of RDs that can convey jointly all the intended propositions. For each proposition pi, we first determine whether RDs that were generated to convey other propositions can decrease a user's belief in pl. Next, for each RD in RD.list(pl), we determine whether it can overcome the detrimental effect of these 'negative' RDs. This step identifies combinations of RDs that cannot succeed in conveying the intended propositions. It results in the following labelling of the RDs in RD-list: RDs that cart overcome all negative effects are labelled with the symbol Jail] (the only RDs that may be labelled in this manner axe Assertions and Negations); RDs that cannot convey an intended proposition by themselves are labelled with the symbol [-] ; RDs that can convey an intended proposition, but cannot overcome any negative effects are labelled with [none]; and the remaining RDs are labelled with the negative RDs they can overcome. We then use a search procedure to generate all the sets of RDs which consist of one RD from each RD.list. The sets of RDs that convey all the intended propositions are then stored in a list called SUCCEED; and the sets of RDs that fall to convey one or more propositions axe stored in a list called FAILED, together with the proposition(s) that failed to be conveyed. Additional minimally sufficient sets of RDs axe then generated from FAILED as follows: we select a proposition pi that was not conveyed, and create pairs of RDs composed of the RD that failed to convey pi and each of the other RDs in RD.list(pl) that is not labelled Jail] (the RDs that are labelled Jail] can convey pi by themselves, and therefore there is no need to combine them with other RDs). Each pair of RDs inherits the negative RDs that can be overcome by its paxents, and may be able to overcome additional negative RDs which caused its parents to fail separately. For each pair of RDs, a new set of RDs is created by replacing the RD which failed to convey pi with this pair of RDs. The search is then continued for each of these new sets of RDs until a minimally sufficient set of RDs is generated or failure occurs again. In this case, the process of generating pairs of RDs is repeated, and the seaxch is resumed. If a pair of RDs fails, then it forms the basis for triplets, and so on. The search stops when the RD.list of a proposition which failed to be conveyed contains no RDs with which the failed RDs (or RD-tuples) can be combined. Algorithm Construct.sets-of-RDs( {p}, RD-list) 1. Initialize two lists, SUCCEED and FAILED, to empty. Consider the combined effect of several RDs to determine whether a set of RDs conveys completely a set of propositions. Determine-I~Ds( nil, {p } , nil). 5. Append the successful combinations of RDs to SUCCEED, and remove any sets of RDs in SUCCEED that subsume other sets of RDs. 6. Append the failed combinations of RDs to FAILED together with the reason for the failure, i.e., the RDs that failed and the propositions that were not conveyed. 7. If FAILED is empty, then exit. 8. Assign to CURRD the first set of RDs in FAILED, and remove it from FAILED. 9. Select from CURRD a proposition pi that was not conveyed, and generate successors of CURRD as follows: I(pl) [none] A(p2) [-] P2 To illustrate this process, let us reconsider the example discussed in Section 3.2.1. Table 2 contains the RD-lists for the propositions in this example, where each RD is labelled according to the RDs whose negative effect it can overcome. For instance, Negate(p3) can overcome the combined negative effect of Assert(p~) and Instantiate(pz), and also the effect of Assert(p2). However, it cannot overcome the combined effect of Assert(p1) and Assert(p2), or Assert(p2) and Instantiate(p1). Instantiate(px) can convey proposition pl, but cannot overcome any negative effects. Assert(p2) contributes to the belief in pl but cannot convey it alone. Figure 2 contains part of the search tree generated in Step 4 of algorithm Construct-sets-o/-RDs. Each path in this tree contains one RD from each row in Table 2 . Successful paths axe drawn with thick lines and are marked S. Failed paths are marked F accompanied by the propositions which were not conveyed by the RDs in these paths. An RD that increases a user's belief in more than one proposition may appear in a path more than once. The repeated instances of such an RD appear in brackets, e.g., {Assert(px)}, indicating that the RD will be mentioned only once. In the successful path to the left, Assert(p1) can overcome all negative effects to convey p~. In addition, it can overcome the negative effect of Instantiate(-,p3) to convey p2, and Instantiate(-,p3) can overcome the negative effect of Assert(p1 ) to convey \",p3. A(pl) A( p~ {Pl,'~P3} In the successful path to the right, Assert(p1) together with Instantiate(pl) overcome the negative effect of Negate(p3) to convey p2, even though neither could do so by itself; and Negate(p3) can overcome the joint effect of Assert(p1 ) and Instantlate(pl ). Table 3 contains the successful minimally sufficient sets of RDs generated by this search and the failed sets of RDs accompanied by the propositions that were not conveyed. In Step 9 of Construct-sets-o/-RDs, the RDs that failed to convey a proposition axe combined with other RDs that can increase the user's belief in this proposition. For instance, Negate(p3) is combined with Instantlate(-~p3) for all the paths where -~p3 failed to be conveyed, and the seaxch is continued. Our procedure does not generate children from repeated RDs that failed to convey a proposition, since this would yield already existing combinations of RDs. Table 4 contains the minimally sufficient sets of RDs returned by algorithm Gonstruet-sets-ofiRDs. Set 5-6 is obtained by com.plementing Set 5 in Table 3 with the RD Instantiate(-~p3), and also by complementing Set 6 with the RD Negate(p3). Additional successful sets of RDs are generated by appending complen~entaxy RDs to the failed sets of RDs in Table 3 . However, these sets subsume Set I, 2 and 5-6, and hence axe not minimally sufficient. For example, when Set 4 in Table 3 is complemented with Instantiate(pl), it yields a set of RDs that is equal to Set 2. This set is removed in Step 5 of Gonstruct-sets-of-RDs. This process ensures that only minimally sufficient sets of RDs are generated, because it generates RD-tuples only from the unsuccessful RDs in the RD-list of a proposition, and it prunes sets of RDs that subsume other sets of RDs. In addition, this process ensures that all the minimally sufficient sets of RDs axe generated, because it considers all the RDtuples resulting from the unsuccessful RDs in the RD.list of a proposition. Determining Prerequisite Propositions The prerequisite propositions to be conveyed depend on the user's expertise with respect to the concepts mentioned in a set of RDs, and on the context where these concepts are mentioned. The context influences both the aspects of these concepts that must be understood by the user and the extent to which these aspects must be understood. The process of determining the relevant aspects of a concept and the required level of expertise is described in [Zukerman and McConachy, 1993a] . The relevant aspects of a concept are determined by considering the predicates of the propositions where a concept is mentioned, and the role of the concept in these propositions. For example, in order to understand the RD Assert[Marsupial has-part pouch], the user must know the aspects type and structure of a pouch, i.e., what it is and what it looks like. The extent to which a user must know the selected aspects of a concept depends on the relevance of this concept to the original propositions to be conveyed, i.e., the system demands a high level of expertise with respect to the more relevant concepts, and a lower level of expertise with respect to the less relevant ones. After the relevant aspects and required level of expertise of each concept have been determined, WISHFUL-II applies the content selection step described in Section 2 to determine the prerequisite propositions of each concept. WISHFUL-II then merges into a single set the prerequisite propositions generated for individual concepts. This merger is executed because some prerequisite propositions of two or more concepts may be conveyed by a single RD. A special case of this happens when two or more concepts have common prerequisite propositions. For example, consider the situation depicted in Figure 3 , where prerequisite information for the set of RDs {RD1,RD2} is being conveyed. RD1 requires the prerequisite propositions {pl,p2}, while RD2 requires the prerequisite propositions {p2, p3, p4 }. If we considered separately the prerequisites of these RDs, we would generate RDs to convey {pl,p2}, and {RD4,RDs} to convey {p2,pa,p4}. This would result in a total of three RDs. However, by considering jointly all the prerequisite propositions of {RDi, RD2}, we will require two RDs only, namely {RD3, RDs}. Evoking the Concepts in a Set of RDs RDs that convey referring information differ from RDs that convey prerequisite propositions in that the former identifies a concept by means of information known to the user, while the latter conveys information that the user does not know about a concept. Further, the process of generating referring information has the flexibility of selecting the propositions that can identify a concept uniquely, while the propositions   Comp4 : expressions such as 2(3z-b 49) and 5(2z -1-3z) that convey prerequisite information are dictated by the context and by the user's expertise. In order to generate referring expressions for the concepts mentioned in a set of RDs, we propose for each concept a list of candidate lexical items that can be used to refer to it. If there is a lexical item that identifies each concept uniquely and is known to the user, the evocation process is finished. However, if there axe concepts that are not identified uniquely by any of their candidate lexical items, then these lexical items axe complemented with additional RDs that help them identify the intended concepts. This task is performed by iteratively selecting propositions that identify an intended concept until this concept is singled out, and generating RDs that convey these propositions. This algorithm differs from the procedure described in [Dale, 1990] in that we generate several alternative sets of complementing RDs in order to avoid dead-end situations where the only identifying information that is generated for a set of concepts is circular. The evocation process then selects the most concise non-circulax combination of referring expressions that identifies all the concepts in a set of RDs. For example, Table 5 illustrates candidate referring expressions generated for the concepts Like-Terms and Algebraic-Terms. Each referring expression is composed of a lexical item and a complement 3 . The noncircular alternatives in this example contain the complements {Co~p~,Comp,}, {co~p~,Co~p~} and {Co~p~,Co~p,} Two Optimization Criteria As indicated in Section 3.1, the optimization criterion determines the manner in which the nodes in OPEN are ranked and pruned. In our implementation we have tried two optimization criteria: (1) conciseness and (2) depth. Optimizing the Depth of the Generated RDs When optimizing the depth of a set of RDs, the nodes in OPEN are pruned according to the following rule: IF Depth(hi) = Depth(n j) AND { Prerequisites-of(n/) D Prerequisites-of(n j) OR { Prerequisites-of(n/) = Prerequisites-of(hi) AND { IReferring-exp-of(ni)l > IReferring-exp-of(nj)[ OR { IReferring-exp-of(n~)l = IReferring-exp-of(n~)l AND Total-Weight(n/) > Total-Weight(hi) } } } } THEN remove hi. 3 A referring expression may contain a null lexical item, i.e., complementing information only. However, at present this option is not generated by WISHFUL-II. Psi The tveight of a node reflects the number of RDs in this node and their type. The total weight of a node is the sum of the weights of the nodes in the path from the root of the search tree to this node. All the RDs have a weight of 1, except an Instantiation of a proposition p that accompanies an Assertion of p or a Negation of ~p. Such an Instantiation has a weight of 1 ~, because it does not contain new information, rather it is a continuation of the idea presented in the Assertion or the Negation. For example, the Instantiation in Set 1 in Table 6 has a weight of 1, because the instantiated proposition is different from the asserted proposition. In contrust, in Set 2, the weight of the Instantiation is \u00bd because it instantiates the asserted proposition. The above rule is also applied after Step 4 of algorithm Expand-sets-afiRDs to prune the list of minimally sufficient sets of 1RDs (Section 3.2). It removes a node if its prerequisites subsume those of another node. It considers the number of referring expressions of a node only when the same prerequisite propositions are required by two nodes, and considers the total weight of a node only when two nodes have the same prerequisite propositions and the same number of referring expressions. This rule compares only nodes at the same depth, because even if the prerequisites of a node at level i subsume the prerequisites of a node at level i -t-1, the node at level i may lead to discourse that has depth i q-1, while the node at level i \u00f7 1 can lead to discourse of depth i -t-2 at best. To illustrate the pruning process let us reconsider the minimally sufficient sets of RDs in Table 4 , assuming that the prerequisite propositions required by these sets are as shown in Table 64 . Here, the pruning rule removes Set 2, since its prerequisite propositions subsume those of Set 1. The nodes remaining in OPEN are ordered as follows: 1. In increasing order of their depth, so that we expand the more shallow nodes first during the optimization process. 2. In increasing order of the number of prerequisite propositions they require, so that the nodes that contain the sets of RDs with the fewest prerequisites are preferred among the nodes at the same level. 3. In increasing order of the number of referring expressions they have, so that the nodes with the fewest referring expressions are preferred among the nodes with the same number of prerequisite propositions. 4. In increasing order of their total weight, so that the most concise set of RDs is preferred when all else is equal. 4 The first coefficient of each prerequisite proposition indicates the RD for which it is required, e.g., pit is a prerequisite of Assert(pl). To illustrate this process let us consider the minimally sufficient sets of RDs that remain after pruning, namely Set 1 and Set 5-6, and assign them to nodes nl and ns-e respectively. Since Set 5-6 has the fewest prerequisite propositions, us-6 will precede nl in OPEN, and will be the next node to be expanded by algorithm Optimize.RDs (Section 3.1). If upon expansion of ns-s we find that there is a minimally sufficient set of RDs that conveys propositions {p21 ,psi } and requires no prerequisite information, then the node which contains this set of RDs is a goal node, and the search is finished. Optimizing the Number of Generated RDs When optimizing the total number of RDs to be presented, the following rule is used to prune the nodes in OPEN: IF Total-Weight(hi) > Total-Weight(hi) AND Prerequisites-of(nl) D Prerequisites-of(n./) THEN remove ni. As in depth optimization, this rule is also applied after Step 4 of algorithm Ea:pand-sets-of-RDs to prune the list of minimally sufficient sets of RDs. The nodes remaining in OPEN are sorted in increasing order of their total weight. To illustrate this process let us consider once more the minimally sufficient sets of RDs in Table 6 . Since the prerequisite propositions of Set 2 subsume those of Set 1, and the total weight of Set 2 is higher than that of Set 1, Set 2 is removed in the pruning stage. The ordering of the remaining nodes in OPEN is different from the ordering obtained for the depth optimization, i.e., nl precedes ns-e in OPEN, since the total weight of Set 1 is less than the total weight of Set 5-6. Results WISHFUL-II was implemented using Common Lisp on a SPARCstation 2 and on a PC-486. The system takes less than 4 seconds of CPU time to produce English output, and the optimization process alone takes less than 2 seconds for discourse of up to 10 RDs. Table 1 in Section 1 and Table 7 (adapted from an example in [Moore and Swartout, 1989] ) illustrate the output generated by WISHFUL-II for the two optimization criteria we have implemented, viz conciseness and depth. Appendix A contains examples of the output produced by WISHFUL-II when the same discourse is generated for the concise and the shallow optimization criteria. Our mechanism can be used as a tool for evaluating different discourse optimization criteria, where the only requirement for implementing a new criterion is the modification of the pruning and ranking rules described in Section 3.3. When WISHFUL-II was tried with the two optimization criteria described in this paper, it often generated the same discourse with both criteria, i.e., the most concise discourse was also the shallowest. However, the two optimization criteria produced different discourse when the most concise discourse mentioned one or more concepts that were not known to the user and therefore had to be explained, while the shallowest discourse avoided these explanations by presenting a larger in x-->'(a b). number of RDs which mentioned different concepts. In particular, the concise discourse was characterized by the presence of Similes that required some in-depth clarification of a non-source concept s, while the shallow discourse was chaxacterized by the presence of a Description composed of a list of Assertions possibly accompanied by Instantiations. Conclusion In this paper, we have cast discourse planning as an optimization process which generates discourse that satisfies a specific optimization criterion. We have described a weak search procedure that implements this process while taking into consideration the following factors: (1) a user's inferences from proposed RDs, (2) the prerequisite information required by the user to understand the concepts mentioned in a set of RDs, and (3) the referring expressions required to enable the user to identify these concepts. Two optimization criteria have been considered, viz conciseness and depth. The system which implements these ideas has been used to generate descriptive discourse in various technical domains. Appendix A: Sample Output Table 8 . 'Racing car' Discourse An indycar is an American racing car. It has a very poeerful engine, wide tires, huge brakes and big wings to make lots of dowuforce. Lots of downforce helps it go around corners quickly, however lots of dognforce does not help it go straight quickly. A formula I car is like an indycar, however a formula I car is a European racin E car. Table 9. 'DOS and UNIX' Discourse DOS is an operating system. It has a command line interface, which is an interface where you type commands at a text prompt, e.g., mkdir, Is. It is a single user operating system and it does not allow multitasking, which is doing more than one job at a time. UNIX is like DOS, however it is a multiuser operating system and does allow multitasking. Some UNIX commands are the same as DOS, e.g., mkdir, however some are different, e.g., pwd. DOS runs on PC compatibles. In addition to PC compatibles UNIX runs on workstations. Table 10 . 'Document layout' Discourse TEX is a layout language for documents. A layout language allows you to control the appearance of of your document, e.g., text size and placement. It uses embedded commands, ehich are commands placed within the document, e.g., ~ageno. These commands are executed after your document is edited. Troll is like TF~, however it has different commands, e.g., .BP. A wordprocessor also allows you to control the appearance of your document, however it is not a layout language. Mordprocessors also use embedded commands, however they are not executed after your document is edited, they are executed while your document is edited. WordPerfect is a vordprocessor.",
    "abstract": "Discourse planning systems developed to date apply local considerations in order to generate an initial presentation that achieves a given communicative goal. However, they lack a global criterion for selecting among alternative presentations. In this paper, we cast the problem of planning discourse as an optimization problem, which allows the definition of a global optimization criterion. In particular, we consider two such criteria: (1) generating the most concise discourse, and (2) generating the 'shallowest' discourse, i.e., discourse that requires the least prerequisite information. These criteria are embodied in a discourse planning mechanism which considers the following factors: (1) the effect of a user's inferences from planned utterances on his/her beliefs, (2) the amount of prerequisite information a user requires to understand an utterance, and (3) the amount of information that must be included in referring expressions which identify the concepts mentioned in an utterance. This mechanism is part of a discourse planning system called WISHFUL-II which generates explanations about concepts in technical domains.",
    "countries": [
        "Australia"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "3",
    "year": "1994",
    "month": "",
    "title": "Discourse Planning as an Optimization Process"
}