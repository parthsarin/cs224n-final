{
    "article": "Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset. Introduction Question answering (QA) is a task of answering questions given a passage. Large-scale QA datasets have attracted many researchers to build effective QA models, and with the advent of deep learning, recent QA models are known to outperform humans in some datasets (Rajpurkar et al., 2016; Devlin et al., 2019; Yang et al., 2019) . Extractive QA is the task that assumes that answers always lie in the passage. Based on this task assumption, various QA models are trained to predict the start and end positions as the answers. Following the \u2020 Corresponding authors structure of earlier deep learning-based QA models (Wang and Jiang, 2016; Seo et al., 2017; Xiong et al., 2017) , recent QA models provide positions of answers without much consideration (Yu et al., 2018; Devlin et al., 2019; Yang et al., 2019) . The popularity of predicting the answer positions is credited to the fact that it reduces the prediction space to O(n) where n is the length of an input document. It is more efficient and effective than directly generating answers from a large vocabulary space. Furthermore, it reduces the QA task to a classification task which is convenient to model. Nevertheless, very few studies have discussed the side effects of predicting the answer positions. Could there be any unwanted biases when using answer positions as prediction targets? In this paper, we demonstrate that the models predicting the position can be severely biased when trained on datasets that have a very skewed answer position distribution. We define this as position bias as shown in Figure 1 . Models trained on a biased dataset where answers always lie in the same  21.44 27.92 -48.54 31.20 37.48 -51.15 38.59 45.27 -43.97  SQuAD k=1  train + First Sentence 53.16 63. 21 -13.25 72.75 81.18 -7.45 74.85 82.84 -6.40  SQuAD k=1  train + Sentence Shuffle 54.40 65.20 -11.26 73.37 81.90 -6.73 77.83 86.18 -3.06 Table 1 : Performance of QA models trained on the biased SQuAD dataset (SQuAD k=1 train ), and tested on SQuAD dev . \u2206 denotes the difference in F1 score with SQuAD train . We use exact match (EM) and F1 score for evaluation. 1 sentence position mostly give predictions on the corresponding sentence. As a result, BERT (Devlin et al., 2019) trained on a biased training set where every answer appear in the first sentence only achieves 37.48% F1 score in the SQuAD development set whereas the same model trained on the same amount of randomly sampled examples achieves 85.06% F1 score. To examine the cause of the problem, we thoroughly analyze the learning process of QA models trained on the biased training sets, especially focusing on BERT. Our analysis shows that hidden representations of BERT preserve a different amount of word information according to the word position when trained on the biased training set. The predictions of biased models also become more dependent on the first few words when the training set has answers only in the first sentences. To tackle the problem, we test various options, ranging from relative position encodings (Yang et al., 2019) to ensemble-based de-biasing methods (Clark et al., 2019; He et al., 2019) . While simple baselines motivated by our analysis improve the test performance, our ensemble-based de-biasing method largely improves the performance of most models. Specifically, we use the prior distribution of answer positions as an additional bias model and train models to learn reasoning ability beyond the positional cues. Contributions of our paper are in threefold; First, we define position bias in extractive question answering and illustrate that common extractive QA models suffer from it. Second, we examine the reason for the failure of the biased models and show that positions can act as spurious biases. Third, we show that the prior distribution of answer positions helps us to build positionally de-biased models, recovering the performance of BERT from 37.48% to 81.64%. We also generalize our findings in many different positions and datasets. 2 2 https://github.com/dmis-lab/position-bias Analysis We first demonstrate the presence of position bias using biased training sets sampled from SQuAD (Rajpurkar et al., 2016) and visualize how position bias propagates in BERT. Position Bias on Synthetic Datasets From the original training set D train , we subsample a biased training set D k train whose answers lie in the k-th sentence. 3 We conduct experiments on SQuAD (D = SQuAD) as most examples in SQuAD are answerable with a single sentence (Min et al., 2018) . Our analysis mainly focuses on SQuAD k=1 train (i.e., all answers are in the first sentence), which has the largest proportion of samples compared to other sentence positions in SQuAD (28,263 out of 87,599). The proportion in the development set (SQuAD dev ) is similar, having 3,637 out of 10,570 answers in the first sentence. Note that while our analysis is based on SQuAD k=1 train , we also test various sentence positions in our main experiments (Section 4.2). We experiment with three popular QA models that provide positions as answers: BiDAF (Seo et al., 2017) , BERT (Devlin et al., 2019) , and XLNet (Yang et al., 2019) . All three models are trained on SQuAD k=1 train and are evaluated on SQuAD dev . For a fair comparison, we also randomly sample examples from the original training set and make SQuAD train (Sampled) which has the same number of examples with SQuAD k=1 train . Table 1 shows the performance of the three models trained on SQuAD k=1 train . The performances of all models drop significantly compared to the models trained on SQuAD train or SQuAD train (Sampled). The relative position encodings in XLNet mitigate position bias to some extent, but its performance still degrades significantly. To better understand the cause of position bias, we additionally perform two pre-processing methods on SQuAD k=1 train . First, we truncate each passage up to the first sentence (SQuAD k=1 train + First Sentence). In this case, most performance is recovered, which indicates that the distributions of answer positions are relatively defined with respect to the maximum sequence length. Shuffling the sentence order of SQuAD k=1 train (SQuAD k=1 train + Sentence Shuffle) also recovers most performance, showing that the spreadness of answers matters. However, these pre-processing methods cannot be a solution as more fine-grained biases (e.g., word level positions) could cause the problem again and models cannot learn proper multi-sentence reasoning from a corrupted context. Visualization of Position Bias To visualize how position bias propagates throughout the layers, we compare BERT models, each trained on SQuAD k=1 train and SQuAD train respectively and BERT without any fine-tuning. The uncased version of BERT-base is used for the analysis. Figure 2 (a) shows the amount of word information preserved in the hidden representations at the last layer of BERT. We define the amount of word information for each word position as the cosine similarity between the word embedding and its hidden representation at each layer. The similarities are averaged over the passage-side hidden representations in SQuAD dev . BERT trained on SQuAD k=1 train (FIRST) has higher similarities at the front of the passages compared with BERT trained on SQuAD train (ORIG). In the biased model, the similarity becomes smaller after the first few tokens, which shows position bias of BERT. Figure 2 (b) shows the Spearman's rank correlation coefficient between the final output logits 4 and the amount of word information at each layer defined by the cosine similarity. A higher correlation means that the model is more dependent on the word information kept in that layer. The correlation coefficient is much higher in the biased model (FIRST), especially in the last few layers. Combined with the observation from Figure 2 (a), this indicates that the predictions of the biased model are heavily relying on the information of the first few words. Why is Position Bias Bad? Our analysis shows that it is very easy for neural QA models to exploit positional cues whenever possible. While it is natural for neural models to learn the strong but spurious correlation present in the dataset (McCoy et al., 2019; Niven and Kao, 2019) , we argue that reading ability should be cultivated independent of such positional correlation. Our study aims to learn proper reading ability even in extreme cases where all answers are in the k-th sentence. Although exploiting the position distribution within the dataset could help the model improve performance on its corresponding test set, position bias should not be learned since we cannot guarantee realistic test environments to follow similar distribution. Method To prevent models from learning a direct correlation between word positions and answers, we introduce simple remedies for BERT and a bias ensemble method with answer prior distributions that can be applied to any QA models. Baselines Randomized Position To avoid learning the direct correlation between word positions and answers, we randomly perturb input positions. We first randomly sample t indices from a range of 1 to maximum sequence length of BERT. We sample t = 384 when the maximum sequence length is 512. Then, we sort the indices in an ascending order to preserve the ordering of input words. Perturbed indices then generate position embedding at each token position, which replaces the original position embedding. Entropy Regularization Inspired by the observation in Section 2.2, we force our model to preserve a constant amount of word information regardless of the word positions. Maximizing the entropy of normalized cosine similarity between the word embeddings and their hidden representations encourages models to maintain a uniform amount of information. As the cosine similarities are not probabilities, we normalize them to be summed to 1. We compute the entropy regularization term from the last layer and add it to the start/end prediction loss with a scaling factor \u03bb. Bias Ensemble with Answer Prior Bias ensemble methods (Clark et al., 2019; He et al., 2019) combine the log probabilities from a pre-defined bias model and a target model to debias. Ensembling makes the target model to learn different probabilities other than the bias probabilities. In our case, we define the prior distribution of the answer positions as our bias model. Specifically, we introduce the sentence-level answer prior and the word-level answer prior. Bias Ensemble Method Given a passage and question pair, a model has to find the optimal start and end positions of the answer in the passage, denoted as y s , y e . Typically, the model outputs two probability distributions p s and p e for the start and end positions. As our method is applied in the same manner for both start and end predictions, we drop the superscript from p s , p e and subscript from y s , y e whenever possible. For ensembling two different log probabilities from the bias model and the target model, we use a product of experts (Hinton, 2002) . Using the product of experts, a probability at the i-th position is calculated as: pi = sof tmax(log(p i ) + log(b i )) (1) where log(p i ) is a log probability from the target model and log(b i ) is a log probability from the bias model. The ensembled probability p is used for the training. To dynamically choose the amount of bias for each sample, Clark et al. (2019) introduce a learned mixing ensemble with a trainable parameter. Probabilities in the training phase are now defined as: pi = sof tmax(log(p i ) + g(X) log(b i )) (2) We use hidden representations before the softmax layer as X. g(X) then applies affine transformation on the representations to obtain a scalar value. Softplus activation followed by max pooling is used to obtain positive values. As BiDAF has separate hidden representations for the start and end predictions, we separately define g(X) for each start and end representation. As models often learn to ignore the biases and make g(X) to 0, Clark et al. (2019) suggest adding an entropy penalty term to the loss function. However, the entropy penalty did not make much difference in our case as g(X) was already large enough. Note that we only use log(b i ) during training, and the predictions are solely based on the predicted log probability log(p i ) from the model. We define bias log probability as pre-calculated answer priors. Using prior distributions in machine learning has a long history such as using class frequency in the class imbalance problem (Domingos, 1999; Japkowicz and Stephen, 2002; Zhou and Liu, 2006; Huang et al., 2016) . In our case, the class prior corresponds to the prior distribution of answer positions. Word-level Answer Prior First, we consider the word-level answer prior. Given the training set having N examples having N answers {y (1) , y (2) , ..., y (N ) }, we compute the word-level answer prior at position i over the training set. In this case, our bias log probability at i-th position is: log(b i ) := 1 N N j=1 1[y (j) = i] (3) where we use the indicator function 1 [cond] . Bias log probabilities for the end position prediction are calculated in a similar manner. Note that the wordlevel answer prior gives an equal bias distribution for each passage while the distribution is more finegrained than the sentence-level prior described in the next section. Sentence-level Answer Prior We also use the sentence-level answer prior which dynamically changes depending on the sentence boundaries of each sample. First, we define a set of sentences {S (j) 1 , ..., S (j) L } for the j-th training passage, where L is the maximum number of sentence in whole training passages. Then, the sentence-level answer prior of the i-th word position (for the start prediction) for the j-th sample, is derived from the frequency of answers appearing in the l-th sentence: log(b (j) i ) := 1 N N k=1 1[y (k) \u2208 S (k) l ], i \u2208 S (j) l (4) Note that as boundaries of sentences in each sample are different, bias log probabilities should be defined in every sample. Again, bias log probabilities for the end positions are calculated similarly. It is very convenient to calculate the answer priors for any datasets. For instance, on D k=1 train , we use the first sentence indicator as the sentence-level answer prior as all answers are in the first sentence. More formally, the sentence-level answer prior for D k=1 train is 1 for l = 1, and 0 when l > 1: log(b (j) i ) := 1 i \u2208 S (j) 1 , 0 i / \u2208 S (j) 1 (5) which is a special case of the sentence-level answer prior. For general datasets where the distributions of answer positions are less skewed, the answer priors are more softly distributed. See Appendix B for a better understanding of the answer priors. Both word-level and sentence-level answer priors are experimented with two bias ensemble methods: product of experts with bias (Bias Product, Equation 1) and learned mixing of two log probabilities (Learned-Mixin, Equation 2 ). Experiments We first experiment the effects of various debiasing methods on three different QA models using both biased and full training sets. Our next experiments generalize our findings in different sentence positions and different datasets such as NewsQA (Trischler et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019) . Effect of De-biasing Methods We first train all three models (BiDAF, BERT, and XLNet) on SQuAD k=1 train with our de-biasing methods and evaluate them on SQuAD dev (original development set), SQuAD k=1 dev , and SQuAD k=2,3,... dev . Note that SQuAD k=2,3,... dev is another subset of SQuAD dev , whose answers do not appear in the first sentence, but in other sentences. We also experiment with BERT trained on the full training set, SQuAD train . For all models, we use the same hyperparameters and training procedures as suggested in their original papers (Seo et al., 2017; Devlin et al., 2019; Yang et al., 2019) , except for batch sizes and training epochs (See Appendix A). \u03bb for the entropy regularization is set to 5. Most of our implementation is based on the PyTorch library. Results with SQuAD k=1 train The results of applying various de-biasing methods on three models with SQuAD k=1 train are in Table 2 . Performance of all models without any de-biasing methods (denoted as 'None') is very low on SQuAD k=2,3,... dev , but fairly high on SQuAD k=1 dev . This means that their predictions are highly biased towards the first sentences. In the case of BERT, F1 score on SQuAD k=1 dev is 85.81%, while F1 score on SQuAD k=2,3,... dev is merely 12.12%. Our simple baseline approaches used in BERT improve the performance up to 34.63% F1 score (Random Position) while the entropy regularization is not significantly effective. Bias ensemble methods using answer priors consistently improve the performance of all models. The sentence-level answer prior works the best, which obtains a significant gain after applying the Learned-Mixin method. We found that the coefficient g(X) in Equation 2 averages to 7.42. during training for BERT + Learned-Mixin, which demonstrates a need of proper balancing between the probabilities. The word-level answer prior does not seem to provide strong position bias signals as its distribution is much softer than the sentencelevel answer prior. Results with SQuAD train The results of training BERT with our de-biasing methods on the full training set SQuAD train are in the bottom of Table 2 . Note that the answer prior is more softened than the answer prior used in SQuAD could be more helpful when evaluating on the development set that has a similar positional distribution, our method maintains the original performance. It shows that our method works safely when the positional distribution doesn't change much. Visualization To investigate the effect of debaising methods, we visualize the word information in each layer as done in Section 2.2. We visualize the BERT trained on SQuAD k=1 train ensembled with sentence-level answer prior in Figure 3 . The bias product method (PRODUCT) and the model without any de-biasing methods (NONE) are similar, showing that it still has position bias. The learned-mixin method (MIXIN), on the other hand, safely delivers the word information across different positions. Generalizing to Different Positions As the SQuAD training set has many answers in the first sentence, we mainly test our methods on SQuAD k=1 train . However, does our method gener- alize to different sentence positions? To answer this question, we construct four SQuAD k train datasets based on the sentence positions of answers. Note that unlike SQuAD k=1 train , the number of samples becomes smaller and the sentence boundaries are more blurry when k > 1, making answer priors much softer. We train three QA models on different biased datasets and evaluate them on SQuAD dev with and without de-biasing methods.   Results As shown in Table 3 , all three models suffer from position bias in every sentence position while the learned-mixin method (+Learned-Mixin) successfully resolves the bias. Due to the blurred sentence boundaries, position bias is less problematic when k is large. We observe a similar trend in BERT and XLNet while a huge performance drop is observed in BiDAF even with a large k. SQuAD dev EM F1 EM F1 EM F1 EM F1 SQuAD k train k = 2 k = 3 k = 4 k = 5, Visualization Figure 4 visualizes the sentencewise position biases. We train BERT, BERT + Bias Product and BERT + Learned Mixin on different subsets of SQuAD training set (SQuAD k train ) and evaluated on every SQuAD k dev whose answers lie only in the k-th sentence. As a result, the low performance in the off-diagonal represent the presence of position bias. The figure shows that the biased model fails to predict the answers in different sentence positions (Figure 4 (a)) while our de-biased model achieves high performance regardless of the sentence position (Figure 4 (c) ). Again, as the value of k increases, the boundary of the k-th sentence varies a lot in each sample, which makes the visualization of sentence-wise bias difficult. NewsQA and NaturalQuestions We test the effect of de-basing methods on datasets having different domains and different degrees of position bias. NewsQA (Trischler et al., 2017) is an extractive QA dataset that includes passages from CNN news articles. NaturalQuestions (Kwiatkowski et al., 2019) is a dataset containing queries and passages collected from the Google search engine. We use the pre-processed dataset provided by the MRQA shared task (Fisch et al., 2019) . 5  For each dataset, we construct two sub-training datasets; one contains samples with answers in the first sentence (k = 1), and the other contains the remaining samples (k = 2, 3, ...). Models are trained on the original dataset and two sub-training datasets and evaluated on the original development set. NewsQA k train NewsQA dev k = All k = 1 k = 2, Implementation Details For NewsQA, we truncate each paragraph so that the length of each context is less than 300 words. We eliminate training and development samples that become unanswerable due to the truncation. For NaturalQuestions, we choose firstly occurring answers for training extractive QA models, which is a common approach in weakly supervised setting (Joshi et al., 2017; Talmor and Berant, 2019) . From NewsQA and NaturalQuestions, we construct two sub-training datasets having only the first annotated samples (D k=1 train ) and the remaining samples (D k=2,3,...   train   ). For a fair comparison, we fix the size of two sub-training sets to have 17,000 (NewsQA) and 40,000 samples (Natu-ralQuestions). Results In Table 4 and Table 5 , we show results of applying our methods. In both datasets, BERT, trained on biased datasets (k = 1 and k = 2, 3, ...), significantly suffers from position bias. Position bias is generally more problematic in the k = 1 datasets while for NaturalQuestions, k = 2, 3, ... is also problematic. Our de-biasing methods prevent performance drops in all cases without sacrificing the performance on the full training set (k = All). Related Work Various question answering datasets have been introduced with diverse challenges including reasoning over multiple sentences (Joshi et al., 2017) , answering multi-hop questions (Yang et al., 2018) , and more (Trischler et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Dua et al., 2019) . Introduction of these datasets rapidly progressed the development of effective QA models (Wang and Jiang, 2016; Seo et al., 2017; Xiong et al., 2017; Wang et al., 2017; Yu et al., 2018; Devlin et al., 2019; Yang et al., 2019) , but most models predict the answer as positions without much discussion on it. Our work builds on the analyses of dataset biases in machine learning models and ways to tackle them. For instance, sentence classification models in natural language inference and argument reasoning comprehension suffer from word statistics bias (Poliak et al., 2018; Minervini and Riedel, 2018; Kang et al., 2018; Belinkov et al., 2019; Niven and Kao, 2019) . On visual question answering, models often ignore visual information due to the language prior bias (Agrawal et al., 2016; Zhang et al., 2016; Goyal et al., 2017; Johnson et al., 2017; Agrawal et al., 2018) . Several studies in QA also found that QA models do not leverages the full information in the given passage (Chen et al., 2016; Min et al., 2018; Chen and Durrett, 2019; Min et al., 2019) . Adversarial datasets have been also proposed to deal with this type of problem (Jia and Liang, 2017; Rajpurkar et al., 2018) . In this study, we define position bias coming from the prediction structure of QA models and show that positionally biased models can ignore information in different positions. Our proposed methods are based on the bias ensemble method (Clark et al., 2019; He et al., 2019) . Ensembling with the bias model encourages the model to solve tasks without converging to bias shortcuts. Clark et al. (2019) conducted de-biasing experiments on various tasks including two QA tasks while they use tf-idf and the named entities as the bias models. It is worth noting that several models incorporate the pointer network to predict the answer positions in QA (Vinyals et al., 2015; Wang and Jiang, 2016; Wang et al., 2017) . Also, instead of predicting positions, some models predict the n-grams as answers (Lee et al., 2016; Seo et al., 2019) , generate answers in a vocabulary space (Raffel et al., 2019) , or use a generative model (Lewis and Fan, 2019) . We expect that these approaches suffer less from position bias and leave the evaluation of position bias in these models as our future work. Conclusion Most QA studies frequently utilize start and end positions of answers as training targets without much considerations. Our study shows that most QA models fail to generalize over different positions when trained on datasets having answers in a specific position. Our findings show that position can work as a spurious bias and alert researchers when building QA models and datasets. We introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful. Our findings also generalize to different positions and different datasets. One limitation of our approach is that our method and analysis are based on a single paragraph setting which should be extended to a multiple paragraph setting to be more practically useful. A Implementation Details Details of Training For all experiments, we use uncased BERT-base and cased XLNet-base. We modify the open-sourced Pytorch implementation of models. 6 BiDAF is trained with the batch size of 64 for 30 epochs and BERT and XLNet are trained for 2 epochs with batch sizes 12 and 10, respectively. The choice of hyperparameters mainly comes from the limitation of our computational resources and mostly follows the default setting used in their original works. Note that our de-biasing methods do not require additional hyperparameters. For all three models, the number of parameters remains the same as default settings with bias product and increases by a single linear layer with learned-mixin. We trained models on a single Titan X GPU. The average training time of the bias ensemble method is similar to the original models. B Examples of Answer Prior To provide a better understanding of our methods,  \ud835\udcd3 train : [0.15, 0.10, 0.12, 0.10, 0.05, 0.08, 0.05, 0.03, 0.02, 0.04 Acknowledgments This research was supported by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HR20C0021). This research was also supported by National Research Foundation of Korea (NRF-2017R1A2A1A17069 645, NRF-2017M3C4A7065887). train without any de-biasing methods (NONE), with sentence-level prior bias product (PRODUCT), with learned-mixin (MIXIN). MIXIN preserves consistent information compared with NONE and prevents the bias propagation.",
    "abstract": "Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.",
    "countries": [
        "South Korea"
    ],
    "languages": [],
    "numcitedby": "26",
    "year": "2020",
    "month": "November",
    "title": "Look at the First Sentence: Position Bias in Question Answering"
}