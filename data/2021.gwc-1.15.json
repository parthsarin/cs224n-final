{
    "article": "The vast majority of the existing approaches for taxonomy enrichment apply word embeddings as they have proven to accumulate contexts (in a broad sense) extracted from texts which are sufficient for attaching orphan words to the taxonomy. On the other hand, apart from being large lexical and semantic resources, taxonomies are graph structures. Combining word embeddings with graph structure of taxonomy could be of use for predicting taxonomic relations. In this paper we compare several approaches for attaching new words to the existing taxonomy which are based on the graph representations with the one that relies on fastText embeddings. We test all methods on Russian and English datasets, but they could be also applied to other wordnets and languages. Introduction Taxonomic structures are often used for the downstream tasks like lexical entailment (Herrera et al., 2005) , entity linking (Moro and Navigli, 2015) , named entity recognition (Negri and Magnini, 2004) . Therefore, they always need to be up-todate and to keep up with the language change. Moreover, with the rapid growth of lexical resources for specific domains it becomes more and more important to develop systems that could automatically enrich the existing knowledge bases with new words or at least facilitate the manual taxonomy extension process. In this paper we tackle the taxonomy enrichment task which aims at associating new words (words not present in a taxonomy) with the appropriate hypernym synsets from the taxonomy. For instance, the word \"foster-child\" should be attached to the hypernym synset \"child.n.1\" (which refers to \"child\", \"kid\", \"youngster\") from Word-Net, and the word \"cactus\" -to the synset \"succulent.n.1\". A word may have multiple hypernyms. The task of finding a single suitable synset is difficult for a machine, and a model trained to solve this task will inevitably return many false answers if asked to provide only one synset candidate. On the other hand, if we relax the requirement of uniqueness and ask instead to provide N (for example, 10 or 15) most suitable candidates, this list can contain correct synsets with higher probability. This setting is also suitable for the manual annotation: presenting an annotator with a small list of candidates will facilitate the annotation process, because the annotator will not need to look through all synsets of the taxonomy. Thus, the task is usually formulated as the soft ranking problem, where we need to rank all the synsets according to their suitability for a given word. While word embeddings demonstrate decent results for predicting hypernyms (Arefyev et al., 2020; Dale, 2020) , much less attention is paid to the approaches based on graph representations. We assume that graph-based representations are complementary to the distributional word embeddings, as they capture the hypo-hypernymy relations from graphs. We expect that models using graph representations could be beneficial for the taxonomy enrichment task in combination with distributed word vector representations or on their own. We check our hypothesis on several models which make use of graph structures: node2vec (Grover and Leskovec, 2016), Poincar\u00e9 embeddings (Nickel and Kiela, 2017) and GCN autoencoder (Kipf and Welling, 2016a) and compare it with an approach of Nikishina et al. (2020b) which applies fastText (Bojanowski et al., 2017) and features from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018) : given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013) , where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a) . Both word and graph representations can be applied to any of these tasks. Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (Camacho-Collados et al., 2018) word embeddings are used by most of participants. Bernier-Colborne and Barri\u00e8re (2018) predict the likelihood of the relationship between an input word and a candidate using word2vec (Mikolov et al., 2013) embeddings. Word2vec is used by Berend et al. (2018) to compute features to train a logistic regression classifier. Maldonado and Klubi\u010dka (2018) simply consider top-10 closest associates from the Skipgram word2vec model as hypernym candidates. Pre-trained GloVe embeddings (Pennington et al., 2014) are also used by Shwartz et al. (2016) to initialize embeddings for their LSTM-based Hypernymy Detection model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval-2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE'2020 (Nikishina et al., 2020a ) also apply word embeddings to find the correct hypernyms in the existing taxonomy. For instance, Tanev and Rotondi (2016) compute a definition vector for the input word by comparing it with the definition vectors of the candidates from a wordnet using cosine similarity. Kunilovskaya et al. (2020) train word2vec embeddings from scratch and cast the task as a classification problem. Arefyev et al. (2020) compare the approach based on XLM-R model (Conneau et al., 2020) with the word2vec \"hypernyms of co-hyponyms\" method. It considers nearest neighbours as co-hyponyms and takes their hypernyms as candidate synsets. Summing up, the usage of distributed word vector representations is a simple yet efficient approach to the taxonomy-related tasks and can be considered a strong baseline (Camacho-Collados et al., 2018; Nikishina et al., 2020a) . Graph-based representations for taxonomies Graph-based representations for taxonomies have already been tested on other tasks related to the taxonomy enrichment. For instance, node2vec embeddings (Grover and Leskovec, 2016) are used by Liu et al. (2018) for taxonomy induction among other network embeddings. Another work on Taxonomy Induction which benefits from graphs-based representations is the one by Aly et al. (2019) who achieve state-of-theart results on all domains. The authors use hyperbolic Poincar\u00e9 embeddings to enhance automatically created taxonomies. The subtask of reattaching orphan words to the taxonomy is quite similar to taxonomy enrichment. However, the datasets of the SemEval-2016 Task 13 are restricted to specific domains, which leaves an open question of the efficiency of Poincar\u00e9 embeddings for the general domain and larger datasets. Moreover, Aly et al. (2019) use Hearst Patterns to discover hyponym-hypernym relationships. This technique operates on words, and cannot be transferred to word-synset relations without extra manipulation. Graph convolutional networks (GCNs) (Kipf and Welling, 2016a) as well as graph autoencoders (Kipf and Welling, 2016b) are mostly applied to the link prediction task on large knowledge bases. Rossi et al. (2020) present an expanded review of the field and compare a wide variety of existing approaches. Graph embeddings are also often used for other taxonomy-related tasks, e.g. entity linking (Pujary et al., 2020) . To the best of our knowledge, GCN embeddings have never been used for enhancing taxonomies like wordnets. Thus, to the best of our knowledge, our work is the first work on Taxonomy enrichment task which considers wordnets from the prospective of graph structure instead of lexico-semantic resource and makes use of graph-based representations computed from the synsets and hypo-hypernym relations for hypernym prediction. Diachronic WordNet Datasets For this task we use two diachronic datasets described by Nikishina et al. (2020b) : one for English, another one for Russian based respectively on Princeton WordNet (Miller, 1995) and Ru-WordNet taxonomies. Each dataset consists of a taxonomy and a set of novel words to be added to this resource. The statistics are provided in Table 1 . Table 1 : Datasets statistics. Dataset English Dataset This dataset is created by selecting words which appear in a newer WordNet version, but do not appear in an older one. The words are added to the dataset if only their hypernyms appear in both snippets. Adjectives and adverbs are excluded, as they often introduce abstract concepts and are difficult to interpret by context. Besides, the taxonomies for adjectives and adverbs are worse connected than those for nouns and verbs, thus making the task more difficult. Russian Dataset For the Russian language we test methods on the RUSSE'2020 (Nikishina et al., 2020a) and non-restricted dataset by Nikishina et al. (2020b) which are based on RuWordNet (Loukachevitch et al., 2016) , a taxonomy analogous to English WordNet. The RUSSE dataset was filtered from short words (< 4 symbols), diminutives, named entities and other words that can distort the results of the competition. In contrast to this data, the non-restricted dataset did not undergo this preprocessing and contains all new words from RuWord-Net2.0. Evaluation Metric The goal of diachronic taxonomy enrichment is to build a newer version of a wordnet given its older version and a list of new terms to be added to the wordnet. We cast this task as a soft ranking problem and use Mean Average Precision (MAP) score for the quality assessment: M AP = 1 N N i=1 AP i ; AP i = 1 M n i prec i \u00d7 I[y i = 1], (1) where N and M are the number of predicted and ground truth values, respectively, prec i is the fraction of ground truth values in the predictions from 1 to i, y i is the label of the i-th answer in the ranked list of predictions, and I is the indicator function. This metric is widely used in the Hypernym Discovery shared tasks, where systems are also evaluated over the top candidate hypernyms (Camacho-Collados et al., 2018) . Following Nikishina et al. (2020b) , we use as gold standard hypernyms not only the immediate hypernyms of each lemma, but also the second-order hypernyms( hypernyms of the hypernyms). Finding the region where a word belongs can already be considered a success. Otherwise, the task of automatically identifying the exact hypernym is too challenging. The MAP score takes into account the whole range of possible hypernyms and their rank in the candidate list. We use the MAP computation strategy as presented by Nikishina et al. (2020b) . It transforms a list of gold standard hypernyms into a list of connectivity components, as new word may have more than one candidate and they could and could not be related directly. Taxonomy Enrichment Methods We test a number of methods that make use of taxonomy structure to predict hypernyms for the unseen words and compare their performance with the existing approach that is based on fastText embeddings. We describe each method in the corresponding section. Word Embeddings with Features Extracted from Wiktionary We consider approach by Nikishina et al. (2020b) as our baseline. There, a vector representation for a synset in the taxonomy is created by averaging vectors of all words from this synset. Then, for each new word top 10 closest synset vectors are retrieved (we refer to them as synset associates). For each of these associates, we extract its immediate hypernyms and hypernyms of all hypernyms (second-order hypernyms). This list of first-and second-order hypernyms forms our candidate set. We rank the candidate set using the following features: \u2022 n \u00d7 sim(v i , v h j ) , where v x is a vector representation of a word or a synset x, h j is a hypernym, n is the number of occurrences of this hypernym in the merged list, sim(v i , v h j ) is the cosine similarity of the vector of the input word i and hypernym vector h j . \u2022 candidate presence in the Wiktionary hypernyms list for the input word (binary feature), \u2022 candidate presence in the Wiktionary synonyms list (binary feature), \u2022 candidate presence in the Wiktionary definition (binary feature), \u2022 average cosine similarity between the candidate and the Wiktionary hypernyms of the input word. Finally, feature weights are computed by training a Linear Regression model with L2regularisation on a training dataset from the previous WordNet/RuWordNet version. Candidate hypernyms are ranked by their model output score and are limited to the k = 10 best candidates. Candidate Generation Using Poincar\u00e9 Embeddings Poincar\u00e9 embeddings is an approach for \"learning hierarchical representations of symbolic data by embedding them into hyperbolic space -or more precisely into an n-dimensional Poincar\u00e9 ball\" (Nickel and Kiela, 2017). Poincare models are trained on hierarchical structures and simultaneously capture hierarchy and similarity due to the underlying hyperbolic geometry. According to the authors, hyperbolic embeddings are more efficient on the hierarchically structured data and may outperform Euclidean embeddings on several tasks, e.g, in Taxonomy Induction (Aly et al., 2019) . Therefore, we use Poincar\u00e9 embeddings of our wordnets for the taxonomy enrichment task. We train Poincar\u00e9 ball model for our wordnets using the default parameters and the dimensionality of 10, which yields the best results on the link prediction task (Nickel and Kiela, 2017). However, applying these embeddings to the task is not straightforward, because Poincar\u00e9 model's vocabulary is non-extensible. It means that new words that we need to attach to the existing taxonomy will not have any Poincar\u00e9 embeddings at all and we cannot make use of the embeddings similarity. To overcome this limitation, we compute top-5 fastText nearest synsets (analogously to the procedure described in Section 4.1) and then aggregate embeddings in hyperbolic space using Einstein midpoint following G\u00fclc \u00b8ehre et al. (2019) . The resulting vector is considered as an embedding of the input word in the Poincar\u00e9 space. Then, we search for the word's top-10 Poincar\u00e9 nearest neighbours and consider them as candidates. We also try to extend the candidate list with the hypernyms of each Poincar\u00e9 associate and rank them according to their frequency and similarity to the input word. Candidate Generation Using Node2vec Embeddings The hierarchical structure of the taxonomy is a graph structure, and we may also consider taxonomies as undirected graphs and apply random walk approaches to compute embeddings for the synsets. For this purpose we apply node2vec (Grover and Leskovec, 2016) approach which represents a \"random walk of fixed length l\" and \"two parameters p and q which guide the walk in breadth of in depth\". Node2vec randomly samples sequences of nodes and then applies a Skip-gram model to train their vector representations. We train node2vec representations of all synsets in our wordnets with the following parameters: dimensions = 300, walk length = 30, num walks = 200. The other parameters are taken from the original implementation. However, analogously to Poincar\u00e9 vector space, node2vec model has no technique for representing out-of-vocabulary words. Thus, it is unable to map new words to the vector space. To overcome this limitation, we apply the same technique of averaging top-5 nearest neighbours from fastText and considering their mean vector as the new word embedding and search for the most similar synsets. We also use an alternative approach to computing out-of-vocabulary node2vec embeddings. Namely, we apply linear transformation from the source fastText to the target node2vec embeddings. For this purpose we train a matrix which is used to project fastText embeddings of the input words to the target node2vec space. Link Prediction Using GCN Autoencoder The models described above have a major shortcoming: the resulting vectors for the input words heavily depend on their representations in fastText model. This can lead to incorrect results if the word's nearest neighbour list is noisy and does not reflect its meaning. In this case the noise will propagate through the Poincar\u00e9 model and result in inaccurate output even if the Poincar\u00e9 model is of high quality. Therefore, we test graph convolutional network architecture (Kipf and Welling, 2016a) that makes use of both fastText embeddings and the graph structure of the taxonomy. In particular, we use graph autoencoder model (Kipf and Welling, 2016b) whose encoder is a graph convolutional network architecture. This model learns vector representations in a completely unsupervised way: it encodes the nodes in the network in a lowdimensional space in such a way that the embeddings can be decoded into a reconstruction of the original network. FastText embeddings are used as input node features. Even though new words are not connected to the taxonomy, it is still possible to compute their embeddings according to their input node features. For each new node we get its vector representation from the encoder and then predict the probability of the link between the new node and all other nodes in the graph. The top-10 synsets from the existing taxonomy with highest probabilities are considered as final candidates. Combining Word and Graph Representations Additionally, we extend the above model with features based on node2vec and Poincar\u00e9 embeddings. Namely, we use two extra features: co-sine similarity between the candidate and the input word in node2vec vector space and similarity between the candidate and the input word in Poincar\u00e9 ball model. The overall formula is the following: score h j = w \u2022 m = n i=1 w i m i (2) Feature weights from the Logistic Regression model are denoted as vector w, m is the feature vector. Experiments In this section, we report the performance of our models on the Taxonomy Enrichment task and discuss reasons of low performance of methods exploiting hierarchical structure of the taxonomy. Results We test the models suggested in Section 4 on both English (Table 2 ) and Russian (Table 3 ) datasets. It is clearly seen that distributed word vector representations outperform graph-based approaches by a large margin. Even though Poincar\u00e9 ball model is designed for the taxonomic structures, the absence of vector representations for the OOV words dramatically affects the results. The aggregated vector of top-5 nearest neighbours retrieved from fastText can often provide a noisy or an overly general representation. Such representation is likely to yield incorrect hypernyms even if the Poincar\u00e9 embeddings for the taxonomy are of perfect quality. Likewise, node2vec model also possesses a non-extensible set of embeddings for the taxonomic synsets and uses averaging of fastText associates for representing the new words which negatively affects the results. However, the approach which uses node2vec embeddings and averages top-5 fastText associates is the best-performing approach across methods with graph representations. Moreover, node2vec embeddings perform much better than the Poincar\u00e9 embeddings. Einstein midpoint aggregation used in our Poincar\u00e9based model makes generalisation of the associate synsets, which results in too abstract synset candidates. On the other hand, averaging node2vec vectors does not have such an effect. The differences between the two models are illustrated by the examples in Table 5 . However, node2vec embeddings still rely on the fastText similarities of the closest embeddings to method nouns verbs 1.6-3.0 1.7-3.0 2.0-3.0 1.6-3.0 1.7-3.0 2.0-3.0 (Nikishina et al., 2020a) the input word vector and propagate the fastText inaccuracies. Linear projection which is an alternative option for the computation of node2vec vectors for out-of-vocabulary words, does not solve the problem either. As it can be seen in Table 5, candidates generated using node2vec with the linear projection come from completely irrelevant domains. GCN autoencoder does not outperform the majority of the approaches for neiter of languages despite being a holistic and self-sufficient approach aimed at combining word representations with the graph structure of taxonomy. The model assigns high probabilities to all synsets in the word's neighbourhood in the graph, whereas only direct and second-order hypernyms are the correct answer. Taxonomic \"uncles\", \"siblings\", \"cousins\", and other distant \"relatives\" are not welcome. The combined approach is not very consistent: incorporating graph-based features leads to an increase in scores for the Russian nouns and verbs datasets, whereas for the English dataset the approach does not yield any improvement except for the WordNet 2.0-3.0 dataset. Nevertheless, the combined method performs on par with the best RUSSE'2020 system for nouns track. Despite the close scores, our model can be considered superior to the winner of RUSSE'2020, because it is more stable across languages and easier to replicate.The best RUSSE'2020 approach for nouns extensively uses external tools such as online Machine Translation (MT) and search engines. This approach is difficult to replicate, because its performance for different languages can vary significantly, and we have no means for quantifying this difference. Conclusion In this work, we experimented with the graphbased representations for the taxonomy enrichment task and compared them to word vector representations. We tested approaches based on Poincar\u00e9 and node2vec embeddings along with the approach based on graph autoencoder to predict hypernym synsets for the input word. Our results show that the use of word vector representations is much more efficient than any of the tested graph-based approaches. Moreover, our baseline method (candidates retrieved from fast-Text nearest neighbour list and ranked with features extracted from Wiktionary) does not benefit from graph-based methods. Namely, combining the baseline scoring function with Poincar\u00e9 and node2vec similarities results in marginal improvements for some datasets, but this does not hold for all of them. According to our experiments, word vector representations are simple, powerful, and extremely effective instrument for taxonomy enrichment, as the contexts (in a broad sense) extracted from the pre-trained fastText embeddings are sufficient to attach new words to the taxonomy. Error analysis also reveals that the correct synsets identified by graph-based models are usually retrieved by the fastText-based model alone. This makes graphs representations irrelevant and excessive. Nonetheless, there exist cases where graph representations were able to identify correctly some hypernyms which were not captured by fastText. Despite the discouraging first results of the application of graph-based methods, we suggest that the taxonomy enrichment task could still benefit from them. In order to improve their performance, we plan to switch from linear transformation to non-linear to project fastText embeddings to node2vec and to apply recently published unsupervised graph word representations Graph-Glove (Ryabinin et al., 2020) . Moreover, we find it promising to experiment with temporal embeddings such of those of Goel et al. (2020) for the taxonomy enrichment task. Acknowledgments The work of Natalia Loukachevitch in the current study (preparation of data for the experiments) is supported by the Russian Science Foundation (project 20-11-20166). We thank Yuriy Nazarov and David Dale for running their approaches from RUSSE'2020 shared task on the English datasets. Error Analysis In order to better understand the difference in systems performance and their main difficulties, we performed quantitative and qualitative analysis of the results on the English nouns subset. First of all, we wanted to know to what extent the set of correct answers of graph-based models overlaps with the one of fastText-based models. In other words, we would like to know if the graph representations are able to discover hypernymy relations which could not be identified by word embeddings. Therefore, for each new word we computed average precision (AP) score and compared those scores across different approaches. We found that at least 90% words for which fastText failed to identify correct hypernyms (i.e. words with AP=0) also have the AP of 0 in all the graph-based models. This means that if fastText cannot provide correct hypernyms for a word, other models cannot help either. Moreover, only 8% to 55% words correctly predicted by fastText are also correctly predicted by any of the graph-based models. At the same time, the number of cases where graphbased models perform better than fastText is very low (3-5% cases). Thus, combining them cannot improve the performance significantly. This observation is corroborated by the scores of the combined models. We list the candidate synsets predicted by different methods in Table 5 . They demonstrate the main features of the tested approaches. As we can see, the Poincar\u00e9 embeddings retrieved by aggregating words from fastText provide too broad concepts which are clearly too far from the correct answers (\"object\", \"person\", \"element\"). GCN is too far from the correct answers in general, whereas node2vec results depend on the fastText embeddings and are semantically close to the ground truth synsets. The quite similar to those generated by the fastText model without additional features. Therefore, it is reasonable that the difference in scores is minor. However, for some cases (like \"emperor.n.01\" and",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions that the work of Natalia Loukachevitch in the current study (preparation of data for the experiments) is supported by the Russian Science Foundation, which is a research agency. There is no mention of funding from defense, corporate entities, foundations, or any indication that there were no other sources of funding."
}