{
    "article": "The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC-6) in November, co-chaired by Ralph Grishman (NYU) and Beth Sundheim (NRaD). Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from previous MUC evaluations. All except the Scenario Template task are defined independently of any particular domain. The evolution and design of the MUC-6 evaluation are described in the conference proceedings [1] . A basic characterization of the challenge presented by each task is as follows: \u2022 Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure. \u2022 Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases. \u2022 Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence ffrom anywhere in the text. \u2022 Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event. later, with results due by the end of the week. Sixteen sites participated in the evaluation; 15 systems were evaluated for the NE task, 7 for CO, 11 for TE, and 9 for ST. 1 The variety of tasks that were designed for MUC-6 reflects the interests of both participants and sponsors in assessing and furthering research that can satisfy some urgent text processing needs in the very near term and can lead to solutions to more challenging text understanding problems in the longer term. The hard work carried out by the planning committee over nearly two years led to extremely interesting and useful evaluation results. oIdentification of names, which constitutes a large portion of the NE task and a critical portion of the TE task, has proven to be largely a solved problem. The majority of systems evaluated on NE had recall and precision over 90%; the highestscoring system had a recall of 96% and a precision of 97%, which was judged to be comparable to human performance on the task. oRecognition of alternative ways of identifying an entity constitutes a large portion of the CO task and another critical portion of the TE task; it has been shown to represent only a modest challenge when the referents are names or pronouns. All but two of the TE systems posted combined recall-precision (F-measure) scores in the 70-80% range; four of the systems were able to achieve recall in the 70-80% range while maintaining precision in the 80-90% range. The top-scoring system had 75% recall, 86% precision. Five of the seven CO systems were in the 51%-63% recall range and 62%-72% precision range. Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The test set for the two information extraction tasks consisted of 100 articles. A subset of 30 articles was selected for use as the test set for the two SGML annotation tasks. The evaluation began with the distribution of the scenario definition and training data at the beginning of September. The test data was distributed four weeks $5 oThe ST task concerned changes in corporate executive management personnel; the extracted information includes answers to the basic questions of \"Who is creating or filling what vacancy at what organization?\". The mix of challenges that the task represents --extraction of domain-specific events and relations along with the pertinent entities (template elements) --yielded levels of performance that are similar to those achieved in previous MUCs (40%-50% recall, 60%-70% precision), but with a much shorter time required for porting. The highest ST performance overall was 47% recall and 70% precision.",
    "abstract": "",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "25",
    "year": "1996",
    "month": "May",
    "title": "The {M}essage {U}nderstanding {C}onferences"
}