{
    "article": "With the increase of the number of medical records written in an electronic format, natural language processing techniques in the medical domain have become more and more important. For the purpose of the development and evaluation of machine learning-based systems to extract medical information, we recently participated in the NTCIR-10 MedNLP task. The task focused on Japanese medical records and aimed at evaluating different information extraction techniques on the common data set provided by the organizers. We implemented our baseline system based on structured perceptron and have developed its extensions. In this paper, we describe our systems and report on the evaluation of and the analysis on their performance. Introduction In recent years, medical records have been increasingly written in an electronic format, which leads to a growing need for natural language processing (NLP) techniques in the medical domain. Specifically, information extraction (IE) techniques, such as named entity recognition (NER), are crucial as they serve as the basis of more intellectual and/or application-oriented tasks, including information retrieval and question answering. Given the background, the NTCIR-10 MedNLP task (Morita et al., 2013) was recently held as a shared task to foster the NLP research for medical texts, specifically targeting Japanese. The participants of the task were provided with an annotated corpus consisting of 50 fictional medical history summary reports. The intended task was a type of NER and required the participants to identify patients' personal and medical information from the reports. For the MedNLP task, we took part in the deidentification subtask and the complaint and diagnosis subtask summarized shortly by adapting an NER model to the medical domain. The model is based on structured perceptron (Collins, 2002) and was previously developed for the biomedical domain (Higashiyama et al., to appear) . This paper reports on the results of the structured perceptron-based model for the MedNLP task and presents their analysis. Additionally, conditional random fields (CRFs) (Lafferty et al., 2001) , a popular model adopted by many participants of the task, are applied for comparison. NTCIR-10 MedNLP Task Dataset The MedNLP task organizers prepared medical history summary reports of fictional patients written by physicians. The medical records consist of 50 documents and include 3,365 sentences. Two thirds of them (2,244 sentences) and remaining one thirds (1,121 sentences) are respectively provided as the sample set and the test set. The sample set is annotated with personal and medical information about patients. The personal information includes age, person's name, sex, time, hospital name and location 1 . The medical information indicates complaint and diagnosis with a modality attribute that is taken to have four values: positive, negation, suspicion and family. Suppose that there is a mention of a particular symptom about a patient. Then, the expression representing the symptom would be annotated with the attribute value of positive if the patient has the symptom, negation if the patient does not have the symptom, suspicion if the patient is suspected of the symptom and family if a member of the patient's family has a history of the symptom. Task Description and Formulation The NTCIR-10 MedNLP task mainly consisted of the following two subtasks. 1. De-identification (DI) task: identifying personal information about patients, such as ages and hospital names. 2. Complaint and diagnosis (CD) task: extracting patients' complaint and diagnosis by physicians and determining their modality status for the patients. The performance of participants' systems for both subtasks was measured by the F -measure (\u03b2 = 1), which is the harmonic mean of precision and recall. These subtasks can be seen as NER tasks recognizing named entities and classifying them into predefined semantic classes. Named entities indicate particular expressions to be extracted, which are represented by proper nouns and technical terms. As for the DI task, this subtask can be formulated as classifying each word in a sentence into one of the labels consisting of a semantic class (e.g. age) and a chunk IOB tag, where I, O, and B respectively denote the inside, outside, and beginning of an entity. For example, if a word \"64\" in \"64 years old\" is assigned with a label \"B-age\", it means that the \"64\" is recognized as the beginning of an entity with a semantic class age. The CD task can be formulated likewise by regarding a complaint and diagnosis tag with a modality attribute x as a class c-x. Description of Baseline System For the MedNLP task, we applied structured perceptron (Collins, 2002) , which is an online algorithm. Despite its simplicity, structured perceptron is reported to have performance that closely approximates that of support vector machines (SVMs), which has been applied successfully to various classification problems. In addition, we introduced a cost function into the perceptron framework to achieve higher performance, and used the model as our baseline system. The cost function is a type of cost-sensitive learning method which lowers the expected cost of misclassification. In the following two sections, we describe the learning and prediction algorithms on an ordinary and a cost-sensitive version of structured perceptron. Structured Perceptron Let X be a set of instances and let Y x be a set of possible label sequences for an instance x \u2208 X , where x denotes a token sequence (i.e., sentence) in the training or test data. Additionally, y \u2208 Y x denotes a possible label sequence of x. Y x is equivalent to the direct product L n , where n is the length of x and L is a set of labels that includes labels such as B-age and O. Learning on structured perceptron can be regarded as finding the weight vector w \u2208 R d so that the discriminative function f predicts the correct label sequences of instances. The discriminative function f : X \u2192 Y is defined as f (x, y) = \u27e8w, \u03a6(x, y)\u27e9 , where \u27e8\u2022, \u2022\u27e9 denotes an inner product of two arguments and \u03a6(x, y) \u2208 R d is the feature vector of x and y. The prediction \u0177 for x is the output of f as in \u0177 = argmax y\u2208Y f (x, y) . ( 1 ) During learning on the training data, we receive a training instance x t on each round t, and output its prediction \u0177t by Eq. (1). Then, w is updated by Eq. (2) if the prediction \u0177t differs from the correct label sequence y t : w t+1 \u2190 w t + \u03a6(x t , y t ) \u2212 \u03a6(x t , \u0177t ) , ( 2 ) where w t is the weight vector on round t. Learning is iterated through all the training instances T times. Label sequences of test instances can be predicted by Eq. ( 1 ) in the same manner as training instances. Cost-Sensitive Structured Perceptron In addition to use of structured perceptron, we exploited information on distance between a correct and a candidate label sequence of each training instance during learning based on cost-sensitive learning of an ML framework for lowering misclassification cost. Cost-sensitive approaches were, for example, applied to semantic role labeling on the study by Johansson and Nugues (2008) , which used passive-aggressive (Crammer et al., 2006) , and to part-of-speech tagging on that by Song et al. (2012) , which used multiclass SVMs. The cost-sensitive learning algorithm on structured perceptron updates the weight vector w using \u1ef9t defined below instead of \u0177t in Eq. ( 2 ). \u1ef9t = argmax y\u2208Y f (x t , y) + \u03b1\u03c1(y t , y) (3) In Eq. ( 3 ), \u03c1 : Y \u00d7Y \u2192 N\u222a{0} is the cost function which returns a larger value for larger distance between y t and y, and \u03b1 is a parameter that is taken to have a positive real number. Here, we define the cost function \u03c1 as \u03c1(y 1 , y 2 ) = |y 1 | \u2211 i=1 \u03b4(y (i) 1 , y (i) 2 ) , where |y| denotes the length of the vector y and the function \u03b4 : L \u2192 {0, 1} is defined as \u03b4(y 1 , y 2 ) = { 0 (y 1 = y 2 ) 1 (y 1 \u0338 = y 2 ) . In the cost-sensitive learning framework, the weight vector can be updated to the reserve margin \u03b1\u03c1(y t , \u1ef9t ) using \u1ef9t instead of \u0177t . That is, w t+1 \u2190 w t + \u03a6(x t , y t ) \u2212 \u03a6(x t , \u1ef9t ) . Features The following features were used in the experiments for both subtasks: \u2022 tokens in the window of size two around the current token and \u2022 the part-of-speech (POS) tag, the subtype of POS tag, the lemma and the pronunciation of the current token. We applied the Japanese morphological analyzer MeCab (Kudo et al., 2004 ) (version 0.996) with the IPA dictionary 2 (version 2.7.0) to word segmentation and used the output of MeCab for each sentence as the latter features. Evaluation and Discussion Evaluation of Baseline System Parameter Setting We determined the optimal value of parameter \u03b1 in Eq. ( 3 ) and the number of iterations T using the sample set as follows. 1. We used 90% of the sample set as the learning set and the remaining 10% as the validation set. 2. Varying the value of \u03b1 and increasing the value of T , we learned a model for particular \u03b1 and T on the learning set and evaluated it on the validation set. 3. Values of \u03b1 and T that yielded the best Fmeasure were regarded as optimal. Consequently, the optimal \u03b1 and the number of iterations T were respectively set to 30 and 20. By use of the cost function, both precision and recall on the validation set improved by around four points, compared with the method without the function. We used these values for producing our official runs on the test set submitted to the MedNLP organizers. Results on Test Set Table 1 shows the performance of our system using the test set. Table 1 (a) shows the overall performance and Table 1 (b) shows the performance of each entity class. The performance was measured by precision, recall, the F -measure (\u03b2 = 1), and accuracy. Recall was always lower than precision for all classes of both tasks, and especially lower in the family and the suspicion classes, which led to degraded F-scores. In addition, the lower performance for the total on the CD task than 2-way indicate difficulty of modality classification. Error Analysis of Baseline System For error analysis, we evaluated our system on the sample set using a five-fold cross-validation method. Subsequently, we analyzed the results on the validation sets for five iterations. As compared with the performance on the test set, the performance on the validation sets was worse by several points for the CD task, and almost equivalent for the DI task. The reason of the former is the fewer training instances, and that of the latter was that the targeted entities for the DI task have much in common as we discuss shortly. Analysis on De-identification Task Despite the smaller number of positive instances of entity classes for the DI task than that for the CD task, the performance for the former classes was relatively high on the whole. The reason is that a large portion of these entities fit typical patterns. For example, over 70 percents of the instances of the age class in the sample set match a simple regular expression, \"[\uff11-\uff19]?[\uff10-\uff19] \u6b73 [\u6642\u9803 (\u3054\u308d)]?[\u2212-(\u304b\u3089)(\u3088\u308a)(\u307e\u3067)]?\" (\"[(from)(to)]?(about)?[1-9]?[0-9](years old)\"). For misclassified cases, we found two major types of errors across all classes in this task: (1) recognition of incorrect boundaries of entities; and (2) undetection of entities (false negatives). Specifically, the most frequent errors on the age class was found to be the first type, such as \"\uff14 \uff17\u6b73\" (47 years old) for a correct boundary \"\uff12 \uff17\u6b73-\uff14\uff17\u6b73\" (27 to 47 years old) and \"\uff11\uff10 \u4ee3' ' (10s) for \"\uff11\uff10\u4ee3\u524d\u534a\" (early 10s). Because words or expressions co-occurring with or including ages themselves as numerical values are limited, it may be effective to fix system outputs by rule-based post-processing. On the other hand, most errors on the hospital class was the second type. For example, entities such as \"\u540c\u9662\" (the hospital) and \"\u7dcf\u5408\u75c5\u9662\" (general hospital) were often undetected. The reason is that these words rarely appeared in the sample set in contrast to frequently appearing words, such as \"\u5f53\u9662\" (our hospital) and \"\u8fd1\u533b\" (local hospital), which were correctly detected. As for the time class, both types of errors were often observed. A large portion of boundary errors were recognizing narrower scopes for entities than their correct ones, e.g., \"\uff11\uff10\u6708\uff12\uff19\u65e5\" (October 29) against a correct boundary \"\uff11\uff10\u6708\uff12\uff19\u65e5\u5915 \u523b\u307e\u3067\" (until the evening on October 29). Many false negatives were found to be expressions using slashes, such as \"\uff17\uff0f\uff12\uff10\". More formal expressions, such as \"\uff17\u6708\uff12\uff10\u65e5\" (July 20), are more often used in the sample set. For dealing with the errors of the hospital and the former type of the time, constructing and using dictionaries composed of expressions which often constitute or cooccur with those type of entities may be beneficial. For the latter type of the time, rule-based postprocessing may be effective, similarly to the age class. Analysis on Complaint and Diagnosis Task In addition to the two types of errors discussed for the previous task, there were mainly two types of errors in detecting complaint entities: (3) misclassification of the modality classes; and (4) misdetection of non-entities (false positive). The most frequent errors were undetection of entities through all classes, and this type of errors frequently observed in the positive and the negation classes. In order to reduce such false negatives and improve recall, we plan to use external knowledge resources such as public dictionaries in future work. The second most frequent errors were misclassification of entities whose boundaries were correctly recognized. They accounted for a major portion of errors on the three classes except the positive class. Especially, the low performance on the family and the suspicion classes was due to misclassification in addition to undetection which occur similarly as the other modality classes. For these modality classes, it was found that there exist typical keywords which often co-occur with entities. Entities of the family class co-occur with family relation names. In particular, most of them in the sample set co-occur in itemized sentences, such as \"\u7236\uff1a\u5fc3\u7b4b\u6897\u585e\" (Father: cardiac infarction). Entities of the negative class and the suspicion class occur ahead of expressions of negations, such as \"\u306a\u3057\" (be absent), and expressions of uncertainty, such as \"\u8003\u3048\u3089\u308c\u308b\" (be concerned), \" \u7591\u3044\u304c\u3042\u308b\" (be suspected), and \"\u53ef\u80fd\u6027\u304c\u3042\u308b\" (be possible). However, our system could not exploit these keywords because of the limited window size of two around the current token, and entities often occur at a distance from keywords, especially in the suspicion class. For example, Figure 1 shows an input sentence containing a suspicion entity \" \u85ac\u5264\u6027\u80ba\u708e\" (drug-induced pneumonia) and its parsed output by the MeCab morphological analyzer. Two out of three tokens constituting the entity (i.e., \"\u85ac\u5264\" (drug) and \"\u6027\" (-induced)) are more than two tokens away from the uncertainty keywords (i.e., \"\u53ef\u80fd\", \"\u6027\" (possibility) and \"\u8003 \u3048\" (concern)). To improve classification performance for modality classes, specifically recall, it is crucial to increase the window size to, for example, sentence boundaries. Alternatively, it may be effective to take advantage of dependency parsing. The other causes of the observed errors were incorrect boundary errors and misdetection errors. The reasons require a further study. Post-submission Experiments To achieve higher performance, we have developed our medical information extracting systems also after implemented and submitted our baseline system. Specifically, we used CRFs as an alternative ML algorithm to structured perceptron. Moreover, we introduced domain-specific terms in medical fields into the default dictionary of the morphological analyzer. In the following subsections, we describe the above conversion and extension from the baseline system and the experiments on those. Alternative ML Algorithm: Conditional Random Fields To improve the performance of the baseline system, we employed CRFs (Lafferty et al., 2001) as an alternative ML algorithm. CRFs are extensions of maximum entropy to structured prediction. Additionally, the algorithm has been widely applied to both NER (McCallum and Wei, 2003; Settles, 2004; Finkel et al., 2005) and other NLP tasks, such as part-of-speech tagging (Lafferty et al., 2001) , noun phrase chunking (Sha and Pereira, 2003) and morphological analysis (Kudo et al., 2004) . Particularly, we utilized CRF++ 3 , which is an open source implementation of CRFs and allows easy customizability of features by describing in the feature template file. We used the same features as those in the baseline system. Use of Medical Lexicon When analyzing texts in a specific domain, morphological taggers with default dictionary in general domain often unsuccessfully analyze sentences that contain domain-specific terms. Consequently, they make errors attributed to unknown words in word segmentation or other processing such as POS tagging and pronunciation prediction. These errors can be negatively affect on NER that is a higher-level task than morphological analysis. Then, we enhanced the regulation dictionary of MeCab by addition of domain-specific terminology from life science dictionary (LSD) (Kaneko et al., 2003) , which consists of a broad range of life science terms such as names of anatomical concepts, biological organisms, diseases and symptoms. By addition of a domain-specific dictionary, not only the morphological tagger can achieve tagging error reduction, but also finely segmented morphemes that are component of domain-specific terms tend to be segmented more coarsely because expressions contained in the dictionary are more frequently regarded as one morpheme. For instance, \"\u85ac\u5264\u6027\u80ba\u708e\" (drug-induced pneumonia) is segmented into \"\u85ac\u5264\" (drug), \"\u6027\" (-induced) and \"\u80ba\u708e\" (pneumonia) before the addition of terms in LSD to the original dictionary and into \"\u85ac\u5264\u6027\u80ba\u708e\" after the addition. Similarly, \"\uff30 \uff29\uff30\u95a2\u7bc0\u88c2\u9699\u72ed\u5c0f\u5316\" (joint space narrowing at the proximal interphalangeal (PIP) joints) is seg- mented into \"\uff30\uff29\uff30\", \"\u95a2\u7bc0\" (joints), \"\u88c2\u9699\" (space), \"\u72ed\u5c0f\" (narrow) and \"\u5316\" (-ing) before and into \"\uff30\uff29\uff30\u95a2\u7bc0\", \"\u88c2\u9699\" and \"\u72ed\u5c0f\u5316\" after. The latter segmentation can be beneficial for exploiting information about strings distant from the token in question in the case of fixed window size around the token. Therefore, in addition to reduction errors in morphological analysis, NER systems can obtain benefit from coarse segmentation, by use of the tagger with the richer language resource. Results and Discussion To measure the performance of CRFs, which we used as an alternative algorithm to structured perceptron, and to evaluate the effectiveness of the enhanced dictionary, we compared four systems based on the two algorithms with or without the enhanced dictionary. Table 2 shows the results on the sample set using five-fold cross-validation. Table 2 (a) and (b) show the overall performance for the DI task and the CD task, respectively. For both subtasks, while recall of structured perceptron was higher than that of CRFs, CRFs outperformed structured perceptron by around 10 points in terms of precision. Additionally, CRFs also outperformed by a few points in terms of F -measure. The both algorithms consider the overall sequence of tokens when predicting their labels, but they defer in the respective training methods. More precisely, structured perceptron minimizes the loss defined by the difference between correct and predicted label sequences. This process can be regarded as the training by a simple (sub) gradient method with fixed step size, which is a firstorder gradient method. On the other hand, CRFs are trained by maximizing the log-likelihood of a given training set. The implementation of CRFs used in our experiment was based on limitedmemory BFGS (L-BFGS), which is a secondorder gradient method. We believe that the more sophisticated optimization algorithm of CRFs resulted in the higher performance. In fact, Sha and Pereira (2003) empirically showed that CRFs based on second-order methods, such as L-BFGS and conjugate gradient, outperformed structured perceptron on a noun phrase chunking task. Contrary to our expectation, use of the morphological analyzer with enhanced dictionary had a little or negative effect for the performance of both algorithm and for both subtasks, except that recall of structured perceptron for the CD task was improved. We believe that this result was due to loss of common characteristics among segmented tokens. Focusing on the complaint entity \"\u85ac\u5264 \u6027\u80ba\u708e\" (drug-induced pneumonia), various expressions occur in the sample set preceding \"\u80ba \u708e\" (pneumonia), e.g. \"\u7d30\u83cc\u6027\" (bacterial), \"\u9593\u8cea \u6027\" (interstitial), \"\u5668\u8cea\u5316\" (organizing), \"\u5f37\u819c\u708e\" (pleuritic) and \"\u30cb\u30e5\u30fc\u30e2\u30b7\u30b9\u30c1\u30b9\" (Pneumocystis), in addition to \"\u85ac\u5264\u6027\" (drug-induced). Furthermore, there are variety of entities containing expressions that co-occur with \"\u80ba\u708e\", e.g. \"\u85ac\u5264 \u6027\u80ba\u969c\u5bb3\" (drug-induced pulmonary disorder), \" \u7d30\u83cc\u611f\u67d3\" (bacteria infection), \"\u5668\u8cea\u5316\u8840\u6813\" (organizing thrombus), \"\u80f8\u819c\u708e\" (pleuritis) and \"\u30cb \u30e5\u30fc\u30e2\u30b7\u30b9\u30c1\u30b9\u30fb\u30ab\u30ea\u30cb\" (Pneumocystis carinii). As we discussed previously, morphemes tend to be segmented more coarsely after augmented terms in the dictionary of the morphological analyzer. Then, entities enumerated above became to be recognized as distinct tokens without common characteristics, by segmented to one or a little larger numbers of morphemes. We consider that this affected the performance negatively and disturbed learning of classifiers. To fix this problem, it may be effective to use prefix and suffix features derived form expressions that are often contained by or co-occurred with entities. After the processing, classifiers may come to be able to exploit information about strings that are distant from the current token and to obtain benefit by reduction errors in morphological analysis. Related Work To the NTCIR-10 MedNLP task, both rule-based and ML-based approaches were applied among the participants. Almost all systems for the DI task and over a half of all systems for the CD task were based on ML, especially supervised learning. It should be note that greater part of systems that achieved higher performance were based on ML and moreover a large portion of them employed CRFs. Specifically, systems of the top three teams for the CD task and of the second and third ranked teams for the DI task were based on CRFs. By contrast, the system that had the highest performance for the DI task was a rule-based approach. As other ML-based approaches than CRFs, structured perceptron, language models and bootstrapping were applied. As to features, general-purpose NER features were widely applied, such as word surface (token) and POS features. Pronunciation and character type features were also used. Besides, domainspecific features including dictionary matching features or heuristic features of data-specific expressions were used. These features are derived from medical knowledge resources such as LSD and MEDIS standard masters 4 , or manually constructed lexica consisting of expressions that are specific to each entity class. Among the features incorporated in the ML-based systems, particularly, those that achieved higher performance, dictionary or heuristic features provided high benefit for their performance. Specifically, Laquerre et al. (Laquerre and Malon, 2013) reported that heuristic features for the DI task improved the F -measure by around three points and heuristic and dictionary features for the CD task improved by around 4.5 points. Miura et al. (Miura et al., 2013 ) also reported that dictionary features for the CD task improved the F -measure by around two points. Nevertheless the limited size of the dataset, the overall performance for the subtasks of the top systems were high: they achieved over 90% and 75% F -measure for the DI task and the CD task, respectively. As regards the performance for each entity type, that for the family entities were over 80% F -measure, which is highest of all entity 4 http://www.medis.or.jp types for the CD task, in spite of smaller numbers of entities in the sample set. This is due to the features for the family class such as family names could capture the characteristics of this entities well. By contrast, the F -measure was only around 50% for the suspicion entities, which occurred less frequently similarly to the family entities. This suggests that the suspicious expressions used for extracting the suspicion entities (e.g. \"\u7591 \u3044\" (suspicious) and \"\u53ef\u80fd\u6027\" (possibility)) were insufficient or there exists other reasons that make it difficult to identify this type of entities. Conclusions This paper described our systems to extract personal and medical information from medical texts. We implemented a simple system based on structured perceptron as a first step toward more effective Japanese medical text processing systems, and extended it to systems based on another machine learning algorithm and on a morphological analyzer with a domain-specific dictionary. Moreover, we analyzed its performance and issues for achieving the goal. The result on the MedNLP dataset indicates that classification of medical entities into their modality classes, especially the suspicion class, is difficult. However, our analysis revealed that the terms and expressions in medical texts have useful patterns and characteristics that could be exploited for more accurate extraction. Although it found that it was not very effective to use output of the morphological analyzer with domain-specific dictionary, we are aiming to use knowledge resources in more effective ways, e.g. incorporating dictionary features into classifiers. Additionally, we plan to explore more useful features such as suffix and prefix features for development of more advanced systems.",
    "abstract": "With the increase of the number of medical records written in an electronic format, natural language processing techniques in the medical domain have become more and more important. For the purpose of the development and evaluation of machine learning-based systems to extract medical information, we recently participated in the NTCIR-10 MedNLP task. The task focused on Japanese medical records and aimed at evaluating different information extraction techniques on the common data set provided by the organizers. We implemented our baseline system based on structured perceptron and have developed its extensions. In this paper, we describe our systems and report on the evaluation of and the analysis on their performance.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "1",
    "year": "2013",
    "month": "October",
    "title": "Developing {ML}-based Systems to Extract Medical Information from {J}apanese Medical History Summaries"
}