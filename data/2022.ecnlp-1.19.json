{
    "article": "Extracting attribute-value information from unstructured product descriptions continue to be of a vital importance in e-commerce applications. One of the most important product attributes is the brand which highly influences customers' purchasing behaviour. Thus, it is crucial to accurately extract brand information dealing with the main challenge of discovering new brand names. Under the open world assumption, several approaches have adopted deep learning models to extract attribute-values using sequence tagging paradigm. However, they did not employ finer grained data representations such as character level embeddings which improve generalizability. In this paper, we introduce OpenBrand, a novel approach for discovering brand names. OpenBrand is a BiLSTM-CRF-Attention model with embeddings at different granularities. Such embeddings are learned using CNN and LSTM architectures to provide more accurate representations. We further propose a new dataset for brand value extraction, with a very challenging task on zero-shot extraction. We have tested our approach, through extensive experiments, and shown that it outperforms state-of-the-art models in brand name discovery. Introduction Brand name plays a very important role in influencing customers' behaviour (Chovanov\u00e1 et al., 2015; Shahzad et al., 2014) . Typically, as customers are aware of the brand, they can deduce knowledge about other product attributes. Let us take the example of the toy shown in Figure 1 . The brand of this product is \"Gentle Monster\". By knowing the brand, customers would have some kind of associations, like this toy would be of \"a soft and smooth wood\", have \"bright colors\", and contain \"small pieces which is suitable for older kids\". So, when shopping for toys, they would pick a particular brand based on the attributes they find important. Such correlations between brands and product attributes make it crucial for e-commerce applications to accurately extract brand names from product descriptions. Retrieving brand names is addressed in the literature within the general problem of attribute-value extraction from product descriptions (Kovelamudi et al., 2011; Vandic et al., 2012; Ghani et al., 2006; Kozareva et al., 2016; Zheng et al., 2018; Xu et al., 2019) . Early approaches rely on rule-based techniques which use domain-specific knowledge to identify attributes and values (Kovelamudi et al., 2011; Vandic et al., 2012; Ghani et al., 2006) . Such approaches adopt a closed world assumption requiring the possible set of values to be known beforehand by mean of dictionaries or hand-crafted rules. Consequently, they are not suitable for discovering unseen values such as newly emerging brands. To tackle this problem, most recent approaches model the extraction task as sequence tagging (Kozareva et al., 2016; Zheng et al., 2018; Xu et al., 2019) and solve it using deep learning models such as BiLSTM enhanced by Conditional Random Field (CRF) and Attention layers. These new approaches achieve promising results, however, they limit the representation of their data to word embeddings which can capture context but penalizes generalizability to new brands. In this paper, we propose to use character level embeddings in sequence tagging models for discovering brand names. In addition to word embeddings, character level embeddings were employed in Named Entity Recognition (NER) tasks (Lample et al., 2016) to handle out-of-vocabulary words. The problem of unseen words is particularly emphasized in brands because of sub-branding, brand fragmentation, or simply emerging businesses. Unseen brand names can be completely new, like in brand fragmentation where new brands share the same parent brand maintaining minimal links between the new and the existing identities. For example, \"Audi\" and \"Porsche\" do not have any similarity although they have the same parent brand \"Volkswagen\". By contrast, sub-branding would maintain stronger links between existing brands and the new generated ones, which can be reflected by similarities in brand names. Examples include \"Uber\" and \"UberPool\", \"McDonalds\" and \"Mc-Cafe\", or \"Samsung\" and \"Samsung Evo\". Thus, the use of character level embedding is crucial for capturing variations in brand names and the occurrence of unseen brands. We summarize the main contributions of this work as follows: 1. We propose OpenBrand, a BiLSTM-CRF-Attention model that combines word embeddings with character level word embeddings. In contrast to previous approaches, we learn character level embeddings based on CNN and LSTM architectures to obtain specific representations of our data. 2. We provide a large real world dataset 1 focusing on brand names to have a thorough analysis of the impact of character level embeddings. We experimentally show that our dataset is challenging on brand name extraction, especially those zero-shot brand values. 3. We empirically demonstrate significant improvements in F1 score over several stateof-the-art baselines on brand name extraction. Additionally, we show that OpenBrand guarantees a better generalizability over new brands and deals more effectively with compound brand names. Problem Statement In this section, we formally define the problem of open brand value extraction. Given a product title, represented as an unstructured text data, and a target attribute (eg. brand), our goal is to extract the appropriate values for the corresponding attribute from the product title. In this context, we want to discover new values that have not been encountered before. We formalize the attribute-value extraction as per the following definition: Definition Given a product title X. The title X is represented as a sequence of tokens X t = {x 1 , ..., x T }, where T is the sequence length. Consider a target attribute A. Attribute-value extraction automatically identifies a sub-sequence of tokens from X t as applicable attribute-value pair. A v = {x i , x i+1 , ..., x k }, for 1 \u2264 i \u2264 k \u2264 T . For example, consider the title for the product given in the example of Figure 1 : X = \"Wooden Stacking Board Games 54 Pieces for Kids Adult and Families, Gentle Monster Wooden Blocks Toys for Toddlers, Colored Building Blocks -6 Colors 2 Dice.\" The tokenization of X yields: X t = {x 1 , x 2 , ..., x 25 } = {\"Wooden\", \"Stacking\", \"Board\", .., \"Dice\"}, where T = 25. For the target attribute: A = {\"Brand\"}. We want to extract: Brand = {x 12 , x 13 } = {\"Gentle\", \"Monster\"}. In order to identify these sub-sequences, the sequence of tokens X t need to be tagged to capture sequential and positional information. For this purpose, we adopt the sequence tagging model and associate a tag from a given tag-set to the sequence of input tokens X t . We experimented with different tagging strategies and, inline with previous work in the literature (Xu et al., 2019) , we found that the {B,I,O} tagging scheme produced the best results, where \"B\", \"I\", and \"O\" represent the beginning, inside, and outside of an attribute, respectively. (A sequence of \"O\" tags corresponds to the absence of an attribute). Table 1 shows an input/output example of the {B,I,O} tagging strategy. OpenBrand Model To address the open brand value extraction problem, we propose a BiLSTM-CRF-Attention model with character level embeddings. Figure 2 shows our OpenBrand model architecture, which is composed of three main layers: an embedding layer that encodes the input sequence, a contextual layer that captures complex relationships among the input sequence, and an output layer that produces the output labels. Embedding Layer In the embedding layer, we map every word in the product description into a d-dimensional embedding vector. The embeddings of the words are obtained by concatenating the word embeddings and character level embeddings. Word embeddings are obtained from the pre-trained GloVe (Pennington et al., 2014) word representations, which are trained over large unlabeled corpus. Pre-trained word embeddings, such as GloVe and Word2Vec (Mikolov et al., 2013) , offer a single representation for each word, which is not useful in the case where words have different meanings depending on the context. To allow our model to learn different representations of embeddings depending on the context, we learn and generate different representations of tokens in the input sequence. For this reason, the weights of our embedding layer are considered to be learnable parameters and not fixed. An important distinction of our approach, compared to previous work on attribute-value extraction, is that we learn character level features in our model. For character level embeddings, we use two different architectures: CNN-based and LSTMbased character level representations. Learning character level embeddings has the advantage of learning task-specific representations. Convolutional Neural Networks (CNN) are designed to discover position-invariant features and they are highly effective in extracting morphological infor-mation (ex. prefix or suffix of words) (Chiu and Nichols, 2016) . On the other hand, LSTMs are capable of encoding long sequences, and are thus capable of extracting position dependent character features. These features are crucial to model the relationships between words and their characters. Given a token of our input sequence x t , the embedding layer maps x t in to the vector: e t = [w t ; c t ], where w t and c t are the word and character level representations of x t , respectively. The embedding representation of the whole input sequence X t would be {e 1 , e 2 , ..., e T }. Figure 3 illustrates the two architectures used to encode the character representations. These character representations are then concatenated with the word embeddings and fed as input to our contextual layer. Contextual Layer The contextual layer captures contextualized representations for every word in the input sequence. In our model, the input sequence to the contextual layer is the concatenation of the character level representations and word embeddings, both mapped by the underlying embedding layer. In this stage, we employ a BiLSTM contextual layer followed by a self-attention layer. Long Short Term Memory Networks (Hochreiter and Schmidhuber, 1997) address the vanishing gradient problems of Recurrent Neural Networks and are thus capable of modeling long-term dependencies between tokens in a sequence. Bidirectional LSTM (BiLSTM) can capture both past and future time steps jointly by using two LSTM layers to produce both forward and backwards states, respectively. Given the input e t (embedding of a token x t ), the hidden vector representations from the backward and forward LSTMs ( \u2212 \u2192 h t and \u2190 \u2212 h t ) is: h t = \u2206([ \u2212 \u2192 h t ; \u2190 \u2212 h t ]) where \u2206 denotes a non-linear transformation. The hidden representation of the whole input sequence X t is H t = {h 1 , h 2 , ...., h T }. In reality, not all hidden states generated by the BiLSTM layer are equally important for the labeling decisions. A mechanism that allows the output layer to be aware of the important features of the sequence can improve the prediction model. This is exactly what attention does. Attention mechanisms have achieved great success in Natural Language Processing (NLP) and were first introduced   in the Neural Machine Translation task (Bahdanau et al., 2015) . In the contextual layer, we use a self-attention mechanism to highlight important concepts in the sequence rather than focusing on everything. The model learns to attend to the important parts of the input states based on the output produced so far. We first compute the similarity between all hidden states representations to obtain an attention matrix A \u2208 R T \u00d7T where \u03b1 t,t \u2032 = \u03c3(w \u03b1 g t,t \u2032 + b \u03b1 ) is the element of matrix A representing the mutual interaction between hidden states h t and h t \u2032 . \u03c3 is the element-wise sigmoid function, and g t,t \u2032 = tanh(W 1 h t + W 2 h t \u2032 + b g ) where W 1 , W 2 , w \u03b1 are trainable attention matrices, and b g , b \u03b1 are trainable biases. The contextualized hidden states can be computed as h t = T t \u2032 =1 \u03b1 t,t \u2032 \u2022 h t \u2032 The contextualized hidden state of the whole input sequence X t is H t = { h 1 , h 2 , ... h T }. CRF Layer In sequence labeling tasks, it is important to consider the dependencies between output tags in a neighborhood. Conditional Random Fields (CRF) allow us to capture the correlation between labels and model their sequence jointly. For example, if we already know the tag of a token is I, then this increases the probability of the next token to be I or O, rather than being B. We feed the contextualized hidden states H t = { h 1 , h 2 , ... h T } to our output CRF layer to get the sequence of labels with highest probabilities. The joint probability distribution of a tag y given the hidden state h t and previous tag y t\u22121 is given by P r(y|x; \u03c8) \u221d T t=1 exp K k=1 \u03c8 k f k (y t\u22121 , y t , h t ) where \u03c8 k is the corresponding learnable weight, f k is the feature function, and K is the number of features. The final output label is the label with the highest conditional probability, given as y * = argmax y P r(y i |x i ; \u03c8) where y * \u2208 {B, I, O} is the output tag. In Section 5.2, we will study in detail the effect of the attention and CRF layers on the discovery of brands in comparison with the embeddings layer. Experimental Setup This section presents the experimental settings of our empirical approach for comparing state-of-theart models on the task of brand value extraction. Dataset To evaluate the effectiveness of OpenBrand, we have collected a dataset that contains information about products from Amazon. Our dataset is derived from a public product collection -the Amazon Review Dataset (Ni et al., 2019) 2 . The categories of the collected dataset contained a large amount of overlapping brands, which might bias the results of the experiments. Thus, we have selected a subset to have a diverse set of brands with minimal overlapping across categories. We also processed  To further examine the generalization ability of our model, we divide the AZ-base dataset into another training and test split with no overlapping brand values. In other words, none of the values in the test set are encountered during training. We refer to this data split as AZ-zero-shot, as it is designed for evaluating zero-shot extraction. The test set of AZ-zero-shot contains more than 8k new and unique brand values. In addition, we have also chosen another subset of products from our collected data with another set of categories. The purpose of this dataset is to test the models capabilities in detecting brand values across different category domains. The dataset contains information about products in three new categories as shown in Table 3 . We refer to this dataset as AZ-new-cat, as it is designed to evaluate the model on a new set of product categories. Models Under Comparison We implemented and compared three state-of-theart baseline models on attribute-value extraction. BiLSTM (Hochreiter and Schmidhuber, 1997) which uses word embeddings from pretrained GloVe (Pennington et al., 2014) for word level representation, then applies BiLSTM to produce the contextual embeddings. BiLSTM-CRF (Huang et al., 2015) on top to model the tagging decisions jointly. This model is considered state-of-the-art sequence tagging model for NER. OpenTag (Zheng et al., 2018) which adds a self attention mechanism between the contextual BiL-STM layer and the CRF decoding layer. OpenTag is considered the pioneer sequence tagging model for attribute-value extraction. We compare the above baseline models with the OpenBrand models we proposed in Section 3. OpenBrand-LSTM In this approach, character level information is obtained by applying a BiL-STM encoder on the sequence of characters in each word. This character level information is used in combination with word-level embeddings as input to the BiLSTM-CRF-Attention model. OpenBrand-CNN This approach is similar to the above model, but CNNs are used instead of LSTMs to encode character level information in the word sequences. We use precision P , recall R and F 1 score as evaluation metrics based on the number of true positives (TP), false positives (FP), and false negatives (FN). We use Exact Match criteria (Rajpurkar et al., 2016) , in our evaluation, with either full or no credit. The implementation details are provided in the Appendix. P = T P T P + F P R = T P T P + F N F1 = 2 \u00d7 P \u00d7 R P + R Results and Discussion We conducted a series of experiments on AZ-base, AZ-zero-shot, and AZ-new-cat datasets under various settings to evaluate the performance of Open-Brand. Baseline Performance Comparison In the first experiment, we compare the performance of OpenBrand with the three state-of-the-art baselines mentioned in Section 4.2 for identifying brand values from product descriptions. 4 reports the comparison results of our two models (OpenBrand-LSTM and OpenBrand-CNN) and three baselines across all categories in the AZ-base dataset. From these evaluation results, we can observe that our models substantially outperform the other compared models in all categories. Open-Brand with LSTM character level and CNN character level embeddings are consistently ranked the best over all competing baselines. The overall improvement in F 1 score is up to 6.1% as compared to OpenTag. The main reason for this result is that our model learns both character and word embeddings during training, thus allowing to learn more effective contextual embeddings that are more suitable for the task of extracting brand values. Impact of Character level Representations To understand the effect of character level representations on brand-value extraction, we extend all baseline models with character level embeddings and test them on the AZ-base dataset. performances of all models. An interesting observation is that character level embeddings improve the model much more effectively than CRF or attention layers. For example, and as shown in the last two rows of Table 5 , adding a CNN-representation to a BiLSTM-CRF model improves the model by 1.15%, while adding an attention layer only improves the model by 0.14%. The experiments also show that using either CNN-char or LSTM-char both lead to an improvement with comparable overall F 1 score. However, CNNs have less training complexity as compared to LSTM models under similar experimental settings. In our experiments, the average training time of models with LSTM-char increased by 59% relative to the baseline BiLSTM-CRF-Att model, while it only increased by 22% with CNN-char, as detailed in Table 6 . CNN-char also produces better performances than LSTM-char as shown in Table 5 Discovering New Brand Values We conduct zero-shot extraction experiment to evaluate the generalization ability of our models on unseen brand values. Table 7 reports the zero-shot extraction results. It can be seen that our model achieves better performance than OpenTag on unseen data. This is because our model can leverage the sub-sequence level similarities in brand names between the train set and test set, through the character level embeddings. However, it is clear that the overall performance of all models is worse as compared to the results in with our expectations as there are no training samples for the zero-shot brand values. This indicates that it is truly a difficult zero-shot extraction task. To further examine the ability of OpenBrand in discovering brand values in new categories, we train the models on the AZ-base dataset, and test them on the AZ-new-cat dataset introduced in Section 4.1. Table 8 reports the results across three different categories in the AZ-new-cat dataset. It is clear that OpenBrand achieves much better performance with gains up to 2.7% in F 1 score as compared to OpenTag. This indicates that our model has good generalization and is able to transfer to other domains. Also, the results are much better than zero-shot extractions. This is because some data in the training set are semantically related to the brand values in AZ-new-cat and thus they provide hints that guide the extraction. For example, many of the brands in Cell Phones & Accessories category (eg. Samsung Galaxy) are sub-brands of products in Electronics category (eg. Samsung). Impact of Brand Entities We also conducted experiments to explore the relationship between the number of entities that consti- tute the brand and the performance of the models. Since we use Exact Match criteria in our evaluations, detecting brand values with more than one entity becomes very challenging in general. We divide the test set of our AZ-base dataset into four subsets according to the number of entities inside a brand (see Figure 4 ). While OpenTag achieves good overall F 1 performance with brand values consisting of single entities (88%), it is much worse on brand values with three or more entities (67% and 61% respectively). OpenBrand, on the other hand, still performs well even on brands with two or more entities (71% and 65% respectively). Discussion Our experimental results show that, for the task of extracting brand values, OpenBrand outperforms baseline approaches by a significant margin. Besides the general F1 score, the gains can be seen in both precision and recall which go up to 2.2% and 11.5%, respectively. This means that character embeddings do not only help discover more brand values but they also improve the accuracy of the extracted information. Furthermore, the gains in recall are also high for the AZ-new-cat and AZzero-shot datasets, reaching 3.3% and 1.46% of improvement respectively. Thus, OpenBrand performs particularly well for unseen data which confirms our initial claim that character embeddings enhance model generalizability. Another important finding of our study is that the performance of OpenBrand depends on the product category. We can observe that, for the Automotive category, the gain in precision is 0.2% while it goes up to 2.2% for the Toys & Games category. This is mainly due to an ambiguity problem in the product descriptions of the Automotive category. Some product descriptions might contain values of other brands other than the one that needs to be detected. Let us take the following product description: \"Honda Shadow 750 Aero Cobra Saddlebag Guards Supports\". This is about a \"Saddlebag Guards Supports\" that is compatible for \"Honda\" cars. The brand of this product is \"Cobra\" but the presence of \"Honda\" in the description can be confusing for the model leading to wrong extractions. We additionally observe that compound brand values are best handled by OpenBrand. This is due to the fact that the combination of character and word embeddings contributes to more meaningful representations. The results also show that OpenBrand-LSTM tends to perform worse, as compared to OpenBrand-CNN. This is inline with prior observations (Bradbury et al., 2017) that LSTM can be difficult to apply on long sequences of input. Related Work There has been significant research on the task of attribute-value extraction from product descriptions (Wong et al., 2009) . Initial approaches (Vandic et al., 2012) formulated the problem as a classification task relying on supervised learning techniques. (Ghani et al., 2006) use a Naive Bayes classifier to extract values that correspond to a predefined set of product attributes. (Putthividhya and Hu, 2011) focus on annotating brands in product listings of apparel products on eBay. (Kovelamudi et al., 2011) propose a domain independent supervised system that can automatically discover product attributes from user reviews using Wikipedia. Similarly, (Ling and Weld, 2012) propose an automatic labeling process of entities by making use of anchor links from Wikipedia text. Other approaches exploited unsupervised learning techniques like (Shinzato and Sekine, 2013) in their task of extracting attribute-values from e-commerce product pages. Following a similar line, (Charron et al., 2016) use consumer patterns to create annotations for datadriven products. (Bing et al., 2016) focus on the discovery of hidden patterns in costumer reviews to improve attribute-value extraction. The above approaches provide promising results, however they poorly handle the discovery of new values due to their closed world assumption. The most recent approaches (Kozareva et al., 2016; Zheng et al., 2018; Xu et al., 2019) make instead an open world assumption using sequence tagging models, similarly to NER tasks (Ma and Hovy, 2016; Huang et al., 2015) . (Kozareva et al., 2016) use a BiLSTM-CRF model to tag several product attributes for brands and models with handcrafted features. (Zheng et al., 2018) develop an end-to-end tagging model utilizing BiLSTM and CRF without using any dictionary or hand-crafted features. After that, (Xu et al., 2019) adopted only one global set of BIO tags for any attributes to scale up the semantic representation models of product titles. In this context, (Karamanolakis et al., 2020) proposed a taxonomy aware knowledge extraction model that takes advantage of the hierarchical relationships between product categories. The latest approaches extend the open world assumption also to attributes and use question answering (QA) models (Wang et al., 2020) to scale to a larger number of attributes. Sequence tagging approaches are the most relevant to our work since extracting brand names does not require scalability. However, these models did not exploit character level embeddings which are crucial for improving generalizability. In our work, we enhance such models using different granularities of embeddings. Conclusion In this paper we have addressed the problem of extracting brand values from product descriptions. Previous state-of-the-art sequence tagging methods faced the challenge of discovering new values that have not been encountered before. To tackle this issue we proposed OpenBrand, a novel attribute-value extraction model with the integration of character level representations to improve generalizability. We presented experiments on realworld datasets in different categories which show that OpenBrand outperforms state-of-the-art approaches and baselines. By exploiting character level embeddings, OpenBrand is capable of learning accurate representations to discover new brand values. Our experiments also show that CNN based representations outperform LSTM based representations in both performance and computation. A natural extension of this work is to deal with the problem of disambiguation discussed in Section 5.5. To this end, we need to have more training data which helps understating the patterns in a better way. Moreover, we need to extend the tagging model to capture ambiguous product descriptions. This extension can be very important when brand values need to be extracted from other data sources other than concise product descriptions. A Appendix A.1 Implementation Details Our models are implemented with Tensorflow 3 and Keras 4 , and they are trained using TPUs on the 3 https://www.tensorflow.org/. cloud. We used the validation set of AZ-base to select the optimal hyper-parameters of our model, while the test set was used to report the final results. During training, optimization is performed with Adam optimizer (Kingma and Ba, 2015) using a 1e \u22123 initial learning rate. For all models, we employed pre-trained 100-dimensional word vectors from GloVe (Pennington et al., 2014) . All models use a dropout layer (Srivastava et al., 2014 ) of size 0.3 both before and after the BiLSTM layer. The minibatch size is fixed to 128. The BIO tagging scheme is adopted. In the training process, we used the loss score on the validation set to assess model improvement. The models were trained for a total of 100 epochs, and early stopping was applied if there was no improvement for a period of 10 epochs. The average training time for each epoch was also recorded. Tables 9 and 10 show the selected hyperparameters in the CNN-based and LSTM-based models respectively, based on the performance on the validation set. These include the character embeddings dimension. The tables also show the total number of trainable parameters for each model. The difference in number of trainable parameters shows that CNNs have less training complexity as compared to LSTM models under similar experimental settings."
}