{
    "article": "Sentiment analysis deals with the task of determining the polarity of a document or sentence and has received a lot of attention in recent years for the English language. With the rapid growth of social media these days, a lot of data is available in regional languages besides English. Telugu is one such regional language with abundant data available in social media, but it's hard to find a labelled data of sentences for Telugu Sentiment Analysis. In this paper, we describe an effort to build a gold-standard annotated corpus of Telugu sentences to support Telugu Sentiment Analysis. The corpus, named ACTSA (Annotated Corpus for Telugu Sentiment Analysis) has a collection of Telugu sentences taken from different sources which were then preprocessed and manually annotated by native Telugu speakers using our annotation guidelines. In total, we have annotated 5410 sentences, which makes our corpus the largest resource currently available. The corpus and annotation guidelines are made publicly available. Introduction Now-a-days, people are commonly found writing comments, reviews, blog posts in social media about trending activities in their regional languages. Unlike English, many regional languages lack NLP tools and resources to analyze these activities. Moreover, English has many datasets available, however, it is not the same with Telugu. The annotation of Telugu data has not received a lot of attention in sentiment analysis community. While there is a wealth of raw corpora with opinionated information, no corpora with annotated sentences in Telugu are publicly available as far as we know. Telugu has a special status as an official standard language in the twin states of Andhra Pradesh and Telangana of India. There are a large variety of dialects that constitute the mother tongues of Telugu speakers. Major Telugu print media, journalism, and electronic media follow the dialects of Krishna and Godavari since it has been conceived as arguably standard and easy to reach the rest of the Telugu speakers (Krishnamurthi, 1961) . We built our corpus over this dialect as this dialect is most prominent and has a strong online presence today on news websites, blogs, forums, and user/reader commentaries. In this work, we present a dedicated gold standard corpus of polarity annotated Telugu sentences. To our knowledge, our corpus is the largest source of polarity annotated Telugu sentences to date. This data also motivates the development of new techniques for Telugu sentiment analysis. The corpus and annotation guidelines are publicly available here 1 . Related Work There is a growing interest within the Natural Language Processing community to build corpora for Indian languages from the data available on the web. (Kaur and Gupta, 2013) surveyed sentiment analysis for different Indian languages including Telugu, but never mentioned about the corpus used. (Mukku et al., 2016) did sentiment classification for Telugu Figure 1 : Process of building the resource text using various ML techniques, but no data was publicly made available. (Wiebe et al., 2005) describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. This was the first attempt to manually annotate the 10,000 sentence corpus of from the news. (Alm et al., 2005) have manually annotated 1580 sentences extracted from 22 Grimms' tales for the task of emotion annotation at the sentence level. (Arora, 2013) performed sentiment analysis task for the Hindi Language with limited corpus made manually annotated by the native Hindi speakers. (Das and Bandyopadhyay, 2010b) aims to manually annotate the sentences in a web-based Bengali blog corpus with the emotional components such as emotional expression (word/phrase), intensity, associated holder and topic(s). (Das and Bandyopadhyay, 2010a) built a lexicon of words to support the task of Telugu sentiment analysis and is made available to the public. (Das and Bandyopadhay, 2010) created an interactive gaming to technology (Dr. Sentiment) to create and validate Senti-WordNet for Telugu. Data Collection In this section, we will explore the different resources where raw data was obtained from and how processing of that data was done, as shown in Figure 1 . Currently, most of the corpora available for Sentiment Analysis are harvested from sources like review data from e-commerce websites where customers express their opinion on products freely, posts from social networking sites like Twitter and Facebook. Although the news genre has received much less attention within the Sentiment Analysis community, news plays an important role in exhibiting the reality and has a strong influence on social practices. Also, a lot of Telugu data is available mostly on news websites. These reasons motivated us to select news genre for building our corpus. We scraped and harvested our raw data from five different Telugu news websites viz., Andhrabhoomi 2 , Andhrajyothi 3 , Eenadu 4 , Kridajyothi 5 and Sakshi 6 . In total we have collected over 453 news articles and filtered down to 321 which were relevant to our work. The extracted data was cleaned in a preprocessing step, e.g. by removing headings and sub-headings, eliminating sentences with non-Telugu words and cleaning any extra dots, extra spaces, URLs, and other garbage values. Later Sentence Segmentation is done where this data was split into individual sentences. The sentences thus obtained were now tested for objectivity manually. Objective sentences are sentences where no sentiment, opinion, etc. is expressed. They state a fact confidently and has an evidence to support it. For example, sentence (1) is an objective sentence as it is a verifiable fact with evidence. \u0c05 \u0c15 \u0c02 \u0c30\u0c24 \u0c36 \u0c05\u0c27 \u0c2a (1) Transliteration: Abdul kal\u0101m\u0307bh\u0101ratad\u0113\u015ba adhyak\u1e63u\u1e0dig\u0101 panic\u0113\u015b\u0101ru English: Abdul Kalam served as the president of India Annotation In this section, we describe the process followed for annotating the sentences (refer Figure 1 ). First, we built a team of seven educated native Telugu speakers for the task of polarity tagging of the extracted Telugu sentences. Then, we developed an annotation schema for this task and the annotators were instructed to thoroughly understand the concepts mentioned in the schema for a precise/perfect annotation. Each sentence is annotated by two annotators. The annotators were required to tag the sentences with three polarities: positive, negative, neutral. For example, sentence (2) should be tagged positive as it expresses positive sentiment by the use of \u0c15\u0c43\u0c24\u0c1c \u0c24 (gratitude). \u0c2e\u0c02 , \u0c06\u0c2f\u0c28 \u0c0e \u0c28 \u0c02 , (2) \u0c2a \u0c1c\u0c32 \u0c15\u0c43\u0c24\u0c1c \u0c24 \u0c35 \u0c15 \u0c02 Transliteration: Mantri, \u0101yananu ennukunnanduku, prajalaku kr taj\u00f1ata vyaktam\u0307c\u0113\u015b\u0101ru English: The minister expressed gratitude to the people for electing him On the other hand sentence (3) should be tagged negative because it expresses negative sentiment with \u0c06\u0c02 \u0c33\u0c28 (concern). \u0c30\u0c02\u0c24\u0c30 \u0c24\u0c32\u0c2a\u0c46 \u0c56 \u0c2a \u0c1c \u0c06\u0c02 \u0c33\u0c28 \u0c35 \u0c15 \u0c02 (3) Transliteration: Nirantara vidyut k\u014dtalapai prajalu \u0101nd\u014d\u1e37ana vyaktam\u0307c\u0113\u015b\u0101ru English: People have expressed concern over continuous power cuts However, sentence (4) is a neutral sentence as it is a speculation about the future. Even though it doesn't contain any sentiment, it is not an objective sentence because it is not a verifiable fact or not something which happened in the past. It is speculating something to happen in the future. \u0c2a \u0c35 \u0c28\u0c46 \u0c32 \u0c1a\u0c46 \u0c56 \u0c38\u0c02\u0c26 \u0c02\u0c1a (4) Transliteration: Pradh\u0101ni vacc\u0113 nelal\u014d cain\u0101nu sandar\u015bincanunn\u0101ru English: The prime minister is expected to visit China next month If in any case annotators were unsure or felt ambiguous about the polarity of a sentence they can label it uncertain. If they feel the sentence is objective but was not removed in Annotators labelled all the sentences, with each sentence annotated by exactly two annotators. We call it annotation step. The sentences marked uncertain by at least one annotator were discarded to avoid any ambiguous sentences in the corpus. The sentences which had a clash between the two annotators' labels were sent for a third independent annotation which we call as a validation step. The most common label among the three annotators was considered as the final label for the sentence. If even after the third annotation the disagreement prevailed, such sentences were discarded as we considered them too ambiguous for getting three different labels by three different annotators. If there were any objective sentences after the validation step, they were discarded. Table 1 shows some example annotations from the corpus. Agreement Study After annotation task, we measured how reliable our annotation scheme was. To measure the reliability of our polarity annotation scheme, we conducted an inter-annotator agreement study on the annotated sentences. Table 2 shows the agreement for the two annotators' judgments for each sentence. We used Cohens\u00b4kappa, \u03ba which is calculated using formula (5) \u03ba = p o \u2212 p e 1 \u2212 p e (5) where p o is the relative observed agreement and p e is the agreement by chance. In general, \u03ba values between 0.6 and 0.8 are considered a substantial agreement. To our surprise we got the \u03ba value to be 0.87, which is in perfect agreement and is an indication of the reliability of the annotations. Corpus Statistics In this section, we present the statistics about our data from raw data collection to final sentences. We scraped several websites for the data. We collected 453 news articles and filtered down to 321 which were relevant for our work. After pre-processing this raw data, we have 11952 sentences. We tested the sentences for subjectivity (as explained in section 3) and removed 4327 objective sentences after which we were left with 7812 sentences. These sentences were given to the annotators for the annotation as mentioned in section 4. 1802 sentences were removed where at least one annotator marked it uncertain. In the remaining 5823 sentences, 512 were with disagreement and were sent for third independent annotation. After the third annotation, 413 sentences were discarded if the disagreement prevailed or if they are objective. The final 5410 sentences forms the required annotated corpus, ACTSA. Statistics about our complete corpus can be found in Table 3 . Experiments and Evaluation A strategy that can give very useful hints about the reliability of the annotated data is the comparison between the results of automated classification and human annotation. (Mukku et al., 2016) described a method to perform automated classification of Telugu sentences into polarity tags: positive, negative and neutral. We followed this method to evaluate our data. We used 2000 sentences from our human automated corpus to train the model for automated classification. To test the reliability of our annotated data, we compared the classification expressed by humans and that of the automated classifier trained above. The testing was done on the remaining 3410 sentences and the error rate was observed to be 12.3% which hints the quality and reliability of the annotated corpus, ACTSA. Conclusion In this work, we presented a gold standard corpus of Telugu sentences taken from different resources, which were then cleaned and annotated by native Telugu speakers. For each sentence, we have a polarity label attached with it. We described our annotation process and gave an overview of our annotation schema. The results from our evaluation study show that our corpus has a reasonable interannotator agreement. The corpus and guidelines are publicly available. In future, we try to automate the task of annotation for new sentences with the help of ACTSA. We would also like to perform sentiment analysis task for Telugu, using this corpus. Acknowledgements We would like to thank all the members of annotation group (native Telugu speakers) for their tireless effort in this process. The work reported in this paper was supported by Kohli Center on Intelligent Systems.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 1.9361263126072004e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, or foundations. Therefore, it appears that the article does not report funding from any of the specified sources."
}