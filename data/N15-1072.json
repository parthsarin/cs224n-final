{
    "framing_question": "If this paper introduces a dataset, what background and expertise is prioritized in data annotators, if any?",
    "response": "The paper does not introduce a new dataset nor does it mention the background or expertise prioritized in data annotators.",
    "article": "Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators. However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems. We propose two mechanisms to reduce the cost of crowdsourcing while maintaining high translation quality. First, we develop a method to reduce redundant translations. We train a linear model to evaluate the translation quality on a sentenceby-sentence basis, and fit a threshold between acceptable and unacceptable translations. Unlike past work, which always paid for a fixed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough. Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences. This also allows us to rank translators, so that we re-hire only good translators to reduce cost. Introduction Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost. Many NLP researchers have started creating speech and language data through crowdsourcing (for example, Snow et al. (2008) , Callison-Burch and Dredze (2010) and others). One NLP application that has been the focus of crowdsourced data collection is statistical machine translation (SMT) which requires large bilingual sentence-aligned parallel corpora to train translation models. Crowdsourcing's low cost has made it possible to hire people to create sufficient volumes of translation in order to train SMT systems (for example, Ambati and Vogel (2010) , Zbib et al. (2012) , Post et al. (2012) , Zbib et al. (2013) ). However, crowdsourcing is not perfect, and one of its most pressing challenges is how to ensure the quality of the data that is created by it. Unlike in more traditional employment scenarios, where annotators are pre-vetted and their skills are clear, in crowdsourcing very little is known about the annotators. They are not professional translators, and there are no built-in mechanisms for testing their language skills. They complete tasks without any oversight. Thus, translations produced via crowdousrcing may be low quality. Previous work has addressed this problem, showing that non-professional translators hired on Amazon Mechanical Turk (MTurk) can achieve professional-level quality, by soliciting multiple translations of each source sentence and then choosing the best translation (Zaidan and Callison-Burch, 2011) . In this paper we focus on a different aspect of crowdsourcing than Zaidan and Callison-Burch (2011) . We attempt to achieve the same high quality while minimizing the associated costs. We propose two complementary methods: (1) We reduce the number of translations that we solicit for each source sentence. Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one. We do so by building models to distinguish between acceptable translations and unacceptable ones. (2) We reduce the number of workers we hire, and retain only high quality translators by quickly identifying and filtering out workers who produce low quality translations. Our work stands in contrast with Zaidan and Callison-Burch (2011) who always solicited and paid for a fixed number of translations for each source sentence, and who had no model of annotator quality. In this paper we demonstrate that: \u2022 Our model can predict whether a given translation is acceptable with high accuracy, substantially reducing the number of redundant translations needed for every source segment. \u2022 Translators can be ranked well even when observing only small amounts of data. Compared with a gold standard ranking, we achieve a correlation of 0.94 after seeing the translations of only 20 sentences from each worker. Therefore, bad workers can be filtered out quickly. \u2022 We can achieve a similar BLEU score as Zaidan and Callison-Burch (2011) at half the cost using our cost optimizing methods. Problem Setup We start with a corpus of source sentences to be translated, and we may solicit one or more translation for every sentence in the corpus. Our targeted task is to assemble a single high quality translation for each source sentence while minimizing the associated cost. This process can be repeated to obtain multiple high quality translations. We study the data collected by Zaidan and Callison-Burch (2011) through Amazon's Mechanical Turk. They hired Turkers to translate 1792 Urdu sentences from the 2009 NIST Urdu-English Open Machine Translation Evaluation set 1 . A total of 52 Turkers contributed translations. Turkers also filled out a survey about their language skills and their countries of origin. Each Urdu sentence was translated by 4 non-professional translators (the Turkers) and 4 professional translators hired by the LDC. The cost of non-professional translation was $0.10 per sentence and we estimate the cost of professional 1 LDC Catalog number LDC2010T23 translation to be approximately $0.30 per word (or $6 per sentence, with 20 words on average). Following Zaidan and Callison-Burch (2011) , we use BLEU (Papineni et al., 2002) to gauge the quality of human translations. We can compute the expected quality of professional translation by comparing each of the professional translators against the other 3. This results in an average BLEU score of 42.38. In comparison, the average Turker translations score only 28.13 without quality control. Zaidan and Callison-Burch trained a MERT (Och, 2003; Zaidan, 2009) model to select one nonprofessional translation out of the four and pushed the quality of crowdsourcing translation to 38.99, closer to the expected quality of professional translation. They used a small amount of professional translations (10%) as calibration data to estimate the goodness of the non-professional translation. The component costs of their approach are the 4 nonprofessional translations for each source sentence, and the professional translations for the calibration data. Although Zaidan and Callison-Burch demonstrated that non-professional translation was significantly cheaper than professionals, we are interested in further reducing the costs. Cost reduction plays an important role if we want to assemble a large enough parallel corpus to train a statistical machine translation system which typically require millions of translated sentences. Here, we introduce several methods for reducing the number of nonprofessional translations while still maintaining high quality. Estimating Translation Quality We use a linear regression model 2 to predict a quality score (score(t) \u2208 R) for an input translation t. score(t) = w \u2022 f (t) where w is the associated weight vector and f (t) is the feature vector of the translation t. We replicate the feature set used by Zaidan and Callison-Burch (2011) in their MERT model: \u2022 Sentence-level features: 9 features based on language model, sentence length and edit distance to other translations. \u2022 Worker-level features: 15 features based on worker's language ability, location and average sentence-level scores. \u2022 Ranking features: 3 features based on the judgments of monolingual English speakers' ranking the translations from best to worst. \u2022 Calibration features: 1 feature based on the average BLEU score of a worker's translations provided is computed against professional references. We additionally introduce a new bilingual feature based on IBM Model 1. We align words between each candidate translation and its corresponding source sentence. The bilingual feature is the average of its alignment probabilities between words in the source sentence and words in the Turker's translation. In Figure 1 , we show how the bilingual feature allows us to distinguish between a valid translation (top) and an invalid/spammy translation (bottom). Reducing the Number of Translations The first way that we optimize cost is to solicit fewer redundant translations. The strategy is to recognize when we have got a good translation of a source sentence and to immediately stop purchasing additional translations of that sentence. The crux of this method is to decide whether a translation is 'good Algorithm 1 How good is good enough Input: \u03b4, the allowable deviation from the expected upper bound on BLEU score (using all redundant translations); \u03b1, the upper bound BLEU score; a training set S = { f s i,j , y s i,j ) j=1..m i=1..n } and a validation set V = {( f v i,j , y v i,j ) j=1..m i=1..n } where f i,j is the feature vector for t i,j which is the jth translation of the source sentence s i and y i,j is the label for f i,j . Output: \u03b8, the threshold between acceptable and unacceptable translations; w, a linear regression model parameter. for i \u2190 1 to n do 6: for j \u2190 1 to m do 7: if w \u2022 f v i,j > \u03b8 \u2227 j < m then select t v i,j for s i and break 8: if j == m then select t v i,m for s i 9: q \u2190 calculate translation quality for V 10: if q > \u03b4 \u2022 \u03b1 then break 11: else \u03b8 = \u03b8 + stepsize 12: w \u2190 train a linear regression model on S \u222a V 13: Return: \u03b8 and model parameter w enough,' in which case we do not gain any benefit from paying for another redundant translation. Our translation reduction method allows us to set an empirical definition of 'good enough'. We define an Oracle upper bound \u03b1 to be the estimated BLEU score using the full set of non-professional translations. We introduce a parameter \u03b4 to set the allowable degradation in translation quality. We train a model to search for a threshold \u03b8 between acceptable and unacceptable translations for a specific value of \u03b4. For instance, we may fix \u03b4 at 95%, meaning that the resulting BLEU score should not drop below 95% of the \u03b1 after reducing the number of translations. For a new translation, our model scores it, and if its score is higher than \u03b8, then we do not solicit another translation. Otherwise, we continue to so- licit translations. Algorithm 1 details the process of model training and searching for \u03b8. Experiments We divide data into a training set (10%), a validation set (10%) and a test set (80%). Each source sentence has four translations in total. We use the validation set to search for \u03b8. The Oracle upper bound on BLEU is set to be 40.13 empirically. We then vary the value of \u03b4 from 90% to 100%, and sweep values of \u03b8 by incrementing it in step sizes of 0.01. We report results based on a five-fold cross validation, rotating the training, validation and test sets. Baseline and upper bound The baseline selection method of randomly picking one translation for each source sentence achieves a BLEU score of 29.56. To establish an upper bound on translation quality, we perform an oracle experiment to select best translation for each source segment from full sets of candidates. It reaches a BLEU score of 40.13. Translation reducing method Table 1 shows the results for translation reducing method. The \u03b4 variable correctly predicts the deviation in BLEU score when compared to using the full set of translations. If we set \u03b4 < 0.95 then we lose 2 BLEU points, but we cut the cost of translations in  half, since we pay for only two translations of each source segment on average. Choosing Better Translators The second mechanism that we use to optimize cost is to reduce the number of non-professional translators that we hire. Our goal is to quickly identify whether Turkers are good or bad translators, so that we can continue to hire only the good translators and stop hiring the bad translators after they are identified as such. Before presenting our method, we first demonstrate that Turkers produce consistent quality translations over time. Turkers' behavior in translating sentences Do Turkers produce good (or bad) translations consistently or not? Are some Turkers consistent and others not? We used the professional translations as a gold-standard to analyze the individual Turkers, and we found that most Turkers' performance stayed surprisingly consistent over time. Figure 2 illustrates the consistency of workers' quality by plotting quality of their individual translations on a timeline. The translation quality is com-puted based on the BLEU against professional translations. Each tick represent a single translation and depicts the BLEU score using two colors. The tick is black if its BLEU score is higher than the median and it is red otherwise. Good translators tend to produce consistently good translations and bad translators rarely produce good translations. Evaluating Rankings We use weighted Pearson correlation (Pozzi et al., 2012) to evaluate our ranking of workers against gold standard ranking. Since workers translated different number of sentences, it is more important to rank the workers who translated more sentences correctly. Taking the importance of workers into consideration, we set a weight to each worker using the number of translations he or she submitted when calculating the correlation. Given two lists of worker scores x and y and the weight vector w, the weighted Pearson correlation \u03c1 can be calculated as: \u03c1(x, y; w) = cov(x, y; w) cov(x, x; w)cov(y, y; w) where cov is weighted covariance: cov(x, y; w) = i w i (x i \u2212 m(x; w))(y i \u2212 m(y; w)) i w i (2) and m is weighted mean: m(x; w) = i w i x i i w i (3) Automatically Ranking Translators We introduce two approaches to rank workers using a small portion of the work that they submitted. The strategy is to filter out bad workers, and to select the best translation from translations provided by the remaining workers. We propose two different ranking methods: Ranking workers using their first k translations We rank the Turkers using their first few translations by comparing their translations against the professional translations of those sentences. Ranking workers on gold standard data would allow us to discard bad workers. This is similar to the idea of a qualification test in MTurk. Ranking workers using a model In addition to ranking workers by comparing them against a gold standard, we also attempt to automatically predict their ranks with a model. We use the linear regression model to score each translation and rank workers by their model predicted performance. The model predicted performance of the worker w is: perf ormance(w) = t\u2208Tw score(t) |T w | (4) where T w is the set of translations completed by the worker w and score(t) is the model predicted score for translation t. Experiments After we rank workers, we keep top-ranked workers and select the best translation only from their translations. For both ranking approaches, we vary the number of good workers that we retain. We report both rankings' correlation with the gold standard ranking. Since the top worker threshold is varied and since we change the value of k in first k sentence ranking, we have a different test set in different settings. Each test set excludes any items which were used to rank the workers, or which did not have any translations from the top workers according to our rankings. Gold standard and Baseline We evaluate ranking quality using the weighted Pearson correlation (\u03c1) compared with the gold standard ranking of workers. To establish the gold standard ranking, we score each Turker based on the BLEU score comparing all of his or her translations to the corresponding professional references. We use the ranking by the MERT model developed by Zaidan and Callison-Burch (2011) as baseline. It achieves a correlation of 0.73 against the gold standard ranking. Ranking workers using their first k translations Without using any model, we rank workers using their first k translations. We select best translation of each source sentence from the top ranked worker who translated that sentence. Table 2 shows the results of Pearson correlations for different value of k. As k increases, our rankings Gold Ranking First 20 Sentences Ranking q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Figure 3: Correlation between gold standard ranking and ranking computed using the first 20 sentences as calibration. Each bubble represents a worker. The radius of each bubble shows the relative volume of translations completed by the worker. The weighted correlation is 0.94. Gold Ranking Model Ranking q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Figure 4: Correlation between gold standard ranking and our model's ranking. The corresponding weighted correlation is 0.95. fit the gold ranking better. Consequently, we can decide whether to continue to hire a worker in a very short time after analyzing the first k sentences (k \u2264 20) provided by each worker. Figure 3 shows the correlation of gold ranking and the ranking based on workers' first 20 sentences. Ranking workers using a model We train a linear regression model on 10% of the data to rank workers. We use the model to select the best translation in one of two ways: \u2022 Using the model's prediction of workers' rank, and selecting the translation from the best worker. \u2022 Using the model's score for each translation and selecting the highest scoring translation of each source sentence. Table 3 shows that the model trained on all features achieves a very high correlation with the gold standard ranking (Pearson's \u03c1 = 0.95), and a BLEU score of 39.80. Figure 4 presents a visualization of the gold ranking and model ranking. The workers who produce the largest number of translations (large bubbles in the figure) are ranked extremely well. Filtering out bad workers Ranking translators would allow us to reduce costs by only re-hiring top workers. Table 4 shows what happens when we vary the percentage of top ranked workers we retain. In general, the model does a good job of picking the best translations from the remaining good translators. Compared to actually knowing the gold ranking, the model loses only 0.55 BLEU when we filter out 75% of the workers. In this case we only need to solicit two translations for each source sentence on average. Cost Analysis We have introduced several ways of significantly lowering the costs associated with crowdsourcing translations when a large amount of data are solicited (on the order of millions of samples): \u2022 We show that after we have collected one translation of a source sentence, we can consult a model that predicts whether its quality is sufficiently high or whether we should pay to have the sentence re-translated. The cost savings for non-professionals here comes from reducing the number of redundant translations. We  can save almost half of the cost associated with non-professional translations to get 95% of the translation quality using the full set of redundant translations. \u2022 We show that we can quickly identify bad translators, either by having them first translate a small number of sentences to be tested against professional translations, or by estimating their performance using a feature-based linear regression model. bad workers. Similarly, we reduce the nonprofessional translation cost to the half of the original cost. \u2022 In both cases we need some amount of professionally translated materials to use as a gold standard for calibration. Although the unit cost for each reference is much higher than the unit cost for each non-professional translation, the cost associated with non-professional translations can dominate the total cost since the large amount of data need to be collected. Thus, we focus on reducing cost associated with nonprofessional translations. 7 Related Work 2013 ) also proposed an approach to detect and avoid spam workers. They measured the performance of worker by comparing worker's labels to the current majority labels. Workers with bad performance can be identified and blocked. Lin et al. ( 2014 ) examined the relationship between worker accuracy and budget in the context of using crowdsourcing to train a machine learning classifier. They show that if the goal is to train a classifier on the labels, that the properties of the classifier will determine whether it is better to re-label data (resulting in higher quality labels) or get more single labeled items (of lower quality). They showed that classifiers with weak inductive bias benefit more from relabeling, and that relabeling is more important when worker accuracy is low. Novotney and Callison-Burch (2010) showed a similar result for training an automatic speech recognition (ASR) system. When creating training data for an ASR system, given a fixed budget, their system's accuracy was higher when it is trained on more low quality transcription data compared to when it was trained on fewer high quality transcriptions. Conclusion In this paper, we propose two mechanisms to optimize cost: a translation reducing method and a translator reducing method. They have different applicable scenarios for large corpus construction. The translation reducing method works if there exists a specific requirement that the quality must reach a certain threshold. This model is most effective when reasonable amounts of pre-existing professional translations are available for setting the model's threshold. The translator reducing method is very simple and easy to implement. This approach is inspired by the intuition that workers' performance is consistent. The translator reducing method is suitable for crowdsourcing tasks which do not have specific requirements about the quality of the translations, or when only very limited amounts of gold standard data is available. Acknowledgements This material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled \"Crowdsourcing Translation\" (contract D12PC00368). The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements by DARPA or the U.S. Government. This research was supported by the Johns Hopkins University Human Language Technology Center of Excellence and through gifts from Microsoft, Google and Facebook.",
    "funding": {
        "military": 0.9999542716355735,
        "corporate": 0.9992675938404221,
        "research agency": 0.04742654038526417,
        "foundation": 8.180224541742831e-06,
        "none": 1.9361263126072004e-07
    }
}