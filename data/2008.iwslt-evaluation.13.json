{
    "article": "The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates kbest translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008. Introduction This paper presents NTT Statistical Machine Translation (SMT) System evaluated in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. The system is composed of two steps: First, k-best translation candidates are generated by a hierarchical phrase-based statistical machine translation decoder, with linear feature combination among several scores on translation and language models. Next, these candidates are reordered and the top-best candidate is chosen, according to approximated BLEU. The reranker is based on Ranking SVMs [1] with a large number of sparse binary features. A large number of sparse features has been successfully applied to SMT in both decoding [2] and reranking [3, 4] . In this year's IWSLT evaluation, we employ Ranking SVMs with fast optimization algorithm [5] for reranking, and introduce three new types of sparse features: alignment-independent word pairs, skip bigrams, and context-dependent features. Since this year's challenge task focuses on translating utterances in dialogues, we incorporate rich information available from the dialogue context into the reranker. In the evaluation on the IWSLT 2008 Chinese-to-English challenge task, our primary submission achieved 32.12% for ASR 1-best and 38.47% for clean in BLEU. However, in the official evaluation, an SVM hyperparameter was not optimized. We fixed it and finally achieved 37.41% for ASR 1-best and 44.38% for clean in a post-evaluation experiment using source-target word pair and target-side skip bigram features; Our context-dependent features did not effectively work in the experiments, because they failed to capture useful context information in the current condition. We discuss these features using distinctive examples between reranker selections and decoder 1-bests. This paper is organized as follows: Section 2 briefly describes our SMT decoder. Section 3 describes our reranking component and sparse features for reranking. Section 4 presents the results for the evaluation campaign of IWSLT 2008, followed by discussion in Section 5. Machine Translation Component Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: \u00ea = argmax e w \u2022 h(f, e) (1) where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6] . Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7] , in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X \u2192 \u03b3, \u03b1, \u223c (2) In the notation above, X is a non-terminal symbol, \u03b3 is a source-side string of terminal and non-terminal symbols, and \u03b1 is a target-side one. \u03b3 and \u03b1 share the same number of non-terminals whose one-to-one mapping is defined by \u223c. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [8] . The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7] . The decoder is an in-house developed CKY-based one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8] . The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10] . Using the extracted phrases, SCFG production rules are accumulated by finding \"holes\" in extracted contiguous phrases: \u2022 For a phrase pair ( f , \u0113), a rule X \u2192 f , \u0113 is extracted. \u2022 For a rule X \u2192 \u03b3, \u03b1 and a phrase pair ( f , \u0113) s.t. \u03b3 = \u03b3 1 f \u03b3 2 and \u03b1 = \u03b1 1 \u0113\u03b1 2 , a rule X \u2192 \u03b3 1 X k \u03b3 2 , \u03b1 1 X k \u03b1 2 is extracted. where boxed indices k indicate one-to-one mapping between non-terminals. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3] : \u2022 Hierarchical phrase translation probabilities \u2022 Lexical translation probabilities in phrase pairs \u2022 Word-based insertion/deletion penalties \u2022 Word 5-gram language model scores \u2022 Reordering penalties \u2022 Length penalties on both words and hierarchical phrases Reranking Component Our reranking component is based on Ranking SVMs [1] . Each decoder k-best translation candidate is represented by a feature vector, and the reranker chooses the best-scored candidate over k vectors. Ranking SVMs Ranking SVM is a variant of support vector machines (SVMs) for the purpose of ranking samples, not classification. However, its optimization problem is equivalent to that of a classification SVM on pairwise difference vectors (see details in [1] ). In our reranking component, we do not define the whole rank order over k-best translation candidates but only distinguish the best candidate among the rest. The best candidate is chosen based on approximated BLEU [3] , which will be explained in 3.2. If more than one (k top ) candidates has the same value of approximated BLEU, all of them are regarded as the best candidates. In this setting, only k top (k \u2212 k top ) pairwise difference vectors between the best candidates and the rest ones are used as training data. We employs Pegasos 1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5] . Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11] . The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document-wise calculation and is not suitable for sentence-level reranking. Given 1-best translation outputs for T input sentences O T 1 = {e 1 1 , ..., Reranker features We use a large number of sparse binary features for reranking, as well as a real-valued feature (decoder score). Word alignment features We use source-target word pairs extracted by separately running IBM Model 1 in both direction [4] . In addition to source-target word unigram pairs, we used pairs of targetside word bigram and their corresponding source-side words in terms of the word alignment. We also include POS-based features, target-side word surfaces are replaced with their POS tags in the word alignment features above. Target-side (English) POS tags are automatically annotated by Brill Tagger. Word pair features Since the word alignment features highly depends on IBM Model 1 word alignments, the features are influenced by word alignment errors. As alignment independent features, we use all possible word unigram and bigram pairs between source-side words and target-side words. We also use POSbased features in the same way as the word alignment features. Target-side skip bigram features We use target-side skip bigrams, which are allowed to skip up to two words, as features. POS-based skip bigrams are also used. Context-dependent word pair features To define context-dependent features, we simply use bag-ofwords of both source-and target-side words in the previous sentence. Target-side context words are extracted from kbest translation candidates. For each pair of context word in the bag-of-words and current target-side word, context word pair feature is defined. Evaluation We present evaluation results of our system on IWSLT 2008 Chinese-to-English challenge task, for field-experiment data in tourism domain. Setup Training and development data came from only IWSLT supplied data for Chinese-to-English challenge task shown in Table 1 , and no extra data resources were used. Chinese sentences are re-tokenized by our in-house developed tool [12] . For estimating feature weights in Eq.( 1 ) by minimum error rate training, we used the development sets 3, 6, and CT CE with an intermediate phrase-table and word 5-gram language model trained using the other training and development sets (1, 2, 4, and 5). Throughout this experiment we share the same feature scaling, instead of re-running minimum error rate training for each different setting. For training Ranking SVMs, we used 100-best outputs for the development set CT CE from the decoder with the estimated feature weights and the models trained using the other training and development sets (1) (2) (3) (4) (5) (6) . For the final decoder, we used all supplied data for training the phrase-table and word 5-gram language model. Source-side test set perplexity by the final language model was 56.2, and 13 (0.5%) Chinese words were not found in the vocabulary. All word 5-gram language models above were trained by SRILM with modified Kneser-Ney smoothing. We compared four methods with varying reranking features. (1) Using 1-best decoder output and did not apply reranking. (2) Using the decoder score and word alignment features, as a baseline of reranking. (3) Adding the word pair and skip bigram features to the baseline features above (primary). (4) Using all features described in 3.3. The number of distinct features extracted from reranker training set (DevCT CE) are presented in Table 2 . Our experiments were conducted on clean and ASR 1best inputs, without consideration of ASR N-best or word lattice information. Note that our SMT decoder and reranker were trained using only supplied clean text data and ASR transcriptions were not used. Pre-evaluation results Our reranker was tuned based on cross-validation between the development sets 4 and 5 (from IWSLT 2006) 2 . The translation and language models for the cross-validation were trained on the training and development sets 1,2,3, and 6. In this condition, the reranker achieved better BLEU than decoder 1-bests as shown in the cross-validation results on Table 3 . The results with context features were slightly worse than those without them in BLEU, so that we abandoned these features in our primary setting. Official results Our results in BLEU [11] are presented in Table 4 . We achieved 35.62 % for ASR 1-best input and 42.16 % for clean input without reranking. Our reranking approach showed worse results than decoder 1-bests; the primary results were Post-evaluation results In the official evaluation, a Pegasos's regularization hyperparameter \u03bb (\u03bb = 1/mC, where m is the number of training samples and C is the soft margin parameter) was not optimized; we used its default value \u03bb = 0.01. Although we could increase BLEU with the default value in the preevaluation experiment, it turned to be inappropriate in the official evaluation. We conducted another post-evaluation experiment with the hyperparameter \u03bb that was optimized to maximize BLEU averaged over two-and three-fold cross-validation 3 on De-vCT CE. The post-evaluation results in BLEU are presented in Table 5 . We achieved 37.41 % for ASR 1-best input and 44.38 % for clean input with our primary setting. Thus, our reranking approach worked well with an appropriate SVM hyperparameter. Discussion The post-evaluation results show that our reranking approach can improve translation performance in BLEU. To investi- \u2022 Word alignment features (in Fig. 1 ) captured lexical correspondence and the reranker (2) chose better translation candidates than decoder 1-bests in terms of adequacy. \u2022 Bigram and skip bigram features captured target-side natural word order and bigram pairs captured their source-target co-occurence, and therefore the reranker (3) chose slightly better translation candidates than the reranker (2) in terms of fluency. \u2022 Context features turned out to capture many general word co-occurence and the reranker (4) failed to distinguish better translation candidate from others. Our current context-dependent features are simply defined without dialogue boundaries and may not be useful for capturing dialogue context. Since utterances in a dialogue are considered to correlate with each other, we need further feature engineering to incorporate such correlations into rerank-ing. Another problem may be that the data consist of only one-sided utterances and do not sufficiently hold dialogue information. Conclusion We evaluated the NTT Statistical Machine Translation System for the evaluation campaign of IWSLT 2008. The system is composed of decoding and reranking components. The decoder is based on the hierarchical phrase-based approach and the linear feature combination. The reranker employs Ranking SVMs and a large number of sparse features of alignment-independent word pairs, skip bigrams, and context-dependent word pairs. Experimental results show that our reranker effectively works for choosing better translation candidates than decoder 1-bests. Our future work involves more effective sparse features, especially contextdependent ones. Acknowledgements We would like to thank the IWSLT 2008 organizing commitee for their efforts to coordinate the evaluation campaign. This work is partly supported by MEXT Grant-Aid for Scientific Research of Priority Areas.",
    "abstract": "The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates kbest translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": "7",
    "year": "2008",
    "month": "October 20-21",
    "title": "{NTT} statistical machine translation system for {IWSLT} 2008."
}