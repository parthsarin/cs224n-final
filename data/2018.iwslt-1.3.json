{
    "article": "To improve the translation adequacy in neural machine translation (NMT), we propose a rewarding model with target word prediction using bilingual dictionaries inspired by the success of decoder constraints in statistical machine translation. In particular, the model first predicts a set of target words promising for translation; then boosts the probabilities of the predicted words to give them better chances to be output. Our rewarding model minimally interacts with the decoder so that it can be easily applied to the decoder of an existing NMT system. Extensive evaluation under both resource-rich and resource-poor settings shows that (1) BLEU score improves more than 10 points with oracle prediction, (2) BLEU score improves about 1.0 point with target word prediction using bilingual dictionaries created either manually or automatically, (3) hyper-parameters of our model are relatively easy to optimize, and (4) undergeneration problem can be alleviated in exchange for increasing over-generated words. Introduction Neural machine translation (NMT) [1, 2, 3] has dramatically improved machine translation quality compared to statistical machine translation (SMT). However, current NMT systems still suffer from the adequacy problem due to inappropriate lexical choice, under-generation, and over-generation [4] . In SMT, bilingual dictionaries have been used to improve adequacy in translation as decoder constraints. Typical example is the XML markup function implemented on MOSES [5] . Inspired by the decoding constraints for SMT, we propose a rewarding model using bilingual dictionaries to address the adequacy problem in NMT. Our model rewards target words that are promising to be used in correct translations by boosting their probabilities to be output by a decoder. It predicts such target words using bilingual dictionaries that are created manually or automatically. By applying byte pair encoding (BPE) [6] to dictionaries, our model can benefit from both BPE and dictionaries. While previous studies incorporate bilingual dictionaries into NMT for translation of rare words [7, 8] and domainspecific words [9] , we do so to improve the adequacy of NMT. Hence, dictionaries are made use of translating not only specific types of words but also all words. In addition, these are methodologically different; our model simply biases the trained decoder while previous models change the inside NMT architectures and require training of the entire systems. Due to this design, our model is easy to add to trained NMT systems and compatible with BPE. Extensive evaluation on Japanese-to-English and English-to-Japanese translation has been conducted using two datasets; IWSLT (TED Talk) [10] , spoken language domain with a small set of bilingual sentences (223k), and ASPEC [11] , a scientific domain with a large set of bilingual sentences (3M). We refer to the former as a resource-poor domain and the latter as a resource-rich domain, hereafter. The results show that the rewarding model with oracle prediction of target words, where all and only target words in references are predicted, BLEU score improves more than 10 points on average in both of the resource-poor and resource-rich domains. When using bilingual dictionaries created manually or automatically in the rewarding model to predict target words, BLEU scores improve about 1.0 point on average in both domains. Detailed analysis of our model reveals that it is relatively insensitive to settings of its hyper-parameters and easy to optimize. In addition, it is shown that our model decreases the number of under-generated words while tends to increase the number of over-generated words. Neural Machine Translation The encoder-decoder model with attention [3, 12] is one of the most popular architectures in NMT. It takes an input sentence X = {x 1 , ..., x n } and generates its translation Y = {y 1 , ..., y m } as: p(Y |X; \u03b8) = m \u220f j=1 p(y j |y <j , X; \u03b8), where \u03b8 is a set of parameters and y <j = {y 1 , \u2022 \u2022 \u2022 , y j\u22121 }. Given a parallel corpus C = {(X, Y )}, the training objective minimizes the cross-entropy loss with regard to \u03b8: L \u03b8 = \u2211 (X,Y )\u2208C \u2212 log p(Y |X). s 0 s j+1 y j ! \" #-1 ! \" # ! \" #+1 \u2026 \u2026 \u2026 \u2026 Encoder Decoder x i-1 x i x i+1 s j \u2026 \u2026 x 1 x 2 x n Word Prediction Rewarding Model Softmax layer D f2e \u2026 y j-1 y j $ % \" i-1 % \" i % \" i+1 Figure 1 : Rewarding model at decoding step j: predicted target words D f 2e are rewarded to have better chances to be output at each decoding time step. Note that the attention model is omitted for clarity. The model consists of three parts, namely, an encoder, a decoder, and an attention model. The encoder has an embedding layer and an recurrent neural network (RNN) layer. The former converts words into their continuous space representations. Taking these embeddings, the RNN layer then computes a state that represents the input sequence till the current time step. Specifically, we use the bi-directional long shortterm memory (LSTM) [13] that encodes the source sentence by forward and backward directions. At time step i, the state is represented by concatenating the forward hidden state \u0125i and the backward one hi as h i = [ \u0125i ; hi ]. In this manner, X can be represented as h = {h 1 , ..., h n }. The decoder remembers all the history of translation and its softmax layer computes the posterior probability p(y j |y <j , X) of a word y j to output as translation. In order to focus on specific parts of the input sentence necessary for translation, the attention model is incorporated. We use the global attention mechanism proposed in [12] . Rewarding Model On top of a decoder, our model rewards predicted words so that they have better chances to be output as translations as shown in Figure 1 . Specifically, it first predicts a set of target words D f 2e that are promising to be used in translations using bilingual dictionaries. Then, our model rewards a target word if it is contained in D f 2e by adding weight to the posterior probability: Q(y j |y <j , X) = log p(y j |y <j , X) + \u03bbr yj , (1) where \u03bb is the weight of reward that will be tuned using a development set. This means that our model boosts the probabilities of predicted words that might have been slipped away during beam search in the conventional decoder. In [14] , a similar rewarding model is proposed, but rewards are based on remaining sequence lengths. We use a simple binary rewarding in this paper: r yj = { 1 (y j \u2208 D f 2e ), 0 (otherwise). (2) We also tried to model the rewarding function using lexical translation probabilities that can be estimated for automatically created dictionaries. However, preliminary experiments empirically showed that this simple form of rewarding worked best. This may be because these probabilities are modeled in completely different ways, i.e., p(y j |y <j , X) in Equation ( 1 ) is conditioned on the entire source sentence while lexical translation probabilities are conditioned on source words. Further investigation is our future work. Finally, a target word is output as: y j = arg max yj Q(y j |y <j , X). Accurate prediction of D f 2e is crucial for our rewarding model. In the next section, we discuss practical implementations to obtain D f 2e from dictionaries. Target Word Prediction with Dictionaries In this study, we look up bilingual dictionaries created manually or automatically as word prediction, which allows to make our model minimally interact with the original NMT system. We will consider a sophisticated prediction model using an information in the encoder in future [15] . Prediction with Manually Created Dictionary Thanks to the accumulated efforts by the academia and industry, bilingual dictionaries have been manually created for language pairs of English and Japanese. Such manual bilingual dictionaries provide reliable translation knowledge, although their coverage is limited. One disadvantage of manual dictionaries is that conjugation and derivative forms are generally not provided in such dictionaries. As a simple way to predict the target word set, we look up source words in a manual bilingual dictionary. Prediction with Automatically Created Dictionary Previous studies have proposed methods to automatically construct bilingual dictionaries. Especially, word alignment techniques for SMT [16, 5] allow us to construct a dictionary directly from a parallel corpus. Similar alignment may be possible using the attention model in NMT, however, reliability is not assured because the attention model is rather soft as a constraint [17, 18] . The biggest advantage of using word alignment for dictionary construction is that the domain of the dictionary matches that of translation targets. In addition, conjugations are available in the dictionary. A disadvantage is that alignment errors may decrease the quality of the dictionary. We apply the GIZA++ toolkit 1 that is an implementation of the IBM alignment models [16] on a parallel corpus to automatically create a bilingual dictionary. To control the precision and recall of target word prediction, we introduce a threshold \u03b4, which is tuned on development data. Target words with lower translation probability than \u03b4 are discarded. Exact and Partial Matching with BPE Conducting translation on sub-words is effective to address the unknown word problem [19] . We apply BPE [6] to dictionaries for word prediction to make our rewarding model compatible to BPE-based NMT. For both the dictionary entries and source sentences, we first apply a BPE model trained on a parallel corpus and then match the entries in dictionaries and source sentences. We use two types of matching methods between an input sentence and dictionary entries: exact match and partial match. The former is precision-oriented and the latter is recall-oriented. After applying BPE, a dictionary headword (lemma) consists of multiple sub-words; a lemma w is denoted as w = w 1 , . . . , w k . Exact match regards w as matched to a source sentence X if and only if: w 1 , . . . , w k \u2208 X, s.t., for \u2200i \u2208 {1, . . . , k \u2212 1}, w i = x j \u21d4 w i+1 = x j+1 . On the other hand, partial match regards w as matched to X if w i \u2208 X for \u2203w i \u2208 w. In both matching methods, translations of w are added to the target word set as predictions. Obviously, target word predictions by partial match subsumes those by exact match. Experiment Settings To investigate the effects of our model, we conducted Japanese-to-English and English-to-Japanese translation experiments on resource-poor and resource-rich domains. Translation Tasks The resource-poor task used the IWSLT 2017 Japanese-English task from the WIT project [10] . The IWSLT task provides 223k parallel sentences for training. We used the dev 2010 and test 2010 sets for development and testing, containing 871 and 1, 549 sentences, respectively. The resource-rich task used the Japanese-English paper excerpt corpus (ASPEC) 2 [11] , which is one subtask of the workshop on Asian translation (WAT) 3 [20] . For training, we used the first 2M parallel sentence pairs among the entire 3M pairs sentences following [21] , because the remaining 1M sentences were noisy. The ASPEC task provides 1, 790, and 1, 812 sentences for development and testing, respectively. We conducted both Japanese-to-English and Englishto-Japanese translation experiments on these two tasks, referred to as IWSLT-JE, IWSLT-EJ, ASPEC-JE, and ASPEC-EJ for short, hereafter. NMT and Rewarding Model We used the mlpnlp-nmt system 4 that is an LSTM based encoder-decoder NMT model with attention, which achieved the best translation performance in human evaluations for both the ASPEC-JE and ASPEC-EJ tasks at WAT 2017 [20] . 5  We implemented our rewarding model on top of the mlpnlpnmt system (our implementation will be public upon acceptance of the paper). We followed the hyper-parameter settings of [21] . The sizes of the source and target side embeddings, the LSTM hidden states, the attention hidden states were all set to 512. We used 2-layer LSTMs for both the encoder and decoder with beam size of 5. Stochastic gradient descent was used as the learning algorithm, with an initial learning rate of 1.0, gradient clipping of 5.0, and a dropout rate of 30% for the inter-layer dropout. The mini batch size was 128. The training epochs for IWSLT-JE, IWSLT-EJ, ASPEC-JE, and ASPEC-EJ were all set to 20, and we chose the model with the best development BLEU score among all the epochs as the baseline systems. 6  For the rewarding models, \u03bb in Equation ( 1 ) was tuned on the development sets from 0.1 to 1.0 by 0.1 interval. The threshold \u03b4 that prunes the automatically constructed dictionaries in Section 4.2 was tuned on 0, 0.0001, 0.001, 0.01 and 0.1. We selected the best combination among all combinations of \u03b4 and \u03bb on the development set for each model. We investigate the upper-bound performance of our rewarding model using oracle target word prediction. On this oracle model, predicted target words are all and only words in a reference translation, i.e., precision and recall of prediction are both 100%. The best weight of \u03bb was searched from 0.1 increasing the value by 0.1 until we observed a decrease in BLEU scores. As preprocessing for the parallel corpora and bilingual dictionaries, we segmented Japanese sentences/entries using MeCab, 7 and tokenized and truecased the English sen- tences/entries with the truecase.perl script in Moses 8 for both translation tasks. We further split the words into subwords using joint BPE [6] with 32, 000 merge operations. The vocabulary sizes of the IWSLT-JE task were 21, 534 and 18, 022, respectively. The vocabulary sizes of ASPEC-JE task were 28, 852 and 22, 340, respectively. Bilingual Dictionaries As the manual dictionary, we used EDR, 9 which is the publicly available English and Japanese bilingual dictionary. 10 The numbers of English-to-Japanese and Japaneseto-English entry pairs are 676k and 1, 052k, respectively. In EDR, only lemmas are provided and thus inflected forms of English verbs are unavailable. To address this issue, inflected forms of the EDR lemmas are extracted from the English dictionary of XTAG project, 11 which is used as the English morphological analysis dictionary for TreeTagger. 12 All the possible inflected forms are added into our dictionary. For dictionary look-up, a source sentence is first lemmatized and matched with the dictionary. We used MeCab for Japanese and TreeTagger for English to lemmatize words. To automatically construct bilingual dictionaries, 13 we used the GIZA++ toolkit on the training corpus in both English-to-Japanese and Japanese-to-English directions. 14  We applied the \"grow-diag-final-and\" heuristic and obtained lexical translation probabilities using Moses. We then prune translation pairs with low probabilities by \u03b4. Results We first investigate the effect of \u03bb using the development sets on both the oracle target word sets and our word prediction methods. Next, we evaluate the translation quality on the test sets using the optimized \u03bb. Finally, we conduct detailed analysis of translation results by our rewarding model. Throughout the section, the BLEU-4 score was used as the evaluation metric, which was computed using the multibleu.perl script in Moses on tokenized and truecased English and word-segmented Japanese sentences, respectively. The significance tests were performed using the bootstrap resampling [22] at p < 0.01. Effects of \u03bb Figure 2 shows the BLEU scores by the oracle word rewarding on the development sets of the IWSLT-JE, IWSLT-EJ, ASPEC-JE, and ASPEC-EJ tasks. The BLEU scores significantly improved according to the \u03bb. The best settings of \u03bb improves 6.00, 8.25, 9.80, and 11.77 BLEU scores on the IWSLT-JE, IWSLT-EJ, ASPEC-JE, and ASPEC-EJ tasks from each baseline system, respectively. Figure 3 shows the BLEU scores with respect to the \u03bb and precision/recall of word prediction on our model with word prediction using manually or automatically created dictionaries. EDR indicates the models predicting target words using EDR. XTAG indicates the models using EDR extended with XTAG, which are only for the Japanese-to-English direction. GIZA indicates the models that predict target words using automatically constructed dictionary by GIZA++. The suffixes e and p in the legends indicate exact match and partial match, respectively. The results show that BLEU scores depend on precision and recall of target word prediction by different dictionaries. The weights of \u03bb that achieved the best BLEU scores varied from 0.1 to 1.0. Notice that these weights are much smaller than the oracle prediction, which are 0.5, 0.4, 0.4, and 0.5 for IWSLT-JE, IWSLT-EJ, ASPEC-JE, and ASPEC-EJ on GIZA partial-match, respectively. This is because predicted words are less reliable and too much rewarding degrades the trans- 3 : BLEU scores by our rewarding models with word prediction using bilingual dictionaries when changing the \u03bb on the development sets. The gentle convex curves of BLEU scores show that the weight of \u03bb is tunable by a simple grid search. lation quality. The gentle convex curves of BLEU scores also show that \u03bb is easily tunable using a simple grid search. Word Prediction and Translation Results Table 1 shows the comparison of BLEU scores on the test sets of the baseline and the rewarding models. We also report the results that use a merged dictionary. We chose the XTAG partial and GIZA partial for Japanese-to-English, EDR partial and GIZA partial for English-to-Japanese for merging because of their individual good performance. We tuned the \u03bb for merged dictionary using the development set. We can see that compared to the baselines, most of our methods significantly improve BLEU scores. Overall, a word prediction method with high recall shows a larger improvement in BLEU score as consistently shown by comparing exact matching v.s. partial matching, as well as comparing EDR v.s. XTAG, EDR or XTAG v.s. GIZA, and GIZA v.s. merged dictionary. However, there is still a gap between rewarding by our target word prediction and rewarding by oracle prediction. Our GIZA and merged dictionary models achieve a high recall of about 90% but a very low precision of 0.1%. Improving the precision for word prediction while keeping a recall high is our future work. The baselines on ASPEC-JE and ASPEC-EJ are our reproduction of the state-of-the-art at WAT competition as single models, which are reported as achieved 27.62 and 39.71 BLEU scores in the paper. Compared to these scores, our rewarding model improved 0.67 and 0.36 points, respectively. Under and Over Generation We investigated the rate of under-generation and overgeneration that are the major adequacy problems in NMT [23] using Translation Edit Rate (TER) [24] . TER aligns a reference and translation result. We counted the number of Deletion and Insertion regarding these are caused by under and over generation, respectively. This is an approximation to detect under and over generations, but we consider it is useful as an automatic and handy evaluation metric. Table 2 shows the average numbers of under and over generations per sentence. The under-generation decreases on all the rewarding models in exchange of increasing overgeneration. The rewarding model with oracle target word prediction reduces under generation about 1.2 word on average. This result shows that our rewarding model is also effective for alleviating the under-generation problem. The over-generation can be reduced by adding global constraint to the rewarding model, which prohibits rewarding the same predicted target. This is our future work. Example translations of the baseline and our rewarding model (GIZA partial match) are shown in the following. The phrase of \"congenital immunity\" and \"cancer of\" were successfully translated by our model. Source IL -1 2 \u306e \u764c \u306b\u5bfe\u3059\u308b \u62b5\u6297 \u6027 ( \u5148\u5929 \u514d\u75ab ) \u306e \u751f \u7269 \u53cd\u5fdc \u306b\u3064\u3044\u3066 \u3082 \u8003\u5bdf \u3057 \u305f Reference biological response of the resistance (congenital immunity ) to cancer of IL -12 was also examined . Baseline the biological response of the resistance to IL -12 is also discussed . Our Model the biological response of the resistance (congenital immunity ) to the cancer of IL -12 is also discussed . Related Work Our rewarding model can be viewed as a constraint on the decoder to output desired target words. There have been studies that aim to output predetermined words or phrases in neural language generation. For this purpose, the grid beam search in NMT is proposed [25] and the SMT lattice is combined into NMT [26] . In neural conversation generation, Wen et al. (2015) input a vector representing which information should be generated to an encoder [27] , and a decoder is designed to explicitly control generation of emotional words [28] . Compared with these previous studies, one benefit of our rewarding model is that the predicted words are used as soft constraints on outputs with minimal interaction to the decoder. The most relevant study from the methodological point of view is [14] that also proposes a rewarding model in a decoder of NMT to improve the translation quality in general, such as remaining sequence lengths to output. We focus on the adequacy problem in NMT and combine word pre-diction with bilingual dictionaries. Some studies tackle the adequacy problem in NMT, but they require an independent SMT system [29, 30] or modification of the decoder [31] . Different from these, ours is simple and a cost-effective solution for the adequacy problem. The under and over-generation problems have been recognized not only in NMT, but in other applications that use the encoder-decoder model for natural language generation. Different solutions have been proposed. First, a coverage vector is introduced in NMT [23, 32, 33] that tracks which source words have been translated by the attention mechanism. A sparse and constrained attention has been proposed [34] , while word prediction, which are also used to reduce computational cost of softmax function at the decoder [35, 36] , has been proposed to solve the undergeneration problem. The decoder in [37] encourages to output predicted target words by initializing the decoder through word prediction, and the model in [38] predicts target words and their expected frequencies to resolve the under and over generation problems in NMT-based summarization. Conclusion We proposed a rewarding model with word prediction to boost the translation probabilities of the predicted target words that should be in correct translations. Our model allows incorporating bilingual dictionaries on a BPE-based NMT system. Extensive evaluation on both resource-poor and resource-rich domains showed its effectiveness. As future work, first, we plan to improve the precision of word prediction preserving the recall at high. Second, we plan to improve our rewarding model to effectively incorporate translation probabilities and extend the model to reward not only words but also phrases. We will also consider a global constraint by predicting not only target words but their frequencies, and adjust rewards when a word has been used in translation. Finally, more experiments on datasets of various domains and language pairs will be conducted to investigate the generality of our approach.",
    "abstract": "To improve the translation adequacy in neural machine translation (NMT), we propose a rewarding model with target word prediction using bilingual dictionaries inspired by the success of decoder constraints in statistical machine translation. In particular, the model first predicts a set of target words promising for translation; then boosts the probabilities of the predicted words to give them better chances to be output. Our rewarding model minimally interacts with the decoder so that it can be easily applied to the decoder of an existing NMT system. Extensive evaluation under both resource-rich and resource-poor settings shows that (1) BLEU score improves more than 10 points with oracle prediction, (2) BLEU score improves about 1.0 point with target word prediction using bilingual dictionaries created either manually or automatically, (3) hyper-parameters of our model are relatively easy to optimize, and (4) undergeneration problem can be alleviated in exchange for increasing over-generated words.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese",
        "English"
    ],
    "numcitedby": "4",
    "year": "2018",
    "month": "October 29-30",
    "title": "Word Rewarding for Adequate Neural Machine Translation"
}