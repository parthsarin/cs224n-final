{
    "article": "Semantic frames are formal linguistic structures describing situations/actions/events, e.g. Commercial transfer of goods. Each frame provides a set of roles corresponding to the situation participants, e.g. Buyer and Goods, and lexical units (LUs) -words and phrases that can evoke this particular frame in texts, e.g. Sell. The scarcity of annotated resources hinders wider adoption of frame semantics across languages and domains. We investigate a simple yet effective method, lexical substitution with word representation models, to automatically expand a small set of frame-annotated sentences with new words for their respective roles and LUs. We evaluate the expansion quality using FrameNet. Contextualized models demonstrate overall superior performance compared to the non-contextualized ones on roles. However, the latter show comparable performance on the task of LU expansion. Introduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982) . Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018) , machine translation (Gao and Vogel, 2011; Zhai et al., 2013) , and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018) . The impact, however, is limited by the scarce availability of Substitutes for Assistance: assist, aid Substitutes for Helper: she, I, he, you, we, someone, they, it, lori, hannah, paul, sarah, melanie, pam, riley Substitutes for Benefited party: me, him, folk, her, everyone, people Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1 : An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) , yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013) . We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to roles of the respective frame) of those sentences and aggregating possible substitutions into an induced frame-semantic resource. Table 1 shows one such induced example. For this purpose, we have experimented with state-of-the-art noncontextualized (static) word representation models including neural word embeddings, i.e. fast-Text (Bojanowski et al., 2017) , GloVe (Pennington et al., 2014) , and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (Bie-mann and Riedl, 2013) ; and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019) , which has set a new benchmark performance on many downstream NLP applications. To complete the comparison, we also include the lexical substitution model of Melamud et al. (2015) , which uses dependency-based word and context embeddings and produces context-sensitive lexical substitutes. To generate substitutes, we decompose the problem into two sub-tasks: Lexical unit expansion: Given a sentence and its target word, the task is to generate frame preserving substitutes for this word. Frame role expansion: Given a sentence and an argument, the task is to generate meaningpreserving substitutes for this argument. Contributions of our work are (i) a method for inducing frame-semantic resources based on a few frame-annotated sentences using lexical substitution, and (ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018) , with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014) , role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017) , and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018) , which is considered very similar to standard Prop-Bank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inverse problem, the case where the number of annotations is insufficient, similar to the idea of Pennacchiotti et al. (2008) who investigated the utility of semantic spaces and WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018) . They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (Qasem-iZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of frame-semantic resources. Inducing Lexical Representations of Frames via Lexical Substitution We experimented with two groups of lexical substitution methods. The first one use no context: non-contextualized neural word embedding models, i.e. fastText (Bojanowski et al., 2017) , GloVe (Pennington et al., 2014) , and word2vec (Mikolov et al., 2013) , as well as distributional thesaurus based models in the form of JoBimText (Biemann and Riedl, 2013) . The second group of methods does use the context: here, we tried contextualized word embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015) . Static Word Representations These word representations models are inherently non-contextualized as they learn one representation of a word regardless of its context. Neural Word Embeddings Neural word embeddings represent words as vectors of continuous numbers, where words with similar meanings are expected to have similar vectors. Thus, to produce substitutes, we extracted the k nearest neighbors using a cosine similarity measure. We use pre-trained embeddings by authors models: fast-Text trained on the Common Crawl corpus, GloVe trained on Common Crawl corpus with 840 billion words, word2vec trained on Google News. All these models produce 300-dimension vectors. Distributional Thesaurus (DT) In this approach, word similarities are computed using complex linguistic features such as dependency relations (Lin, 1998) . The representations provided by DTs are sparser, but similarity scores based on them can be better. JoBimText (Biemann and Riedl, 2013 ) is a framework that offers many DTs computed on a range of different corpora. Context features for each word are ranked using the lexicographer's mutual information (LMI) score and used to compute word similarity by feature overlap. We extract the k nearest neighbors for the target word. We use two JoBimText DTs: (i) DT built on Wikipedia with n-grams as contexts and (ii) DT built on a 59G corpus (Wikipedia, Gigaword, ukWaC, and LCC corpora combined) using dependency relations as context. Contextualized Models Static word representations fail to handle polysemic words. This paves the way for contextaware word representation models, which can generate diverse word-probability distributions for a target word based on its context. Melamud et al. (2015) This simple model uses syntax-based skip-gram embeddings (Levy and Goldberg, 2014) of a word and its context to produce context-sensitive lexical substitutes, where the context of the word is represented by the dependency relations of the word. We use the original word and context embeddings of Melamud et al. (2015) , trained on the ukWaC (Ferraresi et al., 2008) corpus. To find dependency relations, we use Stanford Parser (Chen and Manning, 2014) and collapsed the dependencies that include prepositions. Top k substitutes are produced if both the word and its context are present in the model's vocabulary. Melamud et al. (2015) proposed four measures of contextual similarity which rely on cosine similarity between context and target words, of which we report the two best performing on our task (BalAdd and BalMult). BERT Although BERT was originally trained to restore masked tokens, it can produce a word distribution even without masking the target word. In this case, it will consider both the context and the semantics of the target word, leading to a more accurate probability distribution. For experiments, we choose one of the largest pre-trained models presented in Devlin et al. (2019) , which is bertlarge-cased (340M parameters) from the PyTorch implementation by Wolf et al. (2019) . We produce a substitute word distribution without masking and selected substitutes with top k probabilities. 4 Experimental Setup Datasets We experimented with FrameNet (Baker et al., 1998) version 1.7. It contains around 170k sentences annotated with 1, 014 frames, 7, 878 types of frame roles, and 10, 340 lexical units. Frame roles and LUs can consist of a single token or multiple tokens. For this work, we have only considered a single-token substitution. The datasets for evaluation were derived automatically from FrameNet. To create a gold standard for LU expansion task, for each sentence containing an annotated LU, we consider other LUs of the corresponding semantic frame as ground truth substitutes. We keep only LUs marked as verbs in FrameNet. To make a gold standard for the role expansion task, for each of the sentences that contain an annotation of a given frame role, we consider all the single-word annotations from the rest of the corpus marked with the same role and related to the same frame as ground truth substitutes. The final datasets for experiments contain 79, 584 records for lexical unit expansion and 191, 252 records for role expansion (cf. Tables 4 and 5 ). Evaluation Measures To evaluate the quality of generated substitutes for a given target word, we use precision at k (p@k) top substitutes. To evaluate the quality of the entire list of generated substitutes, we use mean average precision at level k (M AP @k): AP i @k = 1 min(k, R i ) k l=1 r i l \u2022 p i @l, where M AP @k = 1 N N i=1 AP i @k. Here, N is a total number of examples in the dataset; R i is a number of possible correct answers for an example i; r i l equals 1 if the model output at the level l is correct and 0 if not. We present p@k at levels: 1, 5, 10, as well as M AP @50. Sometimes, the postprocessing procedure leads to the generation of a list of substitutes shorter than k; we consider the absence of a substitute for a position as a wrong answer of a model. Post-processing In post-processing, we remove numbers, symbols, special tokens from the generated list. There may also be multiple examples of the same word in different forms, especially word embeddings often produce multiple words with a shared root form. Therefore, we lemmatize the generated substitutes using the Pattern library (Smedt and Daelemans, 2012) . The duplicates and the target words are dropped. For the lexical unit expansion task, as we just experiment with verbs, we drop the substitutes that cannot be verbs. We used a dictionary of verbs that aggregates verb lists taken from Pattern, WordNet (Miller, 1995) , and FreeLing (Padr\u00f3 and Stanilovsky, 2012) . Results Lexical Units Expansion Task The results for the LU expansion task are presented in Table 2 . The best performance was achieved by the BalAdd measure of Melamud et al. (2015) with p@1 = 0.380 and M AP @50 = 0.152. The fastText model achieves a comparable performance and even shows slightly better results for p@5 and p@10. The DTs considered in our experiments perform worse than word2vec, fastText, and models of Melamud et al. (2015) . That is expected since the DTs need much larger datasets for training as compared to embedding-based models. Even though BERT performed comparably to fast-Text and word2vec, it could not outperform them except for p@1. However, a close examination of some examples shows that it does make a difference when the target word is polysemic. Table 4 in the appendix contains example sentences with highlighted target words and top 5 substitutes generated by all models (along with the ground truth FrameNet annotations). The first example presents an LU that is associated with only one frame in FrameNet. Being unam- Frame Role Expansion Task The evaluation results of the methods for the frame roles expansion task are presented in Table 3 . In this experiment, the non-contextualized models were outperformed by BERT with a significant margin with p@1 = 0.384 and M AP @50 = 0.105. The performance of fastText is worst compared to all models, in contrast to the previous experiment. The DTs perform substantially better than neural word embedding models. The better score is achieved by the DT trained on Wikipedia. The models of Melamud et al. (2015) achieve slightly worse results for p@1 and p@5 than BERT, but significantly lose in terms of p@10 and M AP @50. Table 5 in the appendix enlists several substitutes for semantic roles in a hand-labelled seed sentence. The first example demonstrates several valid matching substitutes, because Vehicle is the most common sense of \"car\". Whereas, the other two examples present an argument with multiple roles. Again, BERT was able to distinguish both senses and produced valid substitutes. Conclusion We presented a simple practical technique for the generation of lexical representations of semantic frames using lexical substitution with several contextualized and static word representation models demonstrating that a single frame annotated example can be used to bootstrap a fully-fledged lexical representation of the FrameNet-style linguistic structures. Non-contextualized baseline models proved to be strong baselines, but failed to produce good substitutes for polysemic words (same word but different semantic frame), whereas BERT for such cases produced competitive substitutes. A prominent direction for future work is testing the proposed technology for building frame representations of low-resource languages and domains. Acknowledgements We thank the anonymous reviewers for valuable feedback and acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) under the \"JOIN-T 2\" project (BI 1544/4-2), the German Academic Exchange Service (DAAD) and the Higher Education Commission (HEC), Pakistan. The work of Artem Shelmanov in writing and experiments with BERT model was supported by the Russian Science Foundation, project #20-11-20166 \"Cross-lingual Knowledge Base Construction and Maintenance\".",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.9637757152213727,
        "foundation": 0.9941788598756589,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section of the article mentions support from the Deutsche Forschungsgemeinschaft (DFG), which is a German research funding organization, and the German Academic Exchange Service (DAAD), which is a German academic support organization. It also mentions support from the Higher Education Commission (HEC), Pakistan, which is a government-backed education and research funding body. The Russian Science Foundation is also mentioned as a supporter, which is a foundation that funds scientific research in Russia. There is no mention of defense, corporate, or other types of funding.",
    "abstract": "Semantic frames are formal linguistic structures describing situations/actions/events, e.g. Commercial transfer of goods. Each frame provides a set of roles corresponding to the situation participants, e.g. Buyer and Goods, and lexical units (LUs) -words and phrases that can evoke this particular frame in texts, e.g. Sell. The scarcity of annotated resources hinders wider adoption of frame semantics across languages and domains. We investigate a simple yet effective method, lexical substitution with word representation models, to automatically expand a small set of frame-annotated sentences with new words for their respective roles and LUs. We evaluate the expansion quality using FrameNet. Contextualized models demonstrate overall superior performance compared to the non-contextualized ones on roles. However, the latter show comparable performance on the task of LU expansion.",
    "countries": [
        "Russia",
        "Germany"
    ],
    "languages": [
        "German",
        "Russian"
    ],
    "numcitedby": 0,
    "year": 2020,
    "month": "June",
    "title": "Generating Lexical Representations of Frames using Lexical Substitution",
    "values": {
        "building on past work": "Our method is inspired by the recent work of Amrami and Goldberg (2018) . Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (Qasem-iZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task.",
        "novelty": "We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. A prominent direction for future work is testing the proposed technology for building frame representations of low-resource languages and domains.",
        "performance": "Lexical Units Expansion Task The best performance was achieved by the BalAdd measure of Melamud et al. The fastText model achieves a comparable performance and even shows slightly better results for p@5 and p@10. BERT performed comparably to fast-Text and word2vec, it could not outperform them except for p@1. Frame Role Expansion Task The DTs perform substantially better than neural word embedding models. The models of Melamud et al."
    }
}