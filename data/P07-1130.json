{
    "article": "We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevance to natural language descriptions of a person's professional interests (the topic). We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) . We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader's Digest Word Power (RDWP) questions. Electronic Career Guidance Career guidance is important both for the person involved and for the state. Not well informed decisions may cause people to drop the training program they are enrolled in, yielding loss of time and financial investments. However, there is a mismatch between what people know about existing professions and the variety of professions, which exist in reality. Some studies report that school leavers typically choose the professions known to them, such as policeman, nurse, etc. Many other professions, which can possibly match the interests of the person very well, are not chosen, as their titles are unknown and people seeking career advice do not know about their existence, e.g. electronics installer, or chemical laboratory worker. However, people are very good at describing their professional interests in natural language. That is why they are even asked to write a short essay prior to an appointment with a career guidance expert. Electronic career guidance is, thus, a supplement to career guidance by human experts, helping young people to decide which profession to choose. The goal is to automatically compute a ranked list of professions according to the user's interests. A current system employed by the German Federal Labour Office (GFLO) in their automatic career guidance front-end 1 is based on vocational trainings, manually annotated using a tagset of 41 keywords. The user must select appropriate keywords according to her interests. In reply, the system consults a knowledge base with professions manually annotated with the keywords by career guidance experts. Thereafter, it outputs a list of the best matching professions to the user. This approach has two significant disadvantages. Firstly, the knowledge base has to be maintained and steadily updated, as the number of professions and keywords associated with them is continuously changing. Secondly, the user has to describe her interests in a very restricted way. At the same time, GFLO maintains an extensive database with textual descriptions of professions, called BERUFEnet. 2 Therefore, we cast the problem of ECG as an IR task, trying to remove the disadvantages of conventional ECG outlined above by letting the user describe her interests in a short natural language essay, called a professional profile. Example essay translated to English I would like to work with animals, to treat and look after them, but I cannot stand the sight of blood and take too much pity on them. On the other hand, I like to work on the computer, can program in C, Python and VB and so I could consider software development as an appropriate profession. I cannot imagine working in a kindergarden, as a social worker or as a teacher, as I am not very good at asserting myself. Textual descriptions of professions are ranked given such an essay by using NLP and IR techniques. As essays and descriptions of professions display a mismatch between the vocabularies of topics and documents and there is lack of contextual information, due to the documents being fairly short as compared to standard IR scenarios, lexical semantic information should be especially beneficial to an IR system. For example, the profile can contain words about some objects or activities related to the profession, but not directly mentioned in the description, e.g. oven, cakes in the profile and pastries, baker, or confectioner in the document. Therefore, we propose to utilize semantic relatedness as a ranking function instead of conventional IR techniques, as will be substantiated below. System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004) , and (ii) the German portion of Wikipedia. 3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000) . The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994) , concept indexing (Gonzalo et al., 1998) , or similarity measures as ranking functions (Smeaton et al., 1994; M\u00fcller and Gurevych, 2006) . Recently, Wikipedia has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004) , named entity disambiguation (Bunescu and Pasca, 2006) , and information retrieval (Katz et al., 2005) . Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader's Digest Word Power (RDWP) questions and make conclusions about the ability of the measures to model certain aspects of semantic relatedness and their coverage. Furthermore, we follow the approach by M\u00fcller and Gurevych (2006) , who proposed to utilize the LIN measure and a set of heuristics as an IR model (IR-1). Additionally, we utilize the ESA measure in a semantic information retrieval model, as this measure is significantly better at vocabulary coverage and at modelling cross part-of-speech relations (Gabrilovich and Markovitch, 2007) . We compare the performance of ESA and LIN measures in a taskbased IR evaluation and analyze their strengths and limitations. Finally, we apply ESA to directly compute text similarities between topics and documents (IR-2) and compare the performance of two semantic IR models and a baseline Extended Boolean (EB) model (Salton et al., 1983) with query expansion. 4  To summarize, the contributions of this paper are three-fold: (i) we present a novel system, utilizing NLP and IR techniques to perform Electronic Career Guidance, (ii) we study the properties and intrinsically evaluate two SR measures based on GermaNet and Wikipedia for the tasks of computing semantic relatedness and solving Reader's Digest Word Power Game questions, and (iii) we investigate the performance of two semantic IR models in a task based evaluation. Computing Semantic Relatedness SR Measures GermaNet based measures GermaNet is a German wordnet, which adopted the major properties and database technology from Princeton's Word-Net (Fellbaum, 1998) . However, GermaNet displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006) . Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, domain, and resource independent. Lin is computed as: sim c 1 ,c 2 = 2 \u00d7 log p(LCS(c 1 , c 2 )) log p(c 1 ) + log p(c 2 ) where c 1 and c 2 are concepts (word senses) corresponding to w 1 and w 2 , log p(c) is the information content, and LCS(c 1 , c 2 ) is the lowest common subsumer of the two concepts. The probability p is computed as the relative frequency of words (representing that concept) in the taz 5 corpus. Wikipedia based measures Wikipedia is a free online encyclopedia that is constructed in a collaborative effort of voluntary contributors and still grows exponentially. During this process, Wikipedia has probably become the largest collection of freely available knowledge. Wikipedia shares many of its properties with other well known lexical semantic resources (like dictionaries, thesauri, semantic wordnets or conventional encyclopedias) (Zesch et al., 2007a) . As Wikipedia also models relatedness between concepts, it is better suited for computing semantic relatedness than GermaNet (Zesch et al., 2007b) . In very recent work, Gabrilovich and Markovitch (2007) introduce a SR measure called Explicit Semantic Analysis (ESA). The ESA measure represents the meaning of a term as a high-dimensional concept vector. The concept vector is derived from Wikipedia articles, as each article focuses on a certain topic, and can thus be viewed as expressing a concept. The dimension of the concept vector is the number of Wikipedia articles. Each element of the vector is associated with a certain Wikipedia article (or concept). If the term can be found in this article, the term's tfidf score (Salton and McGill, 1983 ) in this article is assigned to the vector element. Otherwise, 0 is assigned. As a result, a term's concept vector represents the importance of the term for each concept. Semantic relatedness of two terms can then be easily computed as the cosine of their corresponding concept vectors. If we want to measure the semantic relatedness of texts instead of terms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms' concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1 . Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004) . However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. (Wallace and Wallace, 2005) . We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: Muffin (muffin) a) Kleingeb\u00e4ck (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) Wildschaf (moufflon) The task is to find the correct choice -'a)' in this case. This dataset is significantly larger than any of the previous datasets employed in this type of evaluation. Also, it is not restricted to synonym questions, as in the work by Jarmasz and Szpakowicz (2003) , but also includes hypernymy/hyponymy, and few non-classical relations. Analysis of Results Table 2 gives the results of evaluation on the task of correlating the results of an SR measure with human judgments using Pearson correlation. The Ger-maNet based LIN measure outperforms ESA on the Gur65 dataset. On the other datasets, ESA is better than LIN. This is clearly due to the fact, that Gur65 contains only noun-noun word pairs connected by classical semantic relations, while the other datasets also contain cross part-of-speech pairs connected by non-classical relations. The Wikipedia based ESA measure can better capture such relations. Additionally, Table 3 shows that ESA also covers almost all word list extended with highly frequent domain specific terms. Before adding the remaining words to the index, they are lemmatized. We finally split compounds into their constituents, and add both, constituents and compounds, to the index. EB model Lucene 7 is an open source text search library based on an EB model. After matching the preprocessed queries against the index, the document collection is divided into a set of relevant and irrelevant documents. The set of relevant documents is, then, ranked according to the formula given in the following equation: r EB (d, q) = nq i=1 tf (t q , d)\u2022idf (t q )\u2022lengthN orm(d) where n q is the number of terms in the query, tf (t q , d) is the term frequency factor for term t q in document d, idf (t q ) is the inverse document frequency of the term, and lengthN orm(d) is a normalization value of document d, given the number of terms within the document. We added a simple query expansion algorithm using (i) synonyms, and (ii) hyponyms, extracted from GermaNet. IR based on SR For the (IR-1) model, we utilize two SR measures and a set of heuristics: (i) the Lin measure based on GermaNet (LIN), and (ii) the ESA measure based on Wikipedia (ESA-Word). This algorithm was applied to the German IR benchmark with positive results by M\u00fcller and Gurevych (2006) . The algorithm computes a SR score for each query and document term pair. Scores above a predefined threshold are summed up and weighted by different factors, which boost or lower the scores for documents, depending on how many query terms are contained exactly or contribute a high enough SR score. In order to integrate the strengths of traditional IR models, the inverse document frequency idf is considered, which measures the general importance of a term for predicting the content of a document. The final formula of the model is as follows: r SR (d, q) = n d i=1 nq j=1 idf (t q,j ) \u2022 s(t d,i , t q,j ) (1 + n nsm ) \u2022 (1 + n nr ) where n d is the number of tokens in the document, n q the number of tokens in the query, t d,i the i-th document token, t q,j the j-th query token, s(t d,i , t q,j ) the SR score for the respective document and query term, n nsm the number of query terms not exactly contained in the document, n nr the number of query tokens, which do not contribute a SR score above the threshold. For the (IR-2) model, we apply the ESA method for directly comparing the query with documents, as described in Section 3.1. Data The corpus employed in our experiments was built based on a real-life IR scenario in the domain of ECG, as described in Section 1. The document collection is extracted from BERUFEnet, 8 a database created by the GFLO. It contains textual descriptions of about 1,800 vocational trainings, and 4,000 descriptions of professions. We restrict the collection to a subset of BERUFEnet documents, consisting of 529 descriptions of vocational trainings, due to the process necessary to obtain relevance judgments, as described below. The documents contain not only details of professions, but also a lot of information concerning the training and administrative issues. We only use those portions of the descriptions, which characterize the profession itself. We collected real natural language topics by asking 30 human subjects to write an essay about their professional interests. The topics contain 130 words, on average. Making relevance judgments for ECG requires domain expertise. Therefore, we applied an automatic method, which uses the knowledge base employed by the GFLO, described in Section 1. To obtain relevance judgments, we first annotate each essay with relevant keywords from the tagset of 41 and retrieve a ranked list of professions, which were assigned one or more keywords by domain experts. To map the ranked list to a set of relevant and irrelevant professions, we use a threshold of 3, as suggested by career guidance experts. This setting yields on average 93 relevant documents per topic. The quality of the automatically created gold standard depends on the quality of the applied knowledge base. As the knowledge base was created by domain experts and is at the core of the electronic career guidance system of the GFLO, we assume that the quality is adequate to ensure a reliable evaluation. Analysis of Results In Table 5 , we summarize the results of the experiments applying different IR models on the BERUFEnet data. We build queries from natural language essays by (QT-1) extracting nouns, verbs, and adjectives, (QT-2) using only nouns, and (QT-3) manually assigning suitable keywords from the tagset with 41 keywords to each topic. We report the results with two different thresholds (.85 and .98) for the Lin model, and with three different thresholds (.11, .13 and .24) for the ESA-Word models. The evaluation metrics used are mean average precision (MAP), precision after ten documents (P10), the number of relevant returned documents (#RRD). We compute the absolute value of Spearman's rank correlation coefficient (SRCC) by comparing the relevance ranking of our system with the relevance ranking of the knowledge base employed by the GFLO. Using query expansion for the EB model decreases the retrieval performance for most configurations. The SR based models outperform the EB model in all configurations and evaluation metrics, except for P10 on the keyword based queries. The Lin model is always outperformed by at least one of the ESA models, except for (QT-3). (IR-2) performs best on longer queries using nouns, verbs, adjectives or just nouns. Comparing the number of relevant retrieved documents, we observe that the IR models based on SR are able to return more relevant documents than the EB model. This supports the claim that semantic knowledge is especially helpful for the vocabulary mismatch problem, which cannot be addressed by conventional IR models. E.g., only SR-based models can find the job information technician for a profile which contains the sentence My interests and skills are in the field of languages and IT. The job could only be judged as relevant, as the semantic relation between IT in the profile and information technology in the professional description could be found. In our analysis of the BERUFEnet results obtained on (QT-1), we noticed that many errors were due to the topics expressed in free natural language essays. Some subjects deviated from the given task to describe their professional interests and described facts that are rather irrelevant to the task of ECG, e.g. It is important to speak different languages in the growing European Union. If all content words are extracted to build a query, a lot of noise is introduced. Therefore, we conducted further experiments with (QT-2) and (QT-3): building the query using only nouns, and using manually assigned keywords based on the tagset of 41 keywords. For example, the following query is built for the professional profile given in Section 1. Keywords assigned: care for/nurse/educate/teach; use/program computer; office; outside: outside facilities/natural environment; animals/plants IR results obtained on (QT-2) and (QT-3) show that the performance is better for nouns, and significantly better for the queries built of keywords. This suggests that in order to achieve high IR performance for the task of Electronic Career Guidance, it is necessary to preprocess the topics by performing information extraction to remove the noise from free text essays. As a result of the preprocessing, natural language essays should be mapped to a set of keywords relevant for describing a person's interests. Our results suggest that the word-based semantic relatedness IR model (IR-1) performs significantly better in this setting. Conclusions We presented a system for Electronic Career Guidance utilizing NLP and IR techniques. Given a natural language professional profile, relevant professions are computed based on the information about semantic relatedness. We intrinsically evaluated and analyzed the properties of two semantic relatedness measures utilizing the lexical semantic information in a German wordnet and Wikipedia on the tasks of estimating semantic relatedness scores and answering multiple-choice questions. Furthermore, we applied these measures to an IR task, whereby they were used either in combination with a set of heuristics or the Wikipedia based measure was used to directly compute semantic relatedness of topics and 1037 Table 5 : Information Retrieval performance on the BERUFEnet dataset. documents. We experimented with three different query types, which were built from the topics by: (QT-1) extracting nouns, verbs, adjectives, (QT-2) extracting only nouns, or (QT-3) manually assigning several keywords to each topic from a tagset of 41 keywords. In an intrinsic evaluation of LIN and ESA measures on the task of computing semantic relatedness, we found that ESA captures the information about semantic relatedness and non-classical semantic relations considerably better than LIN, which operates on an is-a hierarchy and, thus, better captures the information about semantic similarity. On the task of solving RDWP questions, the ESA measure significantly outperformed the LIN measure in terms of correctness. On both tasks, the coverage of ESA is much better. Despite this, the performance of LIN and ESA as part of an IR model is only slightly different. ESA performs better for all lengths of queries, but the differences are not as significant as in the intrinsic evaluation. This indicates that the information provided by both measures, based on different knowledge bases, might be complementary for the IR task. When ESA is applied to directly compute semantic relatedness between topics and documents, it outperforms IR-1 and the baseline EB model by a large margin for QT-1 and QT-2 queries. For QT-3, i.e., the shortest type of query, it performs worse than IR-1 utilizing ESA and a set of heuristics. Also, the performance of the baseline EB model is very strong in this experimental setting. This result indicates that IR-2 utilizing conventional information retrieval techniques and semantic information from Wikipedia is better suited for longer queries providing enough context. For shorter queries, soft match-ing techniques utilizing semantic relatedness tend to be beneficial. It should be born in mind, that the construction of QT-3 queries involved a manual step of assigning the keywords to a given essay. In this experimental setting, all models show the best performance. This indicates that professional profiles contain a lot of noise, so that more sophisticated NLP analysis of topics is required. This will be improved in our future work, whereby the system will incorporate an information extraction component for automatically mapping the professional profile to a set of keywords. We will also integrate a component for analyzing the sentiment structure of the profiles. We believe that the findings from our work on applying IR techniques to the task of Electronic Career Guidance generalize to similar application domains, where topics and documents display similar properties (with respect to their length, free-text structure and mismatch of vocabularies) and domain and lexical knowledge is required to achieve high levels of performance. Acknowledgments This work was supported by the German Research Foundation under grant \"Semantic Information Retrieval from Texts in the Example Domain Electronic Career Guidance\", GU 798/1-2. We are grateful to the Bundesagentur f\u00fcr Arbeit for providing the BERUFEnet corpus. We would like to thank the anonymous reviewers for valuable feedback on this paper. We would also like to thank Piklu Gupta for helpful comments.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions that the work was supported by the German Research Foundation under a specific grant, indicating funding from a research agency. There is no mention of funding from defense, corporate entities, foundations, or an absence of funding.",
    "abstract": "We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevance to natural language descriptions of a person's professional interests (the topic). We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) . We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader's Digest Word Power (RDWP) questions.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German"
    ],
    "numcitedby": 56,
    "year": 2007,
    "month": "June",
    "title": "What to be? - Electronic Career Guidance Based on Semantic Relatedness"
}