{
    "article": "Semantic role labeling (SRL) and word sense disambiguation (WSD) are two fundamental tasks in natural language processing to find a sentence-level semantic representation. To date, they have mostly been modeled in isolation. However, this approach neglects logical constraints between them. We therefore exploit some pipeline systems which verify the automatic all word sense disambiguation could help the semantic role labeling and vice versa. We further propose a Markov logic model that jointly labels semantic roles and disambiguates all word senses. By evaluating our model on the OntoNotes 3.0 data, we show that this joint approach leads to a higher performance for word sense disambiguation and semantic role labeling than those pipeline approaches. Introduction Semantic role labeling (SRL) and word sense disambiguation (WSD) are two fundamental tasks in natural language processing to find a sentencelevel semantic representation. Semantic role labeling aims at identifying the relations between predicates in a sentence and their associated arguments. Word sense disambiguation is the process of identifying the correct meaning, or sense of a word in a given context. For example, for the sentence in Figure 1 , we can find out that the predicate token \"hitting\" at position 3 has sense \"cause to move by striking\" and the sense label is \"hit.01\". The argument headed by the token \"cat\" at position 1 with sense \"feline mammal\" (cat.01) is referring to the player (A0), and the argument headed by the token \"ball\" at position 5 with sense \"round object that is hit in games\" (ball.01) is referring to the game object (A1) being hit. Normally, semantic role labeling and word sense disambiguation are regarded as two independent tasks, i.e., the word sense information is rarely used in a semantic role labeling system and vice versa. A few researchers have used semantic roles to help the verb sense disambiguation (Dang and Palmer, 2005) . More people used predicate senses in semantic role labeling (Haji\u010d et al., 2009; Surdeanu et al., 2008) . However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008) , joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009) , joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008) , joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008) , and joint parsing and named entity recognition (Finkel and Manning, 2009) . For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman-tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009) . Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related rules. Global constraints (introduced by Punyakanok et al. (2008) ) among semantic roles can be easily added into Markov logic. And the more important, the jointly modeling can be realized using Markov logic naturally. Besides predicates and prepositions, other word senses are also important information for recognizing semantic roles. For example, if we know \"cat\" is an \"agent\" of the predicate \"hit\" in a sentence, we can guess that \"dog\" can also be an \"agent\" of \"hit\", though it does not appear in the training data. Similarly, the semantic role information can also help to disambiguate word senses. In addition, the predicate sense and the argument sense can also help each other. In the sentence \"The cat is hitting a ball.\", if we know \"hit\" here has a game related sense, we can guess that the \"ball\" should have the sense \"is a round object in games\". In the same way, the correct \"ball\" sense can help to disambiguate the sense of \"hit\". The joint probability, that they are disambiguated correctly simultaneously will be larger than other abnormalities. The release of OntoNotes (Hovy et al., 2006) provides us an opportunity to jointly model all word senses disambiguation and semantic role labeling. OntoNotes is a large corpus annotated with constituency trees (based on Penn Treebank), predicate argument structures (based on Penn PropBank), all word senses, etc. It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009) , and word sense disambiguation (Zhong et al., 2008) . In this paper, we first propose some pipeline systems which exploit automatic all word sense disambiguation into semantic role labeling task and vice versa. Then we present a Markov logic model which can easily express useful global con-straints and jointly disambiguate all word senses and label semantic roles. Experiments on the OntoNotes 3.0 corpus show that (1) the automatic all word sense disambiguation and semantic role labeling tasks can help each other when using pipeline approaches, and more important, (2) the joint approach using Markov logic leads to higher accuracy for word sense disambiguation and performance (F 1 ) for semantic role labeling than pipeline approaches. Related Work Joint models were often used in semantic role labeling community. Toutanova et al. (2008) and Punyakanok et al. (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment. Besides jointly learning semantic role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot of engineering work is also required in both models. Recently, Markov logic (Domingos and Lowd, 2009) became a hot framework for joint model. It has been successfully used in temporal relations recognition (Yoshikawa et al., 2009) , co-reference resolution (Poon and Domingos, 2008) , etc. It is very easy to do joint modeling using Markov logic. The only work is to define relevant formulas. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Haji\u010d et al. (2009) , Surdeanu et al. (2008) , and Dang and Palmer (2005) . They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize probability of the preposition senses and the semantic role of prepositional phrases. Except for predicate and preposition senses, Che et al. (2010) explored all word senses for semantic role labeling. They showed that all word senses can improve the semantic role labeling performance significantly. However, the golden word senses were used in their experiments. The results are still unknown when an automatic word sense disambiguation system is used. In this paper, we not only use all word senses disambiguated by an automatic system, but also make the semantic role labeling results to help word sense disambiguation synchronously with a joint model. Markov Logic Markov logic can be understood as a knowledge representation with a weight attached to a firstorder logic formula. Let us describe Markov logic in the case of the semantic role labeling task. We can model this task by first introducing a set of logical predicates such as role(p, a, r) and lemma(i, l), which means that the argument at position a has the role r with respect to the predicate at position p and token at position i has lemma l respectively. Then we specify a set of weighted first order formulas that define a distribution over sets of ground atoms of these predicates (or so-called possible worlds). Ideally, the distribution we define with these weighted formulas assigns high probability to possible worlds where semantic role labeling is correct and a low probability to worlds where this is not the case. For instance, for the sentence in Figure 1 , a suitable set of weighted formulas would assign a high probability to the world: lemma(1, cat), lemma(3, hit), lemma(5, ball) role(3, 1, A0), role(3, 5, A1) and low probabilities to other cases. A Markov logic network (MLN) M is a set of weighted formulas, i.e., a set of pairs (\u03c6, \u03c9), where \u03c6 is a first order formula and \u03c9 is the real weight of the formula. M defines a probability distribution over possible worlds: p(y) = 1 Z exp( (\u03c6,\u03c9)\u2208M \u03c9 c\u2208C \u03c6 f \u03c6 c (y)) where each c is a binding of free variables in \u03c6 to constants. Each f \u03c6 c is a binary feature function that returns 1 if the possible world y includes the ground formula by replacing the free variables in \u03c6 with the constants in c is true, and 0 otherwise. C \u03c6 is the set of all bindings for the variables in \u03c6. Z is a normalization constant. Model We divide our system into two stages: word sense disambiguation and semantic role labeling. For comparison, we can process them with pipeline strategy, i.e., the word sense disambiguation results are used in semantic role labeling or the semantic role labeling results are used in word sense disambiguation. Of course, we can jointly process them with Markov logic easily. We define two hidden predicates for the two stages respectively. For word sense disambiguation, we define the predicate sense(w, s) which indicates that the word at position w has the sense s. For semantic role labeling, the predicate role(p, a, r) is defined as mentioned in above. Different from Meza-Ruiz and Riedel ( 2009 ), which only used sense number as word sense representation, we use a triple (lemma, part-ofspeech, sense num) to represent the word sense s. For example, (hit, v, 01) denotes that the verb \"hit\" has sense number 01. Obviously, our representation can distinguish different word senses which have the identical sense number. In addition, we use one argument classification stage with predicate role to label semantic roles as Che et al. (2009) . Similarly, no argument identification stage is used in our model. The approach can improve the recall of the system. In addition to the hidden predicates, we define observable predicates to represent the information available in the corpus. Table 1 presents these predicates. Local Formula A local formula means that its groundings relate any number of observed ground atoms to exactly one hidden ground atom. For example lemma(p, +l 1 )\u2227lemma(a, +l 2 ) \u21d2 role(p, a, +r) Predicates Description word(i, w) Token i has word w pos(i, t) Token i has part-of-speech t lemma(i, l) Token i has lemma l chdpos(i, t) The part-of-speech string of token i's all children is t chddep (i, d) The dependency relation string of token i's all children is t f irstLemma(i, l) The leftmost lemma of a subtree rooted by token i is l lastLemma(i, l) The rightmost lemma of a subtree rooted by token i is l posF rame(i, f r) f r is a part-of-speech frame at token i dep(h, a, de) The dependency relation between an argument a and its head h is de isP redicate(p) Token p is a predicate posP ath(p, a, pa) The part-of-speech path between a predicate p and an argument a is pa depP ath(p, a, pa) The dependency relation path between a predicate p and an argument a is pa pathLen(p, a, le) The path length between a predicate p and an argument a is le position(p, a, po) The relative position between a predicate p and an argument a is po f amily(p, a, f a) The family relation between a predicate p and an argument a is f a wsdCand(i, t) Token i is a word sense disambiguation candidate, here t is \"v\" or \"n\" uniqe(r) For a predicate, semantic role r can only appear once means that if the predicate lemma at position p is l 1 and the argument lemma at position a is l 2 , then the semantic role between the predicate and the argument is r with some possibility. The + notation signifies that Markov logic generates a separate formula and a separate weight for each constant of the appropriate type, such as each possible pair of lemmas (l 1 , l 2 , r). This type of \"template-based\" formula generation can be performed automatically by a Markov logic engine, such as the thebeast 1 system. The local formulas are based on features employed in the state-of-the-art systems. For word sense disambiguation, we use the basic features mentioned by Zhong et al. (2008) . The semantic role labeling features are from Che et al. (2009) , 1 http://code.google.com/p/thebeast/ Features SRL WSD Lemma \u2022 \u2022 POS \u2022 \u2022 FirstwordLemma \u2022 HeadwordLemma \u2022 HeadwordPOS \u2022 LastwordLemma \u2022 POSPath \u2022 PathLength \u2022 Position \u2022 PredicateLemma \u2022 PredicatePOS \u2022 RelationPath \u2022 DepRelation \u2022 POSUpPath \u2022 POSFrame \u2022 FamilyShip \u2022 BagOfWords \u2022 Window3OrderedWords \u2022 Window3OrderedPOSs \u2022 Table 2: Local Features. the best system of the CoNLL 2009 shared task. The final features are listed in Table 2 . What follows are some simple examples in order to explain how we implement each feature as a formula (or a set of formulas). Consider the \"Position\" feature. We first introduce a predicate position(p, a, po) that denotes the relative position between predicate p and argument a is po. Then we add a formula position(p, a, +po) \u21d2 role(p, a, +r) for all possible combinations of position and role relations. The \"BagOfWords\" feature means that the sense of a word w is determined by all of lemmas in a sentence. Then, we add the following formula set: wsdCand(w, +tw) \u2227 lemma(w, +lw) \u2227 lemma(1, +l1) \u21d2 sense(w, +s) . . . wsdCand(w, +tw) \u2227 lemma(w, +lw) \u2227 lemma(2, +li) \u21d2 sense(w, +s) . . . wsdCand(w, +tw) \u2227 lemma(w, +lw) \u2227 lemma(n, +ln) \u21d2 sense(w, +s) where, the w is the position of current word and t w is its part-of-speech tag, l w is its lemma. l i is the lemma of token i. There are n tokens in a sentence totally. Global Formula Global formulas relate more than one hidden ground atoms. We use this type of formula for two purposes: 1. To capture the global constraints among different semantic roles; 2. To reflect the joint relation between word sense disambiguation and semantic role labeling. Punyakanok et al. (2008) proposed an integer linear programming (ILP) model to get the global optimization for semantic role labeling, which satisfies some constraints. This approach has been successfully transferred into dependency parse tree based semantic role labeling system by Che et al. (2009) . The final results must satisfy two constraints which can be described with Markov logic formulas as follows: C1: Each word should be labeled with one and only one label. role(p, a, r 1 ) \u2227 r 1 = r 2 \u21d2 \u00acrole(p, a, r 2 ) The same unique constraint also happens on the word sense disambiguation, i.e., sense(w, s 1 ) \u2227 s 1 = s 2 \u21d2 \u00acsense(w, s 2 ) C2: Some roles (A0\u223cA5) appear only once for a predicate. It is also easy to express the joint relation between word sense disambiguation and semantic role labeling with Markov logic. What we need to do is just adding some global formulas. The relation between them can be shown in Figure 2 . Inspired by CoNLL 2008 (Surdeanu et al., 2008) and 2009 (Haji\u010d et al., 2009) shared tasks, where most of successful participant systems used predicate senses for semantic role labeling, we also model that the word sense disambiguation implicates the semantic role labeling. Here, we divide the all word sense disambiguation task into two subtasks: predicate sense disambiguation and argument sense disambiguation. The advantages of the division method approach lie in two aspects. First, it makes us distinguish the contributions of predicate and argument word sense disambiguation respectively. Second, as previous discussed, the predicate and argument sense disambiguation can help each other. Therefore, we can reflect the help with the division and use Markov logic to represent it. Experiments Experimental Setting In our experiments, we use the OntoNotes Release 3.0 2 corpus, the latest version of OntoNotes (Hovy et al., 2006) . The OntoNotes project leaders describe it as \"a large, multilingual richly-annotated corpus constructed at 90% internanotator agreement.\" The corpus has been annotated with multiple levels of annotation, including constituency trees, predicate argument structure, word senses, co-reference, and named entities. For this work, we focus on the constituency trees, word senses, and predicate argument structures. The corpus has English, Chinese, and Arabic portions, and we just use the English portion, which has been split into four sections: broadcast conversation (bc), broadcast news (bn), magazine (mz), and newswire (nw). There are several datasets in each section, such as cnn and voa. We will do our experiments on all of the OntoNotes 3.0 English datasets. For each dataset, we aimed for roughly a 60% train / 20% development / 20% test split. See Table 3 for the detailed statistics. Here, we use the human annotated partof-speech and parse trees provided by OntoNotes. The lemma of each word is extracted using Word-Net tool 3 .  Because we used semantic role labeling system which is based on dependence syntactic trees, we convert the constituency trees into dependence trees with an Constituent-to-Dependency Conversion Tool 4 . The thebeast system is used in our experiment as Markov logic engine. It uses cutting planes inference technique (Riedel, 2008) with integer linear programming. The weights are learned with MIRA (Crammer and Singer, 2003) online learning algorithm. To our knowledge, this is the first word sense disambiguation and semantic role labeling experiment on OntoNotes 3.0 corpus. In order to compare our joint model with previous work, we build several systems: Baseline: There are two independent baseline systems: word sense disambiguation and semantic role labeling. In each of baseline systems, we only use the local formulas (Section 4.1) and the global formulas which only express the global constraints (Section 4.2). Pipeline: In a pipeline system, we use additional features outputted by preceded stages. Such as in semantic role labeling pipeline system, we use word sense as features, i.e., we set sense(w, s) as an observable predicate and add sense(p, s) \u21d2 role(p, a, r) and sense(a, s) \u21d2 role(p, a, r) formulas into semantic role labeling task. As for word sense disambiguation 4 http://nlp.cs.lth.se/software/treebank converter/ task, we add a set of formulas role(p, a i , r) \u21d2 sense(p, s), where a i is the i th argument of the predicate at position p, and a formula role(p, a, r) \u21d2 sense(p, s) for the argument at position a respectively. Jointly: We use all global formulas mentioned in Section 4.2. With Markov logic, we can add global constraints and get the word sense disambiguation and the semantic role labeling results simultaneously. Results and Discussion The performance of these systems on test set is shown in Table 4 . All of the parameters are fine tuned on the development set. Here, we only consider the noun and verb word sense disambiguation, which cover most of multisense words. Therefore, the word sense disambiguation performance means the accuracy of all nouns and verbs in the test set. The performance of semantic role labeling is calculated using the semantic evaluation metric of the CoNLL 2009 shared task scorer 5 . It measures the precision, recall, and F 1 score of the recovered semantic dependencies. The F 1 score is used as the final performance metric. A semantic dependency is created for each predicate and its arguments. The label of such dependency is the role of the argument. The same with the CoNLL 2009 shared task, we assume that the predicates have been identified correctly. The first row of Table 4 gives the word sense disambiguation result with the most frequent sense, i.e., the #01 sense of each candidate word which normally is the most frequent one in a balance corpus. The second row shows the baseline performances. Here, we note that the 89.37 word sense disambiguation accuracy and the 83.97 semantic role labeling F 1 we obtained are comparable to the state-of-the-art systems, such as the 89.1 word sense disambiguation accuracy given by Zhong et al. (2008) and 85.48 semantic role labeling performance given by Che et al. (2010) on OntoNotes 2.0 respectively, although the corpus used in our experiments is upgraded version of theirs 6 . Additionally, the performance of word sense disambiguation is higher than that of the most frequent sense significantly (z-test 7 with \u03c1 < 0.01). Therefore, the experimental results show that the Markov logic can achieve considerable performances for word sense disambiguation and semantic role labeling on the latest OntoNotes 3.0 corpus. There are two kinds of pipeline systems: word sense disambiguation (WSD) based on semantic role labeling and semantic role labeling (SRL) based on word sense disambiguation. For the using method of word senses, we first only exploit predicate senses (PS) as mentioned by Surdeanu et al. (2008) and Haji\u010d et al. (2009) . Then, in or-der to examine the contribution of word senses except for predicates, we use argument senses (AS) in isolation. Finally, all word senses (PS + AS) were considered. We can see that when the predicate senses (PS) are used to label semantic role, the performance of semantic role labeling can be improved from 83.97 to 84.17. The conclusion, that the predicate sense can improve semantic role labeling performance, is similar with CoNLL 2008 (Surdeanu et al., 2008) and 2009 (Haji\u010d et al., 2009) shared tasks. However, the improvement is not significant (\u03c7 2 -test 8 with \u03c1 < 0.1). Additionally, the semantic role labeling can improve the predicate sense disambiguation significantly from 89.37 to 89.53 (z-test with \u03c1 < 0.1). The same conclusion was obtained by Dang and Palmer (2005) . However, when we only use argument senses (AS), both of the word sense disambiguation and semantic role labeling performances are almost unchanged (from 89.37 to 89.41 and from 83.97 to 83.94 respectively). For the semantic role labeling task, the reason is that the original lemma and part-of-speech features have been able to describe the argument related information. This kind of sense features is just reduplicate. On the other hand, the argument senses cannot be determined only by the semantic roles. For example, the semantic role \"A1\" cannot predict the argument sense of \"ball\" exactly. The predicates must be considered simultaneously. Therefore, we use the last strategy (PS + AS), which combines the predicate sense and the argument sense together to predict semantic roles. The results show that the performance can be improved significantly (\u03c7 2 -test with \u03c1 < 0.05) from 83.97 to 84.24. Accordingly, the experiment proves that automatic all word sense disambiguation can further improve the semantic role labeling performance. Different from Che et al. (2010) , where the semantic role labeling can be improved with correct word senses about F 1 = 1, our improvement is much lower. The main reason is that the performance of our word sense disambiguation with the most basic features is not high enough. Another limitation of the pipeline strat-egy is that it is difficult to predict the combination between predicate and argument senses. This is an obvious shortcoming of the pipeline method. With Markov logic, we can easily join different tasks with global formulas. As shown in Table 4 , we use five joint strategies: 1. PS \u21d2 SRL: means that we jointly disambiguate predicate senses and label semantic roles. Compared with the pipeline PS system, word sense disambiguation performance is unchanged. However, the semantic role labeling performance is improved from 84.17 to 84.27. Compared with the baseline's 83.97, the improvement is significant (\u03c7 2 -test with \u03c1 < 0.05). 2. AS \u21d2 SRL: means that we jointly disambiguate argument senses and label semantic roles. Compared with the pipeline AS system, both of word sense disambiguation and semantic role labeling performances are improved (from 89.41 to 89.49 and from 83.94 to 84.16 respectively). Although, the improvement is not significant, it is observed that the joint model has the capacity to improve the performance, especially for semantic role labeling, if we could have a more accurate word sense disambiguation. 3. PS \u21d2 AS: means that we jointly disambiguate predicate word senses and argument senses. This kind of joint model does not influence the performance of semantic role labeling. The word sense disambiguation outperforms the baseline system from 89.37 to 89.45. The result verifies our assumption that the predicate and argument senses can help each other. 4. PS + AS \u21d2 SRL: means that we jointly disambiguate all word senses and label semantic roles. Compared with the pipeline method which uses the PS + AS strategy, the joint method can further improve the semantic role labeling (from 84.24 to 84.34). Additionally, it can obtain the predicate and argument senses together. The all word sense disambiguation performance (89.54) is higher than the baseline (89.37) significantly (ztest with \u03c1 < 0.1). 5. Fully: finally, we use all of the three global formulas together, i.e., we jointly disambiguate predicate senses, argument senses, and label semantic roles. It fully joins all of the tasks. Both of all word sense disambiguation and semantic role labeling performances can be further improved. Although the improvements are not significant compared with the best pipeline system, they significantly (z-test with \u03c1 < 0.1 and \u03c7 2 -test with \u03c1 < 0.01 respectively) outperform the baseline system. Additionally, the performance of the fully joint system does not outperform partly joint systems significantly. The reason seems to be that there is some overlap among the contributions of the three joint systems. Conclusion In this paper, we presented a Markov logic model that jointly models all word sense disambiguation and semantic role labeling. We got the following conclusions: 1. The baseline systems with Markov logic is competitive to the state-of-the-art word sense disambiguation and semantic role labeling systems on OntoNotes 3.0 corpus. 2. The predicate sense disambiguation is beneficial to semantic role labeling. However, the automatic argument sense disambiguation itself is harmful to the task. It must be combined with the predicate sense disambiguation. 3. The semantic role labeling not only can help predicate sense disambiguation, but also argument sense disambiguation (a little). In contrast, because of the limitation of the pipeline model, it is difficult to make semantic role labeling to help predicate and argument sense disambiguation simultaneously. 4. It is easy to implement the joint model of all word sense disambiguation and semantic role labeling with Markov logic. More important, the joint model can further improve the performance of the all word sense disambiguation and semantic role labeling than pipeline systems. Acknowledgement",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 1.743258780551038e-06,
        "none": 0.9999925349918634
    },
    "reasoning": "Reasoning: The provided text does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or an explicit statement of no funding. Therefore, without explicit information, we cannot assume any specific funding source was involved.",
    "abstract": "Semantic role labeling (SRL) and word sense disambiguation (WSD) are two fundamental tasks in natural language processing to find a sentence-level semantic representation. To date, they have mostly been modeled in isolation. However, this approach neglects logical constraints between them. We therefore exploit some pipeline systems which verify the automatic all word sense disambiguation could help the semantic role labeling and vice versa. We further propose a Markov logic model that jointly labels semantic roles and disambiguates all word senses. By evaluating our model on the OntoNotes 3.0 data, we show that this joint approach leads to a higher performance for word sense disambiguation and semantic role labeling than those pipeline approaches.",
    "countries": [
        "China"
    ],
    "languages": [
        "Arabic",
        "English",
        "Chinese"
    ],
    "numcitedby": 22,
    "year": 2010,
    "month": "August",
    "title": "Jointly Modeling {WSD} and {SRL} with {M}arkov {L}ogic",
    "values": {
        "building on past work": "However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. Joint models were often used in semantic role labeling community. Toutanova et al. (2008) and Punyakanok et al. (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment. Besides jointly learning semantic role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot of engineering work is also required in both models. Recently, Markov logic (Domingos and Lowd, 2009) became a hot framework for joint model. It is very easy to do joint modeling using Markov logic. The only work is to define relevant formulas. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Haji\u010d et al. They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize probability of the preposition senses and the semantic role of prepositional phrases. Except for predicate and preposition senses, Che et al. (2010) explored all word senses for semantic role labeling. Markov logic can be understood as a knowledge representation with a weight attached to a firstorder logic formula. Let us describe Markov logic in the case of the semantic role labeling task. We can model this task by first introducing a set of logical predicates such as role(p, a, r) and lemma(i, l), which means that the argument at position a has the role r with respect to the predicate at position p and token at position i has lemma l respectively. Then we specify a set of weighted first order formulas that define a distribution over sets of ground atoms of these predicates (or so-called possible worlds). Ideally, the distribution we define with these weighted formulas assigns high probability to possible worlds where semantic role labeling is correct and a low probability to worlds where this is not the case. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic.",
        "ease of implementation": " With Markov logic, we can easily join different tasks with global formulas.  Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related rules.",
        "performance": "The final results must satisfy two constraints which can be described with Markov logic formulas as follows: C1: Each word should be labeled with one and only one label. role(p, a, r 1 ) \u2227 r 1 = r 2 \u21d2 \u00acrole(p, a, r 2 ) The same unique constraint also happens on the word sense disambiguation, i.e., sense(w, s 1 ) \u2227 s 1 = s 2 \u21d2 \u00acsense(w, s 2 ) C2: Some roles (A0\u223cA5) appear only once for a predicate. The first row of Table 4 gives the word sense disambiguation result with the most frequent sense, i.e., the #01 sense of each candidate word which normally is the most frequent one in a balance corpus. The second row shows the baseline performances. Here, we note that the 89.37 word sense disambiguation accuracy and the 83.97 semantic role labeling F 1 we obtained are comparable to the state-of-the-art systems, such as the 89.1 word sense disambiguation accuracy given by Zhong et al. (2008) and 85.48 semantic role labeling performance given by Che et al. For the using method of word senses, we first only exploit predicate senses (PS) as mentioned by Surdeanu et al. (2008) and Haji\u010d et al. The results show that the performance can be improved significantly (\u03c7 2 -test with \u03c1 < 0.05) from 83.97 to 84.24. With Markov logic, we can easily join different tasks with global formulas. Compared with the baseline's 83.97, the improvement is significant (\u03c7 2 -test with \u03c1 < 0.05). Although the improvements are not significant compared with the best pipeline system, they significantly (z-test with \u03c1 < 0.1 and \u03c7 2 -test with \u03c1 < 0.01 respectively) outperform the baseline system."
    }
}