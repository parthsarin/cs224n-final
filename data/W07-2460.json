{
    "article": "This paper presents a thorough examination of the validity of three evaluation measures on parser output. We assess parser performance of an unlexicalised probabilistic parser trained on two German treebanks with different annotation schemes and evaluate parsing results using the PARSE-VAL metric, the Leaf-Ancestor metric and a dependency-based evaluation. We reject the claim that the T\u00fcBa-D/Z annotation scheme is more adequate then the TIGER scheme for PCFG parsing and show that PARSE-VAL should not be used to compare parser performance for parsers trained on treebanks with different annotation schemes. An analysis of specific error types indicates that the dependency-based evaluation is most appropriate to reflect parse quality. Introduction The evaluation of parsing results is a crucial topic in NLP. Despite severe criticism for PCFG parsing the PARSEVAL metric is still the standard evaluation measure. PARSEVAL has been criticised for not representing 'real' parser quality (Carroll et al., 1998; Brisco et al., 2002; Sampson et al., 2003) . Recent studies investigating the impact of different treebank annotation schemes on unlexicalised probabilistic parsing of German (K\u00fcbler, 2005; K\u00fcbler et al., 2006; Maier, 2006) have been using the PARSEVAL metric for evaluation. Results (labelled bracketing f-score) are about 16% higher for a parser trained on the T\u00fcBa-D/Z treebank (Telljohann et al., 2004) than for a parser trained on the NEGRA treebank (Skut et al., 1997) . Maier (2006) takes that as evidence that the NEGRA annotation scheme is less adequate for PCFG parsing, while a parser trained on the T\u00fcBa-D/Z yields PARSEVAL results in the same range as a parser trained on the English Penn-II treebank (K\u00fcbler et al., 2006) . These results are based on the assumption that PARSEVAL is an appropriate measure for comparing parser performance of a PCFG parser trained on treebanks with different annotation schemes. This paper presents parsing experiments with the PCFG parser BitPar (Schmid, 2004) trained on two German treebanks. The treebanks contain text from the same domain, namely two German daily newspapers, but differ considerably with regard to their annotation schemes. We score parsing results using three different evaluation measures and show that the PARSEVAL results do not correlate with the results of the other metrics. An analysis of specific error types shows the differences between the three measures. Our results indicate that dependencybased evaluation is most appropriate to compare parser output for parsers trained on different treebank annotation schemes. Section 2 describes the main features of the two German treebanks, and Section 3 gives an overview over the metrics used for evaluation. Section 4 presents the parsing experiments. In Section 5 we describe the behaviour of the different evaluation metrics for specific error types. Section 6 concludes. \"The professors express their concerns about the state of the economy.\" TIGER and T \u00fcBa-D/Z The two German treebanks used in our experiments are the TIGER Treebank (Release 2) and the T\u00fcba-D/Z (Release 2). The T\u00fcBa-D/Z consists of approximately 22 000 sentences, while the TIGER Treebank is much larger with more than 50 000 sentences. TIGER is based on and extends the NEGRA data and annotation scheme. Both treebanks contain German newspaper text (Frankfurter Rundschau for TIGER and 'die tageszeitung' (taz) for T\u00fcBa-D/Z) and are annotated with phrase structure and dependency (functional) information. Both treebanks use the Stuttgart T\u00fcbingen POS Tag Set (Schiller et al., 1995) . TIGER uses 49 different grammatical function labels, while the T\u00fcBa-D/Z utilises only 36 function labels. For the encoding of phrasal node categories the T\u00fcBa-D/Z uses 30 different cat-egories, the TIGER Treebank uses a set of 27 category labels. Other major differences between the two treebanks are: in the TIGER Treebank long distance dependencies are expressed through crossing branches (Figure 1 ), while in the T\u00fcBa-D/Z the same phenomenon is expressed with the help of grammatical function labels (Figure 2 ). The annotation in the TIGER Treebank is rather flat and allows no unary branching, whereas the nodes in the T\u00fcBa-D/Z do contain unary branches and a more hierarchical structure, resulting in a much deeper tree structure than the trees in the TIGER Treebank. This results in an average higher number of nodes per sentence for the T\u00fcBa-D/Z. Figures 1 and 2 also illustrate the different annotation of PPs in both annotation schemes. In the TIGER Treebank the internal structure of the PP is flat and the adjective and noun inside the PP are directly attached to the PP, while the T\u00fcBa-D/Z is more hierarchical and inserts an additional NP node. Crossing branches show the long distance dependency between the PP and the noun Sorgen (worries) in the TIGER tree, while in the T\u00fcBa-D/Z the node label OA-MOD encodes the information that the PP modifies the accusative object Verbindungen (connections). Another major difference is the annotation of topological fields in the style of Drach (1937) in the T\u00fcBa-D/Z. The model captures German word order, which accepts three possible sentence configurations (verb first, verb second and verb last), by providing fields like the initial field (VF), the middle field (MF) and the final field (NF). The fields are positioned relative to the verb, which can fill in the left (LK) or the right sentence bracket (VC). The ordering of topological fields is determined by syntactic constraints. Differences between TIGER and NEGRA To date, most PCFG parsing for German has been done using the NEGRA corpus as a training resource. The annotation scheme of the TIGER Treebank is based on the NEGRA annotation scheme, but it also employs some important extensions, which include the annotation of verb-subcategorisation, appositions and parentheses, coordinations and the encoding of proper nouns (Brants and Hansen, 2002) . The Evaluation Measures The three evaluation metrics used in our experiments are: -the PARSEVAL metric (PV) -the Leaf-Ancestor metric (LA) -a dependency-based evaluation (DB) Below we demonstrate the differences between the three evaluation measures, using Sentence 4 from the TIGER test set as an example: ( PARSEVAL PARSEVAL checks label and wordspan identity in parser output compared to the original treebank trees, but neither weights results, differentiating between linguistically more or less severe errors, nor does it give credit to constituents where the syntactic categories have been recognised correctly but the phrase boundary is slightly wrong. Figure 3 shows the gold tree for our example sentence (1). In the parser output the second PP was incorrectly attached to the sentence level (Figure 4 ) instead of being attached to the noun inside the PP. (TOP (S (NP (ART Die [the] ) (NN Regierung [government] ) ) (VVFIN rief) (PP (APPR zum [to the] ) (ADJA weltweiten [worldwide] ) (NN Kampf [fight] ) (PP (APPR gegen [against] ) (NN Terror [terror] ) ) ) (PTKVZ auf) ) ($. .) ) Die Regierung rief zum weltweiten Kampf gegen terror auf. \"The government called for a worldwide war against terror.\" Leaf-Ancestor LA (Sampson et al., 2003) measures the similarity between the path from each terminal node in the parser output tree to the root node and the corresponding path in the gold tree. The path consists of the sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance (Levenshtein, 1966) . For each terminal node in the parser output the sequence of node labels between the terminal node and the root node is compared to the same path in the gold tree. This results in an evaluation result for string similarity for each terminal node. The score for the whole tree is the average of the values for all terminals in the tree. Figure 5 shows LA evaluation results for example (1). The numbers in the left column give the LA scores for each terminal node, which results in an average score of 0.963 for the whole sentence. Phrase boundaries are taken into consideration, in order to distinguish between paths like the one for zum ([PP S TOP) and the one for weltweiten (PP S TOP) in Figure 5 . The LA metric does this as follows: for each terminal at the beginning of a phrase LA looks for the highest non-terminal node governing the phrase which also starts with the terminal, and inserts a left boundary marker before the categorial label of the non-terminal node, if the phrase starts with the terminal node. For Die [the] the high- Additionally, LA looks at each terminal node at the end of a phrase and inserts a right boundary marker after the label of the highest non-terminal node of the phrase ending with the terminal node. In the gold tree of our example the terminal node Kampf [fight] is not the final node of the PP. Due to the PP attachment error in the parser output tree, on the other hand, Kampf [fight] is in a phrase-final position, with the PP as the highest non-terminal node governing the terminal. Therefore a right boundary marker is inserted after the PP node in the path of the parser output, which results in a score of 0.889 for path similarity between gold tree and parser output. The average result for the whole sentence is 0.963, while a perfect sentence would get a score of 1. If LA encounters a mismatch between the words in the gold tree and the parser output, it simply stops without returning a result for the whole sentence. Dependency-Based Evaluation The dependency-based evaluation used in the experiments follows the method of Lin (1998) and K\u00fcbler et al. (2002) , converting the original treebank trees and the parser output into dependency relationships of the form WORD POS HEAD. Functional labels have been omitted for parsing, therefore the dependencies do not comprise functional information. Figure 6 shows the dependency relations for example ( 1 The PP attachment error in the parser output leads to an error in the dependency triples, incorrectly assigning rief [called] as the head of gegen [against] (Table 3 ), while in the gold triples the PP gegen Terror [against terror] is a dependent of the preposition zum [to the]. WORD POS HEAD gegen [against] APPR rief Table 3 : Error in parser output dependency triples For our example we get a precision and recall of 88.89 respectively. Following Lin (1998) , our algorithm computes precision and recall: \u2022 Precision: the percentage of dependency relationships in the parser output that are also found in the gold triples \u2022 Recall: the percentage of dependency relationships in the gold triples that are also found in the parser output triples. We assessed the quality of the automatic dependency conversion methodology by converting the 1024 original trees from each of our test sets into dependency relations, using the functional labels in the original trees to determine the dependencies. We then removed all functional information from the trees and converted the stripped trees into dependencies, using heuristics to find the head. We evaluated the dependencies for the stripped gold trees against the dependencies for the original gold trees including functional labels and obtained an f-score of 99.65% for TIGER and 99.13% for the T\u00fcBa-D/Z dependencies. This shows that the conversion is reliable and not unduly biased to either the TIGER or T\u00fcBa-D/Z annotation schemes. Experimental Setup For the experiments we trained the PCFG parser BitPar (Schmid, 2004) on the TIGER treebank and the T\u00fcBa-D/Z. The training sets for each treebank contain 21067 sentences, while the test sets include 1024 sentences each. To allow a meaningful comparison of parsing results we selected sentences comparable with regard to sentence length, syntactic structure and complexity from both treebanks for our test sets. This resulted in an average sentence length of 14.5 for the TIGER test set and of 14.7 for the T\u00fcBa-D/Z. Before extracting the grammars we inserted a virtual root node and resolved the crossing branches in the TIGER treebank by attaching the non-head child nodes higher up in the tree. After this preprocessing step we extracted an unlexicalised PCFG from each of our training sets. We parsed our test sets with the extracted grammars, using raw text as parser input. Results Table 4 shows the evaluation results for the different metrics. 1 PARSEVAL shows higher results for precision and recall for the T\u00fcBa-D/Z. For DB evaluation the parser trained on the TIGER training set achieves about 7% higher results for precision and recall than the parser trained on the T\u00fcBa-D/Z. The LA results are much closer to each other, but also show better results for the TIGER parse trees. Comparing the f-score learning curves in the three metrics shows that for PARSEVAL the gap between TIGER and T\u00fcBa-D/Z is consistent throughout the whole training process. But while during the first stages of training the difference in results adds up to around 12%, the gap becomes smaller with more training data. When trained on 90-100% of the training data, the difference in f-scores decreases to around 5% (Figure 7 ). while the f-score for T\u00fcBa-D-Z does not seem to improve further, the TIGER results clearly show an ongoing learning effect (Figure 8 ). The learning curve for the dependency-based evaluation (Figure 9 ) shows a similar tendency. While the T\u00fcBa-D/Z yields better results when trained on a small amount of training data only, and from more than 20% of the training set onwards only shows a moderate increase, the f-score for TIGER improves faster and shows an advantage of more than 6% over the T\u00fcBa-D/Z f-score when trained on the whole training set. PARSEVAL The wide difference between the results raises the question, which of the metrics is the most adequate for judging parser quality. The next section approaches this question by looking at the behaviour of the different metrics with regard to specific error types. Part-of-Speech Errors Parse trees yielding 100% precision and recall for PARSEVAL and 100% for LA, but failing to get 100% precision and recall for the DB evaluation, often contain POS errors. In most cases the parser assigned a noun tag instead of a proper name, an adjective tag instead of a cardinal number, or mixed up attributive adjectives with predicative adjectives. These error types are attested in 32 sentences in the TIGER and in 23 sentences in the T\u00fcBa-D/Z test set. Missing Nodes / Additional Nodes Parse trees achieving 100% precision and recall for DB evaluation, but not for the PARSEVAL and LA metric mostly lack a non-terminal node such as a proper name node enclosing an NP, a multi-token number for the TIGER treebank or the Nachfeld (final field) for the T\u00fcBa-D/Z. This applies to 23 sentences in the TIGER test set and to 29 sentences in the T\u00fcBa-D/Z test set. In the parser output of the parser trained on the TIGER treebank there are also sentences which show additional categorial nodes not present in the gold trees, such as prepositional phrases enclosing a pronominal adverb, adverbial phrases or adjectival phrases. Both the missing and the additional nodes do not translate into dependency errors as the dependencies for the trees can be extracted correctly. Nonetheless they lead to a significant decrease in precision and recall for the PARSEVAL scores and, to a lesser extent, also for the LA scores. PP Attachment Errors Parse trees with attachment errors often get reasonable results for the PARSEVAL metric but only show mediocre scores for DB evaluation. We demonstrate this for two sentences with PP attachment errors from the TIGER test set (Figure 1 ) and the T\u00fcBa-D/Z (Figure 2 ). In the gold trees one of the PPs is a child respectively grandchild of the other PP, while in the parser outputs both PPs are mis-attached to the same mother node (dotted lines). Despite the similarity of the two trees concerning sentence length and syntactic complexity PARSE-VAL yields strongly different results for the TIGER and the T\u00fcBa-D/Z parser output. The LA scores are much closer, giving better results for the TIGER parse tree, while the PARSEVAL results are clearly in favour of the T\u00fcBa-D/Z tree. The difference be-tween the PARSEVAL results for two comparable trees is caused by the higher ratio of nodes per words in the T\u00fcBa-D/Z annotation scheme. For the TIGER tree the parser is able to match 4 out of 5 brackets which yields a recall of 4/5 = 80%. For the T\u00fcBa-D/Z the parser correctly identifies 12 out of 14 brackets in the gold tree and therefore achieves a recall value of 12/14 = 92.31%. The dependencybased evaluation gives identical results for the two sentences, which is what linguistic intuition would ask for. Conclusions In this paper we rejected the claim that the German T\u00fcBa-D/Z is more appropriate for PCFG parsing than the TIGER treebank. We showed that the PAR-SEVAL metric cannot be used to compare parser output from parsers trained on different treebanks, because it favours annotation schemes with a high ratio of nodes per word. We have also shown that PARSEVAL results do not correlate with other evaluation measures like the Leaf-Ancestor metric or a dependency-based evaluation, and that the results of a dependency-based evaluation best reflect the linguistic notion of a good parse.",
    "abstract": "This paper presents a thorough examination of the validity of three evaluation measures on parser output. We assess parser performance of an unlexicalised probabilistic parser trained on two German treebanks with different annotation schemes and evaluate parsing results using the PARSE-VAL metric, the Leaf-Ancestor metric and a dependency-based evaluation. We reject the claim that the T\u00fcBa-D/Z annotation scheme is more adequate then the TIGER scheme for PCFG parsing and show that PARSE-VAL should not be used to compare parser performance for parsers trained on treebanks with different annotation schemes. An analysis of specific error types indicates that the dependency-based evaluation is most appropriate to reflect parse quality.",
    "countries": [
        "Ireland"
    ],
    "languages": [
        "German"
    ],
    "numcitedby": "23",
    "year": "2007",
    "month": "May",
    "title": "Evaluating Evaluation Measures"
}