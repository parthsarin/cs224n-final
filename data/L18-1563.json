{
    "framing_question": "If this paper introduces a dataset, what information is included about people, if any?",
    "response": "The provided paper does not introduce a dataset or include any information about people.",
    "article": "The increased demand for structured knowledge has created considerable interest in relation extraction (RE) from large collections of documents. In particular, distant supervision can be used for RE without manual annotation costs. Nevertheless, this paradigm only extracts relations from individual sentences that contain two target entities. This paper explores the incorporation of global contexts derived from paragraph-into-sentence embedding as a means of compensating for the shortage of training data in distantly supervised RE. Experiments on RE from Korean Wikipedia show that the presented approach can learn an exact RE from sentences (including grammatically incoherent sentences) without syntactic parsing. Introduction As the demand for structured knowledge has increased, considerable interest has emerged in relation extraction (RE) from large collections of documents written in natural language. In particular, with \"distant supervision\" (DS) (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013) , it is possible to extract the relationships between pairs of entities without human manual annotation using a knowledge base (KB); this heuristically aligns entities in texts to a given KB and then uses this alignment to train an RE system. Although the DS strategy is a more effective method of automatically labeling training data than directly supervised labeling, DS-based approaches can extract only relations that are limited to a \"single complete sentence\" that contains two target entities. This makes it difficult to obtain both the subjects and object entities that participate in the KB in a single sentence, particularly in null subject (or object) languages such as Korean, Japanese, Arabic, and Swedish, that can leave the subject of a sentence unexpressed, unlike English which allows neither. It is also difficult to utilize DS-based approaches for English data when sentences have an informal, grammatically incoherent style, such as the style popularly used on Twitter, in discharge summaries of clinical texts (Marsh and Sager, 1982) , or in a text shortened to bulleted lists in a Wikipedia article. This point can be illustrated by considering the examples in Figure 1 . S 1 contains a subject, object, and predicate, whereas the subject is omitted in S 2 because it is obvious in adjacent sentences in Korean, resulting in differences between the same sentence written in Korean and in English. Therein, we know S 2 is obviously a positive example for tuple founderOf(Steve Jobs, Apple Inc.), but we cannot label the training instance S 2 according to the traditional paradigm of an existing DS-based approach. We propose a novel approach that performs RE across sentences, at the paragraph-level, and does not require labeled data. The proposed method builds upon sentence embedding with global context constraints by spanning multiple  sentences, which is useful for estimating omitted subjects and predicting relations. First, we specifically perform novel zero subject resolution with the entity-relation-based graph analysis by applying the centrality measure. This allows us to learn RE models for informal sentences and has the advantage of compensating for a shortage of training data in the DS-based approach in a DS-based approach to null subject languages. Then, we try to capture the discriminative context features of each document type, such as the specific logical pattern to the relational flow of text within a paragraph, to support sentence embedding. Our work differs from previous related works in two ways: (1) we propose a method of RE at the paragraph-level-i.e., from a collection of multiple sentences-rather than extracting information from an independent single sentence; (2) our approach, which builds upon the sentence embedding, is more effective for language-independent extraction because it avoids high-level natural language processing (NLP) tools. Therefore, the present approach can be generally used for RE, even in languages for which NLP tools are lacking. Related Work We often encounter a lack of explicitly annotated text in RE, instead finding richly structured KBs such as DBpedia (Bizer et al., 2009) or Freebase (Bollacker et al., 2008) , which has raised significant interest in learning RE using DS. Many DS-based approaches (Hoffmann et al., 2011; Roller and Stevenson, 2014; Tsai and Roth, 2016; Craven and Kumlien, 1999; Mintz et al., 2009) use simple but effective heuristics to align existing facts with unlabeled text. This automatically generated labeled text can be used as training data for supervised learners. Our work was inspired by Mintz et al. (2009) , who adopted the Freebase for the distant supervision of the Wikipedia corpus. Unlike existing methods, we performed RE across sentences at the paragraph-level by extending the possibility of labeling incomplete sentences that were unavailable in the traditional DS-based approach. To the best of our knowledge, ours is the first DS-based approach to solve the problem of data sparseness by applying DS to the RE of informal sentences and alleviating DS assumptions. Quirk and Poon (2016) introduced the RE method in two adjacent sentences using the DS approach. Peng et al. (2017) explored a framework for cross-sentence n-ary RE based on graph long short-term memory networks; they used a graph formula to provide a unified method of integrating various intra-and inter-sentential dependencies such as sequential, syntactic, and discourse relationships. According to their experiments on biomedical domains, use of RE beyond sentence boundaries can yield much more knowledge. In this context, we intend to find more information by spanning multiple sentences. While they are based on the various linguistic analyses, our proposed method can be differentiated by using contexts without syntactic information. Relation Extraction at the Paragraph-level We define our task as follows: Given a sentence s that is a complemented form of an informal (e.g. subject-less) sentence s with marked entities e 1 and e 2 and a set of relations R = {r 1 , ..., r n }, we formulate the task of identifying the semantic relation as a standard classification problem as follows: f : (P, E, L) \u2192 R, ( 1 ) where P is the set of all paragraphs, a paragraph p \u2208 P is the set of contiguous sentences {s 1 , s 2 , . . . , s m }, E is the set of entity pairs, and L denotes the set of relation flows . A relation flow l \u2208 l is a tuple ( \u2190 \u2212 s , \u2212 \u2192 s ) in which \u2190 \u2212 s = {r 1 , r 2 , . ..} is the set of labeled relation mentions in which the preceding sentences are (s 1 , ..., s i\u22121 ) and \u2212 \u2192 s is the set of labeled relation mentions in which the succeeding sentences are (s i+1 , ..., s m ) with a given target sentence s i . Our training objective is to learn a joint representation of the sentences and the logical pattern of the relation flow of text within paragraphs such that a regression layer can predict the correct label. We propose an architecture that learns sentence embedding after compensating sentences with zero subject resolution. S 1 [Steve Jobs] e1 was a [Businessman] e2 in [United States] e3 . S 2 \u00f8 1 Former [CEO] e4 and co-founder of [Apple Inc.] e5 . S 3 On October 5, 2011, \u00f8 2 died of [Pancreatic cancer] e6 . Table 1 : Entity-tagged sentences taken from the first paragraph of the \"Steve Jobs\" article in the Korean edition of Wikipedia. Zero Subject Resolution using Graph Analysis of a Paragraph The basic idea of our zero subject (entity) prediction is to perform tasks by finding the central entity being described within a paragraph without parsing. This prediction task allows us to apply our method to many languages in which NLP tools are lacking. We hypothesize that the paragraph consists of contiguous sentences that describe the central entity. Given an unlabeled textual training corpus (\u03a6 = Wikipedia) and the supervision KB (\u03a8 = DBpedia), we first identify all paragraphs in \u03a6 and entities (\u2208 \u03a8) in the sentence. For example, for S 1 , S 2 , and S 3 in Table 1 , we use WikiLinks 1 to identify six DBpedia entities in total. When entities in every sentence of a given paragraph are identified, the entity graph G in the paragraph is constructed based on the relation tuple in \u03a8 between a pair of entities that appear in the paragraph. Then, the center node is selected in G based on the degree centrality (Wasserman and Faust, 1994) for assigning the latent subject entity beyond the sentence boundary. Centrality is important if the entity links to many other entities with one or multiple links to other entities in G. The example entity graph G generated with e 1 -e 6 is shown in Figure 2 ; (a) represents a tuple in the given \u03a8 between a pair of entities that appear together in the paragraph and (b) represents a digraph of the tuples shown in (a), where \"Steve Jobs\" is selected as the pivot by the out-degree centrality measure.  In this paper, the selected center entity is used to resolve zero subjects. Accordingly, a pair of entities that appear together in a single sentence, or head a pivoting entity and appear within a paragraph, is considered a potential relation instance. In the case of sentences S 2 and S 3 in Table 1 , the concealed subjects, \u00f8 1 and \u00f8 2 , both becomes \"Steve Jobs\" and provide an opportunity for acquiring possible labeled instances via heuristic alignments such as founderOf(Steve Jobs, Apple Inc.) and deathCause(Steve Jobs, Pancreatic cancer). Neither sentence explicitly states that \"Steve Jobs\" has such relationships, but have become useful for learning at training time by our extended model. For example, a sentence compensated by a pivot entity is syntactically incomplete, but we may derive a relatively large weight for the context feature associated with founderOf such as \"former CEO and co-founder of.\" At this stage, the only context features we use from s are the words themselves. The vector representation of these words can be obtained using the Paragraph2Vec framework proposed by (Le and Mikolov, 2014) , which maps each word to a vector and then uses a vector to represent all the words in the context window and thus predict the vector representation of the next word. The basic idea behind this method is to use an additional paragraph token (that maps to a vector space using a different matrix from that used to map the word) from the previous sentence in the document in the context window. Then, using the embedding matrix E sen \u2208 R D\u00d7|V | where D is the dimension of embedded words and |V | is the dimension of the word vocabulary, we can obtain the embedding of the word. All words were randomly initialized and then updated during training. Relational Flow Generation Through the background of incorporating a global context into sentence embedding, the important intuition in our proposed model is understanding the whole paragraph as a single flow document. In this paper, we use the intuitive concept that if the semantic flow of a paragraph can be grasped, the relation type with which to classify the target sentence can be more clearly determined by the relation type with the preceding and succeeding sentences. For this, our auxiliary task is to determine the sequence of how preceding and succeeding sentences are classified into their respective relation types. Figure 3 shows an example of each paragraph that consists of contiguous sentences for two different types of entity. When there are two types of entities-in this case \"baseball player\" and \"president\"-it is possible to use a pattern in which there is no relation \"party\" (a type of relationship that points to a group of politically organized people) in the baseball player paragraph, and \"team\" and \"position\" relations are found that are very close to one another. According to this, for all three sentences (S 1 -S 3 in Table 1 ), relationship flow of S 1 is (\u2205, {founderOf, deathCause}), that of S 2 is ({birthPlace, deathPlace}, {deathCause}), and that of S 3 is ({birthPlace, deathPlace, founderOf }, \u2205). We embed this relational flow, thereby aiming to learn continuous representations of it in vector space, similarly to embedding of words. Thus, we can also represent each element, i.e. the preceding and succeeding sequences of the relational flow, as two one-hot vectors of the K-dimension, where K is equal to the amount of relational flow. We then use the matrixes E \u2190 flow \u2208 R D\u00d7K and E \u2192 flow \u2208 R D\u00d7K to obtain its embedding. In succession, we directly concatenate the sentence vector E sen and the relational flow vectors E \u2190 flow and E \u2192 flow to form the final feature vector. This results in lowdimensional sentence embedding where semantically woven sentences and the relation flows of paragraphs reside in  the same part of the space that presents the semantic relationship. We use this vector to train the machine learning algorithm and classify relationships. Experiments We evaluated the performance of our proposed method by performing training and testing using the Korean version of Wikipedia as the textual corpus, specifically a snapshot from December 2016 2 . We used DBpedia to supervise background knowledge, which was a large KB of entities and relationships. As DBpedia provides tuple downloads in multiple languages 3 , it was advantageous to build an efficient RE model for Koreans. KBs in non-Latin languages are relatively smaller than the English Freebase and DBpedia; our procedure used entities and tuples from DBpedia to provide relationship instances. Implementation details Distantly supervised RE can be viewed as a two-step process. This process (A) detects entities of interest and (B) determines the relationship between the possible set of entities. In this paper, we concentrate on the relationships between two entities, i.e., Step B. We processed the Wikipedia text using the following steps. (1) First, paragraphs are extracted from the article where a paragraph consists of two or more consecutive sentences that are separated by blank lines or different section names. (2) Second, the entities of sentences are identified using WikiLinks. In practice, an alternative entity recognition system may be required because the amount of text linked by WikiLinks is relatively small; however, that endeavor is beyond the scope of this study. (3) Third, central entities are selected from each paragraph by calculating the out-degree centrality based on the network model of the entity graph using the DBpedia tuple. (4) Fourth, sentences whose entity scope is recognized are tokenized. (5) Fifth, the pivot entity is employed to supplement the sentence and collect heuristically aligned data for the RE based on distant supervision. ( 6 ) Sixth, these labeled data are leveraged to construct sentence embedding, relation flow embedding, and finally to generate a single concentrated feature vector. (7) Finally, the RE model is trained with the feature vector to maximize the log probability of the correct relationship type. We converted each sentence into a word-level matrix in which each row was a sentence vector extracted from our model. Sentence vectors were learned from the Distributed Memory version of the Paragraph Vector (PV-DM) algorithm using training data to automatically learn and classify relationships into one of the 240 relation types in our evaluation dataset. PV-DM is an extension to Word2Vec (Mikolov et al., 2013) for learning document embeddings that was first applied to train using the entire corpus completely unsupervised. We did not tune the initial learning rate (\u03b1) and minimum learning rate (\u03b1 min ), and used the following values for all experiments: \u03b1 = .025 and \u03b1 min = .002. The learning rate decreased linearly in each epoch from the initial rate to the minimum rate. We used the unchanged parameter min count (\u03b2) that represents the minimum frequency for times that a token must appear to be included in the Para-graph2Vec model's vocabulary. Our model set this as \u03b2 = 1 to ensure that we treated all tokens in the context as meaningful and used them to train. We have optimized the embedding vector size (=400) and we used window sizes (=5) for the left and right fixed context windows. We ran an experiment with 10 epochs as the number of training iterations. All PV-DM training was carried out using the Gensim 4 library in Python. The next step was using a multiclass logistic regression classifier that was optimized using L-BFGS given the sentence embeddings inferred from the PV-DM model. Once the model had been trained, each sentence in the test dataset could be directly inferred. Results Analysis of Extended Labeling The original DS-based RE corresponds to a single sentence that contains two entities, but we extended this in this paper to tasks for two entities in a paragraph. We have made two extensions to the automated labeling schema, as described in Table 2 . Non-Extended denotes the labeling results of two entities in a sentence according to the existing distant supervision paradigm. Extended:Title and Extended:Pivot are extensions of the label rather than Non-Extended. Extended:Title interprets the title of the Wikipedia document as the head entity because the title is the protagonist in the document, whereas Extended:Pivot represents the extension of the central pivoting entity in the paragraph as the subject entity, i.e. the proposed approach. Table 2 shows the proportion of judged documents for 50 sample documents and the precision-the proportion of relevant labeled sentences for RE-among that set. It is clear from this table that the Extended:Pivot run achieved a higher or similar precision for the judged documents that it returned, but returned larger relevant labeled sentences (i.e. Positive Labels), and hence achieved a higher recall@R score, where R is the number of relevant documents in the collection. The Extended:Title method can also raise the precision and recall compared to the default DS paradigm in Wikipedia, but this is difficult to scale to a web-scale without a document title. Held-out Evaluation for RE We evaluated our RE model as a \"held-out\" evaluation. each entity pair instead of the accuracies of the relation labels for each instance. We compared our model with the part-of-speech (POS) tag feature as a baseline that relies on the POS tag sequences of sentences for classification. Table 3 shows the results for the baseline for comparison with our algorithm. The best result was achieved using sentence embedding with relational flow, which led to an F1-measure of 59%. Although there is much room for improvement in precision and recall, our results indicated that it could be useful for extracting the relationship with small amounts of labeled data without advanced NLP tools such as a parser. Conclusion In this paper, we focused on the distant supervision paradigm and proceeded to RE from passages that did not contain both of the entities that are expected to participate in a relation. We showed that it was possible to use a DS-based model that does not require labeling to represent the contexts of sentences and the surrounding relationship mentions to enable relation classification at the paragraph level. Experiments on Korean Wikipedia were conducted and showed the model's effectiveness in practical use. In future research, we intend to implement our technique on a much larger scale and with a more refined set of relation classifications. Alternatively, we may leverage crosslingual joint techniques to transfer knowledge from other languages and to include joint learning with entity linking. Acknowledgments",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 0.0,
        "none": 0.9999996871837189
    }
}