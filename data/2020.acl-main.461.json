{
    "article": "Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs. Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training. We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the \"amount of novelty\" going into the new review or, equivalently, vary the extent to which it deviates from the input. At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder model. Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (\"decoder\") has direct access to the text of input reviews through the pointergenerator mechanism. Experiments on Amazon and Yelp datasets, show that setting at test time the review's latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions. Introduction Summarization of user opinions expressed in online resources, such as blogs, reviews, social media, or internet forums, has drawn much attention due to its potential for various information access applications, such as creating digests, search, and report Summary This restaurant is a hidden gem in Toronto. The food is delicious, and the service is impeccable. Highly recommend for anyone who likes French bistro. Reviews We got the steak frites and the chicken frites both of which were very good ...  generation (Hu and Liu, 2004; Angelidis and Lapata, 2018; Medhat et al., 2014) . Although there has been significant progress recently in summarizing non-subjective context (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018) , modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. Moreover, annotation efforts would have to be undertaken for multiple domains as online reviews are inherently multi-domain (Blitzer et al., 2007) and summarization systems highly domain-sensitive (Isonuma et al., 2017) . Thus, perhaps unsurprisingly, there is a long history of applying unsupervised and weakly-supervised methods to opinion summarization (e.g., Mei et al. 2007; Titov and McDonald 2008; Angelidis and Lapata 2018) , however, these approaches have primarily focused on extractive summarization, i.e., producing summaries by copying parts of the input reviews. In this work, we instead consider abstractive summarization which involves generating new phrases, possibly rephrasing or using words that were not in the original text. Abstractive summaries are often preferable to extractive ones as they can synthesize content across documents avoiding redundancy (Barzilay et al., 1999; Carenini and Cheung, 2008; Di Fabbrizio et al., 2014) . In addition, we focus on the unsupervised setting and do not use any summaries for training. Unlike aspect-based summarization (Liu, 2012) , which rewards the diversity of opinions, we aim to generate summaries that represent consensus (i.e., dominant opinons in reviews). We argue that such summaries can be useful for quick decision making, and to get an overall feel for a product or business (see the example in Table 1 ). More specifically, we assume we are provided with a large collection of reviews for various products and businesses and define a generative model of this collection. Intuitively, we want to design such a model that, when generating a review for a product 1 relying on a set of other reviews, we can control the \"amount of novelty\" going into the new review or, equivalently, vary the extent to which it deviates from the input. At test time, we can force the novelty to be minimal, and generate summaries representing consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder (VAE) model. Both products and individual reviews are associated with latent representations. Product representations can store, for example, overall sentiment, common topics, and opinions expressed about the product. In contrast, latent representations of reviews depend on the product representations and capture the content of individual reviews. While at training time the latent representations are random variables, we fix them to their respective means at test time. As desired for summarization, these 'average' (or 'copycat') reviews differ in writing style from a typical review. For example, they do not contain irrelevant details that are common in customer reviews, such as mentioning the occasion or saying how many family members accompanied the reviewer. In order to encourage the summaries to include spe-cific details, the review generator ('decoder') has direct access to the text of input reviews through the pointer-generator mechanism (See et al., 2017) . In the example in Table 1 , the model included specific information about the restaurant type and its location in the generated summary. As we will see in ablation experiments, without this conditioning, model performance drops substantially, as the summaries become more generic. We evaluate our approach on two datasets, Amazon product reviews and Yelp reviews of businesses. The only previous method dealing with unsupervised multi-document opinion summarization, as far as we are aware of, is MeanSum (Chu and Liu, 2019) . Similarly to our work, they generate consensus summaries and consider the Yelp benchmark. Whereas we rely on continuous latent representations, they treat the summary itself as a discrete latent representation of a product. Although this captures the intuition that a summary should relay key information about a product, using discrete latent sequences makes optimization challenging; (Miao and Blunsom, 2016; Baziotis et al., 2019; Chu and Liu, 2019) all have to use an extra training loss term and biased gradient estimators. Our contributions can be summarized as follows: \u2022 we introduce a simple end-to-end approach to unsupervised abstractive summarization; \u2022 we demonstrate that the approach substantially outperforms the previous method, both when measured with automatic metrics and in human evaluation; \u2022 we provide a dataset of abstractive summaries for Amazon products. 2 Model and Estimation As discussed above, we approach the summarization task from a generative modeling perspective. We start with a high level description of our model, then, in Sections 2.2 and 2.3, we describe how we estimate the model and provide extra technical details. In Section 3, we explain how we use the model to generate summaries. Overview of the Generative Model Our text collection consists of groups of reviews, with each group corresponding to a single product. z 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T We visited this place last week. M = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t > z 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T M = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = \" r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = \" > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = \" r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = \" > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > z N < l a t e x i t s h a 1 _ b a s e 6 4 = \" n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = \" > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 x b o 2 g < / l a t e x i t > z N < l a t e x i t s h a 1 _ b a s e 6 4 = \" n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = \" > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d The waiters were friendly, and the food was great! z i < l a t e x i t s h a 1 _ b a s e 6 4 = \" C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c + N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d + J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > z i < l a t e x i t s h a 1 _ b a s e 6 4 = \" C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c + N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d + J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = \" 6 e j v q f W e T m l k h + x J P T v H F G h m p p k = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m t w u / + 4 R K 8 1 g + m l m C f k T H k o e c U W O l B z X k w 3 L V r b l L k E 3 i 5 a Q K O V r D 8 t d g F L M 0 Q m m Y o F r 3 P T c x f k a V 4 U z g v D R I N S a U T e k Y + 5 Z K G q H 2 s + W p c 3 J p l R E J Y 2 V L G r J U f 0 9 k N N J 6 F g W 2 M 6 J m o t e 9 h f i f 1 0 9 N e O N n X C a p Q c l W i 8 J U E B O T x d 9 k x B U y I 2 a W U K a 4 v Z W w C V W U G Z t O y Y b g r b + 8 S T r 1 m n d d q 9 9 7 1 W Y l j 6 M I F 1 C B K / C g A U 2 4 g x a 0 g c E Y n u E V 3 h z h v D j v z s e q t e D k M + f w B 8 7 n D 0 4 q j b M = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = \" 6 e j v q f W e T m l k h + x J P T v H F G h m p p k = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m t w u / + 4 R K 8 1 g + m l m C f k T H k o e c U W O l B z X k w 3 L V r b l L k E 3 i 5 a Q K O V r D 8 t d g F L M 0 Q m m Y o F r 3 P T c x f k a V 4 U z g v D R I N S a U T e k Y + 5 Z K G q H 2 s + W p c 3 J p l R E J Y 2 V L G r J U f 0 9 k N N J 6 F g W 2 M 6 J m o t e 9 h f i f 1 0 9 N e O N n X C a p Q c l W i 8 J U E B O T x d 9 k x B U y I 2 a W U K a 4 v Z W w C V W U G Z t O y Y b g r b + 8 S T r 1 m n d d q 9 9 7 1 W Y l j 6 M I F 1 C B K / C g A U 2 4 g x a 0 g c E Y n u E V 3 h z h v D j v z s e q t e D k M + f w B 8 7 n D 0 4 q j b M = < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = \" + u 2    We visited this place last week. Z E i 3 F 1 8 m U Z u F 5 m e 4 Z 7 P C A J H 8 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B w E 4 w u Z n 7 n S d U m s f y 0 U w T 9 C M 6 k j z k j B o r P a j B 3 a B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 1 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 o y c W 2 V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 1 7 7 G Z d J a l C y 5 a I w F c T E Z P 4 3 G X K F z I i p J Z Q p b m 8 l b E w V Z c a m U 7 I h e K s v C A J H 8 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B w E 4 w u Z n 7 n S d U m s f y 0 U w T 9 C M 6 k j z k j B o r P a j B 3 a B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 1 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 o y c W 2 V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 1 7 7 G Z d J a l C y 5 a I w F c T E Z P 4 3 G X K F z I i p J Z Q p b m 8 l b E w V Z c a m U 7 I h e K s v r 5 N 2 v e Z d 1 u r 3 X r V R 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T M = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t > z 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T M = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = \" r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = \" > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = \" r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = \" > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > z N < l a t e x i t s h a 1 _ b a s e 6 4 = \" n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = \" > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 x b o 2 g < / l a t e x i t > z N < l a t e x i t s h a 1 _ b a s e 6 4 = \" n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = \" > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d The waiters were friendly, and the food was great!   Our latent summarization model (which we call COPYCAT) captures this hierarchical organization and can be regarded as an extension of the vanilla text-VAE model (Bowman et al., 2016) . COPYCAT uses two sets of latent variables as shown in Figure 1a . Namely, we associate each review group (equivalently, each product) with a continuous variable c, which captures the group's 'latent semantics'. In addition, we associate each individual review (r i ) with a continuous variable z i , encoding the semantics of that review. The information stored in z i is used by the decoder p \u03b8 (r i |z i ) to produce review text r i . The marginal log-likelihood of one group of reviews r 1:N = (r 1 , . . . , r N ) is given by z i < l a t e x i t s h a 1 _ b a s e 6 4 = \" C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c + N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d + J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > z i < l a t e x i t s h a 1 _ b a s e 6 4 = \" C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c + N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d + J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = \" 6 C g G L J g q b L W V C p 8 / r I 9 t i O 4 P 6 n I = \" > A A A B + X i c b V C 7 T s M w F L 3 h W c o r w M h i t U J i q p I y w F i J h b F I 9 C G 1 U e Q 4 b m v V s S P b q V R F / R M W B h B i 5 U / Y + B u c N g O 0 H M n y 0 T n 3 y s c n S j n T x v O + n a 3 t n d 2 9 / c p B 9 f D o + O T U P T v v a p k p Q j t E c q n 6 E d a U M 0 E 7 h h l O + 6 m i O I k 4 7 U X T + 8 L v z a j S T I o n M 0 9 p k O C x Y C N G s L F S 6 L r D S P J Y z x N 7 5 S p k i 9 C t e w 1 v C b R J / J L U o U Q 7 d L + G s S R Z Q o U h H G s 9 8 L 3 U B D l W h h F O F 9 V h p m m K y R S P 6 c B S g R O q g 3 y Z f I G u r B K j k V T 2 C I O W 6 u + N H C e 6 C G c n E 2 w m e t 0 r x P + 8 Q W Z G d 0 H O R J o Z K s j q o V H G k Z G o q A H F T F F i + N w S T B S z W R G Z Y I W J s W V V b Q n + + p c 3 S b f Z 8 G 8 a z U e / 3 q q V d V T g E m p w D T 7 c Q g s e o A 0 d I D C D Z 3 i F N y d 3 X p x 3 5 2 M 1 u u W U O x f w B 8 7 n D z 4 j k / E = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = \" 6 C g G L J g q b L W V C p 8 / r I 9 t i O 4 P 6 n I = \" > A A A B + X i c b V C 7 T s M w F L 3 h W c o r w M h i t U J i q p I y w F i J h b F I 9 C G 1 U e Q 4 b m v V s S P b q V R F / R M W B h B i 5 U / Y + B u c N g O 0 H M n y 0 T n 3 y s c n S j n T x v O + n a 3 t n d 2 9 / c p B 9 f D o + O T U P T v v a p k p Q j t E c q n 6 E d a U M 0 E 7 h h l O + 6 m i O I k 4 7 U X T + 8 L v z a j S T I o n M 0 9 p k O C x Y C N G s L F S 6 L r D S P J Y z x N 7 5 S p k i 9 C t e w 1 v C b R J / J L U o U Q 7 d L + G s S R Z Q o U h H G s 9 8 L 3 U B D l W h h F O F 9 V h p m m K y R S P 6 c B S g R O q g 3 y Z f I G u r B K j k V T 2 C I O W 6 u + N H C e 6 C G c n E 2 w m e t 0 r x P + 8 Q W Z G d 0 H O R J o Z K s j q o V H G k Z G o q A H F T F F i + N w S T B S z W R G Z Y I W J s W V V b Q n + + p c 3 S b f Z 8 G 8 a z U e / 3 q q V d V T g E m p w D T 7 c Q g s e o A 0 d I D C D Z 3 i F N y d 3 X p x 3 5 2 M 1 u u W U O x f w B 8 7 n D z 4 j k / E = < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = \" + u 2 Z E i 3 F 1 8 m U Z u F 5 m e 4 Z 7 P C A J H 8 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B C A J H 8 = \" > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B log p \u03b8 (r 1:N ) = log p \u03b8 (c) N i=1 p \u03b8 (r i |z i )p \u03b8 (z i |c)dz i dc , where we marginalize over variables c and z 1:N . When generating a new review r i , given the set of previous reviews r 1:i , the information about these reviews has to be conveyed through the latent representations c and z i . This bottleneck is undesirable, as it will make it hard for the model to pass fine-grain information. For example, at generation time, the model should be reusing named entities (e.g., product names or technical characteristics) from other reviews rather than 'hallucinating' or avoiding generating them at all, resulting in generic and non-informative text. We alleviate this issue by letting the decoder directly access other reviews. We can formulate this as an autoregressive model: p \u03b8 (r 1:N |c) = N i=1 p \u03b8 (r i |r 1 , ..., r i\u22121 , c). (1) As we discuss in Section 2.3, the conditioning is instantiated using the pointer-generator mechanism (See et al., 2017) and, thus, will specifically help in generating rare words (e.g., named entities). We want our summarizer to equally rely on every review, without imposing any order (e.g., temporal) on the generation process. Instead, as shown in Figure 1b , when generating r i , we let the decoder access all other reviews within a group, r \u2212i = (r 1 , . . . , r i\u22121 , r i+1 , . . . , r N ). This is closely related to pseudolikelihood estimation (Besag, 1975) or Skip-Thought's objective (Kiros et al., 2015) . The final objective that we maximize for each group of reviews r 1:N : log p \u03b8 (c) N i=1 p \u03b8 (r i |z i , r i ) p \u03b8 (z i |c)dz i dc (2) We will confirm in ablation experiments that both hierarchical modeling (i.e., using c) and the direct conditioning on other reviews are beneficial. Model Estimation As standard with VAEs and variational inference in general (Kingma and Welling, 2013), instead of directly maximizing the intractable marginal likelihood in Equation 2 , we maximize its lower bound: 3 L(\u03b8, \u03c6; r 1:N ) = E c\u223cq \u03c6 (c|r 1:N ) N i=1 E z i \u223cq \u03c6 (z i |r i ,c) [log p \u03b8 (r i |z i , r i )] \u2212 N i=1 D KL [q \u03c6 (z i |r i , c)||p \u03b8 (z i |c)] \u2212 D KL [q \u03c6 (c|r 1:N )||p \u03b8 (c)] . 3 See the derivations in Appendix A.1. concat \u2026 review 1 h T1 1 h T1 1 w T1 1 w T1 1 w 2 1 w 2 1 w 1 1 w 1 1 h 1 1 h 1 1 h 2 1 h 2 1 concat w TN N w TN N \u2026 h 1 N h 1 N h TN N h TN N review N w 1 N w 1 N word embeddings \u2026 GRU hidden states \u0125 \u0125 q (c|r 1:N ) q (c|r 1:N ) c c sampling z N z N sampling q (z|r N , c) q (z|r N , c) p \u2713 (z|c) p \u2713 (z|c) Figure 2 : Production of latent code z N for review r N . The lower bound includes two 'inference networks', q \u03c6 (c|r 1:N ) and q \u03c6 (z i |r i , c), which are neural networks parameterized with \u03c6 and will be discussed in detail in Section 2.3. They approximate the corresponding posterior distributions of the model. The first term is the reconstruction error: it encourages the quality reconstruction of the reviews. The other two terms are regularizers. They control the amount of information encoded in the latent representation by penalizing the deviation of the estimated posteriors from the corresponding priors, the deviation is measured in terms of the Kullback-Leibler (KL) divergence. The bound is maximized with respect to both the generative model's parameters \u03b8 and inference networks' parameters \u03c6. Due to Gaussian assumptions, the Kullback-Leibler (KL) divergence terms are available in closed form, while we rely on the reparameterization trick (Kingma and Welling, 2013) to compute gradients of the reconstruction term. The inference network predicting the posterior for a review-specific variable q \u03c6 (z i |r i , c) is needed only in training and is discarded afterwards. In contrast, we will exploit the inference network q \u03c6 (c|r 1:N ) when generating summaries, as discussed in Section 3. Design of Model Components Text Representations A GRU encoder (Cho et al., 2014) embeds review words w to obtain hidden states h. Those representations are reused across the system, e.g., in the inference networks and the decoder. The full architecture used to produce the latent codes c and z i is shown in Figure 2 . We make Gaussian assumptions for all distributions (i.e. posteriors and priors). As in Kingma and Welling (2013), we use separate linear projections (LPs) to compute the means and diagonal log-covariances. 2.3.2 Prior p(c) and posterior q \u03c6 (c|r 1:N ) We set the prior over group latent codes to the standard normal distribution, p(c) = N (c; 0, I). In order to compute the approximate posterior q \u03c6 (c|r 1:N ), we first predict the contribution ('importance') of each word in each review \u03b1 t i to the code of the group: \u03b1 t i = exp(f \u03b1 \u03c6 (m t i )) N j=1 T j k exp(f \u03b1 \u03c6 (m k j )) , where T i is the length of r i and f \u03b1 \u03c6 is a feed-forward neural network (FFNN) 4 which takes as input concatenated word embeddings and hidden states of the GRU encoder, m t i = [h t i \u2022 w t i ], and returns a scalar. Next, we compute the intermediate representation with the weighted sum: \u0125 = N i=1 T i t \u03b1 t i m t i . Finally, we compute the Gaussian's parameters using the affine projections: \u00b5 \u03c6 (r 1:N ) = L \u0125 + b L log \u03c3 \u03c6 (r 1:N ) = G \u0125 + b G 2.3.3 Prior p \u03b8 (z i |c) and posterior q \u03c6 (z i |r i ,c) To compute the prior on the review code z i , p \u03b8 (z i |c) = N (z i ; \u00b5 \u03b8 (c), I\u03c3 \u03b8 (c)), we linearly project the product code c. Similarly, to compute the parameters of the approximate posterior q \u03c6 (z|r i , c) = N (z; \u00b5 \u03c6 (r i , c), I\u03c3 \u03c6 (r i , c)), we concatenate the last encoder's state h T i i of the review r i and c, and perform affine transformations. Decoder p \u03b8 (r i |z i , r i ) To compute the distribution p \u03b8 (r i |z i , r i ), we use an auto-regressive GRU decoder with the attention mechanism (Bahdanau et al., 2015) and a pointergenerator network. We compute the context vector c t i = att(s t i , h i ) by attending to all the encoder's hidden states h i of the other reviews r i of the group, where the decoder's hidden state s t i is used as a query. The hidden state of the decoder is computed using the GRU cell as s t i = GRU \u03b8 (s t\u22121 i , [w t i \u2022 c t\u22121 i \u2022 z i ]). (3) The cell inputs the previous hidden state s t\u22121 i , as well as concatenated word embedding w t i , context vector c t\u22121 i , and latent code z i . Finally, we compute the word distributions using the pointer-generator network g \u03b8 : p \u03b8 (r i |z i , r i ) = T t=1 g \u03b8 (r t i |s t i , c t i , w t i , r i ) (4) The pointer-generator network computes two internal word distributions that are hierarchically aggregated into one distribution (Morin and Bengio, 2005) . One distribution assigns probabilities to words being generated using a fixed vocabulary, and another one probabilities to be copied directly from the other reviews r i . In our case, the network helps to preserve details and, especially, to generate rare tokens. Summary Generation Given reviews r 1:N , we generate a summary that reflects common information using trained components of the model. Formally, we could sample a new review from p \u03b8 (r|r 1:N ) = E c\u223cq \u03c6 (c|r 1:N ) E z\u223cp \u03b8 (z|c) [p \u03b8 (r|z, r 1:N )] . As we argued in the introduction and will revisit in experiments, a summary or summarizing review, should be generated relying on the mean of the reviews' latent code. Consequently, instead of sampling z from p \u03b8 (z|c) = N (z; \u00b5 \u03b8 (c), I\u03c3 \u03b8 (c)), we set it to \u00b5 \u03b8 (c). We also found beneficial, in terms of evaluation metrics, not to sample c but instead to rely on the mean predicted by the inference network q \u03c6 (c|r 1:N ). Experimental Setup Datasets Our experiments were conducted on business customer reviews from the Yelp Dataset Challenge and Amazon product reviews (He and McAuley, 2016) . These were pre-processed similarly to Chu and Liu (2019) shown in Table 2 . Details of the pre-processing are available in Appendix A.2. These datasets present different challenges to abstractive summarization systems. Yelp reviews contain much personal information and irrelevant details which one may find unnecessary in a summary. Our summarizer, therefore, needs to distill important information in reviews while abstracting away from details such as a listing of all items on the menu, or mentions of specific dates or occasions upon which customers visited a restaurant. On the contrary, in Amazon reviews, we observed that users tend to provide more objective information and specific details that are useful for decision making (e.g., the version of an electronic product, its battery life, its dimensions). In this case, it would be desirable for our summarizer to preserve this information in the output summary. For evaluation, we used the same 100 humancreated Yelp summaries released by Chu and Liu (2019) . These were generated by Amazon Mechanical Turk (AMT) workers, who summarized 8 input reviews. We created a new test for Amazon reviews following a similar procedure (see Appendix A.6 for details). We sampled 60 products and 8 reviews for each product, and they were shown to AMT workers who were asked to write a summary. We collected three summaries per product, 28 products were used for development and 32 for testing. Experimental Details We used GRUs (Cho et al., 2014) for sequential encoding and decoding we used GRUs. We randomly initialized word embeddings that were shared across the model as a form of regularization (Press and Wolf, 2017) . Further, optimization was performed using Adam (Kingma and Ba, 2014) . In order to overcome the \"posterior collapse\" (Bowman et al., 2016) , both for our model and the vanilla VAE baseline, we applied cyclical annealing (Fu et al., 2019) Baseline Models Opinosis is a graph-based abstractive summarizer (Ganesan et al., 2010) designed to generate short opinions based on highly redundant texts. Although it is referred to as abstractive, it can only select words from the reviews. LexRank is an unsupervised algorithm which selects sentences to appear in the summary based on graph centrality (sentences represent nodes in a graph whose edges have weights denoting similarity computed with tf-idf). A node's centrality can be measured by running a ranking algorithm such as PageRank (Page et al., 1999) . MeanSum 5 is the unsupervised abstractive summarization model (Chu and Liu, 2019) discussed in the introduction. We also trained a vanilla text VAE model (Bowman et al., 2016) with our GRU encoder and decoder. When generating a summary for r 1 , ..., r N , we averaged the means of q \u03c6 (z i |r i ). Finally, we used a number of simple summarization baselines. We computed the clustroid review for each group as follows. We took each review from a group and computed ROUGE-L with respect to all other reviews. The review with the highest ROUGE score was selected as the clustroid review. Furthermore, we sampled a random review from each group as the summary, and constructed the summary by selecting the leading sentences from each review of a group. Additionally, as an upper bound, we report the performance of an oracle review, i.e., the highest-scoring review in a group when computing ROUGE-L against reference summaries. 5 Evaluation Results Automatic Evaluation As can be seen in Tables 3 and 4 , our model, Copycat, yields the highest scores on both Yelp and Amazon datasets. We observe large gains over the vanila VAE. We conjecture that the vanilla VAE struggles to properly represent the variety of categories under a single prior p(z). For example, reviews about a sweater can result in a summary about socks (see example summmaries in Appendix). This contrasts with our model which allows each group to have its own prior p \u03b8 (z|c) and access to other reviews during decoding. The gains are especially large on the Amazon dataset, which is very broad in terms of product categories. Our model also substantially outperforms Mean-Sum. As we will confirm in human evaluation, MeanSum's summaries are relatively fluent at the sentence level but often contain hallucinations, i.e., information not present in the input reviews. Human Evaluation Best-Worst Scaling We performed human evaluation using the AMT platform. We sampled 50 businesses from the human-annotated Yelp test set and used all 32 test products from the Amazon set. We recruited 3 workers to evaluate each tuple containing summaries from MeanSum, our model, LexRank, and human annotators. The reviews and summaries were presented to the workers in random order and were judged using Best-Worst Scaling (Louviere and Woodworth, 1991; Louviere et al., 2015) . BWS has been shown to produce more reliable results than ranking scales (Kiritchenko and Mohammad, 2016 criteria listed below (we show an abridged version below, the full set of instructions is given in Appendix A.5). The non-redundancy and coherence criteria were taken from Dang (2005) . Fluency: the summary sentences should be grammatically correct, easy to read and understand; Coherence: the summary should be well structured and well organized; Non-redundancy: there should be no unnecessary repetition in the summary; Opinion consensus: the summary should reflect common opinions expressed in the reviews; Overall: based on your own criteria (judgment) please select the best and the worst summary of the reviews. For every criterion, a system's score is computed as the percentage of times it was selected as best minus the percentage of times it was selected as worst (Orme, 2009) . The scores range from -1 (unanimously worst) to +1 (unanimously best). On Yelp, as shown in Table 5 , our model scores higher than the other models according to most criteria, including overall quality. The differences with other systems are statistically significant for all the criteria at p < 0.01, using post-hoc HD Tukey tests. The difference in fluency between our system and gold summaries is not statistically significant. The results on Amazon are shown in Table 6 . Our system outperforms other methods in terms of fluency, coherence, and non-redundancy. As with Yelp, it trails LexRank according to the opinion consensus criterion. Additionally, LexRank is slightly preferable overall. All pairwise differences between our model and comparison systems are statistically significant at p < 0.05. Opinion consensus (OC) is a criterion that cap-tures the coverage of common opinions, and it seems to play a different role in the two datasets. On Yelp, LexRank has better coverage compared to our model, as indicated by the higher OC score, but is not preferred overall. In contrast, on Amazon, while the OC score is on the same par, LexRank is preferred overall. We suspect that presenting a breadth of exact details on Amazon is more important than on Yelp. Moreover, LexRank tends to produce summaries that are about 20 tokens longer than ours resulting in better coverage of input details. Content Support The ROUGE metric relies on unweighted n-gram overlap and can be insensitive to hallucinating facts and entities (Falke et al., 2019) . For example, referring to a burger joint as a veggie restaurant is highly problematic from a user perspective but yields only marginal differences in ROUGE. To investigate how well the content of the summaries is supported by the input reviews, we performed a second study. We used the same sets as in the human evaluation in Section 5.2, and split MeanSum and our system's summaries into sentences. Then, for each summary sentence, we assigned 3 AMT workers to assess how well the sentence is supported by the reviews. Workers were advised to read the reviews and rate sentences using one of the following three options. Full support: all the content is reflected in the reviews; Partial support: only some content is reflected in the reviews; No support: content is not reflected in the reviews. The results in Analysis Ablations To investigate the importance of the model's individual components, we performed ablations by removing the latent variables (z i and c, one at a time), and attention over the other reviews. The models were re-trained on the Amazon dataset. The results are shown in Table 8 . They indicate that all components play a role, yet the most significant drop in ROUGE was achieved when the variable z was removed, and only c remained. Summaries obtained from the latter system were wordier and looked more similar to reviews. Dropping the attention (w/o r i ) results in more generic summaries as the model cannot copy details from the input. Finally, the smallest quality drop in terms of ROUGE-L was observed when the variable c was removed. In the introduction, we hypothesized that using the mean of latent variables would result in more \"grounded\" summaries reflecting the content of the input reviews, whereas sampling would yield texts with many novel and potentially irrelevant details. To empirically test this hypothesis, we sampled the latent variables during summary generation, as opposed to using mean values (see Section 3). We indeed observed that the summaries were wordier, less fluent, and less aligned to the input reviews, as is also reflected in the ROUGE scores (Table 8 ). Copy Mechanism Finally, we analyzed which words are copied by the full model during summary generation. Generally, the model copies around 3-4 tokens per summary. We observed a tendency to copy product-type specific words (e.g., shoes) as well as brands and names. Related Work Extractive weakly-supervised opinion summarization has been an active area of research. A recent example is Angelidis and Lapata (2018) . First, they learn to assign sentiment polarity to review segments in a weakly-supervised fashion. Then, they induce aspect labels for segments relying on a small sample of gold summaries. Finally, they use a heuristic to construct a summary of segments. Opinosis (Ganesan et al., 2010) does not use any supervision. The model relies on redundancies in opinionated text and PoS tags in order to generate short opinions. This approach is not well suited for the generation of coherent long summaries and although it can recombine fragments of input text, it cannot generate novel words and phrases. LexRank (Erkan and Radev, 2004 ) is an unsupervised extractive approach which builds a graph in order to determine the importance of sentences, and then selects the most representative ones as a summary. Isonuma et al. (2019) introduce an unsupervised approach for single review summarization, where they rely on latent discourse trees. Other earlier approaches (Gerani et al., 2014; Di Fabbrizio et al., 2014) relied on text planners and templates, while our approach does not require rules and can produce fluent and varied text. Finally, conceptually related methods were applied to unsupervised single sentence compression (West et al., 2019; Baziotis et al., 2019; Miao and Blunsom, 2016) . The most related approach to ours is MeanSum (Chu and Liu, 2019) which treats a summary as a discrete latent state of an autoencoder. In contrast, we define a hierarchical model of a review collection and use continuous latent codes. Conclusions In this work, we presented an abstractive summarizer of opinions, which does not use any summaries in training and is trained end-to-end on a large collection of reviews. The model compares favorably to the competitors, especially to the only other unsupervised abstractive multi-review summarization system. Furthermore, human evaluation of the generated summaries (by considering their alignment with the reviews) shows that those created by our model better reflect the content of the input. Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379-389. Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of Association for Computational Linguistics (ACL). Ivan Titov and Ryan McDonald. 2008 . Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, pages 111-120. ACM. Peter West, Ari Holtzman, Jan Buys, and Yejin Choi. 2019. Bottlesum: Unsupervised and self-supervised sentence summarization using the information bottleneck principle. arXiv preprint arXiv:1909.07405. A Appendices A.1 Derivation of the Lower Bound To make the notation below less cluttered, we make a couple of simplifications: q \u03c6 (c|\u2022) = q \u03c6 (c|r 1:N ) and q \u03c6 (z|i) = q \u03c6 (z|r i , c). log p \u03b8 (c) N i=1 p \u03b8 (r i |c, r i )dc = log p \u03b8 (c) N i=1 p \u03b8 (r i , z|c, r i )dz dc = log p \u03b8 (c) q \u03c6 (c|\u2022) q \u03c6 (c|\u2022) N i=1 p \u03b8 (r i , z|c, r i ) q \u03c6 (z|i) q \u03c6 (z|i) dz dc = log E c\u223cq \u03c6 (c|\u2022) p \u03b8 (c) q \u03c6 (c|\u2022) N i=1 E z\u223cq \u03c6 (z|i) p \u03b8 (r i , z|c, r i ) q \u03c6 (z|i) \u2265 E c\u223cq \u03c6 (c|\u2022) N i=1 log E z\u223cq \u03c6 (z|i) p \u03b8 (r i , z|c, r i ) q \u03c6 (z|i) \u2212 D KL [q \u03c6 (c|\u2022))||p \u03b8 (c)] \u2265 E c\u223cq \u03c6 (c|\u2022) N i=1 E z\u223cq \u03c6 (z|i) log p \u03b8 (r i , z|c, r i ) q \u03c6 (z|i) \u2212 D KL [q \u03c6 (c|\u2022))||p \u03b8 (c)] = E c\u223cq \u03c6 (c|\u2022) N i=1 E z\u223cq \u03c6 (z|i) [log p \u03b8 (r i |z, r i )] \u2212 N i=1 D KL [q \u03c6 (z|i)||p \u03b8 (z|c)] \u2212 D KL [q \u03c6 (c|\u2022)||p \u03b8 (c)] (5) A.2 Dataset Pre-Processing We selected only businesses and products with a minimum of 10 reviews, and thee minimum and maximum length of 20 and 70 words respectively, popular groups above the 90 th percentile were removed. And each group was set to contain 8 reviews during training. From the Amazon dataset we selected 4 categories: Electronics; Clothing, Shoes and Jewelry, Home and Kitchen; Health and Personal Care. A.3 Hyperparameters For sequential encoding and decoding, we used GRUs (Cho et al., 2014) with 600-dimensional hidden states. The word embeddings dimension was set to 200, and they were shared across the model (Press and Wolf, 2017) . The vocabulary size was set to 50,000 most frequent words, and an extra 30,000 were allowed in the extended vocabulary, the words were lower-cased. We used the Moses' (Koehn et al., 2007) reversible tokenizer and truecaser. Xavier uniform initialization (Glorot and Bengio, 2010) of 2D weights was used, and 1D weights were initialized with the scaled normal noise (\u03c3 = 0.1). We used the Adam optimizer (Kingma and Ba, 2014) , and set the learning rate to 0.0008 and 0.0001 on Yelp and Amazon, respectively. For summary decoding, we used lengthnormalized beam search of size 5, and relied on latent code means. In order to overcome \"posterior collapse\" (Bowman et al., 2016) we applied cycling annealing (Fu et al., 2019) with r = 0.8 for both the z and c related KL terms, with a new cycle over approximately every 2 epochs over the training set. The maximum annealing scalar was set to 1 for z-related KL term in on both datasets, and 0.3 and 0.65 for c-related KL-term on Yelp and Amazon, respectively. The reported ROUGE scores are based on F1. The dimensions of the variables c and z were set to 600, and the c posterior's scoring neural network had a 300-dimensional hidden layer and the tanh non-linearity. The decoder's attention mechanism used a single layer neural network with a 200-dimensional hidden layer, and the tanh non-linearity. The copy gate in the pointer-generator network was computed with a 100-dimensional single-hidden layer network, with the same non-linearity. A.4 Human Evaluation Setup To perform the human evaluation experiments described in Sections 5.2 and 5.2 we combined both tasks into single Human Intelligence Tasks (HITs). Namely, the workers needed to mark sentences as described in Section 5.2, and then proceed to the task in Section 5.2. We explicitly asked then to re-read the reviews before each task. For worker requirements we set 98% approval rate, 1000+ HITS, Location: USA, UK, Canada, and the maximum score on a qualification test that we designed. The test was asking if the workers are native English speakers, and verifying that they correctly understand the instructions of both tasks by completing a mini version of the actual HIT. A.5 Full Human Evaluation Instructions \u2022 Fluency: The summary sentences should be grammatically correct, easy to read and understand. \u2022 Coherence: The summary should be well structured and well organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic. \u2022 Non-redundancy: There should be no unnecessary repetition in the summary. Unnecessary repetition might take the form of whole sentences that are repeated, or repeated facts, or the repeated use of a noun or noun phrase (e.g., \"Bill Clinton\") when a pronoun (\"he\") would suffice. \u2022 Opinion consensus: The summary should reflect common opinions expressed in the reviews. For example, if many reviewers complain about a musty smell in the hotel's rooms, the summary should include this information. \u2022 Overall: Based on your own criteria (judgment) please select the best and the worst summary of the reviews. A.6 Amazon Summaries Creation First, we sampled 15 products from each of the Amazon review categories: Electronics; Clothing, Shoes and Jewelry; Home and Kitchen; Health and Personal Care. Then, we selected 8 reviews from each product to be summaries. We used the same requirements for workers as for human evaluation in A.4. We assigned 3 workers to each product, and instructed them to read the reviews and produce a summary text. We followed the instructions provided in (Chu and Liu, 2019) , and used the following points in our instructions: \u2022 The summary should reflect common opinions about the product expressed in the reviews. Try to preserve the common sentiment of the opinions and their details (e.g. what exactly the users like or dislike). For example, if most reviews are negative about the sound quality, then also write negatively about it. Please make the summary coherent and fluent in terms of sentence and information structure. Iterate over the written summary multiple times to improve it, and re-read the reviews whenever necessary. \u2022 Please write your summary as if it were a review itself, e.g. 'This place is expensive' instead of 'Users thought this place was expensive'. Keep the length of the summary reasonably close to the average length of the reviews. \u2022 Please try to write the summary using your own words instead of copying text directly from the reviews. Using the exact words from the reviews is allowed, but do not copy more than 5 consecutive words from a review . A.7 Latent Codes Analysis We performed a qualitative analysis of the latent variable z to shed additional light on what it stores and sensitivity of the decoder with respect to its input. Specifically, we computed the mean value for the variable c using the approximate posterior q \u03c6 (c|r 1 , ..., r N ), and then sampled z from the prior p \u03b8 (z|c). First, we observed that the summaries produced using the mean of z are more fluent. For example, in Table 9 , the z 1 based summary states: \"The picture quality is very good, but it doesn't work aswell as the picture.\", where the second phrase could be rewritten in a more fluent matter. Also, we found that mean based summaries contain less details that are partially or not supported by the reviews. For example, in the table, z 1 based summary mentions Kindle Fire HD 8.9', while the dimension is never mentioned in the reviews. Finally, different samples were observed to result in texts that contain different details about the reviews. For example, z 1 sample results in the summary that captures the picture quality, while z 3 that the item is good for its price. Overall, we observed that the latent variable z stores content based information, that results in syntactically diverse texts, yet reflecting information about the same businesses or product. A.8 Repetitions We observed an increase in the amount of generated repetitions both in the reconstructed reviews and summaries when the z-related KL term is low and beam search is used. Intuitively, the initial input to the decoder becomes less informative, and it starts relying on learned local statistics to perform reconstruction. When the KLD vanishes to zero, the decoder essentially becomes a uncoditional language model, for which beam search was shown to lead to generation of repetitions (Holtzman et al., 2019) . Ours This place is the best Mexican restaurant i have ever been to. The food was delicious and the staff was very friendly and helpful. Our server was very attentive and made sure we were taken care of. We'll be back for sure. MeanSum A little on the pricey side but I was pleasantly surprised. We went there for a late lunch and it was packed with a great atmosphere, food was delicious and the staff was super friendly. Very friendly staff. We had the enchiladas with a few extra veggies and they were delicious! Will be back for sure! LexRank We will definitely be going back for more great food! Everything we had so far was great. The staff was great and so nice! Good food! Great atmosphere! Gold This place is simply amazing! Its the Mexican spot in town. Their tacos are delicious and full of flavor. They also have chips and salsa that is to die for! The salsa is just delectable! It has a sweet, tangy flavor that you can't find anywhere else. I highly recommend! Rev 1 Classic style Mexican food done nicely! Yummy crispy cheese crisp with a limey margarita will will win my heart any day of the week! The classic frozen with a chambord float is my favorite and they do it well here.The salad carbon was off the chain-served on a big platter and worked for me as 2 full dinners. Rev 2 For delicious Mexican food in north Phoenix, try La Pinata. This was our visit here and we were so stunned by the speed in which our food was prepared that we were sure it was meant for another table. The food was hot and fresh and well within our budget. My husband got a beef chimichanga and I got bean and cheese burrito, which we both enjoyed. Chips and salsa arrived immediately; the salsa tastes sweeter than most and is equally flavorful. We will be back! Rev 3 Good food! Great atmosphere! Great patio. Staff was super friendly and accommodating! We will definately return! Rev 4 This place was very delicious! I got the ranchero burro and it was so good. The plate could feed at least two people. The staff was great and so nice! I also got the fried ice cream it was good. I would recommend this place to all my friends. Rev 5 We arrive for the first time, greeted immediately with a smile and seated promptly. Our server was fantastic, he was funny and fast. Gave great suggestions on the menu and we both were very pleased with the food, flavors, speed and accuracy of our orders. We will definitely be going back for more great food! Rev 6 Well was very disappointed to see out favorite ice cream parlor closed but delightfully surprised at how much we like this spot!!Service was FANTASTIC TOP notch!! Taco was great lots of cheese. Freshly deep fried shell not like SO MANY Phoenix mex restaurants use! Enchilada was very good. My wife really enjoyed her chimichanga. My moms chilli reanno was great too. Everything we had so far was great. We will return. Highly recommended. Rev 7 I'm only on the salsa and it's just as fabulous as always. I love the new location and the decor is beautiful. Open 5 days and the place is standing room only. To the previous negative commentor, they are way took busy to fill an order for beans. Go across the street....you'll be angry lol. Rev 8 I just tried to make a reservation for 15 people in March at 11 am on a Tuesday and was informed by a very rude female. She said \"we do not take reservations\" and I asked if they would for 15 people and she said \" I told you we don't take reservations\" and hung up on me. Is that the way you run a business? Very poor customer service and I have no intentions of ever coming there or recommending it to my friends. Ours This place is the worst service I've ever had. The food was mediocre at best. The service was slow and the waiter was very rude. I would not recommend this place to anyone who wants to have a good time at this location. MeanSum I love the decor, but the food was mediocre. Service is slow and we had to ask for refills. They were not able to do anything and not even charge me for it. It was a very disappointing experience and the service was not good at all. I had to ask for a salad for a few minutes and the waitress said he didn't know what he was talking about. All I can say is that the staff was nice and attentive. I would have given 5 stars if I could. LexRank Food was just okay, server was just okay. The atmosphere was great, friendly server. It took a bit long to get a server to come over and then it took our server a while to get our bread and drinks. However there was complementary bread served.The Pizza I ordered was undercooked and had very little sauce.Macaroni Grill has unfortunately taken a dive. Went to dinner with 4 others and had another bad experience at the Macaroni Grill. Gold I'm really not a fan of Macaroni Grill, well, at least THIS Macaroni Grill. The staff is slow and really doesn't seem to car about providing quality service. It took well over 30 minutes to get my food and the place wasn't even packed with people. I ordered pizza and it didn't taste right. I think it wasn't fully cooked. I won't be coming back. Rev 1 10/22/2011 was the date of our visit. Food was just okay, server was just okay. The manager climbed up on the food prep counter to fix a light. We felt like that was the most unsanitary thing anyone could do -he could have just come from the restroom for all we knew. Needless to say, lackluster service, mediocre food and lack of concern for the cleanliness of the food prep area will guarantee we will NEVER return. Rev 2 We like the food and prices are reasonable. Our biggest complaint is the service. It took a bit long to get a server to come over and then it took our server a while to get our bread and drinks. They really need to develop a better sense of teamwork. While waiting for things there were numerous servers standing around gabbing. It really gave us the impression of \"Not my table . \" \"Not my problem.\" Only other complaint is they need to get some rinse aid for the dishwasher. I had to dry our bread plates when the hostess gave them to us. Rev 3 Not enough staff is on hand the two times I have been in to properly pay attention to paying customers. I agree that the portions have shrunk over the years, and the effort is no longer there. It is convenient to have nearby but not worth my time when other great restaurants are around. Wish I could rate it better but it's just not that good at all. Rev 4 Went to dinner with 4 others and had another bad experience at the Macaroni Grill. When will we ever learn? The server was not only inattentive, but p o'd when we asked to be moved to another table. When the food came it was at best, luke warm. They had run out of one of our ordered dishes, but didn't inform us until 20 minutes after we had ordered. Running out at 6:00 p.m.: Really? More delay and no apologies. There is no excuse for a cold meal and poor service. We will not go back since the Grill seems not to care and there are plenty of other restaurants which do. Rev 5 The service is kind and friendly. However there was complementary bread served.The Pizza I ordered was undercooked and had very little sauce.Macaroni Grill has unfortunately taken a dive. Best to avoid the place or at the very least this location. Rev 6 I know this is a chain, but Between this and Olive Garden, I would def pick this place. Service was great at this location and food not bad at all, although not excellent, I think it still deserves a good 4 stars Rev 7 I had a 2 for 1 $9.00 express dinner coupon so we order up 2 dinners to go. The deal was 9 min or its free, it took 20, but since I was getting 2 meals for $9.00 I did not make a fuss. The actual pasta was fine and amount was fair but it had maybe a 1/4 of a chicken breast. The chicken tasted like it came from Taco Bell, VERY processed. The sauce straight from a can. I have had much better frozen dinners. My husband and I used to like Macaroni Grill it sad too see its food go so down hill. Rev 8 The atmosphere was great, friendly server. Although the food I think is served from frozen. I ordered mama trio. The two of three items were great. Plate came out hot, couldn't touch it. Went to eat lasagna and was ice cold in the center, nit even warm. The server apologized about it offered new one or reheat this one. I chose a new one to go. I saw her go tell manager. The manager didn't even come over and say anything. I was not even acknowledged on my way out and walked past 3 people. I will not be going back. Over priced for frozen food. Table 11 : Yelp summaries produced by different models. Ours My wife and i have been here several times now and have never had a bad meal. The service is impeccable, and the food is delicious. We had the steak and lobster, which was delicious. I would highly recommend this place to anyone looking for a good meal. MeanSum Our first time here, the restaurant is very clean and has a great ambiance. I had the filet mignon with a side of mashed potatoes. They were both tasty and filling. I've had better at a chain restaurant, but this is a great place to go for a nice dinner or a snack. Have eaten at the restaurant several times and have never had a bad meal here. LexRank Had the filet... Really enjoyed my filet and slobster. In addition to excellent drinks, they offer free prime filet steak sandwiches. I have had their filet mignon which is pretty good, calamari which is ok, scallops which aren't really my thing, sour dough bread which was fantastic, amazing stuffed mushrooms. Very good steak house. Gold The steak is the must have dish at this restaurant. One small problem with the steak is that you want to order it cooked less than you would at a normal restaurant. They have the habit of going a bit over on the steak. The drinks are excellent and the stuffed mushrooms as appetizers were amazing. This is a classy place that is also romantic. The staff pays good attention to you here. Rev 1 The ambiance is relaxing, yet refined. The service is always good. The steak was good, although not cooked to the correct temperature which is surprising for a steakhouse. I would recommend ordering for a lesser cook than what you normally order. I typically order medium, but at donovan's would get medium rare. The side dish menu was somewhat limited, but we chose the creamed spinach and asparagus, both were good. Of course, you have to try the creme brulee -Yum! Rev 2 Hadn't been there in several years and after this visit I remember why, I don't like onions or shallots in my macaroni and cheese. The food is good but not worth the price just a very disappointing experience and I probably won't go back Rev 3 My wife and I come here every year for our anniversary (literally every year we have been married). The service is exceptional and the food quality is top-notch. Furthermore, the happy hour is one of the best in the Valley. In addition to excellent drinks, they offer free prime filet steak sandwiches. I highly recommend this place for celebrations or a nice dinner out. Rev 4 I get to go here about once a month for educational dinners. I have never paid so don't ask about pricing. I have had their filet mignon which is pretty good, calamari which is ok, scallops which aren't really my thing, sour dough bread which was fantastic, amazing stuffed mushrooms. The vegetables are perfectly cooked and the mashed potatoes are great. At the end we get the chocolate mousse cake that really ends the night well. I have enjoyed every meal I have eaten there. Rev 5 Very good steak house. Steaks are high quality and the service was very professional. Attentive, but not hovering. Classic menus and atmosphere for this kind of restaurant. No surprises. A solid option, but not a clear favorite compared to other restaurants in this category. Rev 6 Had a wonderful experience here last night for restaurant week. Had the filet... Which was amazing and cooked perfectly with their yummy mashed potatoes and veggies. The bottle of red wine they offered for an additional $20 paired perfectly with the dinner. The staff were extremely friendly and attentive. Can't wait to go back! Rev 7 The seafood tower must change in selection of seafood, which is good, which is also why mine last night was so fresh fresh delicious. Its good to know that you can get top rate seafood in Phoenix. Bacon wrapped scallops were very good, and I sacrificied a full steak (opting for the filet medallion) to try the scallops. I asked for medium rare steak, but maybe shouldve asked for rare...my cousin had the ribeye and could not have been any happier than he was :) yum for fancy steak houses. Its an ultra romantic place to, fyi.the wait staff is very attentive. Rev 8 Donovans, how can you go wrong. Had some guests in town and some fantastic steaks paired with some great cabernets. Really enjoyed my filet and lobster. Table 12 : Yelp summaries produced by different models. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. Also, Stefanos Angelidis for help with the data as well as Jonathan Mallinson, Serhii Havrylov, and other members of Edinburgh NLP group for discussion. We gratefully acknowledge the support of the European Research Council (Titov: ERC StG BroadSem 678254; Lapata: ERC CoG TransModal 681760) and the Dutch National Science Foundation (NWO VIDI 639.022.518). mean z Bought this for my Kindle Fire HD and it works great. I have had no problems with it. I would recommend it to anyone looking for a good quality cable. z1 Works fine with my Kindle Fire HD 8.9\". The picture quality is very good, but it doesn't work as well as the picture. I'm not sure how long it will last, but i am very disappointed. z2 This is a great product. I bought it to use with my Kindle Fire HD and it works great. I would recommend it to anyone who is looking for a good quality cable for the price. Table 9 : Amazon summaries of the full model with sampled and mean assignment to z. The assignment to c was fixed, and was the mean value based on the approximate posterior q \u03c6 (c|r 1 , ..., r N ). Ours I love this tank. It fits well and is comfortable to wear. I wish it was a little bit longer, but I'm sure it will shrink after washing. I would recommend this to anyone. MeanSum I normally wear a large so it was not what I expected. It's a bit large but I think it's a good thing. I'm 5 '4 \"and the waist fits well. I'm 5 '7 and this is a bit big. LexRank I'm 5 '4 'and this tank fits like a normal tank top, not any longer. The only reason I'm rating this at two stars is because it is listed as a 'long' tank top and the photo even shows it going well past the models hips, however I'm short and the tank top is just a normal length. I bought this tank to wear under shirts when it is colder out. I was trying to find a tank that would cover past my hips, so I could wear it with leggings. Gold Great tank top to wear under my other shirts as I liking layering and the material has a good feel. There was a good choice of colors to pick from. Although, the top is a thin material I don't mind since I wear it under something else. Rev 1 The description say it long... NOT so it is average. That's why I purchased it because it said it was long. This is a basic tank.I washed it and it didn't warp but did shrink a little. Nothing to brag about. Rev 2 I'm 5 '4 'and this tank fits like a normal tank top, not any longer. I was trying to find a tank that would cover past my hips, so I could wear it with leggings. Don't order if you're expecting tunic length. Rev 3 This shirt is OK if you are layering for sure. It is THIN and runs SMALL. I usually wear a small and read the reviews and ordered a Medium. It fits tight and is NOT long like in the picture. Glad I only purchased one. Rev 4 The tank fit very well and was comfortbale to wear. The material was thinner than I expected, and I felt it was probably a little over priced. I've bought much higher quality tanks for $5 at a local store. Rev 5 The only reason I'm rating this at two stars is because it is listed as a 'long' tank top and the photo even shows it going well past the models hips, however I'm short and the tank top is just a normal length. Rev 6 I usually get them someplace out but they no longer carry them. I thought I would give these a try. I received them fast, although I did order a brown and got a black (which I also needed a black anyway). They were a lot thinner than I like but they are okay. Rev 7 Every women should own one in every color. They wash well perfect under everything. Perfect alone. As I write I'm waiting on another of the same style to arrive. Just feels quality I don't know how else to explain it, but I'm sure you get it ladies! Rev 8 I bought this tank to wear under shirts when it is colder out. I bought one in white and one in an aqua blue color. They are long enough that the color peeks out from under my tops. Looks cute. I do wish that the neck line was a bit higher cut to provide more modest coverage of my chest. Gold These acupressure mats are used to increase circulation and reduce body aches and pains and are most effective when you can fully relax. Consistence is key to receive the full, relaxing benefits of the product. However, if you are using this product after surgery it is responsible to always consult with your physician to ensure it is right for your situation. Rev 1 Always consult with your doctor before purchasing any circulation product after surgery. I had ankle surgery and this product is useful for blood circulation in the foot. This increase in circulation has assisted with my ability to feel comfortable stepping down on the foot (only after doc said wait bearing was okay). I use it sitting down barefoot. Rev 2 I really like the Acupressure Mat. I usually toss and turn a lot when I sleep, now I use this before I go to bed and it helps relax my body so that I can sleep more sound without all the tossing and turning. Rev 3 I used the mat the first night after it arrived and every-other night since."
}