{
    "article": "We describe an approach ( \\ v ariant transduction\") aimed at reducing the e ort and skill involved in building spoken language interfaces. Applications are created by specifying a relatively small set of example utterance-action pairs grouped into contexts. No intermediate semantic representations are involved in the speci cation, and the con rmation requests used in the dialog are constructed automatically. These properties of variant transduction arise from combining techniques for paraphrase generation, classi cation, and examplematching. We describe how a s p oken dialog system is constructed with this approach and also provide some experimental results on varying the number of examples used to build a particular application. Introduction Developing non-trivial interactive s p o k en language applications currently requires signicant e ort, often several person-months. A major part of this e ort is aimed at coping with variation in the spoken language input by users. One approach to handling variation is to write a large natural language grammar manually and hope that its coverage is su cient f o r m ultiple applications (Dowding et al., 1994) . Another approach is to create a simulation of the intended system (typically with a human in the loop) and then record users interacting with the simulation. The recordings are then transcribed and annotated with semantic information relating to the domain the transcriptions and annotations can then be used to create a statistical understanding model (Miller et al., 1998) or used as guidance for manual grammar development (Aust et al., 1995) . Building mixed initiative s p o k en language systems currently usually involves the design of semantic representations speci c to the application domain. These representations are used to pass data between the language processing components: understanding, dialog, con rmation generation, and response generation. However, such representations tend to be domain-speci c, and this makes it di cult to port to new domains or to use machine learning techniques without extensive handlabeling of data with the semantic representations. Furthermore, the use of intermediate semantic representations still requires a nal transduction step from the intermediate representation to the action format expected by the application back-end (e.g. SQL database query or procedure call). For situations when the e ort and expertise available to build an application is small, the methods mentioned above are impractical, and highly directed dialog systems with little allowance for language variability are constructed. In this paper, we describe an approach t o constructing interactive s p o k en language applications aimed at alleviating these problems. We rst outline the characteristics of the method (section 2) and what needs to be provided by the application builder (sec-tion 3). In section 4 and section 5 we explain variant expansion and the operation of the system at runtime, and in section 6 we describe how con rmation requests are produced by the system. In section 7 we g i v e some initial experimental results on varying the number of examples used to construct a call-routing application. Characteristics of our approach The goal of the approach discussed in this paper (which w e r e f e r t o a s \\ v ariant transduction\") is to avoid the e ort and specialized expertise used to build current research p r ototypes, while allowing more natural spoken input than is handled by s p o k en dialog systems built using current commercial practice. This led us to adopt the following constraints: Applications are constructed using a relatively small number of example inputs (no grammar development or extensive data collection). No intermediate semantic representations are needed. Instead, manipulations are performed on word strings and on action strings that are nal (back-end) application calls. Con rmation queries posed by t h e s y stem to the user are constructed automatically from the examples, without the use of a separate generation component. Dialog control should be simple to specify for simple applications, while allowing the exibility of delegating this control to another module (e.g. an \\intelligent\" back-end agent) for more complex applications. We h a ve constructed two telephone-based applications using this method, an application to access email and a call-routing application. These two applications were chosen to gain experience with the method because they have di erent usage characteristics and back-end complexity. For the e-mail access system, usage is typically habitual, and the system's mapping of user utterances to backend actions needs to take i n to account d ynamic aspects of the current email session. For the call-routing application, the back-end calls executed by the system are relatively simple, but users may only encounter the system once, and the system's initial prompt is not intended to constrain the rst input spoken by the user. 3 Constructing an application with example-action contexts An interactive s p o k en language application constructed with the variant transduction method consists of a set of contexts. Each context provides the mapping between user inputs and application actions that are meaningful in a particular stage of interaction between the user and system. For example the e-mail reader application includes contexts for logging in and for navigating a mail folder. The actual contexts that are used at runtime are created through a four step process: 1. The application developer speci es (a small number of) triples he a ci where e is a natural language string (a typical user input), a is an application action (back-end application API call). For instance, the string read the message from John might be paired with the API call mailAgent.getWithSender(\"jsmith@att.com\"). The third element of a triple, c, i s a n expression identifying another (or the same) context, speci cally, the context the system will transition to if e is the closest match to the user's input. 2. The set of triples for each c o n text is expanded by the system into a larger set of triples. The additional triples are of the form hv a 0 c i where v is a \\variant\" of example e (as explained in section 4 below), and a 0 is an \\adapted\" version of the action a. 3. During an actual user session, the set of triples for a context may optionally be expanded further to take i n to account the dynamic aspects of a particular session. For example, in the mail access application, the set of names available for recognition is increased to include those present as senders in the user's current mail folder. 4. A speech recognition language model is compiled from the expanded set of examples. We currently use a language model that accepts any sequence of substrings of the examples, optionally separated by ller words, as well as sequences of digits. (For a small number of examples, a statistical N-gram model is ineffective because of low N-gram counts.) A detailed account of the recognition language model techniques used in the system is beyond the scope of this paper. In the current implementation, actions are sequences of statements in the Java language. Constructors can be called to create new objects (e.g. a mail session object) which c a n b e assigned to variables and referenced in other actions. The context interpreter loads the required classes and evaluates methods dynamically as needed. It is thus possible for an application developer to build a spoken interface to their target API without introducing any new Java classes. The system could easily be adapted to use action strings from other interpreted languages. A k ey property of the process described above is that the application developer needs to know only the back-end API and English (or some other natural language). Variant compilation Di erent expansion methods can be used in the second step to produce variants v of an example e. In the simplest case, v may b e a paraphrase of e. Such paraphrase variants are used in the experiments in section 7, where domain-independent \\carrier\" phrases are used to create variants. For example, the phrase I'd like to (among others) is used as a possible alternative for the phrase I want to. The context compiler includes an English-to-English paraphrase generator, so the applica-tion developer is not involved in the expansion process, relieving her of the burden of handling this type of language variation. We are also experimenting with other forms of variation, including those arising from lexicalsemantic relations, user-speci c customization, and those variants uttered by users during eld trials of a system. When v is a paraphrase of e, the adapted action a 0 is the same string as a. In the more general case, the meaning of variant v is different from that of e, and the system attempts (not always correctly) to construct a 0 so that it re ects this di erence in meaning. For example, including the variant show the message from Bill Wilson of an example read the message from John, i n volves modifying the action mailAgent.getWithSender(\"jsmith@att.com\") to mailAgent.getWithSender(\"wwilson@att.com\"). We currently adopt a simple approach t o the process of mapping language string variants to their corresponding target action string variants. The process requires the availability of a \\token mapping\" t between these two string domains, or data or heuristics from which such a mapping can be learned automatically. Examples of the token mapping are names to email addresses as illustrated in the example above, name to identi er pairs in a database system, \\soundex\" phonetic string spelling in directory applications, and a bilingual dictionary in a translation application. The process proceeds as follows: 1. Compute a set of lexical mappings between the variant v and example e. This is currently performed by aligning the two s t r i n g i n s u c h a w ay as that the alignment minimizes the (weighted) edit distance between them (Wagner and Fischer, 1974). 2. The token mapping t is used to map substitution pairs identi ed by the alignment ( hread showi and hJohn, Bill Wilsoni in the example above) to corresponding substitution pairs in the action string. In general this will result in a smaller set of substitution strings since not all word strings will be present i n the domain of t. (In the example, this results in the single pair hjsmith@att.com, wwilson@att.comi.) 3. The action substitution pairs are applied to a to produce a 0 . 4. The resulting action a 0 is checked for (syntactic) well-formedness in the action string domain the variant v is rejected if a 0 is ill-formed. Input interpretation When an example-action context is active during an interaction with a user, two c o mponents (in addition to the speech recognition language model) are compiled from the context in order to map the user inputs into the appropriate (possibly adapted) action: Classi er A classi er is built with training pairs hv a i where v is a variant o f a n example e for which the example action pair he ai is a member of the unexpanded pairs in the context. Note that the classi er is not trained on pairs with adapted examples a 0 since the set of adapted actions may be too large for accurate classi cation (with standard classi cation techniques). The classi ers typically use text features such as N-grams appearing in the training data. In our experiments, we h a ve used di erent classiers, including BoosTexter (Schapire and Singer, 2000) , and a classi er based on Phi-correlation statistics for the text features (see Alshawi and Douglas (2000) for our earlier application of Phi statistics in learning machine translation models from examples). Other classi ers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech r e cognizer and a variant v. V arious matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995) . So far we h a ve used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity b e t ween words such as the one described by P ereira et al. (1993) . The output of the matcher is a real number (the distance) and a distortion mapping represented as a sequence of edit operations (Wagner and Fischer, 1974) . Using these two components, the method for mapping the user's utterance to an executable action is as follows: 1. The language model derived from context c is activated in the speech recognizer. 2. The speech recognizer produces a string s from the user's utterance. 3. The classi er for c is applied to s to produce an unadapted action a. 4. The matcher is applied pairwise to compare s with each v ariant v a derived from a triple he a c 0 i in the unexpanded version of c. 5. The triple hv a 0 c 0 i for which v produces the smallest distance is selected and passed along with e to the dialog controller. The relationship between the input s, v ariant v, example e, and actions a and a 0 is depicted in Figure 1 . In the gure, f is the mapping between examples and actions in the unexpanded context r is the relation between examples and variants and g is the search mapping implemented by the classi ermatcher. The role of e 0 is related to con rmations as explained in the following section. Con rmation and dialog control Dialog control is straightforward as the reader might expect, except for two aspects described in this section: (i) evaluation of nextcontext expressions, and (ii) generation of p (prompt): say a mailreader command s (words spoken): now show me messages from Bill v (variant): show the message from Bill Wilson e (example): read the message from John a (associated action): mailAgent.getWithSender(\"jsmith@att.com\") a 0 (adapted action): mailAgent.getWithSender(\"wwilson@att.com\") e 0 (adapted example): read the message from Bill Wilson As noted in section 3 the third element c of each triple he a ci in a context is an expression that evaluates to the name of the next context (dialog state) that the system will transition to if the triple is selected. For simple applications, c can simply always be an identi er for a context, i.e. the dialog state transition network is speci ed explicitly in advance in the triples by the application developer. For more complex applications, next context expressions c may be calls that evaluate to context identi ers. In our implementation, these calls can be Java methods executed on objects known to the action interpreter. They may t h us be calls on the back-end application system, which is appro-priate for cases when the back-end has state information relevant to what should happen next (e.g. if it is an \\intelligent a g e n t\"). It might also be a call to component that implements a dialog strategy learning method (e.g. Levin and Pieraccini (1997) ), though we h a ve not yet tried such methods in conjunction with the present system. A con rmation request of the form do you mean e 0 is constructed for each v ariant-action pair (v a 0 ) of an example-action pair (e a). The string e 0 is constructed by rst computing a submapping h 0 of the mapping h representing the distortion between e and v. h 0 is derived from h by removing those edit operations which w ere not involved in mapping the action a to the adapted action a 0 . (The matcher is used to compute h except when the process of deriving (v a 0 ) from (e a) a lready includes an explicit representation of h and t(h).) The restricted mapping h 0 is used instead of h to construct e 0 in order to avoid misleading the user about the extent to which the application action is being adapted. Thus if h includes the substitution w ! w 0 but t(w) i s not a substring of a then this edit operation is not included in h 0 . T h i s w ay, e 0 includes w unchanged, so that the con rmation asked of the user does not carry the implication that the change w ! w 0 is taken into account i n t h e action a 0 to be executed by the system. For instance, in the example in Figure 2 , the word \\now\" in the user's input does not correspond to any part of the adapted action, and is not included in the con rmation string. In practice, the con rmation string e 0 is computed at the same time that the variant-action pair (v a 0 ) is derived from the original example pair (e a). The dialog ow o f c o n trol proceeds as follows: 1. The active c o n text c is set to a distinguished initial context c 0 indicated by the application developer. 2. A prompt associated with the current a ctive context c is played to the user using a speech s y n thesiser or by p l a ying an audio le. For this purpose the application developer provides a text string (or audio le) for each context in the application. 3. The user's utterance is interpreted as explained in the previous section to produce the triple hv a 0 c 0 i. 4. A match distance d is computed as the sum of the distance computed for the matcher between s and v and the distance computed by the matcher between v and e (where e is the example from which v was derived). 5. If d is smaller than a preset threshold, it is assumed that no con rmation is necessary and the next three steps are skipped. 6. The system asks the user do you mean: e 0 . If the user responds positively then proceed to the next step, otherwise return to step 2. 7. The action a 0 is executed, and any string output it produces is read to the user with the speech s y n thesizer. 8. The active context is set to the result of evaluating the expression c 0 . 9. Return to step 2. Figure 2 gives an example showing the strings involved in a dialog turn. Handling the user's verbal response to the con rmation is done with a built-in yes-no context. The generation of con rmation requests requires no work by the application developer. Our approach thus provides an even more extreme version of automatic con rmation generation than that used by C h u-Carroll and Carpenter (1999) where only a small e ort is required by the developer. In both cases, the bene ts of carefully crafted con rmation requests are being traded for rapid application development. Experiments An important question relating to our method is the e ect of the number of examples on system interpretation accuracy. To measure this e ect, we c hose the operator services call routing task described by Gorin et al. (1997) . We c hose this task because a reasonably large data set was available in the form of actual recordings of thousands of real customers calling AT&T's operators, together with transcriptions and manual labeling of the desired call destination. More speci cally, w e measure the call routing accuracy for unconstrained caller responses to the initial context prompt AT&T. How may I help you?. Another advantage of this task was that benchmark call routing accuracy gures were available for systems built with the full data set (Gorin et al., 1997 Schapire and Singer, 2000) . We h a ve n o t y et measured interpretation accuracy for the structurally more complex e-mail access application. In this experiment, the responses to How may I help you? are \\routed\" to fteen destinations, where routing means handing o the call to another system or human operator, or moving to another example-action context that will interact further with the user to elicit further information so that a subtask (such a s making a collect call) can be completed. Thus the actions in the initial context are simply the destinations, i.e. a = a 0 , and the matcher is only used to compute e 0 . The fteen destinations include a destination \\other\" which is treated specially in that it is also taken to be the destination when the system rejects the user's input, for example because the con dence in the output of the speech recognizer is too low. Following previous work on this task, cited above, we present the results for each experimental condition as an ROC curve plotting the routing accuracy (on non-rejected utterances) as a function of the false rejection rate (the percentage of the samples incorrectly rejected) a classi cation by the system of \\other\" is considered equivalent to rejection. The dataset consists of 8,844 utterances of which 1000 were held out for testing. We refer to the remaining 7,884 utterances as the \\full training dataset\". In the experiments, we v ary two conditions: Input uncertainty The input string to the interpretation component is either a human transcription of the spoken utterance or the output of a speech recognizer. The acoustic model used for automatic speech recognition was a general telephone speech HHM model in all cases. (For the full dataset, better results can be achieved by an applicationspeci c acoustic model, as presented by Gorin et al. (1997) and con rmed by our results below.) We therefore select k utterances for each destination, with k set to 3, 5, and 10, respectively. This selection is random, except for the provision that utterances appearing more than once are preferred, to approximate the notion of a typical utterance. The selected examples are expanded by the addition of variants, as described earlier. For each v alue of k, the results shown are for the median of three runs. Size of example set Figure 3 shows the routing accuracy ROC curves for transcribed input for k = 3 5 10 and for the full training dataset. These results for transcribed input were obtained with BoosTexter (Schapire and Singer, 2000) as the classi er module in our system because we have observed that BoosTexter generally outperforms our Phi classi er (mentioned earlier) for text input. Figure 4 shows the corresponding four ROC curves for recognition output, and an additional fth graph (the top one) showing the improvement that is obtained with a domain speci c acoustic model coupled with a trigram language model. These results for recognition output were obtained with the Phi classi er module rather than BoosTexter the Phi classi er performance is generally the same as, or slightly better than, Boos-Texter when applied to recognition output. The language models used in the experiments for Figure 4 are derived from the example sets for k = 3 5 10 (lower three graphs) and for the full training set (upper two graphs), respectively. As described earlier, the language model for restricted numbers of examples is an unweighted one that recognizes sequences of substrings of the examples. For the full training set, statistical N-gram language models are used (N=3 for the top graph and N=2 for the second to top) since there is sufcient data in the full training set for such language models to be e ective.   There also seem to be diminishing returns as k is increased from 3 to 5 to 10. A likely explanation is that expansion of examples by variants is progressively less e ective a s t h e size of the unexpanded set is increased. This is to be expected since additional real examples presumably are more faithful to the task than arti cially generated variants. Concluding remarks We h a ve described an approach to constructing interactive s p o k en interfaces. The approach is aimed at shifting the burden of handling linguistic variation for new applications from the application developer (or data collection lab) to the underlying spoken language understanding technology itself. Applications are speci ed in terms of a relatively small number of examples, while the mapping between the inputs that users speak, variants of the examples, and application actions, are handled by the system. In this approach, we avoid the use of intermediate semantic representations, making it possible to develop general approaches to linguistic variation and dialog responses in terms of word-string to word-string transformations. Con rmation requests used in the dialog are computed automatically from variants in a way i n tended to minimize misleading the user about the application actions to be executed by the system. The quantitative results we have presented indicate that a surprisingly small number of training examples can provide useful performance in a call routing application. These results suggest that, even at its current early stage of development, the variant transduction approach is a viable option for constructing spoken language applications rapidly without specialized expertise. This may be appropriate, for example, for bootstrapping data collection, as well as for situations (e.g. small businesses) for which d e v elopment of a full-blown system would be too costly. When a full dataset is available, the method can provide similar performance to current t e c hniques while reducing the level of skill necessary to build new applications.",
    "abstract": "We describe an approach ( \\ v ariant transduction\") aimed at reducing the e ort and skill involved in building spoken language interfaces. Applications are created by specifying a relatively small set of example utterance-action pairs grouped into contexts. No intermediate semantic representations are involved in the speci cation, and the con rmation requests used in the dialog are constructed automatically. These properties of variant transduction arise from combining techniques for paraphrase generation, classi cation, and examplematching. We describe how a s p oken dialog system is constructed with this approach and also provide some experimental results on varying the number of examples used to build a particular application.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "5",
    "year": "2001",
    "month": "",
    "title": "Variant Transduction: A Method for Rapid Development of Interactive Spoken Interfaces"
}