{
    "article": "This paper presents the Gavagai Living Lexicon, which is an online distributional semantic model currently available in 20 different languages. We describe the underlying distributional semantic model, and how we have solved some of the challenges in applying such a model to large amounts of streaming data. We also describe the architecture of our implementation, and discuss how we deal with continuous quality assurance of the lexicon. Introduction The availability of large amounts of text data in open sources like online news and social media, coupled with the recent developments of scalable and effective techniques for computational semantics, opens up new possibilities for lexicographic research and resource development. As a complement to traditional lexica and thesauri, it is now possible to automatically build lexical resources by mining large amounts of text data using unsupervised machine learning methods. The perhaps most well-known example of a data-driven thesaurus is the Sketch Engine, 1 which features distributional thesauri for some 60 different languages (Kilgarriff et al., 2014). Another example is the polyglot Python library, 2 which contains word embeddings trained on Wikipedia for 137 different languages (Al-Rfou et al.,  2013). There are also several academic projects that provide data-driven thesauri, like the Wortschatz project, 3 and the JoBimText project. 4 This paper presents a continuously learning distributional thesaurus -the Gavagai Living Lexicon -that updates its semantic model according to the input data stream in an online fashion, and produces lexical entries in several different languages. 5 The lexicon currently includes several different types of entries: string similar terms, topically related terms, sequentially related terms, multiword terms, and semantically similar terms. In the following sections, we describe how we use an online distributional semantic model to identify and compile these various relations. We also describe the architecture of our implementation, and discuss how we deal with continuous quality assurance. 1 www.sketchengine.co.uk 2 pypi.python.org/pypi/polyglot 3 corpora.informatik.uni-leipzig.de 4 maggie.lt.informatik.tu-darmstadt.de/jobimtext 5 As of 2016-03-10, the lexicon includes the following 20 languages: Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hebrew, Hungarian, Italian, Latvian, Lithuanian, Norwegian, Polish, Portuguese, Romanian, Russian, Spanish, Swedish. (Online) Distributional Semantics Distributional Semantic Models (DSMs) represent terms as distributional vectors that record (some function of) cooccurrence counts collected within a context window surrounding each occurrence of the terms. There are many different approaches to building such vectors, ranging from simple accumulation of co-occurrence frequencies to more advanced methods based on matrix factorization ( \u00d6sterlund  et al., 2015) and artificial neural networks (Turian et al.,  2010). The particular framework we use for building DSMs is called Random Indexing (RI) (Kanerva et al., 2000; Kanerva, 2009), which provides an attractive processing model for streaming environments because it can be formulated as an online model that processes data as soon as it becomes available. This is done in RI by accumulating distributional vectors incrementally by vector addition; every time a term a occurs in the text, its distributional vector v(a) is updated according to: v(a) \u2190 v(a i ) + c j=\u2212c,j =0 w(x (i+j) )\u03c0 j r(x (i+j) ) (1) where c is the extension to the left and right of the context window surrounding term a, w(b) is a weight function that quantifies the importance of context term b (the default setting is w(b) = 1 for all terms), r d (b) is a random index vector that acts as a fingerprint of context term b, and \u03c0 j is a permutation that either rotates the random index vectors according to the position j (what we call order-coded windows) or the direction (what we call directional windows) of the context items within the context windows, thus enabling the model to take word order into account (Sahlgren  et al., 2008). Distributional vectors accumulated with Equation (1) encode semantic properties, and can be used to quantify semantic similarities between terms. RI can also be used to accumulate the equivalent of a topic model (Steyvers and  Griffiths, 2006), which can be used to quantify topical relations between terms. Such an RI topic model can be formulated as: v(a) \u2190 v(a) + t\u2208T w(a, t) r(t) (2) where T is the set of documents in the data, w(a, t) is the weight of term a in document t, and r(t) is the random index vector of document t. RI handles data and vocabulary growth by using fixeddimensional vectors of dimensionality d (such that d (V, T ) where V is the size of the vocabulary and T is the cardinality of the document set; d is normally on the order of thousands), which means that a new context does not affect the dimensionality of the distributional vectors. We simply assign a new d-dimensional random index vector to each new context; the dimensionality of the distributional vectors remains constant regardless of the size of the data. Online Frequency Weighting The most important step when building a DSM is dealing with the skewness of the frequency distribution. This amounts to reducing the impact of high-frequent items, since they pollute the distributional representations. For the topic vectors, we can simply use some version of TFIDF, such as: w(a, t) = log TF(a, t) \u00d7 log T DF(a) (3) where TF(a, t) is the frequency of term a in document t, and DF(a) is the document frequency of term a. Note that we take the log of the TF, which is done in order to control for topicality (i.e. we effectively require that terms occur several times in documents in order to count as relevant). For the semantic vectors, frequency weighting is normally done by either using stop lists (based on either part of speech tags or frequency thresholds), or by using association measures such as Pointwise Mutual Information (PMI). 6 The online setting complicates matters when it comes to frequency weighting, since we have to operate with a continuous stream of data. There are streaming versions of PMI (Durme and Lall, 2009), but since we use RI to accumulate distributional statistics in a distributed representation, we do not have access to individual cooccurrence counts that can be transformed with PMI. The way we approach frequency weighting in the online setting is to weight each individual vector addition by an online frequency weight w(b) that operates only on the accumulated frequency f (b) of the current context item b and the total number of unique context items seen thus far V (i.e. the current size of the growing vocabulary): w(b) = e \u2212\u03bb\u2022 f (b) V ( 4 ) where \u03bb is an integer that controls the aggressiveness of the frequency weight. This weighting formula returns a weight that ranges between close to 0 for very frequent terms, and close to 1 for not so very frequent terms. Figure 1 shows the distribution of weights plotted against the frequency of the 20,000 most frequent terms in the ukWaC data. 7 As can be seen in the figure, the weighting formula gives the desired effect of reducing the impact of high-frequent terms, while giving a nearly constant weight to medium-and low-frequent items. Note that the use of the weighting formula does not completely remove all co-occurrences with high-frequent terms; they have a weight close to zero, but not exactly zero, and at the beginning of processing they will still have a useful weight, since the weighting formula has not yet learned the frequency distribution. Dominant Eigenvector Removal Equations ( 1 ) to (4) describe an online DSM that accumulates fixed-dimensional distributional vectors from a continuous stream of data. However, an artifact of only using addition for accumulating the distributional vectors in RI is that all vectors will tend towards the dominant eigenvector of the distributional space (this is the case also for standard DSMs built without RI). One way to reduce the gravitational force of the dominant eigenvector without having to compute a full principal component analysis is to utilize the fact that RI can be seen as the first step in a power iteration, which extracts eigenvectors by iterating: v \u2190 Av ||Av|| (i.e. multiplying the vector v with the matrix A, and normalizing). Thus, by summing all distributional vectors (and normalizing the result), we are effectively approximating the dominant eigenvector of the distributional space. We can then remove this dominant vector from all distributional vectors by using: a = a \u2212 a \u2022 b |b| 2 (5) where a is the original vector and b is the vector we want to remove. This operation is known as the Gram-Schmidt process (Golub and Van Loan, 1996), which has also been referred to as orthogonal negation (Widdows, 2003). 7 wacky.sslmit.unibo.it Collocation Cleaning Since the semantic distributional vectors are built from cooccurrences, they will encode a significant amount of collocations. This unfortunately has a negative effect on the distributional model, since the collocations affect the result of nearest neighbor searches by introducing neighbors that share the trace of the collocate (as an example, the term \"white\" may occur significantly in the collocation \"white house,\" which can introduce nearest neighbors that only share the co-occurrence with \"house,\" like \"brick\" and \"beach\"). Such collocations can be identified by looking for spiky patterns in the distributional vectors. As an example, consider the following two vectors: The vector to the left has a very spiky pattern, with a small number of elements having much higher values than the rest, while the elements in the vector to the right have much more even values. The elements with high values in the vector to the left come from one very frequent collocation, and we can identify that collocation by taking only the elements with high values (i.e. the spiky pattern) and searching (using inverse permutations) for a random index vector that looks like the spiky pattern. The term whose random index vector is most similar to the spiky pattern is the most likely collocate, and since we have used permutations to accumulate the distributional vectors, we also know at what position in the context window the term has occurred. This means that we can now form a multiword unit by concatenating the two terms in the correct order, and we can then include that multiword unit as a term in its own right in the distributional model. Note that this method is also able to detect skip-grams like \"ministry . . . defence.\" We use the following condition for identifying spiky vectors: 0.01 * d > N |vi|> max |v i | 2 (6) i.e. if the number of elements N that have values that are higher than a threshold (defined as the highest value in the vector divided by 2) is less than 1% of the dimensionality. If a distributional vector satisfies this criterion, we make a collocation vector consisting of only the spiky pattern (i.e. all elements satisfying the right-hand part of Equation ( 6 )), and we can now use that collocation vector to remove the impact of the collocation on the distributional vector. We do this by weighted orthogonalization: a = a \u2212 \u03b4 \u2022 a \u2022 b |b| 2 (7) where a is the original distributional vector, b is the collocation vector, and \u03b4 ranges between 0 and 1. Note that with \u03b4 = 1, Equation ( 7 ) equals the Gram-Schmidt process in Equation ( 5 ). We use \u03b4 < 1, since we want to retain some traces of the collocation in the distributional vector (even though \"house\" might primarily co-occur with \"white\" in the collocation \"white house,\" it is possible that there are other occurrences of \"white house\" that are not instances of the collocation in question). By continuously running a collocation extraction job, the online DSM is able to incrementally identify multiword terms of higher order; starting with bigrams, and continuing with higher-order n-grams -in theory all the way up to frequently recurring phrases (idioms) and even whole sentences. Lexicon Compilation The dominant eigenvector removal, the collocation cleaning, and the multiword identification are performed as preprocessing steps before using the distributional vectors for lexicon compilation, which is done by extracting the k nearest neighbors to the n most frequent terms in the vocabulary (what we call the target vocabulary). We use k = 50 and n is on the order of 100,000 to 200,000 depending on the amount of incoming data for the language in question. We also extract the 10 most salient left and right neighbors to each term in the target vocabulary by doing nearest neighbor search over the random index vectors using inversely permuted distributional vectors, as described in Sahlgren et  al. (2008). Multiword terms are identified using the collocation criterion, and for the semantic and topical nearest neighbors, we also compute the relative neighborhood graph (Cuba Gyllensten and Sahlgren, 2015), which uncovers the different usages of terms and can be seen as a form of word-sense discovery. We also include string similar terms in the lexicon, which is computed using standard Levenshtein distance (Levenshtein, 1966). In order to avoid information overload in the GUI, we use the following heuristic for the semantic neighbors: if the neighbors are grouped into more than 5 relative neighborhoods, we only show the top 5 neighborhoods; if not, we show all 50 nearest semantic neighbors. For the left and right neighbors, we only show neighbors with a similarity that exceeds a predefined threshold. System Architecture The Gavagai Living Lexicon is implemented using Enterprise JavaBeans, and is designed to work with large and streaming data in a decentralized and scalable fashion. The distributional vectors are stored in a cluster of Redis instances, 8 and nearest neighbor compilation is performed on a dedicated GPU server. The GPU solution is advantageous to using, e.g., a Hadoop cluster, which is optimized for small calculations that are repeated on large amounts of input data, while the nearest neighbor compilation requires large (and very regular) numerical calculations on fairly small amounts of input data, a scenario that GPUs are ideal for. The GPU server carries out approximately 250 million point-wise comparisons per second. The result of the nearest neighbor calculation is stored in MySQL, which serves as the back-end for the Living Lexicon. The Living Lexicon can be accessed either through an API, 9 or using a web-based GUI, shown in Figure 3 . 10 Data The Living Lexicon is continuously fed with data from a range of different sources, including news media, blogs, and forums. The input data is retrieved from a number of different data providers, e.g., Trendiction 11 , Twingly 12 , and Gnip 13 , as well as by means of internal processes. The data is normalized to a unified internal format, a process that includes language detection, assignment of source definitions, de-duplication, and time-stamping, before the texts are subject to processing with the purpose of updating the lexicon. The data flow contains millions of documents each day; at peak periods, the flow can reach some 20 million documents each day, which amounts to more than a billion tokens each day. The lexicon is currently available in 20 different languages. The amount of data differs considerably between languages: English is by far the largest language, followed by Russian, Swedish, German and French. Examples Table 1 provides a few illustrative examples of the type of entries learned by the lexicon. For each entry in the lexicon, the table lists all types of relations currently represented in the lexicon: string similar terms identified using Levenshtein distance; commonly preceding and succeeding terms computed using inverse permutations; the nearest semantic neighbors, sorted according to their relative neighborhood graph; multiword units identified using the collocation criterion; and topical neighbors, sorted according to their relative neighborhood graph. Note that the preceding and succeeding columns contain different numbers of terms, which is an effect of using a global threshold for the syntagmatic similarities. The entry for \"great\" illustrates the somewhat noisy nature of the syntagmatic relations; the succeeding terms all make sense, but the preceding terms seem less intelligible; the syntagmatic relations for \"great\" would probably have benefited from using a slightly more aggressive threshold. Note also that the topical neighbors do not always make sense. This is due to the lack of topically coherent discussions in online media featuring the terms in question. Out of the four examples in Table 1 , \"apple\" has the clearest topical neighbors, which indicates that this term is used in topically more coherent contexts than the other terms in this example. For terms like \"great\" and \"what the hell\", which occur in topically very diverse contexts in online media, the topical neighbors seem more or less random. The term \"what the hell\" does not have any string similar terms, and it has not been part of any multiword units (or rather, the system has thus far not detected any multiword units featuring \"what the hell\" as a part, but that might happen in the future). For the relative neighborhood graphs, which can be seen as a representation of the different usages of terms (and hence as a sort of word-sense discovery), the lexicon also provides a best guess for labels that describe the various usages. These labels (which are not shown in the table, but are available in the web-based GUI) are produced by looking at common preceding or succeeding terms among the members of each relative neighborhood. Continuous Quality Assurance As should be obvious from the examples provided in the last section, the quality of the lexicon entries can differ considerably. Since one of the points of the Living Lexicon is to investigate how an unsupervised DSM will develop as it continuously reads uncontrolled data from online sources, such qualitative differences are to be expected; the lexicon is nothing more than a representation of the current state of online language use. However, this makes it slightly challenging to perform quality assurance and evaluation of the lexicon. One way we tackle this problem is to include the Living Lexicon in our automated testing routine, which monitors our continuous deployment process, with system changes pushed to production on a daily basis. These automated tests track and identify what the impact of system development is on the quality and consistency of the lexicon. The automated tests include both standard benchmark tests Figure 3 : The Gavagai Living Lexicon GUI. (such as those mentioned in the next section), as well as more specific in-house test sets, where we measure lexicon agreement with external resources. These tests are focused exclusively on the semantically similar terms, since the other types of relations in the lexicon are either more volatile in nature (e.g. the topically related terms), or lack standardized benchmarks (e.g. the multiword terms). Continuous automated quality assurance of a learning semantic model is admittedly a difficult problem, and we do not pretend to have a definite solution in place. 14 We would like to avoid including humans in the loop as far as possible to facilitate continuous delivery, but we acknowledge that human plausibility ratings might be the most relevant evaluation for this type of resource. Batch Evaluation We also regularly perform batch experiments and compare the underlying DSMs to other state-of-the-art algorithms. As an example, we use three different semantic tests: the TOEFL synonym test, 15 the BLESS test (Baroni and Lenci, 14 Evaluating dynamic behavior of semantic models is a challenge in itself (Karlgren et al., 2015). 15 Kindly provided by the late Professor Thomas Landauer. 2011), 16 and the SimLex-999 similarity test (Hill et al.,  2014). As data, we use a dump of Wikipedia, which is tokenized by removing punctuation, and down-casing all terms. The resulting data contains some 1.1 billion tokens. We compare the online model defined in Equations ( 1 ), (4), ( 5 ), ( 6 ) and ( 7 ) with two other well-known types of models: a standard DSM with terms as context in a \u00b12sized window using the raw co-occurrence counts (DSM), optimized frequency thresholds (Frequency), and standard PMI weighting (PMI), and the two models included in the word2vec library, 17 SGNS and CBoW, both with optimized parameters. 18 For the online models, we use 3,000dimensional vectors and a window size of \u00b12. The results are summarized in Table 2 . As can be seen from the results in the different models behave when exposed to increasing amounts of data. Figure 4 shows learning curves for the different models; the left-hand plot shows the results for the TOEFL test using increasing amounts of Wikipedia data, the middle plot shows the results for the BLESS test, and the right-hand plot shows the results for the SimLex-999 test. Note the log scale of the x-axis in the plots, which makes it easier to see the development of the models at the beginning of processing, which is arguably the most critical phase. As can be seen from these learning curves, the PMI model consistently outperforms the other models when there is only limited data available, but when data increases, the results for the PMI model seem to reach a plateau fairly quickly, while the results for the other models continue to improve. For the SimLex-999 test, the results for the PMI model even decrease after having seen approximately half of the Wikipedia data (the top result for the PMI model on SimLex-999 is 0.39). Conclusion This paper has presented an implementation of an online distributional semantic model that learns various types of lexical relations between terms from a continuous large stream of text data. The model is used to produce an unsupervised distributional thesaurus -the Gavagai Living Lexicon -that contains entries for hundreds of thousands of terms in several different languages. The motivation for developing the lexicon is threefold: firstly, the lexicon is a valuable resource for higher-level text analysis systems and natural language processing applications, since it provides a constantly up-to-date semantic base layer; novel terminology and novel language use is automatically included and updated in the lexicon continuously, without the need for editorial intervention. Secondly, the lexicon is valuable from a scientific perspective, since it constitutes a long-term experiment that investigates how a DSM develops when continuously fed with large amounts of uncontrolled data from online sources. Thirdly, the lexicon is a useful tool for lexicographic research, since it represents current language use in online media; by looking at the entry for some term over time, we can get a good understanding of how the use of the term changes and evolves. In this paper, we have described the underlying online DSM, and the various processing steps we use to compile the lexical entries. We have also briefly discussed the system architecture, and the input data, and we have exemplified the varying quality of entries in the Living Lexicon. We have argued that such qualitative variance is to be expected, since the online DSM only reflects the current language use on the internet; extremely diverse contextual behavior limits the statistical regularities that can be utilized by the DSM. Since quality assurance of an unsupervised continuously learning semantic model trained on uncontrolled online data is a challenging problem, we have also provided results from batch experiments using controlled data sets, which demonstrate that the online DSM used by the lexicon performs on a par with current state-of-the-art algorithms. We conclude that the Living Lexicon demonstrates the viability of utilizing unsupervised techniques for compiling lexical resources, and we hypothesize that we will see a much wider use for such systems in the near future.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 4.320199066265573e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, or foundations. Therefore, based on the information provided, it appears there was no disclosed funding for this research.",
    "abstract": "This paper presents the Gavagai Living Lexicon, which is an online distributional semantic model currently available in 20 different languages. We describe the underlying distributional semantic model, and how we have solved some of the challenges in applying such a model to large amounts of streaming data. We also describe the architecture of our implementation, and discuss how we deal with continuous quality assurance of the lexicon.",
    "countries": [
        "Sweden"
    ],
    "languages": [
        "Swedish",
        "Portuguese",
        "German",
        "Latvian",
        "Russian",
        "Danish",
        "Polish",
        "French",
        "Lithuanian",
        "English",
        "Spanish",
        "Hungarian",
        "Finnish",
        "Italian",
        "Romanian",
        "Czech",
        "Dutch",
        "Estonian",
        "Hebrew"
    ],
    "numcitedby": 24,
    "year": 2016,
    "month": "May",
    "title": "The Gavagai Living Lexicon"
}