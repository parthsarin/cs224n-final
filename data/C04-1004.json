{
    "article": "This paper proposes a discriminative HMM (DHMM) with long state dependence (LSD-DHMM) to segment and label sequential data. The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence. As a result, the LSD-DHMM separately models the long state dependence in its state transition model and the observation dependence in its output model. In this paper, a variable-length mutual informationbased modeling approach and an ensemble of kNN probability estimators are proposed to capture the long state dependence and the observation dependence respectively. The evaluation on shallow parsing shows that the LSD-DHMM not only significantly outperforms GHMMs but also much outperforms other DHMMs. This suggests that the LSD-DHMM can effectively capture the long context dependence to segment and label sequential data. Introduction A Hidden Markov Model (HMM) is a model where a sequence of observations is generated in addition to the Markov state sequence. It is a latent variable model in the sense that only the observation sequence is known while the state sequence remains \"hidden\". In recent years, HMMs have enjoyed great success in many tagging applications, most notably part-of-speech (POS) tagging (Church 1988; Weischedel et al 1993; Merialdo 1994 ) and named entity recognition (Bikel et al 1999; Zhou et al 2002) . Moreover, there have been also efforts to extend the use of HMMs to word sense disambiguation (Segond et al 1997) and shallow/full parsing (Brants et al 1997; Skut et al 1998; Zhou et al 2000) . Traditionally, a HMM segments and labels sequential data in a generative way, assigning a joint probability to paired observation and state sequences. More formally, a generative (first-order) HMM (GHMM) is given by a finite set of states including an designated initial state and an designated final state, a set of possible There are several problems with this generative approach. First, many tasks would benefit from a richer representation of observations-in particular a representation that describes observations in terms of many overlapping features, such as capitalization, word endings, part-of-speech in addition to the traditional word identity. Note that these features always depends on each other. Furthermore, to define a joint probability over the observation and state sequences, the generative approach needs to enumerate all the possible observation sequences. However, in some tasks, the set of all the possible observation sequences is not reasonably enumerable. Second, the generative approach fails to effectively model the dependence in the observation sequence. Moreover, it is difficult for the generative approach to model the long state dependence since it is not reasonably practical for ngram modeling(e.g. bigram for the first-order GHMM and trigram for the secnodorder GHMM) to be beyond trigram. Third, the generative approach normally estimates the parameters to maximize the likelihood of the observation sequence. However, in many NLP tasks, the goal is to predict the state sequence given the observation sequence. In other words, the generative approach inappropriately applies a generative joint probability model for a conditional probability problem. In summary, the main reasons behind these problems of the generative approach are the strong context independent assumption and the generative nature in modeling sequential data. While the dependence between successive states can be directly modeled by its state transition model, the generative approach fails to directly capture the observation dependence in the output model. From this viewpoint, a GHMM can be also called an observation independent HMM. To resolve above problems in GHMMs, some researches have been done to move from the generative approach to the discriminative approach. Discriminative HMMs (DHMMs) do not expend modeling effort on the observation sequnce, which are fixed at test time. Instead, DHMMs model the state sequence depending on arbitrary, nonindependent features of the observation sequence, normally without forcing the model to account for the distribution of those dependencies. Punyakanok and Roth (2000) proposed a projection-based DHMM (PDHMM) which represents the probability of a state transition given not only the current observation but also past and future observations and used the SNoW classifier (Roth 1998 , Carlson et al 1999) to estimate it (SNoW-PDHMM thereafter). McCallum et al (2000) proposed the extact same model and used maximum entropy to estimate it (ME-PDHMM thereafter). Lafferty et al (2001) extanded ME-PDHMM using conditional random fields by incorporating the factored state representation of the same model (that is, representing the probability of a state given the observation sequence and the previous state) to alleviate the label bias problem in projection-based DHMMs, which can be biased towards states with few successor states (CRF-DHMM thereafter). Similar work can also be found in Bouttou (1991) . Punyakanok and Roth (2000) also proposed a nonprojection-based DMM which separates the dependence of a state on the previous state and the observation sequence, by rewriting the GHMM in a discriminative way and heuristically extending the notation of an observation to the observation sequence. Zhou et al (2000) systematically derived the exact same model as in Punyakanok and Roth (2000) and used back-off modeling to esimate the probability of a state given the observation sequence (Backoff-DHMM thereafter) while Punyakanok and Roth (2000) used the SNoW classifier to estimate it(SNoW-DHMM thereafter). This paper follows our previous work in Zhou et al (2000) and proposes an alternative nonprojection-based DHMM with long state dependence (LSD-DHMM), which separates the dependence of a state on the previous states and the observation sequence. Moreover, a variablelength mutual information based modeling approach (VLMI) is proposed to capture the long state dependence of a state on the previous states. In addition, an ensemble of kNN probability estimators is proposed to capture the observation dependence of a state on the observation sequence. Experimentation shows that VLMI effectively captures the long state dependence. It also shows that the kNN ensemble captures the dependence between the features of the observation sequence more effectively than classifier-based approaches, by forcing the model to account for the distribution of those dependencies. The layout of this paper is as follows. Section 2 first proposes the LSD-DHMM and then presents the VLMI to capture the long state dependence. Section 3 presents the kNN probability estimator to capture the observation dependence while Section 4 presents the kNN ensemble. Section 5 introduces shallow parsing, while experimental results are given in Section 6. Finally, some conclusion will be drawn in Section 7. LSD-DHMM: Discriminative HMM with Long State Dependence In principle, given an observation sequence , the goal of a conditional probability model is to find a stochastic optimal state sequence s that maximizes n n o o o o L 2 1 1 = ) | ( log 1 1 n n o s p n n s s s L 2 1 1 = ) | ( log max arg 1 1 * 1 n n s o s p S n = (1) By applying the Bayes' rule, we can rewrite the equation (1) as: } ) , ( ) ( {log max arg )} | ( {log max arg 1 1 1 1 1 * 1 1 n n n s n n s o s MI s p o s p s n n + = = (2) Obviously, the second term MI captures the mutual information between the state sequence and the observation sequence o . To compute efficiently, we propose a novel mutual information independence assumption: ) , ( 1 1 n n o s n 1 n s 1 MI ) , ( 1 1 n n o s \u2211 = = n i n i n n o s MI o s MI 1 1 1 1 ) , ( ) , ( or \u2211 = = \u22c5 n i n n n n p o p s p o s p 1 1 1 1 1 log ) ( ) ( ) , ( log \u22c5 n i n i o p s o s p 1 1 ) ( ) ( ) , ( (3) That is, we assume a state is only dependent on the observation sequence o and independent on other states in the state sequence s . This assumption is reasonable because the dependence among the states in the state sequence has been n 1 n 1 n 1 s directly captured by the first term log in equation (2). ) ( 1 n s p | ( log ) ( log } ) | ( ) ( 1 2 1 1 \u2211 = n i n i n i o s p s p o s s p \u2212 i s 1 1 ) By applying the assumption (3) into the equation ( 2 ) and using the chain rule, we have: } ) ) , ( { max arg } ) | ( log ) | ( log { max arg log ) ( log log ) | ( log { max arg 1 2 1 1 1 1 2 1 1 1 1 2 1 1 * 1 1 1 \u2211 \u2211 \u2211 \u2211 \u2211 \u2211 \u2211 = = \u2212 = = \u2212 = = = \u2212 + = + \u2212 = + \u2212 + = n i n i i i s n i n i i n i i i s n i n i i n i i i s s s MI o s p s s p p s p s s p s n n n (4) The above model consists of two models: the state transition model \u2211 which measures the state dependence of a state given the previous states, and the output model which measures the observation dependence of a state given the observation sequence in a discriminative way. Therefore, we call the above model as in equation (4) a discriminative HMM (DHMM) with long state dependence (LSD-DHMM). The LSD-DHMM separates the dependence of a state on the previous states and the observation sequence. The main difference between a GHMM and a LSD-DHMM lies in their output models in that the output model of a LSD-DHMM directly captures the context dependence between successive observations in determining the \"hidden\" states while the output model of the GHMM fails to do so. That is, the output model of a LSD-DHMM overcomes the strong context independent assumption in the GHMM and becomes observation context dependent. Therefore, the LSD-DHMM can also be called an observation context dependent HMM. Compared with other DHMMs, the LSD-DHMM explicitly models the long state dependence and the non-projection nature of the LSD-DHMM alleviates the label bias problem inherent in projection-based DHMMs. For each i = n i i s MI 2 , ( \u2211 = n i n i o s p 1 1 ) | ( log \u2211 = \u2212 n i i i s s MI 2 1 1 ) , ( ) 2 ( n i \u2264 \u2264 , we first find a minimal ) i 0 ( k k p \u2264 where the frequency of s is bigger than a threshold (e.g. 10) and then estimate using 1 \u2212 i k ) 1 \u2212 , ( 1 i i s s MI ) ) ( ( ) , ( 1 \u2212 \u22c5 = i i k i k i p s p s p s s ) ) | 1 n o ) | i i E s N i o + ( ) 1 \u2212 i k s MI n i o s p 1 | ( log ( i s \u2248 ) | ( 1 n i o s ( p i N i o o \u2212 . In this way, the long state dependence can be captured maximally in a dynamical way. Here, the frequencies of variable-length state sequences are estimated using the simple Good-Turing approach (Gale et al 1995) . \u2211 = n i 1 p i E = L L ) | ( i E \u2022 The second is to estimate the output model: . Ideally, we would have sufficient training data for every event whose conditional probability we wish to calculate. Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data. Traditionally, there are two existing approaches to resolve this problem: linear interpolation (Jelinek 1989 ) and back-off (Katz 1987 ). However, these two approaches only work well when the number of different information sources is limited. When a long context is considered, the number of different information sources is exponential and not reasonably enumerable. The current tendency is to recast it as a classification problem and use the output of a classifier, e.g. the maximum entropy classifier (Ratnaparkhi 1999) to estimate the state probability distribution given the observation sequence. In the next two sections, we will propose a more effective ensemble of kNN probability estimators to resolve this problem. kNN Probability Estimator The main challenge for the LSD-DHMM is how to reliably estimate p in its output model. For efficiency, we can always assume , where the pattern entry . That is, we only consider the observation dependence in a window of 2N+1 observations (e.g. we only consider the current observation, the previous observation and the next observation when N=1). For convenience, we denote P as the conditional state probability distribution of the states given E and i ) | ( i i E s p i s i E ) | ( i E P \u2022 ) (E kNN i ) | ( i E P \u2022 FrequentEn FrequentEn | ( \u02c6kNN E p k i as the conditional state probability of given . = = \u2022 i E P ) | ( The kNN probability estimator estimates by first finding the K nearest neighbors of frequently occurring pattern entries and then aggregating them to make a proper estimation of . Here, the conditional state probability distribution is estimated instead of the classification in a traditional kNN classifier. To do so, all the frequently occurring pattern entries are extracted from the training corpus in an exhaustive way and stored in a dictionary . In order to limit the dictionary size and keep efficiency, we constrain a valid set of pattern entry forms ValidEntry to consider only the most informative information sources. Generally, ValidEntry can be determined manually or automatically according to the applications. In Section 5, we will give an example. } ,..., 2 , 1 | { K k E k i = ary tryDiction Form Form Given a pattern entry E and a dictionary of frequently occurring pattern entries , a simple algorithm is applied to find the K nearest neighbors of the pattern entry from the dictionary as follows: i ary tryDiction i E \u2022 compare with each entry in the dictionary and find all the compatible entries i E \u2022 compute the cosine similarity between E and each of the compatible entries i \u2022 sort out the K nearest neighbors according to their cosine similarities Finally, the conditional state probability distribution of the pattern entry is aggregated over those of its K nearest neighbors weighted by their frequencies and cosine similarities : ) ( k i E f ) \u2211 \u2211 = = \u22c5 \u2022 \u22c5 \u22c5 K k k i k i K k k i k i k i E f kNN E p E P E f kNN E p 1 1 ) ( ) | ( \u02c6) | ( ) ( ) | ( \u02c6 (5) p kNN Ensemble In the literature, an ensemble has been widely used in the classification problem to combine several classifiers (Breiman 1996; Hamamoto 1997; Dietterich 1998; Zhou Z.H. et al 2002; Kim et al 2003) . It is well known that an ensemble often outperforms the individual classifiers that make it up (Hansen et al 1990) . In this paper, an ensemble of kNN probability estimators is proposed to estimate the conditional state probability distribution P instead of the classification. This is done through a bagging technique (Breiman 1996) to aggregate several kNN probability estimators. In bagging, the M kNN probability estimators in the ensemble ) | ( i E \u2022 } M ,..., 2 , 1 | { m kNN ENS m = = Shallow Parsing In order to evaluate the LSD-DHMM and the proposed variable-length mutual information modeling approach for the long state dependence in the state transition model and the kNN ensemble for the observation dependence in the output model, we have applied it in the application of shallow parsing. For shallow parsing, we have o , where is the word sequence and is the part-of-speech (POS) sequence, while the \"hidden\" states are represented as structural tags to bracket and differentiate various categories of phrases. The basic idea of using the structural tags to represent the \"hidden\" states is similar to Skut et al (1998) and Zhou et al (2000) . Here, a structural tag consists of three parts: i i w p = 1 n n w w w w L 2 1 1 = n n p p p L \u2022 Boundary Category (BOUNDARY): it is a set of four values: \"O\"/\"B\"/\"M\"/\"E\", where \"O\" means that current word is a whOle phrase and \"B\"/\"M\"/\"E\" means that current word is at the Beginning/in the Middle/at the End of a phrase. \u2022 Phrase Category (PHRASE): it is used to denote the category of the phrase. \u2022 Part-of-Speech (POS): Because of the limited number of boundary and phrase categories, the POS is added into the structural tag to represent more accurate state transition and output models. For example, given the following POS tagged sentence as the observation sequence: He/PRP reckons/VBZ the/DT current/JJ account/NN deficit/NN will/MD narrow/VB to/TO only/RB $/$ 1.8/CD billion/CD in/IN September/NNP ./. We can have a corresponding sequence of structural tags as the \"hidden\" Experimentation The corpus used in shallow parsing is extracted from the PENN TreeBank (Marcus et al. 1993 ) of 1 million words (25 sections) by a program provided by Sabine Buchholz from Tilburg University. All the evaluations are 5-fold crossvalidated. For shallow parsing, we use the Fmeasure to measure the performance. Here, the Fmeasure is the weighted harmonic mean of the precision (P) and the recall (R): 1979) , where the precision (P) is the percentage of predicted phrase chunks that are actually correct and the recall (R) is the percentage of correct phrase chunks that are actually found. Table 1 shows the effect of different number of nearest neighbors in the kNN probability estimator and considered previous states in the variablelength mutual information modeling approach of the LSD-DHMM, using only one kNN probability estimator in the ensemble to estimate in the output model. It shows that finding 3 nearest neighbors in the kNN probability estimator performs best. It also shows that further increasing the number of nearest neighbors does not increase or even decrease the performance. This may be due to introduction of noisy neighbors when the number of nearest neighbors increases. Moreover, Table 1 shows that the LSD-DHMM performs best when six previous states is considered in the variable-length mutual information-based modeling approach and further considering more previous states only slightly increase the performance. This suggests that the state dependence exists well beyond traditional ngram modeling (e.g. bigram and trigram) to six previous states and the variable-length mutual informationbased modeling approach can capture the long state dependence. In the following experimentation, we will only use the LSD-DHMM with 3 nearest neighbors used in the kNN probability estimator and 6 previous states considered in the variablelength mutual information modeling approach. P R RP + + = 2 2 ) 1 ( \u03b2 \u03b2 F with =1 (Rijsbergen ) | ( 1 n i o s p Table 2 shows the effect of different number of kNN probability estimators in the ensemble. It shows that 15 bootstrap replicates are enough for the k-NN ensemble on shallow parsing and increase the F-measure by 0.71 compared with the ensemble of only one kNN probability estimator. Table 3 compares the LSD-DHMM with GHMMs and other DHMMs. It shows that all the DHMMs significantly outperform GHMMs due to the modeling of the observation dependence and allowing for non-independent, difficult to enumerate observation features. It also shows that our LSD-DHMM much outperforms other DHMMs due to the modeling of the long state dependence using the variable-length mutual information-based modeling approach in the LSD-DHMM. Moverover, Table 3 shows that noprojection-based DHMMs (i.e. CRF-DHMM, SNoW-DHMM, Backoff-DHMM and LSD-DHMM) outperform projection-based DHMMs. It may be due to alleviation of the label bias problem inherent in the projection-based DHMMs. Finally, Table 2 also compares the kNN ensemble with popular classifier-based approaches, such as SNoW and Maximum Entropy, in estimating the output model of the LSD-DHMM. It shows that the kNN ensemble outperforms these classifierbased approaches. This suggests that the kNN ensemble captures the dependence between the features of the observation sequence more effectively by forcing the model to account for the distribution of those dependencies. Conclusion Hidden Markov Models (HMMs) are a powerful probabilistic tool for modeling sequential data and have been applied with success to many textrelated tasks, such as shallow paring. In these cases, the observations are usually modified as multinomial distributions over a discrete dictionary and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a discriminative HMM with long state dependence that allows observations to be represented as arbitrary overlapping features and defines the conditional probability of the state sequence given the observation sequence. It does so by assuming a novel mutual information independence to separate the dependence of a state given the observation sequence and the previous states. Finally, the long state dependence and the observation dependence can be effectively captured by a variable-length mutual information model and a kNN ensemble respectively. In future work, we will explore our model in other applications, such as full parsing.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 1.9361263126072004e-07,
        "none": 0.9999998063873687
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the research presented. Therefore, based on the information provided, it is not possible to determine any funding sources.",
    "abstract": "This paper proposes a discriminative HMM (DHMM) with long state dependence (LSD-DHMM) to segment and label sequential data. The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence. As a result, the LSD-DHMM separately models the long state dependence in its state transition model and the observation dependence in its output model. In this paper, a variable-length mutual informationbased modeling approach and an ensemble of kNN probability estimators are proposed to capture the long state dependence and the observation dependence respectively. The evaluation on shallow parsing shows that the LSD-DHMM not only significantly outperforms GHMMs but also much outperforms other DHMMs. This suggests that the LSD-DHMM can effectively capture the long context dependence to segment and label sequential data.",
    "countries": [
        "Singapore"
    ],
    "languages": [
        ""
    ],
    "numcitedby": 4,
    "year": 2004,
    "month": "aug 23{--}aug 27",
    "title": "Discriminative Hidden {M}arkov Modeling with Long State Dependence using a k{NN} Ensemble",
    "values": {
        "building on past work": "This paper proposes a discriminative HMM (DHMM) with long state dependence (LSD-DHMM) to segment and label sequential data. The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence. As a result, the LSD-DHMM separately models the long state dependence in its state transition model and the observation dependence in its output model. In this paper, a variable-length mutual informationbased modeling approach and an ensemble of kNN probability estimators are proposed to capture the long state dependence and the observation dependence respectively.",
        "novelty": " This paper proposes a discriminative HMM (DHMM) with long state dependence (LSD-DHMM) to segment and label sequential data.  The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence.  As a result, the LSD-DHMM separately models the long state dependence in its state transition model and the observation dependence in its output model.",
        "performance": "Table 2 shows the effect of different number of kNN probability estimators in the ensemble. Table 3 compares the LSD-DHMM with GHMMs and other DHMMs. Table 2 also compares the kNN ensemble with popular classifier-based approaches, such as SNoW and Maximum Entropy, in estimating the output model of the LSD-DHMM."
    }
}