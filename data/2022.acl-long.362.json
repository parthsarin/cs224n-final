{
    "article": "There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading. While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics. In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models (BERT and GPT-2). In all experiments, we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories (syntactic complexity, lexical richness, register-based multiword combinations, readability and psycholinguistic word properties). Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading. We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME. Introduction Extensive studies using eye-trackers to observe gaze patterns have shown that humans read sentences efficiently by performing a series of fixations and saccades (for comprehensive overviews, see, e.g. Rayner et al. (2012) , Seidenberg (2017) , and Brysbaert (2019) ). During a fixation, the eyes stay fixed on a word and remain fairly static for 200-250 milliseconds. Saccades are rapid jumps between fixations that typically last 20-40 ms and span 7-9 characters. In addition, when reading, humans do not fixate one word at a time, i.e. some saccades run in the opposite direction, and some words or word combinations are fixed more than once or skipped altogether. Much of the early work in this area was concerned with the careful construction of sentences to model human reading behavior and understand predictive language processing (Staub, 2015; Kuperberg and Jaeger, 2016) . The use of isolated, decontextualized sentences in human language processing research has been questioned on ecological validity grounds. With the growing awareness of the importance of capturing naturalistic reading, new corpora of eye movement data over contiguous text segments have emerged. Such corpora serve as a valuable source of data for establishing the basic benchmarks of eye movements in reading and provide an essential testing ground for models of eye movements in reading, such as the E-Z Reader model (Reichle et al., 1998) and the SWIFT model (Engbert et al., 2005) . They are also used to evaluate theories of human language processing in psycholinguistics: For example, the predictions of two theories of syntactic processing complexity (dependency locality theory and surprisal) were tested in the Dundee Corpus, which contains the eye-tracking record of 10 participants reading 51,000 words of newspaper text (Demberg and Keller, 2008) . Subsequent work has presented accounts where the ability of a language model to predict reading times is a linear function of its perplexity (Goodkind and Bicknell, 2018) . More recent work has employed transformer-based language models to directly predict human reading patterns across new datasets of eye-tracking and electroencephalogra-phy during natural reading (Schrimpf et al., 2021; Hollenstein et al., 2021 , for more details see the related work section below). While this work has made significant progress, there is limited work aimed at determining the role of general text properties in predicting eye movement patterns in corpora of naturalistic reading. To date, research has addressed this issue only peripherally (Lowder et al., 2018; Snell and Theeuwes, 2020; Hollenstein et al., 2021) , examining the role of text features only on the basis of a small number of linguistic features. In this paper, we conduct a systematic investigation of the effects of text properties on eye movement prediction: We determine the extent to which these properties affect the prediction accuracy of two transformer-based language models, BERT and GPT-2. The relationship between these properties and model performance is investigated in two ways: (a) building on the approaches in Lowder et al. (2018) and Hollenstein et al. (2021) , by investigating the sensitivity of model predictions to a wide range of text features, and (b) by incorporating text features into the transformerbased language models. With respect to the latter, we examine the effects of the preceding sentence on gaze measurement within the sentence of interest. This was motivated by psycholinguistic literature that has demonstrated \"spillover\" effects, where the fixation duration on a word is affected by linguistic features of the preceding context (Pollatsek et al., 2008; Shvartsman et al., 2014 , see also Barrett and Hollenstein (2020) for a reference to the utility of information about preceding input). Computational reading models have not addressed linguistic concepts beyond the level of the fixated word much, with a few exceptions, e.g. spillover effects related to previewing the next word n+1 during the current fixation on word n (Engbert et al., 2005) . Here we extend the study of spillover effects to the effects of textual features of the preceding sentence. To our knowledge, this is the first systematic attempt to investigate the effects of textual features on the prediction of eye-tracking measures in a corpus of naturalistic reading by considering a large number of features spanning different levels of linguistic analysis. Related work In this section, we provide a brief overview of the available literature that has used transformerbased language models to predict human reading patterns, as well as the literature that has investigated the role of text properties on word predictability during naturalistic reading. Schrimpf et al. (2021) evaluated a broad range of language models on the match of their internal representations to three datasets of human neural activity (fMRI and ECoG) during reading. Their results indicated that transformer-based models perform better than recurrent networks or wordlevel embedding models. They also found that the models with the best match with human language processing were models with unidirectional attention transformer architectures: specifically the generative pretrained transformer (GPT-2) (Radford et al., 2019) , consistently outperformed all other models in both fMRI and ECoG data from sentence-processing tasks. Hollenstein et al. (2021) presented the first study analyzing to what extent transformer language models are able to directly predict human gaze patterns during naturalistic reading. They compare the performance of language-specific and multilingual pretrained and fine-tuned BERT and XLM models to predict reading time measures of eye-tracking datasets in four languages (English, Dutch, German, and Russian). Their results show that both monolingual and multilingual transformer-based models achieve surprisingly high accuracy in predicting a range of eye-tracking features across all four languages. For the English GECO dataset, which is also used in the current study, the BERT and XLM models yielded prediction accuracies (100 -mean absolute error (MAE)) ranging between 91.15% (BERT-EN) and 93.89% (XLM-ENDE). To our knowledge, the first study to investigate the role of textual characteristics on word predictability during naturalistic reading is an experimental study conducted by Lowder et al. (2018) . This study implemented a large-scale cumulative cloze task to collect word-by-word predictability data (surprisal and entropy reduction scores) for 40 text passages which were subsequently read by 32 participants while their eye movements were recorded. Lowder et al. (2018) found that surprisal scores were associated with increased reading times in all eye-tracking measures. They also observed a significant effect of text difficulty, measured by Flesch-Kincaid grade level of each paragraph (Kincaid et al., 1975) , such that increases in text difficulty were associated with increased reading times. Crucially, their study yielded evidence of interactions between predictability (surprisal scores) and paragraph difficulty. In the abovementioned computational study, Hollenstein et al. (2021) also investigated the influence of textual characteristics (word length, text readability) on model performance. Text readability was measured using Flesch Reading Ease scores (Flesch, 1948) . Their results indicated that the models learned to reflect characteristics of human reading, such as sensitivity to word length. They also found that model accuracy was higher in more easily readable sentences. Experiments Datasets We analyze eye movement data from two eyetracking corpora of natural reading, the Ghent Eye-Tracking Corpus (GECO; (Cop et al., 2017) ) and the Provo corpus (Luke and Christianson, 2018) . In both corpora the participants read full sentences within longer spans of naturally occurring text at their own speed while their eye movements were recorded. The GECO corpus is large dataset of eye movement of a monolingual and bilingual readers who read a complete novel, Agatha Christie's 'The Mysterious Affair at Styles'. It contains eye-tracking data from 14 English native speakers and 19 bilingual speakers of Dutch and English, who read parts of the novel in its original English version and another part of its Dutch translation. In the present work, we focus on the analysis of the data from the monolingual English native speakers. These participants read a total of 5031 sentences amounting to a total of 54364 word tokens. The Provo Corpus is a dataset of eye movements of skilled readers reading connected text. It consists of eye movement data from 84 native English-speaking participants from Brigham Young University, who read 55 short passages from a variety of sources, including online news articles, popular science magazines, and public-domain works of fiction. These passages were an average of 50 words long for a total of 2,689 word tokens. Measurement of text properties The texts from both datasets (GECO and PROVO) were automatically analyzed using CoCoGen (Str\u00f6bel et al., 2016) , a computational tool that implements a sliding window technique to calculate sentence-level measurements that capture the within-text distributions of scores for a given language feature (for current applications of the tool in the context of text classification, see Kerz et al. (2020 Kerz et al. ( , 2021)) ). We extract a total of 107 features that fall into five categories: (1) measures of syntactic complexity (N=16), (2) measures of lexical richness (N=14), (3) register-based n-gram frequency measures (N=25), (4) readability measures (N=14), and (5) psycholinguistic measures (N=38). A concise overview of the features used in this study is provided in Table 5 in the appendix. Tokenization, sentence splitting, partof-speech tagging, lemmatization and syntactic PCFG parsing were performed using Stanford CoreNLP (Manning et al., 2014) . The syntactic complexity measures comprise (i) surface measures that concern the length of production units, such as the mean length of words, clauses and sentences, (ii) measures of the type and incidence of embeddings, such as dependent clauses per T-Unit or verb phrases per sentence or (iii) the frequency of particular types of particular structures, such as the number of complex nominal per clause. These features are implemented based on descriptions in Lu (2010) and using the Tregex tree pattern matching tool (Levy and Andrew, 2006) with syntactic parse trees for extracting specific patterns. Lexical richness measures fall into three distinct sub-types: (i) lexical density, such as the ratio of the number of lexical (as opposed to grammatical) words to the total number of words in a text, (iii) lexical variation, i.e. the range of vocabulary as displayed in language use, captured by textsize corrected type-token ratio and (iii) lexical sophistication, i.e. the proportion of relatively unusual or advanced words in the learner's text, such as the number of New General Service List (Browne et al., 2013) . The operationalizations of these measures follow those described in Lu (2012) and Str\u00f6bel (2014) . The register-based n-gram frequency measures are derived from the five register sub-components of the Contemporary Corpus of American English (COCA, (Davies, 2008) ): spoken, magazine, fiction, news and academic language 1 . These measures consider both the register-specific frequency rank and count: N orm n,s,r = |Cn,s,r|\u2022log c\u2208|Cn,s,r | f reqn,r(c) |Un,s| (1) Let A n,s be the list of n-grams (n \u2208 [1, 5]) appearing within a sentence s, B n,r the list of n-gram appearing in the n-gram frequency list of register r (r \u2208 {acad, f ic, mag, news, spok}) and C n,s,r = A n,s \u2229 B n,r the list of n-grams appearing both in s and the n-gram frequency list of register r. U n,s is defined as the list of unique n-gram in s, and f req n,r (a) the frequency of ngram a according to the n-gram frequency list of register r. The total of 25 measures results from the combination of (a) a 'reference list' containing the top 100k most frequent n-grams and their frequencies from one of five registers of the COCA corpus and (b) the size of the n-gram (n \u2208 [1, 5]). The readability measures combine a word familiarity variable defined by prespecified vocabulary resource to estimate semantic difficulty together with a syntactic variable, such as average sentence length. Examples of these measures are the Fry index (Fry, 1968) or the SMOG (McLaughlin, 1969) . Finally, the psycholinguistic measures capture cognitive aspects of reading not directly addressed by the surface vocabulary and syntax features of traditional formulas. These measures include a word's average age-of-acquisition (Kuperman et al., 2012) or prevalence, which refers to the number of people knowing the word (Brysbaert et al., 2019; Johns et al., 2020) . Eye-tracking measures We analyze data from eight word-level reading time measures, which were also investigated in Hollenstein et al. (2021) . The measures include general word-level characteristics such as (1) the number of fixations (NFX), i.e. the number of times a subject fixates on a given word w, averaged over all participants, (2) mean fixation duration (MFD), the average fixation duration of all fixations made on w, averaged over all participants and (3) fixation proportion (FXP), the number of subjects that fixated w, divided by the total number of participants. 'Early processing' measures pertain to the early lexical and syntactic processing and are based on the first time a word is fixated. These features include: (4) first fixation duration (FFD), i.e. the duration of the first fixation on w (in milliseconds), averaged over all subjects and (5) first pass duration (FPD), i.e. the sum of all fixations on w from the first time a subject fixates w to the first time the subject fixates another token. 'Late processing' measures capture the late syntactic processing and are based on words which were fixated more than once. These measures comprise (6) total fixation duration (TFD), i.e. the sum of the duration of all fixations made on w, averaged over all subjects, (7) number of re-fixations (NRFX), the number of times w is fixated after the first fixation, i.e., the maximum between 0 and the NFIX-1, averaged over all subjects and (8) re-read proportion (RRDP), the number of subjects that fixated w more than once, divided by the total number of subjects. The means, standard deviations and observed ranges for all eye-tracking features are shown in Tables 1 and 2 . Like in Hollenstein et al. (2021) , before being entered into the models, all eye-tracking features were scaled between 0 and 100 so that the loss can be calculated uniformly over all features. Modeling approach Deep neural transformer-based language models create contextualized word representations that are sensitive to the context in which the words appear. These models have yielded significant improvements on a diverse array of NLP tasks, ranging from question answering to coreference resolution. We compare two such models in terms of their ability to predict eye-tracking features: 'Bidirectional Encoder Representations from Transformers' (BERT) (Devlin et al., 2018) and 'Generative Pre-trained Transformer 2' (GPT-2) (Radford et al., 2019) . BERT is an autoencoder model trained with a dual objective function of predicting masked words and the next sentence. It consists of stacked transformer encoder blocks and uses self-attention, where each token in an input sentence looks at the bidirectional context, i.e. tokens on left and right of the considered token. In contrast, GPT-2 is an autoregressive model consisting of stacked transformer decoder blocks trained with a language modelling objective, where the given sequence of tokens is used to predict the next token. While GPT-2 uses selfattention as well, it employs masking to prevent words from attending to following tokens, hereby processing language fully unidirectionally. BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words), whereas GPT-2 is trained on WebText, an 8-million documents subset of CommonCrawl amounting to 40 GB of text. We chose the BERT base model (cased) because it is most comparable to GPT-2 with respect to number of layers and dimensionality (BERT base model (cased) has 110M trainable parameters, GPT-2 has 117M). We evaluate the eye-tracking predictions of the models both on within-domain text, using an 80/10/10 split of the much larger GECO dataset (representing fiction language), as well as on outof-domain text using the complete, much smaller PROVO dataset (comprising also online news and popular science magazine language). Furthermore, since overly aggressive fine-tuning may cause catastrophic forgetting (Howard and Ruder, 2018) , we perform all experiments both with 'frozen' language models, where all the layers of the language model are frozen and only the attached neural network layers are trained, and also 'fully fine-tuned' language models, where the error is back-propagated through the entire architecture and the pretrained weights of the model are updated based on the GECO training set. For all models we explored in this paper, we apply a dropout rate of 0.1 and a l2 regularization of 1\u00d710 \u22124 . We use AdamW as the optimizer and mean squared error as the loss function. We use a fixed learning rate with warmup. During warmup, the learning rates are linearly increased to the peak learning rates and then fixed. For BERT with a 'frozen' language model, the peak learning rate is 5 \u00d7 10 \u22124 with 5 warmup steps and for GPT-2 with a 'frozen' language model, it is 0.001 also with 5 warmup steps. Models with 'fully fine-tuned' language models are trained with two phases. In the first phase, the weights of the lan-guage models are frozen and only regression layers are trained. During this phase, peak learning rates of 3 \u00d7 10 \u22124 for BERT and 0.001 for GPT-2 are used. For both models, the first phase is performed over 12 epochs with 5 warmup steps. In the second phase, we unfreeze the weights of language models and fine-tune the language models together with the regression layers. During this phase, the BERT-based model is trained with a peak learning rate of 5 \u00d7 10 \u22125 while GPT-2-based model is trained with a peak learning rate of 5 \u00d7 10 \u22124 . The number of warmup steps for training both models in this phase is 3. We adopted a two-phase training procedure since preliminary experiments showed that this procedure yields same results as training the entire models from the first epoch, yet it can speed up model convergence. All hyper-parameters are optimized through grid search. Influence of text characteristics on model performance To investigate the impact of the text properties listed in Section 3.2 on prediction accuracy, we partitioned the GECO testset into deciles according to each textual property, i.e. each of the 107 features. We then calculated the Pearson correlation coefficients between the decile of a given textual feature and the mean absolute error (MAE) of a given model. We expected to observe higher prediction accuracy (lower MAE) for sentences with higher readability, lower syntactic complexity, lower lexical richness, higher n-gram frequency and less demanding psycholinguistic properties, i.e. lower age-of-acquisition scores and higher prevalence scores. Integration of text characteristics using a hybrid modeling approach To determine whether eye movement patterns were affected by textual characteristics of the previous sentences (sentence spillover effects), a bidirectional LSTM (BLSTM) model was integrated into the predictive models (Figure 1 ). This BLSTM model reads 107 dimensional vectors of textual features CM i\u2212N , \u2022 \u2022 \u2022 , CM i\u22121 from N previous sentences 2 as its input, transforms them through 4 BLSTM layers of 512 hidden units each, and outputs a 1024 dimensional vector [ \u2212 \u2192 h 4N | \u2190 \u2212 h 41 ], that is a concatenation of the last hidden states of the 4th BLSTM layer in the forward and backward directions \u2212 \u2192 h 4N , \u2190 \u2212 h 41 . A fully connected (FC) layer is added on top of the BLSTM layers to reduce the dimension of BLSTM model output to 256 (C i ). Meanwhile, another FC layer is added to the pre-trained language model (BERT or GPT-2) in order to reduce its logits to the same dimension (E i1 , \u2022 \u2022 \u2022 , E iM ). The reduced BLSTM output is then added to each of the reduced language model logits. Finally, the 256-dimensional joint vectors are fed to a final regression layer to predict human reading behavior. The procedures used to train the 'hybrid' models with textual characteristics of the previous sentences was identical to those specified above. Grid search yielded the same optimized values for all hyperparameters, except for the peak learning rate of 'fully fine-tuned' model with GPT-2 in second training phase, which was 1 \u00d7 10 \u22124 . To assess the relative importance of the feature groups, we employed Submodular Pick Lime (SP-LIME; Ribeiro et al. ( 2016 )), a method to construct a global explanation of a model by aggregating the weights of the linear models. We first construct local explanations using LIME with a linear local explanatory model, exponential kernel function with Hamming distance and a kernel width of \u03c3 = 0.75 \u221a d, where d is the number of feature groups. The global importance score of the SP-LIME for a given feature group j can then be derived by: I j = n i=1 |W ij | , where W ij is the jth coefficient of the fitted linear regression model to explain a data sample x i . Figure 1 : Visualization of approach used to integrate information on complexity of preceding language input for sentence i. Results & Discussion We use sentence-level accuracy (100-MAE) and coefficients of determination (R2) as metrics to evaluate the performance of all models. Table 3 shows the evaluation results for all models averaged over all eye-tracking features. Table 3 shows that both BERT and GPT-2 models predicted the eye-tracking features of both datasets with more than 92% accuracy. The fine-tuned models performed consistently better than the pretrained-only ('frozen') models both on the within-domain text (GECO) and on the out-ofdomain text (PROVO). This result indicates that the learned representations are general enough to be successfully applied both in the prediction of reading patterns of fiction texts as well as in the prediction of news and popular science texts. The BERT models consistently outperformed the GPT-2 models with a difference in R2 of as much as 10.54% on the within-domain data (GECO). This result stands in sharp contrast with those reported in Schrimpf et al. (2021) summarised in Section 2. In their interpretation of the success of GPT-2 in predicting neural activity during reading, Schrimpf et al. (2021) state that \"GPT-2 is also arguably the most cognitively plausible of the transformer models (because it uses unidirectional, forward attention)\". Especially in view of the remarkable margin by which the BERT models outperformed the GPT-2 models here, it appears that arguments that infer cognitive plausibility from prediction success should be viewed with caution (see also Merkx and Frank (2020) for Note: 'fr' = freeze all layers of language model; 'ft' = the entire model is fine-tuned; '+ com S-1' = including textual features of previous sentence further intricacies of the issue). The most accurately predicted individual eye-tracking measures were fixation probability (FXP), mean fixation duration (MFD) and first fixation duration (FFD), indicating that prediction accuracy was generally better for early measures than for late measures. A detailed overview of the results for each eyetracking measure across all models and datasets is provided in Table 7 in the appendix. This finding suggests that the accurate prediction of late measures -that are assumed to reflect higher order processes such as syntactic and semantic integration, revision, and ambiguity resolution -may benefit from the inclusion of contextual information beyond the current sentence. Relationship of prediction accuracy and text characteristics The correlation analyses of the textual features and the mean absolute error revealed that prediction accuracy was affected by the text characteristics of the sentence under consideration. Such effects were found across all eye-tracking met-rics for both BERT and GPT-2 models in both their frozen and fully fine-tuned variants. For reasons of space, we focus our discussion on the predictions of the BERT frozen model of first pass durations on the GECO dataset (additional results for both frozen and fine-tuned BERT models for both first pass duration and total fixation duration are provided in Figure 3 in the appendix). Figure 2 visualizes the impact of all textual features that reached correlation coefficients r > |0.2| along with the feature group they belong to. As is evident in Figure 2 the prediction accuracy of the BERT frozen model was impacted by features from all five feature groups with individual features affecting prediction accuracy in opposite ways. A strong impact (r > |0.5|) was observed for several features of the n-gram feature group: Fixation durations of sentences with higher scores on ngram-frequency features from the news, magazine and spoken registers were predicted more accurately than those with lower scores on these measures. The SMOG readability index, which estimates the years of education a person needs to understand a piece of writing, also has a strong impact: Predicted first pass durations were less accurate in sentences with higher SMOG scores. Several features from the lexical richness, syntactic complexity and readability groups had a moderate impact on prediction accuracy (|0.3| < r < |0.5|): For example, predictions of fixation durations were less accurate on sentences of with a more clausal embedding (ClausesPerSentence) and greater lexical sophistication (MeanLengthWord, Sophistication.ANC and Sophistication.BNC). A similar effect was also observed for the psycholinguistic age-of-acquisition features (AoA mean, AoA max), where predictions of fixations times were less accurate for later acquired words. Note that the finding that the correlation coefficients of the readability features have opposite signs is due to the fact that these are either defined to quantify ease of reading (e.g. Flesch Kincaid Reading Ease) or reading difficulty (e.g. SMOG index). Prediction accuracy of hybrid models Turning to the results of the hybrid models with integrated information on textual characteristics of the preceding sentence, we found that highest accuracy (R2 = 58.36%) was achieved by the finetuned BERT model. This amounts to an increase in performance over a model trained without that information of 1.53%. This result demonstrates that future studies should take textual spillover effects into account. Our best-fitting model outperformed not only the best-performing BERT model in Hollenstein et al. (2021) , BERT-BASE-MULTILINGUAL-CASED (Wolf et al., 2019) but also the overall best-performing transformerbased model, XLM-MLM-ENDE-1024 (Lample and Conneau, 2019) tested in that study. This result demonstrates that the claim put forth in Hollenstein et al. (2021) that multilingual models show an advantage over language specific ones and that multilingual models might provide cognitively more plausible representations in predicting reading needs to be viewed with caution. The results of the feature ablation experiments revealed that the main sources of the greater prediction accuracy of the hybrid models was asso- 4 . We focus here on the results on the out-of-domain testset (PROVO) for which improvements over models without the integrated textual information were more pronounced. As is evident in Table 4, the central role of the three feature groups listed above result was observed across models (BERT vs. GPT-2) and across training procedures (frozen vs. fine-tuning). However, Table 4 also demonstrates clear differences between the models: While the BERT models show greater sensitivity to syntactic complexity, the GPT-2 models mostly benefit from information concerning ngram frequency. A possible interpretation of this finding is that a unidirectional model like GPT-2 relies more strongly on word sequencing than a bidirectional one. Future research is needed to examine this in more detail so that effects associated with differences in model architecture can be disentangled. Conclusion In this paper we conducted the first systematic investigation of the role of general text features in predicting human reading behavior using transformer-based language models (BERT & GPT-2). We have shown (1) that model accuracy is systematically linked to sentence-level text features spanning five measurement categories (syntax, complexity, lexical richness, register-specific N-gram frequency, readability, and psycholinguistic properties), and (2) that prediction accuracy can be improved by using hybrid models that con-sider spillover effects from the previous sentence. A Appendix Davies (2008)",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 0.9996632832718062
    },
    "reasoning": "Reasoning: The provided text does not mention any specific funding sources for the research conducted in the article. Without explicit mention of funding from defense, corporate entities, research agencies, foundations, or an indication of no funding, it is not possible to accurately determine the funding sources based on the information given."
}