{
    "article": "The accuracy of statistical parsing models can be improved with the use of lexical information. Sta tistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. \u2022 We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accu racy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English. Introduction Lexicalized grammars have been shown to be not only linguistically appealing but also desirable for parsing disambiguation . For example, among others, [Charniak, 1996] and [Collins, 1996] have found that lexicalizing a probabilistic model substantially increases parsing accuracy. As introduced in [Schabes et al ., 1988] , lexicalized tree adjoining grammar (LTAG) is a lexicalized grammar formalism in which lexical items are associated with sets of grammatical structures . [Resnik, 1992] shows that parsing disambiguation can be aided by statistical knowledge of cooccurrence relationships between LTA G structures . [Srinivas, 1997] and [Chen et al., 1999] show that considerable parsing disambigua tion is accomplished by assigning LTA G structures to words in the sentence using part of speech tagging techniques (supertagging). An LTA G grq,mmar G and a means of estimating parameters associated with G are prerequisites for probabilistic LTA G. [Schabes, 1992] shows how this may be done through grammar induction from an unbracketed corpus. The number of parameters that must be estimated, however, is prohibitively large for all but the most simple grammars. In contrast, [XTAG-Group, 1995] has developed XTAG, a complex, relatively broad coverage grammar for English . It is difficult, however, to estimate pa rameters with XTAG because it has been verified to accurately parse only relatively small corpora, such as the ATIS corpus . [Marcus et al., 1993) describes the Penn Treebank, a corpus of parsed sen tences that is large enough to estimate statistical parameters . From the treebank, [Srinivas, 1997] heuristically derives a corpus of sentences where each word is annotated with an XTAG tree, thus allowing statistical estimation of an LTA G. This method entails certain drawbacks: the heuristics make several mistakes, some unavoidable because of discrepancies between how XTAG and the Penn Treebank annotate the same grammatical constructions, or because XTAG does not cover all of the In this work, we explore extraction of an LTAG from the Penn Treebank. This allows us not only to obtain a wide coverage LTAG but also one for which statistical parameters can be reliably estimated. First, we develop various methods for extracting an LTAG from the treebank with the aim of being consistent with current principles for developing LTAG grammars such as XTAG. Second, we evaluate each grammar resulting from these methods in terms of its size, its coverage on unseen data, and its supertagging performance. Third, we introduce a preliminary method to extend an extracted grammar in order to improve coverage. Fourth, we situate our current work among other's approaches for tree extraction. Lastly, we present our conclusions and designs for future work. Tree Extraction Procedure In this section, we first describe the goals behind a tree extraction procedure and then describe the tree extraction procedure and its variations. An LTAG G is defined as a set of elementary trees T which are partitioned into a set I of initial trees and a set A of auxiliary trees. The frontier of each elementary tree is composed of a lexical anchor, the other nodes on the frontier are substitution nodes, and, in the case of an auxiliary tree, one node on the frontier will be a / oot node. The foot node of a tree {3 is labeled identically with the root node of /3. The sp ine of an auxiliary tree is the path from its root to its foot node. It is to be distinguished from the trunk of an elementary tree which is the path from its root node to the lexical anchor. Although the formalism of LTAG allows wide latitude in how trees in T may be defined, sev eral linguistic principles generally guide their formation. First, dependencies, including long distance dependencies, are typically localized in the same elementary tree by appropriate grouping of syntac tically or semantically related elements; \u2022 i.e. complements of a lexical item are included in the same tree as shown in Figure l(c). Second, recursion is factored into separate auxiliary trees as shown in Figure l(d) . The genesis of a tree 'Y lexicalized by a word w E S, where S is a bracketed sentence in the Penn Treebank, using our tree extraction procedure proceeds as follows. First, a head percolation table is used to determine the trunk of 'Y \u2022 Introduced in [Magerman, 1995) , a head percolation table assigns to each node in S a headword using local structural information. The trunk of 'Y is defined to be that path through S whose nodes are labeled with the headword w, examples of which are shown in Figure 1 (b). Each node r/ that is immediately dominated by a node 1J on the trunk may either be itself on the trunk , a complement of the trunk 's headword-in which case it belongs to 1 , or an adjunct of the trunk 's headword-in which case it belongs to another (auxiliary) tree /3 which modifies ,. It is therefore necessary to determine a node's status as a complement or adjunct. [Collins , 1997] introduces a procedure which determines just this from the treebank according to the node 's label, its semantic tags , and local structural information. As described in [Marcus et al., 1994] , a node's se mantic tags provide useful information in determining the node's status , such as grammatical function and semantic role. Our procedure for identifying complements or adjuncts closely follows the method in [Collins, 1997] . The main differences lie in our attempt to treat those nodes as complements which are typically localized in LT AG trees. A critical departure from [Collins , 1997] is in the treatment of landing site of wh-movement. [Collins, 1997] 's procedure treats the NP landing site as the head and its sibling (typically labelled S) as a complement. In our procedure for extracting LT AG trees , we project from a lexical item up a path of heads. Then , by adopting [Collins, 1997] 's treatment , the landing site would be on the path of projection and from our extraction procedure , the wh-movement would not be localized. Hence , we treat the sibling (S node) of the landing site as the head child and the NP landing site as a complement. Figure 4 ( c) shows an example of a lexicalized tree we extract that localizes long-distance movement. We have conducted experiments on two procedures for determining a node's status as complement or adjunct. The first procedure that we consider , \"CAl,\" uses the label and semantic tags of node 1J and 11's parent in a two step procedure. In the first step , exactly this information is used as an index into a manually constructed table , which determines complement or adjunct status. \"IF current node is PP-DIR AND parent node is VP THEN assign adjunct to current node\" is an example of an entry in this table. The table is sparse ; should the index not be found in the table then the second step of the procedure is invoked: 1. Nonterminal PRN is an adjunct. 2. Nonterminals with semantic tags NOM, DTV, LGS , PRD, PUT , SB J are complements. 3. Nonterminals with semantic tags ADV, VOC, LOC, PRP are adjuncts. 4. If none of the other conditions apply, the nonterminal is an adjunct. Whereas CAl uses the label and semantic tags of a node 1J and its parent 11', the procedure described in [Xia , 1999] , \"CA2 ,\" uses the label and semantic tags of a node 1J, its head sibling 1Jh , and distance between 1J and 1Jh in order to determine the complement or adjunct status of node 1J. CA2 relies on two manually constructed tables: an argument table and a tagset table. The argument table votes for 1J being a complement given the label and semantic tags of 1J and 1Jh and the distance between them. Fo r example , if 1J is marked as NP, then the argument table votes for 1J if 1Jh is labeled VB and if there is a less than four node separation between 17 and 1Jh. The tagset table votes for 17 being a complement based on the semantic tags of 1J alone. If both the argument table and the tagset table vote that 1] should be a complement , it is labeled as such. Otherwise , it is labeled as an adjunct. A recursive procedure is used to extract trees bottom up given a particular treebank bracketing. Figure 2 (a) shows one step in this proQ_ess. Among all of the children of node 11 2 , one child 171 is selected using the head percolation table so that the trunk </> associated with 11 1 is extended to This only considers conjunction of like categories. Although most conjunction is of this nature , it sometimes occurs that constituents with unlike categories are conjoined. In the Penn Treebank , these are annotated with the nonterminal label UCP. Although our current tree ext raction procedure does not treat these cases specially as conjunction , a similar strategy may be employed that does so , and in any case they are quite rare. The commas that were found in instances of conjunction were only one example of numerous cases of punctuation that are found in the treebank. In general , these are treated the same as adjuncts. On the other hand, it was found difficult to form a coherent strategy for dealing with quotes. Many times , an open quote would be found in one sentence and the closed quote would be found in an entirely different sentence. Therefore , we chose the simple strategy that quotes would anchor auxiliary trees that would adjoin to a neighboring sibling, namely, that sibling that was closer to the head sibling. The Penn Treebank has an extensive list of empty elements which are used to define phenomena\u2022 that are not usually expressed in LTAG. Among these are * U * , expressing a currency value , and * ICH * , indicating constituency relationships between discontinuous constituents. This observation led us to try two different strategies to cope with empty elements. The first strategy \"ALL\" is to include all empty elements in the grammar. The second strategy \"SOME\" is to only include empty elements demarcating empty subjects (0), empty PRO and passive NP trace ( * ), and traces ( * T * ) of syntactic movement ; these are usually found in LTA G grammars of English. The set of nonterminal and terminal labels in the Penn Treebank is quite extensive. A large set generally means that a greater number of trees are extracted from the Treebank ; these trees could miss some generalization and exacerbate the sparse data problem of any statistical model based on them. Also , some nonterminal labels are superfluous because they indicate structural configurations. Fo r example , NX is used to label nodes in the internal structure of multi-word NP conjuncts inside an encompassing NP. If NX were replaced by NP, the tree ext raction procedure can still determine . that an instance of conjunction exists and take appropriate action. On the other hand, distinctions that are made in a larger set of labels may aid the statistical model. Fo r these reasons , we evaluated two different strategies. One strategy, \"FULL,\" uses the original Penn Treebank label set. Another strategy, \"MERGED,\" uses a reduced set of labels. In the latter approach , the original set is mapped onto a label set similar to that used in the XTAG grammar ([XTAG-Group , 1995] ). In our approach , headedness and status as complement or adjunct was first determined according to the full set of labels before the trees were relabeled to the reduced set of labels. Besides modifier auxiliary trees , there are predicative auxiliary trees which are generated as follows. During the bottom up extraction of trees , suppose trunk </> has a node 'T/ that shares the same label as another node 'f/ 1 , where 'f/ 1 is a complement , not on </>, but is immediately dominated by a node on </J. In this case, a predicative auxiliary tree is ext racted where 'T/ is its root , 'T/' is its foot and with </> serving as its trunk. Subsequently, the path </>' dominated by 'T/' becomes a candidate for being extended further. See Figure 4(a) . This mechanism wo rks in concert with other parts of our tree ext raction procedure (notably complement and adjunct identification , merging of nonterminal labels (from SB AR to S), and policy of handling empty elements) in order to p roduce trees that localize long distance movement as shown in Figure 4(c) . Evaluation Each variation of tree extraction procedure was used to extract a grammar from Sections 02-21 of the Penn 'Ireebank. These grammars were evaluated according to size, well formedness of trees, their coverage on Section 22, and their performance in supertagging Section 22. We subsequently evaluated truncated forms of these grammars which we term cutoff grammars. The grammars' sizes in terms of number of lexicalized trees and tree frames are shown in Table 1 . Removing the anchor from a lexicalized tree yields a tree frame. In terms of different tree extraction strategies, MERGED yields more compact grammars than FULL, SOME more than ALL, and CA2 more than CAL Perhaps the last dichotomy requires more of an explanation. Basically, CA2 factors more nodes into auxiliary trees, with the result being that there are fewer trees because each one is structurally simpler. We may also qualitatively judge grammars according to how well they satisfy our goal of extracting well formed trees in the sense of selecting the appropriate domain of locality and factoring recursion when necessary. There is not much difference between SOME and ALL because the empty elements that SOME ignores are the ones that are not usually captured by any LTAG grammar. Likewise, there is little difference between MERGED and FULL because most of MERGE's label simplification does not occur until after completion of tree extraction. The main difference lies between CAI and  CA2, strategies for labeling complements and adjuncts. Nodes detected as complements of a particular lexical item belong in the same ele_ mentary tree, thus satisfying the criterion of localizing dependencies. We believe that CAl labels nodes closer to what is traditionally found in LTAG grammars such as XTAG than does CA2, in that use of CA2 will generate less appropriate subcategorization frames because it tends to factor what might be considered as complements into separate auxiliary trees. It is difficult, however, to quantify the degree to which strategies CAl and CA2 are successful in distinguishing complements from adjuncts because there are no precise definitions of these terms. Here we resign ourselves to a qualitative comparison of an example of a lexicalized tree extracted from the same sentence by a CAl derived grammar G 1 (CAl SOME-MERGED) and a CA2 derived grammar G 2 (CA2-SOME-MERGED). First, a tree frame F is selected from G1 that is absent from G 2 \u2022 A bracketed sentence S out of which the CAl approach extracts Fis then culled from the training corpus at random . Figure 5(a) shows S, (b) shows the tree corresponding to the main verb extracted to G 1 , (c) shows the tree corresponding to the main verb extracted to G 2 \u2022 It is typical of the examples of divergence between CAl and CA2 derived grammars: the CAl approach leads to a verb subcategorization that is more complicated, yet more appropriate. The various extracted grammars may also be evaluated according to breadth of coverage. In order to evaluate coverage of a particular grammar G, the strategy used to produce G was used to produce trees from held out data. We subsequently determined the degree of coverage of that strategy by the overlap in terms of tree frames and lexicalized trees as shown in Table 2 . For lexicalized trees t extracted from held-out data such that t ff: G, we also show the percentage of time the lexical anchors of such trees t were or were not found in the training corpus ( column in diet and not in diet respectively). For ' \\\ufffd :-. 2 reports that use of strategy CAI-ALL-FULL resulted in a grammar such that 99.56% of instances of tree frames in held out data were available in the grammar, and 91.57% of instances of lexicalized trees in held out data were found in the grammar. Of the remaining 8.43%, 2. 76% ( not in diet) of the lexicalized trees in the held out data involved words not seen in the training corpus. The remaining 5.67% therefore are anchored by words in the training corpus but the corresponding associations of tree frames and lexical items were not made in the training corpus. The table shows that strategies that reduce the number of extracted trees (SOME, CA2, MERGED) tend to also increase coverage. CA1 -ALL-FULL -+ CA 1-ALL-MERGED -+--CA 1-SOME-FULL \u2022B\u2022\u2022\u2022 CA 1-SOME-MERGED \u2022\u2022l><\u2022\u2022\u2022\u2022\u2022 CA2-ALL-FULL We also measure the accuracies of supertagging models which are based on the various grammars that we are evaluating. Results are shown in Table 2 . Curiously, the grammars whose associated models achieved the highest accuracies did not also have the highest coverage. For example, CAl SOME-MERGED beat CA2-SOME-MERGED in terms of accuracy of supertagging model although the latter achieved higher coverage. This could possibly be caused by the fact that a high coverage grammar might have been obtained because it doesn't distinguish between contexts on which a sta tistical model can make distinctions. Alternatively, the cause may lie in the fact that a particular grammar makes better (linguistic) generalizations on which a statistical model can base more accurate predictions. A large grammar may lead to a statistical model that is prohibitively expensive to run in terms of space and time resources. Furthermore, it is difficult to obtain accurate estimates of statistical parameters of trees with low counts. And, in any case, trees that only infrequently appear in the training corpus are also unlikely to appear in the test corpus. For these reasons, we considered the effect on a grammar if we removed those tree frames that occurred less than k times, for some cutoff value k. We call these cutoff grammars. As shown in Figure 6 , even low values for k yield substantial Even though a cutoff grammar may be small in size, perhaps a statistical model based on such a grammar would degrade unacceptably in its accuracy. In order to see if this could indeed be the case, we trained and tested the supertagging model on various cutoff grammars. In the training data for these supertagging models, if a particular full grammar suggested a certain tree ti for word Wi, but the cutoff grammar did not include ti then word Wi was tagged as miss. The cutoff value of k = 3 was chosen in order to reduce the size of all of the original grammars at least in half. By the results shown in Table 2 , it can be seen that use of a cutoff grammar instead of a full extracted grammar makes essentially no difference in the accuracy of the resulting supertagging model. Extended Extracted Grammars The grammars that have been produced with the tree extraction procedure suffer from sparse data problems as shown in Table 2 by the less than perfect coverage that these grammars achieve on the test corpus. This is perhaps one reason for the relatively low accuracies that supertagging models based on these grammars achieve compared to, for example, [Srinivas, 1997 ) and [Chen et al., 1999) . Many approaches may be investigated in order to improve the coverage. Fo r example, although XTAG may be inadequate to entirely cover the Penn Treebank, it may be sufficient to ameliorate sparse data. Here we discuss how linguistic information as encoded in XTAG tree families may be used for this purpose and deliver some preliminary results. [ XTAG-Group, 1995) explains that the tree frames anchored by verbs in the XTAG grammar are divided into tree families. Each tree family corresponds to a particular subcategorization frame. The trees in a given tree family correspond to various syntactic transformations as shown in Figure 7 . Hence, if a word Wi is seen in the training corpus with a particular tree frame ti, then it is likely for wo rd Wi to appear with other tree frames t ET where T is the tree family to which t i belongs. This observation forms the basis of our experiment. The extracted grammar G 0 derived from CAl SOME-MERGED was selected for this experiment. Call the extended grammar G 1 . Initially, all of the trees t E Go are inserted into G1. Subsequently, for each lexicalized tree t E G 0 , lexicalized trees t' are added to G1 such that t and t' share the same lexical anchor and the tree frames of t and t' belong to the same tree family. Out of approximately 60 XTAG tree families, those tree families that were considered in this experiment we re those corresponding to relatively common subcategorization frames including intransitive, NP, PP, S, NP-NP, NP-PP and NP -S. We achieved the following results. Recall that in Table 2 we divided the lapses in coverage of a particular extracted grammar into two categories : those cases where a word in the test corpus was not seen in the training corpus ( not in <J&ct), and those cases where the word in the test corpus was seen in training, but not with the appropriate tree frame ( in diet). Because our procedure deals only with reducing the latter kind of error, we report results from the latter 's fr&me of reference. Using grammar G 0 , the in diet lapse in coverage occurs 4.98% of the time whereas using grammar G1, such lapses occur 4.61% of the time, an improvement of about 7.4%. This improvement must be balanced against the increase in grammar size. Grammar Go has 4900 tree frames and 114850 lexicalized trees. In comparison, grammar G1 has 4999 tree frames and 372262 lexicalized trees. The results are somewhat encouraging and we believe that there are several avenues for improve ment. The number of lexicalized trees in the extended extracted grammar can be reduced if we account for morphological information. Fo r example, a verb \"drove\" cannot occur in passive forms. Instead of capitalizing on this distinction, our current procedure simply associates all of the tree frames in the transitive tree fam ily with \"drove.\" In related work, we are currently conducting an experiment to quantitatively extract a measure of similarity between pairs of supertags (tree frames) by taking into account the distribution of the supertags with words that anchor them. When a particular supertag word combination does not appear in the training corpus, instead of assigning it a zero probability, we assign a probability that is obtained by considering similar supertags and their probability of being assigned with this word. This method seems to give promising results, circumventing the need for manually designed heuristics such as those found in the supertagging work of [Srinivas, 1997] and [Chen et al., 1999] . We plan to apply this strategy to our extracted grammar and verify if similar improvements in supertagging accuracy can be obtained. 5 Related Work [Neumann, 1998] presents a method for extracting an LTA G from a treebank. Like our work, [Neumann, 1998] determines the trunks of elementary trees by finding paths in the tree with the same headword, headwords being determined by a head percolation table. Unlike our work, [Neumann, 1998] factors neither adjuncts nor instances of conjunction into auxiliary trees. As a result, [Neumann, 1998] 's method generates many more trees than we do. Using only Sections 02 through 04 of the Penn 'Ireebank, [Neumann, 1998] produces about 12000 tree frames. Our approaches produces about 2000 to 9000 tree frames using Sections 02-21 of the Penn 'Ireebank . [Xia, 1999] also presents work in extracting an LTAG for a treebank, wo rk that was done in parallel with our own work. Like our work, [Xia, 1999] determines the trunk of elementary trees by finding paths in the tree with the same headword. Fu rthermore, [Xia, 1999] factors adjuncts (according to CA2 only) into separate auxiliary trees. Also, following our suggestion [Xia, 1999] factors instances of conjunction into separate auxiliary trees. [Xia, 1999] 's approach yields either 3000 or 6000 tree frames using Sections 02-21 of the Penn 'Ireebank, depending on the preterminal tagset used. Our work explores a wider variety of parameters in the extraction of trees, yielding grammars that have between about 2000 to 9000 tree frames on the same training data. Unlike our work in the extraction of an LTAG, [Xia, 1999] extracts a multi-component LTA G through coindexation of traces in the Penn 'Ireebank. Another difference is that [Xia, 1999] is more concerned with extraction of grammar for the purpose of grammar development (for which [Xia, 1999] for example makes the distinction between extracted tree frames that are \u2022 grammatically correct and those that are incorrect), whereas our current wo rk in extraction of grammar betrays our ultimate interest in developing statistical models for parsing (for which we perform an investigation of coverage, supertagging accuracy, effect of cutoff frequency, as well as explore the issue of extending extracted grammars using XTAG tree families for eventual use in statistical smoothing). This work raises the question as to how parsing with LTA G may compare to parsing where the model is based on a lexicalized context free formalism. Both recent work in parsing with lexicalized models and LTAG appear to manipulate basically the same kinds of information . Indeed, only a few trees in our extracted grammars from the Penn 'freebank have the form to cause the generative capacity of the grammars to exceed those of lexicalized context free grammars . The wo rk presented here makes it possible to see how a statistical parsing model based on an LTAG compares with models based on lexicalized context free grammars . Furthermore, supertagging as a preprocessing step may be used to improve the efficiency of a parsing using a statistical model based on an LTA G. We plan to explore these issues in future research. Conclusions Our wo rk presents some new directions in both the extraction of an LTAG from the Penn 'freebank as well as its application to statistical models . In the extraction of an LTAG from the Penn 'freebank, we have extended (Neumann, 1998 ]'s procedure to produce less unwieldy grammars by fac toring recursion that is found in adjuncts as well as in instances of conjunction . We have explored the effects that different definitions of complement and adjunct, whether or not to ignore empty elements, and extent of label sets have on the quality and size of the extracted grammar, as well as ability to cover an unseen test corpus . We have also evaluated those grammars according to supertagging accuracy. We have experimented with the notion of cutoff grammar, and seen that these grammars are more compact and yet yield little in the way of supertagging accuracy. We have introduced a preliminary technique for extending an extracted grammar using an external resource, namely, the tree families of XTAG. We have seen that this technique expands the coverage of an extracted grammar, and discussed how this technique may be developed in order to achieve better results. There are a number of ways to extend this work of extracting an LTA G from the Penn 'freebank. Because our goal is to develop a grammar around which to base statistical models of parsing, we are in particular interested in better procedures for extending the extracted grammars . Besides merely extending a grammar, it is also necessary to develop a method for estimating how often trees that are unseen in the training corpus, but are part of the e \ufffd tended grammar, are expected to occur in test data . After such issues are resolved, the extracted grammar could be used in a probabilistic model for LTAG, as de lineated by [ Schabes, 1992] and (Resnik, 1992] . This would provide not only another means of comparing different va rieties of extracted grammar, but would also allow comparison of LTAG parsing against the many other lexicalized parsing models mentioned in the introduction. Acknowledgements This wo rk was supported by NSF grants #SBR-97 104 11 and #GER-93 54 869.",
    "abstract": "The accuracy of statistical parsing models can be improved with the use of lexical information. Sta tistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. \u2022 We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accu racy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "109",
    "year": "2000",
    "month": "February 23-25",
    "title": "Automated Extraction of {TAG}s from the {Penn} {Treebank}"
}