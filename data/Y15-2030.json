{
    "article": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920. Introduction Sentiment analysis or opinion mining is the computational study of people's opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics and their attributes (Liu and Zhang, 2012) . The task of sentiment analysis is technically challenging and practically very useful. For example, businesses always want to find public or consumer opinions about their products and services. Consumers also need a sounding board rather than thinking alone while making decisions. With the development of Internet, opinionated texts from social media (e.g., reviews, blogs and micro-blogs) are used frequently for decision making, which makes automated sentiment analysis techniques more and more important. Among those tasks of the sentiment analysis, the key one is to classify the polarity of given texts. Many works have been done in recent years to improve English sentiment polarity classification. There are two categories of such works. One is called \"machine learning\" which is firstly proposed to determine whether a review is positive or negative by using three machine learning methods, including NB, ME and SVM (Pang et al., 2002) . The other category called \"semantic orientation\" is applied to classify words into various classes by giving a score to each word to evaluate the strength of sentiment. And an overall score is calculated to assign the review to a specific class (Turney, 2002) . Recently, researchers have tried to handle tasks of Natural Language Processing (NLP) with the help of deep learning approaches. Among those approaches, a useful one called word2vec has attracted increasing interest. Word2vec translates words to vector representations (called word embeddings) efficiently by using skip-gram algorithm (Mikolov et al., 2013a) . It is also proposed that the induced vector representations capture meaningful syntactic and semantic regularities, for example, \"King\" -\"Man\" + \"Woman\" results in a vector very close to \"Queen\" (Mikolov et al., 2013b) . Besides, with the advancement of information technology, for the first time in Chinese history, a huge volume of Chinese opinionated data recorded in digital form is ready for analysis. Though Chinese language plays an important role in economic globalization, there are few works have been done for Chinese sentiment analysis with huge databases. It inspires us to make an empirical study on Chinese sentiment with bigger databases than usual. The remain of the article is organized as follows: Section 2 briefly describes related work. Section 3 describes details of the methods used in training procedure. Section 4 reports and discusses the results. Finally, we summarize our works in Section 5. Related work According to Liu and Zhang (2012) , the sentiment analysis research mainly started from early 2000 by Turney (2002) and Pang et al. (2002) . Turney (2002) firstly used a few semantically words (e.g., excellent and poor) to label other phrases with the hit counts by queries through search engines. Then, researchers had also proposed several custom techniques specifically for sentiment classification, e.g., the score function based on words in positive and negative reviews (Dave et al., 2003) and feature weighting schemes used to enhance classification accuracy (Paltoglou and Thelwall, 2010) . Besides, the other situation of sentiment analysis is to represent texts by vectors which indicate these words appear in the text but do not preserve word order. And a machine learning approach will be used for classification in the end. In such way, Pang et al. (2002) considered classifying documents according to standard machine learning techniques. In addition, subsequent research used more features in learning, making the main task of sentiment classification engineer an effective set of features (Pang and Lee, 2008) . However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005) . Li and Sun (2007) presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes. They found that SVM achieved the best performance. After that, Tan and Zhang (2008) found 6,000 or bigger for the size of features would be sufficient for Chinese sentiment analysis, and sentiment classifiers were severely dependent on domains or topics. Nowadays, inspired by the availability of large text corpus and the success of deep learning approaches, some researchers (e.g., Collobert et al. (2011), Johnson and Zhang (2014) ) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al. (2014) got some state-of-the-art performance. But the work of Collobert et al. (2011) was paid most attention for describing a unified architecture for NLP tasks which learned features by training a deep neural network even when being given very limited prior knowledge. These NLP tasks included part-of-speech tagging, chunking, named-entity recognition, language model learning and semantic role labeling. Methodology This section presents the methodology used in our experiment. Feature selection methods Sentiment lexicon and CHI A sentiment lexicon accommodating sentiment words plays an important role in sentiment analysis. A combination of two Chinese sentiment lexicons (Hownet (Dong and Dong, 2006) and DLLEX (Xu et al., 2008) ) is constructed, including 30406 words in total. After removing those words which do not appear in the corpus, 10444 sentiment words are preserved. After several experiments, CHI (Galavotti et al., 2000) is chosen for information gain. Finally, 150 most valuable words are added into the new lexicon. At last, 10543 words are obtained as features. Word2vec Word2vec (Mikolov et al., 2013a) has gained kinds of traction today. As the name shows, it translates words to vectors called word embeddings. That is to say, it gets the vector representations of words. Gensim 1 , a python tool is used to get word2vec module. The method of training word2vec model is unsupervised learning and 300 is set as the quantity of the dimension of vectors. Table 1 shows the word embeddings of a Chinese hotel review which means the room is very clean and neat. For convenient display, each value of dimension is multiplied by 10,000 and indicated by d i (i = 1, ..., 300). word d 1 d 2 d 2 ... Traditional methods Naive Bayes Classification Naive Bayes (NB) is widely used in sentiment classification which is used to classify a given review document d to the class c * = argmax c P (c|d). According to Bayes's rule, P (c j |d) = P (c j )P (d|c j ) P (d) where c j is a kind of class and P (d) plays no role in selecting c * . Let's mark f 1 , f 2 , ...f m as the set of features that appear in all reviews, and set n i (d) as the number of times f i appears in d. Usually, n i (d) is set as 1, if f i appears more than one time. Then, a formulation can be gotten as P (c j |d) = P (c j ) m i P (f i |c j ) n i (d) P (d) where the estimation of P (f i |c j ) is calculated as follows, using add-one smoothing P (f i |c j ) = 1 + n ij m + m k=1 n kj Maximum Entropy Classification Maximum Entropy Classification follows the principle of maximum entropy (Jaynes, 1957) , which means, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one with largest entropy. Thus, the estimate of P (c j |d) is showed as follows P (c j |d) = 1 \u03c0 (d) exp m i=1 \u03bb i,c j F i,c j (d, c j ) F i,c j (d, x) = 1 if n i > 0 and x = c j 0 otherwise where \u03c0(d) is a normalization function and \u03bb i,c j is the weight of f j in maximum entropy c j .The other parameters are defined in the same way as Section 3.2.1. After fifteen iterations of the improved iterative scaling algorithm (Pietra et al., 1997) implemented in Natural Language Toolkit (Bird, 2006) , the parameters of \u03bb i,c j are adjusted to maximize the entropy of distribution of training data. Support Vector Machines Support Vector Machines (SVM) is a very effective machine learning method firstly introduced by (Cortes and Vapnik, 1995) . SVM constructs a hyperplane or a set of hyperplanes in a high dimensional space represented by w. Since the larger the margin, the lower the error of the classifier, after training, the largest distance of support vector to nearest trainingdata point in any classes is achieved. Then the problem of maximizing the margin turns to argmin w, b 1 2 ||w|| 2 where y i ( w \u2022 x i \u2212 b) \u2265 1 and its unconstrained dual form is the following optimization problem: maximize L(\u03b1) where L(\u03b1) = n i=1 \u03b1 i \u2212 1 2 i,j \u03b1 i \u03b1 j y i y j k(x i , x j ) = n i=1 \u03b1 i \u2212 1 2 i,j \u03b1 i \u03b1 j y i y j x T i x j subject to \u03b1 i \u2265 0 (i = 1, . . . , n). Usually, the kernel here is linear, which means k(x i , x j ) = x i \u2022 x j For SVM models, python tool scikit-learn (Pedregosa et al., 2011) is chosen for training and testing. Scikit-learn 2 was started in 2007 as a Google Summer of Code project, and has became the most efficient and useful tool for data mining and analysis in Python. With all default parameters, LinearSVC and SVC with linear kernel are used in our article. Ensemble methods Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one. There are two families of ensemble methods, averaging methods and boosting methods. In averaging methods, several estimators will be built to average their predictions. It is a kind of vote, namely, on average. The combined estimator is usually better than any of the fundamental estimators since its variance is reduced (e.g., Bagging methods and Forests of randomized trees). By contrast, in boosting methods, fundamental estimators are built sequentially and each one tries to reduce the bias of the combined estimator. The idea behind it is to combine several weak models to generate a more powerful ensemble model (e.g., AdaBoost and Gradient Tree Boosting). The ensemble method modules are chosen from scikit-learn, including AdaBoost, Gradient Tree Boosting and Random Forests. For each Chinese review, the average value of word embeddings is used as the input. CNN methods CNN is short for Convolutional Neural Networks. Its key module is to calculates the convolution between input and output. Just as CNN used in computer vision, a matrix is needed, as the input of CNN. After several experiments, we set D = 60 as the dimension quantity of word embeddings for CNN. If there are L words in a sentence, combine their word embeddings together to construct a matrix of size L \u00d7 D as shown in Figure 1 . L = 60 is set since fixed L is needed, and which means, only 60 words are preserved from the beginning of a review. On the other hand, if the length is less than 60, the matrix will be filled with used vectors from the beginning of the review by repeating them. At last, every review is represented by a matrix of size 60 \u00d7 60. Formally, in computer vision, given n images (X l , l = 1, ..., n) of size r \u00d7 c, k kernels of size  With the fourth layer, a fully-connect sigmoidal layer is constructed to classify the output values. After experiments, there are some rules can be concluded: \u2022 The quantity of the word embedding dimensions shall be more than 50. \u2022 Do not use pooling between the dimensions of word embedding (thus, in Table 2 , the size of pooling is 2\u00d71). \u2022 Adding more fully-connect sigmoidal layer dose not help in improving F1 score.  4 Experiment results Corpuses Unlike English corpuses, Chinese corpuses are relatively small and usually focus on POS tagging (Mingqin et al., 2003) , parsing (Xue et al., 2005) and translating (Xiao, 2010) . In Chinese sentiment classification, the most popular corpus is ChnSen-tiCorp (Tan and Zhang, 2008) with 7,000 positive reviews and 3,000 negative reviews 5 . Since the amount of data collected by previous Chinese NLP researchers is too small for our work, we build a new corpus, MioChnCorp, with a million Chinese hotel reviews. The corpus is public and can be downloaded directly 6 . The reviews are crawled and filtrated from the website 7 which has coarse-grained rating (5-star scale) for each review. We give up the 3-star reviews which may be ambiguous, and mark the five-star and four-star reviews as positive and the rest as negative. Finally 908189 positive reviews and 101762 negative reviews are obtained. After word segmentation 8 being done, the sentiment classification process is executed. Since ChnSentiCorp is small, the result may be unstable. Thus, Tan and Zhang (2008) gave the best performance and mean performance to evaluate a classification method. Zhai et al. (2011) tried to get a believable result using the average value from 30 experiments. See Figure 2 , Naive Bayes and Logistic Regression are used as classification methods to show the performance curves with different amount of reviews. The first sub-graph is tested on ChnSen-tiCorp, the second on is tested on MioChNCorp. Balanced corpuses are split into 3 equal-sized folds, two for training, the rest for testing. After repeating each experiment five times, the best performance and worst performance are marked. At last, two conclusions can be made: Firstly, when the amount of reviews is less than 60,000, the performance will be improbable (the best performance of model minus worst performance is bigger than 0.01). Secondly, more data usually help to get better performance, but the performance will be finally stable when data are big enough (e.g., 120,000 reviews). The performance measure F1 score (also called F-measure) is a measurement of a test's accuracy which combines recall and precision as follows: F 1 = 2 \u2022 P recision \u2022 Recall P recision + Recall Recall = correct positive predictions amount positive example amount P recision = correct positive predictions amount positive predictions amount since there are two categories (positive and negative) in MioChnCorp, Macro F1 is used to evaluate the performance of classification method over the corpus M acro F 1 = P ostive F 1 + N egative F 1 2 in the rest of this article, F1 score means Macro F1 score. Experimental design Nine methods are designed to classify MioChnCorp using different features. NB and ME use 10543 words (the sentimental words and CHI words). LinearSVC use unigram and bigram. Five methods (SVC, LR, AdaBoost, Gradient Tree Boosting (GBT) and Random Forests (RF)) use the average vectors of word embeddings and CHI words (extending the dimension quantity to 450). CNN use the matrix constructed by word embeddings from words in a review as feature. Though all of these models are effective, the combination of different machine learning methods is supposed to acquire better F1 score. There are two ways to combine those methods. First is vote, the idea is simple, \"the minority is subordinate to the majority\" (marked as Vote all ). The other way is to over-fit in the validate set. Add one more fold for validating into these tree folds. After training, nine models will be constructed. And each model gives one predication list for validating set. For each review, there are nine predications (e.g., [0 0 1 0 1 0 0 1 0 ], 0 means negative, 1 means positive). Using the predication vectors of validating reviews as input, and the labels of validating reviews as the Logistic Regression output, after training on validating set, the combination model (called LR all) is built to test on testing set. The Framework of LR all is shown in Algorithm 1. Comparison and analysis Table 3 , Table 4 and Table 5 show the performance of different machine learning methods. Subjected to hardware recourse (RAM:8G, CPU:Intel I5, GPU:GTX960M), the experiments are explored over corpuses with tree size: 40,000, 80,000 and 120,000. Each corpus is divided into four folds which are equal in size, two for training, one for validating, the rest for testing. There are nine methods to construct the LR all model, but not all of them make contribution. Weka Explorer 9 provides attribute selection module to choose most useful attributes to the target attribute (namely, the label list of validate set in our situation). Extracting the validate list i (0 <= i < n) used in Algorithm 1, and combining these nine prediction lists with the label list of validate set, totally, ten attributes will be gotten. With 10-fold cross-validation, CfsSubsetEval attribute evaluator and BestFisrt search Method, Weka selects five most valuable attributes (ME, SVC, LinearSVC, RF and CNN). It is reasonable because they are most outstanding machine learning models which represent their own feature selection methods. Considering the limit of hardware resource and running time, LR is used to instead of SVC and CNN is abandoned. The result is shown in Figure 3 . To our surprise, even only four feature is chosen, the F1 score is not reduced. The more reviews we use in model building, typically the better performance we get till the performance is stable. SVM (linearSVC and SVC with linear kernel) has best performance not only  not reach the expectant performance. Firstly, the amount of reviews is not big enough to train a deep learning model. Secondly, the architecture of the model may not be enough suitable as a language model. Finally, the features (word embeddings with 60 dimensions) for CNN is not accurate enough to present syntax and semantics in sentence. Vote all does not work well in improving performance, but has the highest negative recall and positive precision. LR all has better performance than Vote all because the same weights chosen by Vote all make these sub-models are equally important. Conclusion and Future Work In this article, an empirical study of sentiment categorization on Chinese hotel review is introduced. In order to conduct this experiment, a Chinese corpus, MioChnCorp 10 , with a million Chinese hotel reviews is collected. Using MioChnCorp, a word2vec model is trained to present distributed representations of words and phrases in Chinese hotel domain. Then the experimental results indicate that the more data we use, the better performance we get. And 60,000 or larger size (e.g. 120,000) of reviews are sufficient in sentiment analysis of Chinese hotel review. What's more, we employ word embeddings as input features without any sentiment lexicons, and find such features perform well by using ensemble methods, LR, SVM and CNN. With respect to these learning methods, SVM works best. Though CNN 10 http://pan.baidu.com/s/1dDo9s8h works not as good as expect, it still has better performance than NB and ME. The roles we used to construct the CNN model is introduce in Section 3. Finally, a methodology, LR all is constructed to combine different machine learning methods and get an outstanding performance in precision, recall and F1 score of 0.920. In the future, more work will be explored in building better CNN model for Chinese sentimental analysis and constructing combinational model in other tasks of NLP using word embedding. Acknowledgements The financial support for this work is provided by The Fundamental Research Funds for the Central Universities, No. ZYGX2014J065. References Steven Bird. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69-72. Association for Computational Linguistics. learning in python. The Journal of Machine Learning Research, 12:2825 -2830 . Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997",
    "abstract": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "34",
    "year": "2015",
    "month": "October",
    "title": "An Empirical Study on Sentiment Classification of {C}hinese Review using Word Embedding"
}