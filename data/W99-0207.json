{
    "article": "In this paper we propose a corpus-based approach to anaphora resolution combining a machine learning method and statistical information. First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the number of potential candidates. In the second step, preference selection is achieved by taking into account the frequency information of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current discourse. Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%. Introduction Coreference information is relevant for numerous NLP systems. Our interest in anaphora resolution is based on the demand for machine translation systems to be able to translate (possibly omitted) anaphoric expressions in agreement with the morphosyntactic characteristics of the referred object in order to prevent contextual misinterpretations. So far various approaches 1 to anaphora resolution have been proposed. In this paper a machine learning approach (decision tree) is combined with a preference selection method based on the frequency information of non-/coreferential pairs tagged in the corpus as well as distance features within the current discourse. The advantage of machine learning approaches is that they result in modular anaphora resolution systems automatically trainable from a corpus with no 1See section 4 for a more detailed comparison with related research. or only a minimal amount of human intervention. In the case of decision trees, we do have to provide information about possible antecedent indicators (syntactic, semantic, and pragmatic features) contained in the corpus, but the relevance of features for the resolution task is extracted automatically from the training data. Machine learning approaches using decision trees proposed so far have focused on preference selection criteria directly derived from the decision tree results. The work described in (Conolly et al., 1994) utilized a decision tree capable of judging which one of two given anaphor-antecedent pairs is \"better\". Due to the lack of a strong assumption on \"transitivity\", however, this sorting algorithm is more like a greedy heuristic search as it may be unable to find the \"best\" solution. The preference selection for a single antecedent in (Aone and Bennett, 1995) is based on the maximization of confidence values returned from a pruned decision tree for given anaphor-candidate pairs. However, decision trees are characterized by an independent learning of specific features, i.e., relations between single attributes cannot be obtained automatically. Accordingly, the use of dependency factors for preference selection during decision tree training requires that the artificially created attributes expressing these dependencies be defined. However, this not only extends human intervention into the automatic learning procedure (i.e., which dependencies are important?), but can also result in some drawbacks on the contextual adaptation of preference selection methods. The preference selection in our approach is based on the combination of statistical frequency information and distance features in the discourse. Therefore, our decision tree is not applied directly to the task of preference selection, but aims at the elimination of irrelevant candidates based on the knowledge obtained from the training data. The decision tree is trained on syntactic (lexical word attributes), semantic, and primitive discourse (distance, frequency) information and determines the coreferential relation between an anaphor and antecedent Candidate in the given context. Irrelevant antecedent candidates are filtered out, achieving a noise reduction for the preference selection algorithm. A preference value is assigned to each \" potential anaphor-candidate pair depending on the proportion of non-/coreferential occurrences of the pair in the training corpus (frequency ratio) and the relative position of both elements in the discourse (distance). The candidate with the maximal preference value is resolved as the antecedent of the anaphoric expression. Corpus-Based Anaphora Resolution In this section we introduce a new approach to anaphora resolution based on coreferential properties automatically extracted from a training corpus. In the first step, the decison tree filter is trained on the linguistic, discourse and coreference information annotated in the training corpus which is described in section 2.1. Data Comferenc\u00a2 Analysis Preference Selection  The resolution system in Figure 1 applies the coreference filter (cf. section 2.2) to all anaphorcandidate pairs (Ai + C/#) found in the discourse history. The detection of anaphoric expressions is out of the scope of this paper and just reduced to tags in our annotated corpus. Antecedent candidates are identified according to noun phrase part-of-speech tags. The reduced set (Ai + C/~) forms the input of the preference algorithm which selects the most salient candidate C~ as described in section 2.3. Preliminary experiments are conducted for the task of pronominal anaphora resolution and the performance of our system is evaluated in section 3. Data Corpus For our experiments we use the ATR-ITL Speech and Language Database (Takezawa et al., 1998) (nominal: 2160, pronominal: 526, ellipsis: 3843) . Besides the anaphor type, we also include morphosyntactic information like stem form and inflection attributes for each surface word as well as semantic codes for content words (Ohno and Hamanishi, 1981) c2: II~,% -7\"4--..x.--~,.x.--9\"4\",-x---~o [yes] rr] [A] [N] {A] [K] [A] [be] \"It's T A N A K A.\" [yes] [tenth] [here] [arrival] [be] \"Okay, you will arrive here on the tenth, right?\" In the example dialog between the hotel reception (r) and a customer (c) listed in Figure 2 the proper noun (rl)\"5,#-# ~Y-~P [City Hotel]\" is tagged as the antecedent of the pronoun (cl)\"~-~5 ~9 [there]\" as well as the noun (cl)\"$ff-)l~ [hotel]\". An example for ellipsis is the ommitted subject (c2)\"@[it]\" referring to (r2)\"Y~-x~P [spelling]\". According to the tagging guidelines used for our corpus an anaphoric tag refers to the most recent antecedent found in the dialog. However, this antecedent might also refer to a previous one, e.g. Based on the corpus annotations we extract the frequency information of coreferential anaphorantecedent pairs and non-referential pairs from the training data. For each non-/coreferential pair the occurrences of surface and stem form as well as semantic code combinations are counted.  In Table 1 some examples are given for pronoun anaphora, whereas the expressions \"{...}\" denote semantic classes assigned to the respective words. The values freq +, freqand ratio and their usage are described in more detailed in section 2.3. Moreover, each dialog is subdivided into utterances consisting of one or more clauses. Therefore, distance features are available on the utterance, clause, candidate, and morpheme levels. For example, the distance values of the pronoun (r3)\"~-\u00a29 [here]\" and the antecedent (rl)\" \"Y if-4 \u2022 ff-)l~ [City Hotel]\" in our sample dialog in Figure 2 are d~tte~=4, dclaus~=7, dcand=14, dmorph=40. Coreferenee Analysis To learn the coreference relations from our corpus we have chosen a C4.52-1ike machine learning algorithm without pruning. The training attributes consist of lexical word attributes (surface word, stem form, part-of-speech, semantic code, morphological attributes) applied to the anaphor, antecedent candidate, and clause predicate. In addition, features like attribute agreement, distance and frequency ratio are checked for each anaphor-candidate pair. The decision tree result consists of only two classes determining the coreference relation between the given anaphor-candidate pair. During anaphora resolution the decision tree is used as a module determining the coreferential property of each anaphor-candidate pair. For each detected anaphoric expression a candidate list 3 is created. The decision tree filter is then successively applied to all anaphor-candidate pairs. If the decision tree results in the non-reference class, the candidate is judged as irrelevant and eliminated from the list of potential antecedents forming the input of the preference selection algorithm. Preference Selection The primary order of candidates is given by their word distance from the anaphoric expression. A straightforward preference strategy we could choose is the selection of the most recent candidate (MRC) as the antecedent, i.e., the first element of the candidate list. The success rate of this baseline test, however, is quite low as shown in section 3. But, this result does not mean that the recency factor is not important at all for the determination of saliency in this task. One reason for the bad performance is the application of the baseline test to the unfiltered set of-candidates resulting in the frequent selection of non-referential antecedents. Additionally, long-range references to candidates introduced first in the dialog are quite frequent in our data. 2Cf. (Quinlan, 1993) 3A list of noun phrase candidates preceding the anaphor element in the current discourse. An examination of our corpus gives rise to suspicion that similarities to references in our training data might be useful for the identification of those antecedents. Therefore, we propose a preference selection scheme based on the combination of distance and frequency information. First, utilizing statistical information about the frequency of coreferential anaphor-antecedent pairs (freq +) and non-referential pairs (freq-) extracted from the training data, we define the ratio of a given reference pair as follows4: I -6 : (freq + --freq-= O) ratio = ]req + -]req-freq+ 4-]req-: otherwise The value of ratio is in the range of [-1,-1-1], whereby ratio = -1 in the case of exclusive nonreferential relations and ratio --+1 in the case of exclusive coreferential relationships. In order for referential pairs occurring in the training corpus with ratio = 0 to be preferred to those without frequency information, we slightly decrease the ratio value of the latter ones by a factor 6. As mentioned above the distance plays a crucial role in our selection method, too. We define a preference value pref by normalizing the ratio value according to the distance dist given by the primary order of the candidates in the discourse. ratio pre f = dist The pref value is calculated for each candidate and the precedence ordered list of candidates is resorted towards the maximization of the preference factor. Similarly to the baseline test, the first element of the preferenced candidate list is chosen as the antecedent. The precedence order between candidates of the same confidence continues to remain so and thus a final decision is made in the case of a draw. The robustness of our approach is ensured by the definition of a backup strategy which ultimately selects one candidate occurring in the history in the case that all antecedent candidates are rejected by the decision tree filter. For our experiments reported in section 3 we adopted the selection of the dialoginitial candidate as the backup strategy. Evaluation For the evaluation of the experimental results described in this section we use F-measure metrics calculated by the recall and precision of the system performance. Let ~]t denote the total number of tagged 4In order to keep the formula simple the frequency types are omitted (cf. Table 1 ) anaphor-antecedent pairs contained in the test data, El the number of these pairs passing the decision tree filter, and ~ the number of correctly selected antecedents. During evaluation we distinguish three classes: whether the correct antecedent is the first element of the candidate list (f), is in the candidate list (i), or is filtered out by the decision tree (o). The metrics F, recall (R) and precision (P) are defined as follows: Z =lfl 2xPxR P+R F= E, p= ~\"]c s Ej t In order to prove the feasibility of our approach we compare the four preference selection methods listed in Figure 3 . First, the baseline test MRC selects the most recent candidate as the antecedent of an anaphoric expression. The necessity of the filter and preference selection components is shown by comparing the decision tree filter scheme DT (i.e., select the first element of the filtered candidate list) and preference scheme PREF (i.e., resort the complete candidate list) against our combined method DT+PREF (i.e., resort the filtered candidate list). tagged corpus~ 5-way cross-validation experiments are conducted for pronominal anaphora resolution. The selected antecedents are checked against the annotated correct antecedents according to their morphosyntactic and semantic attributes. The PREF method seems to reach a plateau at around 300 dialogs which is borne out by the closed test reaching a maximum of 81.1%. Comparing the recall rate of DT (61.2%) and DT+PREF (75.9%) with the PREF result, we might conclude that the decision tree is not much of a help due to the sideeffect of 11.8% of the correct antecedents being filtered out. Training However, in contrast to the PREF algorithm, the DT method improves continuously according to the training size implying a lack of training data for the identification of potential candidates. Despite the sparse data the filtering method proves to be very effective. The average number of all candidates (history) for a given anaphor in our open data is 39 candidates which is reduced to 11 potential candidates by the decision tree filter resulting in a reduction rate of 71.8% (closed test: 81%). The number of trivial selection cases (only one candidate) increases from 2.7% (history) to 11.4% (filter; closed test: 21%). On average, two candidates are skipped in the history to select the correct antecedent. Moreover, the precision rates of DT (69.4%) and DT+PREF (86.0%) show that the utilization of the decision tree filter in combination with the statistical preference selection gains a relative improvement of 9% towards the preference and 16% towards the filter method. Additionally, the system proves to be quite robust, because the decision tree filters out all candidates in only 1% of the open test samples. Selecting the candidate first introduced in the dialog as a backup strategy shows the best performance due to the frequent dialog initial references contained in our data. In our approach frequency ratio and distance information plays a crucial role not only for the identification of potential candidates during decision tree filtering, but also for the calculation of the preference value for each antecedent candidate. In the first case these features are used independently to characterize the training samples whereas the preference selection method is based on the dependency between the frequency and distance values of the given anaphor-candidate pair in the context of the respective discourse. The relative importance of each factor is shown in Table 2 . First, we compare our decision tree filter DT to those methods that do not use either frequency (DTno-freq) or distance (DT-no-dist) information. Frequency information does appear to be more relevant for the identification of potential candidates than distance features extracted from the training corpus. The recall performance of DT-no-freq decreases by 7.6% whereas DT-no-dist is only 1.1% below the result of the original DT filter 5. Moreover, the number of correct antecedents not passing the filter increases by 5.1% (DT-no-freq) and 0.7% (DT-no-dist). However, the distance factor proves to be quite important as a preference criterion. Relying only on the frequency ratio as the preference value, the recall performance of DT+PREF-no-dist is only 73.0%, down 2.9% of the original DT+PREF method. The effectiveness of our approach is not only based on the usage of single antecedent indicators extracted from the corpus, but also on the combination of these features for the selection of the most preferable candidate in the context of the given discourse. 4 Related Research Due to the characteristics of the underlying data used in these experiments a comparison involving absolute numbers to previous approaches gives us less evidence. However, the difficulty of our task can be verified according to the baseline experiment 5So far we have considered the decision tree filter just as a black-box tool. Further investigations on tree structures, however, should give us more evidence about the relative importance of the respective features. results reported in (Mitkov, 1998) . Resolving pronouns in English technical manuals to the most recent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the antecedent (cf. section 3). Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expensive in the cost of human effort at development time and limited ability to scale to new domains, more recent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge. Similarly to them we do not use any sentence parsing or structural analysis, but just rely on morphosyntactic and semantic word information. Moreover, clues are used about the grammatical and pragmatic functions of expressions as in (Grosz et al., 1995) , (Strube, 1998) , \"or (Azzam et al., 1998) as well as rule-based empirical approaches like (Nakaiwa and Shirai, 1996) or (Murata and Nagao, 1997) , to determine the most salient referent. These kinds of manually defined scoring heuristics, however, involve quite an amount of human intervention which is avoided in machine learning approaches. As briefly noted in section 1, the work described in (Conolly et al., 1994) and (Aone and Bennett, 1995) differs from our approach according to the usage of the decision tree in the resolution task. In (Conolly et al., 1994) a decision tree is trained on a small number of 15 features concerning anaphor type, grammatical function, recency, morphosyntactic agreement and subsuming concepts. Given two anaphor-candidate pairs the system judges which is \"better\". However, due to the lack of a strong assumption on \"transitivity\" this sorting algorithm may be unable to find the \"best\" solution. Based on discourse markers extracted from lexical, syntactic, and semantic processing, the approach of (Aone and Bennett, 1995) uses 66 unary and binary attributes (lexical, syntactic, semantic, position, matching category, topic) during decision tree training. The confidence values returned from the pruned decision tree are utilized as a saliency measure for each anaphor-candidate pair in order to se-lect a single antecedent. However, we use dependency factors for preference selection which cannot be learned automatically because of the independent learning of specific features during decision tree training. Therefore, our decision tree is not applied directly to the task of preference selection, but only used as a filter to reduce the number of potential candidates for preference selection. In addition to salience preference, a statistically modeled iexical preference is exploited in (Dagan et al., 1995) by comparing the conditional probabilities of co-occurrence patterns given the occurrence of candidates. Experiments, however, are carried out on computer manual texts with mainly intrasentential references. This kind of data is also characterized by the avoidance of disambiguities and only short discourse units, which prohibits almost any long-range references. In contrast to this research, our results show that the distance factor in addition to corpus-based frequency information is quite relevant for the selection of the most salient candidate in our task. Conclusion In this paper we proposed a corpus-based anaphora resolution method combining an automatic learning algorithm for coreferential relationships with statistical preference selection in the discourse context. We proved the applicability of our approach to pronoun resolution achieving a resolution accuracy of 86.0% (precision) and 75.9% (recall) for Japanese pronouns despite the limitation of sparse data. Improvements in these results can be expected by increasing the training data as well as utilizing more sophisticated linguistic knowledge (structural analysis of utterances, etc.) and discourse information (extra-sentential knowledge, etc.) which should lead to a rise of the decision tree filter performance. Preliminary experiments with nominal reference and ellipsis resolution showed promising results, too. We plan to incorporate this approach in multilingual machine translation which enables us to handle a variety of referential relations in order to improve the translation quality. Acknowledgement We would like to thank Hitoshi Nishimura (ATR) for his programming support and Hideki Tanaka (ATR) for helpful personal communications.",
    "abstract": "In this paper we propose a corpus-based approach to anaphora resolution combining a machine learning method and statistical information. First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the number of potential candidates. In the second step, preference selection is achieved by taking into account the frequency information of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current discourse. Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese",
        "English"
    ],
    "numcitedby": "12",
    "year": "1999",
    "month": "",
    "title": "Corpus-Based Anaphora Resolution Towards Antecedent Preference"
}