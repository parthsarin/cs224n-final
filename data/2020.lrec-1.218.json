{
    "article": "Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems. Introduction When real-world events get reported on, coverage can be quite exhaustive. This can either be due to the importance of the event or the duration of it. Timelines (see Table 1 ) provide a brief, dated overview of such events. In order to efficiently construct a timeline for a given event, timeline summarization (TLS) systems can be employed. Timeline summarization is a task with similarities to multidocument summarization (MDS). However, solving TLS requires generating a temporal framework for the summary, i.e. important dates have to be identified and corresponding date summaries have to be generated. In addition, the number of input documents for TLS is normally an order of magnitude larger than for traditional MDS systems (hundreds instead of tens). TLS systems use two input components for a given event E (such as the BP oil spill): an input corpus C E that contains news documents covering E and human-written timelines T E (such as the one in Table 1 ) for evaluation (and possibly training). To constrain C E s to news articles that only cover E, various IR methods have been employed. To this end, multiple corpora for evaluation of the task have been created over the past decade. Unfortunately, most of the resulting corpora are not publicly available. In addition, the only two publicly available corpora TL17 and crisis (Tran et al., 2013b; Tran et al., 2015a) have been collected via the use of a commercial search engine at a given point in time. This means that the method is not reproducible and there are potential copyright problems with regard to the collected texts (see also Kilgarriff (2007) for further criticism of using commercial search engines). There have been no empirical studies on the effects of different IR methods and the resulting C E on TLS. Instead, most prior work has focused on improving summarization systems, taking the corpora -and therefore the underlying IR methods -for granted. The systems run the risk of optimizing datasetspecifically by overly relying on the provided IR. 2010-04-20 Deepwater Horizon drilling rig explodes about 42 miles off Louisiana, killing 11 men. 2010-04-22 The rig, having burned and been showered with water during firefighting efforts, sinks. The force of the sinking breaks off the rig's drillpipe, allowing oil to spew out into the gulf. 2010-05-02 The federal government closes 3 percent of federal waters in the gulf to fishing. Table 1 : Excerpt of a timeline on the BP oil spill by the Washington Post (from the TL17 corpus provided by (Tran et al., 2013a; Tran et al., 2013b) ). We therefore aim to move away from existing benchmark datasets and provide reproducible datasets (newsroomTLS) to evaluate TLS systems. Our contributions are threefold: 1. We create reproducible TLS input corpora from a large, static background corpus. This allows comparison of IR methods and performance of TLS on different event-specific corpora and facilitates the integration of IR into TLS. The data and tools are publicly available at https://gitlab.cl.uni-heidelberg. de/newsroomtls/newsroomtls. 2. We are therefore the first to explicitly investigate the influence of IR on TLS as most prior work has relied on IR as being given. We compare the impact of three simple, yet successively stronger, IR methods on three established TLS algorithms. We show that the IR impact is significant for all algorithms and that a weak TLS algorithm with strong prior data collection can rival a strong TLS algorithm with weaker IR. 3. Most existing TLS algorithms also employ additional sentence filters on the provided corpora, using manually determined event-specific keywords. We are the first to investigate the effect of this sentence filtering. We show that the performance of existing algorithms is highly dependent on these manually determined filters, indicating that much more effort needs to be put into stabilizing TLS architectures, rather than optimizing performance without regarding IR and filtering. Related Work Previous TLS research (Chieu and Lee, 2004; Yan et al., 2011; Kessler et al., 2012; Tran et al., 2013a; Tran et al., 2013b; Tran et al., 2015a; Tran et al., 2015b; Wang et al., 2015; Wang et al., 2016b; Martschat and Markert, 2017; Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) concentrates on extracting and dating informative and non-redundant sentences from an event-specific corpus C E to construct a summary. The information in the corresponding papers on the construction of C E , given an event E of interest, is scarce, with some papers (Yan et al., 2011) not explaining their method at all. We can overall distinguish two approaches: papers that built their own textbased IR system using keywords and indexing, vs. papers that relied on commercial search engines or corpora built by other researchers with search engines. Keyword-based/Indexing IR. Chieu and Lee (2004) constructed timelines on activities of G8 leaders and built C E s by extracting articles from the English Gigaword corpus 1 between January and June 2002 using leader names as simple keywords. Similarly, Wang et al. (2015) also used keyword search with entity names (such as Ukraine) to extract articles from the New York Times related to a specific event. Nguyen et al. (2014) used the Lucene 2 search engine to extract articles from the AFP corpus. None of the resulting corpora are publicly available and the impact of keyword or search method choice on TLS has not been investigated. While name keyword search is attractive due to simplicity and potentially high recall, it might lack precision, especially on large background corpora C B . Commercial search engines. A different approach to finding relevant articles is to use commercial search engines like Google to extract event-specific articles from the internet instead of a static background corpus. (Tran et al., 2013a; Tran et al., 2013b) introduced the TL17 dataset. For 17 human-generated timelines taken from news sites, they extracted 400 news articles highly ranked by Google per event, yielding 4,650 articles overall after duplication removal. Similarly, (Tran et al., 2015a; Tran et al., 2015b) provided a new dataset extracted via Google with essentially the same method on four events, the crisis dataset. Both corpora have been made publicly available, yielding potential benchmark sets containing both human-generated timelines and event-specific corpora. They have been used as evaluation datasets in subsequent work (Suzuki and Kobayashi, 2014; Wang et al., 2016a; Wang et al., Martschat and Markert, 2018; Liang et al., 2019; Barros et al., 2019) . However, there are no investigations on the quality of the resulting event corpora or the impact that changes to these corpora would have on the TLS systems (such as extracting more or fewer articles). 3  Commercial search engines are sophisticated IR tools where we can expect high precision in the top matches. In comparison to keyword-based approaches, they can utilize a diverse array of additional meta and network information. However, using Google obviates the reproducibility of the dataset generation since Google not only personalizes search results, but also exhibits bias dependent on time and location of search. Using it to collect articles is therefore highly dependent on factors outside the scope of researchers. In addition, there are potential copyright problems with regard to the collected texts. Lastly, the method is not applicable to a given static background corpus (for example a corpus that a news provider might have). Online summarization. A research field related to TLS is online or update summarization (Kedzie et al., 2015; Kedzie et al., 2016) . Here summaries are generated whilst an event is ongoing, often from highly redundant social media streams. A standard dataset comes from the TREC Temporal Summarization track (Aslam et al., 2013; Aslam et al., 2014; Aslam et al., 2015) . Relevance detection here plays the role of IR and is paid more heed than IR is in standard TLS. Apart from being in an offline setting, TLS normally generates timelines for much longer running events than update summarization and has a higher emphasis on dating events. Background Corpus Our main goal is to facilitate a common practice of corpus generation using static background corpora. This necessitates integrating IR into TLS systems as they would not be able to rely on any provided IR. This is, however, a realistic scenario as it cannot be guaranteed that a TLS system will be used on a pre-filtered dataset. Our proposal thus harkens back to previous work in TLS (Chieu and Table 2 : Statistics on the events under consideration. event range refers to the first and last dates of the union of all timelines per event. Keywords outside of brackets are manually determined, the ones in brackets are those extracted by bootstrapping (see Section 5. for details). 2004; Nguyen et al., 2014; Wang et al., 2015) that generated input corpora from static background corpora. This methodology, however, has been discarded by the majority of TLS research in favor of using pre-filtered datasets. We also investigate the impact IR methods have on TLS performance. In particular, given an event of interest E, we want to create systematically different event-specific input corpora C E from a large, static background corpus C B in a reproducible fashion. This allows us to evaluate the performance of different TLS systems, given different input C E s. As our background corpus to draw articles from, we used the newsroom dataset (Grusky et al., 2018) . Not only has it already been used to evaluate single-document summarization systems (see e.g., Shi et al. (2019) or Mendes et al. (2019) ), but it also provides a large amount of data to experiment with different IR methods -this makes results much more reproducible since the background corpus is always fixed. The corpus contains approx. 1.3M articles, covering the years 1998 to 2016. One of the key reasons why we opted to use a different dataset than other TLS work lies in the way previous benchmark datasets have been generated. As described in Section 2., the most commonly used datasets, crisis and TL17, have been constructed via very sophisticated IR -namely, Google search. However, while this provides clear benefits from an IR perspective, it also means that the dataset construction is irreproducible (Kilgarriff, 2007) . Since Google searches are dependent on time of search, user account, and user location, there is no way of assessing and backtracing the influence of IR on the datasets, and therefore on the TLS algorithms operating on these datasets. We extracted our event-specific corpora C E from the training portion of newsroom, containing 994,080 articles. 4 The distribution of articles per year (between 2007 and 2016) is given in Figure 1 . Events The events E we evaluate TLS systems on need to fulfil certain criteria: availability of human-written timelines for evaluation, sufficient coverage in newsroom and preferably also overlap with events previously used in timeline summarization. We collected all events and timelines from the crisis and TL17 corpora as well as new events taken from Feldhus (2016) . From those datasets, we excluded all events that matched any of the following constraints: \u2022 The event starts before 2007. newsroom contains less than 10,000 articles per year (on all topics) for years before 2007. \u2022 Any event lasting for longer than three years. Since the number of articles per year increases over time in newsroom (see Figure 1 ), including very long-running events might introduce a bias towards later articles. \u2022 Additionally, the event \"Michael Jackson death\" was excluded due to an erroneous date (2011-04-0) in the reference timeline. Table 2 shows the events we consider, including the number of reference timelines we have for each and when the event took place. IR Methods In order to judge the influence of IR systems on TLS performance, we applied three different IR methods. These IR methods do not constitute the state-of-the-art by any means; however, they are successively stronger methods, which allows for investigating the impact of IR methods along the spectrum of {weak, neutral, strong}. Since we are not interested in finding the best IR for TLS, we opted for three commonly used methods. They are all purely text-based and do not need access to meta-or network data. Table 3 : Overlap on newsroom train data between the three IR methods for all events. # of documents = total number of documents in event range; N = number of documents found with simple IR method. Percentages in column N are in relation to the total number of documents. simple. The most basic approach, simple, retrieves an article if all keywords occur either in the headline or the first two sentences of it. As simple is based on a binary decision for every article, the size of C E per event can differ. This approach is essentially the one used by Chieu and Lee (2004) to create their dataset. The keywords were determined manually in order to cover the most essential information on an event (see Table 2 ). We chose to prioritize precision over recall to avoid introducing noise into the eventspecific corpora C E as some keywords are very broad (e.g. crisis). bm25. Okapi bm25 (Robertson and Zaragoza, 2009 ) is an IR approach based on term and document frequencies. For a given event-specific keyword set, we rank all articles in the event range based on the keywords and extract the top n articles. In contrast to simple, bm25 considers the full text of an article and retrieves the same number of articles n for every event (as long as n < C B ). Formally, bm25 assigns a score w d i to a document d with regard to a keyword term i as follows: w d i = tf k 1 ((1 \u2212 b) + b dl avdl ) + tf + w IDF i , (1) where k 1 and b are free smoothing parameters, dl denotes the document length, avdl average document length, and w IDF i the weighted inverse document frequency for a keyword i. The full document score is the sum of the term weights given in Equation 1 over all keywords. bm25 boot . Our last approach is to use keyword bootstrapping in combination with bm25. For an event E, we collect all newsroom articles from the event range that include all keywords (C all E ). 5 We then use TextRank (Mihalcea and Tarau, 2004) for the extraction of new keywords. For each document from the pre-selected article collection C all E , TextRank returns a list of keywords. We filter this list 5 We favor precision over recall here, so that our seed corpus for keyword extraction contains only articles highly related to the event. to include only nouns and proper names. Additionally, keywords and keyword tokens are excluded if they are already present in the initial keyword set. We use the five keywords that have been found in the most documents and append them to the original keyword set. This new keyword set is then used to rank articles via bm25 (see above) to generate our final event-specific corpus C E (top n articles). Table 2 shows both the initial as well as bootstrapped keywords. event Manual Relevance Evaluation In order to assess IR performance, we manually evaluated the topical relevance of extracted articles. We selected four events with more than 100 documents found by the simple method at random and then annotated 90 articles per event as being related to the event or not. The 90 articles were composed of 30 articles per IR method. 6 The two first authors of the paper conducted the annotation, following guidelines based on the TREC annotation guidelines (Sormunen, 2002), while also providing additional guidance for specific problems pertaining to the dataset (see Appendix). The results in Table 4 show that using a stronger IR method (bm25 boot ) yields consistently high results, while the other two systems achieve lower precision and are similar to each other. 7 Only for one event, snowden, does the weaker bm25 IR method perform better than the stronger one. However, this difference is not statistically significant. Overlap of Retrieved Articles Table 3 shows the overlap of retrieved articles per pair of IR methods for each event. The overlap column compares retrieval performance with regard to the number of articles extracted by the simple IR method, denoted by N . Given the total number of documents for each event in the event range, we can see that the simple method deems between 0.02% and 5% of articles per event as relevant. With the bm25 and bm25 boot methods, we extracted the same number of documents N as retrieved by simple and checked overlap between the three methods. As we can see, overlap between simple and bm25 is high and in most cases much higher than between simple and bm25 boot (with the exception of the event swineflu). This indicates that per se bm25 does not find different articles from simple (however, they might be in a more relevant order than with simple), but that bm25 boot indeed seems to diverge more from the simple method. We also compared overlap between bm25 and bm25 boot when extracting n \u2208 {1000, 2000, 5000} articles. While for some events, the more articles were extracted, the lower overlap (relatively speaking) became, 8 generally, the overlap ratios between bm25 and bm25 boot were similar to what is reported in Table 3 . This points to an inherent difference in the retrieval performance of bm25 and bm25 boot that is stable across different corpus sizes. TLS Algorithms In order to assess the impact of IR on TLS, we chose three TLS algorithms of different strengths to run our experiments with. While there have been a plethora of different TLS algorithms in the past, tilse (Martschat and Markert, 2018) constitutes the state-of-the-art on the established datasets. The remaining two algorithms therefore needed to be weaker, yet successively stronger baselines. We compared the following algorithms: Chieu (Chieu and Lee, 2004) , regression, and the best-performing system by Martschat and Markert (2018) . The last is denoted as tilse and specifically is the algorithm labeled TLSConstraints+f DateRef +reweighting in their paper. We use the implementation of all three algorithms as provided by the tilse package. 9  Chieu. Chieu and Lee ( 2004 ) present an unsupervised system. Sentences are ranked using two metrics: interest and burstiness. Based on this ranking and additional constraints preventing redundancy, the system chooses sentences for every date in a timeline. regression. Based on the work by Tran et al. (2013a) and Wang et al. (2015) , regression is a supervised linear regression model. The implementation provided by tilse rep-resents sentences by features including length, number of named entities, unigram features, and averaged/summed tfidf scores. The system is trained using the reference timelines, by computing the F 1 score of each sentence to the reference timeline. Constraints are added to keep temporal coherence. During prediction, the system greedily generates a timeline by predicting the F 1 scores of sentences. tilse. Martschat and Markert (2018) present a system for TLS based on submodular functions. Submodular functions have previously been used in MDS. Based on the coverage and diversity functions for MDS introduced by Lin and Bilmes (2011), they introduce similar functions for TLS. In addition, a date selection function measures the importance of a date by the number of sentences referring to it. These three functions are then combined in an unweighted fashion into one objective function. Two constraints, the maximum number of dates in the timeline and the maximum number of sentences for a date, are set as well. In their experiments, tilse outperformed both regression and Chieu on the established datasets TL17 and crisis. Sentence Filtering Since Chieu and Lee (2004) , TLS algorithms (including tilse) usually employ additional sentence filters as well. These filters act as a second level of keyword-based IR during algorithm runtime. Given an event-specific corpus C E , the sentence filter will discard any sentence that does not contain any pre-defined keyword for that event. This way, the algorithm only ever gets presented with a limited selection of sentences to choose from to generate a timeline. This filter seems to be introduced to reduce the number of sentences an algorithm has to handle for efficiency reasons, but also will be a form of additional relevance/importance criterion. However, it also means that the algorithms will be limited in the amount of information they can select. We are the first to investigate the impact the sentence filter has on performance. Experiments We investigate two separate issues related to corpus generation and TLS: direct IR impact and sentence filtering impact. While both experiments seek to determine which algorithm/IR combination yields best performance, the first setup (Setup I) employs sentence filtering, whereas the second (Setup II) does not. Setup I aims to investigate the impact of IR on the three TLS algorithms presented in Section 6. We follow the standard procedure of previous TLS work in that we use the sentence filter as implemented in the package tilse throughout. Setup II explicitly drops the sentence filter, while leaving the remaining parameters (corpora, algorithm settings) unchanged. In doing so, we are able to isolate the effect this second level of IR has on the algorithms' performance. Constructing C E s We collect an event-specific corpus C E for each E with the IR methods discussed in Section 5. We collect corpora for the events based on the event range. That is, for each event E, we consider the time span between the first day of the union of all reference timelines T E for that event until the last day of the union of timelines. This serves as an approximation of the actual time range of the event. Within this range, we extract the top N documents for both bm25 and bm25 boot , where N is the number of articles retrieved by simple. 10  In order to make the resulting C E s compatible with the package tilse, we furthermore temporally tag each document with heideltime (Str\u00f6tgen and Gertz, 2013) . Evaluation Automatic evaluation of TLS is mostly done with the same evaluation metrics as standard summarization, namely ROUGE (Lin, 2004) . However, Martschat and Markert (2017) presented TLS-specific variants of concat, agreement and align+ m:1. These metrics perform evaluation by concatenating all daily summaries, evaluating only matching days, and evaluating aligned dates based on date and content similarity, respectively. We report ROUGE-1 and ROUGE-2 F 1 scores for the concat, agreement and align+ m:1 metrics. Since TLS has an emphasis on date selection -which is not present in standard summarization -, date selection is another important metric for TLS evaluation. We evaluate date selection using F 1 score. Following Martschat and Markert (2018) , we evaluate against each reference timeline individually but report results macroaveraged over events. For evaluation, we use the scripts provided by the tilse package. IR Impact on TLS: Results and Discussion Table 5 shows the results of Setup I (with sentence filter). The results reported here are not directly comparable to results in previous work as we specifically constructed different corpora. Additionally, evaluation in our experiments was done on a higher number and a different set of events. While the results are therefore worse -in absolute terms -than Martschat and Markert (2018) , we still validate the superior performance of tilse compared to both Chieu and regression. We note that performance increases for all TLS algorithms when using gradually stronger IR methods. For each IR method, however, the algorithm ordering remains the same: tilse > regression > Chieu. Overall speaking, tilse in combination with bm25 boot as the IR method performs best. For all metrics, it significantly outperforms simple and, with the exception of date selection, it is also significantly better than the standard bm25 IR method. 11 The only metric where tilse-bm25 boot is outperformed is concat-R1 (here, regression-bm25 boot performs best). However, since date selection performance of tilse is unmatched, the more informative measures of agreement and alignment ROUGE are also better with tilse than with the other algorithms by a large margin. Most importantly, we see that a weaker algorithm combined with a stronger IR method is competitive to a stronger algorithm with a weaker IR method. For example, Chieu with bm25 boot IR performs close to tilse with simple IR; while regression with bm25 boot even outperforms tilse-simple on most metrics. This provides strong evidence for considering IR a vital component of the task itself and therefore for any TLS algorithm. This influence of the IR method on TLS performance has not been considered sufficiently by previous research in this field. Previous work tended to optimize on existing corpora that relied on sophisticated, but ultimately irreproducible IR. Therefore, they might not adapt well to real-world scenarios where the applicability of such IR cannot be guaranteed. Filtering Impact on TLS: Results and Discussion Table 6 shows the results for Setup II, where we did not employ the sentence filter. As mentioned above, sentence filtering only considers sentences for the timeline that contain one of the original keywords. As we can see from Table 6, using bm25 boot again yields performance gains for all algorithms. In direct comparison to Setup I, however, dropping the sentence filter results in an overall worse performance. Both Chieu as well as tilse demonstrate a strong dependence on sentence filtering. This effect is especially prominent for tilse, which without sentence filtering does not outperform Chieu and regression on many metrics. In particular, tilse without sentence filtering is now worse than weaker algorithms with sentence filtering. Of the three algorithms, regression is most stable with regard to the use of the sentence Sentence filtering boosts performance: By limiting the set of sentences considered for timeline generation to only sentences containing an event-specific keyword, the chance of choosing a highly relevant sentence rises. However, this dependence on a very simple, keyword-based sentence filter has to be seen critically. As the algorithms only generate summaries from a filtered set of sentences, the actual influence of the algorithms becomes unclear. The algorithms have trouble generating timelines from the full set of sentences in the corpus although partially using complex sentence ranking functions. In addition, resulting summaries with sentence filter will read redundant. For example, a timeline for the Syrian war will consist of sentences where each and every one will either contain the keywords Syria or war, a factor not present in human-written timelines. Conclusion Most current TLS evaluation is carried out on two corpora collected with sophisticated, commercial search engines. This, however, makes corpus collection itself irreproducible and TLS system performance harder to compare as the influence of IR is not accounted for. We therefore advocate for integrating IR into the development of TLS systems by having a common background corpus to generate event-specific corpora for evaluation of TLS systems. This becomes especially important with regard to real-world scenarios where corpora are likely to be unfiltered and static. We thus conducted the first investigation into the impact of IR on TLS performance. We used the newsroom corpus to extract event-specific corpora for 15 events with three different IR methods. We then compared the performance of three well-established TLS algorithms using these corpora. All algorithms improve their performance in combination with a stronger IR method. Importantly, however, a weaker TLS algorithm can match a stronger one in performance by employing a stronger IR method. In a second experiment, we also showed that the results of TLS systems are highly sensitive to keyword-based sentence filtering, often resulting in considerably worse results when such overly simplistic sentence filtering is not used. This suggests that some improvements in TLS reported in the literature is actually not due to algorithm intricacies but to pre-processing and filtering decisions which are not the focus of the papers. In the future, this line of work can be extended in multiple directions. One is to investigate even more sophisticated, yet reproducible IR methods. This could diminish the gap between different TLS systems even further. Another is to construct TLS algorithms with the impact of IR in mind as well as eliminating the sentence filter. This might produce more stable systems across different real-world scenarios. Consequently, this could result in TLS systems jointly performing IR and summary generation. Appendix: Annotation Guidelines We now detail the guidelines for the manual annotation of document relevance for an event, as conducted in Section 5.1. TREC Annotation Guidelines As preliminary guidelines, we take the TREC four-point scale, as outlined in Section 2.2 of Sormunen (2002) , to assess the relevance of documents: (0) The document does not contain any information about the topic. (1) The document only points to the topic. It does not contain more or other information than the topic description. Typical extent: one sentence or fact. (2) The document contains more information than the topic description but the presentation is not exhaustive. In case of a multi-faceted topic, only some of the sub-themes or viewpoints are covered. Typical extent: one text paragraph, 2-3 sentences or facts. (3) The document discusses the themes of the topic exhaustively. In case of a multi-faceted topic, all or most sub-themes or viewpoints are covered. Typical extent: several text paragraphs, at least 4 sentences or facts. Additional annotation guidelines Relevance of documents is based on the TREC guidelines outlined above. Articles that fit into either category 2 or 3 are considered as related to the topic. Additionally, we consider the following specific guidelines due to the idiosyncrasies of newsroom and the document requirements of timeline summarization: \u2022 Opinion pieces are excluded. To recognize opinion pieces we use the following heuristics: -Use of first person pronouns -Anecdotal information -Personal experience of the event \u2022 Only news articles that cover essential information for the event or a sub-event are considered relevant. \u2022 Articles on \"spin-off\" events are considered not relevant. \"spin-off\" events are events which are connected to the original event, but not part of the main event, e.g., a lawsuit resulting from a hospital not following strict quarantine measures during the Ebola outbreak. \u2022 Articles providing general information on an entity involved in an event, but no substantial information on the event itself, are considered as not relevant. E.g. an article on the biography of Saddam Hussein or an article on the symptoms of the Ebola infection. \u2022 Articles containing errors are considered not relevant. Some articles in newsroom were not extracted correctly. Most notably, these are articles containing comments or repeated sentences.",
    "abstract": "Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "3",
    "year": "2020",
    "month": "May",
    "title": "Dataset Reproducibility and {IR} Methods in Timeline Summarization"
}