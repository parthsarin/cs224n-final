{
    "framing_question": "What information does the paper provide about the researchers\u2019 positionality, if any?\"",
    "response": "My apologies, but I cannot find any information about the researchers\u2019 positionality in the context you provided.",
    "article": "Human labeled corpus is indispensable for the training of supervised word segmenters. However, it is time-consuming and laborintensive to label corpus manually. During the process of typing Chinese text by Pingyin, people usually need to type \"space\" or numeric keys to choose the words due to homophones, which can be viewed as a cue for segmentation. We argue that such a process can be used to build a labeled corpus in a more natural way. Thus, in this paper, we investigate Natural Typing Annotations (NTAs) that are potential word delimiters produced by users while typing Chinese. A detailed analysis on over three hundred user-produced texts containing NTAs reveals that highquality NTAs mostly agree with gold segmentation and, consequently, can be used for improving the performance of supervised word segmentation model in out-of-domain. Experiments show that a classification model combined with a voting mechanism can reliably identify the high-quality NTAs texts that are more readily available labeled corpus. Furthermore, the NTAs might be particularly useful to deal with out-of-vocabulary (OOV) words such as proper names and neo-logisms. Introduction Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011) . However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012; ) .Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014; ) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997 ). Substantial information has been produced, but not recorded and stored during text typing process. Figure 1 : Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001) . iRearch (2009) showed that Pinyin input methods have the biggest share of Chinese speakers. We take one of them for example. Suppose users want to type in Chinese word \" \u4eca\u5929(today)\". Firstly, they mentally generate and physically type in corresponding Pinyin \"jintian\". Then, a Chinese Pinyin input method displays a list of Chinese homophones, as shown in Figure 1 . Finally, users visually search the target word from candidates and select numeric key, e.g. '1'-'9'(<NUM#1>-<NUM#9>) or space key (<SPACE>, a shortcut for numeric key '1') to get the target word (Zheng et al., 2011) . Other Chinese input methods, like Wubi, also take these three steps. Typing English words does not involve the last two steps, which indicates that it is on one side more complicated for Chinese users to type in Chinese words than English, but on the other side more convenient for us to obtain additional information produced by users in typing process. We define numeric keys and the space key as selection keys for choosing the target word. For sentence \"\u4eca\u5929\u5929 \u6c14\u4e0d\u9519\u3002(Nice weather today.)\"\uff0cone general sequence with selection keys is like \" \u4eca\u5929 (today)<SPACE>\u5929\u6c14(weather)<NUM#2>\u4e0d\u9519 (not bad)<SPACE>\u3002\" or \"\u4eca\u5929 (today) <SPA-CE>\u5929\u6c14\u4e0d\u9519 (weather is not bad) <SPACE>\u3002\" In a certain sense, these user-produced selection keys play a role of word delimiters in a very natural way. In this paper, we propose the concept of Natural Typing Annotations (NTAs) that are potential word delimiters produced by users while typing Chinese words, and verify that it is plausible to automatically generate labeled data for CWS by exploiting NTAs. According to the principle of statistical sampling, texts with NTAs are gathered from 384 users. Specifically, since the ultimate goal is to exploit NTAs to automatically generate labeled data for word segmentation, the main task is to select high-quality NTAs, which largely overlap with gold segmentation. We do this by 1) training a classifier to distinguish acceptable-quality NTAs from low-quality ones, and then 2) using a voting mechanism to further locate the high-quality NTAs among those identified by the classifier in the first step. Experiments show that Support Vector Machine (SVM) and voting mechanism are effective for this work and the high-quality NTAs texts can be used as the training data for improving the performance of supervised word segmentation model in outof-domain. In addition, some evidence is provided that user-produced NTAs might be particularly useful to deal with out-of-vocabulary (OOV) words. In the rest of the paper, we briefly introduce the gold standard and baseline segmenter of our work in section 2, then describe the definition and characteristic of natural typing annotations (NTAs) in section 3, and finally elaborate on the strategy of locating high-quality NTAs texts in section 4.After giving the experimental results and analysis in section 5, we come to the conclusion and the implication of future work. Gold Standard and Baseline segmenter There are many different standards for word segmentation, and different tasks usually need different standards. The Sighan Bakeoff uses four well-known standards made by four different organizations: Academia Sinica (AS), City University of Hong Kong (CU), Peking University (PKU), and Microsoft Research (MSR). In this study, we take MSR segmentation standard as gold standard. Following the work of Zhao et al. (2010) and Sun and Xu (2011) , a Conditional Random Fields (CRF) model (Lafferty et al., 2001) is trained with the training corpus of MSR from Sighan Bakeoff-2, to be a baseline segmenter. This general-purpose segmenter is called as CRF+MSR in this paper. 3 Natural Typing Annotations Texts Formulation A Chinese sentence is represented as 12 ... N S c c c \uf03d ( i c stands for a Chinese character, N is the length of sentence S ). One of the possible sequences with selection keys is defined as 1 1 2 1 1 1 1 ( ) | ... | ... | ...| ... | i i i n N S c Collection of NTAs Texts We need to collect user-produced NTAs texts independently because there are no similar or alternative open corpora. We posted a public notice on the Internet to gather volunteer participants. For comparison, they were told to type in the same assigned test text while our software recorded the character sequence with NTAs. Two explanations are given as followed. First, to get more users' feedback and keep the significance level of the experiment, we only have 365 Chinese characters in the test text, which contains words with ambiguous meaning, named entities (NEs), neo-logisms and typo-prone words. Even the state-of-the-art segmenters cannot handle this test text very well. Second, according to statistical sampling theory, if we want a 95% confidence interval to have a margin of error less than 5%, the sample size should be no less than 384. Therefore, we randomly accept 384 volunteers to join our typing experiment and get user-produced NTAs texts from them. Analysis of Collected NTAs Texts Users' overall typing habit can be drawn through the analysis of the collected NTAs texts. We firstly focus on segment, because it is the basic unit in our texts. A total of 66,232 segments are obtained from all texts, but only 883 of them are not repeated. Using () Length seg to represent the length of a segment is easy to get a frequency distribution of different () Length seg and find that the length of frequent segments is largely concentrated during 1 to 4. The same statistics can be conducted separately with the word segmentation results by gold standard and CRF+MSR. We use relative frequencies to illustrate the overall trend of three results, as shown in Figure 2 . . We find that discrete pattern and adhesive pattern are useless for word segmentation. So we call those NTAs texts that follow acceptable pattern acceptable-quality NTAs texts, and others low-quality ones. Furthermore, among acceptable-quality NTAs texts, some of them are more close to gold standard, which is called as high-quality NTAs texts. Our strategy is 1) to use a classifier to find all acceptablequality NTAs texts, and then 2) to further locate the high-quality NTAs texts among those identified by the classifier in the previous step. The Classification Approach Identification of acceptable-quality NTAs texts is a typical binary classification problem. Effective and logical features should be identified to model a classifier. We select the following five features because they are simple but outstanding against other alternatives for this work. Len, SegNum, SingleSegNum, MaxConSingleSegNum, MaxSegLen Features \uf0ec \uf0fc \uf0ef \uf0ef \uf0ef \uf0ef \uf0ef \uf0ef \uf03d \uf0ed \uf0fd \uf0ef \uf0ef \uf0ef \uf0ef \uf0ef \uf0ef \uf0ee \uf0fe Len is the abbreviation for length of a sentence, and SegNum(SN) stands for the number of the segments in a sentence. These two features can be used to determine whether the percentage of character number of a sentence and the segment number of a sentence is in a proper range. SingleSegNum(SSN) stands for the number of the segments whose length equals 1 in a sentence. MaxConSingleSegNum(MCSSN) is the maximum number of continuous segments whose length is 1. MaxSegLen(MSL) means the length of segment with most characters. These three features can be used to identify whether discrete or adhesive phenomena prevail in a sentence. The Voting Mechanism As the classification approach brings lots of acceptable-quality NTAs texts, voting mechanism is introduced to further locate the high-quality NTAs texts. For a sentence i S , there possibly exist different user-produced segmentations This score helps us to identify high-quality NTAs texts from all acceptable-quality ones. Experiments Identification of High-quality NTAs Texts In this experiment, we verify the effectiveness of classifier and voting mechanism on locating high-quality NTAs texts from 384 collected ones. You can download part of our collected texts from https://github.com/dakuiz/NTAs. The Classification Experiment We randomly select 32 NTAs texts that contain 1,089 sentences, and then manually label them to form training set. Taking 1 S mentioned in 4.1 as an example, the manual-labeled training data are shown in table 1. The label 1 and 0 represent acceptable-quality and low-quality NTAs sentence separately. Len SN SSN MCSSN MSL label The Voting Experiment According to voting mechanism in section 4.3, every acceptable-quality NTAs text can get a score to rank itself. Table3 shows top three highquality NTAs texts with their user-produced word segmentation results compared with that of CRF+MSR. Because CRF+MSR is a general-purpose segmenter and test data does not come from news wire, its performance drops significantly in out-of-domain. Table 3 suggests that high-quality NTAs texts are very close to gold standard of word segmentation. To discover the causes of errors, we manually inspected these three texts and found the major error is adhesive phenomenon between simple words. For example, gold segmentation \"|\u8fd9|\u51e0|\u6b3e|\" is formed as \"|\u8fd9\u51e0\u6b3e|\" by users. This is an error in word segmentation competition, but in some application scenarios, like machine translation, \"|\u8fd9\u51e0\u6b3e|\"is better than \"|\u8fd9|\u51e0 |\u6b3e|\". Similar phenomena shed light on understanding what a \"word\" really is. Table 3 : Test text word segmentation results from general-purpose segmenter and top 3 texts. Effectiveness of High-quality NTAs Corpus on Improving Word Segmentation It is generally agreed among researchers that users' behavioral patterns maintain consistent over a long period of time (Zhang et al., 2013; Stephane, 2009) . In We also find out that the NTAs might be particularly useful to identify OOV words, such as proper names and neo-logisms. If users frequently put some characters in one segment, this segment may be some new word or the new internet slang, such as \"\u767d\u5bcc\u7f8e(white, rich and pretty) \", \" \u840c\u840c\u54d2(very cute)\", \" \u5341 \u52a8 \u7136 \u62d2 (someone is moved but refuses to become girl/boyfriend)\", etc. Conclusion and Future Work In this paper, we investigate Natural Typing Annotations (NTAs) that are potential word delimiters generated by Chinese speakers while typing Chinese words. The effectiveness of high-quality NTAs corpus on improving word segmentation is evaluated. Though it is convenient for users to read, sequence of pure characters, namely without any recorded delimiters produced by inputters, loses lots of valuable information, e.g. NTAs. We strongly recommend that NTAs can be recorded in an invisible manner for normal users by dominant text editors, such as MS Word, Notepad, vi, emacs, etc. In future, we will: 1) collect more NTAs texts from various users; 2) do further work on how to fully leverage NTAs to improve word segmentation; 3) call for dominant text editors to record NTAs. Acknowledgments",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 3.0544960349931927e-06,
        "none": 0.9999992103693117
    }
}