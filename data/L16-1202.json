{
    "article": "This paper describes corpora collection activity for building large machine translation systems for Latvian e-Government platform. We describe requirements for corpora, selection and assessment of data sources, collection of the public corpora and creation of new corpora from miscellaneous sources. Methodology, tools and assessment methods are also presented along with the results achieved, challenges faced and conclusions made. Several approaches to address the data scarceness are discussed. We summarize the volume of obtained corpora and provide quality metrics of MT systems trained on this data. Resulting MT systems for English-Latvian, Latvian-English and Latvian-Russian are integrated in the Latvian e-service portal and are freely available on website HUGO.LV. This paper can serve as a guidance for similar activities initiated in other countries, particularly in the context of European Language Resource Coordination action. Project Background and Goal This paper describes the work on language resource collection for the Latvian e-Government Machine Translation (MT) Platform providing multilingual access to the portal of public online services. To foster broader use of public online services, Latvian Government has created a centralized portal Latvija.lv 1 . The aim of the portal is to ensure quick and convenient access to the services provided by the Latvian State institutions and municipalities. The portal provides guidance on requirements (forms, documents, payments, terms etc.) and administrative procedures in order to receive public and municipal services, as well as direct access to those services that are offered online. Currently this portal hosts 108 e-services. Considering the nationalities in Latvia, it is important to make this content accessible not only in Latvian (the official state language), but also in Russian (1/3 of population are Russians, Byelorussians, Ukrainians) and English (for most foreigners). Considering dynamically changing content of online services and high costs of human translation, making use of Machine Translation (MT) was adopted as the only viable solution to provide multilingual access. Latvian Culture Information Systems Centre 2 launched a project to build Machine Translation Platform for e-Government with specific MT systems tuned for state administration domain. The resulting platform is now branded as HUGO.LV. The goal in the long run is to provide MT services not only to portal Latvija.lv but to all Latvian State institutions and keep developing and integrating machine translation in all related public e-services. Language technology company Tilde 3 was chosen as a technology partner in this project, and was commissioned to perform language resource collection and processing, MT systems building and technology delivery. 1 http://www.latvija.lv/ 2 http://www.kis.gov.lv/ The goal in the project was to create a large corpora of language resources and build the best possible MT systems for 3 language pairs -English-Latvian, Latvian-English and Latvian-Russian. Two systems had to be provided for every translation pair -generic MT system and system tuned to the specific of the public administration. Corpora Composition Requirements Latvian e-Government MT Platform (Vasi\u013cjevs et al., 2014) is built by Tilde using LetsMT technologies (Vasi\u013cjevs et al., 2011; 2012) which are based on the Moses toolkit (Koehn et al., 2007) . LetsMT includes facilities to process parallel and monolingual corpora and build translation and language models for phrase-based statistical machine translation. From the perspective of language resources, it requires collecting and processing general and domain specific parallel and monolingual data to create MT systems which are customized for particular application area. To ensure optimal quality of resulting MT systems, the project requirements set 5 million sentences as the minimal amount of parallel data in general domain and 2 million sentences as the minimum for the public administration domain. These are significant amounts taking into account that Latvian language is weakly supported by language resources according to the META-NET White Papers (Rehm & Uszkoreit, 2012). For domain adaptation, in addition to domain specific parallel and monolingual texts Tilde technology allows imposing predefined terminology on the given MT system (Pinnis, 2015) . To benefit from this feature, terminology data needed to be specified, collected and attached to the MT system. Types and sources of corpora To collect required language resources, we identified several sources which we present in this section grouped by their type. Public corpora Several public corpora were identified and used as a source of parallel data: Crawling Web sources Parallel data for the Latvian language is scarce, and effort must be put to find different possible sources, assess them and collect the data. When identifying the sources, attention must be paid how parallel is the data -e.g., seemingly identical web news published on multilingual websites often are not one-to-one translations but are adapted for the target language communities. Although such comparable multilingual data could also be useful to extract data for statistical MT (Skadi\u0146a et al., 2012) , it requires significant additional efforts and validation, this is why we discarded sources that are not fully parallel. We identified and collected the following useful data. We tried and examined different data collection techniques, too. \uf0b7 Parallel in-domain content from public institution websites collected manually and aligned with the Microsoft aligner (see section 2.4 for details). 1200 Latvian, 700 English and 1100 Russian web pages yielded 8493 English-Latvian and 13373 Latvian-Russian segments. \uf0b7 Parallel in-domain content from translations of international documents that are not already included in the existing public corpora. We identified 2500 Latvian language documents on the website of Latvian legislation matching the categories \"convention, declaration, international document, international agreement, international law\", and we sought the Web for the matching counterpart in English and Russian. We Nomenclature, and added them to the parallel corpus, obtaining 21 100 parallel English-Latvian segments. We explored numerous other websites which contain multilingual content which may be relevant for the project purpose, but we had to discard them due to complexity of data extraction. Specific structure of these sites would take too much manual work to find, assess and extract pieces of parallel content, which would not be justified by the possible gains. For efficiency reasons we mostly focused on plain text / html sources; limiting processing of other formats like PDF only for most valuable data. Parallel texts from publishers During the project, we identified some publishers who produce and publish parallel content on regular basis. We got a permission from the publisher of magazine which has identical versions in Latvian and Russian. Another publisher performs translation of Latvian legislation into Russian to make it better understandable for the Russianspeaking entrepreneurs. We managed to reach special licensing agreements with these publishers enabling us to process their data and create an in-domain parallel Latvian-Russian corpus of over 450 000 parallel segments. Data from public administrations Several public institutions provided their taxonomies and collection of documents for customization of MT. We had to admit that it was less than 1% of the total size of corpora that was collected for MT training. The major obstacle is a lack of data management practices in public institutions that would make it easy to select and submit shareable data. A good-will of public institutions to support the project was outweighed by the need to spend additional efforts to prepare the data resulting in a very few resources. The best data obtained from the public administrations: fcontained 970 000 monolingual sentences \u2022 multilingual website of state e-services 13 provided data for 640 000 parallel segments. Data processing workflows The typical workflow that we applied in the project consists of multiple steps. Data processing tools Each of the corpora type and particular corpus was processed individually in order to be prepared for the use (Moore, 2002) to align all kinds of parallel texts from aligned files. We tried also Hunalign (Varga et al., 2005) , and Vanilla (Gale & Church, 1993) . The alignment of Microsoft's Bilingual sentence aligner led to the most accurate results (Skadi\u0146\u0161 et al., 2014) . \uf0b7 Custom PERL script tools were developed to convert between encodings, to convert between formats, to merge hyphenated words, to filter data and integrate tools into workflows. HUGO.LV toolkit also includes tools to build data subsets for human evaluation of QA of the new corpora we add to the repository and use in training of new MT systems. https://ec.europa.eu/digital-single-market/en/europeanlegislation-reuse-public-sector-information and reuse without explicit permission from the data owners. Publishers and data owners are protecting and securing their intellectual assets, however after close consideration they are sometimes positive to contribute for a very specific use in MT. To use such texts with a permission from the data owners while still protecting their intellectual property, other type of data can be generated and shared instead of the original text such as n-grams, shuffled extracts, phrase tables or binary models. To foster development of machine translation and other data-driven language technologies, it would be necessary to modernize European Union copyright legislation to open copyrighted data for use in research and development that does not infringe the normal exploitation of the copyrighted work, such as creation of statistical models and machine learning. Challenges Addressing resource scarceness challenge Terminology data Terminologies and taxonomies. If MT system is trained on a very large parallel corpus, it can \"learn\" how to translate terms from the term occurrences in the data. Since the parallel corpora is scarce for Latvian language, we must apply other approach to ensure proper translation of in-domain terms. MT systems in this project were enhanced by using dynamic terminology and named entity integration in statistical machine translation (Pinnis, 2015) . We collected in-domain terminology and taxonomies and added to the MT system. The following types of named entities were prepared: \uf0b7 Names of state institutions and their translations; \uf0b7 Names of professions and their translations; \uf0b7 Street and place names and their transcriptions / translations; \uf0b7 Popular person names and surnames and their transcriptions; \uf0b7 Geographical names -cities, states, villages etc. and their matching counterparts. These lists underwent special filtering to ensure that these named entities do not conflict with common names. We ended with 9300 entries for English-Latvian and 8200 entries for Latvian-Russian. Resources from IATE 18 (Inter-Active Terminology for Europe) database. IATE data was made public during the project and we considered using its terminology data as either additional MT training data or as a source for terminology to be used in dynamic integration. Closer analysis showed that: \uf0b7 A term entry in one language may be matched with abbreviation instead of full term in another language; \uf0b7 Definitions of terms in different languages are not always direct translations; \uf0b7 Abbreviations and full names are used inconsistently. We concluded that this data is not suitable as parallel content \"as is\". IATE can serve as a helpful reference to translators, but we could not include this data as parallel data source in this project. Elaborated content cleaning / filtering techniques must be applied to IATE data to select the parallel terms or definitions to be used as MT training data, but this was beyond the scope of this project. Using MT to produce additional parallel data In order to deal with data scarceness for Latvian-Russian language pair, we experimented with building a Latvian-Russian parallel corpus using a pivot language. We added such automatically generated corpus to the MT training data and evaluated quality improvements of resulting MT system to determine whether such approach could be used in production. We performed an experiment using a large English-Russian parallel corpus -MultiUN (Eisele & Chen, 2010) with 9.4M segments of translated UN documents. We translated this corpus from English to Latvian, and aligned Russian part of the segment with the machine-translated Latvian part. We translated this corpus from English to Latvian using Tilde MT engine which outperforms in quality other English-Latvian MT systems (Skadi\u0146\u0161 et al., 2014) . One of the challenges to perform the test was to get the this large corpus translated in a reasonable time. Assuming 1 sec/segment translation speed, it requires 4 MT in multiple queues was used, and the total time was reduced to less than 1 calendar month. To decide whether the obtained data are good and shall be used in a production system, we built two MT systems for comparison. One was a baseline system with 3.8M parallel segments of quality training data. Another system was the experimental system made from the baseline clone with the Latvian-Russian data added. We used the same training settings and tuning and evaluation data sets. The results obtained are presented in Table 1 . We were anticipating quality improvement to consider including the MT-generated parallel corpus in production systems. With 1-point BLEU drop it means this approach is not feasible yet, so we decided not to use this method in building production MT system in this project. MT System Results Achieved Corpora size obtained At the end of the project we collected a significant amount of new MT training data (See Human evaluation of corpora Before putting to use, we evaluated each newly created parallel corpora. Some of the corpora may be smallcontain 5 to 10 thousand segments, other bigger corpora may contain 100 to 500 thousand segments. We applied a human evaluation method of evaluating a subset of the corpus of 50..200 randomly selected segments to represent the entire corpus. The annotation was very basic -Good/Still Acceptable/Bad. \uf0b7 Good means that source and target language sentences are parallel. \uf0b7 Still acceptable (still good) means one or two typos or minor errors beyond the alignment process; style aspects attributable to the translator preferences. \uf0b7 Bad means content in the supposedly parallel sentences has major differences; or if two or more words are split (possibly due to hyphenation or extraction from PDF), or have incorrect characters in them (due to OCR or PDF, or encoding issues). After annotation we check the percentage score -a simple formula of dividing the number of good segments by the number of total segments. We have set a quality threshold of over 90% good segments to consider a corpus to be good for use in MT system training. If the quality was below this threshold, we checked the process of building the corpus -its source files, the segment alignment process. If problems cannot be fixed, the lower quality data is rejected. Although our previous research (Skadi\u0146\u0161 et al., 2014) shows that even data with much lower quality level can lead to improvement in BLEU score, in this case we set high corpus quality criteria to avoid random erroneous words in the MT output. Evaluation of MT systems The collected corpus was used to build both general domain and state administration domain MT systems. We evaluated MT systems using BLEU score metric (Papineni et al., 2002) and compared results to Google Translate (See Table 3 ). General domain MT systems were trained using all collected data, and state administration domain MT systems were trained using two language models -indomain language model, and general domain language model. We used domain adaptation methods suggested in earlier research by Koehn and Schroeder (2007) and Lewis et al. (2010) . Both language models have different weights determined with system tuning by MERT (Och, 2003) using in-domain tuning corpus. We compared general domain systems to Google Translate -for all three systems our results were significantly better. Careful collection and procession of training data have made a major contribution to these results. System Conclusions The project successfully achieved its goal to collect the maximum of data useful for training MT systems adopted to the needs of e-Government. Data was collected from various sources that are described in this paper. Another positive effect of the project is a growing awareness in the public sector institutions about the importance of their textual data for language technology development. State institutions are encouraged to take care of their translation data -to require translation memories to be returned as part of each translation contract, to collect, anonymise, reuse and publish the translation data. This process will gradually lead to more parallel data available for research and practical developments. From Research to Real Users. (pp. 135-144) . London, UK: Springer-Verlag. Rehm, G., Uszkoreit, H. (eds). (2012) . META-NET White Paper Series: Europe's Languages in the Digital Age. Springer, Heidelberg. Och, F. J. (2003) . Minimum error rate training in statistical machine translation. In ACL 2003: Proceedings of the 41st Meeting of the Association for Computational . Papineni, K., Roukos, S., Ward, T., Zhu, W. (2002) . BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics.: ACL Pinnis, M., Ion, R., \u015etef\u0103nescu, D., Su, F., Skadi\u0146a, I., Vasi\u013cjevs, A., Babych, B. (2012) . ACCURAT",
    "abstract": "This paper describes corpora collection activity for building large machine translation systems for Latvian e-Government platform. We describe requirements for corpora, selection and assessment of data sources, collection of the public corpora and creation of new corpora from miscellaneous sources. Methodology, tools and assessment methods are also presented along with the results achieved, challenges faced and conclusions made. Several approaches to address the data scarceness are discussed. We summarize the volume of obtained corpora and provide quality metrics of MT systems trained on this data. Resulting MT systems for English-Latvian, Latvian-English and Latvian-Russian are integrated in the Latvian e-service portal and are freely available on website HUGO.LV. This paper can serve as a guidance for similar activities initiated in other countries, particularly in the context of European Language Resource Coordination action.",
    "countries": [
        "Latvia"
    ],
    "languages": [
        "Russian",
        "English",
        "Latvian"
    ],
    "numcitedby": "0",
    "year": "2016",
    "month": "May",
    "title": "Collecting Language Resources for the {L}atvian e-Government Machine Translation Platform"
}