{
    "article": "We present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree. This representation allows us to achieve significantly higher accuracy in the task of HPSG parse selection than standard models, and makes the application of string kernels natural. We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation. We apply SVM ranking models and achieve an exact sentence accuracy of 85.40% on the Redwoods corpus. Introduction In this work we are concerned with building statistical models for parse disambiguation -choosing a correct analysis out of the possible analyses for a sentence. Many machine learning algorithms for classification and ranking require data to be represented as real-valued vectors of fixed dimensionality. Natural language parse trees are not readily representable in this form, and the choice of representation is extremely important for the success of machine learning algorithms. For a large class of machine learning algorithms, such an explicit representation is not necessary, and it suffices to devise a kernel function \u00a2\u00a1 \u00a4\u00a3 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 which measures the similarity between inputs \u00a3 and \u00a7 . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003) , similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997) . Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000) . Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001) , the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003) . Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002) . Here we propose a new representation of parse trees which (i) allows the localization of broader useful context, (ii) paves the way for exploring kernels, and (iii) achieves superior disambiguation accuracy compared to models that use tree representations centered around context-free rules. Compared to the usual notion of discriminative models (placing classes on rich observed data) discriminative PCFG parsing with plain context free rule features may look naive, since most of the features (in a particular tree) make no reference to observed input at all. The standard way to address this problem is through lexicalization, which puts an element of the input on each tree node, so all features do refer to the input. This paper explores an alternative way of achieving this that gives a broader view of tree contexts, extends naturally to exploring kernels, and performs better. We represent parse trees as lists of paths (leaf projection paths) from words to the top level of the tree, which includes both the head-path (where the word is a syntactic head) and the non-head path. This allows us to capture for example cases of non-head dependencies which were also discussed by (Bod, 1998) and were used to motivate large subtree features, such as \"more careful than his sister\" where \"careful\" is analyzed as head of the adjective phrase, but \"more\" licenses the \"than\" comparative clause. This representation of trees as lists of projection paths (strings) allows us to explore string kernels on these paths and combine them into tree kernels. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al., 2002) . HPSG is a modern constraint-based lexicalist (or \"unification\") grammar formalism. 1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999) . We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. The Leaf Projection Paths View of Parse Trees Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redwoods corpus (Toutanova et al., 2002; Toutanova and Manning, 2002) , we use the derivation trees as the main representation for disambiguation. Derivation trees record the combining rule schemas of the HPSG grammar which were used to license the sign by combining initial lexical types. The derivation tree is also the fundamental data stored in the Redwoods treebank, since the full sign can be reconstructed from it by reference to the grammar. The internal nodes represent, for example, head-complement, head-specifier, and head-adjunct schemas, which were used to license larger signs out of component parts. A derivation tree for the 1 For an introduction to HPSG, see (Pollard and Sag, 1994) .  sentence Let us plan on that is shown in Figure 1 . 2  Additionally, we annotate the nodes of the derivation trees with information extracted from the HPSG sign. The annotation of nodes is performed by extracting values of feature paths from the feature structure or by propagating information from children or parents of a node. In theory with enough annotation at the nodes of the derivation trees, we can recover the whole HPSG signs. IMPER verb HCOMP Here we describe three node annotations that proved very useful for disambiguation. One is annotation with the values of the feature path synsem.local.cat.head -its values are basic parts of speech such as noun, verb, prep, adj, adv. Another is phrase structure category information associated with the nodes, which summarizes the values of several feature paths and is available in the Redwoods corpus as Phrase-Structure trees. The third is annotation with lexical type (le-type), which is the type of the head word at a node. The preterminals in Figure 1 are lexical item identifiers -identifiers of the lexical entries used to construct the parse. The le-types are about \u00a2\u00a1 \u00a3\u00a1 types in the HPSG type hierarchy and are the direct super-types of the lexical item identifiers. The le-types are not shown in this figure, but can be seen at the leaves in Figure 2 . For example, the lexical type of LET V1 in the figure is v sorb. In Figure 1 , the only annotation performed is with the values of synsem.local.cat.head. The Leaf Projection Paths View The projection path of a leaf is the sequence of nodes from the leaf to the root of the tree. In Figure 2 , the leaf projection paths for three of the words are shown. We can see that a node in the derivation tree par-ticipates in the projection paths of all words dominated by that node. The original local rule configurations -a node and its children, do not occur jointly in the projection paths; thus, if special annotation is not performed to recover it, this information is lost. As seen in Figure 2 , and as is always true for a grammar that produces non-crossing lexical dependencies, there is an initial segment of the projection path for which the leaf word is a syntactic head (called head path from here on), and a final segment for which the word is not a syntactic head (called non-head path from here on). In HPSG non-local dependencies are represented in the final semantic representation, but can not be obtained via syntactic head annotation. If, in a traditional parsing model that estimates the likelihood of a local rule expansion given a node (such as e.g (Collins, 1997) ), the tree nodes are annotated with the word of the lexical head, some information present in the word projection paths can be recovered. However, this is only the information in the head path part of the projection path. In further experiments we show that the non-head part of the projection path is very helpful for disambiguation. Using this representation of derivation trees, we can apply string kernels to the leaf projection paths and combine those to obtain kernels on trees. In the rest of this paper we explore the application of string kernels to this task, comparing the performance of the new models to models using more standard rule features. Tree and String Kernels Kernels and SVM ranking From a machine learning point of view, the parse selection problem can be formulated as follows: given training examples (\u00a1 \u00a3\u00a2 \u00a5 \u00a5\u00a4 \u00a1 \u00a7\u00a6 \u00a2 \u00a9\u00a8 \u00a9 \u00a4 \u00a1 \u00a7\u00a6 \u00a2 \u00a7\u00a8 \u00a7 \u00a9 \u00a8\u00a9 , where each \u00a1 \u00a2 is a natural language sentence, is the number of such sentences, \"! $# , \u00a6 \u00a2 \u00a7\u00a8% is a parse tree for \u00a1 \u00a3\u00a2 , & \u00a2 is the number of parses for a given sentence \u00a1 \u00a3\u00a2 , \u00a4 \u00a1 \u00a7\u00a6 \u00a2 \u00a7\u00a8% \u00a9 is a feature representation for the parse tree \u00a6 \u00a2 \u00a7\u00a8% , and we are given the training information which of all \u00a6 \u00a2 \u00a7\u00a8% is the correct parselearn how to correctly identify the correct parse of an unseen test sentence. One approach for solving this problem is via representing it as an SVM (Vapnik, 1998) ranking problem, where (without loss of generality) \u00a6 \u00a2 \u00a7\u00a8 is assumed to be the correct parse for \u00a1 '\u00a2 . The goal is to learn a parameter vector ( ) , such that the score of \u00a6 \u00a2 \u00a9\u00a8 ( ( ) 10 \u00a4 \u00a1 \u00a7\u00a6 \u00a2 \u00a7\u00a8 \u00a9 ) is higher than the scores of all other parses for the sentence. Thus we optimize for: 2 43 65 # 7 ( ) 80 ( ) @9 @A CB D \u00a2 \u00a7\u00a8% E E GF 4H \u00a1 PI Q( ) 80 \u00a1 R\u00a4 \u00a1 \u00a7\u00a6 \u00a2 \u00a7\u00a8 \u00a9 TS U\u00a4 \u00a1 \u00a7\u00a6 \u00a2 \u00a9\u00a8% \u00a9 \u00a8\u00a9 WV # S D \u00a2 \u00a7\u00a8% E E GF 4H \u00a1 PI D \u00a2 \u00a7\u00a8% V \u00a1 The D \u00a2 \u00a7\u00a8% are slack variables used to handle the non-separable case. The same formulation has been used in (Collins, 2001) and (Shen and Joshi, 2003) . This problem can be solved by solving the dual, and thus we would only need inner products of the feature vectors. This allows for using the kernel trick, where we replace the inner product in the representation space by inner product in some feature space, usually different from the representation space. The advantage of using a kernel is associated with the computational effectiveness of computing it (it may not require performing the expensive transformation \u00a4 explicitly). We learn SVM ranking models using a tree kernel defined via string kernels on projection paths. Kernels on Trees Based on Kernels on Projection Paths So far we have defined a representation of parse trees as lists of strings corresponding to projection paths of words. Now we formalize this representation and show how string kernels on projection paths extend to tree kernels. We introduce the notion of a keyed string -a string that has a key, which is some letter from the alphabet X of the string. We can denote a keyed string by a pair \u00a1 `Y \u00a5 \u00a8\u00a3 \u00a9 , where Y ba X is the key, and \u00a3 is the string. In our application, a key would be a word ) , and the string would be the sequence of derivation tree nodes on the head or non-head part of the projection path of the word ) . Additionally, for reducing sparsity, for each keyed string \u00a1 ) \u00a5 \u00a8\u00a3 \u00a9 , we also include a keyed string \u00a1 `c ed \u00a3f \u00a5 \u00a8\u00a3 \u00a9 , where c ed f is the le-type of the word ) . Thus each projection path occurs twice in the list representation of the tree -once headed by the word, and once by its le-type. In our application, the strings For a given kernel on strings, we define its extension to keyed strings as follows: \u00a2\u00a1 \u00a8\u00a1 `Y \u00a5 \u00a8\u00a3 \u00a9 \u00a5 \u00a1 eg \u00a5 \u00a8 \u00a7 \u00a9 \u00a8\u00a9 ! \u00a1 \u00a4\u00a3 \u00a5 \u00a8 \u00a7 \u00a9 , if Y ! g , and \u00a2\u00a1 \u00a8\u00a1 `Y \u00a5 \u00a8\u00a3 \u00a9 \u00a5 \u00a1 eg \u00a5 \u00a8 \u00a7 \u00a9 \u00a8\u00a9 ! \u00a1 , otherwise. We use this construction for all string kernels applied in this work. Given a tree \u00a6 Q! \u00a1 \u00a8\u00a1 `Y \u00a5 \u00a8\u00a3 \u00a9 \u00a5 \u00a5 \u00a1 `Y \u00a1 \u00a5 \u00a8\u00a3 \u00a2 \u00a9 \u00a8\u00a9 and a tree \u00a6 \u00a4\u00a3 ! \u00a1 \u00a8\u00a1 eg \u00a5 \u00a8 \u00a7 \u00a9 \u00a5 \u00a5 \u00a1 eg \u00a6\u00a5 \u00a5 \u00a8 \u00a7 \u00a7\u00a5 \u00a9 \u00a8\u00a9 , \u00a8\u00a1 \u00a7\u00a6 \u00a5 \u00a6 \u00a4\u00a3 \u00a9 ! B \u00a2 \u00a5 B % \u00a6 \u00a2\u00a1 \u00a8\u00a1 `Y \u00a2 \u00a5 \u00a8\u00a3 \u00a2 \u00a9 \u00a5 \u00a1 eg % \u00a5 \u00a8 \u00a7 % \u00a9 \u00a8\u00a9 This can be viewed as a convolution (Haussler, 1999 ) and therefore \u00a9\u00a8i s a valid kernel (positive definite symmetric), if is a valid kernel. String Kernels We experimented with some of the string kernels proposed in (Lodhi et al., 2000; Leslie and Kuang, 2003) , which have been shown to perform very well for indicating string similarity in other domains. In particular we applied the N-gram kernel, Subsequence kernel, and Wildcard kernel. We refer the reader to (Lodhi et al., 2000; Leslie and Kuang, 2003) for detailed formal definition of these kernels, and restrict ourselves to an intuitive description here. In addition, we devised a new kernel, called Repetition kernel, which we describe in detail. The kernels used here can be defined as the inner product of the feature vectors of the two strings ( \u00a3 , \u00a7 )= \u00a4 \u00a1 x \u00a9 \u00a4 ( \u00a7 ), with feature map from the space of all finite sequences from a string alphabet X to a vector space indexed by a set of subsequences from X . As a simple example, the #gram string kernel maps each string \u00a3 a X to a vector with dimensionality X and each element in the vector indicates the number of times the corresponding symbol from X occurs in \u00a3 . For example, \u00a4 \"! $# \u00a5 # \u00a1 `Y g \u00a6% Y '& Y )( \u00a9 ! 10 . The Repetition kernel is similar to the 1-gram kernel. It improves on the # -gram kernel by better handling cases with repeated occurrences of the same symbol. Intuitively, in the context of our application, this kernel captures the tendency of words to take (or not take) repeated modifiers of the same kind. For example, it may be likely that a ceratin verb take one PP-modifier, but less likely for it to take two or more. More specifically, the Repetition kernel is defined such that its vector space consists of all sequences from X composed of the same symbol. The feature map obtains matching of substrings of the input string to features, allowing the occurrence of gaps. There are two discount parameters 2 and 2 \u00a3 . 2 serves to discount features for the occurrence of gaps, and 2 \u00a3 discounts longer symbol sequences. Formally, for an input string \u00a3 , the value of the feature vector for the feature index sequence 3 ! Y Y , 3 4 Q! 65 , is defined as follows: Let \u00a1 be the left-most minimal contiguous substring of \u00a3 that contains 3 , \u00a1 U! \u00a1 \u00a1 87 , where for indices ! # \u00a5 \u00a3 \u00a5 9 ! c , \u00a1 \u00a2 A@ ! Y ! \u00a1 \u00a2 B Q! ! \u00a1 \u00a2 C . Then \u00a4 ED GF F H \u00a2 H \u00a2 PI Q ( \u00a3 )=2 7 9 2 9 \u00a3 . For our previous example, if 2 ! \u00a5 2 \u00a3 ! # , \u00a4 # \u00a1 `Y g R% Y \u00a1& Y '( \u00a9 ! # , \u00a4 # S# \u00a1 `Y g R% Y \u00a1& Y '( \u00a9 ! 7 , and \u00a4 # \u00a6# S# \u00a1 `Y g R% Y \u00a1& Y '( \u00a9 ! # 7 . The weighted Wildcard kernel performs matching by permitting a restricted number of matches to a wildcard character. A \u00a1 5 \u00a5 \u00a9 wildcard kernel has as feature indices 5 -grams with up to wildcard characters. Any character matches a wildcard. For example the 3-gram Y Y g will match the feature index Y UT g in a (3,1) wildcard kernel. The weighting is based on the number of wildcard characters used -the weight is multiplied by a discount 2 for each wildcard. The Subsequence kernel was defined in (Lodhi et al., 2000) . We used a variation where the kernel is defined by two integers \u00a1 5 \u00a5 \u00a4V \u00a9 and two discount factors 2 and 2 \u00a3 for gaps and characters. A subseq(k,g) kernel has as features all W -grams with W 1X Y5 . The V is a restriction on the maximal span of the W -gram in the original string -e.g. if 5 ! 7 and V ! a`, the two letters of a 7 -gram can be at most V S 5 C! 7 letters apart in the original string. The weight of a feature is multiplied by 2 T for each gap, and by 2 \u00a3 for each non-gap. For the exam- ple above, if 2 ! \u00a5 2 \u00a3 ! b0 \u00a5 5 ! 7 \u00a5 \u00a4V ! b0 , \u00a4 # \u00a6# \u00a1 `Y g \u00a6% Y '& Y )( \u00a9 ! 10 dc \u00a90 dc Q! ` . The feature in- dex Y Y matches only once in the string with a span at most 0 -for the sequence Y '& Y with # gap. The details of the algorithms for computing the kernels can be found in the fore-mentioned papers (Lodhi et al., 2000; Leslie and Kuang, 2003) . To summarize, the kernels can be implemented efficiently using tries. Experiments In this section we describe our experimental results using different string kernels and different feature annotation of parse trees. We learn Support Vector Machine (SVM) ranking models using the software package e gf ih 7 \u00a2 p Sq H (Joachims, 1999) . We also normalized the kernels: sr \u00a1 \u00a7\u00a6 \u00a5 \u00a6 t\u00a3 \u00a9 ! u v H @ \u00a8H B $w x u v H @ \u00a8H @ yw x u v H B \u00a5\u00a8H B \u00a4w . For all tree kernels implemented here, we first extract all features, generating an explicit map to the space of the kernel, and learn SVM ranking models using e f h 7 \u00a2 p Sq H with a linear kernel in that space. Since the feature maps are not especially expensive for the kernels used here, we chose to solve the problem in its primal form. We were not aware of the existence of any fast software packages that could solve SVM ranking problems in the dual formulation. It is possible to convert the ranking problem into a classification problem using pairs of trees as shown in (Shen and Joshi, 2003) . We have taken this approach in more recent work using string kernels requiring very expensive feature maps. We performed experiments using the version of the Redwoods corpus which was also used in the work of (Toutanova et al., 2002; Osborne and Baldbridge, 2004 ) and others. There are \u00a70 \u00a2\u00a1 \u00a1 annotated sentences in total, 0 \u00a3\u00a2 7 \u00a3\u00a4 of which are ambiguous. The average sentence length of the ambiguous sentences is \u00a2 words and the average number of parses per sentence is # \u00a1 \u00a2 . We discarded the unambiguous sentences from the training and test sets. All models were trained and tested using 10-fold cross-validation. Accuracy results are reported as percentage of sentences where the correct analysis was ranked first by the model. The structure of the experiments section is as follows. First we describe the results from a controlled experiment using a limited number of features, and aimed at comparing models using local rule features to models using leaf projection paths in Section 4.1. Next we describe models using more sophisticated string kernels on projection paths in Section 4.2. The Leaf Projection Paths View versus the Context-Free Rule View In order to evaluate the gains from the new representation, we describe the features of three similar models, one using the leaf projection paths, and two using derivation tree rules. Additionally, we train a model using only the features from the head-path parts of the projection paths to illustrate the gain of using the non-head path. As we will show, a model using only the head-paths has almost the same features as a rule-based tree model. All models here use derivation tree nodes annotated with only the rule schema name as in Figure 1 and the synsem.local.cat.head value. We will define these models by their feature map from trees to vectors. It will be convenient to define the feature maps for all models by defining the set of features through templates. The value \u00a4 Q \u00a1 \u00a7\u00a6 \u00a9 for a feature 3 and tree \u00a6 , will be the number of times 3 occurs in the tree. It is easy to show that the kernels on trees we introduce in Section 3.2, can be defined via a feature map that is the sum of the feature maps of the string kernels on projection paths. As a concrete example, for each model we show all features that contain the node [HCOMP:verb] from Figure 1 , which covers the phrase plan on that. Bi-gram Model on Projection Paths (2PP) The features of this model use a projection path representation, where the keys are not the words, but the le-types of the words. The features of this model are defined by the following template The node [HCOMP:verb] is part of the head-path for plan, and part of the non-head path for on and that. The le-types of the words let, plan, on, and that are, with abbreviations, v sorb, v e p, p reg, and n deic pro sg respectively. In the following examples, the node labels are abbreviated as well; is a special symbol for end of path and e is a special symbol for start of path. Therefore the features that contain the node will be: : \u00a1 `c ed \u00a8 \u00a7 & d \u00a5 W \u00a6\u00a5 & d \u00a2 \u00a5 W \u00a6\u00a5 & d \u00a2 \u00a8 \u00a7 \u00a5 \u00a1 \u00a9 d Y \u00a1& \u00a9 . \u00a1 \u00a9 d Y '& Bi-gram Model on only Head Projection Paths (2HeadPP) This model has a subset of the features of Model 2PP -only those obtained by the head path parts of the projection paths. For our example, it contains the subset of features of 2PP that have last bit # , which will be only the following: Rule Tree Model I (Rule I) The features of this model are defined by the two templates: \u00a1 `c ed \u00a8 \u00a7 & d \u00a5 W \u00a6\u00a5 & d \u00a5 \"% ! c & \u00a5 \"% ! c & \u00a3 \u00a5 # \u00a9 and \u00a1 `c ed \u00a8 \u00a7 & d \u00a5 W \u00a6\u00a5 & d \u00a5 \"% \" c A& \u00a5 \"% \" c A& \u00a3 \u00a5 \u00a1 \u00a9 . The last value in the tuples is an indication of whether the tuple contains the le-type of the head or the non-head child as its first element. The features containing the node [HCOMP:verb] are ones from the expansion at that node and also from the expansion of its parent:  Table 1 : Accuracy of models using the leaf projection path and rule representations. Rule Tree Model II (Rule II) This model splits the features of model Rule I in two parts, to mimic the features of the projection path models. It has features from the following tem- This model has less features than model Rule I, because it splits each rule into its head and nonhead parts and does not have the two parts jointly. We can note that this model has all the features of 2HeadPP, except the ones involving start and end of path, due to the first template. The second template leads to features that are not even in 2PP because they connect the head and non-head paths of a word, which are represented as separate strings in 2PP. plates: \u00a1 `c `d \u00a8 \u00a7 & d \u00a9 d Y '& \u00a5 W \u00a6\u00a5 & d \u00a5 d Y \u00a1& A c A& \u00a5 # \u00a9 and \u00a1 `c ed \u00a8 \u00a7 & d \u00a2\u00a1 \u00a5 W \u00a6\u00a9 d Y '& \u00a5 W \u00a6\u00a5 & d \u00a5 W \u00a6\u00a5 8W \u00a6\u00a9 d Y '& A c & \u00a5 \u00a1 \u00a9 Overall, we can see that models Rule I and Rule II have the information used by 2HeadPP (and some more information), but do not have the information from the non-head parts of the paths in Model 2PP. Table 1 shows the average parse ranking accuracy obtained by the four models as well as the number of features used by each model. Model Rule I did not do better than model Rule II, which shows that joint representation of rule features was not very important. The large improvement of 2PP over 2HeadPP (13% error reduction) shows the usefulness of the non-head projection paths. The error reduction of 2PP over Rule I is also large -9% error reduction. Further improvements over models using rule features were possible by considering more sophisticated string kernels and word keyed projection paths, as will be shown in the following sections. (Osborne and Baldbridge, 2004; Toutanova and Manning, 2002; Toutanova et al., 2002) has explored combining multiple classifiers using different features. We report results from such an experiment as well. Experimental Using Node Label and Head Category Annotations The simplest derivation tree node representation that we consider consists of features \u00a1 and #schema name and category of the lexical head. All experiments in this subsection section were performed using this derivation tree annotation. We briefly mention results from the best string-kernels when using other node annotations, as well as a combination of models using different features in the following subsection. To evaluate the usefulness of our Repetition Kernel, defined in Section 3.3, we performed several simple experiments. We compared it to a # -gram kernel, and to a 7 -gram kernel. The results -number of features per model, and accuracy, are shown in Table 3 . The models shown in this table include both features from projection paths keyed by words and projection paths keyed by le-types. The results show that the Repetition kernel achieves a noticeable improvement over a # -gram model ( \u00a2 \u00a4\u00a3 error reduction), with the addition of only a small number of features. For most of the words, repeated symbols will not occur in their paths, and the Repetition kernel will behave like a # -gram for the majority of cases. The additional information it captures about repeated symbols gives a sizable improvement. The bi-gram kernel performs better but at the cost of the addition of many features. It is likely that for large alphabets and small training sets, the Repetition kernel may outperform the bi-gram kernel. From this point on, we will fix the string kernel for projection paths keyed by words -it will be a linear combination of a bi-gram kernel and a Rep-  etition kernel. We found that, because lexical information is sparse, going beyond 7 -grams for lexically headed paths was not useful. The projection paths keyed by le-types are much less sparse, but still capture important sequence information about the syntactic frames of words of particular lexical types. To study the usefulness of different string kernels on projection paths, we first tested models where only le-type keyed paths were represented, and then tested the performance of the better models when word keyed paths were added (with a fixed string kernel that interpolates a bi-gram and a Repetition kernel). Table 4 shows the accuracy achieved by several string kernels as well as the number of features (in thousands) they use. As can be seen from the table, the models are very sensitive to the discount factors used. Many of the kernels that use some combination of 1-grams and possibly discontinuous bi-grams performed at approximately the same accuracy level. Such are the wildcard(2,1,2 ) and subseq(2, V ,2 ,2 \u00a3 ) kernels. Kernels that use 0grams have many more parameters, and even though they can be marginally better when using le-types only, their advantage when adding word keyed paths disappears. A limited amount of discontinuity in the Subsequence kernels was useful. Overall Subsequence kernels were slightly better than Wildcard kernels. The major difference between the two kinds of kernels as we have used them here is that the Subsequence kernel unifies features that have gaps in different places, and the Wildcard kernel does not. For example, Y dT g \u00a5 T Y g \u00a5 Y g T are different features for Wildcard, but they are the same feature Y g for Subsequence -only the weighting of the feature depends on the position of the wildcard. When projection paths keyed by words are added, the accuracy increases significantly. subseq(2,3,.5,2) achieved an accuracy of \u00a2 ` \u00a4 \u00a3\u00a2 \u00a3 , which is much higher than the best previously published accuracy from a single model on this corpus (\u00a2 7 \u00a3 for a model that incorporates more sources of information from the HPSG signs (Toutanova et al., 2002) ). Baldbridge, 2004)). Other Features and Model Combination Finally, we trained several models using different derivation tree annotations and built a model that combined the scores from these models together with the best model subseq(2,3,.5,2) from Table 4 . The combined model achieved our best accuracy of \u00a2 \u00a3 `\u00a3 . The models combined were: Model I A model that uses the Node Label and letype of non-head daughter for head projection paths, and Node Label and sysnem.local.cat.head for non-head projection paths. The model used the subseq(2,3,.5,2) kernel for le-type keyed paths and bigram + Repetition for word keyed paths as above. Conclusions We proposed a new representation of parse trees that allows us to connect more tightly tree structures to the words of the sentence. Additionally this representation allows for the natural extension of string kernels to kernels on trees. The major source of accuracy improvement for our models was this representation, as even with bi-gram features, the performance was higher than previously achieved. We were able to improve on these results by using more sophisticated Subsequence kernels and by our Repetition kernel which captures some salient properties of word projection paths. In future work, we aim to explore the definition of new string kernels that are more suitable for this particular application and apply these ideas to Penn Treebank parse trees. We also plan to explore annotation with more features from HPSG signs. Acknowledgements We would like to thank the anonymous reviewers for helpful comments. This work was carried out under the Edinburgh-Stanford Link programme, funded by Scottish Enterprise, ROSIE project R36763.",
    "abstract": "We present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree. This representation allows us to achieve significantly higher accuracy in the task of HPSG parse selection than standard models, and makes the application of string kernels natural. We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation. We apply SVM ranking models and achieve an exact sentence accuracy of 85.40% on the Redwoods corpus.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "24",
    "year": "2004",
    "month": "July",
    "title": "The Leaf Path Projection View of Parse Trees: Exploring String Kernels for {HPSG} Parse Selection"
}