{
    "article": "We describe a novel approach to machine translation that combines the strengths of the two leading corpus-based approaches: Phrasal SMT and EBMT. We use a syntactically informed decoder and reordering model based on the source dependency tree, in combination with conventional SMT models to incorporate the power of phrasal SMT with the linguistic generality available in a parser. We show that this approach significantly outperforms a leading string-based Phrasal SMT decoder and an EBMT system. We present results from two radically different language pairs, and investigate the sensitivity of this approach to parse quality by using two distinct parsers and oracle experiments. We also validate our automated BLEU scores with a small human evaluation. Introduction Current example-based (EBMT) and statistical (SMT) machine translation systems both use phrases learned from parallel corpora, yet while the two approaches are closer than ever, some critical differences remain. (Way & Gough, 2005) On the one hand, while statistical systems excel at producing correct, even idiomatic translations at the local level, they are still challenged by many linguistic phenomena, such as global constituent ordering. While SMT excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled in translation (Thurmair, 04) . On the other hand, many EBMT systems do not fully exploit the power that results from a combination of multiple powerful statistical models. In particular, we believe that the recent dominance of SMT systems in competitive evaluations indicates that an end-to-end search over a weighted linear combination of statistical models is essential for high-quality translation. However, there is no indication that these models must necessarily be linguistically uninformed. Limitations of string-based phrasal SMT State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words. Arbitrary reordering of words is allowed within memorized phrases, but typically only a small amount of phrase reordering is allowed, modeled in terms of offset positions at the string level. This reordering model is very limited in terms of linguistic generalizations. For instance, when translating English to Japanese, an ideal system would automatically learn largescale typological differences: English SVO clauses generally become Japanese SOV clauses, English post-modifying prepositional phrases become Japanese pre-modifying postpositional phrases, etc. A phrasal SMT system may learn the internal reordering of specific common phrases, but it cannot generalize to unseen phrases that share the same linguistic structure. In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne\u2026pas except in the context of specific intervening words. Previous work on syntactic SMT and statistical EBMT The hope in the SMT community has been that the incorporation of syntax would address these issues, but that promise has yet to be realized 1 . One simple means of incorporating syntax into SMT decoding is by re-ranking the n-best list of a baseline SMT system using various syntactic models, but Och et al. (04) found very little positive impact with this approach. However, an n-best list of even 16,000 translations captures only a tiny fraction of the ordering possibilities of a 20 word sentence; re-ranking provides the syntactic model no opportunity to boost or prune large sections of that search space. Inversion Transduction Grammars (Wu, 97) , or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar. To make this process computationally efficient, however, some severe simplifying assumptions are made, such as using a single non-terminal label. This results in the model simply learning a very high level preference regarding how often nodes should switch order without any contextual information. Also these translation models are intrinsically word-based; phrasal combinations are not modeled directly, and results have not been competitive with the top phrasal SMT systems. Along similar lines, Alshawi et al. (2000) treat translation as a process of simultaneous induction of source and target dependency trees using headtransduction; again, no separate parser is used. Yamada and Knight (01) employ a parser in the target language to train probabilities on a set of operations that convert a target language tree to a source language string. This improves fluency slightly (Charniak et al., 03) , but fails to significantly impact overall translation quality. This may be because the parser is applied to MT output, which is notoriously unlike native language, and no additional insight is gained via source language analysis. Lin (04) translates dependency trees using paths. This is the first attempt to incorporate large phrasal SMT-style memorized patterns together with a separate source dependency parser and SMT models. However the phrases are limited to linear paths in the tree, the only SMT model used is a maximum likelihood channel model and there is no ordering model. Reported BLEU scores are far below the leading phrasal SMT systems. Aue et al. (04) recently reported incorporating a logical form (LF) or dependency tree-based statistical language model into an existing EBMT system. MSR-MT (Menezes & Richardson, 03) parses both source and target languages to obtain a logical form (LF), and translates source LFs using memorized aligned LF examples to produce a target LF. It utilizes a separate sentence realization component (Ringger et al., 04) to turn this into a target sentence. As a result, Aue could not use an end-to-end search over a linear combination of models, and the simple addition of a single target language model did not provide much improvement. Dependency Treelet Translation In this paper we propose a novel dependency treebased approach to phrasal SMT which uses treebased 'phrases' and a tree-based ordering model in combination with conventional SMT models to produce translations significantly better than a leading string-based system. Our system employs a source-language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from a parallel sentence-aligned corpus. We begin by parsing the source text to obtain dependency trees and word-segmenting the target side, then applying an off-the-shelf word alignment component to the bitext. The word alignments are used to project the source dependency parses onto the target sentences. From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree. A unique feature is that we allow treelets with a wildcard root, effectively allowing mappings for siblings in the dependency tree. This allows us to model important phenomena, such as not \u2026 ne\u2026pas. We also train a variety of statistical models on this aligned dependency tree corpus, including a channel model and an order model. To translate an input sentence, we parse the sentence, producing a dependency tree for that sentence. We then employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models that are combined in a log-linear framework as in (Och, 03) . This approach offers the following advantages over string-based SMT systems: Instead of limiting learned phrases to contiguous word sequences, we allow translation by all possible phrases that form connected subgraphs (treelets) in the source and target dependency trees. This is a powerful extension: the vast majority of surface-contiguous phrases are also treelets of the tree; in addition, we gain discontiguous phrases, including combinations such as verb-object, article-noun, adjective-noun etc. regardless of the number of intervening words. Another major advantage is the ability to employ more powerful models for reordering source language constituents. These models can incorporate information from the source analysis. For example, we may model directly the probability that the translation of an object of a preposition in English should precede the corresponding postposition in Japanese, or the probability that a pre-modifying adjective in English translates into a post-modifier in French. Parsing and alignment We require a source language dependency parser that produces unlabeled, ordered dependency trees and annotates each source word with a partof-speech (POS). An example dependency tree is shown in Figure 1 . The arrows indicate the head annotation, and the POS for each candidate is listed underneath. For the target language we only require word segmentation. To obtain word alignments we currently use GIZA++ (Och & Ney, 03) . We follow the common practice of deriving many-to-many alignments by running the IBM models in both directions and combining the results heuristically. Our heuristics differ in that they constrain manyto-one alignments to be contiguous in the source dependency tree. A detailed description of these heuristics can be found in (Quirk et al, 2004) . Projecting dependency trees Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence. Oneto-one alignments project directly to create a target tree isomorphic to the source. Many-to-one alignments project similarly; since the 'many' source nodes are connected in the tree, they act as if condensed into a single node. In the case of one-to-many alignments we project the source node to the rightmost 2 of the 'many' target words, and make the rest of the target words dependent on it. Unaligned target words 3 are attached into the dependency structure as follows: assume there is an unaligned word t j in position j. Let i < j and k > j be the target positions closest to j such that t i depends on t k or vice versa: attach t j to the lower of t i or t k . If all the nodes to the left (or right) of position j are unaligned, attach t j to the left-most (or right-most) word that is aligned. The target dependency tree created in this process may not read off in the same order as the target string, since our alignments do not enforce phrasal cohesion. For instance, consider the projection of the parse in Figure 1 do not get the original input string: de d\u00e9marrage appears in the wrong place. A second reattachment pass corrects this situation. For each node in the wrong order, we reattach it to the lowest of its ancestors such that it is in the correct place relative to its siblings and parent. In Figure 2c , reattaching d\u00e9marrage to et suffices to produce the correct order. Extracting treelet translation pairs From the aligned pairs of dependency trees we extract all pairs of aligned source and target treelets along with word-level alignment linkages, up to a configurable maximum size. We also keep treelet counts for maximum likelihood estimation. Order model Phrasal SMT systems often use a model to score the ordering of a set of phrases. One approach is to penalize any deviation from monotone decoding; another is to estimate the probability that a source phrase in position i translates to a target phrase in position j (Koehn et al., 03) . We attempt to improve on these approaches by incorporating syntactic information. Our model assigns a probability to the order of a target tree given a source tree. Under the assumption that constituents generally move as a whole, we predict the probability of each given ordering of modifiers independently. That is, we make the following simplifying assumption (where c is a function returning the set of nodes modifying t): \u220f \u2208 = T t T S t c order T S T order ) , | )) ( ( P( ) , | ) ( P( Furthermore, we assume that the position of each child can be modeled independently in terms of a head-relative position: 3a demonstrates an aligned dependency tree pair annotated with head-relative positions; Figure 3b presents the same information in an alternate tree-like representation. ) , | ) , ( P( ) , | )) ( ( P( ) ( T S t m pos T S t c order t c m \u220f \u2208 = Figure We currently use a small set of features reflecting very local information in the dependency tree to model P(pos(m,t) | S, T): \u2022 The lexical items of the head and modifier. \u2022 The lexical items of the source nodes aligned to the head and modifier. \u2022 The part-of-speech (\"cat\") of the source nodes aligned to the head and modifier. \u2022 The head-relative position of the source node aligned to the source modifier. (One can also include features of siblings to produce a Markov ordering model. However, we found that this had little impact in practice). As an example, consider the children of propri\u00e9t\u00e9 in Figure 3 . The head-relative positions of its modifiers la and Cancel are -1 and +1, respectively. Thus we try to predict as follows: P(pos(m 1 ) = -1 | lex(m 1 )=\"la\", lex(h)=\"propri\u00e9t\u00e9\", lex(src(m 1 ))=\"the\", lex(src(h)=\"property\", cat(src(m 1 ))=Determiner, cat(src(h))=Noun, position(src(m 1 ))=-2) \u2022 P(pos(m 2 ) = +1 | lex(m 2 )=\"Cancel\", lex(h)=\"propri\u00e9t\u00e9\", lex(src(m 2 ))=\"Cancel\", lex(src(h))=\"property\", cat(src(m 2 ))=Noun, cat(src(h))=Noun, position(src(m 2 ))=-1) The training corpus acts as a supervised training set: we extract a training feature vector from each of the target language nodes in the aligned dependency tree pairs. Together these feature vectors are used to train a decision tree (Chickering, 02) . The distribution at each leaf of the DT can be used to assign a probability to each possible target language position. A more detailed description is available in (Quirk at al, 2004) . Other models Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03) . The MLE model effectively captures non-literal phrasal translations such as idioms, but suffers from data sparsity. The wordto-word model does not typically suffer from data sparsity, but prefers more literal translations. Given a set of treelet translation pairs that cover a given input dependency tree and produce a target dependency tree, we model the probability of source given target as the product of the individual treelet translation probabilities: we assume a uniform probability distribution over the decompositions of a tree into treelets. Target Model: Given an ordered target language dependency tree, it is trivial to read off the surface string. We evaluate this string using a trigram model with modified Kneser-Ney smoothing. Miscellaneous Feature Functions: The log-linear framework allows us to incorporate other feature functions as 'models' in the translation process. For instance, using fewer, larger treelet translation pairs often provides better translations, since they capture more context and allow fewer possibilities for search and model error. Therefore we add a feature function that counts the number of phrases used. We also add a feature that counts the number of target words; this acts as an insertion/deletion bonus/penalty. Decoding The challenge of tree-based decoding is that the traditional left-to-right decoding approach of string-based systems is inapplicable. Additional challenges are posed by the need to handle treelets-perhaps discontiguous or overlappingand a combinatorially explosive ordering space. Our decoding approach is influenced by ITG (Wu, 97) with several important extensions. First, we employ treelet translation pairs instead of single word translations. Second, instead of modeling rearrangements as either preserving source order or swapping source order, we allow the dependents of a node to be ordered in any arbitrary manner and use the order model described in section 2.4 to estimate probabilities. Finally, we use a log-linear framework for model combination that allows any amount of other information to be modeled. We will initially approach the decoding problem as a bottom up, exhaustive search. We define the set of all possible treelet translation pairs of the subtree rooted at each input node in the following manner: A treelet translation pair x is said to match the input dependency tree S iff there is some connected subgraph S' that is identical to the source side of x. We say that x covers all the nodes in S' and is rooted at source node s, where s is the root of matched subgraph S'. We first find all treelet translation pairs that match the input dependency tree. Each matched pair is placed on a list associated with the input node where the match is rooted. Moving bottomup through the input dependency tree, we compute a list of candidate translations for the input subtree rooted at each node s, as follows: Consider in turn each treelet translation pair x rooted at s. The treelet pair x may cover only a portion of the input subtree rooted at s. Find all descendents s' of s that are not covered by x, but whose parent s'' is covered by x. At each such node s'' look at all interleavings of the children of s'' specified by x, if any, with each translation t' from the candidate translation list 4 of each child 4 Computed by the previous application of this procedure to s' during the bottom-up traversal.  s'. Each such interleaving is scored using the models previously described and added to the candidate translation list for that input node. The resultant translation is the best scoring candidate for the root input node. As an example, see the example dependency tree in Figure 4a and treelet translation pair in 4b. This treelet translation pair covers all the nodes in 4a except the subtrees rooted at software and is. We first compute (and cache) the candidate translation lists for the subtrees rooted at software and is, then construct full translation candidates by attaching those subtree translations to install\u00e9s in all possible ways. The order of sur relative to install\u00e9s is fixed; it remains to place the translated subtrees for the software and is. Note that if c is the count of children specified in the mapping and r is the count of subtrees translated via recursive calls, then there are (c+r+1)!/(c+1)! orderings. Thus (1+2+1)!/(1+1)! = 12 candidate translations are produced for each combination of translations of the software and is. Optimality-preserving optimizations Dynamic Programming Converting this exhaustive search to dynamic programming relies on the observation that scoring a translation candidate at a node depends on the following information from its descendents: the order model requires features from the root of a translated subtree, and the target language model is affected by the first and last two words in each subtree. Therefore, we need to keep the best scoring translation candidate for a given subtree for each combination of (head, leading bigram, trailing bigram), which is, in the worst case, O(V 5 ), where V is the vocabulary size. The dynamic programming approach therefore does not allow for great savings in practice because a trigram target language model forces consideration of context external to each subtree. Lossy optimizations The following optimizations do not preserve optimality, but work well in practice. N-best lists Instead of keeping the full list of translation candidates for a given input node, we keep a topscoring subset of the candidates. While the decoder is no longer guaranteed to find the optimal translation, in practice the quality impact is minimal with a list size \u2265 10 (see Table 5 .6). Variable-sized n-best lists: A further speedup can be obtained by noting that the number of translations using a given treelet pair is exponential in the number of subtrees of the input not covered by that pair. To limit this explosion we vary the size of the n-best list on any recursive call in inverse proportion to the number of subtrees uncovered by the current treelet. This has the intuitive appeal of allowing a more thorough exploration of large treelet translation pairs (that are likely to result in better translations) than of smaller, less promising pairs. Pruning treelet translation pairs Channel model scores and treelet size are powerful predictors of translation quality. Heuristically pruning low scoring treelet translation pairs before the search starts allows the decoder to focus on combinations and orderings of high quality treelet pairs. Greedy ordering The complexity of the ordering step at each node grows with the factorial of the number of children to be ordered. This can be tamed by noting that given a fixed pre-and post-modifier count, our order model is capable of evaluating a single ordering decision independently from other ordering decisions. One version of the decoder takes advantage of this to severely limit the number of ordering possibilities considered. Instead of considering all interleavings, it considers each potential modifier position in turn, greedily picking the most probable child for that slot, moving on to the next slot, picking the most probable among the remaining children for that slot and so on. The complexity of greedy ordering is linear, but at the cost of a noticeable drop in BLEU score (see Table 5 .4). Under default settings our system tries to decode a sentence with exhaustive ordering until a specified timeout, at which point it falls back to greedy ordering. Experiments We evaluated the translation quality of the system using the BLEU metric (Papineni et al., 02) under a variety of configurations. We compared against two radically different types of systems to demonstrate the competitiveness of this approach: \u2022 Pharaoh: A leading phrasal SMT decoder (Koehn et al., 03 ). \u2022 The MSR-MT system described in Section 1, an EBMT/hybrid MT system. Language pairs We ran experiments in English French and English Japanese. The latter was chosen deliberately to highlight the challenges facing string-based MT approaches in language pairs with significant word-order differences. Word order in Japanese is fundamentally very different from English. English is generally SVO (subject first, then verb, then object), where Japanese is SOV with a strong bias for head-final structures. Several other differences include: \u2022 Word order is more flexible, since verbal arguments are generally indicated by postpositions, e.g. a direct object is indicated by the postposition \u3092 (o), a subject by \u304c (ga). \u2022 Most post-modifying English phrases (such as relative clauses and prepositional phrases) are translated as Japanese pre-modifiers; demonstratives and adjectives remain premodifiers. \u2022 Verbal and adjectival morphology in Japanese is relatively complex: information contained in English pre-modifying modals and auxiliaries is often represented as verbal morphology. \u2022 Japanese nouns and noun phrases are not marked for definiteness or number. The word-aligned sentence pair in Figure 1 demonstrates many of these phenomena. Data We used a corpus of Microsoft technical data (e.g., support articles, product documentation) containing over 1 million sentence pairs for each language-pair. We excluded sentences containing XML or HTML tags and for each language pair randomly selected training data sets ranging from 1,000 to 500,000 sentence pairs as well as 10,000 sentences for development testing and parameter tuning, 250 sentences for lambda training and 10,000 sentences for testing. Table 4 .1 presents some characteristics of this corpus. Training We parsed the source (English) side of the corpus using two different parsers: NLPWIN, a broadcoverage rule-based parser developed at Microsoft Research able to produce syntactic analyses at varying levels of depth (Heidorn, 02) The target language models were trained using only the French and Japanese sides, respectively, of the parallel corpus; additional monolingual data may improve its performance. Finally we trained lambdas via Maximum BLEU (Och, 03) on 250 held-out sentences with a single reference translation, and tuned the decoder optimization parameters (n-best list size, timeouts etc) on the development test set. Pharaoh The same GIZA++ alignments as above were used in the Pharaoh decoder. We used the heuristic combination described in (Och & Ney, 03) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 03) . Except for the order model (Pharaoh uses a penalty on the deviance from monotone), the same models were used: MLE channel model, Model 1 channel model, language model, phrase count, and word count. Lambdas were trained in the same manner (Och, 03) . MSR-MT MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 03) on the same training data. MSR-MT does not use lambdas or a target language model. Results We present BLEU scores on an unseen 10,000 sentence test set using a single reference translation for each sentence. Speed numbers are the end-to-end translation speed in sentences per minute. Unless otherwise specified all results are based on a phrase size of 4 and a training set size of 100,000 sentences for English French and 500,000 sentences for English Japanese. Unless otherwise noted all the differences between systems are statistically significant at P < 0.01 Comparative results are presented in Table 5 .1. Pharaoh monotone refers to Pharaoh with phrase reordering disabled. Table 5 .2 compares the systems at different training corpus sizes. All the differences are statistically significant at P < 0.01 except for English Japanese at training set sizes less than 30K. Note that in English French, where word order differences are mainly local, the gap between the systems narrows slightly with larger corpus sizes, however in English Japanese, with global ordering differences, the treelet system's margin over Pharaoh (initially negative) actually increases with increasing corpus size. Table 5 .4 compares different ordering strategies. In contrast to results reported for English-Chinese (Vogel et al., 03) , monotone decoding severely degrades the performance of both systems in English Japanese, presumably due to the large ordering variation between the two languages. In English-French the degradation is less marked.  Table 5 .6 shows the impact of using the top 100 NLPWIN parses even without any parse scoring. The last line in the table is a parse oracle experiment to explore the potential quality impact of better parse selection -the oracle picks and translates the one best parse from the top 100 parses. Table 5 .7 is a translation oracle experiment that demonstrates the impact of model error. The oracle picks the translation with the highest BLEU score from among the top N translations produced by the treelet system. Better models may improve performance, though Och et al. (04) suggests achieving this gain this may be difficult. Human Evaluation Two human raters were presented (in random order) both Pharaoh and Treelet translations of 100 sentences between 10 and 25 words and corresponding source and reference translations. They were asked to pick the more accurate translation. Table 5 .8 shows that for most of the sentences, humans prefer the Treelet translations, which is consistent with the BLEU scores above. Conclusions and Future Work We presented a novel approach to syntacticallyinformed statistical machine translation that leverages a parsed dependency tree representation of the source language via a tree-based ordering model and a syntactically informed decoder. We showed that it outperforms a leading phrasal SMT decoder in BLEU and human quality judgments. We also showed that it out-performed our own logical form-based EBMT/hybrid MT system. Even in the absence of a parse quality metric, we found that employing multiple parses could improve translation quality. Adding a parse probability may help further the gains from these additional possible analyses. The syntactic information used in these models is still rather shallow. Order modeling may benefit from additional information such as semantic roles or morphological features. Furthermore, different model structures, machine learning techniques, and target feature representations all have the potential for significant improvements.",
    "abstract": "We describe a novel approach to machine translation that combines the strengths of the two leading corpus-based approaches: Phrasal SMT and EBMT. We use a syntactically informed decoder and reordering model based on the source dependency tree, in combination with conventional SMT models to incorporate the power of phrasal SMT with the linguistic generality available in a parser. We show that this approach significantly outperforms a leading string-based Phrasal SMT decoder and an EBMT system. We present results from two radically different language pairs, and investigate the sensitivity of this approach to parse quality by using two distinct parsers and oracle experiments. We also validate our automated BLEU scores with a small human evaluation.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English",
        "Japanese"
    ],
    "numcitedby": "63",
    "year": "2005",
    "month": "September 13-15",
    "title": "Dependency Treelet Translation: The Convergence of Statistical and Example-based Machine-translation?"
}