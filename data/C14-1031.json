{
    "article": "Results from psychology show a connection between a speaker's expertise in a task and the language he uses to talk about it. In this paper, we present an empirical study on using linguistic evidence to predict the expertise of a speaker in a task: playing chess. Instructional chess literature claims that the mindsets of amateur and expert players differ fundamentally (Silman, 1999) ; psychological science has empirically arrived at similar results (e.g., Pfau and Murphy (1988) ). We conduct experiments on automatically predicting chess player skill based on their natural language game commentary. We make use of annotated chess games, in which players provide their own interpretation of game in prose. Based on a dataset collected from an online chess forum, we predict player strength through SVM classification and ranking. We show that using textual and chess-specific features achieves both high classification accuracy and significant correlation. Finally, we compare our findings to claims from the chess literature and results from psychology. Introduction It has been recognized that the language used when describing a certain topic or activity may differ strongly depending on the speaker's level of expertise. As shown in empirical experiments in psychology (e.g., Solomon (1990) , Pfau and Murphy (1988) ), a speaker's linguistic choices are influenced by the way he thinks about the topic. While writer expertise has been addressed previously, we know of no work that uses linguistic indicators to rank experts. We present a study on predicting chess expertise from written commentary. Chess is a particularly interesting task for predicting expertise: First, using data from competitive online chess, we can compare and rank players within a well-defined ranking system. Second, we can collect textual data for experimental evaluation from web resources, eliminating the need for manual annotation. Third, there is a large amount of terminology associated with chess, which we can exploit for n-gram based classification. Chess is difficult for humans because it requires long-term foresight (strategy) as well as the capacity for internally simulating complicated move sequences (calculation and tactics). For these reasons, the game for a long time remained challenging even for computers. Players have thus developed general principles of chess strategy on which many expert players agree. The dominant expert view is that the understanding of fundamental strategical notions, supplemented by the ability of calculation, is the most important skill of a chess player. A good player develops a long-term plan for the course of the game. This view is the foundation of many introductory works to chess (e.g., Capablanca (1921) , one of the earliest works). Silman (1999) presents games he played with chess students, analyzing their commentary about the progress of the game. He claims that players who fail to adhere to the aforementioned basic principles tend to perform worse and argues that the students' thought processes reflect their playing strength directly. Lack of strategical understanding marks the difference between amateur and expert players. Experts are mostly concerned with positional aspects, i.e., the optimal placement of pieces that offers a 8 rm0ZkZ0s 7 opo0lpa0 6 0Z0ZpZpo 5 Z0Z0O0Z0 4 0Z0Z0ZbZ 3 Z0MBZNZ0 2 POPZ0OPO 1 S0ZQZRJ0 a b c d e f g h Figure 1 : Example chess position, white to play long-lasting advantage. Amateurs often have tactical aspects in mind, i.e., short-term attacking opportunities and exploits that potentially lead to loss of material for their opponents. A correlation between chess strength and verbalization skills has been shown empirically by Pfau and Murphy (1988) , who used experts to assess the quality of the subjects' writing. In this paper, we investigate the differences between the mindset of amateurs and experts expressed in written game commentary, also referred to as annotated games. When studying chess, it is best practice to review one's own games to further one's understanding of the game (Heisman, 1995) . Students are encouraged to annotate the games, i.e., writing down their thought process at each move. We address the problem of predicting the player's strength from the text of these annotations. Specifically, we want to predict the rank of the player at the point when a given game was played. In competitive play, the rank is determined through a numerical rating system -such as the Elo rating system (Elo, 1978) used in this paper -that measures the players' relative strength using pairwise win expectations. This paper makes the following contributions. First, we introduce a novel training dataset of games annotated by the players themselves -collected from online chess forum. We then formulate the task of playing strength prediction. For each annotated game, each game viewed as a document, we predict the rating class or overall rank of the player. We show that (i) an SVM model with n-gram features succeeds at partitioning the players into two rating classes (above and below the mean rating); and (ii) that ranking SVMs achieve significant correlation between the true and predicted ranking of the players. In addition, we introduce novel chess-specific features that significantly improve the results. Finally, we compare the predictions made by our model to claims from instructional chess literature and results from psychology research. We next give an overview of basic chess concepts (Section 2). Then, we introduce the dataset (Section 3) and task (Section 4). We present our experimental results in Section 5. Section 6 contains an overview of related work. Basic Chess Concepts Chess Terminology We assume that the reader has basic familiarity with chess, its rules, and the value of individual pieces. For clarity, we review some basic concepts of chess terminology, particularly elementary concepts related to tactics and strategy in an example position (Figure 1 ). 1  From a positional point of view, white is ahead in development: all his minor pieces (bishops and knights) have moved from their starting point while black's knight remains on b8. White has also castled (a move where the rook and king move simultaneously to get the king to a safer spot on either side of the board) while black has not. White has a space advantage as he occupies the e5-square (which is in black's 1.e4 e5 2.Nf3 Nc6 3.Bc4 Nh6 4.Nc3 Bd6 Trying to follow basic opening principals, control center, develop. etc 5.d3 Na5 6.Bb5 Moved bishop not wanting to trade, but realized after the move that my bishop would be harassed by the pawn on c7 6...c6 7.Ba4 Moved bishop to safety, losing tempo 7.  half of the board) with a pawn. This pawn is potentially weak as it cannot easily be defended by another pawn. Black has both of his bishops (the bishop pair) which is considered advantageous as bishops are often superior to knights in open positions. Black's light-square bishop is bad as it is obstructed by black's own pawns (although it is outside the pawn chain and thus flexible). Strategically, black might want to improve the position of the light-square bishop, make use of his superior dark-square bishop, and try to exploit the weak e5 pawn. Conversely, white should try create posts for his knights in black's territory. Tactically, white has an opportunity to move his knight to b5 (written Nb5 in algebraic chess notation), from where it would attack the pawn on c7. If the knight could reach c7 (currently defended by black's queen), it would fork (double attack) black's king and rook, which could lead to the trade of the knight for the rook on the next move (which is referred to as winning the exchange). White's knight on f3 is pinned, i.e., the queen would be lost if the knight moved. Black can win a pawn by removing the defender of e5, the knight on f3, by capturing it with the bishop. This brief analysis of the position shows the complex theory and terminology that has developed around chess. The paragraph also shows an example of game annotation (although not every move in the game will be covered as elaborately in practice in amateur analyses). Elo Rating System Our goal in this paper is to predict the ranking of chess players based on their game annotations. We will give a brief overview of the Elo system (Elo, 1978) that is commonly used to rank players. Each player is assigned a score that is changed after each game depending on the expected and actual outcome. On chess.com, a new player starts with an initial rating of 1200 (an arbitrary number chosen for historical reasons, which has since become a wide-spread convention in chess). Assuming the current ratings R a and R b of two players a and b, the expected outcome of the game is defined as E a = 1 1 + 10 \u2212 Ra\u2212R b 400 . E a is then used to conduct a (weighted) update of R a and R b given the actual outcome of the game. Thus, Elo ratings make pairwise adjustments to the scores. The differences between the ratings of two players predict the probability of one winning against the other. However, the absolute ratings do not carry any meaning by themselves. Annotated Chess Game Data For supervised training, we require a collection of chess games annotated by players of various strengths. An annotated chess game is a sequence of chess moves with natural language text commentary associated to specific moves. While many chess game collections are available, some of them containing millions of games, the majority are unannotated. The small fraction of annotated games mostly features commentary by masters rather than amateurs, which is not interesting for a contrastive study. The game analysis forum on chess.com encourages players to post their annotated games for review through the community. While several games are posted each day, we can only use a small subset of them.   Many games are posted without annotations, instead soliciting annotation from the community. Others are missing the rating of the player at the time the game was played -the user profile shows only the current rating for the player which may differ strongly from their historical one. \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 chess.com overall our dataset We first downloaded all available games from the forum archive. The games are stored in portable game notation (PGN, Edwards (1994) ). Next, we manually removed games where the annotation had been conducted automatically by a chess program. We also removed games that had annotations at fewer than three moves. The final dataset consists of 182 games with annotations in English and known player rating. 2 We reproduce an example game from the data in Figure 2 . This game is typical as the first couple of moves are not commented (as opening moves are typically well-known). Then, the annotator comments on select moves that he believes are key to the progress of the game. Table 1 shows some statistics about the dataset. The distribution of the ratings in our dataset is shown in Figure 3 in comparison to the overall standard chess rating distribution on chess.com. 3 Elo ratings assume a normal distribution of players. We see that overall, the distributions are quite similar, although we have a higher peak and our sample mean is shifted towards higher ratings (1347 overall vs 1462 on our dataset). It is more common for mid-level players to request annotation advice than it is for low-rated players (who might not know about this practice) or high-rated players (who do not look for support by the lower-rated community). The dataset is still somewhat noisy as players may obtain different ratings depending on the type of venue (over-the-board tournament vs online chess) or the amount of time the players had available (time control). Differences in these parameters lead to different rating distributions. 4 For this reason, the total ordering given through the ratings may be difficult to predict. Thus, we will conduct experiments both on ranking and on classification where the rating range is binned into two rating classes. Predicting Chess Strength from Annotations Classification and Ranking The task addressed in this paper is prediction on the game level, i.e., predicting the strength of the player of each game at the time when the game was played. We view a game as a document -the concatenation of the annotations at each move -and extract feature vectors as described in Section 4.2. We pursue two different machine learning approaches based on support vector machines (SVMs) to predicting chess strength: classification and ranking. The simplest way to approach the problem is classification. For this purpose, we divide the range of observed rating into two evenly spaced rating classes at the median of the overall rating range (henceforth amateur and expert). The classification view has obvious disadvantages. At the boundaries of the bins, the distinction between them becomes difficult. To predict a total ordering of all players, we use a ranking SVM (Herbrich et al., 1999) . This model casts ranking as learning a binary classification function that decides whether rank(x 1 ) > rank(x 2 ) over all possible pairs of example feature vectors x 1 and x 2 with differing rank. Note that since Elo ratings are continuous real numbers, it would be conceivable to fit a regression model. However, Elo is designed as a pairwise ranking measure. While a relative difference in Elo represents the probability of one player beating the other, the absolute Elo rating is not directly interpretable. 5 Features We extract unigrams (UG) and bigrams (BG) from the texts. In addition, we propose the following two chess-specific feature sets derived from the text: 6 Notation (NOT). We introduce two indicators for whether the annotations contain certain types of formal chess notation. The feature SQUARE is added if the annotation contains a reference to a specific square on the chess board (e.g., d4). If the annotation contains a move in algebraic notation (e.g., Nxb4+, meaning that a knight moved to b4, captured a piece there and put the enemy king in check), the feature MOVE is added. Similarity to master annotations (MS) . This feature is intended to compensate for the lack of training data. We used a master-annotated database consisting of 500 games annotated by chess masters which is available online. 7 As we do not know the exact rating of the annotators, and to avoid strong class imbalances, we cannot make use of the games directly through supervision. Instead, we calculate the cosine similarity between the centroid 8 of the n-gram feature vectors of the master games and each game in the chess.com dataset. The cosine similarity between each game and the master centroid is added as a numerical feature. Additionally, the master similarity scores can be used on their own to rank the games. This can be viewed distant supervision as strength is learned from an external database. We will evaluate this ranking in comparison with our trained models. Experiments This section, contains experimental results on classifying and ranking chess players. We first present quantitative evaluation of the classification and ranking models and discuss the effect of chess-specific Model Features F (\u2193) 1 F (\u2191) 1 F (\u2205) 1 1 Majority BL 67.2 0.0 33.6 2 SVM (linear) UG 73.4 71.6 72.5 3 SVM (linear) UG, BG  74.1 72.0 73.1 4 SVM (linear) UG, BG, NOT 75.7 74.9 75.3 5 SVM (linear) UG, BG, NOT, MS 74.2 73.0 73.6   features. Second, we qualitatively compare the predictions of our models with findings and claims from the literature about the connection between a player's mindset and strength. Experimental Setup To generate feature vectors, we first concatenate all the annotations for a game, tokenize and lowercase the texts, and remove punctuation as well as a small number of stopwords. We exclude rare words to avoid overfitting: We remove all n-grams that occur fewer than 5 times, and add the chess-specific features proposed above. Finally, we L 2 -normalize each vector. We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM (Chang and Lin, 2011) . We run all experiments in a 10-fold cross-validation setup. We measure macro-averaged F 1 for our classification results. We evaluate the ranking model using two measures: pairwise ranking accuracy (Acc r ), i.e., the accuracy over the binary ranking decision for each player pair; and Spearman's rank correlation coefficient \u03c1 for the overall ranking. To test whether differences between results are statistical significant, we apply approximate randomization (Noreen, 1989) for F 1 , and the test by Steiger (1980) for correlations, which is applicable to \u03c1. Classification We first investigate the classification case, i.e., whether we can distinguish players below and above the rating mean. Table 2 shows the results for this experiment. We show F 1 scores for the lower and higher half of the players (F (\u2193) 1 and F (\u2191) 1 , respectively), and the macro average of these two scores (F (\u2205) 1 ). We first note that all SVM classifiers (lines 2-5) score significantly higher than the majority baseline (line 1). When adding bigrams (line 3) and chess-specific notation features (line 4), F 1 increases. However, these improvements are not statistically significant. The master similarity feature (line 5) leads to a drop in F 1 from the previous line. The relatively low rank correlation between the master similarity scores and the two classes (\u03c1 = 0.334) leads to this effect. The low correlation itself may occur because the master games were annotated by a third party (instead of the players), leading to strong differences in style. There are several reasons for misclassification. Many errors occur in the dense region around the class boundary. Also, shorter game annotations are more difficult to classify than longer ones. For detailed error analysis, we first examine the most positively and negatively weighted features of the trained models (Table 3 ). We will provide a more detailed look into the features in Section 5.4. We 4 : Ranking results for standalone master similarity and SVM (linear and RBF kernel). Check in sig column denote significance of correlation with true ranking (p < 0.05). Numbers in sigdiff column denote a significant improvement (p < 0.05) in \u03c1 over the respective line. Model find that there are noticeable differences in the writing styles of amateurs and experts. According to the model, one of the most prominent distinctions is that amateurs tend to refer to the opponent as he, whereas experts use white and black more frequently. However, it is of course not universally true, which leads to the misclassification of some experts as amateurs. Another difference in style is that amateur players tend to write about the game in the past tense. This is a manifestation of an important distinction: Amateurs often state the obvious developments of the game (e.g., Flat out blunder, gave up a knight in Figure 2 ) or speculate about options (e.g., hoping to eventually attack), while experts provide more thorough positional analysis at key points. Ranking We now turn to ranking experiments (Table 4 ). We first evaluate the ranking produced by ordering the games by their similarity to the master centroid (line 1). We find that the resulting rank correlation is low but significant. The results for the linear SVM ranker are shown in lines 2-5. Total ranking is considerably more difficult than binary classification of rating classes. Using a linear SVM, we again achieve low but significant correlations. The linear classifiers (lines 2-5) do not significantly outperform the standalone master similarity (MS) baseline (line 1). Chess-specific features (lines 4 and 5) boost the results, outperforming the bigram models (line 3) significantly. The improvement from adding the MS centroid score feature is not significant. We again perform error analysis by examining the feature weights (Table 5 ). We find an overall picture similar to the classification setup (cf. Table 3 ). The notation feature serves as a good indicator for the upper rating range (cf. Table 3 ) as experienced players find it easier to express themselves through notation. We observed that lower players tend to express moves in words (e.g., \"move my knight to d5\") rather than through notation (Nd5), which could serve as an explanation for why pieces (bishop, knight, rook) appear among the top features for amateur players. However, some features change signs between the two experiments (e.g., king, square). This effect may indicate that the binary ranking problem is not linearly separable, which is plausible; mid-rated players may use terms that neither low-rated nor high-rated players use. Examining correlations at different ranking ranges confirms this suggestion. In top and bottom thirds of the rating scale, the true and predicted ranks are not correlated significantly. This means that the ranking SVM only succeeds at ranking players in middle third of the rating scale. To introduce non-linearity, we conduct further experiments with an SVM with a radial basis function (RBF) kernel. The results of this experiment are shown in lines 6-9 of  the unigram and bigram linear models; all except for the unigram model (lines 7-9) also yield weakly significant improvements over the MS baseline. Adding the notation features (line 8 improves the results and leads to improvements with stronger significance. The RBF kernel makes feature weight analysis impossible, so we cannot perform further error analysis. Comparing the Learned Models and Strength Indicators from the Chess Literature There are many conjectures from instructional chess literature and results from psychological research about various aspects of player behavior. In this section, we compare these to the predictions made by our supervised expertise model. In Table 6 , we list selected weights from the best classification model (line 3 in Table 2 ). We opt for analzying the classifier rather than the ranker as we find the former more directly interpretable. Long-Term vs Short-Term Planning. The SVM model reflect the short-term nature of the amateurs' thoughts in several ways: (i) Amateurs focus on specific moves rather than long-term plans, and thus, terms like capture and take are deemed predictive for lower ratings. (ii) Amateurs often think piecespecific (Silman, 1999) , particularly about moves with minor pieces (bishop or knight), and these terms receive high negative weights, pointing to lower ratings. Related to this, Reynolds (1982) observed that amateurs often focus on the current location of a piece, whereas experts mostly consider possible future locations. The SVM model learns this by weighting bigrams of the form * on, where * is a piece, as indicators for low ratings. (iii) Many terms related to elementary tactics (e.g., pin, fork) indicate lowerrated players, whereas terms relating to tactical foresight (e.g., threat, danger, stop) as well as positional terms (e.g., weakness, light and dark squares, variation) indicate higher-rated players. Emotions. A popular and wide-spread claim is that weaker chess players often lose because they are too emotionally invested in the game and thus get carried away (e.g., Cleveland (1907) , Silman (1999) ). We experimented with a sentiment feature, counting polar terms in the annotations using a polarity lexicon (Wilson et al., 2005) . However, this feature did not improve our results. Manual examination of features expressing sentiment reveals that both amateurs and experts use subjective terms. We note that the vocabulary of subjective expressions is very constrained for stronger players while it is open for weaker ones. Expert players tend to assess positions as winning or losing for a side, whereas weaker players tend to use terms such as like and hate. Both terms are identified as indicators of the respective strength class in our models. Other subjective assessments (e.g., good and bad) are divided among the classes. Emotional tendencies of amateurs can also be observed through objective indicators. As discussed above, stronger players talk about the game with a more distanced view, often referring to their opponent by their color (white or black) rather than using the pronoun he. Lower-rated players appear to use terms indicating competitions more frequently, such as fight. Confidence. Silman (1999) argues that weaker players lack confidence, which leads to them losing track of their own plans and to eventually follow their opponent's will (often called losing the initiative). This process is indeed captured by our trained models. Terms of high confidence (such as know, will) are weighted towards the stronger class, whereas terms with higher uncertainty (such as thinking, believe, maybe, hoping) indicate the weaker class. This observation is in line with findings on self-assigned confidence judgments of chess players (Reynolds, 1992) . The sets of terms expressing certainty and uncertainty, respectively, are small in our dataset, so weights for most terms can be learned directly on the n-grams. Time Management. It has been suggested that deficiencies in time management are responsible for many losses at the amateur level, particularly in fast games (e.g., blitz chess, where each player has 5 minutes to complete the game), for example due to poor pattern recognition skills of beginners (Calderwood et al., 1988) . In the trained models, we see that the term time itself is actually considered a good indicator for stronger players. Time is often used to signify number of moves. So, when used on its own, time is referring to efficient play, which is indicative of strong players. Conversely, the terms clock and time pressure are deemed good features to identify weaker players. Chess Terminology. As shown in Section 2.1 and throughout this paper, there is a vast amount of chess terminology. We observe that frequent usage of such terms (e.g., blunder -a grave mistake, tempo, checkmate -experts use mate, opening, castle) actually indicate a weaker player. This seems counterintuitive at first, as we may expect lower-rated players to be less familiar with such terms. However, it appears that they are frequently overused by weaker players. This also holds for metaphorical terms, such as fall or eat instead of capture. Related Work The treatment of writer expertise in extralinguistic tasks in NLP has mostly focused on two problems: (i) retrieval of experts for specific areas -i.e., predicting the area of expertise of a writer (e.g., Tu et al. (2010; Kivim\u00e4ki et al. (2013) ); and (ii) using expert status in different downstream applications such as sentiment analysis (e.g., Liu et al. (2008) ) or dialog systems (e.g., Komatani et al. (2003) ). Conversely, our work is concerned with predicting a ranking by expertise within a single task. Several publications have dealt with natural language processing related to games. Chen and Mooney (2008) investigate grounded language learning where commentary describing the specific course of a game is automatically generated. Commentator expertise is not taken into account in this study. Branavan et al. (2012) introduced a model for using game manuals to increase the strength of a computer playing the strategy video game Civilization II. Cadilhac et al. (2013) investigated the prediction of player actions in the strategy board game The Settlers of Catan. Our approach differs conceptually from theirs as their main focus lies on modeling concrete actions in the game (either predicting or learning them); our goal is to predict player strength, i.e., to learn to compare players among each other. Rather than explicitly modeling the game, commentary analysis aims to provide insight into specific thought processes. Work in psychology research by Pfau and Murphy (1988) showed the quality of chess players' verbalization about positions is correlated significantly with their rating. While they use manual assessments by chess masters to determine the quality of a player's writing, our approach is to learn this distinction is automatically given the ratings. Conclusion In this paper, we presented experiments on predicting the expertise of speakers in a task using linguistic evidence. We introduced a classification and a ranking task for automatically ranking chess players by playing strength using their natural language commentary. SVM models succeed at predicting either a rating class or an overall ranking. In the ranking case, we could significantly boost the results by using chess-specific features extracted from the text. Finally, we compared the predictions of the SVM with popular claims from instructional chess literature as well as results from psychology research. We found that many of the traditional findings are reflected in the features learned by our models. Acknowledgements We thank Daniel Quernheim for providing his chess expertise, Kyle Richardson and Jason Utt for helpful suggestions, and the anonymous reviewers for their comments.",
    "abstract": "Results from psychology show a connection between a speaker's expertise in a task and the language he uses to talk about it. In this paper, we present an empirical study on using linguistic evidence to predict the expertise of a speaker in a task: playing chess. Instructional chess literature claims that the mindsets of amateur and expert players differ fundamentally (Silman, 1999) ; psychological science has empirically arrived at similar results (e.g., Pfau and Murphy (1988) ). We conduct experiments on automatically predicting chess player skill based on their natural language game commentary. We make use of annotated chess games, in which players provide their own interpretation of game in prose. Based on a dataset collected from an online chess forum, we predict player strength through SVM classification and ranking. We show that using textual and chess-specific features achieves both high classification accuracy and significant correlation. Finally, we compare our findings to claims from the chess literature and results from psychology.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2014",
    "month": "August",
    "title": "Picking the Amateur{'}s Mind - Predicting Chess Player Strength from Game Annotations"
}