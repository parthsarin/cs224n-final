{
    "framing_question": "What motivation does this paper state for the work, if any?",
    "response": "The paper states that recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. Therefore, the paper aims to examine the new task of reason classification.",
    "article": "Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classification (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b) . For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author's stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as \"Should abortion be banned?\" or \"Do you support Obamacare?\", it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.] 1 Let us take a look from the social perspective. [If parents cannot afford to provide for the child, or if the family is facing financial constraints, it is understandable that abortion can remain as one of the options.] 2 Reason 1: Woman's right to abort Reason 2: Unwanted babies are threat to their parents' future should not be difficult for us to come up with a set of reasons people typically use to back up their stances. Given a set of reasons associated with each stance in an online debate, the goal of postlevel reason classification is to identify those reason(s) an author uses to back up her stance in her debate post. A more challenging version of this task is sentence-level reason classification, where the goal is to identify not only the reason(s) an author uses in her post, but also the sentence(s) in the post that the author uses to describe each of her reasons. For example, the author of the post shown in Figure 1 mentions two reasons why she supports abortion, namely it's a woman's right to abort and unwanted babies are threat to their parents' future, which are mentioned in the first and third sentences in the post respectively. Our goal in this paper is to examine post-and sentence-level reason classification (RC) in ideological debates. Many online debaters use emotional languages, which may involve sarcasm and insults, to express their points, thereby making RC and SC in ideological debates potentially more challenging than that in other debate settings such as congressional debates and company-internal discussions (Walker et al., 2012) . Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by (1) determining the reason(s) associated with each of its sentences (if any), and then (2) taking the union of the set of reasons associated with all of its sentences to be the set of reasons associated with the post. We hypothesize that this sentence-based approach, which exploits a training set in which each sentence in a post is labeled with its reason, would achieve better performance than a multilabel text classification approach to post-level RC, which learns to determine the subset of reasons a post contains directly from a training set in which each post is labeled with the corresponding set of reasons. In other words, we hypothesize that we could achieve better results for post-level RC by learning from sentence-level than from post-level reason annotations, as sentence-level reason annotations can enable a learning algorithm to accurately attribute an annotated reason to a particular portion of a post. Second, we propose stance-supported RC systems, hypothesizing that automatically computed stance information can be profitably exploited for RC. Since we are exploiting automatically computed (and thus potentially noisy) stance information, we hypothesize that the effectiveness of such information would depend in part on the way it is exploited in RC systems. As a result, we introduce a set of stance-supported models for RC, starting with simple pipeline models and then moving on to joint models with increasing sophistication. Note that exploiting stance information by no means guarantees that RC performance will improve, as an incorrect determination of stance could lead to an incorrect identification of reasons. Hence, one of our goals is to examine how to model stances and reasons so that RC can benefit from stance information. Finally, since progress on RC is hindered in part by the lack of an annotated corpus, we make our reason-annotated dataset publicly available. 1 To our knowledge, this will be the first publicly available corpus for sentence-and post-level RC. Corpus and Annotation We collected debate posts from four popular domains, Abortion (ABO), Gay Rights (GAY), Obama (OBA), and Marijuana (MAR), from an online debate forum 2 . All debates are two-sided, so each post receives one of two stance labels, for or against, depending on whether the author of the post supports or opposes abortion, gay rights, Obama, or the legalization of marijuana respectively. A post's stance label is given by its author. Note that each post belongs to a thread, which is a tree with one or more nodes such that (1) each node corresponds to a debate post, and (2) a post y i is the parent of another post y j if y j is a reply to y i . Given a thread, we generate post sequences, each of which is a path from the root of the thread to one of its leaves. Hence, a post sequence is an ordered set of posts such that each post is a reply to its immediately preceding post in the sequence. Table 2a shows the statistics of the four stancelabeled datasets. While the debate posts contain the stance labels given by their authors, they are not annotated with reasons. As part of our study of RC, we annotate each post with the reasons it gives for its stance. Our annotation procedure is composed of three steps. First, two human annotators independently examined each post and identified the reasons authors present to support their stances (i.e., for and against) in each domain. Second, they discussed and agreed on the reasons identified for each domain. Third, they independently annotated the text of each post with reason labels from the post's domain. To do this, they labeled each sentence of a post with the set of reasons the author expressed in that sentence. Any sentence that does not belong to any reason class was assigned the NONE class. After the annotators completed the aforementioned steps, they were asked to collapse all the reason classes that occur in less than 2% of the sentences annotated with non-NONE classes into the OTHER class. In other words, all the sentences that were originally annotated with one of these infrequent reason classes will now be labeled as OTHER. Our decision to merge infrequent classes is motivated by two observations. First, from a practical point of view, infrequent reasons do not carry much weight. Second, from a modeling perspective, it is often not worth increasing model complexity by handling infrequent classes. Table 1 : Reason classes and their percentages in the corresponding stance for each domain. sentences given their rarity, we asked each annotator to pick the reason that was highlighted the most in each multi-labeled sentence. Inter-annotator agreement scores at the sentence level and the post level, expressed in terms of Cohen's Kappa (Carletta, 1996) , are shown in Table 2b. Given that the majority of sentences were labeled as NONE, we avoid inflating agreement by not considering the sentences labeled with NONE by both annotators when computing Kappa. As we can see, we achieved substantial post-level agreement and high sentence-level agreement. The major source of inter-annotator disagreement for all four datasets stems from the fact that in many cases, the annotators, while agreeing on the reason class, differ on how long the text span for a reason should be. This hurts sentence-level agreement but not post-level agreement, since the latter only concerns whether a reason was mentioned in a post, and explains why the sentencelevel agreement scores are lower than the corresponding post-level scores. Minor sources of disagreement arise from the facts that (1) the annotators selected different reason labels for some of the multi-labeled sentences, and (2) they tend to disagree in some cases where authors use sarcasm  to present a reason. Each case of disagreement is resolved through discussion among the annotators. 3 Baseline RC System Our baseline system uses a maximum entropy (MaxEnt) classifier to determine whether a reason is expressed in a post and/or its sentence(s). We create one training instance for each sentence in each post in the training set, using the reason label as its class label. We represent each instance using five types of features, as described below. N-gram features. We encode each unigram and bigram collected from the training sentences as a binary feature indicating the n-gram's presence or absence in a given sentence. Dependency-based features. To capture the inter-word relationships that n-grams may not, we employ the dependency-based features previously used for stance classification in Anand et al. (2011) . These features have three variants. In the first variant, the pair of arguments involved in each dependency relation extracted by a dependency parser is used as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or \u2212). Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SE-MAFOR (Das et al., 2010) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f . A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame appears within an element of the second frame. Finally, frame ngram features are a variant of word n-grams. For each word n-gram in the sentence, a frame n-gram feature is created by replacing one or more words in the word n-gram with the name of the frame or the frame element in which the word appears. A detailed description of these features can be found in Hasan and Ng (2013c). Quotation features. We employ two quotation features. IsQuote is a binary feature that indicates whether a sentence is a quote or not (i.e., whether it appeared in its parent post in the post sequence). Note that if an instance is a quote from a previous post, it is unlikely that it represents a reason the author is presenting to support her argument. Instead, the author may have quoted this before stating her counter-argument. FollowsQuote is a binary feature that indicates whether a sentence follows a sentence for which the IsQuote feature value is true. Intuitively, a sentence following a quote is likely to present a counter-argument. Positional feature. We split each post into four parts (such that each part contains roughly the same number of sentences) and create one positional feature that encodes which part of the post contains a given sentence. This feature is motivated by our observations on the training data that (1) reasons are more likely to appear in the second half of a post and (2) on average more than onethird of the reasons appear in the last quarter of a post. After training, we can apply the resulting RC system to classify the test instances, which are generated in the same way as the training instances. Once the sentences of a test post are classified, we simply assume its post-level reason labels to be the set of reason labels assigned by the classifier to its sentences. Stance-Supported RC Systems In this section, we propose a set of systems for RC. Unlike the baseline RC system, these RC systems are stance-supported, enabling us to explore how different ways of modeling automatically computed stances and reasons can improve RC classification. Below we present our systems in increasing order of modeling sophistication. Pipeline Systems We examine two pipeline systems, P1 and P2. Given a set of test posts, both systems first determine the stance of each post and then apply a stance-specific reason classifier to each of them. More specifically, both P1 and P2 employ two stance-specific reason classifiers: one is trained on all the posts labeled as for and the other is trained on all the posts labeled as against. Each stancespecific reason classifier is trained using MaxEnt on the same feature set as that of the Baseline RC system. It computes for a particular stance s the probability P (r|s, t), where r is a reason label and t is a sentence in a test post p. P1 and P2 differ only with respect to the SC model used to stance-label each post. In P1, the stance s of a post p is determined by applying to p a stance classifier that computes P (s|p). To train the classifier, we employ MaxEnt. Each train-ing instance corresponds to a training post and is represented by all but the quotation and positional features used to train the Baseline RC system, since these two feature types are sentencebased rather than post-based. After training, the resulting classifier can be used to stance-label a post independently of the other posts. In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together. 3  As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000) . Given an input post sequence P S = (p 1 , p 2 , . . . , p n ), the MEMM finds the most probable stance sequence S = (s 1 , s 2 , . . . , s n ) by computing P (S|P S ), where: P (S|P S ) = n k=1 P (s k |s k\u22121 , p k ) (1) This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967) . There is a caveat, however. Recall that the post sequences are generated from a thread. Since a test post may appear in more than one sequence, different occurrences of it may be assigned different stance labels by the MEMM. To determine the final stance label for the post, we average the probabilities assigned to the for stance over all its occurrences; if the average is \u2265 0.5, then its final label is for; otherwise, its label is against. System based on Joint Inference One weakness of the pipeline systems is that errors may propagate from the SC system to the RC system. If the stance of a post is incorrectly labeled, its reasons will also be incorrectly labeled. To avoid this problem, we employ joint inference. Specifically, we first train a SC system and a RC system independently of each other. We employ the Baseline as our RC system, since this is the only RC system that is not stance-specific. For the SC system, we employ P2. Since the SC system and the RC system are trained independently of each other, their outputs may not be consistent. For instance, an inconsistency arises if a post is labeled as for but one or more of its reasons are associated with the opposing stance. In fact, an inconsistency can arise in the output of the RC system alone: reasons associated with both stances may be assigned by the RC systems to different sentences of a given post. To enforce consistency, we apply integer linear programming (ILP) (Roth and Yih, 2004 ). We formulate one ILP program for each debate post. Each ILP program contains two post-stance variables (x f or and x against ) and |T | * |L R | reason variables (i.e., one indicator variable z t,r for each reason class r and each sentence t), where |T | is the number of sentences in the post and |L R | is the number of reason labels. Our objective is to maximize the linear combination of these variables and their corresponding probabilities assigned by their respective classifiers (see (2) below) subject to two types of constraints, the integrity constraints and the post-reason constraints. The integrity constraints ensure that each post is assigned exactly one stance and each sentence in a post is assigned exactly one reason class (see the two equality constraints in (3)). The post-reason constraints ensure consistency between the predictions made by the SC and the RC systems. Specifically, (1) if there is at least one reason supporting the for stance, the post must be assigned a for label; and (2) a for post must have at least one for reason. These constraints are defined for the against label as well (see the constraints in (4)). Maximize: s\u2208L S a s x s + 1 |T | |T | t=1 r\u2208L R b t,r z t,r (2) subject to: s\u2208L S x s = 1, \u2200 t r\u2208L R z t,r = 1, x s \u2208 {0, 1}, z t,r \u2208 {0, 1} (3) \u2200 t x s \u2265 z t,r , |T | t=1 z t,r \u2265 x s (4) Note that (1) a s and b t,r are two sets of probabilities assigned by the SC and RC systems respectively; (2) L S and L R denote the set of stance labels and reason labels respectively; and (3) the fraction 1 |T | ensures that both classifiers are contributing equally to the objective function. Systems based on Joint Learning Another way to avoid the error propagation problem in pipeline systems is to perform joint learning. In joint learning, the two tasks, SC and RC, are learned jointly. Below we propose three joint models in increasing level of sophistication. J1 is a joint model that, given a test post p, finds the stance label s and the reason label for each of the sentences that together maximize the probability P (R p , s|p), where R p = (r 1 , r 2 , . . . , r n ) is the sequence of reason labels with r i (1 \u2264 i \u2264 n) being the reason label assigned to t i , the i-th sentence in p. Using Chain Rule, P (R p , s|p) = P (s|p)P (R p |s, p) = P (s|p) n i=1 P (r i |s, t i ) (5) Hence, P (R p , s|p) can be computed by using the stance-specific RC classifier and the SC classifier employed in P1. The second joint model, J2, is the same as J1, except that we recast SC as a sequence labeling task. As before, we employ MEMM to learn how to predict stance sequences. Given a post sequence P S = (p 1 , p 2 , . . . , p n ), J2 finds the stance sequence S = (s 1 , s 2 , . . . , s n ) and reasons R = (R 1 , R 2 , . . . , R n ) that jointly maximize P (R, S|P S ). Note that R i is the sequence of reason labels assigned to the sentences in post i. The R and S that jointly maximize P (R, S|P S ) can be found efficiently via DP, using a modified version of the Viterbi algorithm. Unlike in P2, in J2 the decoding process is slightly more complicated because we have to take into account R i . Below we show the recursive definitions used to compute the entries in the DP table, where v k (h) is the (k,h)-th entry of the table; P (h|p) is provided by the MaxEnt stance classifier used in P1; P (h|j, p) is provided by the MEMM stance classifier used in P2; P (r max i |h, t i ) is provided by the stancespecific reason classifier used in the pipeline systems; and r max i is the reason label for sentence t i that has the highest probability according to the reason classifier. Base case: v 1 (h) = P (h|p) n i=1 P (r max i |h, t i ) (6) Recursive definition: v k (h) = max j v k\u22121 (j)P (h|j, p) n i=1 P (r max i |h, t i ) (7) To motivate our third joint model, J3, we make the following observation. Recall that a post in a post sequence is a reply to its preceding post. An inspection of the training data reveals that in many cases, a reply is a rebuttal to the preceding post, where an author attempts to argue why the points or reasons raised in the preceding post are wrong and then provides her reasons for the opposing stance. Motivated by this observation, we hypothesize that the reasons mentioned in the preceding post could be useful for predicting the reasons in the current post. However, none of the models we have presented so far makes use of the reasons predicted for the preceding post. This motivates the design of J3, which we build on top of J2. Specifically, to incorporate the reason labels predicted for the preceding post in a post sequence, we augment the feature set of the stancespecific reason classifiers with a set of reason features, with one binary feature for each reason. The value of a reason feature is 1 if and only if the corresponding reason is predicted to be present in the preceding post. Hence, in J3, we can apply the same DP equations we used in J2 except that the set of features used by the reason classifier is augmented with the reason features. Evaluation While our primary goal is to evaluate the RC systems introduced in the previous section, we are also interested in whether SC performance can improve when SC is jointly modeled with RC. More specifically, our evaluation is driven by the following question: will RC performance and SC performance improve as we employ more sophisticated methods for modeling reasons and stances? Before showing the results, we describe the metrics for evaluating RC and SC systems. Table 3 : SC accuracies and RC F-scores for our five-fold cross-validation experiments. System Experimental Setup We express SC results in terms of accuracy (i.e., the percentage of test posts labeled with the correct stance) and RC results in terms of F-score micro-averaged over all reason classes except the NONE class. For each RC system, we report its sentence-level RC score and post-level RC score, which are computed over sentences and posts respectively. As mentioned at the end of Section 3, the set of post-level reason labels of a given post is automatically obtained by taking the union of the set of reason labels assigned to each of its sentences. Hence, a reason classifier will be rewarded as long as it can predict, for any sentence in a test post, a reason label that the annotators assigned to some sentence in the same post. We obtain these scores via five-fold crossvalidation experiments. During fold partition, all posts that are in the same post sequence are assigned to the same fold. All reason and stance classifiers are domain-specific, meaning that each of them is trained on sentences/posts from exactly one domain and is applied to classify sentences/posts from the same domain. We use the Stanford maximum entropy classifier 4 for classification and solve ILP programs using lpsolve 5 . Results and Discussion Results are shown in Table 3 . Each row corresponds to one of our seven RC systems, showing its SC accuracy as well as its sentence-and postlevel RC F-scores for each domain. Let us begin by discussing the RC results. First, P1 and P2 significantly beat the Baseline on all four domains by an average of 1.4 and 3.1 points at the sentence level and by an average of 2.3 and 3.8 points at the post level respectively. 6 These results show that stance information can indeed be profitably used for RC even if it is incorporated into RC systems in a simple manner. Second, improving SC through sequence learning can improve RC: the systems in which SC is recast as sequence labeling (P2 and J2) perform significantly better than the corresponding systems that do not (P1 and J1). Third, ILP significantly beats P2 on two domains (ABO and GAY) and achieves the same level of performance as P2 on the remaining domains. These results suggest that joint inference is no worse (and sometimes even better) than pipeline learning as far as exploiting stance information for RC is concerned. Fourth, the systems trained via joint learning (J1 and J2) beat their corresponding pipeline counterparts (P1 and P2) on all four domains, significantly so by an average of 2.3 and 2.5 points at the sentence level and by an average of 2.0 and 2.6 points at the post level respectively, suggesting that joint learning is indeed a better way to incorporate stance information than pipeline learning. Finally, J3, the joint system that exploits reasons predicted for the previous post, significantly beats J2, the system on which it is built, by 1.6 and 1.8 points at the sentence level and by 1.7 and 1.3 points at the post level for ABO and GAY respectively. It also yields small, statistically insignificant, improvements (0.6 points at the sentence level and 0.6-0.9 points at the post level) for the remaining two domains. These results suggest that the reasons predicted for the previous post indeed provide useful information for predicting the current post's reasons. Overall, these results are consistent with our hy-pothesis that the usefulness of stance information depends in part on the way it is exploited, and that RC performance increases as we employ more sophisticated methods for modeling reasons and stances. Our best system, J3, significantly beats the Baseline by an average of 6.7 and 7.5 points at the sentence and post levels respectively. As mentioned earlier, a secondary goal of this work is to determine whether joint modeling can improve SC as well. For that reason, we compare the performances of the best pipeline model (P2) and the best joint model (J3) on each domain. We find that in terms of SC accuracy, J3 is significantly better than P2 on ABO and GAY, and yields slightly, though insignificantly, better performance on the remaining two domains. In other words, our results suggest that joint modeling of SC and RC has a positive impact on SC performance on all domains, and the impact can sometimes be large enough to yield significantly better results. Further Comparison We hypothesized in the introduction that the sentence-based approach to post-level RC would yield better performance than the multi-label text classification approach. In Section 5.2, we presented results of the sentence-based approach to RC. So, to test this hypothesis, we next evaluate the multi-label text classification approach to RC. Recall that the multi-label text classification approach assumes the following setup. Given a set of training posts where each post is multi-labeled with the set of reasons it contains, the goal is to train a system to determine the set of reasons a test post contains. Hence, unlike in the sentencebased approach, in this approach no sentence-level reason annotations are exploited during training. We implement this approach by recasting multilabel text classification as n binary text classification tasks, where n is the number of reason classes for a domain. In the binary classification task for predicting reason i, we train a binary classifier c i using the one-versus-all training scheme. Specifically, to train c i , we create one training instance for each post p in the training set, labeling it as positive if and only if p contains reason i. Note that if i is a minority reason, the class distribution of the resulting training set will be highly skewed towards the negatives, which will in turn cause the resulting MaxEnt classifier to be biased towards predicting a test instance as negative. To address this problem, we adjust the classification thresholds associated with the binary classifiers. Recall that a test instance is classified as positive by a binary classifier if and only if its probability of belonging to the positive class is above the classification threshold used. Hence, adjusting the threshold amounts to adjusting the number of test instances classified as positive, thus addressing the bias problem mentioned above. Specifically, we adjust the thresholds of the classifiers as follows. We train the binary classifiers to optimize the overall F-score by jointly tuning their classification thresholds on 25% of the training data reserved for development purposes. Since computing the exact solution to this optimization problem is computationally expensive, we employ a local search algorithm that changes the value of one threshold at a time to optimize F-score while keeping the remaining thresholds fixed. During testing, classifier c i will classify a test instance as positive if its probability of belonging to the positive class is above the corresponding threshold. We apply this multi-label text classification approach to obtain post-level RC scores for the Baseline, P1 and P2. Note that since P1 and P2 are pipeline systems, the binary classifiers they use to predict a test post's reasons depend on the post's predicted stance. Specifically, if a test post is predicted to have a positive (negative) stance, then only the reason classifiers associated with the positive (negative) stance will be used to predict the reasons it contains. On the other hand, this approach cannot be used in combination with ILP or the joint models to produce post-level RC scores: they all require a reason classifier trained on reason-annotated sentences, which are not available in the multi-label text classification approach. Post-level RC results of the Baseline and the two pipeline systems, P1 and P2, obtained via this multi-label text classification approach are shown in Table 4 . These scores are significantly lower than the corresponding scores in Table 3 by 3.2, 2.9, and 3.1 points for the Baseline, P1, and P2 respectively, when averaged over the four domains. They confirm our hypothesis that the sentencebased approach to post-level RC is indeed better than its multi-class text classification counterpart. Error Analysis To get a better understanding of our bestperforming RC system (J3), we examine its major System ABO GAY OBA MAR Baseline 39.8 37.9 sources of error in this subsection. For the four domains, 75-83% of the errors can be attributed to the system's inability to decide whether a sentence describes a reason or not. Specifically, in 51-54% of the erroneous cases, a reason sentence is misclassified as NONE. On the other hand, 23-30% of the cases are concerned with assigning a reason label to a NONE sentence. The remaining 17-25% of the errors concern mislabeling a reason sentence with the wrong reason. A closer examination of the errors reveals that they resulted primarily from (1) the lack of access to background knowledge, (2) the failure to process complex discourse structures, and (3) the failure to process sarcastic statements and rhetorical questions. We present two examples for each of these three major sources of error from the ABO and OBA domains in Table 5 . In each example, we show their predicted (P) and gold (G) labels. Lack of access to background knowledge. Consider the first example for ABO in Table 5 . Our system misclassifies this sentence in part because it lacks the background knowledge that \"genetic code\" is one of the characteristics of life and a fetus having it means a fetus has life (A3). Similarly, the system cannot determine the reason for the first OBA example without the knowledge that \"deficit spending\" is a term related to the economy and that increasing it is bad (A1). We believe some of these relations can be extracted from lexical knowledge bases such as YAGO2 (Suchanek et al., 2007) , Freebase (Bollacker et al., 2008) , and BabelNet (Navigli and Ponzetto, 2012) . Failure to process complex discourse structures. Our system misclassifies the second example for ABO in part because the first part of the sentence (i.e., Sure, the fetus has the potential to one day be a person) expresses a meaning that is completely inverted by the second part. Such complex discourse structures often lead to classification errors even for sentences whose interpretation requires no background knowledge. We believe that this problem can be addressed in part by a better understanding of the structure of a discourse, particularly the relation between two discourse segments, using a discourse parser. Failure to process sarcastic statements and rhetorical questions. Owing to the nature of our dataset (i.e., debate posts), many errors arise from sentences containing sarcasm and/or rhetorical questions. This is especially a problem in long post sequences, where authors frequently restate their opponents' positions, sometimes ironically. A first step towards handling these errors would therefore be to identify sentences containing sarcasm and/or rhetorical questions. Related Work In this section, we discuss related work in the areas of document-level RC, argument recognition, textual entailment in online debates, argumentation mining, and sentiment analysis. Document-level reason classification. Persing and Ng (2009) apply a multi-label text classification approach to document-level RC of aviation safety incident reports. Given a set of pre-defined reasons, their RC system seeks to identify the reasons that can explain why the incident described in a given report occurred. Their work is different from ours in at least two respects. First, while our posts occur in post sequences (which can be profitably exploited in RC, for example, as in J3), their incident reports were written independently of each other. Second, they do not perform sentence-level RC, as the lack of sentence-level reason annotations in their dataset prevented them from training a sentence-level reason classifier. Argument recognition. Boltu\u017ei\u0107 and \u0160najder (2014) propose a multi-class classification task called argument recognition in online discussions. Given a post and a reason for a particular domain, the task is to predict the extent to which the author of the post supports or opposes the reason as measured on a five-point ordinal scale ranging from \"explicitly supports\" to \"explicitly opposes\". Hence, unlike RC, argument recognition focuses on the magnitude rather than the existence of a post-reason relationship. In addition, Boltu\u017ei\u0107 and \u0160najder focus on post-level (rather than sentence-level) classification and employ perfect (rather than predicted) stance information. Textual entailment in online debates. Given the title of a debate and a post written in response to it, this task seeks to detect arguments in  the post that entail or contradict the title (Cabrio and Villata, 2012) . Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001) , product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012) , newspaper articles (Feng and Hirst, 2011) , and legal documents (Br\u00fcninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013) . A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper's or some other papers' approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moens, 2000) . The argument types for legal documents refer to legal factors which are either pro-plaintiff or pro-defendant (Br\u00fcninghaus and Ashley, 2005) . For instance, for trade secret law cases, factors such as Waiver-of-Confidentiality and Disclosure-in-Public-Forum refer to certain facts strengthening the claim of one of the sides participating in a case. Sentiment analysis. RC resembles certain tasks in sentiment analysis. One such task is pro and con reason classification in reviews (Kim and Hovy, 2006) , where sentences containing opinions as well as reasons justifying the opinions are to be extracted and classified as PRO, CON, or NONE. Hence, this task focuses on categorizing sentences into coarse-grained, high-level groups (e.g., PRO vs. CON, POSITIVE vs. NEGATIVE), but does not attempt to subcategorize the PRO and CON classes into fine-grained reason classes, unlike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author's opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010) , Trivedi and Eisenstein (2013) ) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004) , Wiebe and Riloff (2005), McDonald et al. (2007) , Zhao et al. (2008) ). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. Nevertheless, reason classification remains a challenging task: the best post-level F-scores are in the low 50s. By making our corpus publicly available, we hope to stimulate further research on this task. Acknowledgments We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft of this paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of NSF.",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.7248692030423426,
        "foundation": 0.0,
        "none": 1.0280378716087668e-06
    }
}