{
    "article": "Instead of translating sentences in isolation, document-level machine translation aims to capture discourse dependencies across sentences by considering a document as a whole. In recent years, there have been more interests in modelling larger context for the state-of-the-art neural machine translation (NMT). Although various document-level NMT models have shown significant improvements, there nonetheless exist three main problems: 1) compared with sentence-level translation tasks, the data for training robust document-level models are relatively low-resourced; 2) experiments in previous work are conducted on their own datasets which vary in size, domain and language; 3) proposed approaches are implemented on distinct NMT architectures such as recurrent neural networks (RNNs) and self-attention networks (SANs). In this paper, we aim to alleviate the low-resource and under-universality problems for document-level NMT. First, we collect a large number of existing document-level corpora, which covers 7 language pairs and 6 domains. In order to address resource sparsity, we construct a novel document parallel corpus in Chinese-Portuguese, which is a non-English-centred and low-resourced language pair. Besides, we implement and evaluate the commonly-cited document-level method on top of the advanced Transformer model with universal settings. Finally, we not only demonstrate the effectiveness and universality of document-level NMT, but also release the preprocessed data, source code and trained models for comparison and reproducibility. Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) . Conventional NMT models still translate a text by considering isolated sentences, which may harm translation quality especially in terms of coherence, cohesion, and consistency (Webber, 2014) . To address this problem, documentlevel NMT has attracted increasing attention from the community (Wang et al., 2017a; Jean et al., 2017; Kuang et al., 2017b; Bawden et al., 2018; Maruf and Haffari, 2018; Tu et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2018; Xiong et al., 2019) . Researchers have investigated a variety of approaches to model cross-sentence context for NMT and shown promising results in terms of BLEU score (Papineni et al., 2002) as well as human evaluation (L\u00e4ubli et al., 2018) . For instance, Wang et al. (2017a) tried early attempt proposing a hierarchical neural encoder to summarize context in previous sentences and then integrate the historical representations into standard NMT with three effective strategies. Although such approaches have achieved significant improvements, there nonetheless exist three main drawbacks that may restrict development of document-level NMT. First, parallel documents are too scarce to build robust document-level NMT models. Taking Chinese-English translation task for example, the WMT2019 sentence-level corpus contains 25 million sentence pairs while the size of IWSLT2017 document-level corpus is only 200 thousands. Second, experiments in previous work are usually conducted on their own datasets which vary in size, domain and language. For example, Wang et al. (2017a) only evaluated their hierarchical model on 1 million Chinese-English LDC corpus in the news domain, while (Jean et al., 2017) mainly verified the multi-encoder method with 200 thousands German-English IWSLT dataset in the spoken domain. Third, different document-level NMT models are implemented on distinct architectures including recurrent neural networks (RNN) (Bahdanau et al., 2015) and self-attention networks (SAN) (Vaswani et al., 2017) . Consequently, it is difficult to robustly build document-level models and fairly compare different approaches across different datasets or architectures. In this paper, we aim to alleviate the low-resource and under-universality problems for document-level NMT. More specifically, we collect a large number of existing corpora for document-level translation task and process them using unified preprocessing steps. As a result, the collected corpora contain 7 language pairs (e.g. Chinese-English, English-French, Estonian-English etc.) in 6 domains (e.g. news, subtitle, parliament etc.). In addition, we construct a new document-level corpus in Chinese-Portuguese, which is low-resourced and non-English-centred language pair. Finally, we implement a commonly-used document-aware approach (Wang et al., 2017a; Jean et al., 2017; Voita et al., 2018) on top of a state-of-the-art SAN-based NMT model -Transformer. Experiments are systematically conducted on all collected and built data using Transformer models. Results confirm that the document-level information is indeed useful to NMT. For better comparison and reproducibility, we release the preprocessed data, 1 source code and trained models. We hope this work can be used as a benchmark for other researchers to further improve document-level NMT. The contributions of this paper are listed as follows: \u2022 We investigate a variety of the document-level corpora for NMT, which confirms the superiority of modelling larger context; \u2022 We build a novel document-level parallel corpus for low-resourced language pair, and experiments show that document context is more helpful to long-distant languages. \u2022 For better comparison and reproducibility, we release the preprocessed data, source code and trained models. The rest of the paper is organized as follows. In Section 2, we introduce background of NMT as well as the documentlevel NMT model. The details of document-level parallel corpora are described in Section 3. The experimental results of translation task using different data are reported in Section 4. Related work are given in Section 5. Finally, Section 6 presents our conclusions and future work. Background Neural Machine Translation A standard NMT model directly optimizes the conditional probability of a target sentence y = y 1 , . . . , y J given its corresponding source sentence x = x 1 , . . . , x I : P (y|x; \u03b8) = J j=1 P (y j |y <j , x; \u03b8) (1) where \u03b8 is a set of model parameters and y <j denotes the partial translation. The probability P (y|x; \u03b8) is defined on the neural network based encoder-decoder framework (Sutskever et al., 2014; Cho et al., 2014) , where the encoder summarizes the source sentence into a sequence of representations H = H 1 , . . . , H I with H \u2208 R I\u00d7d , and the decoder generates target words based on the representations. Typically, this framework can be implemented as recurrent neural network (RNN) (Bahdanau et al., 2015) , convolutional neural network (CNN) (Gehring et al., 2017) and Transformer (Vaswani et al., 2017) . Among the different models, the Transformer has emerged as the dominant NMT paradigm. In this study, we re-implement the baseline and document-level models on top of Transformer. The parameters of the NMT model are trained to maximize the likelihood of a set of training examples D = {[x m , y m ]} M m=1 : L(\u03b8) = arg max \u03b8 M m=1 log P (y m |x m ; \u03b8) (2) which is used as a sentence-level baseline in this work. Motivation As shown in Section 2.1, the standard NMT usually models a text by considering isolated sentences based on a strict assumption that the sentences in a text are independent of one another. However, disregarding dependencies across sentences will negatively affect translation outputs of a text in terms of discourse properties. Coherence, cohesion, and consistency are three main properties of discourse. Cohesion occurs whenever \"the interpretation of some element in the discourse is dependent on that of another\" (Halliday and Hasan, 1976) , which refers to various manifest linguistic links (e.g. references, word repetitions) between sentences within a text that holds the text together. Coherence is created referentially, when different parts of a text refer to the same entities, and relationally, by means of coherence relations such as \"Cause-Consequence\" between different discourse segments. Consistency is another critical issue in document-level translation, where a repeated term should keep the same translation throughout the whole document. The underlying assumption is that the same concepts should be consistently referred to with the same words in a translation. Recent studies have shown that incorporating documentlevel is helpful to translations in terms of coherence (Wang et al., 2016b; Xiong et al., 2019) , cohesion (Wang et al., 2016a; Voita et al., 2018; Wang et al., 2018b; Wang et al., 2018a) , and consistency (Xiao et al., 2011; Wang et al., 2017a; Wang et al., 2019) . This motivated us to continue exploit document-level NMT. Document-Level Neural Machine Translation This task aims to consider both the current sentence and its large context in a unified model to improve translation performances, especially in terms of discourse properties. o h i = ATT(q h i , K h )V h \u2208 R d H (3) where h is one of H heads. Q, K and V respectively represent queries, keys and values, which are calculated as: Q, K, V = XW Q , XW K , XW V \u2208 R I\u00d7d (4) where {W Q , W K , W V } \u2208 R \u03bb d = \u03c3(W \u03bb [O d , \u00d4d ] + b d ) (5) O = \u03bb d O d + (1 \u2212 \u03bb d ) \u00d4d (6) in which \u03c3(\u2022) is the logistic sigmoid function and W \u03bb is the parameter. O is the final document-level representation, which is further fed into NMT decoder. Following Voita et al. (2018) , we share the parameters of context encoders and embedding with those of standard NMT encoder. Document-Level Parallel Corpora We reviewed a large number related work on documentlevel NMT (Wang et al., 2017a; Jean et al., 2017; Kuang et al., 2017b; Bawden et al., 2018; Maruf and Haffari, 2018; Tu et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2018; Xiong et al., 2019) , and collected existing data used in their experiments. In addition, we also built a new document-level Chinese-Portuguese corpus based on our previous work Liu et al. (2018) . Table 1 lists all document-level parallel corpora, each of which differs from others in language, topic, genre, style, level of formality, etc. OpenSubtitle This is a collection of translated movie subtitles (Lison and Tiedemann, 2016) , which are usually simple and short. Most of the translations of subtitles do not preserve the syntactic structures of their original sentences at all. We randomly select two episodes as the tuning set, and the other two episodes as the test set. It totally contains 62 language pairs and here we mainly exploited commonly-cited French-English, Spanish-English and Russian-English. 3  IWSLT The corpora are from the machine translation track on TED Talks of IWSLT (Cettolo et al., 2012) . Koehn and Knowles (2017) point out that NMT systems have a steeper learning curve with respect to the amount of training data, resulting in worse quality in low-resource settings. Collected Corpora The TED talks are difficult to translate given the variety of topics in quite small-scale training data. We choose the \"dev2010\" dataset as the tuning set, and the combination of \"tst2010-2013\" datasets as the test set. It totally contains nearly 100 language pairs while here we mainly investigated commonly-used Chinese-English, French-English, Spanish-English and German-English. 4  News Commentary The corpus was created as training data resource for the Conference for Statistical Machine Translation Evaluation Campaign and consists of political and economic commentary crawled from the web site Project Syndicate (Koehn, 2005) . Different from LDC, this corpus mainly focuses on political/economic news with more language pairs. It totally contains 12 language pairs and here we evaluated Spanish-English and German-English. Preprocessing To preprocess the raw data, we use a series of scripts including: full/half-width conversion, Unicode conversion, simplified/traditional Chinese conversion, punctuation normalization, tokenization and sentence boundary detection, letter casing and word stemming (Wang et al., 2016b) . To the end, we employ these methods to uniformly preprocess data described in Section 3.1 and 3.2. Note that, we keep the contextual information for document-level tasks while use only single sentences for sentence-level tasks. Experiment Setup For fair comparison, we implemented baseline and document-level NMT model on the advanced Transformer model (Vaswani et al., 2017) using the open-source toolkit Fairseq (Ott et al., 2019) . We followed Vaswani et al. (2017) to set the configurations of the NMT model, which consists of 6 stacked encoder/decoder layers with the layer size being 512. All the models were trained on 8 NVIDIA P40 GPUs where each was allocated with a batch size of 4,096 tokens. We trained the baseline model for 100K updates using Adam optimizer (Kingma and Ba, 2015) , and the proposed models were further trained with corresponding parameters initialized by the pre-trained baseline model. We fixed the hyperparameters \u03bb and \u03b4 as 0.1. According to previous studies (Wang et al., 2017a; Tu et al., 2018) , we modeled previous K = 3 sentences as document contexts for each current sentence. For the additional encoder, we use the same settings with the standard one as introduced in Figure 1 . We trained standard NMT models on sentence-level data as our baselines (Base) and built document-level NMT models (\"DNMT\") using parallel documents as discussed in Section 3.3. Furthermore, we used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. Results Table 2 shows translation results on different data as described in Section 3. The multi-encoder model (\"DNMT\") is trained and evaluated on document-level data while the baseline model is trained and evaluated on the corresponding data, which are broken into sentence level. As seen, the document-level models significantly improve the translation quality in all cases, although there are considerable differences among different scenarios. Experimental results confirm that the document-level information indeed improves translation performances on various corpora. Corpus In formal domain of corpora such as LDC, News Commentary, Europarl and MacaoGov, the document-level model achieves larger improvements over the Transformer baseline (+0.7 \u223c +1.7 BLEU points). Taking news domain for example, one entity word usually needs to keep consistent translation across the whole document in newswire. Thus, the gains mainly come from better translation consistency contributed by document context. However, in informal domain such as OpenSubtitle and TV-Sub, the improvements are relatively smaller (+0.2 \u223c +0.6 BLEU point) compared with formal domains. As known that, informal data such as dialogue are often difficult to translate due to a lot of discourse phenomena such zero anaphora (Wang et al., 2016a) . Another reason maybe that K = 3 previous sentences are not enough to recall missing information in the context. In our experiments, we also evaluated their performances using different matrices, including METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2009) . We found that the trends are simlar to BLEU scores. Related Work On Document-Level Translation In early studies, document-aware approaches have been investigated for statistical machine translation (SMT) (Tiedemann, 2010; Gong et al., 2011; Xiao et al., 2011; Hardmeier et al., 2012) . In recent years, context-aware architecture has been well studied for NMT (Wang et al., 2017a; Jean et al., 2017; Tu et al., 2018) . Wang et al. (2017a) proposed hierarchical recurrent neural networks to summarize inter-sentential context from previous sentences and then integrate it into a standard NMT model with difference strategies. Jean et al. (2017) introduced an additional set of an encoder and attention to encode and select part of the previous source sentence for generating each target word. Besides, Tu et al. (2018) proposed to augment NMT models with a cachelike memory network, which stores the translation history in terms of bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multiattention model. On Discourse Phenomena More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019) . They mainly exploited general anaphora in non-pro-drop languages such as English\u21d2Russian. In order to evaluate discourse phenomena, Bawden et al. (2018) conducted experiments from three aspects: 1) comparing multi-encoder models (Zoph and Knight, 2016; Jean et al., 2017) with different strategies; 2) investigating the impacts of source-and target-side history information on NMT; 3) presenting a novel evaluation through the use of two discourse test sets targeted at coreference and lexical coherence/cohesion. Voita et al. (2018) introduced a context-aware model and demonstrated its usefulness for anaphora resolution as well as translation. Besides, Xiong et al. (2019) proposed to use discourse context and reward to refine the translation quality from the perspective of coherence. Some researchers proposed to extend the Transformer model to take advantage of document-level context (Miculicich et al., 2018; Zhang et al., 2018) . Following Tu et al. ( 2018 )'s work, Kuang et al. (2017a) and Maruf and Haffari (2018) continue to exploit cache memory for improving the performance of document-level NMT. Through human evaluation, L\u00e4ubli et al. (2018) found that document-level evaluation for MT can improve to discriminate the errors which are hard or impossible to spot at the sentence level. Conclusion and Future Work In this paper we collected and preprocessed a large number of corpora for document-level translation task. Besides, we implemented and evaluated the document-aware approach on top of a universal NMT model -Transformer. We also construct an additional corpus in a novel language pair (Chinese-Portuguese). We conduct experiments on existing and the curated corpora, and compare the performance of different NMT models using these corpora. Results showed that document contexts are more useful to formal domains than informal ones. We hope this work can be used by MT research for further improving document-level translation. In the future, we will investigate more document-level approaches such as the hierarchical encoder proposed by Wang et al. (2017a; Miculicich et al. (2018) . Furthermore, we will continue to exploit Zhang et al. ( 2018 )'s training strategy to make full use of large-amount sentence-level data. Acknowledgements This work is supported by the XJTLU KSF project (Grant Number: KSF-E-24) and GDUFS open project (Grant Number:CTS201501). The authors also wish to thank the anonymous reviewers for many helpful comments. Automatic construction of discourse corpora for dialogue translation. In LREC. Wang, L., Tu, Z., Way, A., and Liu, Q. (2017a) . Exploiting cross-sentence context for neural machine translation. In EMNLP. Wang, R., Finch, A., Utiyama, M., and Sumita, E. (2017b) . Sentence embedding for neural machine translation domain adaptation. In ACL. Wang, L., Tu, Z., Shi, S., Zhang, T., Graham, Y., and Liu, Q. (2018a) . Translating pro-drop languages with reconstruction models. In AAAI. Wang, L., Tu, Z., Way, A., and Liu, Q. (2018b) . Learning to jointly translate and predict dropped pronouns with a shared reconstruction mechanism. In EMNLP. Wang, L., Tu, Z., Wang, X., and Shi, S. (2019) . One model to learn both: Zero pronoun prediction and translation. In EMNLP. Webber, B. (2014). Discourse for machine translation. In Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing. Xiao, T., Zhu, J., Yao, S., and Zhang, H. ( 2011 ). Document-level consistency verification in machine translation. In MT Summit. Xiong, H., He, Z., Wu, H., and Wang, H. (2019) . Modeling coherence for discourse neural machine translation. In AAAI. Zhang, J., Luan, H., Sun, M., Zhai, F., Xu, J., Zhang, M., and Liu, Y. (2018) . Improving the transformer translation model with document-level context. In EMNLP. Zoph, B. and Knight, K. (2016) . Multi-source neural translation. In NAACL.",
    "abstract": "Instead of translating sentences in isolation, document-level machine translation aims to capture discourse dependencies across sentences by considering a document as a whole. In recent years, there have been more interests in modelling larger context for the state-of-the-art neural machine translation (NMT). Although various document-level NMT models have shown significant improvements, there nonetheless exist three main problems: 1) compared with sentence-level translation tasks, the data for training robust document-level models are relatively low-resourced; 2) experiments in previous work are conducted on their own datasets which vary in size, domain and language; 3) proposed approaches are implemented on distinct NMT architectures such as recurrent neural networks (RNNs) and self-attention networks (SANs). In this paper, we aim to alleviate the low-resource and under-universality problems for document-level NMT. First, we collect a large number of existing document-level corpora, which covers 7 language pairs and 6 domains. In order to address resource sparsity, we construct a novel document parallel corpus in Chinese-Portuguese, which is a non-English-centred and low-resourced language pair. Besides, we implement and evaluate the commonly-cited document-level method on top of the advanced Transformer model with universal settings. Finally, we not only demonstrate the effectiveness and universality of document-level NMT, but also release the preprocessed data, source code and trained models for comparison and reproducibility.",
    "countries": [
        "China"
    ],
    "languages": [
        "German",
        "English",
        "French",
        "Chinese",
        "Russian",
        "Spanish"
    ],
    "numcitedby": "4",
    "year": "2020",
    "month": "May",
    "title": "Corpora for Document-Level Neural Machine Translation"
}