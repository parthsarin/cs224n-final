{
    "article": "Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. Introduction Neural machine translation (NMT) is susceptible to coverage errors such as the addition of superfluous target words or the omission of important source content. Previous approaches to detecting such errors make use of reference translations (Yang et al., 2018) or employ a separate quality estimation (QE) model trained on synthetic data for a language pair (Tuan et al., 2021; Zhou et al., 2021) . In this paper, we propose a reference-free algorithm based on hypothetical reasoning. Our premise is that a translation has optimal coverage if it uses as little information as possible and as much information as necessary to convey the source sequence. Therefore, an addition error means that the source would be better conveyed by a translation containing less information. Conversely, an omission error means that the translation would be more adequate for a less informative source sequence. Adapting our contrastive conditioning approach (Vamvas and Sennrich, 2021) , we use probability scores of NMT models to approximate this concept of coverage. We create parse trees for both the source sequence and the translation, and treat their constituents as units of information. Omission errors are detected by systematically deleting constituents from the source and by estimating the probability of the translation conditioned on such a partial source sequence. If the probability score is higher than when the translation is conditioned on the full source, the deleted constituent might have no counterpart in the translation (Figure 1 ). We apply the same principle to the detection of addition errors by swapping the source and the target sequence. When comparing the detected errors to human annotations of coverage errors on the segment level (Freitag et al., 2021) , our approach surpasses a supervised QE baseline that was trained on a large number of synthetic coverage errors. Human raters find that word-level precision is higher for omissions than additions, with 39% of predicted error spans being precise for English-German translations, and 20% for Chinese-English. False positive predictions can occur especially in cases where the translation has different syntax than the source. We believe our algorithm could be a useful aid whenever humans remain in the loop, for example in a post-editing workflow. We release the code and data to reproduce our findings, including a large-scale dataset of synthetic coverage errors in English-German and Chinese-English machine translations. 1 Related Work Coverage errors in NMT Addition and omission of target words have been observed by human evaluation studies in various languages, with omission as the more frequent error type (Castilho et al., 2017; Zheng et al., 2018) . They are included as typical translation issues in the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014) . Addition is defined as an accuracy issue where the target text includes text not present in the source, and omission is defined as an accuracy  ). An NMT model such as mBART50 assigns a higher probability score to Y conditioned on the source with after landing deleted than to Y conditioned on the full source (Step 3). This indicates that there is an omission error (Step 4). issue where content is missing from the translation but is present in the source. 2  Freitag et al. ( 2021 ) used MQM to manually re-annotate English-German and Chinese-English machine translations submitted to the WMT 2020 news translation task (Barrault et al., 2020) . Their findings confirm that state-of-the-art NMT systems still erroneously add and omit target words, and that omission occurs more often than addition. Similar patterns can be found in English-French machine translations that have been annotated with fine-grained MQM labels for the document-level QE shared task (Specia et al., 2018; Fonseca et al., 2019; Specia et al., 2020) . Extract constituents Detecting and reducing coverage errors While reference-based approaches include measuring the n-gram overlap to the reference (Yang et al., 2018) and analyzing word alignment to the source (Kong et al., 2019) , this work focuses on the reference-free detection of coverage errors. Previous work has employed custom QE models trained on labeled parallel data. For example, Zhou et al. ( 2021 ) insert synthetic hallucinations and train a Transformer to predict the inserted spans. Similarly, Tuan et al. ( 2021 ) train a QE model on synthetically noisy translations. In this paper, we propose a method that is based on off-the-shelf NMT models only. Other related work has focused on improving coverage during decoding or training, for example via attention (Tu et al., 2016; Wu et al., 2016; Li et al., 2018; among others) . More recently, Yang et al. (2019) found that contrastive fine-tuning on 2 The terms overtranslation and undertranslation have been used in the literature as well. MQM reserves these terms for errors where the translation is too specific or too unspecific. references with synthetic omissions reduces coverage errors produced by an NMT system. Approach Contrastive Conditioning Properties of a translation can be inferred by estimating its probability conditioned on contrastive source sequences (Vamvas and Sennrich, 2021) . For example, if a certain translation is more probable under an NMT model when conditioned on a counterfactual source sequence, the translation might be inadequate. Application to Omission Errors Figure 1 illustrates how contrastive conditioning can be directly applied to the detection of omission errors. We construct partial source sequences by systematically deleting constituents from the source. If the probability score of the translation (average token logprobability) is higher when conditioned on such a partial source, the deleted constituent is taken to be missing from the translation. To compute the probability score for a translation Y given a source sequence X, we sum up the log-probabilities for every target token and normalize the sum by the number of target tokens: score(Y |X) = 1 |Y | |Y | i=0 log p \u03b8 (y i |X, y <i ) Application to Addition Errors We apply the same method to addition detection, but swap the source and target languages. Namely, we use an NMT model for the reverse translation direction, and we score the source sequence conditioned on the full translation and a set of partial translations. 3 Potential Error Spans In its most basic form, our algorithm does not require any linguistic resources apart from tokenization. For a source sentence of n tokens one could create n partial source sequences with the ith token deleted. However, such an approach would rely on a radical assumption of compositionality, treating all tokens as independent constituents. We thus propose to extract potential error spans from parse trees, specifically from dependency trees predicted by Universal Dependency parsers (de Marneffe et al., 2021) , which are widely available. This allows (a) to skip function words and (b) to include a reasonable number of multiword spans in the set of potential error spans. Formally, we consider word spans that satisfy the following conditions: 1. A potential error span is a complete subtree of the dependency tree. 2. It covers a contiguous subsequence. It contains a part of speech of interest. For every potential error span, we create a partial sequence by deleting the span from the original sequence. This is still a simplified notion of constituency, since some partial sequences will be ungrammatical. Our assumption is that NMT models can produce reliable probability estimates despite the ungrammatical input. Experimental Setup In this section we describe the data and tools that we use to implement and evaluate our approach. Scoring model We use mBART50 (Tang et al., 2021) , which is a sequence-to-sequence Transformer pre-trained on monolingual corpora in many languages using the BART objective (Lewis et al., 2020; Liu et al., 2020) that was fine-tuned on English-centric multilingual MT in 50 languages. Sequence-level probability scores are computed by averaging the log-probabilities of all target tokens. We use the one-to-many mBART50 model if English is the source language, and the many-to-one model if English is the target language. Error spans We use Stanza (Qi et al., 2020) for dependency parsing, a neural pipeline for various languages trained on data from Universal Dependencies (de Marneffe et al., 2021) . We make use of universal part-of-speech tags (UPOS) to define ditioned on the source. However, the scores might be confounded by a lack of fluency in the partial translations. parts of speech that might constitute potential error spans. Specifically, we treat common nouns, proper nouns, main verbs, adjectives, numerals, adverbs, and interjections as relevant parts of speech. Gold Standard Data We use state-of-the-art English-German and Chinese-English machine translations for evaluation, which have been annotated by Freitag et al. (2021) with translation errors. 4 We set aside translations by the system Online-B as a development set, and use the other systems as a test set, excluding translations by humans. The development set was used to identify the typical parts-of-speech of coverage error spans, listed in the paragraph above. Synthetic Data We also create synthetic coverage errors, which we use for training a supervised baseline QE system. We propose a data creation process that is inspired by previous work (Yang et al., 2019; Zhou et al., 2021; Tuan et al., 2021) but is defined such that it works for both additions and omissions, and produces fluent translations. Figure 2 illustrates the process. We start from the original source sentences and create partial sources by deleting randomly selected constituents. Specifically, we delete each constituent with a probability of 15%. We then machine-translate both the original and the partial sources, yielding full and partial machine translations. We retain only samples where the full machine translation is different from the partial one, and can be constructed by addition. This allows us to treat the full translations as overtranslations of the partial sources, and the added words as addition errors. Conversely, the partial translations are treated as undertranslations of the original sources We average over three baseline models trained with different random seeds, reporting the standard deviation. ated by pairing the original sources with the full translations, and the partial sources with the partial translations. 5  Our synthetic data are based on monolingual news text released for WMT. 6 To train the baseline system, we use 80k unique source segments per language pair. Statistics are reported in Table A3 . Supervised baseline system Following the approach outlined by Moura et al. (2020) , we use the OpenKiwi framework (Kepler et al., 2019) to train a separate Predictor-Estimator model (Kim et al., 2017) per language pair, based on XLM-RoBERTa (Conneau et al., 2020) . The supervised task can be described as token-level binary classification. Every token is classified as either OK or BAD, similar to the word-level labels used for the QE shared tasks (Specia et al., 2020) . A source token is BAD if it is omitted in the translation, and a token in the translation is BAD if it is part of an addition error. For English and German, we use the Moses tokenizer (Koehn et al., 2007) to separate the text into labeled tokens; for Chinese we label the text on the character level. Where suitable, we use the default settings of OpenKiwi. We fine-tune the large version of XLM-RoBERTa, which results in a model of similar parameter count as the mBART50 model we use for contrastive conditioning. We train for 10 epochs with a batch size of 32, with early stopping on the validation set. For token classification we train two linear layers, separately for source and target language (which corresponds to omissions and additions, respectively). We use AdamW (Loshchilov and Hutter, 2019) with a learning rate of 1e-5, freezing the pretrained encoder for the first 1000 steps. Evaluation Segment-Level Comparison to Gold Data The accuracy of our approach can be estimated based on the human ratings by Freitag et al. (2021) . Evaluation Design We use the MQM error types Accuracy/Addition and Accuracy/Omission, and ignore other types such as Accuracy/Mistranslation. We count a prediction as correct if any one of the human raters has marked the same error type anywhere in the segment. 7 We exclude segments from the evaluation that might have been incompletely annotated (because raters stopped after marking five errors). For ease of implementation, we also exclude segments that consist of multiple sentences. Results The results of the gold-standard comparison are shown in Table 1 . Our approach clearly surpasses the baseline in the detection of omission errors in both language pairs. However, both approaches recognize addition errors with low accuracy, and especially the supervised baseline has low recall. Considering its high performance on a synthetic test set (Table A1 in the Appendix), it seems that the model does not generalize well to real-world coverage errors, highlighting the challenges of training a supervised QE model on purely synthetic data. Human Evaluation of Precision We perform an additional word-level human evaluation to analyze the predictions obtained via our approach in more detail. Our human raters were presented segments that had been marked as true or false positives in the above evaluation, allowing us to quantify word-level precision. Evaluation Design We employed two linguistic experts per language pair as raters. 8 Each rater was shown around 700 randomly sampled positive predictions across both types of coverage errors. EN-DE ZH-EN Raters were shown the source sequence, the machine translation, and the predicted error span. They were asked whether the highlighted span was indeed translated badly, and were asked to perform a fine-grained analysis based on a list of predefined answer options (Figures 3 and 4 in the Appendix). A part of the samples were annotated by both raters. The agreement was moderate for the main question, with a Cohen's kappa of 0.54 for English-German and 0.45 for Chinese-English. Agreement on the more subjective follow-up question was lower (0.32 / 0.13). Results The fine-grained answers allow us to quantify the word-level precision of the spans highlighted by our approach, both with respect to coverage errors in particular and to translation errors in general (Table 2 ). Precision is higher than expected when detecting omission errors in English-German translations, but is still low for additions. The distribution of the detailed answers (Figures 3 and 4 in the Appendix) suggests that syntactical differences between the source and target language contribute to the false positives regarding additions. Example predictions are provided in Appendix F, which include cases where all three raters of Freitag et al. (2021) had overlooked the coverage error. Finally, Table 2 shows that many of the predicted error spans are in fact translation errors, but not coverage errors in a narrow sense. For example, more than 10% of the spans marked in Chinese-English translations were classified by our raters as a different type of accuracy error, such as mistranslation. Limitations and Future Work We hope that the automatic detection of coverage errors could be an aid to translators and posteditors, given that manually detecting such errors is tedious. Our results on omissions are encouraging, and user studies are recommended in order to validate the usefulness of the predictions to practitioners. Further work needs to be done to improve the detection of additions, of which the real-world data contain few examples. Higher accuracy would be necessary for word-level QE to be helpful (Shenoy et al., 2021) , and so with regard to detecting addition errors, the practical utility of both the baseline and of our approach remains limited. Inference time should also be discussed. In Appendix C we perform a comparison, finding that on a long sentence pair contrastive conditioning can take up to ten times longer than a forward pass of the baseline. However, this is still a fraction of the time needed for generating a translation in the first place. In addition, restricting the potential error spans that are considered could further improve efficiency. Conclusion We have proposed a reference-free method to automatically detect coverage errors in translations. Derived from contrastive conditioning, our method relies on hypothetical reasoning over the likelihood of partial sequences. Since any off-the-shelf NMT model can be used to estimate conditional likelihood, no access to the original translation system or to a quality estimation model is needed. Evaluation on real machine translations shows that our approach outperforms a supervised baseline in the detection of omissions. Future work could address the low precision on addition errors, which are relatively rare in the datasets we used for evaluation. A Annotator Guidelines You will be shown a series of source sentences and translations. One or several spans in the text are highlighted and it is claimed that the spans are translated badly. You are asked to determine whether the claim is true. The highlighted spans can be either in the source sequence or in the translation. If a span is in the source sentence, check whether it has been correctly translated. If a span is in the translation, check whether it correctly conveys the source. Sometimes, multiple spans are highlighted. In that case, focus your answer on the span that is most problematic for the translation. In a second step, you are asked to select an explanation. On the one hand, if you agree that the highlighted span is translated badly, please explain your reasoning by selecting your explanation. On the other hand, if you disagree and think that the span is well-translated, please select an explanation why the span might have been marked as badly translated in the first place. Should multiple explanations be equally plausible, select the first from the top. A2 : Inference times when predicting on a short and a long sentence pair. Since we did not use a parser that is optimized for efficiency, we additionally report inference time without including the time needed for parsing. B Evaluation on Synthetic Errors We used a test split held back from the synthetic data to perform an additional evaluation. On the segment level, we report Precision, Recall and F1score. Like in Section 5.1, a prediction is treated as correct on the segment level if for a predicted coverage error there is indeed a coverage error of that type anywhere in the segment. On the word level, we follow previous work on word-level QE (Specia et al., 2020) and report the Matthews correlation coefficient (MCC) across all the tokens in the test set. Results Results are shown in Table A1 . The supervised baseline has a high accuracy on English-German translations and a moderate accuracy on Chinese-English translations. In comparison, our approach performs clearly worse than the supervised baseline on the synthetic errors. C Inference Time Inference times are reported in Table A2 . We measure the time needed to run the coverage error detection methods on a short sentence pair and on a long sentence pair for English-German. The short sentence pair is taken from Figure 1 and the long sentence pair has 40 tokens in the source sequence and 47 tokens in the target sequence. We average over 1000 repetitions on RTX 2080 Ti GPUs. The higher inference times for our approach can be explained by the number of translation probabilities that need to be estimated. On average, we compute 30 scores per sentence in the English-German MQM dataset, and 44 per sentence in the Chinese-English MQM dataset. Still, the time needed for computing all these scores is only a fraction of the time it takes to generate a translation (254 ms for the short source sentence and 861 ms for the long sentence, assuming a beam size of 5). The required number of scores could be reduced by considering fewer potential error spans. Furthermore, scoring could be parallelized across batches of multiple translations. Finally, using a more efficient parser, or no parser at all, could speed up inference. Acknowledgments This work was funded by the Swiss National Science Foundation (project MUTAMUR; no. 176727). We would like to thank Xin Sennrich for facilitating the recruitment of annotators, and Chantal Amrhein as well as the anonymous reviewers for helpful feedback. D Dataset Statistics E Examples of Synthetic Coverage Errors English-German Example Addition error Partial source: But they haven't played. Full machine translation: Aber sie haben nicht ::::: gegen ::: ein ::::: Team :::: wie :::: uns gespielt. Omission error Full source: But they haven't played :::::: against : a ::::: team :::: like :: us. Partial machine translation: Aber sie haben nicht gespielt. Chinese-English Example Addition error Partial source: \u533b\u9662\u548c\u4f01\u4e1a\u5171\u540c\u7814\u53d1\u76f8\u5173\u68c0\u6d4b\u8bd5\u5242\u76d2\uff0c\u60e0\u53ca\u66f4\u591a\u60a3\u8005\u3002 Full translation: Hospitals and enterprises jointly develop related test kits to benefit more :::::: cancer patients. Omission error Full source: \u533b\u9662\u548c\u4f01\u4e1a\u5171\u540c\u7814\u53d1\u76f8\u5173\u68c0\u6d4b\u8bd5\u5242\u76d2\uff0c\u60e0\u53ca\u66f4\u591a ::::: \u80bf\u7624\u60a3\u8005\u3002 Partial translation: Hospitals and enterprises jointly develop related test kits to benefit more patients. G Detailed Results of Human Evaluation EN-DE ZH-EN The span adds information that is supported by the context or trivial. The span adds unsupported information. The span is badly translated because of an accuracy error. The span is badly translated because of a fluency error. The words in the span are redundant but fluent. The span adds information that is supported by the context or trivial. The translation is syntactically different from the source. No phenomenon identified EN-DE ZH-EN 100 300 samples 100 300 100 samples 100 Correctly predicted additions Falsely predicted additions Figure 3 : Results for the human evaluation of predicted addition errors. If human raters answered that the highlighted span in the translation was indeed badly translated, they were offered the four explanation options on the left. Otherwise they chose from the four options on the right. The words in the span do not need to be translated. The span contains information that is missing but can be inferred or is trivial. The span contains information that is missing in the translation. The span contains information that is missing but can be inferred or is trivial. The span is badly translated because of an accuracy error. The span is badly translated because of a fluency error. Correctly predicted omissions Falsely predicted omissions Figure 4 : Results for the human evaluation of predicted omission errors. If human raters answered that the highlighted span in the source sequence was indeed badly translated, they offered the four explanation options on the left. Otherwise they chose from the four options on the right."
}