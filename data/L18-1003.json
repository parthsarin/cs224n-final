{
    "framing_question": "If this paper introduces a dataset, what background and expertise is prioritized in data annotators, if any?",
    "response": "This paper does not introduce a new dataset or mention anything about data annotators. Therefore, I cannot answer your question.",
    "article": "There are many machine translation (MT) papers that propose novel approaches and show improvements over their self-defined baselines. The experimental setting in each paper often differs from one another. As such, it is hard to determine if a proposed approach is really useful and advances the state of the art. Chinese-to-English translation is a common translation direction in MT papers, although there is not one widely accepted experimental setting in Chinese-to-English MT. Our goal in this paper is to propose a benchmark in evaluation setup for Chinese-to-English machine translation, such that the effectiveness of a new proposed MT approach can be directly compared to previous approaches. Towards this end, we also built a highly competitive state-of-the-art MT system trained on a large-scale training set. Our system outperforms reported results on NIST OpenMT test sets in almost all papers published in major conferences and journals in computational linguistics and artificial intelligence in the past 11 years. We argue that a standardized benchmark on data and performance is important for meaningful comparison. Introduction Over the years, there have been many published papers on machine translation (MT), proposing novel ideas by showing improvements over certain baselines. However, a paper often has a different experimental setup from the others. These variations include the approach (algorithm) and dataset. Ideally, research and development work on MT should be based on a benchmark of system setup with good performance. Otherwise, there is no support in asserting that a proposed method advances the state of the art. Unfortunately, for Chinese-to-English MT, two widely spoken human languages and one of the most well-studied language translation directions in MT, there is no widely accepted standard benchmark for evaluation, comprising a standardized training set, development set, and test set. Throughout the past decade, Chinese-to-English translation has been most commonly performed on NIST OpenMT 1 test sets, trained on parallel and monolingual corpora from the Linguistic Data Consortium (LDC) 2 . Our goal in this paper is to propose a benchmark in evaluation setup for Chinese-to-English machine translation, such that the effectiveness of a new proposed MT approach can be directly compared to previous approaches. Towards this end, we also built a highly competitive state-of-the-art MT system trained on a large-scale training set. Our system outperforms reported results on NIST OpenMT test sets in almost all papers published in major conferences and journals in computational linguistics and artificial intelligence in the past 11 years. The rest of this paper is organized as follows. Section 2 describes our MT approach. Section 3 elaborates our experimental setup. Section 4 presents our experimental results. Section 5 describes related work. Finally, Section 6 gives 1 www.nist.gov/itl/iad/mig/ open-machine-translation-evaluation 2 catalog.ldc.upenn.edu the conclusion. Neural Machine Translation We built a neural machine translation (NMT) system based on the encoder-decoder approach with attention mechanism (Bahdanau et al., 2015) . This NMT approach encodes an input sentence into a continuous representation by an encoder recurrent neural network (RNN) and produces translation output by a decoder RNN. The decoder RNN, through an attention mechanism, looks into different parts of the encoded input sentence while decoding is in progress. Encoder-Decoder Model with Attention Given a target language sentence y = (y 1 , ..., y n ) and the corresponding source language sentence x = (x 1 , ..., x m ), the neural machine translation model is formulated as p(y|x) = n i=1 p(y i |y 1 , ..., y i\u22121 , x) (1) in which the probability of the target word y i at time step i is computed by the decoder RNN as follows: p(y i |y 1 , ..., y i\u22121 , x) = F (y i , y i\u22121 , s i , c i ) = t i [y i ] (2) where F is a function to compute the probability of the word y i to be generated at time step i and t i is a vector having the size of the target language vocabulary, in which each vector dimension t i [y] stores the probability of a word y, computed as follows: t i = softmax(W t (tanh(U t s i + V t E[y i\u22121 ] + C t c i + b t ))) (3) where U t , V t , and C t are matrices mapping the hidden state s i , the embedding of the previous word E[y i\u22121 ], and the context vector c i respectively to an intermediate vector representation, with b t being the bias vector. Then W t transforms the intermediate vector representation to a vocabulary-sized probability vector. The decoder hidden state at a time step i is computed by s i = g y (E[y i\u22121 ], s i\u22121 , c i ) (4) where g y is the RNN unit function to compute the current hidden state given the hidden state of the previous time step, the previous word embedding, and the context. Equation 3 indicates that the target word to be generated at a given time step takes into account the context vector c i , which is a weighted sum of each annotation vector h j , representing the source language sentence at position j: c i = m j=1 \u03b1 ij h j (5) in which the scalar weight \u03b1 ij for each h j is computed by a softmax function: \u03b1 ij = exp(e ij ) m k=1 exp(e ik ) (6) where e ij is computed by e ij = v T a tanh(W a s i + U a h j + b a ) + \u03b2 (7) where W a and U a are the weight matrices and b a is the bias vector to compute a vector, which is then converted by the weight vector v a and the bias term \u03b2 into a scalar e ij , i.e., the degree of matching between the target word at time step i and the input word at position j. This is conceptually a soft alignment model. To compute the decoding hidden state s i in Equation 4 , we adopt an approach that incorporates the context c i from the attention mechanism by using two transitions (Sennrich et al., 2017b) . The decoder hidden state function 4 first passes the embedding E[y i\u22121 ] of the input word y i\u22121 to the first recurrent unit function, resulting in an intermediate hidden state s i , which is computed by the decoder recurrent unit functions g y,1 as: g y (E[y i\u22121 ], s i\u22121 , c i ) in Equation s i = g y,1 (E[y i\u22121 ], s i\u22121 ) (8) and is passed to Equation 7 . Then, the second recurrent unit function g y,2 processes the context c i defined in Equation 5 and the intermediate hidden state s i as follows: s i = g y,2 (c i , s i ) = g att (E[y i\u22121 ], s i\u22121 , c i ) (9) where g att (E[y i\u22121 ], s i\u22121 , c i ) is a composition of g y,1 and g y,2 . It is to be noted that as the decoder generates an output word y i at current time step i, the input word at the time step is y i\u22121 . The recurrent unit function is described further in Section 2.2. We made use of the bidirectional encoder RNN, where each annotation vector h j is a concatenation of the forward and the backward RNN hidden states, \u2212 \u2192 h j and \u2190 \u2212 h j respectively, defined as follows: h j = [ \u2212 \u2192 h j ; \u2190 \u2212 h j ] (10) \u2212 \u2192 h j = \u2212 \u2192 g x (E[x j ], \u2212 \u2192 h j\u22121 ) (11) \u2190 \u2212 h j = \u2190 \u2212 g x (E[x j ], \u2190 \u2212 h j+1 ) (12) where \u2212 \u2192 g x is the forward RNN unit function to compute the RNN hidden state at the current encoding position j given the embedding of the current word E[x j ] and the hidden state at the previous position, while \u2190 \u2212 g x is the backward RNN unit function to compute the hidden state at j given the word embedding E[x j ] and the hidden state at the next position. Training an end-to-end NMT model is conducted by the back-propagation through time (BPTT) algorithm, which updates the parameters of the RNN while the time steps are unrolled, to minimize a cost function. The parallel training corpus is divided into mini-batches, each consisting of N parallel sentences. The NMT model parameters are updated in each mini-batch. Translation decoding is performed by a beam search algorithm, which produces translation output sequentially in the target language order. NMT decoding proceeds by generating one word at each time step. In NMT, as described in Equation 3 , computing the probability involves mapping the hidden state vector to a vector with the dimension of the vocabulary size. Therefore, to make computation tractable, the NMT vocabulary size is limited. To cope with the limitation of the vocabulary size, we adopt fragmentation of words into sub-words of character sequences through the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) . This algorithm finds the N most frequent character sequences of variable length, through N character merge operations, and splits less frequent words based on this list of character sub-sequences. Recurrent Unit Function To compute the hidden state representations in Equations 4 and 10-12, we made use of recurrent unit functions with gate mechanism to control the flow of information from the input and the previous hidden state. There are two commonly adopted gate mechanisms in the encoder-decoder RNN NMT model, namely the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and the gated recurrent unit (GRU) (Cho et al., 2014) . The LSTM RNN unit consists of a memory cell \u00b5 j and three gates, i.e., the input gate \u03b9 j that controls the intensity of the new information to be stored in the memory cell, the forget gate f j that controls how much to remember or to forget from the previous memory cell, and the output gate o j that controls how much information is output to the hidden state from the memory. At each time step j, given the input \u03c7 j , the hidden state \u03b7 j is formulated as: \u03b7 j = LST M (\u03c7 j , \u03b7 j\u22121 ) = o j \u2022 tanh(\u00b5 j ) (13) where \u03b9 j = \u03c3(W \u03b9 \u03c7 j + U \u03b9 \u03b7 j\u22121 + b \u03b9 ) f j = \u03c3(W f \u03c7 j + U f \u03b7 j\u22121 + b f ) \u00b5 j = f j \u2022 \u00b5 j\u22121 + \u03b9 j \u2022 tanh(W \u00b5 \u03c7 j + U \u00b5 \u03b7 j\u22121 + b \u00b5 ) o j = \u03c3(W o \u03c7 j + U o \u03b7 j\u22121 + b o ) W and U denote the weight matrices transforming the input embedding and the previous hidden state into the corresponding outputs, and b denotes the bias vectors. Meanwhile, the recurrent unit for GRU at each time step j consists of two gates, i.e., the update gate z j and the reset gate r j . At each time step j, given the time step input \u03c7 j , the hidden state \u03b7 j is formulated as: \u03b7 j = GRU (\u03c7 j , \u03b7 j\u22121 ) = (1 \u2212 z j ) \u2022 \u03b7 j\u22121 + z j \u2022 \u03b7 j (14) where \u03b7 j = tanh(W \u03c7 j + U [r j \u2022 \u03b7 j\u22121 ] + b) z j = \u03c3(W z \u03c7 j + U z \u03b7 j\u22121 + b z ) r j = \u03c3(W r \u03c7 j + U r \u03b7 j\u22121 + b r ) W and U denote the weight matrices transforming the input embedding and the previous hidden state to the corresponding outputs (denoted by the subscript), and b denotes the bias vectors. As shown in Equations 13 and 14, both LSTM and GRU make use of gate mechanisms to control the information flow from the input and the hidden state. But unlike LSTM, GRU does not have the memory cell and the output gate. GRU proposes the hidden state \u03b7 j and interpolates each dimension with that of the previous hidden state, controlled by the update gate z j . Meanwhile, the reset gate r j controls the intensity of the previous hidden state to be taken into account in the current pre-computed hidden state. The LSTM encoder re-defines Equations 11 and 12 respectively as: \u2212 \u2192 h j = \u2212 \u2192 g x (E[x j ], \u2212 \u2192 h j\u22121 ) = \u2212\u2212\u2212\u2212\u2192 LST M (E[x j ], \u2212 \u2192 h j\u22121 ) \u2190 \u2212 h j = \u2190 \u2212 g x (E[x j ], \u2190 \u2212 h j+1 ) = \u2190\u2212\u2212\u2212\u2212 LST M (E[x j ], \u2190 \u2212 h j+1 ) while the GRU encoder re-defines the two equations respectively as: \u2212 \u2192 h j = \u2212 \u2192 g x (E[x j ], \u2212 \u2192 h j\u22121 ) = \u2212 \u2212\u2212 \u2192 GRU (E[x j ], \u2212 \u2192 h j\u22121 ) \u2190 \u2212 h j = \u2190 \u2212 g x (E[x j ], \u2190 \u2212 h j+1 ) = \u2190 \u2212\u2212 \u2212 GRU (E[x j ], \u2190 \u2212 h j+1 ) with the embedding representation E[x j ] of word x j at position j as the input to the recurrent unit function. For decoding, in Equations 8 and 9, both g y,1 and g y,2 can be instantiated by LSTM (Equation 13 ) as: g y,1 (E[y i\u22121 ], s i\u22121 ) = LST M 1 (E[y i\u22121 ], s i\u22121 ) g y,2 (c i , s i ) = LST M 2 (c i , s i ) or with GRU (Equation 14 ) as: g y,1 (E[y i\u22121 ], s i\u22121 ) = GRU 1 (E[y i\u22121 ], s i\u22121 ) g y,2 (c i , s i ) = GRU 2 (c i , s i ) where the input to the LSTM or GRU function for g y,1 is the embedding of the previous word E[y i\u22121 ] and the input for g y,2 is the context vector c i . Deep Recurrent Layers Following (Sennrich et al., 2017a) , we adopt deep RNN models for encoding and decoding. There are two alternatives to achieve this, namely the deep stacked RNN and deep transition RNN. The deep stacked RNN passes the whole input sequence to the first layer of RNN and feeds the sequence of the output hidden representations to the next layer of RNN. This is done subsequently depending on the number of RNN layers. Meanwhile, the deep transition RNN passes an input at each time step through a series of transitions (i.e., recurrent unit functions) and passes the hidden state of the last transition to the next time step. Deep Stacked RNN We adopt the deep stacked RNN that involves residual connection, summing the output of the previous RNN layer with the computed hidden state of the current RNN layer, and alternation of direction (Zhou et al., 2016) , as illustrated in Figure 1 . In this model, for each layer l and time step j, we need to distinguish between the computed hidden state of the current RNN unit function without residual connection, i.e., \u2212 \u2192 h l j , and the hidden state which includes the residual connection, i.e., \u2212 \u2192 w l j . The alternation of direction is designed such that the odd-numbered and evennumbered RNN layers process the sequence in the left-toright and right-to-left directions respectively. In the stacked RNN with layer depth D x , the forward encoder hidden state at time step j in Equation 11 , \u2212 \u2192 h j , is computed as follows: \u2212 \u2192 h j = \u2212 \u2192 g x (E[x j ], \u2212 \u2192 h j\u22121 ) = \u2212 \u2192 w Dx j \u2212 \u2192 w Dx j = \u2212 \u2192 h Dx j + \u2212 \u2192 w Dx\u22121 j \u2212 \u2192 w 1 j = \u2212 \u2192 h 1 j = \u2212 \u2192 g 1 x (E[x j ], \u2212 \u2192 h 1 j\u22121 ) \u2212 \u2192 h 2k j = \u2212 \u2192 g 2k x ( \u2212 \u2192 w 2k\u22121 j , \u2212 \u2192 h 2k j+1 ), for 1 < 2k \u2264 D x \u2212 \u2192 h 2k+1 j = \u2212 \u2192 g 2k+1 x ( \u2212 \u2192 w 2k j , \u2212 \u2192 h 2k+1 j\u22121 ), for 1 < 2k + 1 \u2264 D x \u2212 \u2192 w l j = \u2212 \u2192 h l j + \u2212 \u2192 w l\u22121 j , for 1 < l \u2264 D x In the above equations, \u2212 \u2192 g l x is the forward encoder recurrent unit function of a layer l in the deep RNN stack. It can be instantiated with LSTM or GRU. While the above equations compute the forward encoder hidden state, the backward encoder hidden state \u2190 \u2212 h j is computed similarly by changing the arrow direction from right (\u2192) to left (\u2190) and swapping j \u2212 1 with j + 1. Since at each time step, the decoder has no knowledge of the next word, there is no alternation of direction therein. In addition, we only use the recurrent function with attention in the first layer of the RNN stack s 1 i , while the deeper layers are simple RNN without attention. Therefore, the decoder RNN with layer depth D y computes the hidden state s i in Equation 4, as follows: s i = g y (E[y i\u22121 ], s i\u22121 , c i ) = w Dy i w Dy i = s Dy i + w Dy\u22121 i s Dy i = g Dy y (w Dy\u22121 i , s Dy i\u22121 ) w 1 i = s 1 i = g 1 att (E[y i\u22121 ], s 1 i\u22121 , c i ) s l i = g l y (w l\u22121 i , s l i\u22121 ), for 1 < l \u2264 D y w l i = s l i + w l\u22121 i , for 1 < l \u2264 D y The first RNN stack layer, s 1 i = g 1 att (E[y i\u22121 ], s 1 i\u22121 , c i ) , is a composition of two recurrent unit functions like in Equa- s 1 i = g 1 att (E[y i\u22121 ], s 1 i\u22121 , c i ) s i = g 1 y,1 (E[y i\u22121 ], s 1 i\u22121 ) s 1 i = g 1 y,2 (c i , s i ) Like the encoders, the decoder recurrent unit function g l y at each layer l can be instantiated by LSTM or GRU. Deep Transition RNN The deep transition RNN (Miceli-Barone et al., 2017) involves a number of layers within a time step j through which an input word is fed, as illustrated in Figure 2 . The recurrent unit function of each layer l is defined as a transition, which outputs an intermediate state \u2212 \u2192 h j,l for the encoder and s j,l for the decoder. With L x transitions, the hidden state representation at j is equivalent to the output of the last layer, so for the forward encoder state, Equation 11 defines \u2212 \u2192 h j as: \u2212 \u2192 h j = \u2212 \u2192 g x (E[x j ], \u2212 \u2192 h j\u22121 ) = \u2212 \u2192 h j,Lx \u2212 \u2192 h j,1 = \u2212 \u2192 g x,1 (E[x j ], \u2212 \u2192 h j\u22121,Lx ) \u2212 \u2192 h j,l = \u2212 \u2192 g x,l (0, \u2212 \u2192 h j,l\u22121 ), for 1 < l \u2264 L x where the input to the first recurrent unit transition is E[x j ], the embedding of the input word at j, while the subsequent higher level transitions only receive the output from the previous transition and do not receive the input word. The reverse hidden state \u2190 \u2212 h j in Equation 12 is computed similarly by substituting j \u2212 1 with j + 1. The encoder recurrent unit function \u2212 \u2192 g x,l at each layer l can be instantiated by LSTM or GRU. While the baseline shallow decoder RNN already contains two transitions, without and with attention respectively, the deep transition decoder RNN is extended similarly to the deep transition encoder RNN, such that the decoder hidden state with depth L y is computed as: s i,1 = s i = g y,1 (E[y i\u22121 ], s i\u22121,Ly ) s i,2 = g y,2 (c i , s i,1 ) s i,l = g y,l (0, s i,l\u22121 ), for 2 < l \u2264 L y s i = s i,Ly Similarly, the decoder recurrent unit function g y,l at each layer l can be instantiated by LSTM or GRU. Experimental Setup Datasets We conducted experiments using the parallel training corpora from LDC to test on the NIST test sets. In addition, we also conducted experiments on the United Nations Parallel Corpus (Ziemski et al., 2016) , following (Junczys-Dowmunt et al., 2016) . We used the pre-defined training, development, and test sets of the corpus following (Junczys-Dowmunt et al., 2016) and conducted NMT experiments accordingly. We pre-processed our parallel training corpora by segmenting Chinese sentences, which originally have no spaces to demarcate words, and tokenizing English sentences to split punctuation symbols from words. Chinese word segmentation was performed by a maximum entropy model (Low et al., 2005) trained on the Chinese Penn Treebank (CTB) segmentation standard. To alleviate the effect of rare words in NMT, we fragmented words to sub-words through the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) with 59,500 merge operations. All our training sentences are lowercased. LDC Corpora We divide the LDC corpora we used into older corpora 3 and newer corpora 4 . Due to the dominant older corpora, we duplicate the newer corpora of various domains ten times to achieve better domain balance. In addition to the parallel sentences, we also utilized a large amount of monolingual English texts, consisting of the English side of FBIS parallel corpus (LDC2003E14) and all the sub-corpora of the English Gigaword Fourth Edition (LDC2009T07). Altogether, the combined corpus consists of 107M sentences and 3.8B tokens. Each individual Gigaword sub-corpus (i.e., AFP, APW, CNA, LTW, NYT, and Xinhua) is used to train a separate N -gram language model. The English side of FBIS is also used to train another separate language model (LM). These individual language models are then interpolated to build one single large LM, via perplexity tuning on the English side of the development data. We use this LM for translation output re-ranking. To recover the original casing on the translation output, we trained a statistical MT recaser model by using Moses (Koehn et al., 2007) UN Parallel Corpus The training set of the UN Parallel Corpus, after preprocessing and filtering those exceeding 50 sub-words, consists of 9.73M parallel sentence pairs, 207M Chinese subword tokens (equivalent to 204M word tokens), and 225 English sub-word tokens (equivalent to 223M word tokens). We also utilized larger English monolingual text, i.e., all the English side of the UN Parallel Corpus before length filtering, consisting of 11.3M sentences and 335M word tokens. The development set of the UN Parallel Corpus contains 4,000 sentence pairs, 107K Chinese word tokens, and 118K English word tokens, while the test set contains 4,000 sentence pairs, 106K Chinese word tokens, and 118K English word tokens. There is only one reference translation in the development and test sets of the UN Parallel Corpus. NMT Model Parameters We built our neural machine translation (NMT) system by using Nematus (Sennrich et al., 2017b) , an open-source NMT toolkit which implements the encoder-decoder NMT architecture with attention mechanism. Our system is based on the NMT system in (Sennrich et al., 2017a) . We built an ensemble model consisting of 4 independent models, which are the cross product of two different deep RNN architectures, i.e., deep stacked RNN and deep transition RNN, and two different recurrent unit functions, i.e., GRU and LSTM. For all our models, the word embedding dimension is 500, and the hidden layer dimension is 1,024. Our deep stacked RNN contains 4 stack layers on each of the encoder and decoder. Meanwhile, our deep transition RNN contains 4 encoder transitions and 8 decoder transitions. Training for each individual model progresses by updating the model parameters at each mini-batch of 40 sentence pairs to minimize the negative log-likelihood loss function on the parallel training data. We use the Adam algorithm (Kingma and Ba, 2015) with learning rate of 0.0001. At each update, we clip the gradient norm to 1.0. We apply layer normalization (Ba et al., 2016) on the model parameters for faster convergence and tie the target-side embedding with the transpose of the output weight matrix (Press and Wolf, 2017) . Model parameters are saved at every checkpoint of 10,000 update iterations. At this stage, the negative log-likelihood loss function on the development set is checked. Training stops when there has been no improvement over the lowest loss function value on the development set for 10 consecutive checkpoints. The main difference between our system and (Sennrich et al., 2017a ) is that while they only built NMT models with GRU, we also made use of LSTM. Another difference is in the usage of the larger monolingual English text. They built a synthetic Chinese-English parallel corpus by translating the monolingual English text to Chinese with an Englishto-Chinese (reverse direction) NMT model and appended those sentence pairs to their parallel training corpus. Meanwhile, we exploited the English monolingual corpus by building a 5-gram language model to re-rank the k-best translation outputs produced by our NMT system. Experimental Results For experiments using the LDC corpora, translation quality is measured by case-insensitive BLEU (Papineni et al., 2002) , for which the brevity penalty is computed based on the shortest reference (NIST-BLEU) 6 . Statistical significance testing between systems is conducted by bootstrap resampling (Koehn, 2004) . We also compare the best results of our system on NIST test sets with the best results reported in published papers on Chinese-to-English MT systems in major computational linguistics and artificial intelligence publication venues 7 . Over the years 2007-2017 (both years inclusive), our 4model ensemble system with re-ranking achieves a higher BLEU score than the best results reported in almost all (402 out of 403) papers 8 . Note that the LDC training datasets used in the published papers that we compare to may not be the same as ours. This incomparability would not have happened if there were a widely adopted, standardized dataset for training. As shown in In addition, since there are two ways of computing BLEU scores with respect to word casing, i.e., case-insensitive and case-sensitive, we have taken care to compare BLEU scores using the same word casing, that is, by comparing our casesensitive BLEU scores only to case-sensitive BLEU scores published previously, and similarly for case-insensitive BLEU scores. Moreover, there are two ways of computation with respect to brevity penalty calculation involving multiple reference translations, namely \"shortest\", used in NIST-BLEU, and \"closest\", used in IBM-BLEU. The former sets brevity penalty against the shortest reference translation while the latter sets brevity penalty against the reference translation whose length is the most similar to the system translation output. We have also taken this into account by comparing NIST-BLEU with NIST-BLEU, and IBM-BLEU with IBM-BLEU 9 . For UN Parallel Corpus experiments, translation quality is measured by case-insensitive BLEU using the script provided by Moses 10 , following the evaluation setup in (Junczys-Dowmunt et al., 2016) 11 . As shown in Table 3 , our best result is obtained by an ensemble of 4 independent models with k-best output reranking using N -gram LM trained on the whole English side of the UN Parallel Corpus. Our system with re-ranking is 0.3 BLEU point better than without re-ranking, and the improvement is statistically significant (p < 0.05). Both of our systems achieve higher BLEU scores than the best published result for Chinese-to-English translation reported in (Junczys-Dowmunt et al., 2016) . Dataset Related Work Establishing standards for the state of the art by publicly accessible resources is important in research. In speech recognition, for instance, there has been work on building a virtual machine as a means of collaboratively building a state-of-the-art system for speech recognition (Metze et al., 2013) , aiming at realizing a standardized state-of-theart system in a collaborative manner. While we do not provide any virtual machines, we have a similar intention of making available a state-of-the-art MT system. On NMT, Denkowski and Neubig (2017) argued that experiments should be based on a strong baseline system to ensure that a newly proposed approach indeed improves over the best prior published approaches. They performed their experiments on WMT and IWSLT tasks (but not on Chinese-to-English translation) which have fixed training, development, and test sets. The problem for Chinese-to-English MT is greater in that there is no pre-defined set of training data that must be used for experiments, and various groups used different tuning sets and reported their results on different NIST OpenMT test sets. In addition, the lack of a standardized benchmark is not limited to neural MT approaches, but has been widespread since statistical MT approaches began to be tested on Chinese-to-English translation. Conclusion The problem of lack of consistent experimental setups for Chinese-to-English MT poses a challenge in evaluating newly proposed approaches over the pre-existing state of the art. This can be avoided if there is a clear benchmark which consists of standardized training, development, and test sets, as well as a common benchmark system setup. In this paper, we have shown that our proposed MT approach can be used to build a competitive system on the NIST OpenMT test sets that outperforms systems in almost all 403 published papers in the past 11 years. We encourage Chinese-to-English MT experiments to use our common benchmark consisting of standard data and evaluation. As the NIST dataset from LDC is widely used in the Chinese-to-English MT research community, we have put up a scoreboard listing the scores achieved by all prior published papers when evaluated on the NIST dataset and released the source code and translation output of our best NMT system 12 . By doing so, we hope future work can make more meaningful comparisons to previous MT research.",
    "funding": {
        "military": 1.9361263126072004e-07,
        "corporate": 0.0,
        "research agency": 4.155641950809308e-05,
        "foundation": 3.1736992638364825e-06,
        "none": 1.0
    }
}