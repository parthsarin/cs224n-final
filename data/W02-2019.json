{
    "article": "Introduction This report describes the application of Markov models to the problem of language-independent named entity recognition for the CoNLL-2002 shared task (Tjong Kim Sang, 2002) . We approach the problem of identifying named entities as a kind of probabilistic tagging: given a sequence of words w 1 $ %$ %$ w n , we want to find the corresponding sequence of tags t 1 $ %$ %$ t n , drawn from a vocabulary of possible tags T , which satisfies: S & argmax t 1 ' ' ' t n P( t 1 $ %$ %$ t n ) w 1 $ %$ %$ w n 0 (1) The possible tags are 1 \u00a62 43 65 67 and 8 42 93 @5 \u00a87 , which mark the beginning and continuation of personal names; 1 A2 @B 97 C and 8 42 6B 97 C , which mark names of organizations; 1 A2 4D AB 6E and 8 42 4D AB 6E , which mark names of locations; 1 \u00a62 9F 8 4G \u00a8E and 8 42 9F 8 4G \u00a8E , which mark miscellaneous names; and B , which marks non-name tokens. We will assume that a sequence of tags can be modeled by Markov process, and that the probability of assigning a tag to a word depends only on a fixed context window (say, the previous word and tag). Thus, the sequence probability in (1) can be restated as the product of tag probabilities: P( t 1 $ %$ %$ t n ) w 1 $ %$ %$ w n 0 & \u220f iH 1I n P( t i ) w i P t iQ 1 P w iQ 1 P $ %$ %$ R0 For each of the models described in the next section, the model parameters were estimated based on the provided training data, with no preprocessing or filtering. Then, the most likely tag sequence (based on the model) is selected for each sentence in the test data, and the results are evaluated using the S 9T 9U \u00a4V 6V 6W 4X AY V script. Models In the first model (ba dc fe hg pi Rq de ), the tag probabilities depend only on the current word: P( t 1 $ %$ %$ t n ) w 1 $ %$ %$ w n 0 & \u220f iH 1I n P( t i ) w i 0 The effect of this is that each word in the test data will be assigned the tag which occurred most frequently with that word in the training data. The next model considered (r ts us ) is a simple Hidden Markov Model (DeRose, 1988; Charniak, 1993) , in which the tag probabilities depend on the current word and the previous tag. Suppose we assume that the word/tag probabilities and the tag sequence probabilities are independent, or: P( w i ) t i P t iQ 1 0 & P( w i ) t i 0 P( t i ) t iQ 1 0 (2) Then by Bayes' Theorem and the Markov property, we have: P( t 1 $ %$ %$ t n ) w 1 $ %$ %$ w n 0 & P( w 1 $ %$ %$ w n ) t 1 $ %$ %$ t n 0 P( t 1 $ %$ %$ t n 0 P( w 1 $ %$ %$ w n 0 & \u220f iH 1I n P( w i ) t i 0 P( t i ) t iQ 1 0 P( w 1 $ %$ %$ w n 0 Since the probability of the word sequence P( w 1 $ %$ %$ w n 0 is the same for all candidate tag sequences, the optimal sequence of tags satisfies: S & argmax t 1 ' ' ' t n \u220f iH 1I n P( w i ) t i 0 P( t i ) t iQ 1 0 (3) The probabilities P( w i ) t i 0 and P( t i ) t iQ 1 0 can easily be estimated from training data. Using (3) to calculate the probability of a candidate tag sequence, the optimal sequence of tags can be found efficiently using dynamic programming (Viterbi, 1967) . While this kind of HMM is simple and easy to construct and apply, it has its limitations. For one, (3) depends on the independence assumption in (2). In the next model (s wv ), we avoid this by using a conditional maximum entropy model to estimate tag probabilities. Maximum entropy models (Jaynes, 1957; Berger et al., 1996; Della Pietra et al., 1997) are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources. In this model, the optimal tag sequence satisfies: S & argmax t 1 ' ' ' t n \u220f iH 1I n P( t i ) w i P t iQ 1 0 where P( t i ) w i P t iQ 1 0 & exp x \u2211 j \u03bb j f j ( t iQ 1 P w i P t i 0 y \u2211 \u03c4 T exp x \u2211 j \u03bb j f j ( t iQ 1 P w i P \u03c4 0 y (4) The indicator functions f j 'fire' for particular combinations of contexts and tags. For instance, one such function might indicate the occurrence of the word Javier with the tag 1 A2 43 65 67 : f ( t iQ 1 P w i P t i 0 & 1 if w i & Javier & t i & 1 A2 43 @5 \u00a87 0 otherwise (5) and another might indicate the tag sequence B 1 \u00a62 93 @5 \u00a87 : f ( t iQ 1 P w i P t i 0 & 1 if t iQ 1 & B & t i & 1 A2 43 65 67 0 otherwise (6) Each indicator f j function also has an associated weight \u03bb j , which is chosen so that the probabilities (4) minimize the relative entropy between the empirical distribution P (derived from the training data) and the model probabilities P, or, equivalently, which maximize the likelihood of the training data. Unlike the parameters of an HMM, there is no closed form expression for estimating the parameters of a maximum entropy model from the training data. So, we proceed iteratively, gradually refining the parameter estimates until the desired level of precision is reached. For these experiments, the parameters were fit to the training data using a limited memory variable metric algorithm (Malouf, in press) . The basic structure of the model is very similar to that of Borthwick (1999) . However, in the models described here, no feature selection is performed. Also note that this formulation of maximum entropy Markov models differs slightly from that of McCallum et al. (2000) . Here we use a single maximum entropy model, while McCallum, et al. use a separate model for each source state. Using separate models increases the sparseness of the training data and, at least for this task, slightly reduces the accuracy of the final tagger. Using indicator functions of the type in ( 5 ) and ( 6 ), the model encodes exactly the same information as the HMM in (3), but with much weaker independence assumptions. This means we can add information to the model from partially redundant and overlapping sources. The model s wv \u00a6 adds two additional types of information that were used by Borthwick (1999) . It includes capitalization features, which indicate whether the current word is capitalized, all upper case, all lower case, mixed case, or non-alphanumeric, and whether or not the word is the first word in the sentence. And it also adds additional context sensitivity, so that the tag probabilities depend on the previous word, as well as the previous tag and the current word. The next model, s wv \u00a6 , adds one additional feature to s wv \u00a6 that takes advantage of the structure of the training and test data. Often in newspaper articles, the first reference to an individual is by full name and title, while later references use only the person's surname. While an unfamiliar full name can often be identified as a name by the surrounding context, the surname appearing alone is more difficult to catch. For example, one article begins: El presidente electo de la Repblica Dominicana, Hiplito Meja, del Partido Revolucionario Dominicano (PRD) socialdemcrata, manifest que mantendr su apoyo a los XIV Juegos Panamericanos del 2003 en Santo Domingo. Meja, quien gan los comicios presidenciales en las votaciones del pasado 16 de mayo, asegur que ni l ni su partido cambiarn la posicin asumida ante el pueblo dominicano de respaldar la organizacin de los Juegos. In the first sentence, the phrase Hiplito Meja can likely be identified as a personal name even if the surname is an unknown word, since the phrase consists of two capitalized words (the first a common first name) set off by commas. In the second sentence, however, Meja is much more difficult to identify as a name: a sentence-initial capitalized unknown word is most likely to be tagged as B . To allow the use in the first sentence to provide information about the second, s wv A uses a feature which is true just in case the current word occurred as part of a personal name previously in the text being tagged. With this feature, the model can take advantage of easy instances of names to help with more difficult instances later in the text. All of the models described to this point are completely language independent and use no information not contained in the training data. The final model, s wv \u00a6 , includes one additional feature which indicates whether or not the current word appears in a list of 13,821 first names collected from a number of multi-lingual sources on the Internet. While the names are drawn from a wide range of languages and cultures, the emphasis is on European names, and in particular English and Spanish. Results Each of the models described in the previous section were trained using W \u00a6 t \" @ Y \u00a6 U and evaluated on W \u00a6 t \" AW \u00a6 b Y . The results are summarized in Table 1. As would be expected, r ts us performs substantially better than ba dc fe hg pi Rq de for every category but locations, though earlier cross-validation experiments suggest that this exception is an accident of the particular split between training and test data. Perhaps more surprisingly, s wv outperforms r ts us by an even wider margin. In these two models, the tag probabilities are conditioned on exactly the same properties of the contexts. The only difference between the models is that the probabilities in s wv are estimated in a way which avoids the independence assumption in (2). The poor performance of r s us suggests that this assumption is highly problematic. Adding additional features, in s wv \u00a6 and s wv \u00a6 , offer further gains over the base model. However, the addition of a database of first names, in s wv \u00a6 d , only slightly improves the performance on personal names and actually reduces the overall performance. This is likely due to the fact that the list of names contains many words which can also be used as locations and organizations. Perhaps the use of additional databases of geographic and nonpersonal names would help counteract this effect. For the final results, the model which preformed the best on the evaluation data, s wv \u00a6 , was trained on W t \" 6 AY \u00a6 U and evaluated with W \u00a6 t \" AW \u00a6 b Y and W e d f AW d \u00a8g , and trained on U \u00a6W 6h i \" @ Y \u00a6 U and evaluated with U \u00a6W 6h i \" AW \u00a6 b Y and U \u00a6W 6h j f AW \u00a6 b \u00a8g U W \u00a8h j f @ AY A dU , to allow a more direct cross-language comparison of the performance of s wv \u00a6 . The results of the final evaluation are given in Table 2. The performance of the model is roughly the same for both test samples of each language, though the performance differs somewhat between the two languages. In particular, the performance on F l8 4G \u00a8E entities is quite a bit better for Dutch than it is for Spanish, and the performance on 3 65 67 entities is quite a bit better for Spanish than it is for Dutch. These differences are somewhat surprising, as nothing in the model is language specific. Perhaps the discrepancy (especially for the F l8 9G 6E class) reflects differences in the way the training data was annotated; F l8 9G 6E is a highly heterogenous class, and the criteria for distinguishing between F 8 4G 6E and B entities is sometimes unclear. Conclusion The models described here are very simple and efficient, depend on no preprocessing or (with the exception of s wv \u00a6 ) external databases, and yet provide a dramatic improvement over a baseline model. However, the performance is still quite a bit lower than results for industrial-strength language-specific named entity recognition systems. There are a number of small improvements which could be made to these models, such as feature selection (to reduce overtraining) and the use of whole sentence sequence models, as in Lafferty et al. (2001) shared task. m @ 4 dn %o 6o @V S 9p A2 @q @q 6q t sr l 4Y t Y AS u vg \u00a6W @o S 4T hU \u00a4V @V 6w 6x 6x w @o bU W 4 \u00a6o . Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260-269. Acknowledgements The research of Dr. Malouf has been made possible by a fellowship of the Royal Netherlands Academy of Arts and Sciences and by the NWO PIONIER project Algorithms for Linguistic Processing.",
    "abstract": "",
    "countries": [
        "Netherlands"
    ],
    "languages": [
        "Spanish",
        "Dutch",
        "English"
    ],
    "numcitedby": "97",
    "year": "2002",
    "month": "",
    "title": "{M}arkov Models for Language-independent Named Entity Recognition"
}