{
    "article": "We formulate a generalization of Petrov et al. (2006) 's split/merge algorithm for interpreted regular tree grammars (Koller and Kuhlmann, 2011), which capture a large class of grammar formalisms. We evaluate its effectiveness empirically on the task of discontinuous constituent parsing with two mildly context-sensitive grammar formalisms: linear context-free rewriting systems (Vijay-Shanker et al., 1987) as well as hybrid grammars (Nederhof and Vogler, 2014). Introduction Probabilistic grammars are a standard model for language processing tasks. Their fundamental principle is a rewriting process in which nonterminals are repeatedly unfolded in accordance to rewrite rules until a structure consisting solely of terminals is obtained. Context-free independence assumptions imply that the applicability as well as the probability of a rewrite step depend only on the nonterminal that is unfolded but not on the context or history in which the nonterminal occurs. The independence assumptions allow for tractable algorithms when processing data with these grammars. Then again, the expressiveness of such a grammar is constrained by the number of its nonterminals. This is why it was found beneficial to refine naturally emerging sets of nonterminals (such as syntactic categories). Strategies of refinement of context-free grammars (CFGs) involve for instance Markovization, i.e., the encoding of limited context into the nonterminals (Collins, 1999; Klein and Manning, 2003) , and automatic state-splitting by means of latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006) . An important observation is that these refinements are latent, i.e., they are not observed in the predictions that the CFG is supposed to provide. Automatic state-splitting has been successfully applied also to tree-substitution grammars (Shindo et al., 2012) and tree-adjoining grammars (Ferraro et al., 2012) . Koller and Kuhlmann (2011) proposed interpreted regular tree grammars (IRTGs) as a uniform way to describe a large class of grammar formalisms that share the context-free rewriting mechanism. IRTGs decouple the derivational process, in which derivation trees are generated by a (probabilistic) regular tree grammar (RTG), from the interpretation of derivation trees in one or multiple algebras. IRTGs enable the development of generic algorithms for binarization (B\u00fcchse et al., 2013) , parsing and decoding (Groschwitz et al., 2016; Teichmann et al., 2017) , and estimation techniques (Teichmann et al., 2016) . The central hypothesis of this article is that Petrov et al. (2006) 's split/merge algorithm (a) can be transferred from CFGs to a large class of grammar formalisms and (b) that its application improves the probabilistic behavior of a given grammar for parsing and decoding tasks. The first contribution of our paper is a generic version of Petrov et al. (2006) 's split/merge algorithm in the IRTG framework. We choose IRTGs because the separation of the derivational process in an RTG from the interpretation in algebras implies that (i) only one algorithm needs to be formulated to capture a large class of grammar formalisms and that (ii) the nonterminals cannot be observed in the generated structures. Because of (ii) nonterminals of IRTGs may be viewed as already being latent, i.e., with IRTGs latent annotations come for free. We also transfer objectives for efficient parsing and decoding as proposed by Matsuzaki et al. (2005) and Petrov and Klein (2007) into the IRTG framework. An implementation of the generic algorithms is provided. Then, for a case study of (b) we apply the generalized split/merge algorithm and the different parsing objectives to both linear context-free rewriting systems (LCFRSs) (Vijay-Shanker et al., 1987; Kallmeyer and Maier, 2013) and hybrid grammars (Nederhof and Vogler, 2014) on the task of discontinuous constituent parsing. This choice is relevant because the application of the split/merge algorithm to either grammar formalism has been supposed by Evang and Kallmeyer (2011) and Gebhardt et al. (2017) , respectively, but to our knowledge not yet been performed. We find that the split/merge algorithm improves the parsing accuracy of the grammars by up to 14.5 points in labeled F1. However, the grammars do not reach the accuracy of recent transition-based discriminative parsers. Preliminaries Let A, B, and C be sets and f : A \u2192 B and g : B \u2192 C be functions. The powerset of A is denoted by P(A). We extend f in the natural way to f : P(A) \u2192 P(B) and f \u22121 : P(B) \u2192 P(A). We call f surjective, if for each b \u2208 B, there exists a \u2208 A with f (a) = b. We let g \u2022 f : A \u2192 C denote the composition of f and g. We identify a singleton set {a} with its element a. Interpreted regular tree grammars An interpreted regular tree grammar generates an object a of some domain A in two phases: firstly a derivation tree \u03be is generated and secondly \u03be is interpreted in an algebra A to a. Figure 1 shows two derivation trees \u03be 1 and \u03be 2 over operator symbols f 0 , f 1 and f 2 . Each operator symbol admits a fixed number of arguments called its rank, e.g., the ranks of f 0 , f 1 , and f 2 are 0, 1, and 2, respectively. A finite, non-empty set of operator symbols constitutes a signature \u03a3. The set of derivation trees over \u03a3, denoted by T \u03a3 , is the smallest set U where for any f \u2208 \u03a3 of rank k and \u03be 1 , . . . , \u03be k \u2208 U we have f (\u03be 1 , . . . , \u03be k ) \u2208 U . Let \u03be = f (\u03be 1 , . . . , \u03be k ) be in T \u03a3 . The set of positions of \u03be is pos(\u03be) = {\u03b5}\u222a{i\u03c0 | 1 \u2264 i \u2264 k, \u03c0 \u2208 pos(\u03be i )}. The operator symbol at the position \u03c0 in \u03be, denoted by \u03be(\u03c0), is f if \u03c0 = \u03b5 and \u03be i (\u03c0 ) if \u03c0 = i\u03c0 . f As 0 ()= b f At 0 ()= B(b) f As 1 (x 1 )= x 1 f At 1 (x 1 )= S(x 1 ) f As 2 (x 1 , x 2 )= x 1 x 2 f At 2 (x 1 , x 2 )= B(x 1 , x 2 ) f 1 f 2 f 2 f 0 f 0 f 0 f 1 f 2 f 0 f 2 f 0 f 0 S B B B b B b B b S B B b B B b B b s : b b b [[\u2022]] At [[\u2022]] At [[\u2022]] As [[\u2022]] As t 1 : t 2 : \u03be 1 : \u03be 2 : Figure 1 : Operations of the \u03a3 ex -algebras A s and A t , derivation trees \u03be 1 and \u03be 2 , and evaluation of \u03be 1 and \u03be 2 in A s and A t . Next, we describe the interpretation of a derivation tree from T \u03a3 by a \u03a3-algebra. A \u03a3algebra A consists of a set A (domain) and, for each operator symbol f in \u03a3 of rank k, an operation f 1 shows the operations of the \u03a3 ex -algebras A s and A t with the set of strings and parse trees as domains, respectively. Also, it shows the evaluation of \u03be 1 , \u03be 2 \u2208 T \u03a3ex in A s to the string s = bbb and the evaluation of \u03be 1 and \u03be 2 in A t to the parse trees t 1 and t 2 , respectively. A : A k \u2192 A. Each deriva- tion tree f (\u03be 1 , . . . , \u03be n ) in T \u03a3 can be evalu- ated in A to an element [[f (\u03be 1 , . . . , \u03be k )]] A = f A ([[\u03be 1 ]] A , . . . , [[\u03be k ]] A ) in A. Let \u03a3 ex = {f 0 , f 1 , f 2 }. Figure Regular tree grammars are useful to describe formal languages of derivation trees. A regular tree grammar (RTG) (Brainerd, 1969) 2a shows the rules of G ex = ({S, B}, S, R G ) over \u03a3 ex . over \u03a3 is a tuple G = (N G , S G , R G ). The finite set N G is disjoint from \u03a3 and contains nonterminals. S G in N G is the start nonterminal. The set R G is a finite subset of the set of prototypical rules R[N G , \u03a3]. R[N G , \u03a3] contains each rule of the form B \u2192 f (B 1 , . . . , B k ) where f \u2208 \u03a3 is of rank k and B, B 1 , . . . , B k are in N G . Figure An RTG G generates a derivation tree \u03be \u2208 T \u03a3 if it has a valid run on it: A run of G on \u03be is a mapping r : pos(\u03be) \u2192 N G . The rule at position \u03c0 of r is rule \u03c0 r = r(\u03c0) \u2192 \u03be(\u03c0)(r(\u03c01), . . . , r(\u03c0k)) where k is the rank of \u03be(\u03c0). We call r valid if r(\u03b5) = S G and r is consistent with R G , i.e., for every \u03c0 \u2208 pos(\u03be), we require that rule \u03c0 r \u2208 R G . We denote the set of all valid runs of G on \u03be by Figure 2a shows the only valid runs r 1 and r 2 of G ex on the derivation trees \u03be 1 and \u03be 2 , respectively, in tree-like form. runs v G (\u03be). The language of G is L(G) = {\u03be \u2208 T \u03a3 | runs v G (\u03be) = \u2205}. Moreover, we let runs v G = {(\u03be, r) | \u03be \u2208 T \u03a3 , r \u2208 runs v G (\u03be)}. (a) (G ex , p) :S \u2192 f 1 (B) #1.0 B \u2192 f 2 (B, B) #0.2 B \u2192 f 0 () #0.8 S B B B B B S B B B B B 0.2 2 \u2022 0.8 3 0.2 2 \u2022 0.8 3 r 1 : r 2 : (b) G s G (s,t1) G (s,t2) B 0,1 \u2192f 0 () B 1,2 \u2192f 0 () B 2,3 \u2192f 0 () B 0,2 \u2192f 2 (B 0,1 , B 1,2 ) B 0,3 \u2192f 2 (B 0,2 , B 2,3 ) B 1,3 \u2192f 2 (B 1,2 , B 2,3 ) B 0,3 \u2192f 2 (B 0,1 , B 1,3 ) S 0,3 \u2192f 1 (B 0,3 ) (G ex , p ) :S \u2192 f 1 (B 1 ) #1.0 B 1 \u2192 f 2 (B 1 , B 2 ) #0.5 (c) B 1 \u2192 f 2 (B 2 , B 1 ) #0.25 B 1 \u2192 f 0 () #0.25 B 2 \u2192 f 0 () #1.0 S B 1 B 1 B 1 B 2 B 2 S B 1 B 1 B 2 B 1 B 2 S B 1 B 2 B 1 B 1 B 2 S B 1 B 2 B 1 B 2 B 1 0.5 2 \u2022 0.25 1 0.5 1 \u2022 0.25 2 0.5 1 \u2022 0. Interpreted RTGs. An interpreted RTG (IRTG) (Koller and Kuhlmann, 2011 ) is a tuple G = (G, A 1 , . . . , A l ) where G is an RTG over \u03a3 and each A j is a \u03a3-algebra 1 . Given an object a = (a 1 , . . . , a l ) from the domain and L(G) . If, for each a j \u2208 A j , there is an RTG whose language is D a j , then A j is called regularly decomposable. In the following we consider only such algebras. Since the languages of RTGs are closed under intersection, we can construct RTGs G a j with L(G a j ) = D a j \u2229 L(G) and the RTG G a with L(G a ) = j L(G a j ) = parses G (a). We call G a j and G a the chart of a j and the chart of a, respectively. As a running example we extend the RTG G ex to an IRTG G ex = (G ex , A s , A t ). Then G ex is equivalent to a CFG where A s specifies the generation of strings and A t describes the corresponding parse trees. Figure 2b depicts the rules of the charts G s , G (s,t 1 ) , and G (s,t 2 ) where the nonterminals were annotated with spans as usual and the start nonterminal is S 0,3 . A = A 1 \u00d7 . . . \u00d7 A l , the parsing problem for IRTG is to compute parses G (a) = {\u03be \u2208 L(G) | \u2200j : [[\u03be]] A j = a j }. This set is equal to the intersection of all sets D a j = {\u03be \u2208 T \u03a3 | [[\u03be]] A j = a j } Grammar morphisms. In order to relate two RTGs G and G (e.g., G could be a chart G a ), we consider mappings \u03d5 : N G \u2192 N G between the respective sets of nonterminals. We lift \u03d5 to \u03d5 : R[N G , \u03a3] \u2192 R[N G , \u03a3] by setting \u03d5(B \u2192 f (B 1 , . . . B n )) = \u03d5(B ) \u2192 f (\u03d5(B 1 ), . . . , \u03d5(B n )). If \u03d5 \u22121 (S G ) = {S G } and \u03d5(R G ) \u2286 R G , then we call \u03d5 a grammar morphism from G to G. Intuitively, \u03d5 has to establish a correspondence between the grammars' start nonterminals and rules. For instance, we can choose any set M and any mapping \u03d5 : N G \u2192 M that satisfies \u03d5 \u22121 (\u03d5(S G )) = {S G } and construct a new RTG \u03d5(G) from G: we set \u03d5(G) = (\u03d5(N G ), \u03d5(S G ), \u03d5(R G )). Obviously, \u03d5 is a grammar morphism from G to \u03d5(G). For each chart G a we assume a grammar morphism \u03d5 a from G a to G that gives rise to a one-to-one correspondence between the valid runs of G a and the valid runs of G for derivations trees of a. Formally, for each \u03be \u2208 parses G (a) we require that \u03c6a : runs v Ga (\u03be) \u2192 runs v G (\u03be) where \u03c6a (r) = \u03d5 a \u2022 r is a bijection. For instance, \u03d5 s , \u03d5 (s,t 1 ) , and \u03d5 (s,t 2 ) strip the spans from each nonterminal, e.g., \u03d5 s (B i,j ) = B. Extending IRTGs with probabilities RTGs and IRTGs can be equipped with probabilities in the standard way: A weight assignment for an RTG G is a mapping p : R G \u2192 [0, 1]. We define the weight of a run r on a derivation tree \u03be as W (G,p) (\u03be, r) = \u03c0\u2208pos(\u03be) p(rule \u03c0 r ) and the weight of a derivation tree \u03be as W (G,p) (\u03be) = r\u2208runs v G (\u03be) W (G,p) (\u03be, r). We call p proper if, for every A \u2208 N G , we have 1 = =(A\u2192f (B 1 ,...,B k )) in R G p( ). If 1 = \u03be\u2208T \u03a3 W (G,p) (\u03be) , then we call p consistent. In this case we may write P (\u03be, r | G, p) and P (\u03be | G, p) for W (G,p) (\u03be, r) and W (G,p) (\u03be), respectively, and call (G, p) probabilistic RTG. Let G be an RTG and \u03d5 be a grammar morphism from G to G. Then p \u2022 \u03d5 : R G \u2192 [0, 1] is a weight assignment for G . Given an IRTG G = (G, A 1 , . . . , A l ) and a proper and consistent weight assignment p for G, we define a probability distribution on A by P (a | G, p) = \u03be\u2208parses G (a) W (G,p) (\u03be). Using the chart G a the same quantity can be obtained: P (a | G, p) = \u03be\u2208L(Ga) W (Ga,p \u2022 \u03d5a) (\u03be) (see Appendix A.1). Considering G ex and the probability assignment p for G in Figure 2a , we get that P ((s, t 1 ) | G ex , p) = P ((s, t 2 ) | G ex , p) because r 1 and r 2 have the same probability. In contrast, G ex in Figure 2c has two runs (r 3 /r 4 and r 5 /r 6 , respectively) on each of \u03be 1 and \u03be 2 . Taking the sum of their probabilities yields P ((s, t 1 ) | G ex , p ) > P ((s, t 2 ) | G ex , p ) for the IRTG G ex = (G ex , A s , A t ). Inside and outside weights are a tool for efficient calculation of probabilities during training and parsing. The inside weight \u03b2(B) and the outside weight \u03b1(B) of a nonterminal B \u2208 N G are defined as \u03b2(B) = =B\u2192f (B 1 ,...,B k )\u2208R G p( ) \u2022 \u03b2(B 1 ) \u2022 . . . \u2022 \u03b2(B k ) and \u03b1(B) = \u03b4 S B + =C\u2192f (B 1 ,...,B k ) in R G 1\u2264i\u2264k : B i =B \u03b1(C) \u2022 p( ) \u2022 j =i \u03b2(B j ) , respectively, where \u03b4 S B = 1 if B = S and 0 otherwise. The sum of the probabilities of all runs of an RTG G equals \u03b2(S G ). Hence, an efficient way to obtain and Satta, 2004) . For any rule of the form P (a | G, p) is computing \u03b2(S Ga ). The expected frequency with which a nonterminal B occurs in a run of G is obtained by \u03b1(B) \u2022 \u03b2(B)/\u03b2(S G ) (Nederhof B \u2192 f (B 1 , . . . , B k ), we let \u03b1( ) = \u03b1(B) and \u03b2( ) = \u03b2(B 1 ) \u2022 . . . \u2022 \u03b2(B k ). A parsing or decoding problem Suppose we want to employ G ex for syntactic parsing (for clarity we write G instead of G ex ). This can be framed as a decoding problem: for a given string s, the chart G s is computed, from which we obtain the set T = [[L(G s )]] At of parse trees of s (i.e., T = {t 1 , t 2 } in our example). If the IRTG is equipped with a probability assignment, then, alternatively, one can ask for the best parse tree t, which may be formalized as t = arg max t\u2208T \u03be\u2208L(Gs) : [[\u03be]] A t =t P (\u03be | G, p) . (1) Computing this expression turns out to be infeasible in general because maximizing the sum over the potentially infinite set of derivation trees and the sum over the exponential number of runs over each derivation tree resists dynamic programming. A tractable option is to compute the Viterbi run r of the grammar (Knuth, 1977; Nederhof, 2003) , defined as ( \u03be, r) = arg max (\u03be,r)\u2208runs v Gs P (\u03be, r | G, p) (2) or, with a small overhead, the n-best runs (Huang and Chiang, 2005) . For a given derivation tree \u03be \u2208 T \u03a3 , we can efficiently compute or approximate P (\u03be | G, p) by restricting G to \u03be, which yields an RTG G \u03be , and computing \u03b2(S G \u03be ). Likewise, for a given parse tree t, we can compute P (t | s, G, p) = \u03b2(G (s,t) )/\u03b2(G s ). Expectation/Maximization training The expectation/maximization (EM) algorithm (Dempster et al., 1977) in the inside-outside variant (Lari and Young, 1990 ) carries over to IRTGs. We recall it for sake of completeness. During the expectation step, we compute a corpus c : R G \u2192 R \u22650 over rules given a corpus c A : A \u2192 R \u22650 over the domain such that c( ) = a\u2208A c A (a) \u2022 \u2208\u03d5 \u22121 a ( ) \u03b1( ) \u2022 p( ) \u2022 \u03b2( ) \u03b2(S Ga ) . In the maximization step the probability assignment p is updated to match the empirical distribution of c( ). Both steps are iterated until p changes only slightly or until the likelihood of a validation set drops. 3 Refinement of IRTGs with the split/merge algorithm Probabilistic context-free grammars with latent annotation (PCFG-LAs) were introduced by Matsuzaki et al. (2005) as a way to tackle the too strong independence assumptions of probabilistic CFG. Instead of assigning each CFG rule just a single probability, different probabilities are assigned depending on the substate which is annotated to each nonterminal. These substates are latent, i.e., when calculating the probability of a parse tree, any assignment of substates to nonterminals is considered. The work of Petrov et al. (2006) extends the PCFG-LA approach by a procedure that adaptively refines the latent state set. Petrov et al. (2006) start from a binarized PCFG-LA where each nonterminal has just one substate. In multiple cycles each such substate is split in two and the resulting grammar is trained with the EM algorithm. Then 50% of the splits are undone depending on how much likelihood gets lost and the grammar is trained with EM again. Finally, the rule weights are smoothed and trained once more. The idea of latent annotated grammar states can be easily formalized in the IRTG framework: The probabilistic behavior of the IRTG depends only on the nonterminals and the applied rules of its RTG G. However, in the derivation trees in L(G) and their interpretations, the nonterminals of G are no longer visible. To calculate the probability of a derivation tree \u03be, we have to consider any valid run r on \u03be and its weight. Thus, the latent states of a PCFG-LA naturally correspond to the nonterminals of the RTG G. We reformulate the split/merge algorithm by Petrov et al. (2006) for an arbitrary IRTG G = (G, \u03a3, A 1 , . . . , A l ). In the process a (fine) probabilistic RTG (G , p ) is constructed from the (coarse) probabilistic RTG (G, p), while \u03a3 and the algebras are not changed. The refinement from N G to N G allows defining a more subtle probabilistic behavior in (G , p ). Thus, L(G) = L(G ), however W (G,p) and W (G ,p ) may differ. In analogy to the original algorithm, each nonterminal is split in two by an inverse grammar morphism \u00b5 \u22121 sp yielding an intermediate grammar G f . The probabilities of the rules of G f are tuned by EM training. Afterwards, splits that turn out less useful are reversed by a grammar morphism \u00b5 \u2206 yielding the grammar G . The probabilities of this grammar are trained again and smoothed. The grammar morphisms between the grammars G, G f , and G , also imply the existence of grammar morphisms between the charts of these grammars. Consequently, charts can be easily transformed (sparing recomputation from scratch) which increases the efficiency of EM training and parsing (cf. Section 3.4). Splitting and merging We define the splitting and merging of nonterminals in a generic way by (inverse) grammar morphisms. Let (G, p) be a probabilistic RTG. For splitting the nonterminals of (G, p) we consider a surjective mapping \u00b5 : N \u2192 N G where N is a finite set (fine nonterminals) and \u00b5 \u22121 (S G ) is a singleton set. Splitting corresponds to applying the inverse of \u00b5 to G, i.e., \u00b5 \u22121 (G) = (N , \u00b5 \u22121 (S G ), R ) where R = \u00b5 \u22121 (R) = { \u2208 R[N , \u03a3] | \u00b5( ) \u2208 R}. The corresponding weight assignment for \u00b5 \u22121 (G) is p \u2022 \u00b5, which may be normalized to obtain a genuine probability assignment. For merging the nonterminals of (G, p) we consider a surjective mapping \u00b5 : N G \u2192 M where M is a finite set (merged nonterminals) and \u00b5 \u22121 (\u00b5(S G )) = {S G }. Merging is as simple as applying \u00b5 to G, i.e., computing \u00b5(G) = (M, \u00b5(S G ), \u00b5(R)). In order to construct a probability assignment \u00b5(p) for \u00b5(G), we let for every \u02c6 \u2208 \u00b5(R): (\u00b5(p))(\u02c6 ) = \u2208\u00b5 \u22121 (\u02c6 ) p( ) \u2022 \u03b1( ) \u2022 \u03b2( ) \u2208\u00b5 \u22121 (\u02c6 ) \u03b1( ) \u2022 \u03b2( ) , where \u03b1 and \u03b2 are computed with respect to (G, p). Two instances. We give two instances for grammar morphisms in reminiscence of Petrov et al. (2006) . For splitting we consider a grammar morphism \u00b5 sp that splits every nonterminal B of G but the start nonterminal into B 1 and B 2 . Formally, \u00b5 sp : N \u2192 N G where N = {B q | B \u2208 N G , B = S, q \u2208 {1, 2}} \u222a {S} and \u00b5 sp (B ) = B if B = B 1 or B = B 2 B if B = S In order to partially reverse the split of \u00b5 sp , we define the grammar morphism \u00b5 \u2206 that merges each pair B 1 and B 2 back to B based on a utility measure \u2206(B 1 , B 2 ). Formally, \u00b5 \u2206 : N \u2192 M where \u00b5 \u2206 (B ) = B if B = B q for B \u2208 N G , q \u2208 {1, 2}, and \u2206(B 1 , B 2 ) > \u03b7 B otherwise. We chose M to be the largest subset of N G \u222a N such that \u00b5 \u2206 is surjective. The function \u2206 is meant to approximate the quotient of likelihood after and before merging. This approximation, introduced by Petrov et al. (2006) , uses inside and outside weights of charts, which were precomupted during EM training, to simulate merging of single instances of B 1 and B 2 in one chart. A generalization of \u2206 can be defined for arbitrary IRTG as long as each nonterminal of some chart occurs at most once in a run (App. A.2). The parameter \u03b7 is set dynamically such that 50% of the splits are merged. 3.2 The complete split/merge cycle Algorithm 3.1 Split/merge cycle Input: IRTG G = (G, A 1 , . . . , A l ), prob. ass. p corpus c A : A \u2192 R \u22650 Output: IRTG G , prob. assignment p 1: (G f , p f ) \u2190 (\u00b5 \u22121 sp (G), p \u2022 \u00b5 sp ) 2: p f \u2190 B R E A K T I E S(p f ) 3: p f \u2190 E M -T R A I N I N G(G f , p f , A 1 , . . . , A l , c A ) 4: (G , p ) \u2190 (\u00b5 \u2206 (G f ), \u00b5 \u2206 (p f )) 5: p \u2190 E M -T R A I N I N G(G , p , A 1 , . . . , A l , c A ) 6: p \u2190 S M O O T H(G , p ) 7: p \u2190 E M -T R A I N I N G(G , p , A 1 , . . . , A l , c A ) 8: output (G , A 1 , . . . , A l ), p Splitting, merging, the EM training of Section 2.4, and a tie-breaking and smoothing step, yet to be defined, is composed to a complete split/merge cycle in Algorithm 3.1. Multiple split/merge cycles are iteratively applied to a base grammar G 0 until the resulting grammar G i reaches the desired level of refinement. For every refined grammar G i , there exists a grammar morphism \u00b5 i from G i to G 0 . During tie-breaking each rule's probability obtains a small random perturbation. During smoothing the probability of a rule is set proportional to \u03b3\u2022p( )+(1\u2212\u03b3)\u2022u where u is the sum of probabilities of rules \u2208 \u00b5 \u22121 i (\u00b5 i ( )). Intuitively, the probability of different refinements of the same rule from the base grammar is slightly aligned. Following Petrov et al. (2006) , we set \u03b3 to 0.9 for rules without nonterminals on the right-hand side. Otherwise, we set \u03b3 to 0.99. Efficient refinement of a chart G f a G f G c a G c \u03d5 a \u00b5 \u03d5 a \u00b5 Let G c be a coarse grammar, a \u2208 A be in the domain, and the chart G c a be already computed. We assume that G c was refined to G f = \u00b5 \u22121 (G c ) and that we want to compute the chart G f a . Due to the definition of splitting and merging, we do not need to compute G f a from scratch. Instead we construct (a grammar that is isomorphic to) G f a via the grammar morphisms \u03d5 a and \u00b5 such that the diagram on the right commutes. Let N = {(B, q) | B \u2208 N G c a , q \u2208 \u00b5 \u22121 (\u03d5 a (B))}, and for every (B, q) \u2208 N , set \u03d5 a (B, q) = q and \u00b5 (B, q) = B. We define G f a = \u00b5 \u22121 (G c a ). Parsing objectives and weight projections In order to apply a refined IRTG to parsing, we return to the question on how to substitute Equation 1. We consider alternative parsing objectives inspired by Matsuzaki et al. (2005) . In the following we refer to the base RTG and the one resulting from several iterations of Algorithm 3.1 by G c and G f , respectively. Also, we consider charts of a string s and assume grammar morphisms \u03d5 s , \u03d5 s , \u00b5, and \u00b5 as in Section 3.3. The t of Equation 1 is sometimes called most probable parse. A subproblem that is in general still infeasible is finding the parse tree corresponding to the most probable derivation tree, i.e., t = [[arg max \u03be \u2208L(G f s ) P (\u03be | G f , p f )]] At . For usual PCFG(-LA) this objective coincides with the most probable parse because derivation trees and parse trees are in a one-to-one correspondence. The parse corresponding to the viterbi derivation tree is t = [[ \u03be]] where ( \u03be, r) is computed according to Equation 2 for (G f s , p f \u2022 \u03d5 s ). This objective is tractable but reported to yield suboptimal parses for PCFG-LA in terms of the usual bracket scoring metric (Matsuzaki et al., 2005; Petrov et al., 2006) . A combination of coarse-to-fine parsing with n-best parsing yields the base-n-rerank objective: n-best runs of the base grammar are computed and the corresponding derivation trees are reranked according to the refined grammar. Formally, we compute t Parsing by weight projection. Matsuzaki et al. (2005) propose an alternative parsing objective, where a new weight assignment q for G c s is computed based on (G f s , p f \u2022 \u03d5 s ) such that the KL-divergence of = [[ \u03be]] At with ( \u03be, r) = arg max (\u03be,r) \u2208 n-best-runs(G c s ,p c \u2022 \u03d5s) P (\u03be | G f , p f ) . SQ \u2192 yf 1 (MD, VP, NP) yf As 1 (x 1 , (x 2 , x 3 ), x 4 ) = (x 2 x 1 x 3 x 4 ) yf At 1 (x 1 , x 2 , x 3 ) = SQ(x 1 , x 2 , x 3 ) VP \u2192 yf 2 (WHNP, VB) yf As 2 ((x 1 ), (x 2 )) = (x 1 , x 2 ) yf At 2 (x 1 , x 2 ) = VP(x 1 , x 2 ) WHNP \u2192 yf 3 () yf As 3 () = (What) yf At 3 () = WHNP(What) MD \u2192 yf 4 () yf As 4 () = (should) yf At 4 () = MD(should) NP \u2192 yf 5 () yf As 5 () = (I) yf At 5 () = NP(I) VB \u2192 yf 6 () yf As 6 () = (do) yf At 6 () = VB(do) yf 1 yf 4 yf 2 yf 3 yf 6 yf 5 SQ MD should VP WHNP What NP I VB do What should I do [[\u2022]] A s [[\u2022]] A t P (\u03be | G c s , q) to P (\u03be | s, G f , p f ) is minimized. Precisely, q = \u00b5 (p f \u2022 \u03d5 s ). Subsequently the parse tree t corresponding to the Viterbi derivation tree is computed using q, i.e., t = [[ \u03be]] At with ( \u03be, r) = arg max (\u03be,r) \u2208runs v G c s P (\u03be, r | G c s , q) . An empirically superior way to define q called max-rule-product is due to Petrov and Klein (2007) . They intent to optimize for \"the tree with greatest chance of having all rules correct, under the (incorrect) assumption that the rules correctness are independent.\" To this end, each rule is assigned the sum of the expected frequencies of its refinements, i.e., q( ) is set to \u2208\u00b5 \u22121 ( ) \u03b1( ) \u2022 (p f \u2022 \u03d5 s )( ) \u2022 \u03b2( )/\u03b2(S G f s ). Hence, the value q( ) does not have to be in the interval [0, 1] and max-rule-product (as well as maxrule-sum -where the weight of a run is the sum of rule weights rather than the product) is in theory a potentially non-monotonic and, thus, ill-defined objective (cf. Appendix A.3). The rewriting is linear, that is, each input to yf is used at most once. Parsing of binary LCFRS is in O(n 3m ) where n is the sentence length and m is the maximum fanout of nonterminals of the LCFRS (Seki et al., 1991) . Grammars for discontinuous parsing We use the following straightforward representation of LCFRSs as IRTGs: \u03a3 is the set of yield function symbols, A s is the set of tuples over strings, and the string algebra A s interprets each yield function symbol by the respective yield function. To obtain the corresponding parse tree, the derivation tree is interpreted in an algebra A t . Figure 4 depicts an LCFRS with start nonterminal SQ and the interpretation of a derivation tree. In a LCFRS/sDCP-hybrid grammar (Nederhof and Vogler, 2014; Gebhardt et al., 2017 ) (short: hybrid grammar), an LCFRS is coupled with a tree generating device called simple definite clause program (sDCP) (Deransart and Ma\u0142uszynski, 1985) . An object in the domain of a hybrid grammar is a hybrid tree, i.e., a string and a tree together with a linking between sentence positions and tree positions. Hybrid trees (and hybrid grammars) are suitable to model (formal languages of) discontinuous constituent structures. Figure 5 depicts a hybrid grammar in IRTG notation and the evaluation of the derivation tree \u03be 3 in the algebra A h to the hybrid tree h. The algebra A s is a copy of the string component of A h and evaluates \u03be 3 to \"What should I do\". We observe that the structure of \u03be 3 deviates notably from the structure of the tree component of h. In fact, the hybrid grammar generates a discontinuous tree although its string component is equivalent to a CFG.  Nederhof and Vogler (2014) present an algorithm that induces an LCFRS/sDCP-hybrid grammar G given a corpus of phrase structure trees. The algorithm is parametrized by an integer k \u2265 1 that limits the maximum fanout of the LCFRS component of G to k. A second parameter of the algorithm is one of two nonterminal labeling schemes called child labeling and strict labeling of which the former is coarser. Drewes et al. (2016) present an algorithm that computes the chart G h in time polynomial in the size of some hybrid tree h. Computing G s , given a string s, inherits the parsing complexity of LCFRSs. G : S \u2192 g 1 (A, B) g As 1 ((x 1 ), (x 2 )) = (x 1 x 2 ) A \u2192 g 2 () g As 2 () = (What should) B \u2192 g 3 () g As 3 () = (I do) g A h 1 (x 1 , x 2 ) (y 1 ) , (x 3 , x 4 ) (y 2 ) = SQ x 2 VP x 1 x 3 x 4 \uf8eb \uf8ed \uf8f6 \uf8f8 ( y 1 y 2 ) g A h 2 () = WHNP What MD should , What should ( ) g A h 3 () = NP I VB do , I do ( ) SQ MD should VP WHNP What NP I VB do What should I do h: g 1 g 2 g 3 \u03be 3 : [[\u2022]] A h Experimental evaluation We implemented 2 the generic split/merge algorithm and the parsing objectives in C++ with bindings to python3. We evaluate it with LCFRSs and hybrid grammars for discontinuous phrase structure parsing. We use the TiGer corpus (Brants et al., 2004) and employ the split of the SPMRL shared task (Seddah et al., 2014) (TiGerSPMRL) and the one of Hall and Nivre (2008) (TiGerHN08) . TiGer contains ca. 50k annotated sentences of German news text, exhibits discontinuity, and is predominantly used for evaluation in recent literature on discontinuous parsing. Evaluation with other languages is subject of further research. Part-of-speech (POS) tags for the TiGerHN08 test set are predicted using the MATE tagger (Bj\u00f6rkelund et al., 2010) , which we trained on the training and development section of TigerHN08. The base LCFRS is induced from the treebank following Maier and S\u00f8gaard (2008) . For each rule we remember the grammatical function symbol of the left-hand side nonterminal to its parent in the algebra A t . We binarize the LCFRS either right-to-left (LCFRS r2 ) or head-outward (LCFRS ho ) (cf. Kallmeyer and Maier, 2013) and apply Markovization (v = 1, h = 1). The base LCFRS/sDCP-hybrid grammar is induced according to a modified version of the algorithm by Nederhof and Vogler (2014) where we include only the POS tag in the sDCP component of a lexical rule but no additional unary categories. Also, we include syntactic function labels into the rules' sDCP component. We restrict the fanout to 2 and use strict and child labeling (abbreviated hybrid strict and hybrid child , respectively). The numbers of nonterminals and all/lexical rules of either grammar, and the coverage of trees from the development set are given in the table on the right for TiGerHN08. The LCFRSs and hybrid grammars are refined by 5 and 4 split/merge cycles, respectively. We stop EM training if the likelihood of the covered trees in the development set decreases. Then we apply the grammars in some of the ways outlined in Section 3.4 for parsing unseen sentences. Each sentence is a tokenized string s over pairs of words and (gold) POS tags. For both LCFRSs and hybrid grammars we have to carry out an LCFRS parsing step on s, for which we employ the implementation 3 of van Cranenburgh et al. (2016, see Sec. 6.4) . Precisely, a pruned version of the chart G s is computed via a coarse-to-fine pipeline that utilizes a PCFG approximation of the probabilistic LCFRS. Once a best parse has been selected, we compare it to the gold tree by computing F1 and exact match (EM) for labeled brackets, F1 for discontinuous labeled brackets, and F1 including function tags (F1-fun) using disco-dop 3 . 76.59 (29.01) 35.87 63.45 77.32 (30.94) 36.83 65.48 76.56 (29.66) 39.27 65.03 73.34 (34.47) 33.95 61.06 variational 79.09 (33.17) 41.30 67.23 79.04 (34.32) 40.85 68.74 77.48 (30.53) 40.79 66.96 73.94 (33.75) 35.28 62.34 max-rule-prod. 79.44 (33.74) 41.73 67.51 79.21 (34.54) 40.95 68.83 77.69 (30.45) 41.18 67.05 73.99 (34.02) 35.48 62.37 base-500-rerank 74.09 (29.31) 36.77 55.65 74.52 (28.82) 36.49 56.15 69.30 (25.61) 31.82 52.16 72.53 (32.98) 33.68 55.41 Table 1 : Results on the TiGerHN08 development set with gold POS tags for sentences up to length 40. The results on the TiGerHN08 development set are depicted in Table 1 . The refined grammars notably improve over the respective base grammars by up to 14.50 points in F1. If we fix a base grammar but alter the parsing objectives, then we observe the following variations in F1 and EM: Reranking the 500-best derivations of the base grammar gives worse results than the Viterbi objective on the refined grammar. The weight projection approaches again improve over the Viterbi objective by up to 2.85 and 1.13 points in F1 for LCFRSs and hybrid grammars, respectively, where max-rule-product is consistently superior to variational. We do not observe an instance where max-rule-product is ill-defined in our experiments. Figure 6 shows that the F1 decreases for longer sentences by the example of LCFRS ho /max-rule-product. LCFRS r2 is superior to LCFRS ho with the Viterbi objective but the opposite holds for the projection-based objectives. Hybrid strict , which suffers from 165 parse failures, produces worse results than hybrid child (12 parse failures) except for the reranking objective. For the discontinuous F1 and the F1-fun we find that LCFRS r2 performs better than LCFRS ho in all cases but one. Also hybrid strict outperforms hybrid child with respect to discontinuous F1 but not for F1-fun. Overall, LCFRSs outperform hybrid grammars in terms of F1. The best F1 of 79.44 and 77.69 are obtained by LCFRS ho and hybrid child , respectively, with max-rule-product. We parse the test set with LCFRS ho and hybrid child using the max-rule-product objective. We present the results and compare to other discontinuous constituent parsers of the recent literature in Table 2 . Most of these parsers are discriminative. The parsers by Hall and Nivre (2008) , Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015) , and Corro et al. (2017) employ different forms of dependency representation which are converted into constituent structures (dep2const). In contrast, Maier (2015) , Maier and Lichte (2016) , Coavoux and Crabb\u00e9 (2017) , and Stanojevi\u0107 and Garrido Alhama (2017) employ transition systems (SR-swap, SR-gap, SR-adj-gap) that can produce discontinuous constituent structures directly. Lastly, the chart-based parser of van Cranenburgh et al. ( 2016 ) is a generative model that enhances LCFRSs to discontinuous tree substitution grammars where tree fragments are learned according to the data-oriented parsing (DOP) paradigm. The results on the HN08 test set are close to the one on the development set. The F1 on the SPMRL test set is more than 4 points lower than in the HN08 split. Other parsers exhibit the same phenomenon that is probably caused by a shift in the distribution to longer sentences. Using predicted POS tags decreases the F1 by 2.38 and 2.00 points for the LCFRS and the hybrid grammar, respectively. Discussion. The results indicate a strong influence of the granularity of the base grammar's nonterminals. A low granularity results in a higher coverage but also decreases the performance of the base grammar. For instance, for hybrid grammars we see that, despite many parse failures, strict labeling outperforms child labeling with the reranking objective. The drastically lower scores of the reranking objective for LCFRS r2 , LCFRS ho and hybrid child in light of the small difference with hybrid strict are likely caused by the base grammars being too coarse and, thus, assigning higher probabilities to bad candidates. Moreover, we suppose that including some context in the base grammars' nonterminals helps to guide the split/merge algorithm and avoids overfitting: For hybrid child the accuracy drops if we run more than 4 split/merge cycles. Also, in early experiments we observed worse performance if the conditioning context of Markovization for LCFRSs is further restricted. It will be interesting to study hybrid grammars whose base grammars have slightly finer nonterminals than with child labeling. The EM algorithm is prone to overfitting to the training corpus. In fact, in our experiments we observed that the validation likelihood decreased after some epochs of training whereas the smoothing step counteracted this trend. To improve robustness, we plan to investigate changes in the training regime,  e.g., adding a prior on probability assignments or merging a higher percentage of nonterminal splits. It is not surprising that the best results are obtained with the projection-based parsing objectives: the loss function of EM training does not guarantee that the probability mass of one derivation is concentrated in a single run. That max-rule-product outperforms the variational approach in terms of F1 and EM is in accordance with the findings and interpretations of Petrov and Klein (2007) . For hybrid grammars the improvements of the projection-based methods are smaller than for LCFRSs which may be explained by the additional layer of spurious ambiguity (i.e., multiple derivation trees for one hybrid tree) they exhibit. The F1 on discontinuous brackets is much lower than the overall F1 where the discontinuous recall is particularly low. Each grammar predicts only between 631 and 989 discontinuous brackets where there are 1837 in the gold standard. This could be affected by the way we approximate the PCFG in the coarse-to-fine pipeline which penalizes discontinuous rules. Most discontinuous brackets are predicted with the reranking objective but, as the lower discontinuous F1 indicates, these brackets are often incorrect. Table 2 shows that the systems proposed in the literature exhibit a higher F1 than our grammars. This holds in particular for the systems that employ discriminative (and sometimes global) features, which are not available in our system. On the other hand, also van Cranenburgh et al. (2016) 's DOP-based parser is superior to our system in the predicted POS tag scenario. One reason might be error propagation due to wrong POS tags predicted by the external tagger that we use. In contrast, in van Cranenburgh et al. (2016) POS tags are predicted during parsing. Also, the probability assignment that we obtained by EM training may be less robust than the rule probabilities obtained according to the DOP paradigm. Conclusion. The state refinement method considerably improves over the baseline grammars, which confirms our hypothesis that the usefulness of the split/merge algorithm goes beyond parsing with CFGs. However, at least with the used version of EM training, the initial granularity of the nonterminal set has a decisive impact on the quality of the resulting grammar and should be chosen carefully. When considering the task of discontinuous parsing, the results fall behind recent advances in the literature. This holds in particular for the discriminative deterministic transition-based systems where the representable discontinuity is not restricted by grammar constants, non-local features may be considered, and the parsing is much faster. One remedy to the lower accuracy and speed could be the enhancement of our chart-based method with a discriminative classifier to guide the pruning of the chart (Vieira and Eisner, 2017) . In the future one may advance the understanding of the split/merge algorithm by applying it to other IRTGs such as synchronous hyperedge replacement grammars for graph parsing (Peng et al., 2015) or in a task different from parsing like syntax-based machine translation with synchronous grammars (Chiang, 2007) . Acknowledgements The author thanks the anonymous reviewers for many helpful comments, Markus Teichmann for contributing to the implementation, Heiko Vogler and Tobias Denkinger for feedback on a draft of this paper, and Andreas van Cranenburgh and Toni Dietze for helpful discussions. The implementation utilizes disco-dop by Andreas van Cranenburgh, treetools by Wolfgang Maier, and the Eigen template library. A Appendix In this appendix we provide auxiliary calculations and definitions, an example in which the max-ruleproduct parsing objective is ill-defined, and choices of hyperparameters and preprocessing steps used during the experiments. A.1 Computing the probability of an object using its chart In Section 2.2 we state that P (a | G, p) = \u03be\u2208L(Ga) W (Ga,p\u2022\u03d5a) (\u03be). This follows from: A.2 Approximation of the likelihood loss. Let G c be the RTG at the beginning of a particular split/merge cycle and G f = \u00b5 \u22121 sp (G c ) be the RTG after splitting. We describe how \u2206(B 1 , B 2 ) is computed for a pair B 1 and B 2 of nonterminals in G f that are candidates for merging. Similar to Section 3.3, for each a \u2208 A we assume grammar morphisms Firstly, we compute merge factors p 1 and p 2 based on the relative frequency of B 1 and B 2 where . To compute the expected frequency of B i (i \u2208 {1, 2}), we sum over the expected frequency of each occurrence Secondly, we consider pairs (B 1 , B 2 ) of occurrences of B 1 and B 2 in some chart G f a such that \u00b5 sp (B 1 ) = \u00b5 sp (B 2 ). For each such pair, we introduce a hypothetical nonterminal B which symbolizes merging just B 1 and B 2 . Its inside and outside weight is obtained by \u03b1 Using these hypothetical nonterminals, we approximate the loss in likelihood due to merging B 1 and B 2 by accumulating likelihood losses due to merging pairs of occurrences: In the above formula, the numerator shall express the probability of the chart after merging and the denominator expresses the probability before merging. If a nonterminal A occurs more than once in a run of the chart, then the sum of the probability of all runs that contain A is not equal to its expected frequency \u03b1(A ) \u2022 \u03b2(A ). Thus, the above formula is reasonable under the assumption that each nonterminal A in N Ga occurs at most once in a run on a derivation tree in L(G f a ). This assumption is violated if, e.g., the chart contains a chain rule A \u2192 f (A ). A.3 The max-rule-product objective can be ill-defined We give an example of a CFG with chain rules where max-rule-product is an ill-defined parsing objective. Consider the refined CFG G f with nonterminals {S, A 1 , A 2 , A 3 }, terminals {a}, and rules with probabilities: Consider the chart G f a for the string a which is isomorphic to G f . The inside and outside weight of each nonterminal of G f a is 1.0. If we merge A 1 , A 2 , and A 3 to A and project weights according to the max-rule-product principle, then we obtain the CFG G c a with weights Now the weight of some parse tree of G c a is exponential in the number of occurrences of the chain rules A \u2192 A it contains. Consequently, there cannot be a best tree. A.4 Hyperparameters and preprocessing The TiGer corpus is preprocessed before grammar induction. Specifically, punctuation tokens are attached to lower nodes in order to reduce the number of discontinuous brackets using disco-dop. Also, we replace words with less than 4 occurrences in the training corpus by a fresh \"UNKNOWN\" symbol. In addition, the training set and the validation set are enriched by copies of the constituent trees where every word is replaced by the \"UNKNOWN\" symbol. These copies get assigned frequency 0.1 in either set. During the split/merge algorithm EM training is applied as follows. The probability assignment p i of the i-th epoch (m \u2264 i \u2264 20) with the best validation likelihood is selected as result. We set m = 2 after smoothing and m = 5 otherwise. Also, we skip all remaining EM epochs, if validation likelihood dropped in 6 consecutive epochs. When computing validation likelihood, we ignore trees with probability 0 except after smoothing.",
    "abstract": "We formulate a generalization of Petrov et al. (2006) 's split/merge algorithm for interpreted regular tree grammars (Koller and Kuhlmann, 2011), which capture a large class of grammar formalisms. We evaluate its effectiveness empirically on the task of discontinuous constituent parsing with two mildly context-sensitive grammar formalisms: linear context-free rewriting systems (Vijay-Shanker et al., 1987) as well as hybrid grammars (Nederhof and Vogler, 2014).",
    "countries": [
        "Germany"
    ],
    "languages": [
        "German"
    ],
    "numcitedby": "9",
    "year": "2018",
    "month": "August",
    "title": "Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing"
}