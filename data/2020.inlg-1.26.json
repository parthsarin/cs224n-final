{
    "article": "Personalised response generation enables generating human-like responses by means of assigning the generator a social identity. However, pragmatics theory suggests that human beings adjust the way of speaking based on not only who they are but also whom they are talking to. In other words, when modelling personalised dialogues, it might be favourable if we also take the listener's social identity into consideration. To validate this idea, we use gender as a typical example of a social variable to investigate how the listener's identity influences the language used in Chinese dialogues on social media. Also, we build personalised generators. The experiment results demonstrate that the listener's identity indeed matters in the language use of responses and that the response generator can capture such differences in language use. More interestingly, by additionally modelling the listener's identity, the personalised response generator performs better in its own identity. Introduction Persona plays an important role in our daily communication since it affects the way we render our dialogues. Social variables, such as gender, age, place of birth or even wealth and social status, account for a large proportion in each individual's persona. Numerous previous studies have suggested that these variables strongly affect each speaker's word preference in dialogues. A growing body of works has been carried out to implicitly or explicitly model these variables in dialogues (Li et al., 2016b; Qian et al., 2017; Kottur et al., 2017; Zhang et al., 2018; Zheng et al., 2019 Zheng et al., , 2020b)) . Despite the reported success, most previous studies for personalised dialogue modelling consider only the persona of speakers. 1 Nevertheless, the 1 For using the terminology consistently, we use \"speaker\" pragmatics theory suggests that the speaking style will be adjusted not only by who the speaker is, but also whom the speaker is talking to (Wish et al., 1976; Hovy, 1987) . In the computational linguistics community, Dinan et al. (2020) investigates this issue by measuring and mitigating gender bias in dialogue dataset utilising a gender classifier. From the aspect of personalised dialogue generation, Zhang et al. (2018) and Zheng et al. (2020b) tried to attach the listener persona to the encoder of their generator, but interestingly, they obtained very different results, namely, the performance of Zhang et al. (2018) went down while that of Zheng et al. (2020b) went up. Nonetheless, no systematic studies have been conducted to investigate what role does the listener's identity play in personalised response generation. Research questions that we wish to answer by the proposal put forward in this paper are: 1. How the listener's social identity impacts the responder's language use; 2. Can a response generator capture this impact, referring to the person who produces the response (who is also a personalised dialogue system heading to model) and \"listener\" referring to the one who utters the post. if yes, in which way? To this end, we apply analysis and build a response generator on a Chinese personalised dialog dataset: PERSONALDIALOG, a corpus extracted from Weibo 2 . There are two reasons to use this dataset: one is that the PERSONALDIALOG dataset origins from the real conversations on social media Weibo, in which speakers' social variables play an important role; the other is that this dataset provides a massive amount of dialogue data (over 20M sessions) between a large population of speakers (over 8M speakers). It is of sufficient size to capture a variety of linguistic phenomena that are associated with social variables. Each speaker/listener in PER-SONALDIALOG comes up with 4 social variables: gender, age, location, and interests. For simplicity and for conducting controlled analysis and experiments, we only focus on gender in this paper. As for the first research question, we postulate that a speaker behave differently when s/he speaks to people with different gender stylistically. This yields four possible speaking styles: ff, mf, fm, and mm 3 . We, therefore, build a classifier to separate these styles defining on \"gender-pairs\". Previous analysis on blogging data (Schler et al., 2006; Goswami et al., 2009; Nguyen et al., 2011; Bamman et al., 2014) has identified that one of the key features for distinguishing contents produced by a female from those by a male is the sentence length, i.e., females tend to utter longer sentences. As shown in Figure 1 , the same phenomenon is found in PERSONALDIALOG: females' responses are generally longer than males'. Further statistics on the response length falling the above four styles suggest that gender-pairs are also separable, perhaps excepting mf and fm at first glance. To validate this and understand why, we build a gender-pair classifier and conduct so-called pivot word analysis. We find out which word contributes the most for helping the classifier make decisions. Experiment results show that these styles are separable, but mf and fm are often confused with each other. As for the second research question, we build a personalised response generator conditioning on these styles. The outcomes suggest that the generator could capture the difference between those styles and, in addition, modelling the listener's identity helps the generator to express its own identity. Moreover, based on previous analyses, we have also tried to merge the style of mf and fm into a single integrated style mf/fm. However, the final results of the response generator suggests that it is hard to model utterances with this integrated style. Gender-Pair Classification To approach the first research question, we build a gender-pair classifier to simultaneously recognise the speaker's and listener's social identity based on the dialogue utterances. Concretely, as aforementioned in section 1, we assume the present task as a style classification task and design four labels for each input dialogue utterance: mm (male talking to male), mf (female talking to male), fm (male talking to female), and ff (female talking to female). However, in light of the Linguistic Style Matching theory (Niederhoffer and Pennebaker, 2002) , speakers will imitate the linguistic style of their conversation companion to pursue higher engagement. In other words, when two different gendered speakers communicate with each other, their speaking style may assimilate to each other as the conversation proceed. On the top of this observation, one may say that dissociating fm and mf is hard, and, therefore, it would be favourable if we merge fm and mf into a single category, namely mf/fm. Build Gender-Pair Classifiers Building on what has been discussed, to further get insight from conventional gender classification, we consider the following three classification tasks basing on three speaking style categorisation schemes: 1) two-way classification: classifying only speakers' gender, in which two labels are used: male and female; 2) three-way classification: classifying the conversational texts based on a merged labelling scheme, i.e., three labels are considered mm, fm/mf, and ff; and 3) four-way classification: the gender-pair classification which classifies the conversational texts into mm, fm, mf, and ff. Classification Models We test a number of text classification algorithms, including fastText 4 (Joulin et al., 2017) , TextCNN (Kim, 2014) and LSTM (Hochreiter and Schmidhuber, 1997) (in which the hidden states of all the tokens are max pooled before being feed into the final Softmax layer). In order to conduct interpretable analysis, we train a Bag-of-Word (BOW) classifier: a logistic regression with only unigram features. Experimental Settings Building on the fact that classifying the social variables based on the social media data is hard (Nguyen et al., 2013 (Nguyen et al., , 2014)) , and the exhibition of speakers' social identities is sparse in social media text (Zheng et al., 2020b) , we adopt the classification strategy used by Zheng et al. (2019) . Specifically, each classifier input is a concatenations of N randomly sampled responses with the same style. In this study, we use N = 20. We train and test the classifiers on PERSONALDIALOG, where the dataset has been divided into training and testing sets without overlapping. The training data are down-sampled to balance the corpus. 10% of the training set is held out for tuning parameters, and the final models are trained on the whole training set. The classifiers are evaluated using F1 scores. Experimental Results Table 1 depicts the performances of these classifiers. FastText performs remarkably well. It outper- 4 The official implementation of fastText from Facebook is used: https://github.com/facebookresearch/ fastText. forms both TextCNN and LSTM, which are models having much higher complexity and capacity. It is surprising that the simplest BOW classifier also achieves comparably good performance, which suggests that the word usage is the most important feature for distinguishing speakers' social identity (at least for the gender). Further comparison of the fastText and BOW classifier embodies that the unigram features are sufficient for conducting gender classification in the coarse 2-way classification setting, while higher-ordered N-gram features (used by the fastText) are useful in more fine-grained 3-way and 4-way classification settings. The F1 score of the 4-way gender-pair classification using fastText reaches 0.68. This means that it is feasible to identify the style of the listener by only considering the utterances issued by the speaker. We print the confusion matrix of this result in Figure 2 . The utterances from ff and mm are rarely confused with each other. This indicates that the language use of both males and females have clear differences when they speak to people with the same gender When they talk to people with different gender, in line with the results of gender classification, they tend to express stylistic characteristics related to their own gender since confusions appear between fm and mm as well as between mf and ff. Nonetheless, we also observe equally severe confusion between fm and mf, which approves that the linguistic style matching hypothesis plays a certain role when people expressing their social identities. In addition, we also observe a certain level of confusion between fm and ff as well as between mf and mm. This said, the classifier sometimes confuse between, for example, an utterance from a male and an utterance from a female when they both speak to male listeners. This, yet again, could be seen as an evidence for the existence of linguistic style matching. Although the utterance from fm and mf shows a tendency of assimilation, it appears that the speakers still maintain the characteristics of their own gender and, in this sense, there are still certain reasons to disassociate the style of fm from mf. Pivot Word Discovery To understand how people change their language use with respect to social identities of themselves and of whom they speak to, or, in other words, to understand how the gender-pair classifiers make end if 12: end for 13: return All t in \u2126 c if p(t, s) > \u03b2 for all s \u2208 S their decisions, we apply the Pivot Word Analysis. Pivot words are words that have substantial influence on the classifier's decision making and have been widely used for interpreting the language use in many language generation tasks such as Style Transfer (Fu et al., 2019) and Table-to-Text Generation (Ma et al., 2019) . Pivot Word Extraction Algorithm. Since the expression of social identity is sparse in the social media data, the appearance of pivot words in the utterance is also sparse. Therefore, the pivot word discovery algorithms introduced in (Fu et al., 2019) and (Ma et al., 2019) are not applicable in the present task. Instead, we use a simple yet efficient pivot word discovery algorithm coined as Classifier-based Pivot Word Discovery for extracting pivot words using the trained BOW classifier. The algorithm is of finding out which word type in the training data plays a major role in the BOW classifier's decision-making. It is sketched in Algorithm 1. As can be seen from lines 2-5, this algorithm only considers samples that have been correctly classified. For each word type t in a sample x, it compares the classification results and confidences when including and excluding t in x (lines 2-8). Specifically, if the classifier's predicted result is changed or the prediction confidence's change exceeds a certain threshold of \u03b2, we extract it as a pivot word candidate (line 10). If the same word type has been extracted as a candidate for more than \u03b1 times under a single category, the algorithm returns it as a pivot word (line 15). In this work, we set \u03b1 and \u03b2 to 10 and 0.5, respectively. Extracted Pivot Words. Table 2 lists typical examples of the extracted pivot words in each category for the gender classifier and the gender-pair classifier. As for the gender classification, we observe that the general topics used by males and females have clear differences on Weibo. Specifically, males focus on the topic of digital products, politics, and games while females like talking about starstruck, teleplays, makeup, and shopping. It is worth noting that one reason that Weibo users concentrate on these topics is that most of them are young people according to the statistics in Zheng et al. (2019) . These topics might change if use data extracted in more recent years since the PERSONALDIALOG dataset was crawled in 2018. More interestingly, we also find that differences exist in the use of punctuation and pronouns. Males use punctuation in a more formal way on social media (in which comma and period are frequently used), but females eager to concatenate a sequence of punctuation to express certain emotions or speech acts (e.g., \"\u223c\u223c\", \"!!!!\"). The first person pronoun was extracted as pivot word for the female category, which might suggest that males are more likely to drop pronoun on social media. 5  To say the last word on how the use of zero pronouns is affected by the speaker's social identity needs further research, which is not the focus of this paper. As for comparing the extracted pivot words for the gender-pair classifier and the gender classifier, in line with the classification results detailed in section 2.1, we observe more overlaps between female and ff as well as male and mm than between female and mf as well as male and fm. When comparing the words from different gender-pair categories, we find that people would talk about different topics when they talk to people of the same gender and with a different gender. For Model Example Pivot Words mm \u534e\u4e3a (Huawei), \u82f9\u679c (Apple), \u4e09\u661f (Samsung), \u5c0f\u7c73 (Xiaomi), \u7f8e\u56fd (America), \u65e5\u672c (Japan), \u4e2d\u56fd (China), \u5927\u9646 (Mainland), \u53f0\u6e7e (Taiwan),\"\uff0c\", \"\u3002\" fm \u6e38\u620f (game), \u738b\u8005 (Honer of Kings), \u65e9\u5b89 (good morning), \u665a\u5b89 (good night), \u62cd\u7167 (photograph), \u8bfb\u4e66 (reading), \u5de5\u4f5c (working), \u6211 (I), \u4f60 (you) mf \u5927\u53d4 (Uncle), \u5f1f\u5f1f (little Brother), \u54e5\u54e5 (elder Brother), \u4e0a\u73ed (Working), \u559d\u9152 (Drinking), \u53a6\u95e8 (Xiamen), \u5e7f \u4e1c (Guangdong), \u5e7f\u5dde (Guangzhou), \u55ef\u55ef (Uh-huh), \u6211 (I), \u4f60 (you), \"\u223c\u223c\", \"!!!!\", \"???\" ff \u738b\u4fca\u51ef (a celebrity), \u6613\u70ca\u5343\u73ba (a celebrity), \u9e7f\u6657 (a celebrity), KPop, \u7537\u4e3b (leading actor), \u7535\u89c6\u5267 (teleplay), \u5316\u5986 (make up), \u6f02\u4eae (beauty), \u88d9\u5b50 (skirt), \u4fbf\u5b9c (cheap), \u6dd8\u5b9d (Taobao)\uff0c \u55ef\u55ef (Uh-huh), \u554a\u554a\u554a (Ah Ah Ah), \u6211 (I), \u4f60 (you), \"\u223c\u223c\u223c\u223c\", \"!!??\", \"!!!!\" male \u534e\u4e3a (Huawei), \u82f9\u679c (Apple), \u7f8e\u56fd (America), \u5927\u9646 (Mainland), \u53f0\u6e7e (Taiwan), \u59b9\u5b50 (girl), \u5ab3\u5987 (wife), \u6e38\u620f (game), \"\uff0c\", \"\u3002\" female \u738b\u4fca\u51ef (a celebrity), \u6613\u70ca\u5343\u73ba (a celebrity), \u7537\u4e3b (leading actor), \u7535\u89c6\u5267 (teleplay), \u5316\u5986 (make up), \u88d9\u5b50 (skirt), \u9762\u819c (mask), \u5218\u6d77 (bang), \u6211 (I), \"\u223c\u223c\", \"!!!!\", \"\u223c\u223c\u223c\u223c\", hhh, QAQ, mua  example, when a female talks to another female, they discuss \"idols\" they like, shopping, and dressing, which are rarely mentioned when she talks to a male. These observations explain why utterances with style mf (fm) are separable from those with style ff (mm) and suggest that the identities of listeners really matter the way of how speakers speaking. As for the linguistic matching hypothesis, some evidences have been found. For example, fm and mf shared some topics including travelling, studying, working or gaming. Moreover, first person pronouns are more likely to be used when males speaking to females, but similar matching not appears in the use of punctuation. Pivot Free Classification In order to quantify how the gender-pair influences the language use, we do a Pivot Free Classification experiment, where the BOW classifier is evaluated on the test data, in which the pivot words from a certain category are removed. Since we care about, by removing the pivot words, how many samples of a category are mis-classified into other categories, we report the recall scores in Table 3 . We test the performance of the gender-pair classifier \"attacked\" by pivot words extracted by the gender-pair and the gender classifier. We name the category on which we report the performance as the target category and the category from which we extract the pivot words as the source category. On the basis of the results in Table 3 , we have the following observations: First, the performance reduces to almost zero if the source and the target are the same categories, which implies that the extracted pivot words are those which actually bias the decision making of the classifier. Second, ff and mm are definitely separable as no impact is found when they \"attack\" each other. Third, in line with the previous findings and the linguistic style matching theory, mf and fm are highly confused with each other, which can be approved from two dimensions: 1) as source categories, they highly reduce each other's performance; 2) Pivot words from female have remarkably effects on not only mf and ff but also fm. Fourth, mf and fm are not exactly the same, since, for instance, the impact of mf on ff is clearly higher than that of fm on ff. Last, the style of a conversation for speakers with a different gender is more similar to the style of how females speak. Personalised Response Generation For exploring the second research question, that is, can a personalised response generator capture the differences of language use when imitating a speaker talking to listeners with different social identities? We train multiple response generators conditioning on the three style categorising schemes mentioned in section 2. We start by introducing the basic architecture of our generator and the experimental settings. We then describe the evaluation metrics we use, with which we evaluate and analyse the generators. The Personalised Response Generator Since inventing a new state-of-the-art personalised response generator falls out of the scope of this paper, we build the model following a simplified paradigm of Zheng et al. (2020a,b) . The architecture of the model we used is sketched in Figure 3 . Concretely, given the dataset, containing N dialogue pairs with each of their style: D = {(x 1 , y 1 , s 1 ), ..., (x N , y N , s N )}, where x i is the post, y i is the response, and s i is the style label of that response (i.e., in our case, it could be female or fm). As depicted in Figure 3 , each post x is firstly mapped into word embedding space using e w (\u2022) and then is encoded via a Transformer (Vaswani et al., 2017) based encoder to a representation E x . Encoding Style Information Following Zheng et al. (2020b) , in the decoding phase, we inject the style information by utilising the attention routing mechanism. Specifically, different from the standard Transformer decoder, both multi-head attention (MHA) and masked multihead attention (MMHA) are deployed. In each decoder block, given the E x and the embedded previously decoded response E ypre = e w (y pre ), they are encoded to: R pre = MMHA(E ypre , E ypre , E ypre ) (1) R post = MHA(E ypre , E x , E x ) (2) Together with the mapped style, E s is mapped using the style embedding e s (s). These set of representations are merged in the following way to R before being feed for layer normalisation: R = (R pre + R post )/2 + E ypre + E s (3) in which, R pre and R post are averaged. Despite of the simplicity, one major reason of why we do not use the original model of Zheng et al. (2020b) in this study is that they did not encode personae (i.e., gender in our case) of speaker and listener symmetrically. To be more specific, they encode the persona of the listener as a number of style embeddings, which were added to the input together with the positional embeddings, while the speaker's persona was encoded as a sequence of words and was concatenated with the embeded post x. This kind of disassociation makes our experiments less controlled. Instead, in this study, we merge the label for speakers and listeners (i.e., the label such as mf) and map it into a single style embedding. Parameter Sharing and Pre-training. Encoders and decoders in our model are sharing their parameters. To further increase the quality of the generated responses, akin to many previous research in dialogue modelling (Wolf et al., 2019; Wang et al., 2020a) , we initialise the parameters in our model using a pre-trained Chinese GPT model (Radford et al., 2019; Wang et al., 2020b) . Experiments Experimental Settings We train and evaluate the model on the PERSONAL-DIALOG dataset. For simplicity, in line with Zheng et al. (2019) , we only train and test our model using the first turn of each dialogue session in PER-SONALDIALOG. For conducting a controlled and fair analysis, we train three models corresponding to the three style categorisation schemes introduced in section 2 (see Table 4 ). In the following sections, we refer them with their ID, i.e., model 1, 2, or 3. Evaluation Metrics Recall that our target is not of defeating state-ofthe-art personalised response generator in the sense of generating better responses. Nonetheless, we still report some relevant results using commonly used automatic metrics including: BLUE (Papineni et al., 2002) , a metric comparing overlaps of n-grams (n = 1, 2) between the reference responses and the generated responses for evaluating the adequacy and fluency; and DIST (Li et al., 2016a) , measuring the proportion of distinct ngrams (n = 1) for evaluating the diversity of the model outputs. To help obtaining insights from the system outputs for the second research question, we design a number of new metrics based on the built classifiers and extracted pivot words from section 2. Specifically, for evaluating a model with n style categories, we propose the following metrics: 1. ACC. evaluates whether the generated responses incorporate the target style using the trained n-way classifier. Similar approach is employed to evaluate the outputs of conditional language generators with off-line classifiers (Zhou et al., 2018; Zheng et al., 2019; Li et al., 2020) . During evaluation, the system outputs are concatenated in the same way as the train data of those classifiers. Considering the speed and the performance, we use the fastText classifier in the evaluation; 2. ACC-2. evaluates whether the generated response reflect gender information using the trained gender classifier. It is worth noting that this metric is not applicable to model 2 since we have merged the mf and fm, we expect that they are no longer separable; 3. Pivot Word Precision (PWP). evaluates to what proportion the generated tokens are pivot words. Suppose the system outputs with style s is \u0176s with the vocabulary V and the pivot words extracted by n-way classifier is \u2126 s , the PWP is computed by: PWP s = w\u2208\u2126s #(w, \u0176s ) w\u2208V #(w, \u0176s ) where #(w, \u0176s ) is the frequency of w in \u0176s . PWP is calculated for each style and is then micro-averaged; 4. Pivot Word Recall (PWR). evaluates how many word types in pivot words has been generated: PWR s = w\u2208\u2126s I(w, \u0176s ) |\u2126 s | (5) where I(w, \u0176s ) equals to one if w appears in \u0176s , otherwise it equals to 0. Experimental Results Table 4 charts the results of all the metrics above. It is not surprising that no significant difference is found in BLEU and DIST score between all three models since all of them have the same model architecture, the same parameter setting and, thus, the same capacity. Due to the fact that different off-line classifiers have very different performance in their own domain (see  ). It appears that although the generator has produced fine amount of pivot words for expressing the style of mf/fm, but, the frequency of many of them might not be high. This also suggests that even though we found some evidences from experiments in section 2 supporting the theory of linguistic matching and the merging of mf and fm, but it seems that the generator we use cannot handle this. More interestingly, we also find that model 3 not only has the highest performance on PWP, which means more than half of the tokens it produces are pivot words of the correct style, but also has the highest score on ACC-2 (i.e., the accuracy of gender classification), which is even better than model 1, a model that originally designed having two styles. This approves that by additionally modelling the social identities of the listeners, it helps the generator to utter more speaker identity related words because it takes the difference on speaking style when talking to listeners with different social identities into account. Cross-category PWR To understand how model 3 works, we consider similar experiment to the one in section 2.3 by measuring the cross-category PWR. From Table 5 , we observe similar phenomenon as in section 2.3. For example, the pair mm and ff yields the lowest PWR when being as the pivot word source of each other. In contrast, they reach the highest score if they are their own pivot word source. fm and mf have relatively high PWR when being each other's pivot word source. When a male talks to an another male, they say very few words that females always say. Nevertheless, we also observe that sentences produced by ff always have the highest PWR regardless of where the pivot words are coming from. This should be a result of two reasons: most conversations in PERSONALDIALOG dataset are between two females and PRR is a metric that sensitive to the size of test data (i.e., it is very likely that the more sentences are produced the more pivot words are included). Discussion We investigated the language use on Chinese social media regarding to the social identities of speakers and listeners. Specifically, we aim to explore whether the listener's social identities impact the responder's language use and whether such differences are separable. The primary answers to both of these questions are \"Yes\" on the basis of our experiments and, additionally, by conducting pivot word analysis, we also found that mf and fm are less separable owing to the linguistic matching phenomenon. This raises as open question of which style categorisation scheme (i.e., whether to distinguishing mf and fm or not) is better for modelling personalised dialogues. We then trained personalised response generators which take the social identities of listeners into account. To conduct insightful analysis, we design a number of new metrics with the help of the speaking style classifiers and the extracted pivot words. The outcomes show that modelling listener's identity assists the dialogue system to express more of its own identity. However, our system failed to model the style of mf/fm, which suggests the necessity of disassociating the style between mf and fm. Note that our work focus mainly on the gender, which from our perspective, underlies further studies on investigating the influence of other listener's social variables, such as age or location, or even of listener's persona as a whole. Likewise, since we study only on data from Chinese social media, it is also worth to validate whether our findings still hold in multilingual platforms like Twitter. As for the designing of dialogue systems, we highlighted the importance of modelling listener's persona for the Chatbot to express its own personality, it is also worthwhile to evaluate the built system in other angles, such as relevance and fluency, or to validate whether the resulting chat machine is empathetic (Fung et al., 2018) or not. Our decision on using single turn dialogue also limits the generalisability of our conclusion to real conversations since the assimilation of each others style may progress in the course of a dialogue. This may result in under-estimating the effect of the linguistic matching between speakers and listeners. In future, we will extend our work into multi-turn dialogue modelling. Ethical Statement In this paper, we use the gender as an example of social identity to understand how the speaking style of a speaker is influenced. To this end, we build gender classifiers and stylised dialogue systems. In light of the discussion in Larson (2017) , gender is notoriously difficult to detect (Buolamwini and Gebru, 2018) , and mis-gendering individuals is harmful to users (Keyes, 2018) . Therefore, we are not and will not apply or extend the built classifiers and dialogue systems into real applications. We hope our findings could help with further works on mitigating gender bias (Liu et al., 2020) or improving fairness (Liu et al., 2019) in dialogue systems. Acknowledgements We thank the anonymous reviewers for their helpful comments. Guanyi Chen is supported by China Scholarship Council (No.201907720022).",
    "abstract": "Personalised response generation enables generating human-like responses by means of assigning the generator a social identity. However, pragmatics theory suggests that human beings adjust the way of speaking based on not only who they are but also whom they are talking to. In other words, when modelling personalised dialogues, it might be favourable if we also take the listener's social identity into consideration. To validate this idea, we use gender as a typical example of a social variable to investigate how the listener's identity influences the language used in Chinese dialogues on social media. Also, we build personalised generators. The experiment results demonstrate that the listener's identity indeed matters in the language use of responses and that the response generator can capture such differences in language use. More interestingly, by additionally modelling the listener's identity, the personalised response generator performs better in its own identity.",
    "countries": [
        "Netherlands",
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "0",
    "year": "2020",
    "month": "December",
    "title": "Listener{'}s Social Identity Matters in Personalised Response Generation"
}