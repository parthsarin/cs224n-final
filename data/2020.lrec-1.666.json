{
    "article": "Semantic matching measures the dependencies between query and answer representations, which is an important criterion for evaluating whether the matching is successful. In fact, such matching does not examine each sentence individually, because the context information between sentences should be considered equally important to the syntactic context inside a sentence. Considering the above, we propose a novel QA matching model, built upon a cross-sentence context-aware architecture. Specifically, an interactive attention mechanism with a pre-trained language model is presented to automatically select salient positional answer representations that contribute more significantly to the answer relevance of a given question. In addition to the context information captured at each word position, we incorporate a quantity of context jump dependencies to leverage the attention weight formulation. This can capture the amount of useful information brought by the next word, is computed by modeling the joint probability between two adjacent word states. The proposed method is compared with multiple state-of-the-art methods using the TREC library, WikiQA, and the Yahoo! community question datasets. Experimental results show that the proposed method outperforms satisfactorily the competing ones. Introduction Question answering (QA) is the task of enabling a machine to automatically answer questions posted by humans in a natural language form. The selection of the best answer from an existing pool of candidate answers is referred to as community QA (cQA) (Shah and Pomerantz, 2010) , whereas enabling the computer to automatically generate a novel answer, through some natural language models, is also known as machine dialogue (Shen et al., 2017b) . In this paper, we focus on cQA by working on the semantic matching between question and answer texts. In general, semantic matching requires the accurate modeling of the relevance between two portions of text, and, in addition to QA, is widely used for tasks, such as paraphrase identification (Cheng and Kartsaklis, 2015) , machine translation (Bahdanau et al., 2015) , image caption generation (Karpathy and Fei-Fei, 2015) and video hyperlinking (Hao et al., 2019b) . In order to compute an accurate measure of relevance between the question and answer, it is beneficial to take the lexical, syntactic and semantic information of the text pairs into account. Traditional matching seeks effective ways of extracting semantic features to improve a given similarity metric (Meng et al., 2013) . Recent advances have managed to replace this manual feature engineering process with a model that automatically learns distributed representations of words and sentences via neural networks (Wan et al., 2016; Hao et al., 2019a) . As previously mentioned, the goal of a cQA matching task is to select the correct answers from a set of candidate answers based on the content of a given question. One of the key hindrances in this, is that the key lexical components and information might not be shared between question and answer texts. In some cases, ambiguous contents in questions or answers may impede this process. Hence, the question-answer matching process can become more effective when the sentence representations for questions and answers are learned interactively, rather than in isolation. Question Q 1 \u2022 What is the color of that cat? Candidate Answers: A 31 and A 32 \u2022 A11: The cat was sitting on a mat. \u2022 A12: The cat that was sitting on a mat was red. Answer Key Components A11 The cat was sitting on a mat. A12 The cat that was sitting on a mat was red. Figure 1 : cQA Example scenario 1 with 'cat' case. Fig. 1 shows a cQA scenario, where the aim is to answer the question, each with their own pool of answers. We highlight the key components for the answer in the figure. In both examples, these salient components in the answers directly reflect or respond to the context of the questions, which contribute more significantly towards the relevance of the given question. Such salient information or the key components in sentences can be captured by an attractive approach called attention mechanism (Bahdanau et al., 2015) . This mechanism has been mostly used in the tasks related to translations (Bahdanau et al., 2015) . Recent findings, for example in (Tan et al., 2016; Hermann et al., 2015) , have demonstrated their applicability in assigning degrees of importance, known as attention weights, to different word positions in a sentence. In the cQA community, it has been of increasing interest to develop effective ways of building attention mechanisms, so that the matching per-formance can be improved by learning from and paying more attention to salient text (Tan et al., 2016; Xiong et al., 2017) . In this paper, we address the aspects that have been highlighted above, and propose a novel approach to improve the standard of the response accuracy. In particular, we make the following key contributions: 1. We extend the notion of interactive learning by developing a cross-sentence context-aware bi-directional LSTM model, where we generate the hidden representations for both the question and answer texts, thereby making them aware of each other's context. As such, in the proposed model, the hidden representation for the question text, and particularly the state values for each word position, is affected not only by its previous or next states, but also by the multi-positional representations of the answer text. 2. As the interaction between question and answer texts is bi-directional, the content of the question text should also affect the way that the answer text is encoded or characterized. We propose an interactive attention mechanism for answer representation learning, and augment our proposed approach to consider the relationship between adjacent words, instead of simply concatenating the word representations. A new quantity, referred to as the context jump dependencies, is proposed to represent the joint probability between adjacent words. 3. We perform an exhaustive evaluation for the proposed approach using three community datasets, namely TREC, Yahoo! and WikiQA, and share our findings. The remaining of the paper is organized as follows: In the Section 2., we present the proposed method, explaining the structure of proposed model. This is followed by a detailed discussion on the experimental results in Section 3.. We finally conclude the paper in Section 4.. Proposed Method One common matching strategy is to first learn representations for question and answer sentences separately based on their content, and then compute a similarity score using the learned representations. Given, however, the fact that sentence matching does not examine each sentence individually, context information between different sentences should also be considered equally important to the syntactic level of the context within a single sentence. This motivates us to design a matching model built upon a cross-sentence context-aware bi-directional LSTM architecture with interactive pre-trained attention mechanism, referred to as IAM. The proposed model learns the sentence hidden representation at each state (word position) not only based on its previous (or next) state, but also the multi-positional representations of an other sentence. In this way, a contextaware sentence representation that is self-adaptive to the corresponding content is learned. Since the interaction between question and answer is mutual, we also introduce an adaptive answer representation by taking into account the question content to enhance the matching. In the following subsections, we describe in detail the proposed model; its overall architecture is illustrated in Fig. 2 . Context-Aware Matching We denote a sentence as x = {x 1 , x 2 , . . . , x T } where x t is the t-th word in the sentence. A language model that uses RNN to learn a sentence representation h t \u2208 R K , and it contains word context information accumulated up to the t-th word, computed from the current word representation w t . In this work, the Bi-LSTM architecture is used for generating sentence representation, where the input is a sequence of words in the sentence. We denote the hidden representation of the t-th word position in the forward direction of Bi-LSTM by the column vector \u2212 \u2192 h t while the vector \u2190 \u2212 h t for the reverse direction. We use the symbols q and a to distinguish a question sentence and an answer sentence respectively, for example, h (q) t = \u2212 \u2192 h (q) t ; \u2190 \u2212 h (q) t for question representation, and h (a) t = \u2212 \u2192 h (a) t ; \u2190 \u2212 h (a) t for answer representation, where t = 1, 2, . . . , T . We use the contextual representations to encode the final distributed matching degree vector s between the question and answer sentences s = tanh Uq \u2212 \u2192 h (q) T \u2190 \u2212 h (q) T + Ua T t=1 \u03b1 (qa) t \u2212 \u2192 h (a) t \u2190 \u2212 h (a) t + bs . (1) The weight matrices U q , U a \u2208 R H\u00d72K (K is the dimensionality of sentence representation) and bias vector b s \u2208 R H are the model variables to be learned. Here, the answer representation is learned by considering query content. Particularly, the resulted embedding h (q) T of Bi-LSTM at last time T is for the question representation. While, for the answer representation, we fuse the multiple positional representations of words by a weighted sum operation, where each weight \u03b1 (qa) t is used to control the importance degree of the combined representation contributing to its relevance to a given question. Working with the distributed similarity vector s as obtained by Eq. ( 1 ), the sentence matching task can be formulated as a binary classification problem. The probability that an answer is related to a question can be modeled using a two-way softmax function based on the computed similarity vector s, given as p(y = 1|s) = exp s T \u03d1 1 exp (s T \u03d1 0 ) + exp (s T \u03d1 1 ) , (2) where the two column vectors \u03d1 0 and \u03d1 1 are softmax parameters with the same dimensionality as s. Based on the above formulation, model variables can be optimized by minimizing a regularized cross-entropy cost by following the logistic regression model (Bengio, 2009; Dreiseitl and Ohno-Machado, 2002) . We detail all the components below, including the learning of standard Bi-LSTM, the question representation learning, answer representation learning and the computation of attention weight \u03b1 (qa) t . Standard Bi-LSTM Learning The column vectors h (q) t and h (a) t are initially computed from two Bi-LSTMs, storing a hidden representation en- \ud835\udc65 \" ($) \ud835\udc65 \"&' ($) \ud835\udc65 \"(' ($) \ud835\udc65 ) ($) \ud835\udc65 ' ($) \u2026 w \" ($) w \"&' ($) w \"(' ($) w ) ($) w ' ($) \u2026 A-Q aware attention self-attention Aggregation Pre-trained Bi-LSTM \u210e \" ($) \u210e \"&' ($) \u210e \"(' ($) \u210e ) ($) \u210e ' ($) \u2026 \ud835\udc66 ' ($) \ud835\udc66 ) ($) \ud835\udc66 \"(' ($) \ud835\udc66 \" ($) \ud835\udc66 \"&' ($) BERT \ud835\udc65 \" (-) \ud835\udc65 \"&' (-) \ud835\udc65 \"(' (-) \ud835\udc65 ) (-) \ud835\udc65 ' (-) \u2026 w \" (-) w \"&' (-) w \"(' (-) w ) (-) w ' (-) \u2026 BERT Pre-trained Bi-LSTM \u210e ' ($) \u210e ) ($) \u210e \"(' ($) \u210e \" ($) \u210e \"&' ($) \u2026 \u2207\u210e - (',)) \u2207\u210e -(\"(',\") \u2207\u210e - coding the question/answer content. These vectors are obtained by training a standard Bi-LSTM over a sentence generation task, assisted by the BERT model (Devlin et al., 2018) . Here, we pre-train the parameters of Bi-LSTM by using the probabilistic language model (Bengio et al., 2003) , the probability of generating a question sentence x q = x (q) 1 , x (q) 2 , . . . , x (q) T is modeled by p(x q ) = T t=1 exp W (q) t \u2212 \u2192 h (q) t \u2190 \u2212 h (q) t + b (q) t T i=1 exp W (q) i \u2212 \u2192 h (q) i \u2190 \u2212 h (q) i + b (q) i . (3) The weight matrices W (q) t T t=1 and bias terms b (q) t T t=1 are model variables to be optimized. Working with the above probability function, model optimization relies on maximizing the log-likelihood over a training corpus of question sentences. Moreover, h (q) t further considers its interaction with the answer, where the answer hidden representation h (a) t is used in the attention module. Question-aware Representation Learning To compute a context-aware question sentence representation that is self-adaptive to the answer content, we modify the Bi-LSTM activation function, so that the current state \u2212 \u2192 h (q) t is computed not only from the previous state of the current word, but also the answer content. This corresponds to the following set of equations \u2212 \u2192 h (q) t \u2190 \u2212 h (q) t = Bi-LSTM \u03c6 (qa) t , \u2212 \u2192 h (q) t\u22121 \u2190 \u2212 h (q) t\u22121 , (4) where \u03c6 (qa) t = tanh V q w (q) t + V a T t=1 \u03b1 (qa) t \u2212 \u2192 h (a) t \u2190 \u2212 h (a) t + b \u03c6 . (5) The weight matrices V q and V a and the bias vector b \u03c6 are the variables to be optimized. The question word vector w (q) t is pre-trained by the BERT. Through adding the answer content, the input of the above model is changed from the current word vector w (q) t to a richer vector \u03c6 (qa) t . For notational convenience, let c a = T t=1 \u03b1 (qa) t \u2212 \u2192 h (a) t \u2190 \u2212 h (a) t . (6) This vector c a is designed to be adaptive to the question content controlled by its weights \u03b1 (qa) t T t=1 . It has the same function in Eqs. ( 1 ) and ( 5 ). The answer representations \u2212 \u2192 h (a) t and \u2190 \u2212 h (a) t are computed from another Bi-LSTM. Now, the remaining question is how to construct the weights \u03b1 (qa) t to allow the question content to affect the importance of each positional answer representation. For this we propose an interactive attention mechanism is explained in Section 2.4. firstly. Pre-trained Interactive Attention The answer context vector c a in Eq. ( 6 ) is proposed to fuse within-sentence and cross-sentence context, aiming at serving better the task of question answer matching. The syntactic level of context within the answer sentence is encoded by its positional representations, which combine \u2212 \u2192 h (a) t and \u2190 \u2212 h (a) t computed by the standard Bi-LSTM, for t = 1, 2, . . . , T . To automatically discover the keyword positions that capture better the answer relevance to a given question, we construct the weight function as follows \u03b1 (qa) t = exp e (qa) t T i=1 exp e (qa) i , (7) where the energy function e (qa) t is formulated as e (qa) t = tanh u T h \u2212 \u2192 h (q) t \u2190 \u2212 h (q) t + u T h \u2212 \u2192 h (a) t \u2190 \u2212 h (a) t +v 1 \u2207h (t,t+1) a + u T a d a + v 2 d T q Md a . (8) The column u vectors with different subscript symbols, the matrix M and the scalars v 1,2 are model variables to be optimized. These weight functions e (qa) t T t=1 select salient positional representations more relevant to the question context, and are referred to as attention weights (Bahdanau et al., 2014) . As seen in Eq. ( 8 ), there are different types of information that contribute to the computation of e (qa) t . The answer and question content jointly controls the selection of the salient representation, and comprises the interactive attention mechanism. Naturally, the characteristic of each positional representation itself \u2212 \u2192 h (a) t \u2190 \u2212 h (a) t contributes to its own importance degree. Additionally, we consider other quantities, such as \u2207h (t,t+1) a , d q and d a . Specifically, the column vectors d q , d a \u2208 R D are bag-ofword representations of the question and answer sentences, with D denoting the size of the word vocabulary. Each dimension of the vector corresponds to the term frequency -inverse document frequency (tf-idf) of the corresponding word. The bilinear score d T q Md a examines relevance between sentences based on their shared words weighted by a set of between-word interaction scores that are stored as elements of the word similarity matrix M. The incorporation of basic word frequency information (via d q and d a ) and word co-occurrence information (via d T q Md a ) is motivated by the loss of specific information (such as years and proper nouns), which may not be accounted for in the distributed representation of the words, but is useful when matching a query to an answer. Local Context Jump Dependencies In this section, we introduce the quantity \u2207h (t,t+1) a that participates in the attention weight computation of Eq. ( 8 ). When using a Bi-LSTM to learn the sentence representation, each obtained positional representation accumulates context information up to the targeted word position in a sentence. It is reasonable to assume that if the subsequent word brings significant change to the sentence semantics, it can directly affect the importance degree of the positional representation at the current word. Such a change in sentence semantics could be indicated by the information change contained within the learned hidden representations between the current and next states. Therefore, given an answer sentence, we aim to formulate a quantity \u2207h (t,t+1) a that can be potentially used as an indicator of its information change between the current (t) and the next (t + 1) word positions. It is known that the joint entropy between the positional representations of two adjacent states measures the uncertainty associated with the two word positions. The amplitude of such uncertainty is a good indicator of the amount of new information to be brought by the next word. Thus, we attempt to estimate the joint probability of the answer representations at the current and next word positions, which fundamentally decides the joint entropy between the two states. Given a set of learned positional representations for an answer sentence, denoted by h (a) t for t = 1, 2, . . . , T . The joint probability of the two adjacent states in a sentence is estimated by following a similar model architecture to RBM (LeCun et al., 2006; Salakhutdinov and Hinton, 2009) . By treating the two adjacent states as the observable units, their joint probability can be modeled as p h (a) t , h (a) t+1 |\u03b8 = 1 Z(\u03b8) exp \u2212E h (a) t , h (a) t+1 , \u03b8 , (9) with E h (a) t , h (a) t+1 , \u03b8 = h (a) t T Qh (a) t+1 + q T 1 h (a) t + q T 2 h (a) t+1 , (10) where \u03b8 = {Q, q 1 , q 2 } is the set of hidden units to be estimated. The partition function Z(\u03b8) is used to normalize the joint probabilities, so that they sum to 1 over all the state possibilities. Let h (ai) t and h (ai) t+1 denote the t-th and (t + 1)-th states of the i-th answer sentence. The loglikelihood L(\u03b8) of a set of observed example pairs of adjacent states, constructed from a training corpus of answer sentences {a i } M i=1 , is maximized, given as L(\u03b8) = 1 M M i=1 T \u22121 t=1 log p h (ai) t , h (ai) t+1 |\u03b8 . (11) In the above, M denotes the total number of answer sentences used for training, and maximization of the loglikelihood involves computing the partition function Z(\u03b8) and its partial derivative. This is difficult due to the fact that samples cannot be drawn directly from p h (ai) t , h (ai) t+1 |\u03b8 as the value of the partition function is unknown. Instead, we employ the contrastive divergence method (Hinton, 2002), and estimate the variable change in each update according to (13) \u03b8 k+1 \u221d \u2202E h (a) t , h (a) t+1 , \u03b8 k \u2202\u03b8 \u2212 E h (a) t , h ( Because this quantity is used as an indicator of the degree by which new information is conveyed by the next word between two adjacent states of an answer sentence, we refer to it as context information jump. Model Training and Initialization In this section, we briefly provide some implementation details for the model training and initialization phases. Given a training corpus for question answer matching, we first train the Bi-LSTM model using the question sentences and answer sentences, independently. The model is trained to solve the language generation task via log-likelihood maximization based on the sentence generation probabilities, as formulated by Eq. ( 3 ). Sentence representations are learned by the language model, such as h (q) t T t=1 and h (a) t T t=1 , acting as fixed input of the proposed matching model. By minimizing a regularized cross-entropy cost, the matching model is trained to solve a binary classification problem that decides the relevance. Instead of random initialization, we initialize all the distributed word representation vectors (w (q) t and w (a) t ) by the word vectors trained by the BERT language model (Devlin et al., 2018) . Cosine similarities computed from these word vectors are also used to initialize the word similarity matrix M. The LSTM variables used to compute the sentence representation is initialized by the previously trained Bi-LSTM model. The remaining variables in the matching model are initialized randomly. Experimental Results and Analysis We have proposed a cross-sentence context-aware Bi-LSTM architecture with interactive pre-trained attention mechanism, referred to as IAM. In this section, it is compared and evaluated with various state-of-the-art neural matching models using three benchmark cQA datasets. In addition to the performance comparisons in Table 1 , we provide various examples in Tables 2-4 to offer insights on the intermediate results learned by different models. Datasets and Experimental Setup The dataset TREC is generated from TREC QA tracks 8-13, containing a set of factoid queries and candidate answers (Wang et al., 2007) . The correct answers for each query are manually labeled and ranked in the dataset. Three partitions of TRAIN, DEV and TEST are provided in the data. Following the benchmark evaluation scheme as used in existing work, two experiments are conducted in which the TRAIN is used to train separate models. In both experiments, DEV is used for model validation and TEST to report the answer selection performance. The Yahoo! answer collection is a large-scale dataset collected through Yahoo! Webscope Program 1 based on community service. It includes approximately 4 million questions and answers, and each question is associated with a 1 http://webscope.sandbox.yahoo.com best answer and a category. The BM25 retrieval algorithm 2 is used to retrieve the top 100 answers for each question. These retrieved answers are also labeled as the correct ones for each corresponding question, ranked after its best answer provided by the collection (Zhou et al., 2016) . WikiQA is a new released question answering dataset on open-domain area. All questions in dataset are sampled from query logs of Bing website, which are posted by users. Each question is related to a Wikipedia website that potentially has correct answers. The candidate answers are collected from Wikipedia website (Yang et al., 2015) . To process the datasets, a special end-of-sentence symbol EOS is added to the end of each sentence, and the outof-vocabulary words are mapped to a special token symbol UNK . We follow the same text pre-processing procedure as in (Severyn and Moschitti, 2015) . The used Bi-LSTM architecture contains two layers each with 100 hidden units. Dimensionality of each word embedding vector is set to K = 100. To initialize the word embedding vectors for the TREC data, a wor2vec model (Mikolov et al., 2013) is trained using the Wikipedia dumps 3 containing approximately 3 million words (after removing words that appear less than 5 times in the corpus). The learning rate is set to 0.025. To initialize the word embedding vectors for the Yahoo! cQA dataset, the Glove model 4 is trained using the 2B Tweets corpus containing approximately 1.2 million words after removing the infrequent ones (Pennington et al., 2014) . For words do not appear in Wikipedia (or Tweeter), a random value uniformly sampled from the interval [\u22120.3, 0.3] is assigned to each embedding dimension. For other model variables to be initialized, random values uniformly sampled from the interval [\u22120.05, 0.05] are used. Stochastic gradient descent is used for model optimization with a mini-batch containing 50 training examples, a learning rate of 0.1, and a dropout rate of 0.5 (Srivastava et al., 2014) . The learning rate is decreased by a factor of 0.5 after 10 epochs. Gradient clipping (Pascanu et al., 2013) is used to scale the gradient when the norm of gradient exceeds a threshold of 5. The performance of existing methods is reported using the implementation settings provided in their corresponding published papers, and with the same training, validation and test data split as the proposed method. Results and Analysis To report model performance using the test set, we use three performance metrics, namely mean reciprocal rank (MRR), mean average precision (MAP). We collected the reported results from the published works of the compared models, wherever possible. Wherever this was not feasible, we implemented them to match with the reported specification and experimental evaluation. Performance comparison between the proposed IAM and multiple compared neural matching models are reported in Table 1 for the TREC, Wiki and Yahoo! datasets. It can be seen that IAM achieves the best performance for both datasets in terms of multiple performance measures. There is a significant performance (Tan et al., 2016) 0.8322 0.7111 0.7045 0.6821 0.6468 0.6157 AP-CNN (Santos et al., 2016) 0.8511 0.7530 0.6957 0.6886 0.6489 0.6047 Ab-CNN (Yin et al., 2016) 0.8539 0.7741 0.7108 0.6921 0.6530 0.6325 KV-MemNNs (Miller et al., 2016) 0.8523 0.7857 0.7265 0.7069 0.6749 0.6431 IARNN (Wang et al., 2016) 0.8208 0.7369 0.7418 0.7341 0.6687 0.6275 BiMPM (Wang et al., 2017) 0.8750 0.8020 0.7310 0.718 0.6892 0.6353 IWAN (Shen et al., 2017a) 0.8890 0.8220 0.7500 0.7330 0.7010 0.6521 CAM (Wang and Jiang, 2017) 0.8659 0.8145 0.7545 0.7433 0.7035 0.6630 ELMo (Peters et al., 2018) 0.8810 0.8247 0.7430 0.7369 0.7163 0.6758 BERT-based (Devlin et al., 2018) 0 No.1: Dry them and use then through out the year. You can also use them raw or cooked, but I would dry them and then when you need one or two then just rehydrate in water. No.2: As for the habaneros, be careful when handling them, these are some of the hottest peppers known and they make great jellies. No.3: I've been growing peppers in my garden this year. I've just got a good crop of them. Top 3 answers by BERTbased (Devlin et al., 2018) No.1: I've been growing peppers in my garden this year. I've just got a good crop of them. No.2: Dinner that will go good with pasta salad. Any of your favourite meats: ribs, chicken, pork chops. No.3: As for the habaneros, be careful when handling them, these are some of the hottest peppers known and they make great jellies. improvement of more than 7% over the CNN-based matching models, and an improvement of around 1-2% improvement over the state-of-the-art BERT-based transformer architecture. In Table 2 , we compare the top three answers of the example question returned by the proposed model and the BERT-based transformer model (Devlin et al., 2018) . The two models return two same sentences in their top three list in the first example of Table 2. For the example, the rankings produced by IAM are more accurate. Compared to the existing attention mechanism used by the attention Bi-LSTM baseline model (Tan et al., 2016) , the proposed one provides better matching performance. In Table 3 , we illustrate the learned salient word positions of two example answer sentences, which are interactively controlled by both question and answer content. Attention weights learned by the proposed and existing attention mechanisms are compared for the same pair of question and answer. It can be seen from Table 3 , that the proposed indicated by T@K for K=1,2 (highlighted by bold), or the smallest two values of \u2207h (t,t+1) a indicated by B@K for K=1,2 (underlined). We use Q, A + and A \u2212 to distinguish the question, correct answer and incorrect answer sentences. Example 1 Q: How do I remove tag the tag names on Facebook? A + : View (T@2) video and click the \"remove (T@1) tag\" link next (B@1) to that person's name. It will no longer be linked (B@2) to their profile. A \u2212 : It allows (T@2) scientists to trace (T@1) evolution of species (B@2) based on the mutations of their genetic code, as well as looking for new (B@1) ones. Example 2 Q: What percentage of alcohol freezes? A + : Vodka is num (T@2) ethanol (and num water). When (B@1) a substance is dissolved in water, it lowers the freezing (T@1) point of (B@2) the solution. A \u2212 : Well I was watching a cooking (T@1) show, (jamies family christmas) and he cut the top off (B@2) of a plastic (T@1) bottle and put a bottle of vodka inside it, he then filled the remaining space with water, then he put it in (B@1) the freezer. method is able to capture more accurately the salient word positions that are important for the matching task. To examine the importance of incorporating the quantity of context information jump \u2207h (t,t+1) a to the attention calculation, the proposed model is trained on TRAIN-ALL set with \u2207h (t,t+1) a removed for the TREC data. This leads to a performance drop of 1.2%-1.55%, compared to the complete IAM as shown in Table 1 . This indicates that it is effective to take into account information change at the targeted word position when evaluating its contribution to the whole sentence semantics. To illustrate the effect of this quantity, we highlight some word positions possessing either very high or very low values of \u2207h (t,t+1) a in one correct and one incorrect answer of a given question. Two such examples are displayed in Table 4 . It is interesting to observe that some highlighted word positions indeed contribute significantly to the question/answer relevance. Conclusion We have proposed the question answer sentence matching model IAM, based on a cross-sentence context-aware bi-directional LSTM architecture with interactive attention mechanism. It improves semantic matching between sentences by taking into account context information, in addition to the syntactic level of context within the sentence. IAM learns the hidden representation at each word position for a question, not only based on the previous (or next) state, but also the multi-positional representations of the answer sentence. This results in a context-aware question representation, self-adaptive to the answer content. A novel attention mechanism is designed to generate an answer representation adaptive to the question content. In addition to the positional answer and question representations as used in most existing attention mechanisms, we include word frequency and co-occurrence information to the attention weight computation. More importantly, we propose the new quantity of context information jump to improve the attention computation by taking into account information changes at different word positions. The proposed model is compared to various neural matching models, based on CNN or RNN architectures. IAM outperforms all the competing ones for both TREC and Yahoo cQA datasets, as evidenced by multiple matching performance measures. We also provide illustrating examples, such as the top selected answers and automatically learned salient word positions, to demonstrate the effectiveness of the proposed method. Acknowledgements This work is supported by the National Natural Science Foundation of China (Grant No. 61671337).",
    "abstract": "Semantic matching measures the dependencies between query and answer representations, which is an important criterion for evaluating whether the matching is successful. In fact, such matching does not examine each sentence individually, because the context information between sentences should be considered equally important to the syntactic context inside a sentence. Considering the above, we propose a novel QA matching model, built upon a cross-sentence context-aware architecture. Specifically, an interactive attention mechanism with a pre-trained language model is presented to automatically select salient positional answer representations that contribute more significantly to the answer relevance of a given question. In addition to the context information captured at each word position, we incorporate a quantity of context jump dependencies to leverage the attention weight formulation. This can capture the amount of useful information brought by the next word, is computed by modeling the joint probability between two adjacent word states. The proposed method is compared with multiple state-of-the-art methods using the TREC library, WikiQA, and the Yahoo! community question datasets. Experimental results show that the proposed method outperforms satisfactorily the competing ones.",
    "countries": [
        "China"
    ],
    "languages": [],
    "numcitedby": "0",
    "year": "2020",
    "month": "May",
    "title": "Cross-sentence Pre-trained Model for Interactive {QA} matching"
}