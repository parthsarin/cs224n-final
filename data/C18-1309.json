{
    "article": "Events in text documents are interrelated in complex ways. In this paper, we study two types of relation: Event Coreference and Event Sequencing. We show that the popular tree-like decoding structure for automated Event Coreference is not suitable for Event Sequencing. To this end, we propose a graph-based decoding algorithm that is applicable to both tasks. The new decoding algorithm supports flexible feature sets for both tasks. Empirically, our event coreference system has achieved state-of-the-art performance on the TAC-KBP 2015 event coreference task and our event sequencing system beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these event relations. Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011) , news summarization (Vossen and Caselli, 2015) , story understanding (Mostafazadeh et al., 2016) . Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957) . The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018) . By analogy to entity coreference, the \"same\" conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018) : Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, mentions that share similar temporal and location scope, though not necessarily the same expression, are considered to be coreferent (Attack in Baghdad on Thursday vs. Bombing in the Green Zone last week). This means that the event arguments of coreferential events mentions can be non-coreferential (18 killed vs. dozens killed), as long as they refer to the same event, judging from the available evidence. Event Sequencing: The coreference relations build up events from scattered mentions. On the basis of events, various other types of relations can then be established between them. The Event Sequencing task studies one such relation. The task is motivated by Schank's scripts (Schank and Abelson, 1977) , which suggests that human organize information through procedural data structures, reassembling sequences of events. For example, the list of verbs order, eat, pay, leave may trigger the restaurant script. A human can conduct reasoning with a typical ordering of these events based on common sense (e.g., order should be the first event, leave should be the last event). The ES task studies how to group and order events from text documents belonging to the same script. Figure 1 shows some annotation examples. Conceptually, event sequencing relations hold between the events, while coreference relations hold between textual event mentions. Given a document, the ES task requires systems to identify events within the same script and classify their inter-relations. These relations can be represented as labeled Directed Acyclic Graphs (DAGs). There are two types of relations 1 : After relations connect events following script orders (e.g. order followed by eating); Subevent relations connect events to a larger event that contains them. In this paper, we focus only on the After relations. Since script-based understanding is built in the ES task, it has some unique properties comparing to pure temporal ordering: 1) event sequences from different scripts provide separate logical divisions of text, while temporal ordering considers all events to lie on a single timeline; 2) temporal relations for events occurring at similar time points may be complicated. Script-based relations may alleviate the problem. For example, if a bombing kills some people, the temporal relation of the bombing and kill may be \"inclusion\" or \"after\". This is considered an After relation in ES because bombing causes the killing. For structure prediction, decoding -recovering the complex structure from local decisions -is one of the core problems. The most successful decoding algorithm for coreference nowadays is mention ranking based (Bj\u00f6rkelund and Kuhn, 2014; Durrett and Klein, 2014; Lee et al., 2017) . These models rank the antecedents (mentions that appear earlier in discourse) and recover the full coreference clusters from local decisions. However, unlike coreference relations, sequencing relations are directed. Coreference decoding algorithms cannot be directly applied to such relations ( \u00a73.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task ( \u00a74.4) and beats an informed baseline on the event sequencing task ( \u00a74.5). Finally, we analyze the results and discuss the difficult challenges for both tasks ( \u00a75). Detailed definitions of these tasks can be found in the corresponding task documents 2 . Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997) . Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015) . Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015) . There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017) . The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014) . This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002) . There have been several studies on building automatic temporal reasoning systems (Uzzaman and Allen, 2010; Do et al., 2012; Chambers et al., 2014) . In comparison, the Event Sequencing task is motivated by the Script theory, which places more emphasis on common-sense knowledge about event chronology. Model Graph-Based Decoding Model In the Latent Antecedent Tree (LAT) model popularly used for entity coreference decoding (Fernandes et al., 2012; Bj\u00f6rkelund and Kuhn, 2014) , each node represents an event mention and each arc a coreference relation, and new mentions are connected to some past mention considered most similar. Thus the LAT model represents the decoding structure as a tree. This can represent any coreference cluster, because coreference relations are by definition equivalence relations 3 . In contrast, tree structures cannot always fully cover an Event Sequence relation graph, because 1) the After links are directed, not symmetric, and 2) multiple event nodes can link to one node, resulting in multiple parents. To solve this problem, we extend the LAT model and propose its graph version, namely the Latent Antecedent Graph (LAG) model. Formally, we define the series of (pre-extracted) event mentions of the document as M = {m 0 , m 1 , ..., m n }, following their discourse order. m 0 is an artificial root node preceding all mentions. For each mention m j , let A j be the set of its potential antecedents: A j = {m 0 , m 1 , ..., m j\u22121 }. Let A denotes the set of antecedents for all the mentions in the sequence {A 0 , A 1 , ..., A n }. The two tasks in question can be considered as finding the appropriate antecedent(s) from A. Similarly, we define the gold antecedent set \u00c3 = { \u00c30 , \u00c31 , ..., \u00c3n }, where \u00c3i represent the set of antecedents of m i allowed by the gold standard. In the coreference task, \u00c3i contains all antecedents that are coreferent with m i . In the sequencing task, \u00c3i contains all antecedents that have an Af ter relation to m i . We can now describe the decoding process. We represent each arc as m i , m j , r (i < j), where r is the relation name. The relation direction can be specified in the relation name r (e.g. r can be after.forward or after.backward). Further, an arc from the root node m 0 to node m j represents that m j does not have any antecedent. The score of the arc is the dot product between the weight parameter w and a feature vector \u03a6( m i , m j , r ), where \u03a6 is an arc-wise feature function. The decoded graph z can be determined by a set of binary variables z, where z ijr = 1 if there is an arc m i , m j , r or 0 otherwise. The final score of z is the sum of scores of all arcs: score(z) = i,j,r z ijr w \u2022 \u03a6( m i , m j , r ) (1) The decoding step is to find the output \u1e91 that maximizes the scoring function: \u1e91 = arg max z\u2208Z(A) score(z) (2) where Z(A) denotes all possible decoding structures given the antecedent sets A. It is useful to note that the decoding step can be applied in the same way to the gold antecedent set \u00c3. Algorithm 1 shows the Passive-Aggressive training algorithm (Crammer et al., 2006) used in our decoding framework. Line 8 decodes the maximum scored structure from all possible gold standard structures using the current parameters w. Intuitively, this step tries to find the \"easiest\" correct graphthe correct graph with the highest score -for the current model. Several important components remain unspecified in algorithm 1: (1) the decoding step (line 6, 8); (2) the match criteria: whether to consider the system decoding structure as correct (line 7); (3) feature delta: computation of feature difference (line 9); (4) loss computation (line 10). We detail the actual implementation of these steps in \u00a73.1.2. Minimum Decoding Structure Similar to the LAT model, there may be many decoding structures representing the same configuration. In LAT, since there is exactly one link per node, the number of links in different decoding structures is the same, hence comparable. In LAG, however, one node is allowed to link to multiple antecedents, creating a potential problem for decoding. For example, consider the sequence m 1 after \u2212 \u2212 \u2192 m 2 after \u2212 \u2212 \u2192 m 3 , both of the following structures are correct: 1. m 1 , m 2 , af ter , m 2 , m 3 , af ter 2. m 1 , m 2 , af ter , m 2 , m 3 , af ter , m 1 , m 3 , af ter However, the last relation in the second decoding structure can actually be inferred via transitivity. We do not intend to spend the modeling power on such cases. We empirically avoid such redundant cases by using the transitive reduction graph for each structure. For a directed acyclic graph, a transitive 12 return w; reduction graph contains the fewest possible edges that have the same reachability relation as the original graph. In the example above, structure 1 is a transitive reduction graph for structure 2. We call the decoding structures that corresponding to the reduction graphs as minimum decoding structures. For LAG, we further restrict Z(A) to contain only minimum decoding structures. Training Details in Latent Antecedent Graph In this section, we describe the decoding details for LAG. Note that if we enforce a single antecedent for each node (as in our coreference model), it falls back to the LAT model (Bj\u00f6rkelund and Kuhn, 2014) . Decoding: We use a greedy best-first decoder (Ng and Cardie, 2002) , which makes a left-to-right pass over the mentions. The decoding step is the same for line 6 and 8. The only difference is that we will use gold antecedent set ( \u00c3) at line 8. For each node m j , we keep all links that score higher than the root link 0, m j , r . Cycle and Structure Check: Incremental decoding a DAG may introduce cycles to the graph, or violate the minimum decoding structure criterion. To solve this, we maintain a set R(m i ) that is reachable from m i during the decoding process. We reject a new link ( m j , m i if m j \u2208 R(m i )) to avoid cycles. We also reject a redundant link ( m i , m j if m j \u2208 R(m i )) to keep a minimum decoding structure. Our current implementation is greedy, we leave investigations of search or global inference based algorithms to future work. Selecting the Latent Event Mention Graph: Note that sequence relations are on the event level. Given a unique event graph, it may still correspond to multiple mention graphs. In our implementation, we use a minimum set of event mentions to represent the full event graph by taking one single mention from each event. Following the \"easiest\" intuition, we select the single mention that will result in the highest score given the current feature weight w. Match Criteria: We consider two graphs to match when their inferred graphs are the same. The inferred graph is defined by taking the transitive closure of the graph and propagate the links through the coreference relations. For example, in Figure 1 , the mention fired will be linked to two killed mentions after propagation. Feature Delta: In structural perceptron training (Collins, 2002) , the weights are updated directly by the feature delta. For all the features f of the gold standard graph z and features f of a decoded graph \u1e91, the feature delta is simply: \u2206 = f \u2212 f . However, a decoded graph may contain links that are not directly presented but inferable from the gold standard graph. For example, in Figure 2 , the prediction graph has a link from M 5 to M 1 (the orange arc), which is absent but inferable from the gold standard tree. If we keep these links when computing \u2206, the model does not converge well. We thus remove the features on the inferable links from f when computing \u2206. Head Headword token and lemma pair, and whether they are the same. Type The pair of event types, and whether they are the same. Realis The pair of realis types and whether they are the same. POS POS pair of the two mentions and whether they are the same. Exact Match Whether the 5-word windows of the two mentions matches exactly. Distance Sentence distance between the two mentions. Frame Frame name pair of the two mentions and whether they are the same. Syntactic Whether a mention is the syntactic ancestor of another. (Das and Smith, 2011) . by 2. For example, in Figure 2 the prediction graph (bottom right) incorrectly links m 4 to Root and misses a link to m 3 , which cause a total loss of 3. In addition, to be consistent with the feature delta computation, we do not compute loss for predicted links that are inferable from the gold standard. Features Event Coreference Features For event coreference, we design a simple feature set to capture syntactic and semantic similarity of arcs. The main features are summarized in Table 1 . In the TAC KBP 2015 coreference task setting, the event mentions are annotated with two attributes. There are 38 event types and subtype pairs (e.g., Busness.Merge-Org, Conflict.Attack). There also 3 realis type: events that actually occurred are marked as Actual; events that are not specific are marked as Generic; other events such as future events are marked as Other. For these two attributes, we use the gold annotations in our feature sets. Event Sequencing Features An event sequencing system needs to determine whether the events are in the same script and order them. We design separate feature sets to capture these aspects: the Script Compatibility set considers whether mentions should belong to the same script; the Event Ordering set determines the relative ordering of the mentions. Our final features are the cross products of features from the following 3 sets. 1. Surface-Based Script Compatibility: these features capture whether two mentions are script compatible based on the surface information, including: \u2022 Mention headword pair. \u2022 Event type pair. \u2022 Whether two event mentions appear in the same cluster in Chambers's event schema database (Chambers and Jurafsky, 2010 ). \u2022 Whether the two event mentions share arguments, and the semantic frame name of the shared argument (produced by the Semafor parser (Das and Smith, 2011) ). 2. Discourse-Based Script Compatibility: these features capture whether two event mentions are related given the discourse context. \u2022 Dependency path between the two mentions. \u2022 Function words (words other than Noun, Verb, Adjective and Adverb) in between the two mentions. \u2022 The types of other event mentions between the two mentions. \u2022 The sentence distance of two event mentions. \u2022 Whether there are temporal expressions (AGM-TMP slot from a semantic parser (Tratz and Hovy, 2011) ) in the sentences of the two mentions. 3. Event Ordering: this feature set tries to capture the ordering of events. We use the discourse ordering of two mentions (forward: the antecedent is the parent; backward: the antecedent is the child), and temporal ordering produced by Caevo (Chambers et al., 2014) . Taking the after arc from fired to killed in Figure 1 as an example, a feature after the cross product is: Event type pair is Conflict.Attack and Life.Die, discourse ordering is backward, and sentence distance is 0. Experiments Dataset We conduct experiments on the dataset released in Text Analysis Coreference (TAC-KBP) 2017 Event Sequencing task (released by LDC under the catalog name LDC2016E130). This dataset contains rich event relation annotations, with event mentions and coreference annotated in TAC-KBP 2015, and additional annotations on Event Sequencing 4 . There are 158 documents in the training set and 202 in the test set, selected from general news articles and forum discussion threads. The event mentions are annotated with 38 type-subtype and 3 realis status (Actual, Generic, Other). Event Hopper, After, and Subevent links are annotated between event mentions. For all experiments, we develop our system and conduct ablation studies using 5-fold cross-validation on the training set, and report performance on the test set. Baselines and Benchmarks Coreference: we compare our event coreference system against the top performing systems from TAC-KBP 2015 (LCC, UI-CCG, and LTI). In addition, we also compare the results against two official baselines (Mitamura et al., 2015) : the Singleton baseline that put each event mention in its own cluster and the Match baseline that creates clusters based on mention type and realis status match. Sequencing: This work is an initial attempt to this problem, so there is currently no comparable prior work on the same task. We instead compare with a baseline using event temporal ordering systems. We use a state-of-the-art temporal system named Caevo (Chambers et al., 2014) . To make a fair comparison, we feed the gold standard event mentions to the system along with mentions predicted by Caevo 5 . However, since the script-style After links are only connected between mentions in the same script, directly using the output of Caevo produces very low precision. Instead, we run a stronger baseline: we take the gold standard script clusters and then only ask Caevo to predict links within these clusters (Oracle Cluster + Temporal). Evaluation Metrics Evaluating Event Coreference: We evaluate our results using the official scorer provided by TAC-KBP, which uses 4 coreference metrics: BLANC (Recasens and Hovy, 2011) , MUC (Chinchor, 1992) , B 3 (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) . Following the TAC KBP task, systems are ranked using the average of these 4 metrics. Evaluating Event Sequencing: The TAC KBP scorer evaluates event sequencing using the metric of the TempEval task (UzZaman, 2012; UzZaman et al., 2013) . The TempEval metric calculates special precision and recall values based on the closure and reduction graphs: P recision = |Response \u2212 \u2229 Ref erence + | |Response \u2212 | Recall = |Ref erence \u2212 \u2229 Response + | |Ref erence \u2212 | where Response represents the After link graph from the system response and Ref erence represents the After link graph from the gold standard. G + represents the graph closure for graph G and G \u2212 represents the graph reduction for graph G. As preprocessing, relations are automatically propagated through coreference clusters (currently using gold standard clusters). The final score is the standard F-score: geometric mean of the precision and recall values. B Evaluation Results for Event Coreference The test performance on Event Coreference is summarized in Table 2 . Comparing to the top 3 coreference systems in TAC-KBP 2015, we outperform the best system by about 2 points absolute F-score on average. Our system is also competitive on individual metrics. Our model performs the best based on B 3 and CEAF-E, and is comparable to the top performing systems on MUC and BLANC. Note that while the Matching baseline only links event mentions based on event type and realis status, it is very competitive and performs close to the top systems. This is not surprising since these two attributes are based on the gold standard. To take a closer look, we conduct an ablation study by removing the simple match features one by one. The results are summarized in Table 3 . We observe that some features produce mixed results on different metrics: they provide improvements on some metrics but not all. This is partially caused by the different characteristics of different metrics. On the other hand, these features (parsing and frames) are automatically predicted, which make them less stable. Furthermore, the Frame features contain duplicate information to event types, which makes it less useful in this setting. Besides the presented features, we have also designed features using event argument. However, we do not report the results since the argument features decrease the performance on all metrics. Evaluation Results for Event Sequencing The evaluation results on Event Sequencing is summarized in Table 4 . Because the baseline system has access to the oracle script clusters, it produces high precision. However, the low recall value shows that it fails to produce enough After links. Our analysis shows that a lot of After relations are not indicated by clear temporal clues, but can only be solved with script knowledge. In Example 3, the baseline system is able to identify \"fled\" is after \"ousted\" from explicit marker \"after\". However, it fails to identify that \"extradited\" is after \"arrested\", which requires knowledge about prototypical event sequences. In our error analysis, we noticed that our system produces a large number of relations due to coreference propagation. One single wrong prediction can cause the error to propagate. Besides memorizing the mention pairs, our model also tries to capture script compatibility through discourse signals. To further understand how much these signals help, we conduct an ablation study of the features in the discoursed based compatibility features (see \u00a73.2.2). Similarly, we remove each feature group from the full feature set one by one and observe the performance change. The results are reported in Table 5 . While most of the features only affect the performance by less than 1 absolute F1 score, the feature sets after removing mention or sentences show a significant drop in both precision and recall. This shows that discourse proximity is the most significant ones among these features. In addition, the mention feature set captures the following explain away intuition: the event mentions A and B are less likely to be related if there are similar mentions in between. One such example can be seen in Figure 1 , the event mention fired is more likely to relate to the closest killed, instead of the other killed in the first paragraph. In addition, our performance on the development set is higher than the test set. Further analysis reveals two causes: 1) the coreference propagation step causes the scores to be very unstable, 2) our model only learns limited common sense ordering based on lexical pairs, which overfit to the small training corpus. Since the annotation is difficult to scale, it is important to use methods to harvest script common sense knowledge automatically, as in the script induction work (Chambers and Jurafsky, 2008) . Discussion Event Coreference Challenges Although we have achieved good performance on event coreference, upon closer investigation we found that most of the coreference decisions are still made based on simple word/lemma matching (note that the type and realis baseline is as high as 0.72 F1 score). The system exploits little semantic information to resolve difficult event coreference problems. A major challenge is that our system is not capable of utilizing event arguments: in fact, Hasler and Orasan (2009) found that only around 20% of the arguments in the same event slot are actually coreferent for coreferential event pairs in the ACE 2005 corpus. Furthermore, the TAC-KBP corpus uses a relaxed participant identity requirement for event coreference, which makes argument-based matching more difficult. Event Sequencing Challenges Our event sequencing performance is still low despite the introduction of many features. This task is inherently difficult because it requires a system to solve both the script clustering and event ordering tasks. The former task requires both common-sense knowledge and discourse reasoning. Reasoning is more important for long-term links since there are no explicit clues like prepositions and dependencies to be exploited. The ablation study shows that discourse features like sentence distance are more effective, which indicates that our model mainly relies on surface clues and has limited reasoning power. Furthermore, we observe a strong locality property of After links by skimming the training data: most After link relations are found in a small local region. Since reasoning and coreference based propagation will accumulate local decisions, a system must be accurate on them. The Ambiguous Boundary of a Script Besides the above-mentioned challenges, a more fundamental problem is to define the boundary of scripts. Since the definition of scripts is only prototypical event sequences, the boundaries between them are not clear. In Example 3, the event jailed is considered to belong to a \"Judicial Process\" script and killing is considered to belong to an\"Attack\" script 7 . No link is annotated between these two mentions since they are considered to belong to different clusters, even though the \"jailed\" event is to punish the \"killing\". Therefore essentially, the current Event Sequencing task simply requires the system to fit these human defined boundaries. In principle, the \"Judicial Process\" script and the \"Attack\" script can form a larger script structure, on a higher hierarchical level. While it is possible to manually define scripts and what kind of events they may contain specifically in a controlled domain, it is difficult to generalize the relations. Most previous work on script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) treats scripts as statistical models where probabilities can be assigned, thereby avoiding the boundary problem. While the script boundaries may be application dependent, a possible solution may rely on the \"Goals\" in Schank's script theory. The Goal of a script is the final state expected (by the script protagonist) from the sequence of events. Goal oriented scripts may be able to help us explain whether killing and jailed should be separate: if we take the\"killer\" as the protagonist, the goal of \"kill\" is achieved at the point of the victim dying. We leave the investigation on proper theoretical justification to future work. Conclusion In this paper, we presented a unified graph framework to conduct event coreference and sequencing. We have achieved state-of-the-art results on event coreference and report the first attempt at event sequencing. While we only studied two types of relations, we believe the method can be adopted in broader contexts. In the future, we plan to build a joint model to allow the tasks to mutually improve each other. In general, analyzing event structure can bring new aspects of knowledge from text. For instance, Event Coreference systems can help group scattered information together. Understanding Event Sequencing can help clarify the discourse structure, which can be useful in other NLP applications, such as solving entity coreference problems (Peng et al., 2015) . However, in our investigation, we find that the linguistic theory and definitions for events are not adequate for the computational setting. For example, proper theoretical justification is needed to define event coreference, which should explain the problems, such as argument mismatches. In addition, we also need a theoretical basis for script boundaries. In the future, we will devote our effort to understanding the theoretical and computational aspects of events relations, and utilizing them for other NLP tasks. Loss: We define the loss to be the number of different edges in two graphs. Following Bj\u00f6rkelund and Kuhn (2014) , we further penalize erroneous root attachment: an incorrect link to the root m 0 adds the loss Acknowledgements This research was supported in part by DARPA grant FA8750-18-2-0018 funded under the AIDA program."
}