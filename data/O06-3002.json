{
    "article": "This paper presents a new Chinese chunking method based on maximum entropy Markov models. We firstly present two types of Chinese chunking specifications and data sets, based on which the chunking models are applied. Then we describe the hidden Markov chunking model and maximum entropy chunking model. Based on our analysis of the two models, we propose a maximum entropy Markov chunking model that combines the transition probabilities and conditional probabilities of states. Experimental results for two types of data sets show that this approach achieves impressive accuracy in terms of the F-score: 91.02% and 92.68%, respectively. Compared with the hidden Markov chunking model and maximum entropy chunking model, based on the same data set, the new chunking model achieves better performance. Introduction Text chunking is a useful step and a relatively tractable median stage in full parsing. Abney [1991] proposed to divide sentences into labeled, non-overlapping sequences of words based on superficial analysis and local information. Ramshaw and Marcus [1995] regarded chunking as a tagging problem and used a machine learning method to resolve it. A uniform standard of English chunking, including the chunking specification, data set, and evaluation method, was developed in the CoNLL-2000 shared task [Kim Sang and Buchholz 2000] , which extracted 1 The work was done while the first author was visiting Microsoft Research Asia. The second research issue is chunking algorithms. Many algorithms have been applied to perform chunking. Koeling [2000] and Osborne [2000] utilized the maximum entropy model which was defined 24 feature templates. Kudoh and Matsumoto [2000] applied weighted voting of 8 support vector machines (SVM) systems trained with distinct chunk representations. Park and Zhang [2003] employed a hybrid of hand-drafted rules and a memory-based learning algorithm (MBL). Kinyon [2001] used a rule-based chunking model, which can be used to generate a robust chunking model for any language. Other algorithms have also been utilized, such as the Sparse Network of Winnows (SNoW) [Li and Roth 2001] , and MBL [Bosch and Buchholz 2002] . With the CPTB and MSRA Chinese chunking specifications and data sets, we implement a chunking system based on maximum entropy Markov models (MEMM), which combine the transition probabilities and conditional probabilities of states. In open tests, we obtained F-scores of 92.68% with the CPTB data set and 91.02% with the MSRA data set; both results are better than those obtained by Li [2004] with the hidden Markov models (HMM) and maximum entropy model (MEM) under the same training and test data sets. Section 2 describes two types of chunking specifications that were used in our experiments. Section 3 describes in detail the MEMM chunking model and compares it with the MEM chunking model and HMM chunking model. Section 4 presents experimental results obtained with our system, based on two types of chunking data sets. Finally, we draw some conclusions. Chinese Chunking Specification For the sake of comparing the results of different chunking models, two types of chunking specifications and data sets mentioned in Section 1 are defined below. The following constraints that guarantee feasible consistency and make chunks more applicable are obeyed in both chunking specifications. 1) No chunk can destroy phrase structures. In particular, object-predicate and verb-argument structures cannot be included in one chunk. 2) Any phrase composed of chunks has a flat structure. Neither the relations between chunks nor the words' relations in chunks are divided. CPTB Chunking Specification Guided by Luo's [2003] definition of chunks, we define a chunk as a constituent whose children are all preterminals. Twenty-three types of chunks can be extracted directly from CPTB without performing any pre-and post extraction process. Table 1 shows the tag of each chunk type in the CPTB specification. The tags and tag descriptions are the same as those for CPTB syntactic tags [Xue and Xia 2000] . In order to identify the boundaries of each chunk in sentences, we define two boundary types, which are denoted by B and I. Let B be the beginning of a chunk, and let I be the interior of a chunk. To sum up, combining chunk types with boundary types, the CPTB specification contains forty-six tags. The following is an example tagged based on the CPTB specification: With this specification, the CPTB chunking data set can be automatically extracted from CPTB. MSRA Chunking Specification Guided by the CoNLL-2000 English chunking specification and the characteristics of Chinese, eleven chunk types are defined in the MSRA chunking specification. Table 2 shows the tag, description and examples for each chunk type. In order to identify the boundaries of each chunk in sentences, we define four boundary types, which are denoted by B, I, E, S. Let B be the beginning of a chunk, let I be the interior of a chunk, let E be the ending of a chunk and let S be a single word chunk. Besides the above types, some special function words ('\u7684/of', '\u548c/and', '\u4e0e/and', '\u6216 /or') in Chinese cannot be divided into any chunk types. We use O to tag these words and the punctuations as outside of any chunks. To sum up, combining chunk types with boundary types, the MSRA specification contains forty-five tags plus O. The following is an example tagged based on the MSRA specification: With this specification, all the chunks can be manually annotated in the Peking University corpus which has been segmented and tagged with POS tag manually. Chunking Model 2 Through the use of the chunk tags described in Section 2, the Chinese chunking problem can be abstracted as a classification problem. Below, we briefly introduce the HMM chunking model and MEM chunking model, and discuss these models' limitations. To overcome these limitations, we propose the MEMM chunking model and describe it in detail. HMM for Chunking HMM is a statistical structure with stochastic transitions and observations [Rabiner 1989 ]. It can be used to solve classification problems involved in modeling sequential data. Li [2004] proposed the Chinese chunking model based on conventional HMM. Given a word sequence W = w 1 , w 2 , \u2026 , w k and its POS sequence T = t 1 , t 2 , \u2026 , t k , where k is the number of words in the sentence, the result of chunking is assumed to be a sequence, in which the words are grouped into chunks as follows: ... [w i w i+1 ... w i+m ] [w i+m+1 w i+m+2 ... w i+m+h ] ... The corresponding POS tag sequence is grouped as follows: C =... [t i t i+1 ... t i+m ] [t i+m+1 t i+m+2 ... t i+m+h ] ... ... c j c j+1 ... Here c j corresponds to the POS tag sequence of a chunk. [t i t i+1 ... t i+m ] c j may also be thought of as a chunk rule. Therefore, C is a sequence of eleven possible chunk rules and some outside words, which we refer to as O. The chunking task is, thus, converted to that of finding a rule sequence. According to Bayes' rule, it can be computed as follows [Xun et al. 2000 ]: * arg max ( / , ) = arg max ( / , ) ( , ) = arg max ( / , ) ( ) c c c C PCWT P W C T P C T P W C T P C = . (1) Here, ( ) P C is the probability of transition. It is seen as the rule's n-gram model. A tri-gram among chunks are used to approximate 1 2 1 1 2 3 ( ) ( ) ( / ) ( / , ) k i i i i P C P c P c c P c c c \u2212 \u2212 = \u2248 \u220f . (2) Smoothing follows application of the method proposed by Gao et al. [2002] . ( / , ) P W C T is the probability of emission. The employed independent assumption is that the current word i w is related to the current POS tag i t , the current word's boundary type i m (including B, I, E, S, and O), and the current word's chunk type i x (including eleven types of chunks). It is approximated as follows: 1 ( / , ) ( / , , ) m i i i i i P W C T P w t m x = = \u220f . ( 3 ) If the triple ( , , , ) i i i i w t m x is unseen, formula (4) is used: 2 , ( , , ) ( / , , ) max( ( , , )) i i i i i i i i j k j k count t m x P w t m x count t m x = , (4) where ( , , ) i i i count t m x is the frequency when the triple ( , , ) i i i t m x occurs. There are three problems with the HMM chunking model. Firstly, HMM is a generative model focusing on the joint probability of states and observations. But the chunking problem is a conditional probability problem when observations are given. Secondly, independent assumption of HMM makes the current observation relevant to the current state and irrelevant to the context observation; however, context words should have an impact on chunking. Thirdly, many representations give the observation a particular description by means of overlapping features that are not independent of each other. These representations cannot be used in HMM. MEM for Chunking As an alternative to HMM, MEM is proposed to solve the chunking problem. MEM is an exponential model that offers the flexibility of integrating multiple sources of knowledge into a model [Berger 1996 ]. One of the main advantages of using MEM is the ability to incorporate various features into the conditional probability framework. Furthermore, the conditional probability model focuses on the modeling of tagging sequence, replacing the modeling of observation sequence. Let H denote the histories that consist of W and T. Given H, the goal of MEM is to find the optimal chunk tag sequence S = s 1 , s 2 , \u2026 , s k that contains forty-five chunk tags. The model decomposes ( / ) P S H into the product of probabilities of individual chunk actions ( / ) i i P s H . i H represents the histories of i s . The conditional entropy of a distribution ( / ) P s h is defined as , ( ) ( ) ( | )log ( | ) s S h H H p phpsh psh \u2208 \u2208 = \u2212 \u2211 . ( 5 ) By maximizing the conditional entropy subject to certain constraints, we can estimate ( / ) P s h based on the maximum entropy theory [Ratnaparkhi 1996 ]. The constraints are defined as follows: { | , } p j p j j P p E f E f f = = \u2200 , ( 6 ) ( | ) 1 s p s h = \u2211 , ( 7 ) where j f is the feature function of MEM. p j E f is the model's expectation of j f . p j E f is the empirical expectation of j f . They are defined as follows: 1 * * ( , ) 0 j j if h h and s s f s h otherwise = = \u23a7 \u23aa = \u23a8 \u23aa \u23a9 , (8) , ( ) ( | ) ( , ) p j j s h E f p h p s h f s h = \u2211 , (9) , ( , ) ( , ) p j j s h E f p s h f s h = \u2211 . ( 10 ) Let s * be a certain chunk tag, and let h * be a certain instance of context. The model's distribution ( / ) P s h can be inferred by means of Lagrange transformation: 1 ( | ) exp ( , ) ( ) j j j p s h f s h Z h \u03bb \u239b \u239e = \u239c \u239f \u239c \u239f \u239d \u23a0 \u2211 , ( 11 ) ( ) exp ( , ) j j s j Z h f sh \u03bb \u239b \u239e = \u239c \u239f \u239c \u239f \u239d \u23a0 \u2211 \u2211 , ( 12 ) where ( ) Z h is the normalization constant. i \u03bb is the multiplier parameter with respect to each feature function. Given a set of features and a corpus of training data, the Improved Iterative Scaling algorithm [Della Pietra 1997] can be used to find the optimal parameters { i \u03bb }. MEMM for Chunking MEM, which combines independent and dependent overlapping features together to predict chunk tags, can overcome the deficiency of HMM mentioned above. However, it does not apply the relations between each tags because MEM labels each word separately without considering the probability of neighboring chunk tag transition. For chunking, the neighboring tags are dependent; for example the chunk tag next to B-NP should be I-NP or E-NP. To overcome this shortcoming, MEMM has been proposed. In it, the current state i s depends not only on the previous state   P s s \u2212 is the transition probability of a state. Because only some chunk tag pairs occur in the training data, a smoothing algorithm is needed to solve the data sparseness problem of the tag bi-gram. Since not all chunk tags can be followed between each other, three transition restricted rules are used to reduce the number of tag pairs. This can make smoothing more reliable. Let X be a certain chunk type, and let Y be a random chunk type. B, I, E, S, and O were defined in Section 2.2. Thus: i i P s s O \u2212 is estimated as follows: 1 1 ( | , ) ( | ) ( | ) i i i i i i P s s O P s s P s H \u2212 \u2212 = , (13) 1) B-X can be followed by I-X or E-X; 2) I-X can be followed by I-X or E-X; 3) E-X, S-X, and O can be followed by B-Y, S-Y, or O. Through three rules, five hundred and seventy-three types of tag pairs can be enumerated. P s s \u2212 can be combined dynamically. The Viterbi algorithm is used to search for the optimal sequence of states. Figure 2 shows the structure of the Chinese chunking model based on MEMM. Figure 2. The structure of the MEMM Chinese chunking model Features in MEMM and MEM MEM and MEMM are both highly dependent on feature templates. For the sake of making a fair comparison between MEM and MEMM, both MEM and MEMM use the same feature template. The histories of the current state are a source for feature collection. The lexical and POS information of the current word, the left context consisting of two words, and the right context consisting of two words are regarded as histories. In addition, the affix information of the current word and the chunk tag of the previous word are atomic features [Ratnaparkhi 1996; Koeling 2000 ]. Table 3 shows the atomic features. AF i Two-character suffix of the current word In order to compare the effectiveness of different types of features, we selected three types of feature templates. Table 4 shows the template based on lexical information only. Table 5 shows the template based on POS information only. Table 6 shows the template based on both lexical and POS information. Results obtained using different feature templates will be given in Section 4. The heuristic that low frequency features are not reliable was used to cut off the features that occurred less than three times. Through feature selection, more reliable features could be used. Table 4. Feature template based on lexical information Feature type Features Atomic features W i , W i-1 , W i-2 , W i+1 , W i+2 , S i-1 , PF i , AF i Combined features W i-1 W i , W i-2 W i-1 , W i W i+1 , W i+1 W i+2 , W i-1 W i+1 , W i-1 W i W i+1 , W i-2 W i-1 W i , W i W i+1 W i+2 , Table 5. Feature template based on POS information Feature type Features Atomic features P i , P i-1 , P i-2 , P i+1 , P i+2 , S i-1 Combined features P i-1 P i , P i-2 P i-1 , P i P i+1 , P i+1 P i+2 , P i-1 P i+1 , P i-1 P i P i+1 , P i-2 P i-1 P i , P i P i+1 P i+2 , Table 6. Feature template based on both lexical and POS information Feature type Features Atomic features W i , W i-1 , W i-2 , W i+1 , W i+2 , P i , P i-1 , P i-2 , P i+1 , P i+2 , S i-1 , PF i , AF i Combined features W i-1 W i , W i W i+1 , W i-1 W i+1 , P i-1 P i , P i-2 P i-1 , P i P i+1 , P i-1 P i+1 , P i-1 P i P i+1 , P i-2 P i-1 P i , P i P i+1 P i+2 , W i P i+1 , W i P i+2 , P i W i-1 , W i-2 P i-1 P i , P i W i+1 P i+1 , P i-1 W i P i , S i-1 P i P i+1 , S i-1 P i , S i-1 P i-1 P i , P i W i+1 , Evaluation and Discussion We will firstly describe in detail our Chinese chunking data set. Then we will present the chunking performance and discuss it. Data Set The CPTB chunking data set is based on data automatically extracted from CPTB, which has a total of around 100,000 word tokens. Following Bikel's [2000] The MSRA chunking data set is based on the Peking University corpus, which has been segmented, POS tagged, and chunk annotated manually. The data set consisted of 18,239 sentences with 243,868 chunks and 473,179 word tokens. The vocabulary size was 34,793. Forty-two types of POS tags and forty-three types of chunk tags occurred in the data set. The AL of the chunks is 1.377 word tokens 3 . Table 8 shows details of the training and test data sets. Table 9 shows the distribution of each type of chunk in the data set. Experimental Results Following the measurement approach adopted in CoNLL-2000, we measured the performance of Chinese chunking in terms of the precision (P), recall (R), and F-score (F). All the results were obtained in open tests. For the CPTB chunking data set, the results are listed in Table 10 . The results for HMM [Li 2004 ] are listed in the first row of Table 10 . The second and third rows list the results for MEM and MEMM, respectively, where the same feature template defined in Table 6 was used. The empirical value \u03bb mentioned in Section 3.3 was set to 0.65, based on the training data. It can be seen that, MEMM achieved the best results on the CPTB chunking data set. In order to test the feature impact on MEMM, we tested MEMM chunking on the CPTB data set with the different types of feature templates described in Section 3.4. Table 11 shows the results. The chunk tag that had maximum occurrence probability for each word token was used to chunk its corresponding token. With this method, we got the baseline results listed in the first row of Table 11 . The results obtained using the feature template in Table 4 are listed in the second row of Table 11 , and then the third and fourth row is for Table 5 and Table 6 . It can be seen that, the performance achieved using POS information only is much better than the performance achieved using lexical information only. The performance achieved using lexical and POS information is much better than the performance achieved using POS information only. Table 12 shows the performance of different chunk types for the CPTB chunking data set when the total MEMM F-score in total was 92.68%. As shown, some chunk types achieved much poorer performance, such as PRN, UCP, VNV, and VSB. The reason was that they rarely occurred in the training data set, so it was difficult to tag them correctly. NP was the most frequent chunk type, but its performance was much poorer than the average performance. The reason is that the boundary of NP is difficult to distinguish. Table 12. The performance of each chunk type for the CPTB data set For the MSRA chunking data set, Table 13 shows the chunking results. As before, MEMM and MEM used the same feature template, defined in Table 6 . The experimental results show that the MEMM chunking model was more efficient for resolving the Chinese chunking problem. The reason is that MEMM chunking model uses sufficient context information that can describe actual language phenomena effectively, as explained in Section 3.3. Table 14 shows the MEMM chunking results for the MSRA data set with different types of feature templates. The baseline and feature templates were defined the same as in Table 11 . The performance achieved using POS information only was again much better than the performance achieved using lexical information only. One reason is that the model using lexical features has a more serious data sparseness problem than the model using POS features does. The other reason is that POS tags have a stronger ability to predict chunk tags and that POS tag are the gold standard (because they are manually annotated). The performance achieved using lexical and POS information was again better than the performance achieved using POS information only. This means that lexical information can improve chunking accuracy because it provides sufficient context information for predicting the current chunk tag. Table 15 shows the performance of different chunk types for HMM and MEM when the total MEMM F-score in total was 91.02% on the MSRA data set. Because NP and VP chunks accounted for 75.76% of all chunks, their performance dominated the overall chunking performance. As shown, the performance of VP was somewhat better, while the performance of NP was much lower than average, just as in the experimental results for the CPTB data set (shown in Table 12 ). The performance of PP, CONJP, and INTJP was somewhat better because most of them are single words. For almost all the chunk types, the performance of MEMM is the best. HMM was better for the INDP chunk type because the AL of INDP was 4.297 and the HMM method can classify chunk types that have longer AL. In order to show the relationship between MEMM and the data set size, we split the MSRA training data set into parts with different sizes. Figure 3 shows the results for different sizes of training data sets with the feature template shown in Table 6 . When the size of the training data set increased to 6,900 sentences, that is, forty percent of the whole training data set, the F-score was 90%. However, when the size of the training data set increased to 17,253 sentences, the F-score only increased by one percent. Thus, it can be seen that expanding the scale of the training data set helps the chunking performance very little after the data set reaches a certain scale. Figure 4 shows the results for training data sets of different sizes using the feature template shown in Table 4 , which only has lexical information. When the entire training data set was used, the F-score was 74.27%. But the curve shows that the F-score could still improve significantly if the scale of the training data set were increased. This means that there is much room to improve the accuracy if we enlarge the training corpus further.  Table 16 shows the number and percentage of each type of error in the MEMM results, compared with those in the HMM and MEM results. Four types of Chinese chunking errors are defined: wrong labeling, under-combining, over-combining, and overlapping. Since one chunking error can possibly result in two chunk tagging errors, there were 852 chunking errors. Under-combining and over-combining errors amounted to almost 90% in all the errors for all three models, so identifying the boundaries of chunks is important to get better performance. The reason why MEMM has the best performance is that the numbers of the two types of errors decrease when the sequential relations of the chunk tags are considered. Conclusion In this paper we have proposed a new method of Chinese chunking based on MEMM. The transition probabilities of chunk tags are estimated using the Markov model. A smoothing algorithm is applied to deal with the data sparseness problem of the chunk tag bi-gram. The conditional probabilities of chunk tags along with histories are estimated through MEM. The two probabilities are combined dynamically in MEMM. For the purpose of comparing the performance of different models, chunking models were applied to both the CPTB chunking data set and MSRA chunking data set. The experiments on the PTCB data set showed that the new model achieved an F-score of 92.68%, which was better than the F-scores of HMM and MEM in Chinese chunking. The improvement was 2.74% and 1.06%, respectively. The experiments on the MSRA data set showed that the new model had an F-score of 91.02%, which was also better than the F-scores of HMM and MEM. The improvement in this case was 2.49% and 1.19%, respectively. The reasons for the improvement have been analyzed through error analysis. We have also discussed the effects of different feature types and different sizes of training data sets on the performance of MEMM. Acknowledgements We thank Dr. Ming Zhou, Dr. Jianfeng Gao, Dr. Mu Li, Dr. Yajuan Lv, Dr. John Chen, and Dr. Hongqiao Li for their valuable suggestions and for checking the English in this paper. We also thank the members of the Natural Language Computing Group at Microsoft Research Asia. We especially thank the anonymous reviewers for their insightful comments and suggestions, based on which the paper has been revised.",
    "funding": {
        "defense": 0.0,
        "corporate": 2.463006067510154e-05,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not explicitly mention any funding sources, including defense, corporate, research agencies, foundations, or otherwise. The only mention related to affiliations is that the work was done while the first author was visiting Microsoft Research Asia, which does not directly imply funding.",
    "abstract": "This paper presents a new Chinese chunking method based on maximum entropy Markov models. We firstly present two types of Chinese chunking specifications and data sets, based on which the chunking models are applied. Then we describe the hidden Markov chunking model and maximum entropy chunking model. Based on our analysis of the two models, we propose a maximum entropy Markov chunking model that combines the transition probabilities and conditional probabilities of states. Experimental results for two types of data sets show that this approach achieves impressive accuracy in terms of the F-score: 91.02% and 92.68%, respectively. Compared with the hidden Markov chunking model and maximum entropy chunking model, based on the same data set, the new chunking model achieves better performance.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": 15,
    "year": 2006,
    "month": "June",
    "title": "{C}hinese Chunking Based on Maximum Entropy {M}arkov Models"
}