{
    "article": "We present BPEmb, a collection of pre-trained subword unit embeddings in 275 languages, based on Byte-Pair Encoding (BPE). In an evaluation using fine-grained entity typing as testbed, BPEmb performs competitively, and for some languages better than alternative subword approaches, while requiring vastly fewer resources and no tokenization. BPEmb is available at https://github.com/bheinzerling/bpemb. Introduction Learning good representations of rare words or words not seen during training at all is a difficult challenge in natural language processing. As a makeshift solution, systems have typically replaced such words with a generic UNK token. Recently, based on the assumption that a word's meaning can be reconstructed from its parts, several subwordbased methods have been proposed to deal with the unknown word problem: character-based recurrent neural networks (RNN) (Luong and Manning, 2016) , character-based convolutional neural networks (CNN) (Chiu and Nichols, 2016) , word embeddings enriched with subword information (FastText) (Bojanowski et al., 2017) , and byte-pair encoding (BPE) (Sennrich et al., 2016) , among others. While pre-trained FastText embeddings are publicly available, embeddings for BPE units are commonly trained on a per-task basis (e.g. a specific language pair for machinetranslation) and not published for general use. In this work we present BPEmb, a collection of pre-trained subword embeddings in 275 languages, and make the following contributions: \u2022 We publish BPEmb, a collection of pre-trained bytepair embeddings in 275 languages; \u2022 We show the utility of BPEmb in a fine-grained entity typing task; and \u2022 We show that BPEmb performs as well as, and for some languages better than, alternative approaches while being more compact and requiring no tokenization. We apply BPE 1 to all Wikipedias 2 of sufficient size with various o and pre-train embeddings for the resulting BPE symbol using GloVe (Pennington et al., 2014) , resulting in byte-pair embeddings for 275 languages. To allow studying the effect the number of BPE merge operations and of the embedding dimensionality, we provide embeddings for 1000, 3000, 5000, 10000, 25000, 50000, 100000 and 200000 merge operations, with dimensions 25, 50, 100, 200, and 300. BPEmb: Byte-pair Embeddings Evaluation: Comparison to FastText and Character Embeddings To evaluate the quality of BPEemb we compare to Fast-Text, a state-of-the-art approach that combines embeddings of tokens and subword units, as well as to character embeddings. FastText enriches word embeddings with subword information by additionally learning embeddings for character n-grams. A word is then represented as the sum of its associated character n-gram embeddings. In practice, representations of unknown word are obtained by adding the embeddings of their constituting character 3-to 6-grams. We use the pre-trained embeddings provided by the authors. Merge ops Byte-pair encoded text 5000 \u8c4a \u7530 \u99c5 ( \u3068 \u3088 \u3060 \u3048 \u304d ) \u306f \u3001 \u6771\u4eac\u90fd \u65e5 \u91ce \u5e02 \u8c4a \u7530 \u56db \u4e01\u76ee \u306b\u3042\u308b 10000 \u8c4a \u7530 \u99c5 ( \u3068 \u3088 \u3060 \u3048\u304d ) \u306f \u3001 \u6771\u4eac\u90fd \u65e5 \u91ce\u5e02 \u8c4a \u7530 \u56db \u4e01\u76ee\u306b\u3042\u308b 25000 \u8c4a \u7530\u99c5 ( \u3068\u3088 \u3060 \u3048\u304d ) \u306f \u3001 \u6771\u4eac\u90fd \u65e5 \u91ce\u5e02 \u8c4a\u7530 \u56db \u4e01\u76ee\u306b\u3042\u308b 50000 \u8c4a \u7530\u99c5 ( \u3068\u3088 \u3060 \u3048\u304d ) \u306f \u3001 \u6771\u4eac\u90fd \u65e5 \u91ce\u5e02 \u8c4a\u7530 \u56db\u4e01\u76ee\u306b\u3042\u308b Tokenized \u8c4a\u7530 \u99c5 ( \u3068 \u3088 \u3060 \u3048 \u304d ) \u306f \u3001 \u6771\u4eac \u90fd \u65e5\u91ce \u5e02 \u8c4a\u7530 \u56db \u4e01\u76ee \u306b \u3042\u308b 10000 \u8c50 \u7530 \u7ad9 \u662f \u6771 \u65e5\u672c \u65c5 \u5ba2 \u9435 \u9053 ( JR \u6771 \u65e5\u672c ) \u4e2d\u592e \u672c \u7dda \u7684 \u9435\u8def \u8eca\u7ad9 25000 \u8c50\u7530 \u7ad9\u662f \u6771\u65e5\u672c\u65c5\u5ba2\u9435\u9053 ( JR \u6771\u65e5\u672c ) \u4e2d\u592e \u672c \u7dda\u7684\u9435\u8def\u8eca\u7ad9 50000 \u8c50\u7530 \u7ad9\u662f \u6771\u65e5\u672c\u65c5\u5ba2\u9435\u9053 ( JR \u6771\u65e5\u672c ) \u4e2d\u592e \u672c\u7dda\u7684\u9435\u8def\u8eca\u7ad9 Tokenized \u8c50\u7530\u7ad9 \u662f \u6771\u65e5\u672c \u65c5\u5ba2 \u9435\u9053 ( JR \u6771\u65e5\u672c ) \u4e2d\u592e\u672c\u7dda \u7684 \u9435\u8def\u8eca\u7ad9 1000 to y od a station is a r ail way station on the ch \u016b \u014d main l ine 3000 to y od a station is a railway station on the ch \u016b \u014d main line 10000 toy oda station is a railway station on the ch \u016b \u014d main line 50000 toy oda station is a railway station on the ch\u016b \u014d main line 100000 toy oda station is a railway station on the ch\u016b\u014d main line Tokenized toyoda station is a railway station on the ch\u016b\u014d main line v = A(s) \u2208 R d by an encoder A. In this work the encoder architecture is either averaging across the SU sequence, an LSTM, or a CNN. Finally, the prediction y is: (Shimaoka et al., 2017) . y = 1 1 + exp(\u2212v) Data. We obtain entity mentions from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and their entity types by mapping to Freebase (Bollacker et al., 2008) , resulting in 3.4 million English 5 instances like (Melfordshire: /location,/location/city). Train and test set are random subsamples of size 80,000 and 20,000 or a proportionally smaller split for smaller Wikipedias. In addition to English, we report results for a) the five languages having the largest Wikipedias as measured by textual content; b) Chinese and Japanese, i.e. two high-resource languages without tokenization markers; and c) eight medium-to lowresource Asian languages. Experimental Setup. We evaluate entity typing performance with the average of strict, loose micro, and loose macro precision (Ling and Weld, 2012) . For each combination of SU and encoding architecture, we perform a Tree-structured Parzen Estimator hyper-parameter search (Bergstra et al., 2011) with at least 1000 hyper-parameter search trials (English, at least 50 trials for other languages) and report score distributions (Reimers and Gurevych, 2017) . See Table 3 for hyper-parameter ranges. Results and Discussion Subwords vs. Characters vs. Tokens Figure 1 shows our main result for English: score distributions of 1000+ trials for each SU and architecture. Token-based results using two sets of pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) BPEmb performs well with low embedding dimensionality Figure 2 , right) and can match FastText with a fraction of its memory footprint (6 GB for FastText's 3 million embeddings with dimension 300 vs 11 MB for 100k BPE embeddings (Figure 2 , left) with dimension 25.). As both Fast-Text and BPEmb were trained on the same corpus (namely, Wikipedia), these results suggest that, for English, the compact BPE representation strikes a better balance between learning embeddings for more frequent words and relying on compositionality of subwords for less frequent ones. FastText performance shows the lowest variance, i.e., it robustly yields good results across many different hyperparameter settings. In contrast, BPEmb and characterbased models show higher variance, i.e., they require more careful hyper-parameter tuning to achieve good results. Architectures. Averaging a mention's associated embeddings is the worst architecture choice. This is expected for character-based models, but somewhat surprising for tokenbased models, given the fact that averaging is a common method for representing mentions in tasks such as entity typing (Shimaoka et al., 2017) or coreference resolution (Clark and Manning, 2016) perform equally, with the exception of BPEmb giving a significant improvement for English. For high resources languages without explicit tokenization (middle), byte-pair encoding appears to yield a subword segmentation which gives performance comparable to the results obtained when using FastText with pre-tokenized text 7 . Results are more varied for mid-to low-resource Asian languages (bottom), with small BPEmb gains for Tibetan and Lao. The large performance degradation for Khmer appears to be due to inconsistencies in the handling of unicode control characters between different software libraries used in our experiments and have a disproportionate effect due to the small size of the Khmer Wikipedia. Multilingual Analysis Limitations Due to limited computational resources, our evaluation was performed only for a few of the 275 languages provided by BPEemb. While our experimental setup allows a fair comparison between FastText and BPEmb through extensive hyper-parameter search, it is somewhat artificial, since it disregards context. For example, Myxomatosis in the phrase Radiohead played Myxomatosis has the entity type /other/music, which can be inferred from the contextual music group and the predicate plays, but this ignored in our specific setting. How our results transfer to other tasks requires further study. Replicability All data used in this work is freely and publicly available. BPEmb and code to replicate our experiments is available at https://github.com/bheinzerling/bpemb. Conclusions We presented BPEmb, a collection of subword embeddings trained on Wikipedias in 275 languages. Our evaluation showed that BPEmb performs as well as, and for some languages, better than other subword-based approaches. BPEmb requires no tokenization and is orders of magnitudes smaller than alternative embeddings, enabling potential use under resource constraints, e.g. on mobile devices. (2, 3, 4, 5), (2, 3, 4, 5, 6) , (3), (3, 4), (3, 4, 5), (3, 4, 5, 6) , (4), (4, 5), (4, 5, 6), (5), (5, 6), (6) number of filters 25, 50, 100, 200, 300, 400, 500, 600 , 700 output dropout 0.0, 0.1, 0.2, 0.3, 0.4, 0.5 Average output dropout 0.0, 0.1, 0.2, 0.3, 0.4, 0.5 Acknowledgments This work has been supported by the German Research Foundation as part of the Research Training Group \"Adaptive Preparation of Information from Heterogeneous Sources\" (AIPHES) under grant No. GRK 1994/1, and partially funded by the Klaus Tschira Foundation, Heidelberg, Germany.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.9999998063873687,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions support from the German Research Foundation, which is a research agency, and the Klaus Tschira Foundation, a foundation. There is no mention of defense, corporate funding, or an absence of funding.",
    "abstract": "We present BPEmb, a collection of pre-trained subword unit embeddings in 275 languages, based on Byte-Pair Encoding (BPE). In an evaluation using fine-grained entity typing as testbed, BPEmb performs competitively, and for some languages better than alternative subword approaches, while requiring vastly fewer resources and no tokenization. BPEmb is available at https://github.com/bheinzerling/bpemb.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English",
        "Tibetan",
        "Lao",
        "Khmer"
    ],
    "numcitedby": 167,
    "year": 2018,
    "month": "May",
    "title": "{BPE}mb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages"
}