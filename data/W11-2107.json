{
    "article": "This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. Introduction The Meteor 1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010) . However, previous versions of the metric are still limited by lack of punctuation handling, noise in paraphrase matching, and lack of discrimination between word types. We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists. We show that the addition of these resources to Meteor allows tuning versions of the metric that show higher correlation with human translation rankings and adequacy scores on unseen 1 The metric name has previously been stylized as \"ME-TEOR\" or \"METEOR\". As of version 1.3, the official stylization is simply \"Meteor\". test data. The evaluation resources are modular, usable with any other evaluation metric or MT software. We also conduct a MT system tuning experiment on Urdu-English data to compare the effectiveness of using multiple versions of Meteor in minimum error rate training. While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002) , a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set. The versions of Meteor corresponding to the translation evaluation task submissions, (Ranking and Adequacy), are described in Sections 3 through 5 while the submission to the tunable metrics task, (Tuning), is described in Section 6. New Metric Resources Meteor Normalizer Whereas previous versions of Meteor simply strip punctuation characters prior to scoring, version 1.3 includes a new text normalizer intended specifically for translation evaluation. The normalizer first replicates the behavior of the tokenizer distributed with the Moses toolkit (Hoang et al., 2007) , including handling of non-breaking prefixes. After tokenization, we add several rules for normalization, intended to reduce meaning-equivalent punctuation styles to common forms. The following two rules are particularly helpful: \u2022 Remove dashes between hyphenated words. (Example: far-off \u2192 far off) While intended for Meteor evaluation, use of this normalizer is a suitable preprocessing step for other metrics to improve accuracy when reference sentences are stylistically different from hypotheses. Filtered Paraphrase Tables The original Meteor paraphrase tables (Denkowski and Lavie, 2010b) are constructed using the phrase table \"pivoting\" technique described by Bannard and Callison-Burch (2005) . Many paraphrases suffer from word accumulation, the appending of unaligned words to one or both sides of a phrase rather than finding a true rewording from elsewhere in parallel data. To improve the precision of the paraphrase tables, we filter out all cases of word accumulation by removing paraphrases where one phrase is a substring of the other. outputs can differ by a single word, such as mistranslating either a main verb or a determiner. To improve Meteor's discriminative power in such cases, we introduce a function word list for each WMT language and a new \u03b4 parameter to adjust the relative weight given to content words (any word not on the list) versus function words (see Section 3). Function word lists are estimated according to relative frequency in large monolingual corpora. For each language, we pool freely available WMT 2011 data consisting of Europarl (Koehn, 2005) , news (sentence-uniqued), and news commentary data. Any word with relative frequency of 10 \u22123 or greater is added to the function word list. Table 2 lists corpus size and number of function words learned for each language. In addition to common words, punctuation symbols consistently rise to the tops of function word lists. Meteor Scoring Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores. This section describes our extended version of the metric. For a hypothesis-reference pair, the search space of possible alignments is constructed by identifying all possible matches between the two sentences according to the following matchers: Exact: Match words if their surface forms are iden-tical. Stem: Stem words using a language-appropriate Snowball Stemmer (Porter, 2001) and match if the stems are identical. Synonym: Match words if they share membership in any synonym set according to the Word-Net (Miller and Fellbaum, 2007) database. Paraphrase: Match phrases if they are listed as paraphrases in the paraphrase tables described in Section 2.2. All matches are generalized to phrase matches with a start position and phrase length in each sentence. Any word occurring less than length positions after a match start is considered covered by the match. The exact and paraphrase matchers support all five WMT languages while the stem matcher is limited to English, French, German, and Spanish and the synonym matcher is limited to English. Once matches are identified, the final alignment is resolved as the largest subset of all matches meeting the following criteria in order of importance: 1. Require each word in each sentence to be covered by zero or one matches. 2. Maximize the number of covered words across both sentences. 3. Minimize the number of chunks, where a chunk is defined as a series of matches that is contiguous and identically ordered in both sentences. 4. Minimize the sum of absolute distances between match start positions in the two sentences. (Break ties by preferring to align words and phrases that occur at similar positions in both sentences.) Given an alignment, the metric score is calculated as follows. Content and function words are identified in the hypothesis (h c , h f ) and reference (r c , r f ) according to the function word lists described in Section 2.3. For each of the matchers (m i ), count the number of content and function words covered by matches of this type in the hypothesis (m i (h c ), m i (h f )) and reference (m i (r c ), m i (r f )). Calculate weighted precision and recall using matcher weights (w i ...w n ) and content-function word weight (\u03b4): P = i w i \u2022 (\u03b4 \u2022 m i (h c ) + (1 \u2212 \u03b4) \u2022 m i (h f )) \u03b4 \u2022 |h c | + (1 \u2212 \u03b4) \u2022 |h f | Target WMT09 WMT10 R = i w i \u2022 (\u03b4 \u2022 m i (r c ) + (1 \u2212 \u03b4) \u2022 m i (r f )) \u03b4 \u2022 |r c | + (1 \u2212 \u03b4) \u2022 |r f | The parameterized harmonic mean of P and R (van Rijsbergen, 1979) is then calculated: F mean = P \u2022 R \u03b1 \u2022 P + (1 \u2212 \u03b1) \u2022 R To account for gaps and differences in word order, a fragmentation penalty is calculated using the total number of matched words (m, average over hypothesis and reference) and number of chunks (ch): P en = \u03b3 \u2022 ch m \u03b2 The Meteor score is then calculated: Score = (1 \u2212 P en) \u2022 F mean The parameters \u03b1, \u03b2, \u03b3, \u03b4, and w i ...w n are tuned to maximize correlation with human judgments. Parameter Optimization Development Data The 2009 and 2010 WMT shared evaluation data sets are made available as development data for WMT 2011. Data sets include MT system outputs, reference translations, and human rankings of translation quality. Table 3 lists the number of judgments for each evaluation and combined totals. Tuning Procedure To evaluate a metric's performance on a data set, we count the number of pairwise translation rankings preserved when translations are re-ranked by metric score. We then compute Kendall's \u03c4 correlation coefficient as follows: For each language, the \u03b4 parameter is above 0.5, indicating a preference for content words over function words. In addition, the fragmentation penalties are generally less severe across languages. The additional features in Meteor 1.3 allow for more balanced parameters that distribute responsibility for penalizing various types of erroneous translations. \u03c4 = concordant pairs \u2212 discordant Evaluation Experiments To compare Meteor 1.3 against previous versions of the metric on the task of evaluating MT system outputs, we tune a version for each language on 2009 WMT data and evaluate on 2010 data. This replicates the 2010 WMT shared evaluation task, allowing comparison to Meteor 1.2. Table 5 lists correlation of each metric version with ranking judgments on tune and test data. Meteor 1.3 shows significantly higher correlation on both tune and test data for English, French, and Spanish while Czech and German demonstrate overfitting with higher correlation on tune data but lower on test data. This overfitting effect is likely due to the limited number of systems providing translations into these languages and the difficulty of these target languages leading to significantly noisier translations skewing the space of metric scores. We believe that tuning to combined 2009 and 2010 data will counter these issues for the official Ranking version. (Olive, 2005) . For each type of judgment, metric versions are tuned and tested on each year and scores are compared. We compare Meteor 1.3 results with those from version 1.2 with results shown in Table 6 . For both adequacy data sets, Meteor 1.3 significantly outperforms version 1.2 on both tune and test data. The version tuned on MT09 data is selected as the official Adequacy version of Meteor 1.3. H-TER versions either show no improvement or degradation due to overfitting. Examination of the optimal H-TER parameter sets reveals a mismatch between evaluation metric and human judgment type. As H-TER evaluation is ultimately limited by the TER aligner, there is no distinction between content and function words, and words sharing stems are considered nonmatches. As such, these features do not help Meteor improve correlation, but rather act as a source of additional possibility for overfitting. MT System Tuning Experiments The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al., 2009) devtest set, we select a version of Meteor by exploring the effectiveness of using multiple versions of the metric to tune phrase-based translation systems for the same language pair. We use the 2009 NIST Open Machine Translation Evaluation Urdu-English parallel data (Przybocki, 2009) plus 900M words of monolingual data from the English Gigaword corpus (Parker et al., 2009) to build a standard Moses system (Hoang et al., 2007) as follows. Parallel data is word aligned using the MGIZA++ toolkit (Gao and Vogel, 2008) and alignments are symmetrized using the \"growdiag-final-and\" heuristic. Phrases are extracted using standard phrase-based heuristics (Koehn et al., 2003) and used to build a translation table and lexicalized reordering model. A standard SRI 5-gram language model (Stolke, 2002) is estimated from monolingual data. Using Z-MERT, we tune this system to baseline metrics as well as the versions of Meteor discussed in previous sections. We also tune to a balanced Tuning version of Meteor designed to minimize bias. This data set provides a single set of reference translations for MERT. To account for the variance of MERT, we run end-to-end tuning 3 times for each metric and report the average results on two unseen test sets: newswire and weblog. Test set translations are evaluated using BLEU, TER, and Meteor 1.2. The parameters for each Meteor version are listed in Table 7 while the results are listed in Table 8 . The results are fairly consistent across both test sets: the Tuning version of Meteor outperforms BLEU across all metrics while versions of Meteor that perform well on other tasks perform poorly in tuning. This illustrates the differences between evaluation and tuning tasks. In evaluation tasks, metrics are engineered to score 1-best translations from systems most often tuned to BLEU. As listed in",
    "abstract": "This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system.",
    "countries": [
        "United States"
    ],
    "languages": [
        "German",
        "English",
        "French",
        "Czech",
        "Urdu",
        "Spanish"
    ],
    "numcitedby": "393",
    "year": "2011",
    "month": "July",
    "title": "Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"
}