{
    "article": "In a log-likelihood ratio (LLR)-based speaker verification system, the alternative hypothesis is usually difficult to characterize a priori, since the model should cover the space of all possible impostors. In this paper, we propose a new LLR measure in an attempt to characterize the alternative hypothesis in a more effective and robust way than conventional methods. This LLR measure can be further formulated as a non-linear discriminant classifier and solved by kernel-based techniques, such as the Kernel Fisher Discriminant (KFD) and Support Vector Machine (SVM). The results of experiments on two speaker verification tasks show that the proposed methods outperform classical LLR-based approaches. Introduction In essence, the speaker verification task is a hypothesis testing problem. Given an input utterance U, the goal is to determine whether U was spoken by the hypothesized speaker or not. The log-likelihood ratio (LLR)-based detector [Reynolds 1995 ] is one of the state-of-the-art approaches for speaker verification. Consider the following hypotheses: H p U H L U H H p U H \u03b8 \u03b8 \u2265 \u23a7 = \u23a8 < \u23a9 (1) where ( | ), 0, 1, i p U H i = is the likelihood of hypothesis H i given the utterance U, and \u03b8 is the threshold. H 0 and H 1 are, respectively, called the null hypothesis and the alternative hypothesis. Mathematically, H 0 and H 1 can be represented by parametric models denoted as \u03bb and \u03bb , respectively; \u03bb is often called an anti-model. Though H 0 can be modeled straightforwardly using speech utterances from the hypothesized speaker, H 1 does not involve any specific speaker, thus lacks explicit data for modeling. Many approaches have been proposed to characterize H 1 , and various LLR measures have been developed. We can formulate these measures in the following general form [Reynolds 2000 ]: 1 2 ( | ) ( | ) ( ) log log , ( ( | ), ( | ),..., ( | )) ( | ) N p U p U L U p U p U p U p U \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb = = \u03a8 (2) where \u03a8(\u22c5) is some function of the likelihood values from a set of so-called background models {\u03bb 1 ,\u03bb 2 ,...,\u03bb N }. For example, the background model set can be obtained from N representative speakers, called a cohort [Rosenberg 1992 ], which simulates potential impostors. If \u03a8(\u22c5) is an average function [Reynolds 1995] , the LLR can be written as: 1 1 1 ( ) log ( | \u03bb) log ( | \u03bb ) . N i i L U p U p U N = \u23a7 \u23ab = \u2212 \u23a8 \u23ac \u23a9 \u23ad \u2211 (3) Alternatively, the average function can be replaced by various functions, such as the maximum [Liu 1996 ], i.e.: 2 1 ( ) log ( | \u03bb) max log ( | \u03bb ), i i N L U p U p U \u2264 \u2264 = \u2212 (4) or the geometric mean [Liu 1996 ], i.e., 3 1 1 ( ) log ( | \u03bb) l o g ( |\u03bb ) . N i i L U p U p U N = = \u2212 \u2211 (5) A special case arises when \u03a8(\u22c5) is an identity function and N = 1. In this instance, a single background model is usually trained by pooling all the available data, which is generally irrelevant to the clients, from a large number of speakers. This is called the world model or the Universal Background Model (UBM) [Reynolds 2000 ]. The LLR in this case becomes: 4 ( ) log ( | \u03bb) log ( | ), L U p U p U = \u2212 \u2126 (6) where \u2126 denotes the world model. Kernel Discriminant Analysis for LLR-Based Speaker Verification However, none of the LLR measures developed so far has proven to be absolutely superior to any other, since the selection of \u03a8(\u22c5) is usually application and training data dependent. In particular, the use of a simple function, such as the average, maximum, or geometric mean, is a heuristic that does not include any optimization process. The issues of selection, size, and combination of background models motivate us to design a more comprehensive function, \u03a8(\u22c5), to improve the characterization of the alternative hypothesis. In this paper, we first propose a new LLR measure in an attempt to characterize H 1 by integrating all the background models in a more effective and robust way than conventional methods. Then, we formulate this new LLR measure as a non-linear discriminant classifier and apply kernel-based techniques, including the Kernel Fisher Discriminant (KFD) [Mika 1999 ] and Support Vector Machine (SVM) [Burges 1998 ], to optimally separate the LLR samples of the null hypothesis from those of the alternative hypothesis. SVM-based techniques have been successfully applied to many classification and regression tasks, including speaker verification. Unlike our work, existing approaches [Bengio 2001; Wan 2005] only use a single background model, i.e., the world model, to represent the alternative hypothesis, instead of integrating multiple background models to characterize the alternative hypothesis. For example, Bengio et al. [Bengio 2001 ] proposed a decision function: 5 1 2 ( ) log ( | \u03bb) log ( | ) , L U a p U a p U b = \u2212 \u2126+ (7) where a 1 , a 2 , and b are adjustable parameters estimated using SVM. An extended version of Eq. ( 7 ) with the Fisher kernel and the LR score-space kernel for SVM was investigated in Wan [Wan 2005 ]. The results of speaker verification experiments conducted on both the XM2VTS database [Messer 1999 ] and the ISCSLP2006-SRE database [Chinese Corpus Consortium 2006] show that the proposed methods outperform classical LLR-based approaches. The remainder of this paper is organized as follows. Section 2 describes the design of the new LLR measure in our approach. Sections 3 and 4 introduce the kernel discriminant analysis used in this work and the formation of the characteristic vector by background model selection, respectively. Section 5 contains our experiment results. Finally, in Section 6, we present our conclusions. New LLR Measure Design Analysis of the Alternative Hypothesis First of all, we redesign the function \u03a8(\u22c5) in Eq. (2) as: 1 2 1 2 1/( ... ) 1 2 ( | ) ( ) ( ( | \u03bb ) ( |\u03bb ) ... ( |\u03bb ) ) , N N w w w w w w N p U p U p U p U \u03bb + + + = \u03a8 = \u22c5 \u22c5 \u22c5 u (8) where 1 2 [ ( | ), ( | ),..., ( | )] T N p U p U p U \u03bb \u03bb \u03bb = u is an N\u00d71 vector and i w is the weight of the likelihood p(U | \u03bb i ), i = 1,2,..., N. This function gives N background models different weights according to their individual contribution to the alternative hypothesis. It is clear that Eq. ( 8 ) is equivalent to a geometric mean function when 1 i w = , i = 1,2,..., N. If some background model \u03bb i contrasts with an input utterance U, the likelihood p(U | \u03bb i ) may be extremely small, thus causing the geometric mean to approximate zero. In contrast, by assigning a favorable weight to each background model, the function \u03a8(\u22c5) defined in Eq. ( 8 ) may be less affected by any specific background model with an extremely small likelihood. Therefore, the resulting score for the alternative hypothesis obtained by Eq. ( 8 ) will be more robust and reliable than that obtained by a geometric mean function. It is also clear that Eq. ( 8 ) will reduce to a maximum function when * 1 i w = , 1 * argmax log ( | \u03bb ) i N i i p U \u2264 \u2264 = ; and 0 i w = , * i i \u2200 \u2260 . By substituting Eq. ( 8 ) into Eq. ( 2 ), we obtain: 1 2 1 2 1 2 1 2 6 1/( ... ) 1 2 1/( ... ) 1 2 ( | ) ( ) log ( | ) ( | ) log ( ( | ) ( | ) ... ( | ) ) ( | ) ( | ) ( | ) log ... ( | ) ( | ) ( | ) N N N N w w w w w w N w w w w w w N p U L U p U p U p U p U p U p U p U p U p U p U p U \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb + + + + + + = = \u22c5 \u22c5 \u22c5 \u239b \u239e \u239b \u239e \u239b \u239e \u239b \u239e \u239c \u239f = \u22c5 \u22c5\u22c5 \u239c \u239f \u239c \u239f \u239c \u239f \u239c \u239f \u239d \u23a0 \u239d \u23a0 \u239d \u23a0 \u239d \u23a0 1 2 1 2 1 2 1 2 1 ( | ) ( | ) ( | ) log log ... log ... ( | ) ( | ) ( | ) accept 1 reject ... ' accept ' reject, N N N T N T p U p U p U w w w w w w p U p U p U w w w \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb \u03b8 \u03b8 \u03b8 \u03b8 \u239b \u239e = + + + \u239c \u239f + + + \u239d \u23a0 \u2265 \u23a7 = \u23a8 < + + + \u23a9 \u2265 \u23a7 = \u23a8 < \u23a9 w x w x (9) where 1 2 [ , ..., ] T N w w w = w is an N\u00d71 weight vector, the new threshold 1 2 ' ( ... ) N w w w \u03b8 \u03b8 = + + + , and x is an N \u00d7 1 vector in the space R N , expressed by p U p U p U p U p U p U \u03bb \u03bb \u03bb \u03bb \u03bb \u03bb = x (10) The implicit idea in Eq. ( 10 ) is that the speech utterance U can be represented by a characteristic vector x. If we replace the threshold ' \u03b8 in Eq. ( 9 ) with a bias b, the equation can be rewritten as: ( ) ( ) T L U b f = + = w x x , (11) where f(x) forms a so-called linear discriminant classifier. This classifier translates the goal of solving an LLR measure into the optimization of w and b, such that the utterances of clients and impostors can be separated. To realize this classifier, three distinct data sets are needed: one for generating each client's model, one for generating the background models, and one for optimizing w and b. Since the bias b plays the same role as the decision threshold of the conventional LLR measure, which can be determined through a trade-off between false acceptance and false rejection, the main goal here is to find w. Existing linear discriminant analysis techniques, such as Fisher's Linear Discriminant (FLD) [Duda 2001 ] or Linear SVM [Burges 1998 ], can be applied to implement Eq. ( 11 ). Linear Discriminant Analysis Fisher's Linear Discriminant (FLD) is one of the popular linear discriminant classifiers [Duda 2001 ]. Suppose the i-th class has n i data samples, 1 { ,.., } i i i i n = X x x , i = 1, 2. The goal of FLD is to seek a direction w in the space R N such that the following Fisher's criterion function J(w) is maximized: ( ) , T b T w J = w S w w w S w (12) where S b and S w are, respectively, the between-class scatter matrix and the within-class scatter matrix defined as 1 2 1 2 ( ) ( ) T b = \u2212 \u2212 S m m m m (13) and 1,2 ( )( ) , i T w i i i= \u2208 = \u2212 \u2212 \u2211 \u2211 x X S x m x m (14) where m i is the mean vector of the i-th class computed by 1 1 . i n i i s s i n = = \u2211 m x (15) According to Duda [Duda 2001 ], the solution for w, which maximizes J(w) defined in Eq. ( 12 ), is the leading eigenvector of Kernel Discriminant Analysis Intuitively, f(x) in Eq. ( 11 ) can be solved via linear discriminant training algorithms [Duda 2001 ], such as FLD or Linear SVM. However, such methods are based on the assumption that the observed data of different classes is linearly separable, which is obviously not feasible in most practical cases with nonlinearly separable data. To solve this problem more effectively, we propose using a kernel-based nonlinear discriminant classifier. It is hoped that data from different classes, which is not linearly separable in the original input space R N , can be separated linearly in a certain higher dimensional (maybe infinite) feature space F via a nonlinear mapping \u03a6. Let \u03a6(x) denote a vector obtained by mapping x from R N to F. Then, the objective function, based on Eq. ( 11 ), can be re-defined as: ( ) ( ) , T F f b = \u03a6 + x w x ( 16 ) which constitutes a linear discriminant classifier in F, where F w is a weight vector in F. In practice, it is difficult to determine the kind of mapping that would be applicable; therefore, the computation of \u03a6(x) might be infeasible. To overcome this difficulty, a promising approach is to characterize the relationship between the data samples in F, instead of computing \u03a6(x) directly. This is achieved by introducing a kernel function k(x, y)=<\u03a6(x),\u03a6(y)>, which is the dot product of two vectors \u03a6(x) and \u03a6(y) in F. The kernel function k(\u22c5) must be symmetric, positive definite and conform to Mercer's condition [Burges 1998 ]. A number of kernel functions exist, such as the simplest dot product kernel function k(x, y) = x T y, and the very popular Radial Basis Function (RBF) kernel k(x, y) = exp(\u2212 ||x \u2212 y|| 2 / 2\u03c3 2 ) in which \u03c3 is a tunable parameter. Existing techniques, such as KFD [Mika 1999 ] or SVM [Burges 1998 ], can be applied to implement Eq. ( 16 ). Kernel Fisher Discriminant (KFD) Suppose the i-th class has n i data samples, 1 { ,.., } i i i i n = X x x , i = 1, 2. The goal of KFD is to seek a direction F w in the feature space F such that the following Fisher's criterion function ( ) F J w is maximized: ( ) , T F b F F T F w F J \u03a6 \u03a6 = w S w w w S w (17) where b \u03a6 S and w \u03a6 S are, respectively, the between-class scatter matrix and the within-class scatter matrix in F defined as: 1 2 1 2 ( ) ( ) T b \u03a6 \u03a6 \u03a6 \u03a6 \u03a6 = \u2212 \u2212 S m m m m (18) and 1,2 ( ( ) )( ( ) ) , i T w i i i \u03a6 \u03a6 \u03a6 = \u2208 = \u03a6 \u2212 \u03a6 \u2212 \u2211 \u2211 x X S x m x m (19) where 1 (1/ ) ( ) i n i i i s s n \u03a6 = = \u03a6 \u2211 m x , and i = 1, 2, is the mean vector of the i-th class in F. Let 1 2 1 1 2 2 1 2 1 1 1 { ,.., } { ,.., } { ,.., } l n n \u222a = \u222a = X X x x x x x x Let \u03b1 T = [\u03b1 1 , \u03b1 2 ,..., \u03b1 l ]. Accordingly, Eq. ( 16 ) can be re-written as: 1 ( ) ( , ) . l j j j f k b \u03b1 = = + \u2211 x x x (21) Our goal, therefore, changes from finding F w to finding \u03b1, which maximizes ( ) , T T J = \u03b1 M\u03b1 \u03b1 \u03b1 N\u03b1 (22) where M and N are computed by: 1 2 1 2 ( ) ( ) T = \u2212 \u2212 M \u03b7 \u03b7 \u03b7 \u03b7 (23) and 1,2 ( ) , i i T i n n i i= = \u2212 \u2211 N K I 1 K (24) respectively, where i \u03b7 is an l\u00d71 vector whose j-th element 1 ( ) (1/ ) ( , ) i n i i j i j s s n k = = \u2211 \u03b7 x x , j = 1,2,..., l; K i is an l\u00d7n i matrix with ( ) ( , ) i i js j s k = K x x ; I n i is an n i \u00d7n i identity matrix; and 1 n i is an n i \u00d7n i matrix with all entries equal to 1/n i . Following Mika [Mika 1999 ], the solution for \u03b1, which maximizes J(\u03b1) defined in Eq. ( 22 ), is the leading eigenvector of N -1 M. Support Vector Machine (SVM) Alternatively, Eq. ( 16 ) can be solved with an SVM, the goal of which is to seek a separating hyperplane in the feature space F that maximizes the margin between classes. Following Burges [Burges 1998 ], where each training sample x j belongs to one of the two classes identified by the label y j \u2208{\u22121,1}, j=1, 2,..., l. We can find the coefficients \u03b1 j by maximizing the objective function, 1 1 1 1 ( ) ( , ), 2 l l l j i j i j i j j i j Q y y k \u03b1 \u03b1 \u03b1 = = = = \u2212 \u2211 \u2211 \u2211 \u03b1 x x (27) subject to the constraints, 1 0, l j j j y \u03b1 = = \u2211 and 0 , , j C j \u03b1 \u03b1 \u2264 \u2264 \u2200 ( 28 ) where C \u03b1 is a penalty parameter [Burges 1998 ]. The problem can be solved using quadratic programming techniques [Vapnik 1998 ]. Note that most \u03b1 j are equal to zero, and the training samples associated with non-zero \u03b1 j are called support vectors. A few support vectors act as the key to deciding the optimal margin between classes in the SVM. An SVM with a dot product kernel function is known as a Linear SVM. Formation of the Characteristic Vector In our experiments, we use B+1 background models, consisting of B cohort set models and one world model, to form the characteristic vector x in Eq. ( 10 ); and B cohort set models for L 1 (U) in Eq. ( 3 ), L 2 (U) in Eq. ( 4 ), and L 3 (U) in Eq. ( 5 ). Two cohort selection methods [Reynolds 1995 ] are used in the experiments. One selects the B closest speakers to each client; and the other selects the B/2 closest speakers to, plus the B/2 farthest speakers from, each client. The selection is based on the speaker distance measure [Reynolds 1995 ], computed by: ( |\u03bb ) ( |\u03bb ) (\u03bb , \u03bb ) log log , ( |\u03bb ) ( |\u03bb ) j j i i i j i j j i p U p U d p U p U = + ( 29 ) where i \u03bb and j \u03bb are speaker models trained using the i-th speaker's utterances U i and the j-th speaker's utterances U j , respectively. Two cohort selection methods yield the following two (B+1)\u00d71 characteristic vectors: cst 1 cst ( | \u03bb) ( |\u03bb) ( | \u03bb) log log ... log ( | ) ( | \u03bb ) ( |\u03bb ) T B p U p U p U p U p U p U \u23a1 \u23a4 = \u23a2 \u23a5 \u2126 \u23a3 \u23a6 x (30) and cst1 cst / 2 fst1 fst / 2 ( |\u03bb) ( | \u03bb) (| \u03bb) ( | \u03bb) (| \u03bb) ( | ) ( |\u03bb ) ( | \u03bb ) ( | \u03bb ) ( | \u03bb ) = [log log log log log ] B B p U p U p U p U p U T p U p U p U p U p U \u2126 x , ( 31 ) where cst i \u03bb and fst i \u03bb are the i-th closest model and the i-th farthest model of the client model \u03bb , respectively. Experiments We evaluate the proposed approaches on two databases: the XM2VTS database [Messer 1999 ] and the ISCSLP2006 speaker recognition evaluation (ISCSLP2006-SRE) database [Chinese Corpus Consortium 2006] . For the performance evaluation, we adopt the Detection Error Tradeoff (DET) curve [Martin 1997 ]. In addition, the NIST Detection Cost Function (DCF), which reflects the performance at a single operating point on the DET curve, is also used. The DCF is defined as: Evaluation on the XM2VTS Database The first set of speaker verification experiments was conducted on speech data extracted from the XM2VTS database [Messer 1999 ], which is a multimodal database consisting of face images, video sequences, and speech recordings taken on 295 subjects. The raw database contained approximately 30 hours of digital video recordings, which was then manually annotated. Each subject participated in four recording sessions at approximately one-month intervals, and each recording session consisted of two shots. In a shot, every subject was prompted to read three sentences \"0 1 2 3 4 5 6 7 8 9\", \"5 0 6 9 2 8 1 3 7 4\", and \"Joe took father's green shoe bench out\" at his/her normal pace. The speech was recorded by a microphone clipped to the subject's shirt. In accordance with Configuration II of the evaluation protocol described in Luettin [Luettin 1998 ], the XM2VTS database was divided into three subsets: \"Training\", \"Evaluation\", and \"Test\". In our speaker verification experiments, we used the \"Training\" subset to build the individual client's model and the world model 1 , and the \"Evaluation\" subset to estimate the decision threshold \u03b8 in Eq. ( 1 ) and the parameters w, F w , and b in 1 Currently, we do not have an external resource to train the world model and the background models. We follow the evaluation protocol in [Luettin 1998 ], which suggests \"If a world model is needed, as in speaker verification, a client-dependent world model can be trained from all other clients but the actual client. Although not optimal, it is a valid method.\" We will train the world model and the background models using an external resource in our future work. Eq. ( 11 ) or Eq. ( 16 ). The performance of speaker verification was then evaluated on the \"Test\" subset. As shown in Table 1 , a total of 293 speakers 2 in the database were divided into 199 clients, 25 \"evaluation impostors\", and 69 \"test impostors\". We used 12 (2\u00d72\u00d73) utterances/speaker from sessions 1 and 2 to train the individual client's model, represented by a Gaussian Mixture Model (GMM) [Reynolds 1995 ] with 64 mixture components. For each client, the other 198 clients' utterances from sessions 1 and 2 were used to generate the world model, represented by a GMM with 256 mixture components; 20 or 40 speakers were chosen from these 198 clients as the cohort. Then, we used 6 utterances/client from session 3, and 24 (4\u00d72\u00d73) utterances/evaluation-impostor over the four sessions, which yielded 1,194 (6\u00d7199) client samples and 119,400 (24\u00d725\u00d7199) impostor samples, to estimate \u03b8 , w, F w , and b. However, as a kernel-based classifier can be intractable when a large number of training samples is involved, we reduced the number of impostor samples from 119,400 to 2,250 using a uniform random selection method. In the performance evaluation, we tested 6 utterances/client in session 4 and 24 utterances/test-impostor over the four sessions, which produced 1,194 (6\u00d7199) client trials and 329,544 (24\u00d769\u00d7199) impostor trials. Table 2 summarizes all the parametric models used in each system. Using a 32-ms Hamming-windowed frame with 10-ms shifts, each speech utterance (sampled at 32 kHz) was converted into a stream of 24-order feature vectors, each consisting of 12 Mel-scale frequency cepstral coefficients [Huang 2001 ] and their first time derivatives.   An analysis of the results based on HTER is given in Table 3 . For each approach, the decision threshold, \u03b8 or b, was used to minimize HTER on the \"Evaluation\" subset and then applied to the \"Test\" subset. From Table 3 , we observe that all the proposed LLR systems outperform the baseline systems and, for the \"Test\" subset, a 29.72% relative improvement was achieved by \"KFD_w_20c\", compared to \"L5\" -the best baseline system. The advantage of integrating multiple background models in our methods could be the reason why the proposed LLR systems based on the linear SVM (\"LSVM_w_20c\" and \"LSVM_w_10c_10f\") outperform \"L5\", which applied the kernel-based SVM in L 5 (U). We also observe that, in the proposed LLR systems, all of the kernel-based methods outperform the linear-based methods. To analyze the effect of the number of background models, we implemented several proposed LLR systems and baseline systems with B = 40. An analysis of the results based on the HTER is given in Table 4 . Compared to Table 3 , the performance of each system with B = 40 is, in general, better than that of its counterpart with B = 20, but not always. For instance, \"KFD_w_20c_20f\" in Table 4 achieved a lower HTER for \"Evaluation\" but a higher HTER for \"Test\", compared to \"KFD_w_10c_10f\" in Table 3 . This may be the result of overtraining. However, from Table 4 , it is clear that the superiority of the proposed LLR systems over the baseline systems is again demonstrated. Evaluation on the ISCSLP2006-SRE Database We participated in the text-independent speaker verification task of the ISCSLP2006 Speaker Recognition Evaluation (ISCSLP2006-SRE) plan [Chinese Corpus Consortium 2006] . The database contained 800 clients. Each client has one long training utterance, ranging in duration from 21 to 85 seconds, with an average length of 37.06 seconds. In addition, there are 5,933 utterances in the \"Test\" subset, each of which ranges in duration from 5 seconds to 54 seconds, with an average length of 15.66 seconds. Each test utterance is associated with the client claimed by the speaker, and the task is to judge whether it is true or false. The ratio of true compared the proposed systems with the baseline GMM-UBM system and Bengio et al.'s system (L5). . In each system, the decision threshold, \u03b8 or b, was selected to minimize the DCF on the \"Evaluation\" subset, and then applied to the \"Test\" subset. The minimum DCFs for the \"Evaluation\" subset and the associated DCFs for the \"Test\" subset are given in Table 5 . We observe that \"KFD_w_50c_50f\" achieved a 34.08% relative improvement over \"GMM-UBM\", and a 19.73% relative improvement over \"L5\". Kernel Discriminant Analysis for LLR-Based Speaker Verification Table 2. A summary of the parametric models used in each system for the XM2VTS task. H 0 H 1 System a 64-mixture client GMM a 256-mixture world model B 64-mixture cohort GMMs Experiment Results First, B was set to 20 in the experiments. We implemented the proposed LLR system based on linear-based classifiers (FLD and Linear SVM) and kernel-based classifiers (KFD and SVM) in eight ways: 1) FLD with Eq. ( 30 ) (\"FLD_w_20c\"), 2) FLD with Eq. ( 31 ) (\"FLD_w_10c_10f\"), 3) Linear SVM with Eq. ( 30 ) (\"LSVM_w_20c\"), 4) Linear SVM with Eq. ( 31 ) (\"LSVM_w_10c_10f\"), 5) KFD with Eq. ( 30 ) (\"KFD_w_20c\"), 6) KFD with Eq. ( 31 ) (\"KFD_w_10c_10f\"), 7) SVM with Eq. (30) (\"SVM_w_20c\"), and 8) SVM with Eq. (31) (\"SVM_w_10c_10f\"). Both SVM and KFD used an RBF kernel function with \u03c3= 5. For performance comparison, we used six systems as our baselines: 1) L 1 (U) with the 20 closest cohort models (\"L1_20c\"), 2) L 1 (U) with the 10 closest cohort models plus the 10 farthest cohort models (\"L1_10c_10f\"), 3) L 2 (U) with the 20 closest cohort models (\"L2_20c\"), 4) L 3 (U) with the 20 closest cohort models (\"L3_20c\"), 5) L 4 (U) (\"L4\"), and 6) L 5 (U) using an RBF kernel function with \u03c3= 10 (\"L5\"). Figure 1 shows the results of the baseline systems evaluated on the \"Test\" subset in DET curves. We observe that the curves \"L1_10c_10f\", \"L4\" and \"L5\" are better than the others. Thus, in the subsequent experiments, we focused on the performance improvements of our proposed LLR systems over these three baselines. Kernel Discriminant Analysis for LLR-Based Speaker Verification clients to imposters is approximately 1:20. The answer sheet was released after the evaluation finished. To form the \"Evaluation\" subset for estimating \u03b8, w, F w , and b, we extracted some speech from each client's training utterance in the following way. First, we sorted the 800 clients in descending order according to the length of their training utterances. Then, for the first 100 clients, we cut two 4-second segments from the end of each client's training utterance; however, for the remaining 700 clients, we only cut one 4-second segment from the end of each client's training utterance. This yielded 900 (2\u00d7100+700) \"Evaluation\" utterances. In estimating \u03b8, w, F w , and b, each \"Evaluation\" utterance served as a client sample for its associated client, but acted as an imposter sample for each of the remaining 799 clients. This yielded 900 client samples and 719,100 (900\u00d7799) impostor samples. We used all the client samples and 2,400 randomly-selected impostor samples to estimate F w of the kernel-based classifiers. To determine \u03b8 or b, we used the 900 client samples and 18,000 randomly-selected impostor samples. This follows the suggestion in the ISCSLP2006-SRE Plan that the ratio of true clients to imposters in the \"Test\" subset should be approximately 1:20. The remaining portion of each client's training utterance was used as \"Training\" to train that client's model through UBM-MAP adaptation [Reynolds 2000 ]. This was done by first pooling all the speech in \"Training\" to train a UBM [Reynolds 2000 ] with 1,024 mixture Gaussian components, and then adapting the mean vectors of the UBM to each client's GMM according to his/her \"Training\" utterance. The signal processing front-end was same as that applied in the XM2VTS task. Experiment Results The GMM-UBM [Reynolds 2000 ] system is the current state-of-the-art approach for the text-independent speaker verification task. Thus, in this part, we focus on the performance improvements of our methods over the baseline GMM-UBM system. As with the GMM-UBM system, we used the fast scoring method [Reynolds 2000 ] for likelihood ratio computation in the proposed methods. Both the client model \u03bb and the B cohort models were adapted from the UBM \u2126. Since the mixture indices were retained after UBM-MAP adaptation, each element of the characteristic vector x was computed approximately by only considering the C mixture components corresponding to the top C scoring mixtures in the UBM [Reynolds 2000 ]. In our experiments, the value of C was set to 5. B was set to 100 in the experiments. We implemented the proposed LLR system in four ways: 1) KFD with Eq. (30) (\"KFD_w_100c\"), 2) KFD with Eq. (31) (\"KFD_w_50c_50f\"), 3) SVM with Eq. (30) (\"SVM_w_100c\"), and 4) SVM with Eq. (31) (\"SVM_w_50c_50f\"). We Kernel Discriminant Analysis for LLR-Based Speaker Verification Conclusions We have presented a new LLR measure for speaker verification that improves the characterization of the alternative hypothesis by integrating multiple background models in a more effective and robust way than conventional methods. This new LLR measure is formulated as a non-linear classification problem and solved by using kernel-based classifiers, namely, the Kernel Fisher Discriminant and Support Vector Machine, to optimally separate the LLR samples of the null hypothesis from those of the alternative hypothesis. Experiments, in which the proposed methods were applied to two speaker verification tasks, showed notable improvements in performance over classical LLR-based approaches. Finally, it is worth noting that the proposed methods can be applied to other types of data and hypothesis testing problems.",
    "abstract": "In a log-likelihood ratio (LLR)-based speaker verification system, the alternative hypothesis is usually difficult to characterize a priori, since the model should cover the space of all possible impostors. In this paper, we propose a new LLR measure in an attempt to characterize the alternative hypothesis in a more effective and robust way than conventional methods. This LLR measure can be further formulated as a non-linear discriminant classifier and solved by kernel-based techniques, such as the Kernel Fisher Discriminant (KFD) and Support Vector Machine (SVM). The results of experiments on two speaker verification tasks show that the proposed methods outperform classical LLR-based approaches.",
    "countries": [
        "Taiwan"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "3",
    "year": "2007",
    "month": "September",
    "title": "A Novel Characterization of the Alternative Hypothesis Using Kernel Discriminant Analysis for {LLR}-Based Speaker Verification"
}