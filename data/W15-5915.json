{
    "article": "We describe Authorship Attribution of Bengali literary text. Our contributions include a new corpus of 3,000 passages written by three Bengali authors, an end-to-end system for authorship classification based on character n-grams, feature selection for authorship attribution, feature ranking and analysis, and learning curve to assess the relationship between amount of training data and test accuracy. We achieve state-of-theart results on held-out dataset, thus indicating that lexical n-gram features are unarguably the best discriminators for authorship attribution of Bengali literary text. Introduction Authorship Attribution is a long-standing and wellstudied problem in Natural Language Processing where the goal is to classify documents (often short passages) according to their authorship. Different flavors of the problem treat it as either \"closed-class\" (train and test authors come from the same set), or \"open-class\" (test authors may be different from train authors). A related variant is Authorship Verification, where the goal is to verify if a given document/passage has been written by a particular author via, e.g., binary classification. Although Authoship Attribution in English has received a lot of attention since the pioneering study of Mosteller and Wallace (1963) on the disputed Federalist Papers, equivalent work in Bengali -one of the most widely spoken South Asian languageshas spawned only three strands of research till date (Das and Mitra, 2011; Chakraborty, 2012; Jana, 2015) . Part of the reason behind this lack of research progress in Bengali Authorship Attribution is a shortage of adequate corpora and tools, which has only very recently started to change (Mukhopadhyay et al., 2012) . In this paper, our contributions are as follows: \u2022 Corpus: a new corpus of 3,000 literary passages in Bengali written by three eminent Bengali authors (Section 3). \u2022 Authorship Attribution System: a classification system based on character bigrams that achieves 98% accuracy on held-out data (Section 4). \u2022 Feature Selection: six types of lexical n-gram features, and selection of the best-performing combination on an independent development set (Section 5). \u2022 Learning Curve: how the performance on heldout data changes as the number of training instances varies (Section 6). \u2022 Feature Ranking: most discriminative lexical features by Information Gain (Section 7). \u2022 Feature Analysis: frequency analysis of discriminative features, grouped by authors (Section 7). We would like to mention that there are many different ways in which our Authorship Attribution system could potentially improve or be extended (more powerful learning algorithms; syntactic, semantic and discourse features; etc). However, given that we already achieved impressive accuracy values on held-out data (Section 6), such improvements would necessarily be incremental, unless new corpora are introduced that warrant different feature sets and/or classifiers. Related Work A general overview of the topic of Authorship Attribution has been given in the surveys by Juola (2006) , Stamatatos (2009), and Koppel et al. (2009) . Unsurprisingly, there are many recent studies in English Authorship Attribution. Seroussi et al. (2012) showed that author-topic model outperforms LDA for Authorship Attribution tasks with many authors. They came up with a combination of LDA and authortopic model (DADT -disjoint author-document-topic model) that outperforms the vanilla author-topic model and an SVM baseline. Seroussi et al. (2014) further showed state-of-the-art performance on PAN 11, blog, IMDB, and court judgment datasets. As we discussed in Section 1, Authorship Attribution in Bengali is a relatively new problem. Among the three studies we found, Chakraborty (2012) performed a ten-fold cross-validation on three classes (Rabindranath, Sarat Chandra, others) with 150 documents in each, and showed that SVM outperforms decision tree and neural network classifiers. The best accuracy was 84%. An earlier study by Das and Mitra (2011) also worked with three authors -Rabindranath, Bankim Chandra, and Sukanta Bhattacharya. They had 36 documents in total. Unigram and bigram features were rich enough to yield high classification accuracy (90% for unigrams, 100% for bigrams). However, their dataset was not very large to draw reliable conclusions. Further, the authors they experimented with had very different styles, unlike our (more difficult) case where two of the authors often had similar styles in their prose (Rabindranath and Sarat Chandra). Jana (2015) looked into Sister Nivedita's influence on Jagadish Chandra Bose's writings. He notes that \"The results reveal a distinct change in Bose's writing style after his meeting with Nivedita. This is reflected in his changing pattern of usage of these three stylistic features. Bose slowly moved back towards his original style of writing after Nivedita's death, but his later works still carried Nivedita's influence.\" This study, while interesting, is not directly comparable to ours, because it did not perform any classification experiments. Among other recent studies in Authorship Attribution in Indian languages, Nagaprasad et al. (2015) worked on 300 Telugu news articles written by 12 authors. SVM was used on word and character n-grams. It was observed that Fscore and accuracy decrease as size of training data decreases, and/or the number of authors increases. In their 30-class classification problem, orthographic features performed well, and Naive Bayes was shown to perform better than KNN. The best accuracy was close to 90%. Bogdanova and Lazaridou (2014) experimented with cross-language Authorship Attribution. They designed cross-language features (sentiment, emotional, POS frequency, perceptual, average sentence length), and posited that Machine Translation could be used as a starting point to cross-language Authorship Attribution. Using six authors' English books and their Spanish translations, they obtained 79% accuracy with KNN. The best pairwise accuracy was 95%. Nasir et al. (2014) framed Authorship Attribution as semi-supervised anomaly detection via multiple kernel learning. They learned author regions from the feature space by representing the optimal solution as a linear mixture of multiple kernel functions. Luyckx and Daelemans (2008) introduced the important problem of Authorship Verification. To model realistic situations, they experimented with 145 authors and limited training data (student essays on Artificial Life). They showed that Authorship Verification is much harder than Authorship Attribution, and that more authors and less training data led to decreased performance. Memorybased learning (e.g., KNN) was shown to be robust in this scenario. An interesting study was presented by van Cranenburgh ( 2012 ), where he focused on content words rather than function words, and showed that tree kernels on fragments of constituency parse trees provide information complementary to a baseline trigram model for Authorship Attribution. Literary texts from five authors were used, and the best (combined) accuracy reached almost 98%. Sarawgi et al. (2011) attempted to remove topic bias for identifying gender-specific stylistic markers. They used deep syntactic patterns with PCFG, shallow patterns with token-level language models, morphological patterns with character-level language models, and bag of words (BoW) with MaxEnt classifier. Per-gender accuracy reached 100% using morphological features on blog data. On paper data, BoW features also reached 100% per-author accuracy for both male and female authors. Corpus In this work, we focused on Authorship Attribution of Bengali literary text, in keeping with prior studies (Das and Mitra, 2011; Chakraborty, 2012; Jana, 2015) . Note that with the emergence of social media, it would be completely valid to pursue this problem on news articles, tweets, Facebook posts, online forum threads, blog entries, or other social media outlets. However, the problems with such data are: (a) they are less clean than literary text, leading to a lot of surface variation, and (b) the number of authors is essentially unbounded, thereby rendering the problem more difficult and lowering accuracy values (Layton et al., 2010; Schwartz et al., 2013) . We chose three eminent Bengali authors for our study, and extracted 1000 passages from the works of each author. The authors are: 1. Rabindranath Tagore  2. Sarat Chandra Chattopadhyay (1876 Chattopadhyay ( -1938) ) 3. Bankim Chandra Chattopadhyay (1838-1894) Note that all three are male authors, and lived during the golden age of Bengali Renaissance, thus their writing styles could often be very similar -echoing the premises of the original Mosteller and Wallace study (1963) . Besides, these authors have an extensive repertoire of works (novels, essays, poetry, songs, dramas, short stories, reviews, letters, critiques, etc) that have been completely digitized for researchers to leverage (Mukhopadhyay et al., 2012) . 1  We sampled 1,000 random passages for each author as follows. We first removed poetry and songs because they are not uniformly distributed across all three authors. Thereafter, we merged the remaining prose in a single large file, and sampled 25 random fragments for each passage (25K fragments in total). We have taken necessary care to ensure that passage contents were disjoint. The above procedure yielded a balanced corpus of passages for the three authors. The corpus is realistic, because it embodies the fragmentary nature of realistic authorship attribution scenarios where all too often texts are not recovered in their entirety. Furthermore, it sidesteps the problem of unequal sample lengths (e.g., by having whole documents or books as samples). Our corpus statistics are shown in Table 1 . Note that the corpus has been divided into (balanced) train, test, and development sets, with 1,500 samples in the train set, and 750 samples in the test and development sets. Table 1 shows that passages from Bankim Chandra are the longest (on average), followed by Rabindranath and Sarat Chandra. The reason is the former's usage of complex and formal language constructs in his writings which typically led to longer and more intricate fragments. Authorship Attribution System We pose the problem as one of supervised classification. With three classes, our accuracy on held-out data reaches 98%. 2  In accordance with previous research, we found that the best results are obtained from most frequent lexical n-grams. Among the features we experimented with are: \u2022 Stop words: 355 Bengali stop words. 3 \u2022 Uni, bi, and trigrams: Word n-grams (n = 1, 2, 3) that are most frequent on the complete dataset. \u2022 Character bi and trigrams: Character n-grams (n = 2, 3) that are most frequent on the complete dataset. Whitespace characters were ignored. We have tried three feature representations on the above categories -binary (presence/absence), tf, and tfidf. While it may seem that such shallow features are not enough to capture the variability and complexity of individual authors, as we shall see in Section 6, the impressive performance values we obtained dispel such doubts. We used three classifiers from Weka (Hall et al., 2009) -Naive Bayes (NB), SVM SMO, and J48 decision tree -to test their performance on the development set. As shown in Table 2 , J48 performs significantly worse than NB and SMO across the board, whereas NB and SMO perform close to each other. We chose NB as our final classifier. This decision is guided by the fact that NB is simpler to conceptualize and implement, and faster to train than SMO. Note further from Table 2 that word unigrams give the best performance. However, as we shall see in Section 5, best values are obtained from character bigrams (tf), so our final system consists of 300 most frequent character bigrams (tf) as features on Naive Bayes classifier. Feature Selection As we see from Table 2 , best numbers are in the region of stop words, word unigrams, character bigrams, and character trigrams. It is therefore instructive to look into what performance benefit we can achieve by varying the number of features in these categories, along with their representation (binary/tf/tfidf). The results are shown in Figure 1 . We observed that the best development accuracy of 97.87% was obtained for 300 character bigrams (tf) feature combination, so we used that combination for our final system. Note from Figure 1 that in almost all cases, increasing the number of features led to improved performance on the development set. However, overfitting is clearly visible for character bigrams and trigrams beyond a certain number of features (around 300). This observation offers a completely organic feature selection strategy -cut off where the development accuracy dips for the first time. Note also that the features were ordered by Information Gain, so e.g. the top k n-grams are the most discriminative k n-grams within the most frequent. Learning Curve With the feature set and classifier now optimized on the development data, we re-trained the model on train set (1,500 instances) and train + development set (2,250 instances), and measured accuracy on the test set that was untouched so far. In both cases, we obtained 97.73% test accuracy -thereby showing the viability of our approach on completely untouched held-out data. To be noted is the fact that this high test accuracy is similar to the high development accuracy we obtained in Section 5. This is because our samples were drawn from the same universe of authors. Furthermore, our test accuracy is superior to the state-of-the-art (84% reported by Chakraborty ( 2012 )), and more reliable because we worked with a much larger sample of passages than (Chakraborty, 2012) and (Das and Mitra, 2011) , and because we followed a more rigorous experimental paradigm by splitting our data into three parts and selecting the model on the development set. We next asked the following question: Can we reduce the amount of training data, and still get the same (or better) performance? To answer this question, we plotted two learning curves, shown in Figure 2 . Figure 2a shows the case when we train on the training data only, and test on the test data. Figure 2b shows the case when we train on training + development data, and test on the test data. In both cases, we varied the number of training samples from 100 to 1,500 in steps of 100. 4  We empirically observed that the best test accuracy of 98.4% was obtained for 200 training instances + the development set (the first spike in Figure 2b ). In general, the performance almost always stayed within a tight band between 95 and 99%, thus indicating the validity of our approach, and (relative) insensitivity to the number of training examples. We would like to recommend that 500 training examples should be good enough for practical applications. Feature Ranking We next investigated the most discriminative features among Bengali stop words. The top 20 stop words are shown in Table 3 , ordered by their Information Gain (IG) on the training set. Note that apart from pronouns such as ta, E, o ek, and o eJ, we also obtained do-verbs such as ko o ir, ko o irya, and ko o iro et in the top ranks. This is an interesting finding. Further, we show the term frequency of the features in the last three columns of Table 3 , grouped by authors. Note that in all cases, Bankim Chandra's passages contain many more of the stop words, indicating that the passages are longer and more complex (as mentioned in Section 3). Among Rabindranath and Sarat Chandra, the variations are less systematic. Sometimes Sarat Chandra has more occurrences of a particular word, sometimes Rabindranath. Conclusion We presented the first large-scale study of Authorship Attribution in Bengali. As part of our study, we constructed a corpus of 3,000 literary passages from three eminent Bengali authors. On our balanced dataset, we performed classification experiments, and reached state-of-the-art test accuracy of 98% using character bigrams (tf) as features and Naive Bayes classifier. We further showed how performance varied on held-out data as the number of features and the number of training samples were altered. In most cases, we obtained a range of accuracy values between 95 and 99%. We analyzed the most discriminative features (stop words) and showed that the passages from one of our authors (Bankim Chandra) was longer and more complex than others. To the best of our knowledge, our study is the first reliable attempt at Authorship Attribution in Bengali, especially because prior studies had very limited training and test data. As future work, we would like to extend our approach to other forms of text, such as blogs, news articles, tweets, and online forum threads.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 3.128162811005808e-07,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the conducted research.",
    "abstract": "We describe Authorship Attribution of Bengali literary text. Our contributions include a new corpus of 3,000 passages written by three Bengali authors, an end-to-end system for authorship classification based on character n-grams, feature selection for authorship attribution, feature ranking and analysis, and learning curve to assess the relationship between amount of training data and test accuracy. We achieve state-of-theart results on held-out dataset, thus indicating that lexical n-gram features are unarguably the best discriminators for authorship attribution of Bengali literary text.",
    "countries": [
        "United States",
        "India"
    ],
    "languages": [
        "Telugu",
        "Bengali"
    ],
    "numcitedby": 17,
    "year": 2015,
    "month": "December",
    "title": "Authorship Attribution in {B}engali Language"
}