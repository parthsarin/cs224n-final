{
    "article": "Although adapting pre-trained language models with few examples has shown promising performance on text classification, there is a lack of understanding of where the performance gain comes from. In this work, we propose to answer this question by interpreting the adaptation behavior using post-hoc explanations from model predictions. By modeling feature statistics of explanations, we discover that (1) without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although few-shot fine-tuning can mitigate the prediction bias and demonstrate promising prediction performance, our analysis shows models gain performance improvement by capturing non-task-related features (e.g. stop words) or shallow data patterns (e.g. lexical overlaps). These observations alert that pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot fine-tuning. Introduction Pre-trained language models (Brown et al., 2020; Liu et al., 2019; Devlin et al., 2019) have shown impressive adaptation ability to dowstream tasks, achieving considerable performance even with scarce task-specific training data, i.e., few-shot adaptation (Radford et al., 2019; Schick and Sch\u00fctze, 2021a; Gao et al., 2021) . Existing fewshot adaptation techniques broadly fall in finetuning and few-shot learning (Shin et al., 2020; Schick and Sch\u00fctze, 2021b; Chen et al., 2021b) . Specifically, fine-tuning includes directly tuning pre-trained language models with few task-specific examples or utilizing a natural-language prompt to transform downstream tasks to masked language modeling task for better mining knowledge from pre-trained models (Petroni et al., 2019; Jiang et al., 2020; Wang et al., 2021a) . Few-shot learning lever-ages unlabeled data or auxiliary tasks to provide additional information for facilitating model training (Zheng et al., 2021; Wang et al., 2021b; Du et al., 2021a) . Although much success has been made in adapting pre-trained language models to dowstream tasks with few-shot examples, some issues have been reported. Utama et al. (2021) found that models obtained from few-shot prompt-based finetuning utilize inference heuristics to make predictions on sentence pair classification tasks. Zhao et al. (2021) discovered the instability of model performance towards different prompts in few-shot learning. These works mainly look at prompt-based fine-tuning and discover some problems. This paper looks into direct fine-tuning and provides a different perspective on understanding model adaptation behavior via post-hoc explanations (Strumbelj and Kononenko, 2010; Sundararajan et al., 2017) . Specifically, post-hoc explanations identify the important features (tokens) contribute to the model prediction per example. We model the statistics of important features over prediction labels via local mutual information (LMI) (Schuster et al., 2019; Du et al., 2021b) . We track the change of feature statistics with the model adapting from pre-trained to fine-tuned and compare it with the statistics of few-shot training examples. This provides insights on understanding model adaptation behavior and the effect of training data in few-shot settings. We evaluate two pre-trained language models, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) , on three tasks, including sentiment classification, natural language inference, and paraphrase identification. For each task, we test on both in-domain and out-of-domain datasets to evaluate the generalization of model adaptation performance. We discover some interesting observations, some of which may have been overlooked in prior work: (1) without fine-tuning, pre-trained mod-els show strong prediction bias across labels; (2) fine-tuning with a few examples can mitigate the prediction bias, but the model prediction behavior may be pathological by focusing on non-taskrelated features (e.g. stop words); (3) models adjust their prediction behaviors on different labels asynchronously; (4) models can capture the shallow patterns of training data to make predictions. The insight drawn from the above observations is that pursuing model performance with fewer examples is dangerous and may cause pathologies in model prediction behavior. We argue that future research on few-shot fine-tuning or learning should do sanity check on model prediction behavior and ensure the performance gain is based on right reasons. Setup Tasks. We consider three tasks: sentiment classification, natural language inference, and paraphrase identification. Each task contains an indomain/out-of-domain dataset pair: IMDB (Maas et al., 2011) /Yelp (Zhang et al., 2015) for sentiment classification, SNLI (Bowman et al., 2015) /MNLI (Williams et al., 2018) for natural language inference, and QQP (Iyer et al., 2017) /TwitterPPDB (TPPDB) (Lan et al., 2017) for paraphrase identification. The data statistics are in Table 4 in Appendix A.1. Models. We evaluate two pre-trained language models, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) . For each task, we train the models on the in-domain training set with different ratio (r%, r \u2208 [0, 1]) of clean examples and then test them on in-domain and out-of-domain test sets. Explanations. We explain model prediction behavior via post-hoc explanations which identify important features (tokens) in input texts that contribute to model predictions. We test four explanation methods: sampling Shapley (Strumbelj and Kononenko, 2010) , integrated gradients (Sundararajan et al., 2017) , attentions (Mullenbach et al., 2018) , and individual word masks (Chen et al., 2021a) . For each dataset, we randomly select 1000 test examples to generate explanations due to computational costs. We evaluate the faithfulness of these explanation methods via the AOPC metric (Nguyen, 2018; Chen et al., 2020) . Table 6 in Appendix A.2 shows that the sampling Shapley generates more faithful explanations than other methods. In the following experiments, we adopt it to explain model predictions. More details about the models, datasets and explanations are in Appendix A. Experiments We report the prediction results (averaged across 5 runs) of BERT and RoBERTa trained with different ratio (r% : 0 \u223c 1%) of in-domain training examples on both in-domain and out-of-domain test sets in Table 2 . Overall, training with more examples, BERT and RoBERTa achieve better prediction accuracy on both in-domain and out-of-domain test sets. We look into the predictions of models from pretrained to fine-tuned and analyze model prediction behavior change during adaptation via post-hoc explanations. In subsection 3.1, we observe that pretrained models without fine-tuning show strong prediction bias across labels. The models fine-tuned with a few examples can quickly mitigate the prediction bias by capturing non-task-related features, leading to a plausible performance gain. In subsection 3.2, we further quantify the prediction behavior change by comparing the feature statistics of model explanations and training data. We discover that the models adjust their prediction behavior on minority labels first rather than learning information from all classes synchronously and can capture the shallow patterns of training data, which may result in pathologies in predictions. Prediction bias in pre-trained models In our pilot experiments, we find the predictions of pre-trained models without fine-tuning are biased across labels (see an example of confusion matrix in Figure 2 in Appendix B). Original pre-trained models tend to predict all examples with a specific label on each dataset. We denote the specific label as the majority label and the rest labels as minority labels. The results of majority labels are in Table 1 . We propose a metric, prediction bias (PB), to quantify the bias of model predictions across labels, PB = T i 1 \u2212 T i 2 T i 1 + T i 2 \u2212 D i 1 \u2212 D i 2 D i 1 + D i 2 , (1) i 1 = argmax i\u2208{1,...,C} (T i ), i 2 = argmin i\u2208{1,...,C} (T i ) where i 1 and i 2 are the majority and most minority labels respectively.  Figure 1 shows LMI distributions of BERT on the IMDB dataset with different r, where top 5 tokens are pointed in each plot (see Table 7 in Appendix B for more results on other datasets). When r = 0, we can see that BERT makes biased predictions on the positive label (in Table 1 ) by focusing on some non-task-related high-frequency tokens. The top features associated with the negative label include some relatively low-frequency tokens (e.g.   ##men, ##zog) which may have been seen by the model during pre-training. Models adjust prediction bias by capturing nontask-related features on minority labels. Finetuning BERT with a few examples (r = 0.05, exactly 9 examples) from IMDB can quickly mitigate the prediction bias along with a plausible improvement on prediction accuracy (in Table 2 ). However, Figure 1 (the middle upper plot) shows that the model captures non-task-related high-frequency tokens to make predictions on the minority label (negative), implying the performance gain is not reasonable. Only when the model is fine-tuned with more examples (r = 0.5), it starts capturing task-specific informative tokens, such as \"bad\", \"good\". Quantifying model adaptation behavior To quantify the model prediction behavior change (in Figure 1 ) during adaptation, we compute the Kullback-Leibler divergence (KLD) between the LMI distributions of the model without/with finetuning, i.e. KL y (P 0 LM I (w, y), P r LM I (w, y)). The superscripts (\"0\" or \"r\") indicate the ratio of training examples used in fine-tuning. Besides, we also evaluate how much the model prediction behavior is learned from the patterns of training data. Specifically, we compute the LMI distribution of few-shot training examples via Equation 2 and Equation 3, except that E represents the set of features appearing in those examples. Then we use the LMI distribution of data as the reference and compute the KLD between it and the LMI distribution of model explanations. Table 3 records the results of KLD with the LMI distribution of original pre-trained model explanations as the reference (columns of \"Ori\") or that of training data as the reference (columns of \"Data\"). Note that we do not have the results of RoBERTa on some labels (e.g. \"Neg\") in \"Ori\" columns because the pre-trained RoBERTa does not make any predictions on those labels and we do not have the reference LMI distributions. Models adjust their prediction behaviors on different labels asynchronously. In \"Ori\" columns, the KLDs on minority labels are larger than those on majority labels when r is small (e.g. 0.05). The changes of KLDs are discrepant across labels with r increasing. The results show that the models focus on adjusting their prediction behavior on minority labels first rather than learning from all classes synchronously in few-shot settings. Models can capture the shallow patterns of training data. In \"Data\" columns, the KLDs on SNLI and QQP are overall smaller than those on IMDB, illustrating that it is easier for models to learn the patterns of datasets on sentence-pair classification tasks. With r increasing, the KLDs on the entailment label of SNLI are smaller than those on other labels, which validates the observations in previous work (Utama et al., 2021; Nie et al., 2019) that models can capture lexical overlaps to predict the entailment label. Another interesting observation is the KLDs on Yelp in \"Data\" columns are mostly smaller than those on IMDB. This indicates that models may rely on the shallow patterns of in-domain datasets to make predictions on out-ofdomain datasets. Conclusion In this work, we take a closer look into the adaptation behavior of pre-trained language models in few-shot fine-tuning via post-hoc explanations. We discover many pathologies in model prediction behavior. The insight drawn from our observations is that promising model performance gain in fewshot learning could be misleading. Future research on few-shot fine-tuning or learning requires sanity check on model prediction behavior and some careful design in model evaluation and analysis. A Supplement of Setup A.1 Models and Datasets We adopt the pretrained BERT-base and RoBERTabase models from Hugging Face 1 . For sentiment classification, we utilize movie reviews IMDB (Maas et al., 2011) as the in-domain dataset and Yelp reviews (Zhang et al., 2015) as the out-ofdomain dataset. For natural language inference, the task is to predict the semantic relationship between a premise and a hypothesis as entailment, contradiction, or neutral. The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) are used as the in-domain and out-of-domain datasets respectively. The task of paraphrase identification is to judge whether two input texts are semantically equivalent or not. We adopt the Quora Question Pairs (QQP) (Iyer et al., 2017) as the in-domain dataset, while using the TwitterPPDB (TPPDB) (Lan et al., 2017) as the out-of-domain dataset. Table 4 shows the statistics of the datasets. We implement the models in PyTorch 3.6. We set hyperparameters as: learning rate is 1e\u22125, maximum sequence length is 256, maximum gradient norm is 1, and batch size is 8. All experiments were performed on a single NVidia GTX 1080 GPU. We report the time for training each model on each in-domain dataset (with full training examples) in Table 5 . A.2 Explanations We adopt four explanation methods: \u2022 sampling Shapley (SS) (Strumbelj and Kononenko, 2010) : computing feature attributions via sampling-based Shapley value (Shapley, 1953); \u2022 integrated gradients (IG) (Sundararajan et al., 2017) : computing feature attributions by integrating gradients of points along a path from a baseline to the input; \u2022 attentions (Attn) (Mullenbach et al., 2018) : attention weights in the last hidden layer as feature attributions; \u2022 individual word masks (IMASK) (Chen et al., 2021a) : learning feature attributions via variational word masks (Chen and Ji, 2020) . 1 https://github.com/huggingface/ pytorch-transformers Explanation faithfulness. An important criterion for evaluating explanations is their faithfulness to model predictions (Jacovi and Goldberg, 2020) . We evaluate the faithfulness of the four explanation methods via the AOPC metric (Nguyen, 2018; Chen et al., 2020) . AOPC calculates the average change of prediction probability on the predicted class over all examples by removing top 1 . . . u words identified by explanations. AOPC = 1 U + 1 \u27e8 U u=1 p(y|x) \u2212 p(y|x \\1...u )\u27e9 x , (4) where p(y|x \\1...u ) is the probability for the predicted class when words 1 . . . u are removed and \u27e8\u2022\u27e9 x denotes the average over all test examples. Higher AOPC score indicates better explanations. We test the BERT and RoBERTa trained with 1% in-domain training examples on each task. For each dataset, we randomly select 1000 test examples to generate explanations due to computational costs. We report the results of AOPC scores when U = 10 in Table 6 . Sampling Shapley consistently outperforms other three explanation methods in explaining different models on both in-domain and out-of-domain datasets. B Supplement of Experiments Acknowledgments We thank the anonymous reviewers for many valuable comments.",
    "abstract": "Although adapting pre-trained language models with few examples has shown promising performance on text classification, there is a lack of understanding of where the performance gain comes from. In this work, we propose to answer this question by interpreting the adaptation behavior using post-hoc explanations from model predictions. By modeling feature statistics of explanations, we discover that (1) without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although few-shot fine-tuning can mitigate the prediction bias and demonstrate promising prediction performance, our analysis shows models gain performance improvement by capturing non-task-related features (e.g. stop words) or shallow data patterns (e.g. lexical overlaps). These observations alert that pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot fine-tuning.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "May",
    "title": "Pathologies of Pre-trained Language Models in Few-shot Fine-tuning"
}