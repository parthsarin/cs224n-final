{
    "article": "We introduce an adaptable monolingual chunking approach-Alignment-Guided Chunking (AGC)-which makes use of knowledge of word alignments acquired from bilingual corpora. Our approach is motivated by the observation that a sentence should be chunked differently depending the foreseen end-tasks. For example, given the different requirements of translation into (say) French and German, it is inappropriate to chunk up an English string in exactly the same way as preparation for translation into one or other of these languages. We test our chunking approach on two language pairs: French-English and German-English, where these two bilingual corpora share the same English sentences. Two chunkers trained on French-English (FE-Chunker ) and German-English (DE-Chunker ) respectively are used to perform chunking on the same English sentences. We construct two test sets, each suitable for French-English and German-English respectively. The performance of the two chunkers is evaluated on the appropriate test set and with one reference translation only, we report Fscores of 32.63% for the FE-Chunker and 40.41% for the DE-Chunker. Introduction Chunking plays an important role in parsing, information extraction and information retrieval. Chunking is often a useful preprocessing step for many bilingual tasks, such as machine translation, cross language information retrieval, etc. We introduce an adaptable chunking approach guided by word alignments automatically acquired from a bilingual corpus. Our approach is motivated by the observation that a sentence should be chunked differently depending the end-task in mind. Our approach employs bilingual word alignment in training and is tested on the monolingual chunking task. Our goal is to build adaptable monolingual chunkers for different language pairs, with the aim of facilitating bilingual language processing tasks. We investigate our chunking approach on two language pairs: French-English and German-English, where these two bilingual corpora share the same English sentences. Two chunkers trained on French-English (FE-Chunker ) and German-English (DE-Chunker ) respectively are used to perform chunking on the same English sentences. We construct two test sets, each suitable for French-English and German-English respectively. The performance of the two chunkers is evaluated on the appropriate test set and with one reference translation only, we report F-scores of 32.63% for the FE-Chunker and 40.41% for the DE-Chunker. We also extend our chunking approach with Multilevel Chunking, which is more tolerant of any chunking errors obtained. The remainder of this paper is organized as follows. In Section 2, we review the previous research on chunking including monolingual chunking and bilingual chunking. Section 3 describes our chunking method. In Section 4, the experimental setting is described. In Section 5, we evaluate our chunking method on a one-reference 'gold standard' testset. Section 6 concludes the paper and gives avenues for future work. Previous Research Monolingual Chunking Most state-of-the-art monolingual chunking methods are linguistically motivated. The CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) defined chunking as dividing text into syntactically related nonoverlapping groups of words. Chunks are directly converted from the Penn Treebank (Marcus et al., 1993) and each chunk is labelled with a specific grammatical category, such as NP, VP, PP, ADJP etc. This chunking method is sensitive to the grammars of a specific language and performs chunking in a monolingual context. Marker-based chunking is another syntaxaware chunking strategy. This chunking approach is based on the \"Marker Hypothesis\" (Green, 1979) , a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or \"marker\") words, such as determiners, conjunctions, prepositions, possessive and personal pronouns, aligned source-target sentences are segmented into chunks. A chunk is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. Although marker-based chunking has been used in bilingual tasks such as machine translation between European languages (Gough and Way, 2004; Groves and Way, 2005; Stroppa and Way, 2006) , which are relatively similar with regard to marker words and word orders, it is less appropriate for language pairs as different as Chinese and English (Ma, 2006) . Bilingual Chunking Bilingual chunkers are usually based on parsing technology. (Wu, 1997) proposed Inversion Transduction Grammar (ITG) as suitable for the task of bilingual parsing. The stochastic ITG brings bilingual constraints to many corpus analysis tasks such as segmentation, bracketing, and parsing, which are usually carried out in a monolingual context. However, it is difficult to write a broad bilingual ITG grammar capable of dealing with long sentences. (Wang et al., 2002) proposed an algorithm integrating chunking and alignment and obtained good precision. However, this method needs quite a lot of syntax information and prior knowledge. (Liu et al., 2004) proposed an integrated probabilistic model for bilingual chunking and alignment independent of syntax information and grammatical rules. Alignment-Guided Chunking Notation While in this paper, we focus on both French-English and German-English, the method proposed is applicable to any language pair. The notation however assumes the French-English task in what follows. Given a French sentence f I 1 consisting of I words {f 1 , . . . , f I } and an English sentence e J 1 consisting of J words {e 1 , . . . , e J }, A F \u2192E (resp. A E\u2192F ) will denote a French-to-English (resp. an English-to-French) word alignment between f I 1 and e J 1 . As 1-to-n alignments are quite common, A F \u2192E can be represented as a set of pairs a i = f i , E i denoting a link between one single French word f i and a few English words E i (and similarly for A E\u2192F ). The set E i is empty if the word f i is not aligned to any word in e J 1 . Given a French-English sentence pair f I 1 , e J 1 , suppose f i is aligned to a set of En-glish words E i = {e j , . . . , e j+m }, and E I i+1 = E i+1 \u2022 \u2022 \u2022 E I = {e k , . . . , e k+n } denotes a union of English words that are aligned to the set of French words {f i+1 . . . , f I }. There should be a partition between f i and f i+1 , iff. k > j + m. We can partition the English sentence using the same method. Given a French-English sentence pair and the word alignment between them, we can partition both French and English sentences following the criteria described above. As this chunking is guided by the word alignment, we call it Alignment-Guided Chunking. Assume the French-English sentence pair and their word alignment in (1): (1) French: Cette ville est charg\u00e9e de symboles puissants pour les trois religions monoth\u00e9istes . English: The city bears the weight of powerful symbols for all three monotheistic religions . Word alignment: 0-0 1-1 2-2 3-4 4-5 5-7 6-6 7-8 8-9 9-10 10-12 11-11 12-13 The AGC chunks derivable via our method are displayed in Figure 1 . Note that the method is able to capture adjective-noun combinations in each language, as well as the determiner-noun pair in English. Data Representation (Ramshaw and Marcus, 1995) introduced a data representation for baseNP chunking by converting it into a tagging task: words inside a baseNP were marked I, words outside a baseNP receive an O tag, and a special tag B was used for the first word inside a baseNP immediately following another baseNP. (Tjong Kim Sang and Veenstra, 1999) examined seven different data representations for noun phrase chunking and showed that the choice of data representation has only a minor influence on chunking performance. In our chunking approach, every word is classified into a chunk and no fragments are left in a sentence. Accordingly, we do not need the tag O to mark any word outside a chunk. We can employ three data representations similar to (Tjong Kim Sang and Veenstra, 1999) named IB, IE, IBE1, IBE2, where the I tag is used for words inside a chunk. They differ in their treatment of chunk-initial and chunk-final words as shown in Table 1 . In our experiments, we use IE to represent the data, so that the problem of chunking is transformed instead into a binary classification task. The IE tag representation for the English sentence in Figure 1 is shown in ( 2 ): (2) The/E city/E bears/E the/I weight/E of/E powerful/I symbols/E for/E all/E three/E monotheistic/I religions/E ./ Again, note the dependence of determiners and adjectives on their following head noun. Parameter Estimation In this section, we briefly introduce two wellknown machine learning techniques we used for parameter estimation, namely Maximum Entropy (MaxEnt) and Memory-based learning (MBL). Both of them are widely used in Natural Language Processing (NLP). Maximum Entropy was first introduced in NLP by (Berger et al., 1996) . It is also used for chunking (Koeling, 2000) . Memorybased learning (e.g. (Daelemans and Van den Bosch, 2005) ) is based on the simple twin ideas that: \u2022 learning is based on the storage of exemplars, and MBL can be used simply and effectively to perform a range of classification tasks. Feature Selection Feature selection is important for the performance for both machine learning techniques. In practice, the features we used are shown in Table 2 . The information we used was contained in a 7-word window, i.e. the leftmost three words and their Part-of-Speech (POS) tags, the current word and its POS tag, and the rightmost three words and their POS tags. Multi-level Chunking Notation Given a sentence s I 1 containing I words {w 1 , . . . , w I }, chunking can be considered as the process of inserting a chunk boundary marker c i between two consecutive words w i , w i+1 . The probability of inserting a chunk boundary marker c i between two consecutive words w i , w i+1 (i.e. the partition probability) can be defined as: \u00d4(c i |s I 1 ) = p \u03bb M 1 (c i |s I 1 ) = exp[ M m=1 \u03bb m h m (c i , s I 1 )] c \u2032 i exp[ M m=1 \u03bb m h m (c \u2032 i , s I 1 )] For sentence s I 1 , we can derive a set of partition probabilities with I \u2212 1 elements: P P = {\u00d4(c 1 |s I 1 ), . . . , \u00d4(c I\u22121 |s I 1 )} By setting different thresholds for our partition probabilities, we can obtain different chunking results for the same sentence. This threshold can be adjusted depending on the task at hand with the result that different chunking patterns for the same sentence are obtained. We call this chunking model Multilevel Chunking. If we relate this model to our IE data representation (cf. (2) above), it is equivalent to determining the probability of a word being labelled E. While most chunking approaches are essentially classification-based, our model attempts to transform the classification-based approach into a ranking problem and decide the partition point of a sentence by examining competitive scores at each point. We call this chunking approach Ranking-based Chunking. The set of parameters in this model include (i) the set of partition probabilities, and (ii) estimates of thresholds for partition probabilities bearing in mind the specific task to be performed.   Word w i\u22123 w i\u22122 w i\u22121 w i w i+1 w i+2 w i+3 POS t i\u22123 t i\u22122 t i\u22121 t i t i+1 t i+2 t i+3 Table 2 : Features for chunking Threshold Estimation The average length of chunks can be estimated from training data acquired following the criteria described in Section 3.1. With an estimation of average chunk length, we can set a chunking threshold to chunk a sentence. Experimental Setting Evaluation Using the Alignment-Guided Chunking approach described in Section 3, we can train two different chunkers on French-English (FE-Chunker) and German-English (DE-Chunker) bilingual corpora respectively. We use the two chunkers to perform chunking on the same English sentences. Two test sets are constructed, each suitable for the FE-Chunker and the DE-Chunker respectively. The performance of the two chunkers is evaluated on the appropriate test set. Gold Standard Test Set For each sentence E in the test set, there could be N translation references r N 1 . For each sentence pair < E, r i >, a unique word alignment A i can be acquired. Following the criteria described in Section 3.1, we can derive N chunking results C N 1 using < E, A i > (i \u2208 [0, N ]). All these chunking results should be considered to be correct. Chunking results for E using our approach are evaluated on C N 1 using just one 'gold standard' reference. We firstly construct the test set automatically using the criteria described in Section 3.1. After that we check all the sentences manually to correct all the chunking errors due to word alignment errors. Data The experiments were conducted on French-English and German-English sections of the Europarl corpus (Koehn, 2005) Release V1. 1 This corpus covers April 1996 to December 2001, and we use the Q4/2000 portion of the data (2000-10 to 2000-12) for testing, with the other parts used for training. The English sentences in the French-English and German-English corpora are not exactly the same due to differences in the sentence-alignment process. We obtain the intersection of the English sentences and their correspondences to construct a new French-English corpus and German-English corpus, where these two corpus now share exactly the same English sentences. In order to test the scalability of our chunking approach, we first use 150k of the sentence pairs for training, which we call the Small Data set. Then we use all the sentence pairs (around 300k sentence pairs) for training. We call this the Large Data set. We tag all the English sentences in the training and test sets using a maximum entropy-based Part-of-Speech tagger-MXPOST (Ratnaparkhi, 1996) , which was trained on the Penn Treebank (Marcus et al., 1993) . We use the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) 2 and refinement heuristics described in (Koehn et al., 2003) to derive the final word alignment. We used the Maximum Entropy toolkit 'maxent', 3 and the Memory-based learning toolkit TiMBL 4 for parameter estimation. Statistics on Training Data To demonstrate the feasibility of adapting our chunking approach to different languages, we obtained some statistics on the chunks of two training sets derived from French-English (F-E, 300k-sentence pairs) and German-English (D-E, 300k-sentence pairs) corpora respectively. There are 3,316,887 chunks identified in the F-E corpus and 2,915,325 chunks in the D-E corpus. A number of these chunks overlap: 42.08% in the F-E corpus and 47.87% in the D-E corpus (cf. Table 3 ). The number of overlapping chunks (OL chunks) between these two corpora is 1,395,627. F-E D-E No. of Chunks 3, 316, 887 2, 915, 325 OL Chunks[%] 42.08% 47.87% We can also estimate the average chunk length on training data. Using the F-E corpus, the average chunk length for English is 1.84 words and 2.10 words using the D-E corpus. This demonstrates definitively that our approach does carve up sentences differently depending on the target language in question. Experimental Results Results Two machine learning techniques-Maximum Entropy (MaxEnt) and Memory-based learning (MBL)-are used for chunking. In order to test the scalability of our chunking model, we carried out experiments on both the Small data and Large data sets described in Section 4.3. The detailed results are shown in Table 4 . Here we can see that the F-score is quite low because we have just one reference in the test set (see Section 4.2). Furthermore, we see no significant improvement with the maximum entropy method when more data is used. F-scores for German chunks are on the whole between 25 and 33% higher than for French. For German, when using MaxEnt Precision scores are significantly higher than Recall, but the opposite is seen when MBL chunks are used. For French, Recall scores are higher in general than those for Precision. Figure 4 gives an example of chunking results using MaxEnt. Note the differences between this output and that in Figure 3 : the determiner the has now been properly grouped with the following N-bar weight of powerful symbols ..., and similarly all belongs more closely to three monotheistic religions than it did before. Multi-level Chunking As an extension to our classification-based chunking method, multi-level chunking can be regarded as an application of ranking. We obtain the global chunk length from the training data to derive the optimal partition threshold. We use the average chunk length from the training data described in Section 4.4, i.e. for the French-English task, the average English chunk length is 1.84 words, whereas it is 2.10 words for German-English. The results of applying the multi-level chunking method (Multi) are shown in Table 5 . By using the multi-level chunker, we can see a slight increase in recall together with a sharp decrease in precision. This demonstrates that deriving chunks using just a global average chunk length is likely to be sub-optimal for any given sentence. Conclusions and Future Work In this paper, we have introduced a novel chunking approach guided by the word alignment acquired from bilingual corpora. We investigate our chunking approach on two language pairs: French-English and German-English, where these two bilingual corpora share the same English sentences. Two machine learning techniques-Maximum Entropy and Memory-based learning-were employed to perform chunking. We demonstrate the impact of chunking results on the English side due to the differences between French-English word alignment and German English word alignment, demonstrating the merit of such a chunking approach in a bilingual context. We evaluate the performance of our chunking approach on a one-reference gold standard test set and report an F-score As for future work, we want to experiment with other methods of word alignment (e.g. (Tiedemann, 2004; Liang et al., 2006; Ma et al., 2007) ) in order to establish which one is most appropriate for our task. We also want to apply this method to other corpora and language pairs, especially using IWSLT data where for 4 language pairs we have 16 reference translations. We anticipate that our chunking approach is likely to be of particular benefit, at least in theory, in a statistical machine translation task given the complexities of the decoding process. Nonetheless, the principal remaining concern is whether the better motivated yet considerably smaller number of bilingual chunks derived via our method will lose out in a real task-oriented evaluation compared to a baseline system seeded with phrase pairs produced in the usual manner. Acknowledgments This work is supported by Science Foundation Ireland (grant number OS/IN/1732). We would also like to thank the anonymous re-viewers whose insightful comments helped improve this paper.",
    "abstract": "We introduce an adaptable monolingual chunking approach-Alignment-Guided Chunking (AGC)-which makes use of knowledge of word alignments acquired from bilingual corpora. Our approach is motivated by the observation that a sentence should be chunked differently depending the foreseen end-tasks. For example, given the different requirements of translation into (say) French and German, it is inappropriate to chunk up an English string in exactly the same way as preparation for translation into one or other of these languages. We test our chunking approach on two language pairs: French-English and German-English, where these two bilingual corpora share the same English sentences. Two chunkers trained on French-English (FE-Chunker ) and German-English (DE-Chunker ) respectively are used to perform chunking on the same English sentences. We construct two test sets, each suitable for French-English and German-English respectively. The performance of the two chunkers is evaluated on the appropriate test set and with one reference translation only, we report Fscores of 32.63% for the FE-Chunker and 40.41% for the DE-Chunker.",
    "countries": [
        "Ireland"
    ],
    "languages": [
        "English",
        "French",
        "German"
    ],
    "numcitedby": "11",
    "year": "2007",
    "month": "September 7-9",
    "title": "Alignment-guided chunking"
}