{
    "article": "In this paper we report results of a supervised machine-learning approach to Chinese word segmentation. A maximum entropy tagger is trained on manually annotated data to automatically assign to Chinese characters, or hanzi, tags that indicate the position of a hanzi within a word. The tagged output is then converted into segmented text for evaluation. Preliminary results show that this approach is competitive against other supervised machine-learning segmenters reported in previous studies, achieving precision and recall rates of 95.01% and 94.94% respectively, trained on a 237K-word training set. Introduction It is generally agreed among researchers that word segmentation is a necessary first step in Chinese language processing. However, unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community [Fan and Tsai, 1988; Gan, 1995; Gan, Palmer, and Lua, 1996; Guo, 1997; Jin and Chen, 1998; Sproat and Shih, 1990; Sproat et al., 1996; Wu and Jiang, 1998; Wu, 2003] . It is easy to demonstrate that the lack of natural delimiters itself is not the heart of the problem. In a hypothetical language where all words are represented with a finite set of symbols, if one subset of the symbols always start a word and another subset, mutually exclusive from the previous subset, always end a word, identifying words would be a trivial exercise. Nor can the problem be attributed to the lack of inflectional morphology. Although it is true in Indo-European languages inflectional affixes can generally be used to signal word boundaries, it is conceivable that a hypothetical language can use symbols other than inflectional morphemes to serve the same purpose. Therefore the issue is neither the lack of natural word delimiters nor the lack of inflectional morphemes in a language, rather it is whether the language has a way of unambiguously signaling the boundaries of a word. The real difficulty in automatic Chinese word segmentation is the lack of such unambiguous word boundary indicators. In fact, most hanzi can occur in different positions within different words. The examples in Table 1 show how the Chinese character \u4ea7 (\"produce\") can occur in four different positions. This state of affairs makes it impossible to simply list mutually exclusive subsets of hanzi that have distinct distributions, even though the number of hanzi in the Chinese writing system is in fact finite. As long as a hanzi can occur in different word-internal positions, it cannot be relied upon to determine word boundaries as they could be if their positions were more or less fixed. The fact that a hanzi can occur in multiple word-internal positions leads to ambiguities of various kinds, which are described in detail in [Gan, 1995] . For example, \u6587 can occur in both word-initial and word-final positions. It occurs in the word-final position in \u65e5\u6587 (\"Japanese\") but in the word-initial position in \u6587\u7ae0(\"article\"). In a sentence that has a string \"\u65e5\u6587\u7ae0\", as in (1) 1 , an automatic segmenter would face the dilemma whether to insert a word boundary marker between \u65e5 and \u6587, thus grouping \u6587\u7ae0 as a word, or to mark \u65e5\u6587 as a word, to the exclusion of \u7ae0. The same scenario also applies to \u7ae0, since like \u6587, it can also occur in both word-initial and word-final positions. Table 1. A hanzi can occur in multiple word-internal positions 1. (a) Segmentation I \u65e5\u6587 \u7ae0\u9b5a \u600e\u9ebc \u8aaa? Japanese octopus how say \"How to say octopus in Japanese?\" (b) Segmentation II 1 Adapted from [Sproat et al.,1996] \u65e5 \u6587\u7ae0 \u9b5a \u600e\u9ebc \u8aaa? Japan article fish how say Ambiguity also arises because some hanzi should be considered to be just word components in certain contexts and words by themselves in others. For example, \u9b5a can be considered to be just a word component in \u7ae0\u9b5a. It can also be a word by itself in other contexts. Presented with the string \u7ae0\u9b5a in a Chinese sentence, a human or automatic segmenter would have to decide whether \u9b5a should be a word by itself or form another word with the previous hanzi. Given that \u65e5, \u6587\u7ae0, \u7ae0\u9b5a, \u9b5a are all possible words in Chinese, how does one decide that \u65e5\u6587 \u7ae0\u9b5a is the right segmentation for the sentence in (1) while \u65e5 \u6587\u7ae0 \u9b5a is not? Obviously it is not enough to know just what words are in the lexicon. In this specific case, a human segmenter can resort to world knowledge to resolve this ambiguity, knowing that \u65e5 \u6587\u7ae0 \u9b5a would not make any kind of real-world sense. In other cases a human segmenter can also rely on syntactic knowledge to properly segment a sentence. For instance, \u67aa should be considered a word in (2a) and two words in (2b): 2. a \u8b66\u5bdf \u67aa-\u6740 \u4e86 \u90a3 \u4e2a \u9003\u72af police gun-kill LE that CL escapee \" Police killed the escapee with a gun.\" b \u8b66\u5bdf \u7528 \u67aa \u6740 \u4e86 \u90a3 \u4e2a \u9003\u72af Police with gun kill LE that CL escapee \" Police killed the escapee with a gun\" In (2b), \u67aa is a word by itself and forms a phrasal constituent with the preceding \u7528. In order to get the segmentation right for the example in (2) one needs to know, for example, that \u7528 has to take a complement and in the case of (2b) the complement is \u67aa. Therefore it is impossible for \u67aa to be part of the word \u67aa\u6740. The human segmenter has little difficulty resolving these ambiguities and coming up with the correct segmentation since they have linguistic and world knowledge at their disposal. However, the means available to the human segmenter cannot be made available to computers just as easily. As a result, an automatic word segmenter would have to bypass such limitations to resolve these ambiguities. In addition to the ambiguity problem, another problem that is often cited in the literature is the problem of so-called out-of-vocabulary or \"unknown\" words [Wu and Jiang, 1998 ]. The unknown word problem arises because machine-readable dictionaries cannot possibly list all the words encountered in NLP tasks exhaustively 2 . For one thing, although the number of hanzi generally remains constant, Chinese has several productive new word creation mechanisms. First of all, new words can be created through compounding, in which new words are formed through the combination of existing words, or through suoxie, in which components of existing words are extracted and combined to form new words. Second, new names are created by combining existing characters in a very unpredictable manner. Third, there are also transliterations of foreign names. These are just a few of the many ways new words can be introduced in Chinese. The key to accurate automatic word identification in Chinese lies in the successful resolution of these ambiguities and a proper way to handle out-of-vocabulary words. We have demonstrated that the ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions. Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. If the positions of all the hanzi in a sentence can be determined with the help of the context, the word segmentation problem would be solved. This is the line of thinking we are going to pursue in the present work. There are several reasons why we may expect this approach to work. First, Chinese words generally have fewer than four characters. As a result, the number of positions is small. Second, although each hanzi can in principle occur in all possible positions, not all hanzi behave this way. A substantial number of hanzi are distributed in a constrained manner. For example, \u4eec, the plural marker, almost always occurs in the word-final position. Finally, although Chinese words cannot be exhaustively listed and new words are bound to occur in naturally occurring text, the same is not true for hanzi. The number of hanzi stays fairly constant and we do not generally expect to see new hanzi. In this paper, we model the Chinese word segmentation problem as a hanzi tagging problem and use a machine-learning algorithm to determine the word-internal positions of hanzi with the help of contextual information. The remainder of this paper is organized as follows. In Section 2, we briefly review the representative approaches in the previous studies on Chinese word segmentation. In Section 3, we describe how the word segmentation problem can be modeled as a tagging problem and how the maximum entropy model is used to solve this problem. We describe our experiments in Section 4. In Section 5, we report our experimental results, using the maximum matching algorithm as a baseline. We also evaluate these results against previous approaches and discuss the contributions of different feature sets and the effectiveness of different tag sets. We conclude this paper and discuss future work in Section 6. Previous Work Various methods have been proposed to address the word segmentation problem in previous studies. Noting that linguistic information, syntactic information in particular, can help identify words, [Gan, 1995] and [Wu and Jiang, 1998 ] treated word segmentation as inseparable from Chinese sentence understanding as a whole. As a result, the success of the word segmentation task is tied to the success of the sentence understanding task, which is just as difficult as the word segmentation problem, if not more difficult. Most of the word segmentation systems reported in previous studies are stand-alone systems and they fall into three main categories, depending on whether they use statistical information and electronic dictionaries. These are purely statistical approaches [Sproat and Shih, 1990; Sun, Shen, and Tsou, 1998; Ge, Pratt, and Smyth, 1999; Peng and Schuurmans, 2001] , non-statistical dictionary-based approaches [Liang, 1993; Gu and Mao, 1994] and statistical and dictionary-based approaches [Sproat et al., 1996] . More recently work on Chinese word segmentation also includes supervised machine-learning approaches [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001] . Purely dictionary-based approaches generally addresses the ambiguity problem with some heuristics, and the most successful heuristics are variations of the maximum matching algorithm. A maximum matching algorithm is a greedy search routine that walks through a sentence trying to find the longest string of hanzi starting from a given point in the sentence that matches a word entry in a pre-compiled dictionary. For instance, assuming \u5173 (\"close\"), \u5fc3 (\"heart\") and \u5173\u5fc3 (\"care about\") are all listed in the dictionary, given a string of hanzi \u5173-\u5fc3, the maximum matching algorithm always favors \u5173\u5fc3 as a word, over \u5173-\u5fc3 as a string of two words. This is because \u5173\u5fc3 is a longer string than \u5173 and both of them are in the dictionary. When the segmenter finds \u5173, it will continue to search and see if there is a possible extension. When it finds another word \u5173\u5fc3 in the dictionary it will decide against inserting a word boundary between \u5173 and \u5fc3. When the algorithm can no longer extend the string of hanzi it stops searching and inserts a word boundary marker. The process is repeated from the next hanzi till it reaches the end of the sentence. The algorithm is successful because in a lot of cases, the longest string also happens to be correct segmentation. For example, for the example in (1), the algorithm will rightly decide that (1a) rather than (1b) is the correct segmentation for the sentence, assuming \u65e5, \u65e5\u6587, \u6587\u7ae0, \u7ae0\u9c7c and \u9c7c are all listed in the dictionary. However, this algorithm will output the wrong segmentation for (2b), in which it will incorrectly group \u67aa\u6740 as a word. In addition, the maximum matching algorithm does not have a built-in mechanism to deal with out-of-vocabulary words. In general, the completeness of the dictionary to a large extent determines the degree of success for segmenters using this approach. As a representative of purely statistical approaches, [Sproat and Shih, 1990 ] relies on the mutual information of two adjacent characters to decide whether they form a two-character word. Given a string of characters 1 n cc ... , the pair of adjacent characters with the largest mutual information greater than a pre-determined threshold is grouped as a word. This process is repeated until there are no more pairs of adjacent characters with a mutual information value greater than the threshold. This algorithm is extended by [Sun, Shen, and Tsou, 1998 ] so that association measures other than mutual information are also taken into consideration. More recently, [Ge, Pratt, and Smyth, 1999; Peng and Schuurmans, 2001] applied expectation maximization methods to Chinese word segmentation. For example, [Peng and Schuurmans, 2001 ] used an EM-based algorithm to estimate probabilities for words in a dictionary and use mutual information to weed out proposed words whose components are not strongly associated. Purely statistical approaches have the advantage of not needing a dictionary or training data, and since unsegmented data are easy to obtain, they can be easily trained on any data source. The drawback is that statistical approaches generally do not perform well in terms of the accuracy of the segmentation. Statistical dictionary-based approaches attempt to get the best of both worlds by combining the use of a dictionary and statistical information such as word frequency. [Sproat et al., 1996] represents a dictionary as a weighted finite-state transducer. Each dictionary entry is represented as a sequence of arcs labeled with a hanzi and its phonemic transcription, starting from an initial state 0 and terminated by a weighted arch labeled with an empty string \u03b5 and a part-of-speech tag. The weight represents the estimated cost of the word, which is its negative log probability. The probabilities of the dictionary words as well as morphologically derived words not in the dictionary are estimated from a large unlabeled corpus. Given a string of acceptable symbols (all the hanzi plus the empty string), there exists a function that takes this string of symbols as input and produces as output a transducer that maps all the symbols to themselves. The path that has the cheapest cost is selected as the best segmentation for this string of characters. Compared with purely statistical approaches, statistical dictionary-based approaches have the guidance of a dictionary and as a result they generally outperform purely statistical approaches in terms of segmentation accuracy. Recent work on Chinese word segmentation has also used the transformation-based error-driven algorithm [Brill, 1993] and achieved various degrees of success [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001] . The transformation-based error-driven algorithm is a supervised machine-learning routine first proposed by [Brill, 1993] and initially used in POS tagging as well as parsing. It has been applied to Chinese word segmentation by [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001] . Although the actual implementation of this algorithm may differ slightly, in general the transformation-based error-driven approaches try to learn a set of n -gram rules from a training corpus and apply them to segment new text. The input to the learning routine is a (manually or automatically) segmented corpus and its unsegmenteded (or undersegmented) counterpart. The learning algorithm compares the segmented corpus and the undersegmented dummy corpus at each iteration and finds the rule that achieves the maximum gain if applied. The rule with the maximum gain is the one that makes the dummy corpus most like the reference corpus. The maximum gain is calculated with an evaluation function which quantifies the gain and takes the largest value. The rules are instantiations of a set of pre-defined templates. After the rule with the maximum gain is found, it is applied to the dummy corpus, which will better resemble the reference corpus as a result. This process is repeated until the maximum gain drops below a pre-defined threshold, which indicates improvement achieved through further training will no longer be significant. The output of the training process would be a ranked set of rules instantiating the predefined set of templates. The rules will then be used to segment new text. Like statistical approaches, this approach provides a trainable method to learn the rules from a corpus and it is not labor-intensive. The drawback is that compared with statistical approaches, this algorithm is not very efficient. The present work represents another supervised machine-learning approach. Specifically, we applied the maximum entropy model, a statistical machine-learning algorithm to Chinese word segmentation. A supervised machine-learning algorithm to Chinese word segmentation In this section, we first formalize the idea of tagging hanzi based on their word-internal positions and describe the tag set we used. We then briefly describe the maximum entropy model, which has been successfully applied to POS tagging as well as parsing [Ratnaparkhi, 1996; Ratnaparkhi, 1998] . Reformulating word segmentation as a tagging problem Before we apply the machine-learning algorithm first we convert the manually segmented words in the corpus into a tagged sequence of Chinese characters. To do this, we tag each character with one of the four tags, LL, RR, MM and LR depending on its position within a word. It is tagged LL if it occurs on the left boundary of a word, and forms a word with the character(s) on its right. It is tagged RR if it occurs on the right boundary of a word, and forms a word with the character(s) on its left. It is tagged MM if it occurs in the middle of a word. It is tagged LR if it forms a word by itself. We call such tags position-of-character (POC) tags to differentiate them from the more familiar part-of-speech (POS) tags. For example, the manually segmented string in (3a) will be tagged as (3b): Given a manually segmented corpus, a POC-tagged corpus can be derived trivially with perfect accuracy. The reason why we use such POC-tagged sequences of characters instead of applying n -gram rules to segmented corpus directly [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001] is that they are much easier to manipulate in the training process. In addition, the POC tags reflect our observation that the ambiguity problem is due to the fact that a hanzi can occur in different word-internal positions and it can be resolved in context. Naturally, while some characters have only one POC tag, most characters will receive multiple POC tags, in the same way that words can have multiple POS tags. Table 2 shows how all four of the POC tags can be assigned to the character \u4ea7 (\"produce\"): Table2. A character can receive as many as four tags The maximum entropy tagger The maximum entropy model used in POS-tagging is described in detail in [Ratnaparkhi, 1996] and the POC tagger here uses the same probability model. The probability model is defined over \u00d7 HT , where H is the set of possible contexts or \"histories\" and T is the set of possible tags. The model's joint probability of a history h and a tag t is defined as () 1 ()\u03c0\u03bc\u03b1 ht j k f j j pht , = ,= \u220f (1) where \u03c0 is a normalization constant, The success of the model in tagging depends to a large extent on the selection of suitable features. Given () ht , , a feature must encode information that helps to predict t . The features we used in this experiment are instantiations of the feature templates in (5). Feature \u2026 C -3 C -2 C -1 C 0 C 1 C 2 C 3 \u2026 \u2026 T -3 T -2 T -1 T 0 T 1 T 2 T 3 \u2026 Figure 1 Features used in the maximum entropy segmenter In general, given () ht , , these features are in the form of co-occurrence relations between t and some type of context h , or between t and some properties of the current character. For The feature templates in (5) encode three types of contexts. First, features based on the current and surrounding characters (5b, 5c, 5d, 5e) are extracted. Given a character in a sentence, this model will look at the current character, the previous two and next two characters. For example, if the current character is \u4eec (plural marker), it is very likely that it will occur as a suffix in a word, thus receiving the tag RR. On the other hand, for other characters, they might be equally likely to appear on the left, on the right or in the middle. In those cases where it occurs within a word depends on its surrounding characters. For example, if the current character is \u7231 (\"love\"), it should perhaps be tagged LL if the next character is \u62a4 (\"protect\"). However, if the previous character is \u70ed (\"warm\"), then it should perhaps be tagged RR. Second, features based on the previous tags (5f) are extracted. Information like this is useful in predicting the POC tag for the current character just as the POS tags are useful in predicting the POS tag of the current word in a similar context. For example, if the previous character is tagged LR or RR, this means that the current character must start a word, and should be tagged either LL or LR. Finally, a default feature (5a) is used to capture cases where no other features are available. When the training is complete, the features and their corresponding parameters will be used to calculate the probability of the tag sequence of a sentence when the tagger tags unseen data. Given a sequence of characters 1 {} n cc ,..., , the tagger searches for the tag sequence 1 {} n tt ,..., with the highest probability 11 1 ()() n nnii i PttCCPth = ,...|,...=| \u220f (3) and the conditional probability of for each POC tag t given its history h is calculated as () () () Pht tT pht Pth \u2032 , \u2032\u2208 , |= \u2211 (4) Experiments We conducted two experiments. In the first experiment, we used the maximum matching algorithm to establish a baseline, as comparing results across different data sources can be difficult. This experiment is also designed to test the performance of the maximum matching algorithm with or without unknown words. In the second experiment, we applied the maximum entropy model to the problem of Chinese word segmentation. The data we used is from the Penn Chinese Treebank [Xia et al., 2000; Xue, Chiou, and Palmer, 2002] Experiment One In this experiment, we conducted two sub-experiments. In the first sub-experiment, we used a forward maximum matching algorithm to segment the testing data with a dictionary compiled from the training data. There are 497 (or 3.95%) new words (words that are not found in the training data) in the testing data. In the second sub-experiment, the same algorithm was used to segment the same testing data with a dictionary compiled from BOTH the training data and the testing data. In other words, there is no new word in the testing data. Experiment Two In the second experiment, a maximum entropy model was trained on a POC-tagged corpus derived from the training data described above. In the testing phase, the sentences in the testing data were first split into sequences of hanzi and then tagged with this maximum entropy tagger. The tagged testing data is then converted back into word segments for evaluation. Note that converting a POC-tagged corpus into a segmented corpus is not entirely straightforward when inconsistent tagging occurs. For example, it is possible that the tagger assigns a LL-LR sequence to two adjacent characters. We made no effort to ensure the best possible conversion. The character that is POC-tagged LL is invariably combined with the following character, no matter how the latter is tagged. The example in (6) illustrates this process. (a) Tagged output \u5728/LR \u521a/LL \u521a/RR \u8fc7/LL \u53bb/RR \u7684/LR \u4e00/LL \u4e5d/MM \u4e5d/MM \u4e03/MM \u5e74 /RR \uff0c /LR \u4e2d/LL \u56fd/RR \u8fdb/LL \u51fa/MM \u53e3/RR \u8d38/LL \u6613/RR \u4e2d/LR \uff0c /LR \u56fd /LL \u6709 /RR \u4f01/LL \u4e1a/RR \u4e0e/LR \u5916/LL \u5546/RR \u6295/LL \u8d44 /RR \u4f01/LL \u4e1a /RR \u9f50/LL \u5934/RR \u5e76/LL \u8fdb/RR \uff0c/LR \u56fd/LL \u6709/RR \u4f01/LL \u4e1a/RR \u7ee7/LL \u7eed/RR \u5c45/LL \u4e8e/RR \u4e3b/LL \u5bfc/RR \u5730/LL \u4f4d./RR \uff0c/LR \u5916/LL \u5546/RR \u6295/LL \u8d44/RR \u4f01 /LL \u4e1a /RR \u4ecd /LL \u7136 /RR \u53d1 /LL \u6325 /RR \u91cd /LL \u8981 /RR \u7684/LR \u4f5c /LL \u7528 /RR \u3002/LR (b) Segmented output \u5728\uff5c\u521a\u521a\uff5c\u8fc7\u53bb\uff5c\u7684\uff5c\u4e00\u4e5d\u4e5d\u4e03\u5e74\uff5c\uff0c\uff5c\u4e2d\u56fd\uff5c\u8fdb\u51fa\u53e3\uff5c\u8d38\u6613\uff5c\u4e2d\uff5c\uff0c\uff5c\u56fd \u6709\uff5c\u4f01\u4e1a\uff5c\u4e0e\uff5c\u5916\u5546\uff5c\u6295\u8d44\uff5c\u4f01\u4e1a\uff5c\u9f50\u5934\uff5c\u5e76\u8fdb\uff5c\uff0c\uff5c\u56fd\u6709\uff5c\u4f01\u4e1a\uff5c\u7ee7\u7eed\uff5c\u5c45 \u4e8e\uff5c\u4e3b\u5bfc\uff5c\u5730\u4f4d\uff5c\uff0c\uff5c\u5916\u5546\uff5c\u6295\u8d44\uff5c\u4f01\u4e1a\uff5c\u4ecd\u7136\uff5c\u53d1\u6325\uff5c\u91cd\u8981\uff5c\u7684\uff5c\u4f5c\u7528\uff5c\u3002 (c) Gold Standard \u5728\uff5c\u521a\u521a\uff5c\u8fc7\u53bb\uff5c\u7684\uff5c\u4e00\u4e5d\u4e5d\u4e03\u5e74\uff5c\uff0c\uff5c\u4e2d\u56fd\uff5c\u8fdb\u51fa\u53e3\uff5c\u8d38\u6613\uff5c\u4e2d\uff5c\uff0c\uff5c\u56fd \u6709\uff5c\u4f01\u4e1a\uff5c\u4e0e\uff5c\u5916\u5546\uff5c\u6295\u8d44\uff5c\u4f01\u4e1a\uff5c\u9f50\u5934\u5e76\u8fdb\uff5c\uff0c\uff5c\u56fd\u6709\uff5c\u4f01\u4e1a\uff5c\u7ee7\u7eed\uff5c\u5c45 \u4e8e\uff5c\u4e3b\u5bfc\uff5c\u5730\u4f4d\uff5c\uff0c\uff5c\u5916\u5546\uff5c\u6295\u8d44\uff5c\u4f01\u4e1a\uff5c\u4ecd\u7136\uff5c\u53d1\u6325\uff5c\u91cd\u8981\uff5c\u7684\uff5c\u4f5c\u7528\uff5c\u3002 Results In evaluating our model, we calculated both the tagging accuracy and segmentation accuracy. The calculation of the tagging accuracy is straightforward. It is simply the total number of correctly POC-tagged characters divided by the total number of characters. In evaluating segmentation accuracy, we used three measures: precision, recall and balanced F-score. Precision p is defined as the number of correctly segmented words divided by the total number of words in the automatically segmented corpus. Recall r is defined as the number of correctly segmented words divided by the total number of words in the gold standard, which is the manually annotated corpus. F-score f is defined as follows: 2 pr f pr \u00d7\u00d7 = + (5) The results of the three experiments are tabulated in Table 3 : The results from Experiment One show that the accuracy of the maximum matching algorithm degrades sharply when there are new words in the testing data, even when there is only a small proportion of them. Assuming an ideal scenario where there is no new word in the testing data, the maximum matching algorithm achieves an F-score of 95.15%. However, when there are new words (words not found the training data), the accuracy drops to only 89.77% in F-score. In contrast, the maximum entropy tagger achieves an accuracy of 94.98% by the balanced F-score even when there are new words in testing data. This result is only slightly lower than the 95.15% that the maximum matching algorithm achieves when there is no new word. An analysis of the new words (words not in the training data) is more revealing. Of the 510 words that are found in the testing data but not in the training data, 7 or 1.37% of them are correctly segmented by the maximum matching algorithm (Experiment 1a), while the maximum entropy model correctly segmented 70.20%, or 358 of them. The 7 words the maximum matching algorithm segmented correctly happen to be single-character words. This is expected because the maximum matching algorithm stops when it can no longer extend a string of hanzi based on a dictionary. In contrast, for the maximum entropy model, unknown words are predicted based on the distribution of their components. Even though the new words are not found in the training data, their components can still be found and words can be proposed based on the distribution of their components, a property that is typical of back-off statistical models. The fact the recall of the unknown words is well below the overall recall suggests that statistics of the unknown words are harder to collect than the known words. The results of this segmenter against previous studies are harder to assess. One reason why this is difficult is that the accuracy representing segmenter performance can only be meaningfully interpreted if there is a widely accepted definition of wordhood in Chinese. It has been well-documented in the linguistics literature [Dai, 1992; Packard, 2000; Xue, 2001] that phonological, syntactic and semantic criteria do not converge to allow a single notion of \"word\" in Chinese. In practice, noting the difficulty in defining wordhood, researchers in automatic word segmentation of Chinese text generally adopt their own working definitions of what a word is, or simply rely on native speakers' subjective judgments. The problem with native speakers' subjective judgements is that native speakers generally show great inconsistency in their judgments of wordhood, as should perhaps be expected given the difficulty of defining what a word is in Chinese. For example, Wu and Fung [1994] introduced an evaluation method which they call nk -blind. To deal with the inconsistency they proposed a scheme in which n human judges are asked to segment a text independently. They then compare the segmentation of an automatic segmenter with those of the human judges. For a given \"word\" produced by the automatic segmenter, there may be k human judges agreeing that this is a word, where k is between zero and n . For eight human judges, the precision of the segmentation with which all the human judges agree is only 30%, while the precision of the segmentation that at least one human judge agrees with is 90%. [Sproat et al., 1996] adopted a different evaluation method since their work on Chinese word segmentation is tailored for use in a text-to-speech system. Their subjects, who have no training in linguistics, are instructed to segment sentences by marking all the places they might be plausibly pause if they were reading the text aloud. They tested inter-subject consistency on six native speakers of Mandarin Chinese and the average inter-subject consistency is 76%. These experiments attest the difficulty of evaluating the performance of different segmenters. The situation is improving with the emergence of published segmentation standards and corpora manually segmented in keeping with these standards [Xia, 2000; Yu et al.,1998; CKIP, 1995] . Still, the corpora can vary by size, the complexity of the sentences in the corpora, so on and so forth. Unless the segmenters are tested with a single standard corpus, the performance of different segmenters are still hard to gauge. Still some preliminary observations can be made in this regard. Our accuracy is much higher that those reported in [Hockenmaier and Brew, 1998 ] and [Xue, 2001] , who used error-driven transformation-based learning to learn a set of n-gram rules to do a series of merge and split operations on data from Xinhua news, the same data source as that of ours. The results they reported are 87.9% (trained on 100,000 words) and 90.2% (trained on 80,000 words) respectively, measured by the balanced F-score. Using a statistical model called prediction by partial matching (PPM), Teahan et al. [2000] reported a significantly better result. The model was trained on a million words from Guo Jin's Mandarin Chinese PH corpus and tested on five 500-segment files. The reported F-scores are in a range between 89.4% and 98.6%, averaging 94.4%. Since the data is also from Xinhua newswire, some comparison can be made between our results and this model. With less training data, our results using the maximum entropy model are slightly higher (by 0.48%). Tested on the same test data as ours, the Microsoft system [Wu, 2003] achieved a higher accuracy, achieving precision and recall rates of 95.98% and 96.36% respectively, using a dictionary of around 89K words, compared with around 19K unique words in our training data. We believe our approach can achieve higher accuracy with more training data. Personal names It has long been noted that personal names often pose a serious problem for automatic word segmentation, presumably because new names are constantly made up and it is impossible to list them exhaustively in pre-compiled dictionaries that dictionary-based approaches heavily rely on. It is expected that these names should not generally be a problem for the present character-based approach in the same way because new words are not distinct problems for this approach. Among the 137 personal names (122 unique names, both Chinese names and foreign name transliterations) found in the testing data, 119 of them are segmented correctly, with a recall of 86.86%. The 18 wrongly segmented names are given in Table 4 . In general, longer names, especially foreign names, are more likely to cause problems for this model. Contribution of Features In an effort to assess the effectiveness of the different types of features, we retrained our system by taking out each group of features in (5). The most effective features are the ones which, when not used, result in the most loss in accuracy. Table 5 shows that there is loss of accuracy when any of the six groups of features are not used. This means that all of the features in ( 5 Effects of Tag Sets The choice of our POC tag set is based on linguistic intuitions. The use of four tags is linguistically intuitive in that LL tags morphemes that are prefixes or stems in the absence of prefixes, RR tags morphemes that are suffixes or stems in the absence of suffixes, MM tags stems with affixes and LR tags stems without affixes. The results in Table 6 show that our linguistically intuitive tag set is also the most effective. The use of three tags (LL for beginning of a word, RR for continuation of a word and LR for word by itself) that has been proven to be the most useful for baseNP chunking [Ramshaw and Marcus, 1995] results in comparable performance in segmentation accuracy. The use of two tags (LL for beginning of a word and RR otherwise) results in substantial loss in segmentation accuracy while gaining in tagging accuracy. This is a somewhat surprising result since there is no inconsistent tagging with this tag set and thus no loss in accuracy in the post-tagging conversion process. Conclusions and Future Work The preliminary results show that the maximum entropy model can be effectively applied to Chinese word segmentation. It is more robust than the maximum matching algorithm in the sense that it can handle unknown words much more effectively. The results also show that our approach is competitive against other machine-learning models. Much work needs to be done to evaluate this approach more thoroughly. For example, more experiments need to be performed on data sources other than the newswire type and on standards other than the Penn Chinese Treebank. In addition, we plan to explore ways to further improve this segmenter. For instance, we expect that the segmenter accuracy can still be improved as more training data become available. Refined pre-processing or post-processing steps could also help improve segmentation accuracy. For example, instead of tagging hanzi directly it might be possible to tag morphemes, which may or may not be composed of just one hanzi. There might also be better ways to convert a tagged sequence into a word sequence than the simple approach we adopted. Acknowledgement I would like to thank Susan Converse for her detailed comments on a previous version of this work. The comments of the three anonymous reviewers also led to significant improvement of this paper. I would also like to thank Andi Wu for for his help in evaluating the results reported here and Richard Sproat, the guest editor of this special issue, for help with references. All inadequacies that still exist in this work are my own, of course. This research was funded by DARPA N66001-00-1-8915.",
    "abstract": "In this paper we report results of a supervised machine-learning approach to Chinese word segmentation. A maximum entropy tagger is trained on manually annotated data to automatically assign to Chinese characters, or hanzi, tags that indicate the position of a hanzi within a word. The tagged output is then converted into segmented text for evaluation. Preliminary results show that this approach is competitive against other supervised machine-learning segmenters reported in previous studies, achieving precision and recall rates of 95.01% and 94.94% respectively, trained on a 237K-word training set.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Mandarin",
        "Chinese"
    ],
    "numcitedby": "226",
    "year": "2003",
    "month": "February",
    "title": "{C}hinese Word Segmentation as Character Tagging"
}