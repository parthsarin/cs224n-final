{
    "article": "In this paper we describe the ADAPT Centre's (Team ID: adapt-dcu) submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) translation task. We only considered translating from Japanese to English for this task and secured the third position in the competition as per the rankings of the MT systems based on the human evaluation scores. The machine translation (MT) systems that we built for this task are state-of-the-art Transformer models. In order to improve the translation quality of our neural MT (NMT) systems, we made use of both in-domain and out-ofdomain data for training. We applied various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we carried out for this task and reports the MT systems' performance on the evaluation test set. Introduction We participated in the WAT 2020 1 (Nakazawa et al., 2020) document-level BSD translation task and only submitted systems that translate from Japanese-to-English (Ja-to-En). Our MT systems are Transformer models (Vaswani et al., 2017) which were trained using the Marian-NMT toolkit. 2  In this work, we applied different domain adaptation techniques, such as using synthetic data from source-and target-side monolingual data through the use of forward-and back-translation (Sennrich et al., 2016; Chinea-R\u00edos et al., 2017; Poncelas et al., 2018) and out-of-domain parallel data to train our models. As far as fine-tuning the model parameters is concerned, we experimented with conventional fine-tuning which consists of fine-tuning on in-domain data only, mixed fine-tuning and lastly document-level fine-tuning. 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2020/index.html 2 https://github.com/marian-nmt/marian This paper is organised as follows. Firstly, the data used and processing steps are described in Section 2. Next, the methods for training the baseline MT systems are described in Section 3, and the results obtained from our MT systems are presented in Section 4. Finally, Section 5 concludes our work with the avenues for the future work. Data and Preprocessing This section outlines the corpora used and the steps that were taken to preprocess the data for training. Corpora Used To train the baseline models a mixture of three corpora was used, where one corpus contains indomain sentences and the other two corpora contain out-of-domain sentences. The in-domain BSD corpus (Rikters et al., 2019 ) consists of a training set of 20,000 sentences, a development set of 2,051 sentences and a test set of 2,120 sentences. The out-of-domain data that was added to the BSD training set includes the JESC 3 (Pryzant et al., 2018) corpus consisting of 2.8 million sentences and the OpenSubtitles 4 (Lison and Tiedemann, 2016) corpus consisting of 2.2 million sentences. Furthermore, monolingual data from the target-side (En) of the JW300 5 corpus (Agi\u0107 and Vuli\u0107, 2019) was used to create synthetic data with the use of backtranslation. Finally, source-language monolingual data with n-grams similar to that of the documents in the test set was mined from the Common Crawl Corpus 6 to be used as a source-side original synthetic corpus (SOSC) for fine-tuning the NMT model parameters. Preprocessing The source-side sentences (Ja) were segmented using MeCab 7 and the target-side sentences (En) were tokenized and lower-cased using the standard scripts from the Moses toolkit (Koehn et al., 2007) . For both languages Subword-NMT 8 was used to apply byte pair encoding (BPE). We experimented with two different vocabulary sizes: 6,000 and 32,000. Details around these vocabulary sizes will be discussed in more detail in Section 3. The Baseline MT Systems We started off by training three baseline models (B1, B2 and B3) and the best baseline model was used to experiment with different data augmentation methods when fine-tuning model parameters. These methods will be described in detail in Section 4. The first baseline model (B1) was trained on the BSD, JESC and OpenSubtitles corpora combined. As mentioned earlier, we used the MarianNMT toolkit to train our Transformer models. The setup described in Sennrich et al. (2017) was used as is for B1, more specifically the mini-batch size for validation was 64 and the learning rate 0.0003. The changes to this setup for the other two baseline setups (B2 and B3) are discussed below. The setup differs since BPE with a vocabulary of 32,000 was applied to the training set of B1 and a vocabulary size of 6,000 was used for the other two baseline models (B2 and B3). We obtained the BLEU (Papineni et al., 2002) scores to evaluate these baseline MT systems on the evaluation test set and the scores are presented in Table 1 . The second baseline model (B2) was trained on the same training set as B1. The third baseline model's (B3) training set consisted of the targetside original synthetic corpus added to the training set of B1 (cf. Table 1 ). The setup for B1 and B2 was changed by setting the mini-batch size for validation to 32 and the learning rate to 0.0005. Improving the Baseline MT Systems The BLEU score of each of the baseline models described in Section 3 is shown in Table 1 and it is clear that B2 is the best-performing MT system out of the three baseline models. Therefore, we decided to conduct the remainder of our experiments on this baseline model (B2) alone. In order to improve upon the scores achieved by the baseline model B2, we fine-tuned the parameters of the model. This was done by restarting training on model B2, after initial training has ended, with a newly selected corpus. We implemented four different scenarios for fine-tuning the parameters. Scenario 1: The first scenario is the most basic, where we simply performed conventional finetuning of the model parameters on in-domain data only, namely the BSD training set. Scenario 2: In the second scenario we implemented mixed fine-tuning of model parameters, where fine-tuning is conducted on the training data that consists of both in-domain data and out-ofdomain data as described in Chu et al. (2017) . The in-domain data was augmented by oversampling the BSD training set 50 times and the out-ofdomain data is a mixture of JESC and OpenSubtitles. Scenario 3: As for the third scenario, source-side monolingual sentences were mined that are similar in styles to the BSD test set sentences. We followed Nayak et al. (2020) and Parthasarathy et al. (2020) in order to mine those sentences from large monolingual data that could be beneficial for finetuning the original NMT models. We identified terms in the test set to be translated. For this, we followed the monolingual terminology extraction methods described in Haque et al. (2014 Haque et al. ( , 2018)) , which used a large corpus that is generic in nature as a reference corpus. In our setup, we used the source-side of the authentic training bitexts on which our NMT system (B2) was trained as the reference corpus. The intuition is to extract those terminological expressions from the test set that do not occur or rarely occur in the training data and are more indicative of the test corpus. Given the list of extracted terms, we mined sentences from the large monolingual corpus 9 mentioned in Section 2. The Japanese source sentences (a total of 153,402 sentences) that have been mined were translated into English using B2 to create synthetic data (i.e. source-side original synthetic corpus (SOSC)) to be used for fine-tuning our baseline model. Thus, in Scenario 3, the model parameters were fine-tuned on the out-of-domain data only. Scenario 4: Finally, in Scenario 4, we once again tested the mixed fine-tuning strategy. In this case, the in-domain data consists of the BSD training set and the out-of-domain data consists of SOSC. The results obtained from the different scenarios are shown in Table 2 , where the mixed fine-tuning of Scenario 2 combined with the data augmentation technique provides us the best BLEU score on the test set. The MT system of this setup produces a 1.37 BLEU points corresponding to 7.9% relative gain over the baseline. The gain is statistically significant (Koehn, 2004) . As for Scenario 1, fine-tuning model parameters on in-domain-data only does not work well and the corresponding MT system performs poorly in comparison to the performance of other MT systems (cf. Table 2 ). We conjecture that this happened due to the fact that only a small-sized in-domain training corpus was used for fine-tuning. The WAT 2020 shared task organisers reported the evaluation results in terms of the BLEU and RIBES (Isozaki et al., 2010) metrics. As for RIBES, Scenario 1 once again is found to be less effective, and the corresponding MT system produces a low score on the test set. However, the difference between the lowest and highest RIBES scores is much smaller than that of the BLEU scores. Another notable difference is that Scenario 4 seems to be most effective since the MT system of this setup provides us the highest RIBES score on the test set. This is a contrasting outcome to the one with the BLEU metric, where Scenario 2 provided the highest BLEU score on the test set. We submitted translations of the MT system of the Scenario 2 setup for the human evaluation task conducted by the shared task organisers since it produced the highest BLEU score on the evaluation test set. As can be seen from the last column of Table 2 , we received a score of 3.930 as far as the results of the human evaluation task is concerned. We secured the third position in the competition for the Japanese-to-English BSD translation task as per the rankings of the MT systems based on the human evaluation scores. Conclusion In this paper, we described our MT systems that were submitted to the WAT 2020 document-level Business Scene Dialogue translation shared task. We presented the WAT 2020 official results that we obtained by submitting the translations of our MT systems. We showed that, in the case of limited in-domain training data, both in-domain and out-of-domain data is useful for fine-tuning model parameters, which essentially provides the best results in this translation task. Furthermore, making use of synthetic parallel data in training also greatly increased the performance of our MT systems. In future, we aim to exploit document-level syntactic context in the fine-tuning step. We also aim to explore increasing training batch size at the finetuning step as this may capture wider contexts during training. programme under the Marie Sk\u0142odowska-Curie grant agreement No. 713567, and the publication has emanated from research supported in part by a research grant from SFI under Grant Number 13/RC/2077 and 18/CRT/6224 . Acknowledgments The ADAPT Centre for Digital Content Technology is funded under the Science Foundation Ireland (SFI) Research Centres Programme (Grant No. 13/RC/2106) and is co-funded under the European Regional Development Fund. This project has partially received funding from the European Union's Horizon 2020 research and innovation",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 1.9361263126072004e-07
    },
    "reasoning": "Reasoning: The article explicitly mentions funding from the European Union's Horizon 2020 research and innovation programme and the Science Foundation Ireland (SFI) Research Centres Programme, which are research agencies. It also mentions co-funding under the European Regional Development Fund, which is a form of regional support but falls under the broader category of research agency funding due to its governmental nature. There is no mention of defense, corporate, or foundation funding.",
    "abstract": "In this paper we describe the ADAPT Centre's (Team ID: adapt-dcu) submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) translation task. We only considered translating from Japanese to English for this task and secured the third position in the competition as per the rankings of the MT systems based on the human evaluation scores. The machine translation (MT) systems that we built for this task are state-of-the-art Transformer models. In order to improve the translation quality of our neural MT (NMT) systems, we made use of both in-domain and out-ofdomain data for training. We applied various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we carried out for this task and reports the MT systems' performance on the evaluation test set.",
    "countries": [
        "Ireland"
    ],
    "languages": [
        "Japanese",
        "English"
    ],
    "numcitedby": 2,
    "year": 2020,
    "month": "December",
    "title": "The {ADAPT} Centre{'}s Neural {MT} Systems for the {WAT} 2020 Document-Level Translation Task",
    "values": {
        "performance": "As for Scenario 1, fine-tuning model parameters on in-domain-data only does not work well and the corresponding MT system performs poorly in comparison to the performance of other MT systems (cf. Table 2 ). The MT system of this setup produces a 1.37 BLEU points corresponding to 7.9% relative gain over the baseline. The gain is statistically significant (Koehn, 2004) . We submitted translations of the MT system of the Scenario 2 setup for the human evaluation task conducted by the shared task organisers since it produced the highest BLEU score on the evaluation test set. As can be seen from the last column of Table 2 , we received a score of 3.930 as far as the results of the human evaluation task is concerned. We secured the third position in the competition for the Japanese-to-English BSD translation task as per the rankings of the MT systems based on the human evaluation scores."
    }
}