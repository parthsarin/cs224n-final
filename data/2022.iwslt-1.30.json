{
    "article": "This paper describes the University of Maryland's submission to the Special Task on Formality Control for Spoken Language Translation at IWSLT, which evaluates translation from English into 6 languages with diverse grammatical formality markers. We investigate to what extent this problem can be addressed with a single multilingual model, simultaneously controlling its output for target language and formality. Results show that this strategy can approach the translation quality and formality control achieved by dedicated translation models. However, the nature of the underlying pre-trained language model and of the finetuning samples greatly impact results. Introduction While machine translation (MT) research has primarily focused on preserving meaning across languages, linguists and lay-users alike have long known that pragmatic-preserving communication is an important aspect of the problem (Hovy, 1987) . To address one dimension of this, several works have attempted to control aspects of formality in MT (Sennrich et al., 2016; Feely et al., 2019; Schioppa et al., 2021) . Indeed, this research area was formalized as formality-sensitive machine translation (FSMT) by Niu et al. (2017) , where the translation is not only a function of the source segment but also the desired target formality. The lack of gold translation with alternate formality for supervised training and evaluation has lead researchers to rely on manual evaluation and synthetic supervision in past work (Niu and Carpuat, 2020) . Additionally, these works broadly adapt to formal and informal registers as opposed to specifically controlling grammatical formality. The Special Task on Formality Control on Spoken Language Translation provides a new benchmark by contributing high-quality training datasets \u21e4 equal contribution. Source: Do you like 1 Legos? did you 2 ever play with them as a child or even later? German Informal: Magst du 1 Legos? Hast du 2 jemals als Kind mit ihnen gespielt oder sogar sp\u00e4ter? German Formal: M\u00f6gen Sie 1 Legos? Haben Sie 2 jemals als Kind mit ihnen gespielt oder sogar sp\u00e4ter? for diverse languages (N\u0203dejde et al., 2022) . In this task, a source segment in English is paired with two references which are minimally contrastive in grammatical formality, one for each formality level (formal and informal; Table 1 ). Training and test samples are provided in the domains of \"telephony data\" and \"topical chat\" (Gopalakrishnan et al., 2019) for four language pairs (English-{German (DE), Spanish (ES), Hindi (HI), Japanese(JA)}) and a test dataset for two additional \"zero-shot\" (ZS) language pairs (EN-{Russian (RU), Italian (IT)}). Markers of grammatical formality vary across these languages. Personal pronouns and verb agreement mark formality in many Indo-European languages (e.g., DE, HI, IT, RU, ES), while in JA, Korean (KO) and other languages, distinctions can be more extensive (e.g., using morphological markers) to express polite, respectful, and humble speech. In this work, we investigate how to control grammatical formality in MT for many languages with minimal resources. Specifically, we ask whether a single multilingual model can be finetuned to translate in the appropriate formality for any of the task languages. We introduce additive vector interventions to encode style on top of mT5-large (Xue et al., 2021) and mBART-large (Liu et al., 2020) , and investigate the impact of finetuning on varying types of gold and synthetic samples to minimize reliance on manual annotation. Method Given an input sequence x, we design a single model that produces an output y \u21e4 = arg max p(y|x, l, f ; \u2713 LM , \u2713 F ) for any language l and formality level f considered in this task. The bulk of its parameters \u2713 LM are initialized with a pre-trained multilingual language model. A small number of additional parameters \u2713 F enable formality control. All parameters are finetuned for formality-controlled translation. Multilingual Language Models We experiment with two underlying multilingual models: 1) mT5-large 1 -a multilingual variant of T5 that is pre-trained on the Common Crawl-based dataset covering 101 languages and 2) mBARTlarge 2 -a Transformer encoder-decoder which supports multilingual machine translation for 50 languages. While mBART-large is pre-trained with parallel and monolingual supervision, mT5large uses only monolingual dataset during the pre-training phase. Following standard practice, mT5 controls the output language, l, via prompts (\"Translate to German\"), and mBART replaces the beginning of sequence token in the decoder with target language tags (<2xx>). Additive Formality Control While large-scale pre-trained language models have shown tremendous success in multiple monolingual and multilingual controlled generation (Zhang et al., 2022) and style transfer tasks, their application to controlled cross-lingual text generation have been limited. Few-shot style-transfer approaches (Garcia et al., 2021; Riley et al., 2021; Krishna et al., 2022) hold the promise of minimal supervision but perform poorly on low-resource settings and their outputs lack diversity. A popular way of introducing control when generating text with a particular style attribute is tagging, where the desired control tags (e.g., <2formal>) are appended to the source or the target sequence. However, as discussed in Schioppa et al. (2021) , this approach has several limitations, including but not limited to the necessity of including the control tokens in the vocabulary at the start We introduce formality control by adapting the vector-valued interventions proposed by Schioppa et al. (2021) for machine translation (MT), as illustrated in Figure 1 . Formally, given source text x, a formality level f , an encoder E and decoder D, parameterized by \u2713 LM , and a style embedding layer (Emb) parameterized by \u2713 F with the same output dimension as E, we have Z = E(x), V = Emb(f ) y = D(Z + V ) Our formality levels can take values corresponding to formal, informal, and \"neutral\" translations, the last of which is used to generate \"generic\" translations in which there is no difference in the grammatical formality of the translation of the source if translated formally or informally. Unlike Schioppa et al. (2021) who use a zero-vector as their neutral vector, we learn a separate vector. Finetuning Finetuning each multilingual model requires triplets of the form (x, y, f ) for each available target language, l, where x, y and f are the source text, the reference translation and the formality label corresponding to the reference translation respectively. The loss function is then given by: the fact that Y i and Y f align to the same input: L = X (x,y,l,f ) log p(y|x, l, f ; \u2713 LM , \u2713 F ) (1) L = X (x,y f ,l) log p(y f |x, l, f ; \u2713 LM , \u2713 F )+ X (x,y if ,l) log p(y if |x, l, if ; \u2713 LM , \u2713 F ) (2) Synthetic Supervision Since paired contrastive samples are expensive to obtain, we explore the use of synthetic training samples to replace or complement them. This can be done either by automatically annotating naturally occurring bitext for formality, which yields formal and informal samples, and additionally by rewriting the translation to alter its formality to obtain paired contrastive samples. The second approach was used by Niu and Carpuat (2020) to control the register of MT output. However, since this shared task targets grammatical formality and excludes other markers of formal vs. informal registers, we focus on the first approach, thus prioritizing control on the nature of the formality markers in the output over the tighter supervision provided by paired contrastive samples. Given a translation example (x, y), we predict a silver-standard formality label (f ) for the target y using two distinct approaches: \u2022 Rules (ES, DE, IT, RU): We label formality using heuristics based on keyword search, dependency parses, and morphological features. We use spaCy (Honnibal et al., 2020) to (nonexhaustively) retrieve documents that imply a necessarily formal, necessarily informal, or ambiguously formal label. In the case of an ambiguously formal label, we treat it as unambiguously formal (for examples, see B). The complete set of rules for each of the languages are included in the Appendix Table 12 . While simple to implement, these heuristics privilege precision over recall, and risk biasing the synthetic data to the few grammatical aspects they encode. \u2022 Classifiers (HI, JA, IT, RU): We train a binary formal vs. informal classifier on the shared task data (HI, JA) and on the synthetic data (IT, RU). Unlike rules, they can also be transferred in a zero-shot fashion to new languages, and might be less biased toward precision when wellcalibrated. Evaluation Settings Data The shared task provides English source segments paired with two contrastive reference translations, one for each formality level (informal and formal) for four language pairs: EN-{DE, ES, JA, HI} in the supervised setting and two language pairs: EN-{RU, IT} in the zero-shot setting. The sizes and properties of the datasets for the supervised language pairs are listed in Table 2 . Formal texts tend to be longer and more diverse than informal texts for JA compared to other language pairs. The percentage of neutral samples (same formal and informal outputs) vary from 2% (in JA) to 17% (in HI). In the zero-shot setting, 600 test samples are released for the two language pairs (RU, IT). During development, the last 50 paired contrastive examples from each domain are set aside as a validation set for each of the supervised languages (TASK DEV) and use the remaining samples for training (TASK TRAIN). Metrics We evaluate the translation quality of the detruecased detokenized outputs from each systems using BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020) . We use the 13A tokenizer to report SACREBLEU 3 scores for all languages, except Japanese, for which we use the JA-MECAB. We also report the official formality accuracy (ACC.). Given a set of hypotheses H, sets of corresponding phrase-annotated formal references F and informal  references IF , and a function yielding phraselevel contrastive terms from a reference, the taskspecific evaluation metric is defined as follows: match f = X j [ (F j ) 2 H j ^ (IF j ) / 2 H j ] match i = X j [ (F j ) / 2 H j ^ (IF j ) 2 H j ] acc j = match j match f + match i , j 2 {f, i} We note that the task accuracy is a function of the number of matches in the hypotheses, not the number of expected phrases, i.e. match f + match if \uf8ff kHk and discuss the implications in the Appendix (Section C). Experimental Conditions We compare multilingual models, where a single model is used to generate formal and informal translations for all languages with bilingual models trained for each language pair, as detailed below. Multilingual Models Data We consider three finetuning settings: \u2022 Gold finetuned: the model is finetuned only on paired contrastive shared task data (400 to 1000 samples per language pair). \u2022 Synthetic finetuned: the model is finetuned on synthetic silver-labelled triplets (up to 7500 samples per formality level and language as described below). \u2022 Two-pass finetuned: the Synthetic finetuned model is further finetuned on a mixture of gold data and 1000 examples re-sampled from the synthetic training set for unseen languages, which we use to avoid catastrophic forgetting from the silver finetuning stage. Synthetic samples are drawn from multiple data sources (3), sampling at most 7500 examples for each language and formality level. 4 The formality labels are predicted as described in 2.4. Rule-based predictors directly give a label. With classifiers, we assign the formal label if P (formal|y) 0.85 and informal if P (formal|y) \uf8ff 0.15. We additionally compare with the translations generated from the base mBART-large model with no finetuning, referred to as the \"formality agnostic mBART-large\". Training settings We finetune mT5-large and mBART-large with a batch size of 2 and 8 respectively for 10 and 3 epochs respectively. We mask the formality labels used to generate vector-valued interventions with a probability of 0.2. The mT5large model -\"synthetic finetuned mT5-large\"is trained for an additional 5 epochs, with a batch size of 2 on a mixture of task data for seen languages and a subset of the sampled synthetic data for unseen languages. Again, we mask the formality tag with probability 0.2 except in the case of unseen languages where the formality tag is masked with probability 1.0, resulting in the \"two-pass finetuned mT5-large\" model. Formality Classifiers Following Briakou et al. (2021) , we finetune XLM-R on binary classification between formal and informal classes, using the shared task datasets for each of the supervised language pairs (DE, ES, JA, HI) and synthetic datasets for zero-shot language pairs (RU, IT). We treat the \"neutral\" samples as both \"formal\" and \"informal\" when training the classifiers. We use the Adam optimizer, a batch size of 32, and a learning rate of 5 \u21e5 10 3 to finetune for 3 epochs. We report the accuracy of the learned classifiers trained on the TASK TRAIN dataset in Appendix Table 14 . Bilingual Models We consider two types of bilingual models: 1. Formality Agnostic: These models were released by the shared task organizers. Each model is bilingual and trained on a sample of 20 million lines from the Paracrawl Corpus (V9) using the Sockeye NMT toolkit. Models use big transformers with 20 encoder layers, 2 decoder layers, SSRU's in place of decoder selfattention, and large batch training. 2. Formality Specific (Gold): We finetune the models in [1] to generate a formal model and an informal model for each language pair (except the zero-shot language pairs). The effective capacity of the bilingual, formality specific models is 3.14B parameters.Each model has 314M parameters, resulting in (314 \u21e5 2 \u21e5 4) = 2.5B parameters for the four supervised languages (DE, ES, HI, JA) and two pre-trained models (314 \u21e5 2) = 628M parameters for the unseen languages (RU, IT).This is significantly larger than the capacities of our single multilingual models (Additive mT5-large: 1.25B, Additive mBARTlarge: 610M). System Development Results During system development, we explore the impact of different types of training samples and finetuning strategies on translation quality and formality accuracy on TASK DEV. Contrastive Samples We estimate the benefits of fine-tuning on informal vs. formal translations of the same inputs for this task. We train two variants of the gold finetuned mT5-large model using 50% of the paired contrastive samples and 100% of the unpaired triplets (i.e., selecting one formality level per unique source sentence) from the TASK TRAIN samples (Table 4 ). Results show that sample diversity resulting from unpaired triplets leads to better translation quality as measured by BLEU (Average Gain: Formal +3.2. Informal +5.38), without compromising on the formality accuracy. Training with paired samples result in lower TER between formal and informal output compared to unpaired triplets (Table 5 ), suggesting that the outputs generated by the model trained on paired samples are more contrastive. This further motivates our two-pass finetuned model which uses gold contrastive samples on the final stage of finetuning to bias the model towards generating contrastive MT outputs. While TASK DEV is too small to make definitive claims, we report our system development results in Tables 6 and 7 . We observe that finetuning on gold contrastive examples (gold-finetuned) improves the translation quality and accuracy of the translation models (formality-agnostic), highlighting the importance of limited but highquality in-domain supervision on the resulting models. Further, each of the mT5-large models improves in translation quality with additional data and training. While the results are dramatic due to size of both TASK TRAIN and TASK DEV, the trends validate the approach to augment both mBART-large and the mT5-large with additive interventions to control formality. Official Results Submissions We submit five variants of multilingual models (numbered [1-5] in Tables 8-11 ), and compare them to the bilingual models built on top of the organizers' baselines. We first discuss results on the official test split for the supervised setting (Tables 8, 9 ). To better understand the degree of overall control afforded, we also report the average scores of the formal and informal settings in Table 10 before turning to the zero-shot setting in Table 11 . Multilingual Approach The best multilingual models ([1] & [4] ) consistently outperform the bilingual formality-agnostic baselines, improving both translation quality (Worst-case gain in Average BLEU: Formal (+1.67), Informal: (+3.7)) and formality accuracy (Worst-case gain in Average ACC.: Formal (+40.38), Informal: (+31.6)). They approach the quality of formal and informal bilingual systems, but the gap in translation quality and formality accuracy varies across languages. While for DE and ES, there is a large difference in translation quality (approx. 10 BLEU points) between the multilingual models and the bilingual baselines, the multilingual models consistently get higher formality accuracy across language pairs and style directions and also perform comparably with the bilingual models in matching the translation quality for HI and JA. We attribute these differences to the amount of training data used across the language pairs (HI: 0.7M to DE 20M). This is an encouraging result, since the bilingual approach uses a much larger language-specific parameter budget and bitext for training than the all purpose multilingual models, which can benefit from transfer learning across languages. mBART vs. mT5 The gold finetuned mBART-large model achieves the best overall translation quality among the multilingual variants as expected given that mBART-large is pre-trained on parallel text. Its translation quality is higher than that of mT5-large models according to BLEU and COMET for all languages except HI (informal), which could be attributed to the nature and amount of pre-training data used for HI. Its formality accuracy is in the 90's and within 5 percentage points to the highest score for all languages except Japanese (78.2%) in the formal direction. In the informal direction, the gap between mBART-large and the best system on formality accuracy is larger across the board (Average Acc.: +19.3), suggesting that finetuning on gold data cannot completely recover an informal translation despite generally strong performance in formal translations. Finetuning strategies Results show that the combination of synthetic and gold data is crucial to help the mT5-large-based model learn to translate and mark formality appropriately. Finetuning only on the gold data leads to overfitting: the model achieves high formality accuracy scores, but poor translation quality (BLEU < 10). Manual inspection of mT5-large-based system outputs suggests that translations often include tokens in the wrong language (Appendix Table 13 ). Finetuning on synthetic data improves translation qual-ity substantially compared to gold data only (Average gain in BLEU: Formal (+15.8), Informal (+14.6)). Two-pass finetuning improves formality control (Average gain in ACC.: Formal (+5.43), Informal (+27.85)), with additional translation quality improvement across the board over syntheticfinetuned model (Average gain in BLEU: Formal (+10.27), Informal (+11.03); COMET: Formal (+0.247), Informal (+0.252)). While we primarily focused on the impact of synthetic supervision on mT5-large, we believe a similar investigation using mBART-large would yield interesting results and leave this as future work. Performance across languages While the higher resource language pairs (DE, ES) achieve better translation quality (in BLEU and COMET) over the relatively lower resource languages (HI, JA), the formality accuracy is more comparable across the language pairs for the multilingual models  (standard deviation: mT5-large (4), mBART-large (10)). We can observe that the task accuracy is lowest (< 90%) when translating to formal Japanese. By inspection, we observe three broad classes of errors: 1) lexical choice, 2) cross-script matching, 3) ambiguity in politeness levels (Feely et al., 2019) . Lexical choice is invariant in machine translation and is occasionally a valid error in the case of mistranslation, so we focus on the latter two error cases. Japanese has three writing systems and false positives in formality evaluation can occur when surface forms do not match as in the case of s\u221a\u2305 which can also be written as \u2326B\u232bM\u2305 (gloss: 'interesting'). Finally, there are cases in which the system and reference formality mismatch but can both be interpreted as formal (e.g., \" >\u21e1 vs. \"\u270f; gloss: 'work' (polite) vs. 'work' (formal)). Zero-Shot We observe limited zero-shot transfer of grammatical formality to unseen languages (Table 11 ). For both mBART-large and mT5-large models, the EN-IT performance is biased towards informal translations, while EN-RU is biased in the formal direction. In the case of EN-IT, both mBART-large and mT5-large almost always interpret the English second person pronoun as second person plural when translating to formal, exploiting the ambiguity of English on the source side. By contrast, when generating informal translations, pronouns are typically preserved as singular. In comparison, with mT5-large-based translations into RU, we see almost unanimous preference toward the formal, likely due to sampling bias when curating the synthetic training set. We also observe that mBART-large prefers to translate in a formal manner irrespective of desired target. In addition, when mBART-large fails to account for the target formality, it often generates paraphrases of the formal target. These strong preferences might be symptoms of systematic differences in formality across languages in the training data of these models. Finally, the use of silver standard formality labels (\"fully supervised\" setting (FS)) does not improve over the zero-shot approach, with similar observations of mT5-large-based translations as outlined above. We observe that in the case of EN-RU, there is a higher incidence of code-switched translations. This may indicate noise introduced in the automatic labeling process and requires further examination in future work. Related Work Most MT approaches only indirectly capture the style properties of the target text. While efforts have been made to generate better outputs in their pragmatic context via controlling formality (Sennrich et al., 2016; Feely et al., 2019; Niu and Carpuat, 2020; Schioppa et al., 2021) , complexity (Marchisio et al., 2019; Agrawal and Carpuat, 2019) , gender (Rabinovich et al., 2017) , these studies only focus a single language pair. Due to the paucity of style annotated corpora, zero-shot style transfer within and across languages has received a lot of attention. However, adapting pre-trained large-scale language models during inference using only a few examples (Garcia et al., 2021; Riley et al., 2021; Krishna et al., 2022) limits their transfer ability and the diversity of their outputs. While prior works use pre-trained language models like BERT, GPT to intialize \u2713 LM for improving translation quality (Guo et al., 2020; Zhu et al., 2019) , in this work, we focus on adapting sequence-tosequence multilingual models for controlled generation of a desired formality and study style transfer in multilingual supervised and zero-shot settings. Conclusion We present the University of Maryland's submission which examines the performance of a single multilingual model allowing control of both target language and formality. Results show that while multilingual FSMT models lag behind large, bilingual, formality-specific models in terms of MT quality, they show stronger formality control performance across all the language pairs. Furthermore, while synthetic unpaired triplets help mT5-large with FSMT performance and the two-stage finetuning process improves MT quality and contrastive task performance, mBART-large still outperforms this class of models, likely due to its large amount of pre-training supervision. In future work, we suggest a deeper investigation of potentially confounding roles in the study of FSMT, such as the impact of formal register as compared to grammatical formality in training data. We also suggest a thorough analysis of what is transferred in the zero-shot setting. Finally, we recommend an audit of underlying pre-training and finetuning data sources for pre-trained multilingual models, which we believe hinder zero-shot formality transfer for EN-IT and EN-RU in which a single formality is strongly preferred. 'Where are you (form.) from?' B.2 Necessarily informal Appropriate pronouns with accompanying conjugation imply the sentence is grammatically informal. Note that Spanish is pro-drop, which relaxes the requirement on personal pronouns. (3) 'When {were you (form.), was {he, she, it}} born?' C Official Evaluation We report the number of examples labeled as FORMAL, INFORMAL, NEUTRAL, OTHER by the formality scorer for the best multilingual models ( [1, 4]) and the baseline systems for each language pair and formality direction. As described in 3, the accuracy is computed based on realized matches, which excludes examples labelled as NEUTRAL and OTHER. Figure 2 shows that the number of these excluded NEUTRAL samples can range from 15% to 43%. D Example Outputs Source: Wow, that's awesome! Who is your favorite Baseball team? I like my Az team lol German Formal Hypothesis: Wow, das ist toll! Wer ist Ihr Lieblings-Baseballteam? Ich mag meine Az-Team lol. German Formal Reference: Wow, das ist fantastisch! Welches ist Ihr Lieblingsbaseballteam? Ich stehe auf mein AZ-Team lol. German Informal Hypothesis: Wow, das ist toll! Wer ist dein Lieblings\u91ce\u7403team? Ich mag meine Az Team lol. German Informal Reference: Wow, das ist fantastisch! Welches ist dein Lieblingsbaseballteam? Ich stehe auf mein AZ-Team lol. Table 13 : Contrastive outputs from English-German. Note that there is not only variety in lexical choice between references and hypotheses, but also between hypotheses of varying formality (i.e., \u91ce\u7403 is \"baseball\" in Japanese) E Accuracy of Formality Classifiers We report the accuracy of the learned classifiers on the TASK TRAIN dataset in Table 14 . LANGUAGE",
    "abstract": "This paper describes the University of Maryland's submission to the Special Task on Formality Control for Spoken Language Translation at IWSLT, which evaluates translation from English into 6 languages with diverse grammatical formality markers. We investigate to what extent this problem can be addressed with a single multilingual model, simultaneously controlling its output for target language and formality. Results show that this strategy can approach the translation quality and formality control achieved by dedicated translation models. However, the nature of the underlying pre-trained language model and of the finetuning samples greatly impact results.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Japanese",
        "German"
    ],
    "numcitedby": "1",
    "year": "2022",
    "month": "May",
    "title": "Controlling Translation Formality Using Pre-trained Multilingual Language Models"
}