{
    "article": "In this paper we describe our work on the development and enrichment of OFrLex, a freely available, large-coverage morphological and syntactic Old French lexicon. We rely on several heterogeneous language resources to extract structured and exploitable information. The extraction follows a semi-automatic procedure with substantial manual steps to respond to difficulties encountered while aligning lexical entries from distinct language resources. OFrLex aims at improving natural language processing tasks on Old French such as part-of-speech tagging and dependency parsing. We provide quantitative information on OFrLex and discuss its reliability. We also describe and evaluate a semi-automatic, word-embedding-based lexical enrichment process aimed at increasing the accuracy of the resource. Results of this extension technique will be manually validated in the near future, a step that will take advantage of OFrLex's viewing, searching and editing interface, which is already accessible online. Introduction Old French regroups romance languages qualified as O\u00efl languages used in the north of France, south of Belgium and in the Anglo-Norman islands spoken from 8th century to 14th century. They contrast with the Oc languages that come from the south of France. Contrary to Middle French, Old French possesses nominal declination. Both led to contemporary French and possess relatively free word order: verbs are often in second position following a non subject constituent. Moreover, there is no spelling standardisation in Old French, even for proper nouns from the same author. The main textual databases with semi-automatic lemmas and part-of-speech tags (PoS) are the Base de Fran\u00e7ais M\u00e9di\u00e9val (BFM -Medieval French Base) (Guillot et al., 2017) 1 with more than 4 million words and the Nouveau Corpus d'Amsterdam (NCA -New Amsterdam Corpus) (Stein and al., 2008) 2 with more than 3 million words. The main treebanks for Old French are the Syntactic Reference Corpus of Medieval French (SRCMF) (Stein and Pr\u00e9vost, 2013) and the Old French subpart from the Mod\u00e9liser le changement : les voies du fran\u00e7ais (MCVF) corpus (Martineau, 2008) . However, they do not share the same syntactic and POS tag sets, and only SRCMF is on open access with part of it in Universal Dependencies (UD) (McDonald et al., 2013 ) format 3 . In the available resources different kinds of text are gathered. Some vary in style (prose, verse), literary genre (religious, historical, didactical, etc.) , or even in time span (from 10th century to 13th century). Nevertheless, there is no available morphological lexicon, 4 and a fortiori no 1 http://bfm.ens-lyon.fr 2 https://sites.google.com/site/ achimstein/research/resources/nca 3 https://github.com/ UniversalDependencies/UD_Old_French-SRCMF/ 4 A morphological lexicon is a collection of entries of the form syntactic lexicon 5 for Old French. Most of the existing lexicons and dictionaries are either not made for later natural language processing exploitation or only contains minimal morphological (and sometimes syntactic) information. In this paper, we present the morphological and syntactic lexicon for Old French named OFrLex. The creation of this lexicon is semi-automatic with a substantial manual process. Moreover, it forced the resolution of multiple obstacles: to structure and merge multiple resources not necessarily originally structured, to fuse the heterogeneous and not always consistent lexical information, and to create lexicon information such as morphological classes and valency from scratch or from incomplete source information. Hence, OFrLex was made using automatic tools and manual correction or addition of information. This lexicon can be used for improving Old French dependency parsing and PoS tagging. The paper is organised as follows. We start by summarising related work (Section 2.) before presenting the lexicon initial creation process with the different language resources used (Section 3.3.). We then present our methodology to automatically enrich the lexicon (Section 4.2.) and explain our distribution strategy for OFrLex (Section 5.). Finally, we show preliminary results on PoS tagging using the lexicon (Section 6.) before tackling future work and improvements (Section 7.). inflected form, lemma (often a citation form), morphological features (extensional inflectional lexicon) or a collection of entries of the form citation form, inflection class label associated with an inflectional grammar that defined how to generate inflected forms given the citation form and an inflection class label (intensional inflectional lexicon). 5 A syntactic lexicon associates each entry (generally at the lexeme level) with syntactic information, including valency information, control/raising/attribution information, and other types of information describing the syntactic behaviour of the entry. Related Work Recent work used the previously mentioned textual databases for Natural Language Processing (NLP) tasks. PoS tagging has been applied on SRCMF using TreeTagger (Schmid, 1999; Stein, 2014) and Conditional Random Fields (Lafferty et al., 2001; Guibon et al., 2014; Guibon et al., 2015) as a preparation for Old French dependency parsing using Mate (Bohnet, 2010) . On the other hand, lexicon enrichment is a part of the lexicon creation process and has been the subject of several research work, particularly for morphological lexicons. Nicolas et al. (2010) developed an unsupervised morphological rule acquisition tool which was combined with the Alexina framework (Walther and Nicolas, 2011; Nicolas et al., 2012) to enrich morphological lexicons. Another approach used to enrich or create a lexicon is derived from parsebanking (Ros\u00e9n and de Smedt, 2007) which consists of creating a new treebank by applying a well-known and tested grammar or parser on the corpus. Recently, incremental parsebanking showed good results for enriching morphological lexicons with high coverage (Ros\u00e9n et al., 2016) . Valency retrieval through deverbative nouns was also tackled (Fu\u010d\u00edkov\u00e1 et al., 2016) but requires a task oriented gold dataset. Another recent enrichment strategy consists into using word embeddings to obtain clusters of words in order to enrich a lexicon (Sikl\u00f3si, 2016) . Morphological lexicons have been used for several tasks. From constraints derived from lexicon at PoS tagging time (Kim et al., 1999; Haji\u010d, 2000) to additional lexicon-based features combined with standard ones during the training process (Chrupa\u0142a et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012) . To improve these lexicon usages for different tasks such as multilingual PoS tagging supported by a lexicon (Sagot, 2016) , we need to create a computational morphological lexicon for Old French: the OFrLex lexicon. Lexicon Creation Heterogeneous Language Resources The idea behind OFrLex is to derive all information from different sources in order to obtain a morphological Old French lexicon. We try to take into consideration all freely available language resources for this task. FROLEX With this objective in mind we first used FROLEX 6 (Serge Heiden, 2016). The FROLEX lexicon is a combination of information coming from the Base de Fran\u00e7ais Medieval (BFM -Medieval French Base) (Guillot et al., 2017) , the Nouveau Corpus d'Amsterdam (NCA -New Amsterdam Corpus) (Stein and al., 2008) , and the Dictionnaire du Moyen Fran\u00e7ais (ATILF, 2015) (DMF -Middle French Dictionary). These language resources being already merged in one resource, we use the million extensional entries from FROLEX. By extensional entry, we refer to the fact that each one of these entries links to an attested inflected form, and not a lexeme, as visible in Table 1 . Depending on the sources, information for each entry 6 https://github.com/sheiden/ Medieval-French-Language-Toolkit may vary. However, the part-of-speech tags (PoS) are already converted to their CATTEX 7 (Guillot et al., 2010) equivalent with additional gender and number. Even if this resource is convenient as it merge multiple ones, some of the entries have noise (i.e. multiple entries for one form with same incomplete information). Moreover, lemmas do not follow the same convention depending on the source from which they were extracted. The usage of DMF, a dictionary for Middle French, and the fact that lemmas are not represented by all their inflected forms, makes some entries and silence irrelevant for our purpose of obtaining a morphological lexicon for Old French. Wiktionary Wiktionary 8 is a free dictionary which contains 6,500 entries for Old French corresponding to a lexeme and containing formalised descriptions for the inflection classes. The lexeme mengier 9 (i.e. to eat) comes with alternative forms such as mangier, along with the etymology, and english gloss, and inflection information. We use the extraction process described in Sagot (2014) : converting Wiktionary (wiki format) into a structured XML file before using it to extract morphological entries. A morphological entry consists of a citation form, an inflection class identifier, and the list of stems or irregular forms if relevant. Finally, we manually developed a morphological grammar describing the most important inflection classes present in Wiktionary. This morphological grammar use the Alexina parsli format (Sagot and Walther, 2013) . For instance, for verbs we use a model containing 8 stems and 2 exponent levels: an intermediate level for some consonant palatalisation at the end a stem for instance, and higher level for standard terminations in 4 set of rules. The latter follows the Paradigm Function Morphology principle (Stump, 2006) . Altfranz\u00f6sisches W\u00f6rterbuch by Tobler and Lommatzsch (TL) Altfranz\u00f6sisches W\u00f6rterbuch (shorten as TL) is the reference dictionary for Old French, written in German. We used two versions created and distributed by Peter Blumenthal and Achim Stein 10 . \u2022 The first version is made of a list of lemmas manually obtained accompanied by an index of forms from the Godefroy's dictionary. Each information possesses a source information \"tl\" for TL and \"g\" for the Godefroy's dictionary. Simplified entries are visible in Table 2. In this We partially corrected this version manually by focusing on the important parts such as the type of the word. Table 3 presents an example of this manual correction. We then automatically extracted informations by first checking form errors and ignoring the entry if we found any. An example of the extraction result is visible in the bottom part of Table 3 . Lexique de l'ancien fran\u00e7ais by Godefroy We consider the Wikisource version of the Lexique de l'ancien fran\u00e7ais (Old French Lexicon) 11 . Figure 1 shows the online version used. This resource has already been made by applying OCR over the original text and then partially correcting it. It possesses a wide coverage albeit with ghost words and meanings. These ghost words are lexical units wrongly considered as such. Thus, we filtered it using the dedicated ghost words base named Base des mots fant\u00f4mes [du Godefroy] 12 dedicated to identify these entries and to clean them. Moreover, this lexicon covers up to the XV century, which is not Old French anymore but Middle French. This data being structured, we easily extracted citation forms, CAT-TEX PoS tags with additional gender if relevant, a definition, and the link to the corresponding page. Dictionnaire \u00c9lectronique de Chr\u00e9tien de Troyes (DECT). The dictionary by Chr\u00e9tien de Troyes was written during the 12th century and is distributed by the CNRTL 13 in a PDF format (DECT). We converted it in a textual format and extracted entries in a semi-automatic fashion using simple rules. This resource is useful because it links entries with other dictionaries such as TL and Godefroy. Inflected forms are also available for each entry. Merging information To create the OFrLex lexicon we need to aggregate all sources by linking information to unique entries. To do so, we first use the citation forms contained in TL using all DECT entries and their explicit reference to TL entries. Very few errors were found during this process. However, to obtain a large coverage we also use other sources when lemmas linked to multiple matches from Godefroy, TL, and/or DECT. If a lemma differ from one source to another, we create multiple entries and disambiguate them manually based on the definitions obtained from other resources. However if the lemma is the same we fuse their information. Morphology. Morphological features such as gender, number, person, tense and mood are extracted from Wiktionary entries in a semi-automatic way. Indeed, if the citation form is available we retrieve information automatically from sources. If it is not available we add it manually when possible. Form variants are associated based on FROLEX entries. Result. By applying this semi-automatic process, we obtain a morphological lexicon where one entry (i.e. one lexeme) is linked to the different sources. This lexicon also contains information derived from glosses, definitions and variants from the different sources. Note that the Universal Part-of-Speech (UPoS, i.e. the UD morphological category) is also extracted from these sources by converting each different PoS tags into a matching UPoS. Table 4 show quantity information from OFrLex per UPoS. Syntactic Information Addition. We complete this morphological lexicon for Old French with syntactic information. To do so, we follow the Alexina conventions already used for the contemporary French morphological lexicon Lefff (Sagot, 2010) . From Lefff we obtain different types of syntactic information such as redistribution and valency. To retrieve them we make the hypothesis that verbs syntactically similar between Old French and Contemporary French can share information if and only if the former do not possesses any in the lexicon. To be more precise, we use different types of information from Lefff with a hierarchical priority presented in Algorithm 1. In this process, valency is retrieved from multiple sources (Godefroy, TL and DECT) looking for textual markers such as \"I\" (intransitiv in TL), \"trans.\" (transitiv) or \"refl.\" with multiple spelling variants. Finally, Table 5 presents 3 entries from OFrLex: \"afiner\", \"afiner 2 \" and \"effiner\". Those entries are Old French vari-  ants for the contemporary French verb \"affiner\" (i.e. to refine). This example show two entries for the same spelling differentiated by their syntactic information. The third row is the variant of the second row \"afiner 2 \" and serves to decipher the encapsulated syntactic information from the Alexina standard to natural language. In Alexina, information are separated in two parts. First the citation form (in bold), the inflection class (v-er) and the syntactic information are visible as tabulated separated values (TSV). Then, unlimited comments can be added after the hashtag (#) for meta-information such as a link, source of information or variants. These information are XML encoded, using tags as categories and attributes as detailed information (Sagot, 2010) . Algorithm Lexicon Enrichment Once created, the lexicon is not reliable enough to be used as a reference source of information for Old French. We need to enrich it and validate it by Old French or diachrony specialists. However, the manual process is long and tedious especially when it comes to enrich the lexicon by decreasing the silence rate. This is why we first automatically enrich the lexicon before planning on the validation phase. The objective is now to obtain additional information for the lexicon. We derive from Sikl\u00f3si (2016) by being stricter: the user interface (UI) only allows to dispatch information. Plus, we focus on variant candidates for a non spoken language. This is why this additional information is not necessarily expected to be correct. We make the following hypothesis: it is easier and faster to correct or validate some information and errors than trying to find missing values from scratch, especially when dealing with a non living language with relatively few experts and resources. Information from variants Valency Valency information was initially obtained from Lefff for verbs that were considered syntactically similar, or by manual insertion (see Section 3.2.). Nevertheless, not all verbs were covered and other categories were not taken into account in the process. This is why we computed valency information from the variants found for each entry. In fine valency can come from the manual valency inserted, the gloss, the pseudo-gloss, Godefroy or Tobler definitions, or from itself (e.g. if it is directly available in DECT). Lemmas When a lemma is missing, we compute candidate lemmas by analysing variants in a two-way fashion by afiner 1 v-er 100;Lemma;v;<Suj:cln|sn,Obj:(cla|sn)>;upos=VERB,cat=v;%actif # <link src=\"TL\" loc=\"TL:1:189:5+1:1224:51\" entry=\"afiner1\" ms=\"v.\" def=\"[intr.] enden || [mit pers. obj.] jem. den Garaus machen || [trans. mit s\u00e2chl obj.] beenden, zu Ende f\u00fchren\"/> <syntinfosource via=\"tldef\" synttype=\"I\"/> afiner 2 v-er 100;Lemma;v;<Suj:cln|sn,Obj:(cla|sn)>;upos=VERB,cat=v;%actif,%passif # <link src=\"TL\" loc=\"TL:1:189:47+1:1224:52\" entry=\"afiner2\" ms=\"v.\" def=\"[trans.] l\u00e4utern\"/> <syntinfosource via=\"tldef\" synttype=\"T\"/><hasvariant lemma=\"effiner\" id=\"1\" cat=\"VER\"/> effiner v-er 100;Lemma;v;<Suj:cln|sn,Obj:(cla|sn)>;upos=VERB,cat=v;%actif,%passif # <link src=\"TL\" loc=\"TL:1:189:47\" entry=\"afiner2\" ms=\"v.\" def=\"[trans.] l\u00e4utern\"/> <syntinfosource via=\"tldef\" synttype=\"T\"/><variantof lemma=\"afiner\" id=\"2\" cat=\"VER\"/>  populating and spreading the same information across variants and the original lexeme. To deal with multiple lemmas we decided to take the first lemma found for the same category. If there is none we take another one randomly. This information computed from variants and distant variants-the variant of a variant or the variant of a glosswill not be used as reference but as pre-annotations provided to human validators. Generation of pseudo-synonyms Lemmas and valency information were obtained using variants, making the process of dispatching information between entries a relevant strategy. In order to continue using this approach, we aim to generate variant candidates that we name pseudo-synonyms. These pseudo-synonyms are not necessarily morphological variants but can find their similarity in morphology, spelling or sense. Our objective is to propose possible enrichment automatically obtained that the user will be able to validate or refute. To automatically obtain pseudo-synonyms we need to consider the words in context given their morphosyntactic category. This is why we use the BFM corpus in its two available versions: 170 raw texts and 42 CONLL files with verified part-of-speech (PoS) tags. The new annotated BFM corpus which is the current biggest annotated corpus for Old French. We start by using the raw texts as input to train a FastText model (Joulin et al., 2017) using the Gensim implementation 14 . FastText was selected for various reasons. First, we need to take into account morphological information about the words with their inflections. Formal similarity could be used externally but the bag of n-grams used in this model is already dedicated to this. Second, we do possess relatively small data (see Table 6 ) in comparison to other languages with a lot of resources. Thus, we cannot use latest models such as Bert (Devlin et al., 2019) or ELMo (Peters et al., 2018) which require a large amount of data. In fact, we tried both architectures of Word2Vec (Mikolov et al., 2013) with inconclusive results. In our methodology we need to distinguish inflectional forms (f ) with lexemes (L), the former are obtained from raw text while the latter are extracted from OFrLex lexicon. To obtain a lexeme embedding (e(L)), we apply the FastText model trained on the raw text corpus made of inflectional forms (ft(f )). We then make the average of the form embeddings from the lexeme, weighted by the occurrences of each form with the same the PoS tag (p) as the lexeme. The weighted average has recently been demonstrated to be a good approach to obtain meta-embeddings (Coates and Bollegala, 2018) . Here we apply this logic while taking into account occurrences per PoS tag. This is formalised in Equation 1 . e(L) = f \u2208F (L) ft(f ) occ(f ) f \u2208F (L) occ(f, p) (1) We use the set of lexeme embeddings obtained using equation 1 as an input for clustering. We cluster this lexeme embedding space using Spectral Clustering (Ng et al., 2002) . As for the hyper-parameters we set a Gaussian kernel, a gamma of 0.7 and discretisation to assign clusters. Moreover, we do not use eigenvalue decomposition strategy and set the number of targeted clusters as 20, according to the number of PoS tags (n clusters = n distinct PoS tags). These predicted clusters are meant to be used as an additional verification for pseudo-synonyms, but cannot be evaluated as we do not possess gold labels for them. Once we have the lexeme embeddings and their predicted cluster, we can obtain the nearest neighbours for each lexeme. Given a lexeme, we take all the other lexemes with the same PoS tag and that share the same cluster. We then compute the cosine distance between their embeddings (equation 2). This process is formalised in equation 3 where C(L i ) is the cluster for the given lexeme. K(e i , e j ) = e i \u2022 e j ||e i ||.||e j ]] (2) \u2200L i , \u2200L j \u2208 C(L i ), d(L i , L j ) = 1 \u2212 K (e(L i ), e(L j )) (3) Finally, nearest neighbours (nn) are obtained by keeping the lexeme with the minimum cosine distance with the targeted lexeme (d(L i , L j )), visible in Equation 4 . The resulted nearest neighbour is controlled by the clustering, the formal similarity induced by the bag of n-grams, and the lexemes' PoS tag. Because it is not exactly a nearest neighbour nor a variant, we name it pseudo-synonym. nn(L i ) = arg min Lj \u2208C(Li),j =i d(L i , L j ) (4) By applying this methodology we get a pseudo-synonym for 15,041 lexemes out of 54,087. Thus, only 27.81% of lexemes obtain a pseudo-synonym, i.e. a candidate for possible source information retrieval. These pseudo-synonyms are then used as propositions for the validator. It is not an easy task to automatically evaluate these pseudosynonyms but we try to give a glimpse of its quality and usage by taking 10 random pseudo-synonyms found and verifying manually their soundness. For each pseudosynonym we want to know if it is a probable variant or if it already exists as a variant in OFrLex. Table 7 shows this information in the first two columns, followed by the UPoS, the source lexeme from which we want to find pseudo-synonyms, and the pseudo-synonym (v1) followed by nested pseudo-synonym -pseudo-synonym of the preceding pseudo-synonym -. The first pseudo-synonyms are voutroillier and voutroiier for the lexeme voutrillier (i.e. se vautrer -to wallow -). This pseudo-synonym is a correct candidate for information extraction as this variant occurrence is already known from OFrLex and was extracted from TL. The second, menestralsie for the lexeme menestraucie (i.e. act of production of a minstrel) is a new graphical variant which we can verify by manually looking at the definition of menestraudie from DMF which reports them. Plus, they follow the same genre and valency. For the lexeme auberc haubert (coat of chain mail) multiple spelling variants can be found in text as reported in the Littr\u00e9 15 , this pseudo-synonym is a correct candidate for information dispatchment. We apply the same manual checking for each one and showed that 60% are correct in this small non representative subset. However, among the 4 ones that we do not seem to find any proof for, two pseudo-synonyms are probable good candidates considering their form (gaagnier) or the quite similar definitions (mas is related to arable ground where massiz and mass\u00ebiz define something made of the same material). In any case, we use the pseudo-synonyms as a support (proposition) for validators to find or discover new variants and finally enrich the lexicon, and not to directly insert it in OFrLex without validation. 15 https://www.littre.org/definition/ haubert Distribution and Improvements Language resources used to create OFrLex are either free (DMF is free for non commercial usage), from public domain (Godefroy's lexicon and dictionary), or follow a copy left pattern -BFM, SRCMF, FROLEX and Wiktionary follow a CC BY-NC-SA 16 ). Hence, we follow the same licence and distribute OFrLex from its git repository: https://gitlab.inria.fr/ almanach/alexina/ofrlex. The OFrLex repository possesses files for the intentional lexicon (Alexina parsli format) and the extensional lexicon. The latter is the ready-to-use lexicon with all entries and their inflected forms automatically derived from the set of inflectional rules contained in the intensional lexicon. Table 8 shows an example for the \"afiner\" entry in the intensional lexicon and 2 of the many inflected forms derived from it for the extensional lexicon. OFrLex is created semi-automatically and requires thorough validation by Old French specialists. To deal with this issue and to facilitate validation, we developed a user interface dedicated to OFrLex edition and validation. All modifications made in the interface will be automatically integrated in future versions of OFrLex following the architecture shown in Figure 2 . Fully automatic lexicon enrichment such as pseudo-synonyms (see Section 4.2.) and valency or variants information are indicated in the interface as \"propositions.\" They are not fully integrated in the OFrLex source database but are visible for the annotator which can validate them, thereby triggering their integration. This relies on the distinction between 3 information types distinguished by colours: validated information, semi-automatic information, and propositions (with source/confidence indicator). This web interface also serves as a search engine (at the lexeme level) via the public API.  Extensional Lexicon (inflected forms generated from intensional entries) afinent 1 v 100 pred=\"afiner___60674__1<Suj:cln|sn,Obj:(cla|sn)>\",@pers,cat=v,upos=VERB,@pl.3.subj.prs.std afinera 1 v 100 pred=\"afiner___60674__1<Suj:cln|sn,Obj:(cla|sn)>\",@pers,cat=v,upos=VERB,@sg.3.ind.fut.std Preliminary Usage This lexicon can be used for multiple purposes such as diachronic studies, dependency parsing for Old French or PoS tagging. We evaluated OFrLex impact on PoS tagging using the Universal Dependencies (Nivre and al., 2019) version of SRCMF treebank's training set. In order to do so we trained three models using alVWTagger 17 initially developed for the CONLL-2017 shared task (Villemonte de La Clergerie et al., 2017) . Like MElt (Denis and Sagot, 2012) , this PoS tagger can use an external lexicon to infer complementary information from the train set or the test set. Thus, we only use OFrLex to extract the inflected forms with their associated PoS tag as the external lexicon for one model, and no external resource for the second model. test set) supports the need of a dedicated lexicon for NLP tasks. Moreover, we also trained alVWTagger with OFrLex after validation and enrichment using the interface. This led to an improvement in accuracy, both overall and on unknown words. This promising results motivates the need for an incremental validation phase helped by automatic suggestions. Of course, this represents only a small task and cannot be enough to fully take advantage of OFrLex which contains more information than just the PoS tags. However it serve as a preliminary example of its use. Future Work In this paper we presented the OFrLex creation process to obtain a morphological and syntactic lexicon for Old French from heterogeneous resources, along with the methodology used to enrich it, taking into account the fact that it is not a living language. For the moment, syntactic information is mostly limited to verbs; why we plan on extending it to adjectives and nouns in the near future. As shown in Section 5., the user interface is currently used for the lexicon validation phase supported by multiple enrichment propositions, such as those described in Section 4.2.. Even if our preliminary results focused on part-of-speech tagging, we plan to also use parsebanking as a way to improve the lexicon. To do so, a meta grammar for Old French parsing is under development (Regnault et al., 2019) and already uses OFrLex to improve parsing quality and to incrementally fix possible noise or silence present in the lexicon. OFrLex is available for everyone and future validation will yield new versions once the validation phase is done. Acknowledgements This work was partly funded by the French national ANR grant PROFITEROLE (ANR-16-CE38-0010) headed by Sophie Pr\u00e9vost, as well as by the second author's chair in the PRAIRIE institute, 18 funded by the French national agency ANR as part of the \"Investissements d'avenir\" programme under the reference ANR-19-P3IA-0001.",
    "abstract": "In this paper we describe our work on the development and enrichment of OFrLex, a freely available, large-coverage morphological and syntactic Old French lexicon. We rely on several heterogeneous language resources to extract structured and exploitable information. The extraction follows a semi-automatic procedure with substantial manual steps to respond to difficulties encountered while aligning lexical entries from distinct language resources. OFrLex aims at improving natural language processing tasks on Old French such as part-of-speech tagging and dependency parsing. We provide quantitative information on OFrLex and discuss its reliability. We also describe and evaluate a semi-automatic, word-embedding-based lexical enrichment process aimed at increasing the accuracy of the resource. Results of this extension technique will be manually validated in the near future, a step that will take advantage of OFrLex's viewing, searching and editing interface, which is already accessible online.",
    "countries": [
        "France"
    ],
    "languages": [
        "French"
    ],
    "numcitedby": "0",
    "year": "2020",
    "month": "May",
    "title": "{OF}r{L}ex: A Computational Morphological and Syntactic Lexicon for {O}ld {F}rench"
}