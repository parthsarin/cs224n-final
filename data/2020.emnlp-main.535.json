{
    "framing_question": "What information does the paper provide about the researchers\u2019 positionality, if any?\"",
    "response": "The provided paper does not contain any information about the researchers\u2019 positionality.",
    "article": "Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can ever be claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of moviedomain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work 1 . * Corresponding Authors. Work done before Xiaoyu Shen joins Amazon. \u2020 Work done while interning at Wechat. 1 Dataset and model are available at https://github. com/chin-gyou/MovieChats. \u6211\u521a\u770b\u5b8c\u300a\u6d77\u4e0a\u94a2\u7434\u5e08\u300b\uff0c\u611f\u89c9\u2f3c\u7075\u5f88\u9707\u64bc I just finished watching \"The Legend of 1900\" and feel shocked DA: Inform_fact, Inform_feeling Aspects: Name Mov_Tracker: The Legend of 1900 \u771f\u7684\u5417\uff0c\u8fd9\u662f\u90e8\u4ec0\u4e48\u7535\u5f71 Really, what kind of movie is this DA: Request_fact Aspects: Type Mov_Tracker: The Legend of 1900 \u2f00\u90e8\u610f\u2f24\u5229\u62cd\u7684\u82f1\u8bed\u7535\u5f71\uff0c\u8bb2\u2f00\u4f4d\u5929\u624d\u94a2\u7434\u5e08\u2f00\u8f88\u2f26\u90fd\u4f4f\u8f6e\u8239\u4e0a An English film made in Italy, where a talented pianist lived on the ship all his life DA: Inform_fact Aspects: Region, Language, Plot Mov_Tracker: The Legend of 1900 \u8fd9\u4e48\u79bb\u5947\u7684\u6545\u4e8b\uff0c\u4e3a\u4ec0\u4e48\u4ed6\u4e0d\u79bb\u5f00\u90a3\u8258\u8239\u5462 Such a bizarre story, why didn't he leave that boat? DA: Inform_feeling, Request_fact Aspect: Plot Mov_Tracker: The Legend of 1900 \u4ed6\u628a\u8fd9\u2fa5\u5f53\u6210\u4e86\u4ed6\u7684\u7cbe\u795e\u5bb6\u56ed\uff0c\u6240\u4ee5\u8239\u8981\u88ab\u70b8\u6389\u4ed6\u90fd\u4e0d\u80af\u79bb\u5f00 He regarded this as his spiritual home, so when the last ship was about to be blown up, he refused to leave DA: Inform_feeling, Inform_fact Aspects: Plot Mov_Tracker: The Legend of 1900 \u5c31\u50cf\u963f\u2f62\u6b63\u4f20\u2f00\u6837\u3002\u73b0\u4ee3\u793e\u4f1a\u5f88\u5c11\u6709\u2f08\u80fd\u548c\u4e16\u4fd7\u5272\u88c2\u53bb\u8ffd\u6c42\u2f83\u2f30\u7684\u7cbe\u795e\u4e16\u754c Just like in Forrest Gump. Few people in modern society can separate themselves from the world to pursue their own spiritual world DA: Inform_feeling Aspects: Name Mov_Tracker: Forrest Gump Name: The Legend of 1900 | Comment: One of the favorite movies that shocked my soul. Region: Italy | Language: English | Plot: \"a talented pianist lived on the ship all his life\" Plot: \"why didn't he leave that boat\" Plot: \"when the last ship was about to be blown up, he refused to leave\" Comment: What he cannot leave is not the boat, but the spiritual home that nurtured him. Introduction Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot, which further requires transiting among various domains, can be designed (Gao et al., 2019; Su et al., 2020) . Nonetheless, even if constrained in a specific domain, current chatbots are still far from satisfactory. Unlike task-oriented systems that can be relatively well-resolved with handcrafted templates, human conversations feature a complex mixture of QA, chitchat, recommendation, etc. without pre-specified goals or conversational patterns (Dodge et al., 2016; Akasaki and Kaji, 2017; Shen et al., 2018) . Selecting proper domain knowledge to support response generation at all the different situations is challenging (Milward and Beveridge, 2003; Lian et al., 2019; Shen et al., 2019) . In this work, we direct our focus to the movie domain and present a large-scale, crowdsourced Chinese dataset with fine-grained annotations in hope of boosting the study towards a human-like closed-domain chatbot. A variety of dialogue datasets with grounded domain knowledge have already been proposed. However, they are collected either through (1) online forum crawling (Dodge et al., 2016; Ghazvininejad et al., 2018; Liu et al., 2018; Zhou et al., 2018a; Qin et al., 2019) , which are noisy, multi-party, mostly contain only single-exchange QA, or (2) crowdsourced (Zhu et al., 2017; Zhou et al., 2018b; Moon et al., 2019; Wu et al., 2019) , which are small-scale and often created in an overconstrained setting like teacher-student (Moghe et al., 2018) . Even for datasets crowd-sourced in unconstrained scenarios, suggestive domain knowledge is provided for humans before an utterance is provided. This would inevitably prompt humans to utilize these knowledge deliberately, yielding unnatural conversations simply connecting the knowledge (Dinan et al., 2019; Zhou et al., 2020) . We show examples from other datasets in Appendix Table 10 . In comparison, our dataset has the following advantages: 1. Natural: Crowdworkers chat in a free environment without further constraint or prompt in order to mimic the human daily conversations to the largest extent. 2. Large-scale: It covers 270k human dialogues with over 3M utterances, which is at least one order of magnitude larger than all the other crowd-sourced datasets. 3. Annotated: Utterances are labeled with entity information and dialogue acts classified into 15 fine-grained aspects, based on which linked into different types of knowledge. Different from previous crowd-sourced works, our annotation process is conducted posteriori so that it will not interfere with human conversations, e.g., prompt them to overuse suggested knowledge. Built upon our dataset, we propose a simple unified language model approach to push the limits of movie-domain chatbots. The model is first pretrained on 2.2B words collected from various general-domain conversational resources, then finetuned on the movie dataset with additional knowledge and dialogue acts incorporated. We pool all components like intent prediction and knowledge retrieval into a sequence prediction task and solve them with a unified language model architecture. It avoids designing complex systems for individual components separately and all subtasks can be easily trained simultaneously (Hosseini-Asl et al., 2020; Peng et al., 2020) . We show our simple unified approach outperforms strong baselines for each separate subtask. Knowledge retrieval, dialogue acts prediction and general-domain pretrain benefit from each other and altogether bring improvement to the generation quality. In the online interactive test, our best model succeeds at chatting with humans for 11.4 turns without being detected to be a machine, outperforming even commercial chatbots Mitsuku 2 and Microsoft XiaoIce 3 which further rely on complex rules. By analyzing the limitations of our model, we find it especially has difficulty at dealing with in-depth discussions over long turns. Future research can consider employing larger knowledge base or explicit state tracking. In summary, our main contributions are (1) presenting a high-quality, large-scale Chinese conversational corpus with fine-grained annotations in the movie domain to benefit future study, (2) showing that a simple unified neural model trained on the high-quality dataset can approach human performance and even outperform commercial systems replying on complex rules, and (3) studying the shortcomings of current techniques, providing suggestive directions for future research. Dataset Construction The dataset construction consist of (1) crowdsourcing the dialogues, (2) annotating dialog acts and entities and (3) linking utterances into grounded knowledge. We explain these three steps in order and present the dataset statistics in the end. Dialogue Crowd-sourcing We obtain the dialogue dataset through a two-phase Wizard-of-Ozstyle collection (Kelley, 1984; Dahlb\u00e4ck et al., 1993) . In the first phase, we run small-scale pilot studies and examine the quality of collected conversations. Based on the examination, we created tutorials and qualification tests. They are used to train and qualify crowd-workers for the second phase. During this second phase, we consistently monitor the collected dialogue datasets and perform periodic quality check on samples from every individual work pairs. If more than 5% from one pair are considered invalid, their collections will be removed. Before a conversation started, two workers are paired and a movie is chosen agreed by both 4 . We constrain at least one of them to have watched the movie to make sure the conversation is contentful 5 . The annotators are especially instructed to (1) behave naturally as in daily life, (2) avoid dirty words and (3) talk differently in each conversation. Duplicate conversations will be removed if more than 70% of their contents are overlapped. To encourage diverse movies, we further set an upper limit to forbid one movie from being talked about for more than 100 times. The whole collecting process lasts two months. In the end, 245 participants are involved with 66,424 movies being talked about in total. Dialogue Act and Entity Annotation Following prior work, we base our annotation schema on the ISO 24617-2 standard (Bunt et al., 2010 (Bunt et al., , 2012)) . Table 1 shows our annotation schema, counts, descriptions, and brief examples. The dialogue acts (DAs) are organized in a hierarchical structure. The first layer makes distinctions on three concepts: objective facts, recommendations and subjective feelings. Each concept can either be either requested or informed during the conversation. We further define an \"Other\" class to include actions that do not belong to any of the three concepts, like some general non-contentful greetings or echos. The second layer includes 15 finer-grained aspects covering most popular topics being discussed. Every first-layer DA (except Other) will be further group it into one of these 15 aspects, e.g., the de-Figure 1 : An example of our dataset. The annotations include dialogue act, aspects and movie tracker and grounded knowledge. tailed DA of the first example in Table 1 will be request fact director. If one utterance contains multiple dialogue acts, we order the dialogue acts based on their turn of appearance in the utterance. As for the named entity recognition, we labeled 5 kinds of entities: movie names, director, actor, type and role (first 5 aspects). To speed up the annotation process, we first define a set of handcrafted regular expressions, which covers most frequent patterns at each class, to train a DA and NER classifier . The annotators are instructed to post-correct the auto-labeled dialogues instead of doing everything from scratch. The classifiers are trained with online learning (Sahoo et al., 2018) to keep improving the accuracy and lower down the frequency of post-correction in consequence. As we observe, this semi-automated way significantly speeds up the labeling process. All the dataset is finished labeling within three weeks with 188 annotators involved. Knowledge Linkage We extract fact knowledge from the structured table in Douban Movie 6 , a popular Chinese platform for movies. The knowledge is organized in the form of key-value pairs, where the key corresponds to the 15 aspects defined by us. Some aspects, like lines or music, are not directly available from the structured table. We extract these missing information from other sources and combine it into our knowledge base. For utterances labeld as inform/request fact, we link them to the key-value pairs from the same aspect. Apart from the objective knowledge, we also crawl movie comments from Douban Movie to support the generation of responses expressing subjective feelings. These comments can be a good supplementary to provide knowledge that can be hardly organized in the structured form (Moghe et al., 2018) . For utterances labeled as inform/request feeling, we compare them with Douban comments from the same movie and compute the similarity score based on weighted average of edit distance, Jaccard distance, tf-idf, sentence vector cosine similarity, common words and entities. Each utterance is linked to the most similar comment with a threshold cutoff. In the end, 51.7% of the utterances about feelings have grounded comments. For utterances about recommendations, we simply ground them to the men- Model Architecture Language models have demonstrated impressive performance as a universal learner across NLP tasks (Shen et al., 2017; Peters et al., 2018; Radford et al., 2019; Brown et al., 2020) . Inspired by this, our dialogue generation model is implemented as a Transformer-based language model like GPT2 (Radford et al., 2019; Zhang et al., 2019) . It contains a pipeline process of movie tracker, intent prediction, knowledge retrieval and text gener- Intent Prediction The intent prediction is also cast as a sequence prediction task. Compared with the traditional way of multi-label classification, casting it as sequence prediction is better at addressing the coexistence of multiple DAs and capturing the sequential dependencies among the hierarchy (Raffel et al., 2019; Vedula et al., 2020) . For example, to predict the DAs of the 4th utterance in Figure 1 , the sequence fed to the language model will be \"[context] dialogue context [intent] inform, feeling, plot, request, fact, plot\". By this means, before predicting a DA, the model can condition on both the dialogue context and its previous DAs to improve the accuracy. Knowledge Retrieval The knowledge retrieval component is similar to the classical DSSM model (Huang et al., 2013) . We replace the MLP with our language model encoder to get the embedding for knowledge. Note that we only select knowledge from the current movie, which can be obtained from the movie tracker, so it is possible to run exact softmax over all knowledge candidates and maximize the likelihood of the ground truth. We condition on both the dialogue context and the intent. The sequence fed to the language model would be \" [context] dialogue context [intent] DA sequence [retrieve] knowledge\" where knowledge will be predicted. When an utterance is not linked with any knowledge, it will predict a None token. Text Generation The text generation is conditioned on the dialogue context, intent and the retrieved knowledge. All the conditions are concatenated into a long sequence. The knowledge can be fact, comment, movie names of mixture of them according to the DA of the utterance. For example, if grounded on comment knowledge, \" [context] dia- logue context [intent] DA sequence [comment] com- ment [ response]\" will be fed to the language model to generate the response. To make it consistent with the pretrained general-domain dialogue, the position embedding of the decoded response will skip the concatenated intent and knowledge and directly follow the dialogue context. We find this beneficial when combined with pretrained models. The objective also follows the pretrained model mixing maximum lilkelihood and unlikelihood training. Experiment Setting We tokenize the Text in the unit of Chinese characters and keep all unique non-Chinese unique tokens appearing for more than 5 times. The whole vocabulary contains 13,317 words. We train our model on 24 Nvidia V100 GPUs (32GB) with three different model sizes as shown in Table 4 . The batch size is fixed as 64 per GPU. The context length is truncated to be 300 words. We optimize models with the Adam gradient descent and a drop out rate of 0.  PyTorch (Paszke et al., 2019) . Results and Analysis Automatic Evaluation In Table 5 , we report the perplexity, BLEU scores and distinct uni/bigrams for three model sizes. To investigate the effects of incorporating annotations and pretraining, we start from a basic model which trains from scratch on our movie corpus. At each time, we add one more condition to see its influence. The results show a clear tendency of gradual improvement as more conditions are added to the training. Adding knowledge especially boosts the performance, which is understandable considering movie-domain chats usually contain many movie-specific rare names. Without knowledge grounding, it can hardly predict the correct tokens. Pretraining on general-domain conversations can improve both the overlap with ground truth. The distinct uni/bigrams also consistently increase, implying the model can learn useful patterns in the pretrained corpus to enrich its generations in the movie domain. In unseen testset, the performance generally drops for all, especially for models without knowledge grounding as they have to make up facts and comments for totally unseen movies in the training set. Table 6 measures the accuracy of predicting dialogue act (DA), aspect and movie tracker of our model. Our models are all pretrained with general-domain corpus beforehand. Apart from being trained only to predict the individual tasks, we include the results where all subtasks are cotrained end-to-end in the last line. We compare our models with the Chinese RoBERTa (Liu et Table 7 : Hit rates of knowledge retrieval. Table 7 measures the performance of retrieving fact knowledge, movie comments and recommen-dation respectively. We report the hit@1 and hit@5 scores for them (Zhang et al., 2018) . We compare our model with a random baseline, bag-ofword (BOW) and the Bert (Devlin et al., 2019) model (we pass sentences through Bert and derive a fixed-sized vector by averaging the outputs from the second-to-last layer (May et al., 2019) ). The BOW and Bert model are finetuned with our knowledge linkage annotations. We find that our unified model again outperforms all baseline approaches. Adding the DA as a condition further helps. Fact retrieval has the highest hit rate as it is well structured and easy to match. Recommendation, on the other hand, is very hard to predict. As an accurate recommendation system is clearly beyond the scope of this paper, it is understandable that our simple way fails to provide satisfying recommendations. Human Evaluation Automatically evaluating dialogue systems are known to be extremely hard (Liu et al., 2016; Su et al., 2018) . We further conduct a set of static and interactive human evaluations. We focus on evaluate the machine-generated response from four perspectives. Apart from the oft-used metrics (1) Sensibleness (Sens) and ( 2 ) Engagement (Enga) for open-domain chatbots, we further evaluate on (3) Factuality (Fact) and ( 4 ) Informativeness (Info) to see if models can actively provide informative responses based on movie facts. Details are in Appendix B. As evaluating factuality requires specific movie knowledge, this metric is only evaluated by the same person who produced the dialogue. The other metrics are evaluated by 3 workers each. Table 8 shows the agreement scores. The agreement is reasonable considering the evaluations are subjective. The results are the majority votes of the binary scores. In the static evaluation, we sample 300 responses for each model from the test set (mixing seen and unseen). The responses can come from any turn in a conversation. We show the results in Figure 2 . Our largest model with 762M is clearly preferred by human evaluators on almost all metrics and approaches human performance. By training a larger model and increasing the training size, the gap might be further closed. In the interactive evaluation, humans can chat with any topic but restricted in the movie domain. We conduct an online Turing test where one side is always a human participant not aware whom he is talking with. The other side could be either Mitsuku, XiaoIce 13 , our model (762M with pretraining) or a real human. Mitsuku interacts in English, so we hire only English native speakers for the experiment. We collect 100 conversations for all models. Humans can stop interacting once they (1) find the other side is a machine or (2) reaches the maximum turn of 20. Responses from all models are later passed to the third party to judge the scores. The results are shown on the right of Figure 2 . Our model outperforms Mitsuku and XiaoIce by a large margin. As Mitsuku and XiaoIce are designed to be open-domain chatbots, restricting to be on the movie domain will give our model some natural advantage. We can also notice that Mitsuku and XiaoIce almost never produce fake facts. The cost is the extremely low ratio of informative responses since they tend to behave over-safely and will only answer it when they are 100% sure. Our model is closer to humans in that sense. It will converse actively at some risk of containing fact errors. Figure 3 : Change of SEA and FIA as the turn proceeds. 13 We use its chat service through Weibo. It will sometimes generate responses containing keywords like \"XiaoIce\". We manually replace it to prevent disclosing its identity.  Distance from Human Performance In the interactive evaluation, compared with human performance, our model loses a bit on sensibleness and factuality but wins on the other two. To investiwhere our model fails, figure 3 visualizes the change of SSA (Sensibleness-Engagement average) and FIA (Factuality-Informativeness average) when the conversational turn proceeds. A good chatbot should balance well these skills (Adiwardana et al., 2020) . SEA can reflect how it behaves as a general chatbot while FIA can better test its capability at incorporating domain knowledge. We can see a clear trend of decrease for all models. As for human performance, however, the score is quite consistent across turn rounds, implying a large improvement space for current models to deal with multi-turn context. In figure 4 , we further show the \"dying distribution\" of our model, namely, in which DA our model fails to pass the Turing test and thereby \"dies\". Unsurprisingly, we can see the system fails mostly when informing facts or feelings. Only a small portion are from non-grounded chitchats (other). This suggests the most crucial bottleneck lies in the interaction with movie-specific knowledge and seamlessly incorporating it into the response generation. We show some snippets of interactions with our model in Table 9 . The first two are failing cases labeled by humans as not factual and sensible. We can see the model struggles at replying to too specific facts. This is understandable since our knowledge base only provide short introductions and cannot cover all what happened in the movie. The second case shows its shortcoming at handing long-range consistency. It still recommends the current movie when the user asks about \"which other movie\". Employing larger knowledge bases and explicitly tracking the states by a checklist (Kiddon et al., 2016) might potentially alleviate both issue. We also provide examples for controllable generations where the DA and aspect are manually assigned. As observed, the model shows decent performance at fitting both the dialogue con-text and specified conditions. This can be helpful when finer-grained control is needed. Conclusion We present MovieChats: a movie-domain chatbot built upon a large-scale, high-quality conversational corpus with fine-grained annotations. The model can be trained end-to-end with a simple unified language model architecture. We show that our model, powered by well-defined knowledge grounding, is able to approach human performance in some perspective, though still lagged behind when it comes to dealing with detailed knowledge or long-turn consistency. Acknowledge We thank anonymous reviewers and the dialogue system team at Wechat AI for their valuable comments. Xiaoyu Shen was funded by IMPRS-CS fellowship. Ernie Chang is supported by SFB 248 \"Foundations of Perspicuous Software Systems\" (E2). Online Forum Liu et al. (2018) A: \u6c42\u63a8\u8350\u52b1\u5fd7\u7535\u5f71\uff0c\u8c22\u8c22\u5927\u5bb6 (Any inspirational movies? Thanks everyone). B: \u300a\u5f53\u5e78\u798f\u6765\u6572\u95e8\u300b (The Pursuit of Happyness). Crowd-sourced (Constrained) Zhou et al. (2018b) A: Hey have you seen the inception? B: No, I have not but have heard of it. What is it about ? A: It's about extractors that perform experiments using. . . Crowd-sourced (unconstrained) Zhou et al. (2020) A: \u77e5\u9053\u91cd\u5e86\u68ee\u6797\u8fd9\u4e2a\u7535\u5f71\u5417 (Do you know the movie Chungking Express)? B: \u77e5\u9053\u554a\uff0c\u662f\u738b\u5bb6\u536b\u5bfc\u6f14\u7684 (Yes, it's directed by Wong Kar-Wai). A: \u5177\u4f53\u662f\u54ea\u5e74\u4e0a\u6620\u7684\u4f60\u8fd8\u8bb0\u5f97\u5417\uff1f (Remember which year it was on)? B: \u8bb0\u5f97\uff0c\u662f\u57281994\u5e7407\u670814\u65e5 (Yes, July 14th, 1994). . . Ours A: \u5a01\u5c14\u53f2\u5bc6\u65af\u6f14\u6280\u771f\u7684\u5f88\u68d2 (Will Smith's acting skill is really good). B: \u4ed6\u7684\u5f53\u5e78\u798f\u6765\u6572\u95e8\u592a\u7ecf\u5178\u4e86 (His The Pursuit of Happyness is a classic). A: \u4e00\u76f4\u90fd\u6302\u5728\u7535\u5f71\u6392\u884c\u699c\u9760\u524d\u7684\u4f4d\u7f6e (That' always among top ranked movies). B: \u55ef\u55ef\uff0c\u8fd9\u90e8\u7535\u5f71\u771f\u7684\u5f88\u52b1\u5fd7\u554a (Yes, it's really motivational). A: \u5a01\u5c14\u53f2\u5bc6\u65af\u4e5f\u6f14\u51fa\u4e86\u5f88\u60e8\u7684\u611f\u89c9\u4e86 (Will Smith plays like he is a real tragedy). B: \u6f14\u6280\u7279\u522b\u597d (Yes, he acts pretty well). A Dataset Collection Table 10 shows examples comparing our dataset and the others. As observed, forum conversations are mostly single-turn QA or comments. Current crowd-sourced datasets are either collected on constrained scenarios (the scenario in (Zhou et al., 2018b) fixed the roles in a conversation as one introducer and one listener), or unconstrained but prompting people to deliberately connect knowledge. Our dataset simulates real-life conversations to the largest extent. We classify the utterances into one of 15 aspects. The definitions, counts, and examples of them are shown in Table 11 . When annotating the corpus, tutorials and examples are provided to the annotators, we show some examples of in the following tables. All the examples are provided only in Chinese as that is the native language among annotators. B Human Evaluation As for the four human evaluation metrics. The first two will focus only on the conversational backbones without considering domain knowledge. The second two will check if the responses can provide informative and correct responses powered by domain knowledge. The detailed definitions of them are: 3. Factuality: Factuality checks the information correctness in case the model fabricate wrong facts. This is orthogonal to the above metrics. The response can be factual but not sensible or the other way around. The same for Engagement. 4. Informativeness: Informativeness checks if the response contains new information about the movie. If the bot replies sth like \"That's so interesting, I've never heard of that!\". It is considered specific yet non-informative, because the bot provides no new information. Intuitively a good closed-domain chatbot should balance well all these skills. Sensibleness plus engagement can evaluate its general performance as a chatbot. Factuality and informativeness can evaluate its capability at actively injecting appropriate domain knowledge into the dialogue. All the three metrics are evaluated by three crow-workers each except for factuality. As evaluating factuality requires in-depth knowledge about one movie, it is quite difficult for random human evaluators to judge them. Even if we filter to only keep people who have watched a movie, it is hard to guarantee they can recall all the scenes in the movie. Therefore, the factuality check is only done by the person who performed this dialogue. In the static evaluation, it is evaluated by the annotator who produced the reference response. In the interactive evaluation, it is evaluated by the person who chat with the bot. It is nevertheless not accurate though. However, if the bot can cheat the human into believing its false information, it can also somehow be considered a \"success\". We provide examples for guiding the human evaluators in Table 12 . Table 13 shows some interactive examples with humans. We observe Mitsuku can XiaoIce perform decently in single-turn exchanges but strongly struggle at understanding multi-turn user intents. Most conversations stop at turn 4 and will not move on.",
    "funding": {
        "military": 0.0,
        "corporate": 0.3276691679463798,
        "research agency": 0.04336646318194026,
        "foundation": 0.0006667031046541094,
        "none": 0.999999091165773
    }
}