{
    "article": "There are, hopefully, many computer programs for automatically determining which sense of a word is being used in a given context, according to a variety of semantic, defining or other types of dictionaries. SENSe EVALuation (SENSEVAL) is an open, community-based evaluation exercise for Word Sense Disambiguation (WSD) programs, arranged for a second consecutive time. The purpose of the exercise is to be able to say which programs and methods perform better, which worse, which words, or varieties of language, present particular problems to which programs. Moreover, not only do we want to know which programs perform best, but also, the developers of a program want to know when modifications improve performance, and how much and what combinations of modifications are optimal. Introduction According to dictionaries, common words have more than one meaning. Usually, only one of these meanings apply in a given context, either written or spoken. This is no issue for people in their daily interaction with others, but it is a difficult task for computers. The task is of great importance in a number of Natural Language Processing (NLP) applications, such as Machine Translation (MT) or (Cross-Language) Information Retrieval ([CL]IR). Word sense ambiguity is a potential source for errors in such tasks and it is considered as the great open problem at the lexical level of NLP. There are, however, several computer programs for automatically determining which sense of a word is being used in a given context, according to a variety of semantic, or defining dictionaries. SENSe EVALuation (SENSEVAL), Kilgarriff (1998) , Kilgarriff & Palmer (2000) is an open, community-based evaluation exercise for Word Sense Disambiguation (WSD) programs arranged for a second consecutive time. The purpose of the exercise is to be able to say which programs and methods perform better, which worse, which words, or varieties of language, present particular problems to which programs. Moreover, not only do we want to know which programs perform best, but also, the developers of a program want to know when modifications improve performance, and how much and what combinations of modifications are optimal. Specifically for Swedish, we would also like to investigate to what extent WSD can be done, the potential resources available for the task and create a framework that can be shared both within SENSEVAL and for future evaluation exercises of similar kind, national and international. SENSEVAL is designed to meet all these needs. This paper will present some of the experiences we gained by participating as developers and organisers in the SENSEVAL exercise for Swedish. Particularly, the choice of the lexical and textual material, the annotation process, the scoring scheme, and the motivations for choosing the \"lexical-sample\" branch of the exercise. Short History SENSEVAL-1 was the first open evaluation exercise for WSD programs. Three languages (English [18 systems], French [5 systems] and Italian [2 systems]) and a total of 23 research groups participated. SENSEVAL-1 was held in Sussex, UK in 1998. The exercise was conceived at the SIGLEX workshop: \"Tagging Text with Lexical Semantics. Why, What and How?\" held in 1997 in Washington. A range of Machine Learning algorithms and a variety of lexical resources were utilised. Two important points are worth to be mentioned w.r.t. SENSEVAL-1. One was the fact that by the end of the exercise the purity of the approach was less important than the robustness of the system performance; and second, the discussion created more awareness among the participants of how fundamental the lexicon is to the task. Lexical Sample Three tasks were identified for SENSEVAL-2, these are: the lexical-sample, the all-words and the 'in a system' tasks. In the lexical sample task, first, we sample the lexicon, then we find instances in context of the sample words and the evaluation is carried out on the sampled instances (SENSEVAL-1 was a lexical-sample exercise). In the all-word task a system will be evaluated on its disambiguation performance on every word in the test collection. Finally, in the third type of task, a WSD system is evaluated on how well it improves the performance of a NL system (MT, IR etc). The reasons we chose the lexical-sample task for Swedish are summarised below: 1. Cost-effectiveness of annotation: it is easier and quicker for the human annotators to sensetag the evaluation material; 2. The lexical-sample reduces the work of preparing training data since only a subset of the sense inventory is used; 3. More systems can/could (eventually) participate; 4. The all-words task requires access to a full dictionary, which is problematic from the copyright point of view, since industrial partners were also allowed to participate; 5. Provided that the sample is well chosen, the lexical sample strategy would be more informative about the current strengths and failings of WSD research than the all-words task (Kilgarriff & Palmer (2000) ). 1. Table 1 gives brief information w.r.t. the different languages participating in the lexical-sample part of SENSEVAL-2. SENSEVAL-2: Development Process In this section we will give a concise description of how the whole exercise (for Swedish) was set up, putting more emphasis on some of the main ingredients of the work, i.e. resources, sampling, annotation and scoring. A number of likely participants were invited to express their interest and participate in the Swedish SENSEVAL (summer, 2000) . A plan for selecting the evaluation material was agreed in Spr\u00e5kdata, and human annotators were set on the task of generating the training and testing material. The material was released to the participants by the end of April, 2001 and the stateof-affairs at this moment (May, 2001) is that the participants are working with the material. During the second week of June, 2001 the results will be available, a two-day workshop will be held in Toulouse, France, devoted to the SENSEVAL-2 exercise. The Swedish SENSEVAL material was divided into three parts and released in stages: \u2022 Trial data: freezing and showing the data formatting conventions (lexicon & corpus); \u2022 Training data: the finalised sense inventory and portion of the 'gold standard'; \u2022 Evaluation data: the rest of the 'gold standard', untagged. Dictionary and Text At least three lexical resources were candidates for the Swedish lexicon-sample task. These were the Swedish versions of S-WordNet (http://www.ling.lu.se/projects/Swordnet) and SIMPLE (http://spraakdata.gu.se/simple/), and the Gothenburg Lexical Data Base (GLDB/SDB) (http:// spraakdata.gu.se/lb/gldb.html). The GLDB/SDB was chosen since the S-WordNet had (up to that point) limited coverage and is also an ongoing project; while SIMPLE, although available, has limited coverage (in principle it could be used since it is linked to GLDB/SDB). GLDB/SDB is a generic defining dictionary of 65,000 entries. Creating a sense-annotated reference corpus is a laborious task. Therefore, we developed the majority of the test and reference material within an ongoing, highly relevant for our mission project, namely SemTag ('Lexikalisk betydelse och anv\u00e4ndningsbetydelse' -Lexical Sense and Sense in Context); see J\u00e4rborg (1999) . For the textual material the Stockholm-Ume\u00e5 Corpus (SUC), Ejerhed et al. (1992) , was chosen, basically for two reasons. One because it is available to the research community, and, second because it is the corpus utilised in SemTag. Table 2 shows information on the sense inventory, the amount of corpus instances and the distribution of senses (lexemes) and sub-senses (cycles) in the material. Sampling There is no standard method for sampling the lexical data. However, certain features were considered. These were: Words were chosen based not so much on intuition, but rather on their frequency and polysemy. Still, it is hard to find a balance between these two features since high frequency words tend to be monosemous in a corpus, while high polysemous words tend to have few senses in a corpus. In the case that a word was frequent and polysemous we tried to provide more data (context), than words that were less frequent. Part-of-speech information was accounted for choosing more nouns in the sample (highest portion in the GLDB/SDB), than verbs (less than nouns, but more than adjectives in the GLDB/SDB) and adjectives (which are less than nouns and verbs in GLDB/SDB). We chose a sample of words where the amount of senses was evenly distributed, i.e. lemmas with 2-7 senses and 1-23 subsenses. Annotation Process The annotation was carried out interactively using a concordance-based interface, Figure 1 . Due to our limited financial resources only two professional lexicographers and a trained phd student were involved in the tagging process, which was preferred to (untrained) students doing the annotation. The replicability between those were on the 95% level.  Frequency Polysemy Part-of-speech Distribution of senses development of the annotated instances for SENSEVAL-2 gave us a chance to revise our sense inventory and make adjustments and improvements to the descriptions found in the database; i.e. in the form of adding new sub-senses or modifying definitions of senses. Interchange and Result Format The corpus instances and dictionary format was in XML with DTDs provided. An example of a corpus instance (SUC file:AD04_BRV) for the 5th sense of the verb h\u00f6ra here: 'belong' is: <instance id=\"h\u00f6ra.301\"><answer instance=\"h\u00f6ra.301\" senseid=\"h\u00f6ra_1_5\"/> <context>Den \u00e4mnesdidaktiska forskningen kom ig\u00e5ng i Sverige f\u00f6rst p\u00e5 70-talet. Geografiundervisningen diskuterades dock redan p\u00e5 50-talet. Sverige <head>h\u00f6rde </head> till de ledande nationerna n\u00e4r det g\u00e4llde den\" nya geografin\". Utbytet mellan Lund och USA var livligt och G\u00f6sta Wennberg som bodde och arbetade i Lund p\u00e5 den tiden tog starka intryck. 1964 kom han till Uppsala och blev metodiklektor p\u00e5 l\u00e4rarh\u00f6gskolan. </context> </instance> The systems required to return, for scoring, a one-line-per-answer for each unique corpus reference for the token being tagged and for which they were returning a result. One or more sense-identifiers, optionally associated with a probability measure (see also Section 5), could be attached. The BNF for scoring is: Scoring Prior to SENSEVAL evaluating WSD performance was based on the exact match criterion given by the formula: %correct=100 \u00d7 (#exactly matched sense tags/#assigned sense tags) which is not consider a \"fair\" metric, and has a lot of drawbacks, such as that it does not account for the semantic distance between senses when assigning penalties for incorrect labels, and that it does not offer a mechanism to offer partial credit; cf. Resnik & Yarowsky (2000) . Instead, in SENSEVAL-2 three scoring policies are adopted: 1. Fine-grained: answers must match exactly 2. Coarse-grained: answers are mapped to coarse-grained senses and compared to the gold standard tags, also mapped to coarse-grained ones (sense map is required; see below) 3. Mixed-grained: if a sense subsumption hierarchy is available, then the mixed-grained scoring gives some credit to choosing a more coarse-grained sense than the gold standard tag, but not full credit (also using a sense map; see below). A \"sense map\" contains a complete list of all sense-ids involved in the evaluation and is necessary for performing the two last types of scoring policies. Each line in the sense map includes sense subsumption information and contains a list of the subsumer senses and branching factors. Participants Three groups showed interest on participating in the Swedish task: Conclusions The process of Word Sense Disambiguation is a complex, controversial matter, but relevant for a number of Natural Language Processing applications. Our contribution to the exercise will eventually sharpen the focus of WSD in Sweden; the material developed in SENSEVAL-2(Swedish) can be used as benchmark for other researchers that need to measure their sys- tem's WSD performance against a concrete reference point (although the number of words is rather small). We think that WSD opens up exciting opportunities for linguistic analysis, contributing with very important information for the assignment of lexical semantic knowledge to polysemous and homonymous content words. The existence of sense ambiguity (polysemy and homonymy) is one of the major problems affecting the usefulness of basic corpus exploration tools. In this respect, we regard WSD as a very important process and component when it is seen in the context of a wider and deeper NL processing system.",
    "abstract": "There are, hopefully, many computer programs for automatically determining which sense of a word is being used in a given context, according to a variety of semantic, defining or other types of dictionaries. SENSe EVALuation (SENSEVAL) is an open, community-based evaluation exercise for Word Sense Disambiguation (WSD) programs, arranged for a second consecutive time. The purpose of the exercise is to be able to say which programs and methods perform better, which worse, which words, or varieties of language, present particular problems to which programs. Moreover, not only do we want to know which programs perform best, but also, the developers of a program want to know when modifications improve performance, and how much and what combinations of modifications are optimal.",
    "countries": [
        "Sweden"
    ],
    "languages": [
        "Swedish"
    ],
    "numcitedby": "0",
    "year": "2001",
    "month": "May",
    "title": "{S}wedish {SENSEVAL}, a Developer{'}s Perspective"
}