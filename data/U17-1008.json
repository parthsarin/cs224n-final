{
    "article": "The automatic detection of negation and speculation in clinical notes is vital when searching for genuine instances of a given phenomenon. This paper describes a new corpus of negation and speculation data, in the veterinary clinical note domain, and describes a series of experiments whereby we port a CRF-based method across from the BioScope corpus to this novel domain. Introduction Negation and speculation are common in clinical texts, yet pose a challenge for natural language processing of these texts. Negation indicates the absence or opposite of something, and is defined within the previously released BioScope corpus (a collection of biomedical and clinical documents annotated for the task of negation/speculation detection) to be the \"implication of the non-existence of something\" (Szarvas et al., 2008) . For example, the statement no abnormalities were found in the patient indicates the absence of abnormalities in the patient. Speculation is used to indicate uncertainty or the possibility of something, and is defined within BioScope to be statements of \"the possible existence of something\". For example, there is possible bacterial infection indicates that an infection might be present, without any certainty that it is. Both are commonly used in clinical texts as a means of ruling out diagnostic possibilities and hypothesising. This paper will discuss a method for detecting negation and speculation over clinical records from the Veterinary Companion Animal Surveillance System (VetCompass) project. 1 The Vet-Compass project is a database of veterinary clinical records for tracking animal health. The database may be used for research on the effects and usage of a particular drug, or the prevalence and distribution of a disease. Such studies are typically performed by querying for terms relevant to a drug or disease of interest, and analysing the retrieved clinical records. However, results identified using keyword matching are often speculative or negated mentions rather than true occurrences. By automatically detecting negation and speculation, we aim to suppress these results, and provide a higher-utility set of documents to the user. The task of negation/speculation detection is often defined in terms of two subtasks: (1) signal (or cue) detection; and (2) scope detection. Negation/speculation signal (or cue) detection involves determining which words in a sentence indicate that a negation/speculation is occurring. Negation/speculation scope detection involves determining which words in a sentence the negation/speculation applies to, under the constraints that: (a) the cue word is contained within the span of the scope; and (b) the span is contiguous. Consider two examples from the clinical notes subset of the BioScope corpus: (1) The lungs are well expanded, but [[NEG not hyperinflated NEG ] ]. (2) Mild thoracic curvature, [[SPEC possibly positional SPEC ]]. The cues here for negation and speculation are not and possibly, respectively, and the words inside the brackets are within the scope of the cues. We apply this task formulation to the veterinary clinical notes of VetCompass. The VetCompass records (which mainly consists of notes from veterinary general practitioners) have a few important differences from the radiology clinical notes of the publicly available BioScope corpus. First, radiology notes are often shared between clinicians treating the same patient, and as such are generally written to be accessible to others. In notes from veterinary general practitioners, it is often the case that a single clinician treats the patient, meaning that clinical notes are largely for personal consumption, and thus are highly idiosyncratic in nature. Second, while radiology clinical notes are often professionally transcribed from an oral account by the clinician, in the veterinary general practice context, notes are authored directly by the clinician as text. Inevitably, this is done under time pressure, meaning that the text is often ungrammatical and lacks punctuation. Examples (3) and ( 4 ) exemplify negation and speculation in Vet-Compass: ( 3) Mm -moist [[NEG no skin tent NEG ]] (4) Adv [[SPEC poss bacterial infection SPEC ]], adv [[SPEC can be allergy in origin SPEC ]] Such differences in usage between veterinary clinicians and other medical professionals such as radiologists are a major focus of this work, in adapting the annotation framework from BioScope to this new domain. This paper attempts to address the following research questions: (1) Can the task of negation/speculation detection be applied to veterinary clinical records? (2) Are models trained over the human clinical records of the BioScope corpus applicable to veterinary clinical notes? This paper describes the process of annotating negation and speculation in veterinary clinical records. We then demonstrate that the task of negation and speculation detection can be successfully applied to veterinary clinical notes using a simple conditional random field (CRF) model. We additionally show that models trained on a related out-of-domain corpus such as the BioScope have utility over veterinary clinical records, in particular for negation detection. Literature Review Previous Work in Negation and Speculation Detection Most work on negation and speculation detection has focused on biomedical documents such as biological research papers and clinical notes, with the latter being most relevant to this research. Early approaches to negation detection were primarily rule-based. One of the best-known systems for negation detection is NegEx (Chapman et al., 2001) , which is based on regular expressions containing a negation cue term (such as no or not). Another rule-based negation detection system is NegFinder (Mutalik et al., 2001) . More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a) , and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante and Daelemans, 2009b) . Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b ) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D\u00edaz et al. ( 2012 ) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detection methods over different domains, and found that performance often suffers without in-domain training data. Miller et al. (2017) also investigated the use of different unsupervised domain adaptation algorithms for negation detection in the clinical domain and found that such algorithms only achieved marginal increase in performance compared to systems that use in-domain training data. Veterinary NLP We are only aware of a few papers that have applied natural language processing in the veterinary domain. Ding and Riloff (2015) conducted work on detecting mentions of medication usage in a discussion forum for veterinarians, and categorizing the usage of the medication. A classifier determines whether each word is part of a medication mention using features such as the POS tags and neighbouring words The output of the medication mention detector is used by another classifier to determine its usage category such as whether the clinician prescribed the medication or changed it. Text classification is a task that had been previously applied to veterinary clinical records. Anholt et al. (2014) performed classification of a collection of veterinary medical records to identify cases of enteric syndrome. Lam et al. (2007) used clinical records of racing horses to categorise their reason for retirement. Duz et al. (2017) used classification to identify cases of certain conditions and drug use in clinical records from equine veterinary practices. In each of these studies, a dictionary was compiled to identify and detect phrases that indicate a certain category. BioScope Corpus Currently, there is no publicly available corpus for training models over veterinary clinical notes. However, the BioScope corpus (Szarvas et al., 2008) provides a relevant dataset from which to train out-of-domain models. It is a publicly available collection of biomedical documents that have been annotated for both negation and speculation, in the form of cue words and their scope (see Section 1). BioScope consists of three subcollections: clinical radiology notes, biological papers, and abstracts of biological papers from the GENIA corpus (Collier et al., 1999) . VetCompass Corpus The VetCompass project is a collection of clinical records of veterinary consultations from several participating practices, to support analysis of animal health trends (McGreevy et al., 2017) . To conduct these studies, clinicians use an information retrieval (IR) front-end to retrieve clinical records related to their particular information need, based on Boolean searches. A major bottleneck for the naive IR setup of returning all matching documents is the prevalence of term occurrences in negated or speculative contexts, which dominate the results for many queries. This is the primary motivation for this research: to improve the quality of the search results by filtering out document matches where the component term only occurs in negated or speculative context. The major challenge here is that the language used in the veterinary clinical notes of VetCompass differs from that used in related publicly available datasets such as the BioScope radiology clinical notes. Discussion of VetCompass Corpus The corpus used in this work was constructed from a random sample of 1 million clinical records from VetCompass UK. 2 VetCompass clinical records contain a wide variety of text. Many records contain free text describing the clinician's observations, hypotheses, and descriptions of treatments and future actions. However, there are also records that contain only billing information, document the weight of the patient, or are reminders to perform certain actions like sending an invoice to the owner of the patient. Compared to the BioScope radiology clinical notes, VetCompass clinical notes are much more informal, possibly due to the fact that they are largely \"notes to self\" (see Section 1). As such, ad hoc abbreviations and shortening of terms as shown in Examples ( 3 ) and ( 4 ) are very common, and informal speculative expressions such as feels like and looks like are prevalent: ( There are certain negation and speculation cue terms that appear only in the VetCompass corpus such as: The term nad is often used in place of no acute distress or no abnormalities detected, and is an instance of negation. Question marks were often used as speculative cue terms such as in Example (9). The use of domain-specific cue terms presents a challenge for applying models that were trained on a corpus like BioScope clinical notes. Misspellings, grammatical errors and lack of punctuation are also common in the text of the veterinary general practice clinical notes, e.g.: In Example (10), the negation cue without is misspelled. In Example (11), punctuation is missing, making it hard to clearly separate the different statements in the sentence, and suggesting that pure parser-based approaches will struggle over this data. In terms of annotation, while some abbreviations, shorthands, misspellings, and punctuation errors are easy to interpret, others are more difficult to understand: (12) -other poss: renal diz (given that had low sg + proteinuria, \u02c6BUN/\u02c6Phosp BUT N -creat)/liver diz (given hepatomegally on rads + \u02c6ALP, Bile acids, Cholest, ? low sod/K+ ratio -could be related to kids or addisonian crisis BUT no hx of pu/pd Symbols like \u02c6require domain expertise to interpret. The appearance of terms like poss indicates that the sentence contains speculation but the irregular use of punctuation makes determining the correct boundaries of the speculation scope difficult. In fact, the absence of certain punctuation marks such as full stops can make it difficult for sentence tokenizers to work correctly. In the VetCompass corpus, a single statement of speculation is sometimes expressed using multiple speculation cue terms, e.g.: (13) History-o concerned swollen lower lip, [[SPEC thinks poss stung SPEC ]], been there 2d Here, the clinician is reporting that the owner of the patient (shortened to o) speculated that the patient was stung, as indicated by two cue terms, thinks and poss, presumably to indicate their lack of confidence in the statement. Such instances of \"double hedging\" are very rare in BioScope, presenting an extra point of differentiation. Annotation Guidelines Here, we outline the annotation guidelines for the VetCompass corpus, which borrow heavily from the BioScope annotation guidelines. As per the BioScope annotation guidelines, sentences from VetCompass are annotated for speculation if they express uncertainty or speculation, and annotated for negation if they express the non-existence of something. The min-max strategy of Bio-Scope annotation is also followed (Szarvas et al., 2008) . Negation/speculation cues are annotated such that the minimal unit that expresses negation/speculation by itself is marked. Scopes are then annotated relative to cue words, to have maximal size or the largest syntactic unit possible. Below, we detail important deviations from the Bio-Scope annotation guidelines, which are motivated in part by the usage of the negation/speculation detection system in an information retrieval context. Annotation of Cues The VetCompass annotation guidelines use the same set of cue words as BioScope, with the addi-tion of NAD (a negation cue -see above), question marks (which are potentially speculation cues -see above), and shortened and misspelled variants of cue words (like poss for possible). As with BioScope, not all occurrences of a negation or speculation keyword indicate negation or speculation. For instance, occurrences of negation or speculation keywords in descriptions of proposed actions are generally not annotated for negation or speculation. Examples of such cases are: (14) Advised to not give last onsior due to d+. (15) Suggested FNA if increase in size In Example ( 14 ), not is not annotated as a negation cue since the sentence is stating a recommendation rather than expressing the absence or opposite of anything. In Example (15), suggested is not annotated since it is being used in the sense of proposing an action rather than hypothesising. These examples are also not annotated because of the utility they might provide for a clinician. If a clinician was researching FNA, the document containing Example (15) would be potentially useful for understanding situations where such a procedure was proposed. However, actions that were performed in the past that contains negation or speculation would be annotated such as cannot in Example (6) which is clearly expressing the opposite of the ability to perform that action. Conditionals are another situation where negation or speculation keywords may not always be annotated as cues. If a negation or speculation keyword appears in the clause expressing the condition (clause containing the if ), then they should not be annotated as cues as demonstrated in the following examples: Here, there is not clear negation or speculation, but rather the lack of something in the conditional (e.g. consider euthanasia) or consequent (e.g. treatment). While these two sentences may be annotated under the BioScope annotation guidelines, we chose not to do this for the VetCompass clinical records because of the utility they might provide for a clinician. Even if a certain term is negated inside of a conditional, there is usually other information in the clinical record that provides instructions about what to do in non-negated circumstances which is useful for a clinician. In the case of a term being speculated inside of a conditional, the consequences of the term occurring is certain even if the condition had not occurred. Annotation of Scopes In many cases, negation and speculation scopes start at cue terms and end at the end of the clause or sentence. However, punctuation is often omitted, meaning that boundaries of clauses and sentences can be unclear. The annotator must use their own judgement and interpretation of the sentence in order to create a suitable annotation. The following example demonstrates a sentence where an annotator must interpret the sentence to understand where the clause boundaries are: Unlike the BioScope annotations, VetCompass clinical records were not annotated to contain nested speculation scopes, i.e. speculation scopes are never contained within other speculation scopes. This decision was motivated by the expected retrieval usage of the negation/speculation system: such information does not provide additional information to help filter out negated or speculated mentions of certain terms from search results. An example of the implication of this guideline is shown in the following sentence that is annotated with one negation scope and one speculation scope: The above sentence would have been annotated as three nested speculation scopes under the Bio-Scope annotation guidelines. However, using the VetCompass annotation guidelines, only a single speculation scope will be annotated, containing three separate speculations cues. If a user had wanted to search for documents with trichobezoars, this sentence will not be retrieved regardless of whether the nested structure is annotated or not. However, nested negation scopes in VetCompass are annotated. Moreover, speculation scopes that are nested within a negation scope and vice versa are also annotated. The data was singleannotated by the first author using the BRAT annotation tool (Stenetorp et al., 2012) , in consultation with the other authors in instances of doubt. 100 records (containing 586 sentences) from the test set were selected and annotated by one of the other authors, following the guidelines in Section 3.1. The agreement between the two annotators was calculated using Cohen's kappa (\uf8ff) and F1-score (obtained by treating the annotations made by the main annotator as the goldstandard). We measure the amount that the two annotators agreed that a particular token is a negation/speculation cue or scope. The inter-annotator agreement is described in Table 1 . The \uf8ff values in Table 1 demonstrate a reasonable amount of agreement between the two annotators. However, there is still some subjectivity, particularly for the speculation cues. There are several reasons for the discrepancy in annotations between the two annotators: (1) the limited experience in linguistics and text analysis on the part of the main annotator of VetCompass; (2) the lack of pre-training for annotating the Vet-Compass corpus for the other annotator, beyond receiving the annotation guidelines; and (3) the different levels of familiarity with the datasets of BioScope and VetCompass. Preparation of corpus Sentence tokenization was performed to prepare the corpus for usage, based on the findings of Read et al. (2012) . The output of the sentence tokenizer was converted into the BRAT annotation format so that the output could be manually corrected if needed. However, the correction was not a systematic process. A sentence tokenization output was corrected only if it was clearly incorrect from a quick inspection during the annotation process. Most corrections only occurred when nega- Summary of Corpus Table 2 provides details of the annotated corpus. In general, large variations in sentence length can be observed: some sentences are as short as two words (e.g. reporting the patient weight), while others contain long detailed descriptions of the consultation. The annotated VetCompass corpus contains a slightly lower proportion of negated sentences compared to those in the BioScope clinical notes (where 13.55% of the sentences were annotated as negated), and a much lower proportion of speculative sentences (compared to 13.39% in BioScope). Methodology Model Description To evaluate whether the task of negation and speculation detection can be applied to the veterinary clinical notes of VetCompass, a simple linearchain conditional random field (CRF: Lafferty et al. ( 2001 )) model was trained, in the form of a re-implementation of the negation and speculation detection methods proposed by Agarwal and Yu (2010a,b) . The negation detection system consists of two parts: a cue detection system, and a scope detection system. The cue detection system is a CRF that classifies whether or not a given token is a negation cue. A CRF was used for cue detection to be able to model contexts in which cues appear in both negation and non-negation contexts, and to model multiword cues. The scope detection system is also a CRF, and classifies whether or not a token in a sentence is part of a negation scope. The negation cue CRF uses only the words of the sentence as features. For the negation scope CRF, both the words of the sentence and the POS tags were used. When POS tags are used, the words that are part of a negation cue (that were detected by the negation cue CRF model) were either retained or replaced with a special CUE tag. The speculation detection system has a similar setup, except the system classifies a token as being the inside or outside of a speculation cue or scope. The cue detection system is based on the following features: the target word, and the two words to the left and right of the target word. The scope detection system determines if a token is inside or outside a negation or speculation signal using either the words and POS tags of the token, five tokens to the left and right. Our experiments are based on the corpus described in Section 3.4. The size of the context window for the CRF model was selected based on preliminary experiments with the development set. The parameter that achieved the best F-score over that set was chosen. NLTK 3 was used to tokenise the sentence and obtain the POS tags. As our CRF learner, we used CRF++ v0.58. 4 In our experiments, CRF models were either trained on BioScope clinical dataset, VetCompass, or both. Baselines We used NegEx system and LingScope as baselines. LingScope is a Java implementation of the CRF models developed by Agarwal and Yu (2010a,b) . It contains models that were pretrained using the BioScope clinical data. Though our CRF model and LingScope were based on the same paper, LingScope differs from our models through the use of a different CRF implementation (using the CRF model provided by the Abner tool (Settles, 2005) ), the size of context window used for the classification, and the POS tagger (the Stanford POS tagger). We used a Python implementation of NegEx. 5 This version of NegEx detects negation scopes to be between a trigger term/phrase identified by NegEx and either a conjunction, start or end of a sentence (which can be longer than the limit of five tokens in the original version of NegEx by Chapman et al. (2001) ). Experimental Setup Our experiments were based on the fixed split of the corpus described in Section 3.3. We evaluate both the cue detection and scope detection system using precision (P), recall (R) and microaverage F-score (F). Evaluation was performed on a token-level based on whether it is inside or outside of any negation/speculation cue or scope. We experimented with using different training data to determine whether models trained on outof-domain data such as BioScope clinical data are suitable for veterinary clinical notes. Since Bio-Scope clinical dataset is much larger than Vet-Compass, we also experimented with oversampling of instances from the VetCompass training data when both corpora were used for training (at oversampling rates of 1, 2 and 5). When an oversampling rate of 2 is used, we use two duplicates of each VetCompass training record during the training process, and similarly for oversampling rate of 5. Results Results for negation cue detection and negation scope detection are presented in Table 3 and Table 4, respectively. Results for speculation cue detection and speculation scope detection are presented in Table 5 and Table 6 , respectively. When trained only on BioScope clinical data, the CRF systems (for both cue detection and scope detection) performed worse than their respective baselines. The model only outperforms the baselines when VetCompass training data is used. For negation cue detection and scope detection, incorporating both BioScope clinical data with VetCompass records as training instances helps improves the F-scores for most cases. Further marginal improvements can be achieved with oversampling of the VetCompass training instances as well in most cases. However, for speculation cue and scope detection, the inclusion of BioScope clinical data with VetCompass training data helps improve the recall but reduces the precision, leading to only marginal improvements in F-scores. Oversampling Vet-Compass helps to improve the precision, recall and F-score slightly, but the precision is still lower than when the BioScope clinical data was not included in the training set. In both speculation cue detection and scope detection results, the recall is consistently much lower than the precision. The re- Training data used for CRF models are either Bio-Scope (BIO) and VetCompass (VC) or both clude NAD, unable, and contractions such as doesn't. For speculation, these cues include question marks, poss and think. In speculation cue detection, it was particularly important to have indomain training data as there are more domainspecific speculation cues. However, even with VetCompass training data, the cue detection systems (particularly speculation cue detection) still have difficulty detecting all of the cues. Some of this was caused by cue words being misspelled (e.g. doestn instead of doesn't) or a variant not seen in the training data (such as susp for suspect). A useful feature could be to use word or string similarity to known cue terms to overcome this issue. Author or patient metadata could also be useful, since some of this is consistent across consultations for a given individual. Such data could be used as additional features for a classifier or by having separate models for different authors/patients. However, even cues where the form appears in the training data are still sometimes not detected by our system, particularly for speculation cues. This may be because the system was not able to generalise from the limited training data. There was also a greater variety of speculation cues than negation cues. This observation, combined with the smaller proportion of sentences that were speculative, means that there were less training instances for each possible speculation cue. Both negation and speculation cues also have false-positives that resulted from identifying negation-like or speculation-like terms, such as not bad. The speculation cue detection system also often did not detect speculation cues that contained negation-like terms such as not sure, while the negation cue detection system incorrectly classifies the not in this example as a negation cue. The errors in cue detection create further errors in the associated scope detection system. However, even with correctly detected cues, the scope detection system still has problems with recall. In most of these cases, the system does not correctly determine one token at the start or end of the scope as being part of it. If the scope is very long, the system will often only detect the first few tokens as being part of the scope and miss the remaining tokens. Scopes where the cues are question marks are also often smaller than the reference annotation, as the system usually only includes the token directly to the left or right of the question mark as part of the speculation scope. Conclusions and Further Work This paper describes the annotation of a new dataset for negation and speculation detection over veterinary clinical notes. We reimplemented a simple CRF approach for detecting negation and speculation cues and scope, and trained the model over VetCompass training data, BioScope, or both. Our results demonstrated that while datasets such as the BioScope clinical corpus have utility, indomain training data is often necessary to attain reasonable performance levels, particularly for speculation detection. Further work will focus on improving the recall of negation and speculation detection systems for veterinary clinical notes. Improving the recall is important for the IR use case that the system will be deployed in. We will also focus on expanding the features used for classification, and experiment with different classifiers. Another focus could be on learning features that are particular to the different authors of notes, and using these to improve negation and speculation detection.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 0.9993734205829202
    },
    "reasoning": "Reasoning: The article does not provide specific information regarding its funding sources. Without explicit mention of support from defense, corporate entities, research agencies, foundations, or an indication of no funding, it is not possible to accurately determine the funding sources based on the provided text.",
    "abstract": "The automatic detection of negation and speculation in clinical notes is vital when searching for genuine instances of a given phenomenon. This paper describes a new corpus of negation and speculation data, in the veterinary clinical note domain, and describes a series of experiments whereby we port a CRF-based method across from the BioScope corpus to this novel domain.",
    "countries": [
        "Australia"
    ],
    "languages": [],
    "numcitedby": 10,
    "year": 2017,
    "month": "December",
    "title": "Automatic Negation and Speculation Detection in Veterinary Clinical Text"
}