{
    "article": "In this paper, we investigate the annotation projection of semantic units in a practical setting. Previous approaches have focused on using parallel corpora for semantic transfer. We evaluate an alternative approach using loosely parallel corpora that does not require the corpora to be exact translations of each other. We developed a method that transfers semantic annotations from one language to another using sentences aligned by entities, and we extended it to include alignments by entity-like linguistic units. We conducted our experiments on a large scale using the English, Swedish, and French language editions of Wikipedia. Our results show that the annotation projection using entities in combination with loosely parallel corpora provides a viable approach to extending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained. Introduction Data-driven approaches using natural language processing tackle increasingly complex tasks with ever growing scales and in more varied domains. Semantic role labeling is a type of shallow semantic parsing that is becoming an increasingly important component in information extraction (Christensen et al., 2010) , question answering (Shen and Lapata, 2007) , and text summarization (Khan et al., 2015) . The development of semantic resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) made the training of models for semantic role labelers using supervised techniques possible. However, as a consequence of the considerable manual efforts needed to build proposition banks, they exist only for a few languages. An alternative approach to using supervision is to transfer knowledge between resources, a form of distant or related supervision. Methods for directly projecting semantic labels from a resource-rich language to a resource-scarce one were introduced in Pad\u00f3 (2007) . In this paper, we describe a method for aligning and projecting semantic annotation in loosely parallel corpora by using entities and entity-like linguistic units. Our goal is to generate multilingual PropBanks for resource-scarce languages. We used multiple language editions of Wikipedia: An English edition annotated up to a semantic level using the PropBank semantic roles, and syntactically annotated editions of Swedish and French Wikipedias. By aligning Wikipedias by entities, we constructed loosely parallel corpora and we used them to generate PropBanks in Swedish and French. We provide an evaluation of the quality of the generated PropBanks, together with an evaluation on two external FrameNets. Previous Work As an alternative to using supervised efforts for relation extraction, distant supervision can be employed to transfer relational knowledge representations from one resource to another. Distant supervision for relation extraction was introduced by Craven and Kumlien (1999) in the context of biomedical information extraction. Mintz et al. (2009) describe a method of using an external knowledge base as an indirect way of annotating text. Hoffmann et al. (2010) introduced the usage of Wikipedia infoboxes in distantly supervised relation extraction. The concept of transferring linguistic annotation, in the context of part-of-speech tags, across parallel corpora was introduced in Yarowsky et al. (2001) . Cross-lingual annotation projection of FrameNet semantics has been described by Pad\u00f3 and Lapata (2009) and Basili et al. (2009) . In Van der Plas et al. (2011) , the authors describe an automatic method of direct transfer of PropBank semantics requiring no manual effort. Akbik et al. (2015) describe an approach to generate multilingual PropBanks using filtered annotation projection and bootstrap learning in order to handle errors stemming from translation shifts in corpora. Most previous approaches have used professionally translated parallel corpora, mainly EuroParl (Koehn, 2005) and United Nations Corpora (Rafalovitch and Dale, 2009) , to transfer semantic annotation. However, creating these resources requires manual efforts; they are thus limited in size and in the number of languages they cover. In contrast to parallel corpora, loosely parallel corpora describe similar concepts and events, but are not necessarily the result of a focused effort to translate a large corpus. In Exner et al. (2015) , we introduced the concept of using entities as a method for aligning sentences and transferring semantic content in loosely parallel corpora. However, the presented approach has the following limitations: (1) it was evaluated on one language only and (2) the evaluation was performed on the generated PropBank itself. The contributions of this paper are the following: (1) We extend Exner et al. (2015) by including pronouns and other linguistic units that in a local context exhibit the characteristics of entities. (2) We present and evaluate two methods for aligning sentences by using entities. (3) We demonstrate the effectiveness and generalizability of our approach by projecting semantic annotations to two languages, Swedish and French, and we evaluate it using two external proposition databases, the Swedish SweFN++ (Borin et al., 2010) and French ASFALDA (Candito et al., 2014; Djemaa et al., 2016) that are both semantically-annotated corpora using adaptations of FrameNet frames. (4) We release the source code used in the annotation projection and we provide the generated PropBanks in Swedish and French 1 . Method The aim of the method is to generate PropBank-like resources by fully annotating sentences in target languages using semantic content, in whole or partially, from a source language. We start with loosely parallel corpora in two languages: a source language (SL) expressing the semantic content that we want to transfer to a target language (TL). We then disambiguate and uniquely identify the entities in all the sentences. By using the unique identifier of each entity, we gain the ability to align sentences from two different languages forming sentence pairs (s SL , s T L ). We annotate the (s SL , s T L ) pairs, s SL to semantic and syntactic levels and s T L to a syntactic level. From each (s SL , s T L ) pair, we learn the alignments between predicates (p SL ) in s SL and verbs (v T L ) in s T L . Finally, using the aligned entities and the predicate-verb alignments in each (s SL , s T L ) pair, we transfer the semantic annotation in the form of predicate-argument structures. Figure 1 shows an overview of this approach. Using Loosely Parallel Corpora A prerequisite to projecting semantic annotation between two sentences is that they share the same semantic structure. To this end, we assumed that entities have a constraining property on the sets of predicate-argument structures they can instantiate. By aligning loosely parallel corpora through entities, pairs of sentences in two different languages that we will extract, although they are not translations of each other, should overall express the same semantic content. Furthermore, we believe that by applying our method on a large scale, the most frequent alignments of entities will elicit valid alignments. In this context, even partial semantic content from a source sentence, s SL , may be useful for annotating a target sentence, s T L . As an example, consider the following sentence pair: Figure 1: An overview of the approach for transferring semantic annotation from a source language (SL) to a target language (TL) in which s SL has been aligned with s T L through the entities (the Simpsons, Italy). s SL expresses the two predicates f eature.01(it A0 , Kelsey Grammer A1 ) and visit.01(the f irst time AM \u2212T M P , the Simpsons A0 , Italy A1 ). Although s T L is not an exact translation of s SL , as it lacks the predicate f eatures.01 and the temporal argument (AM -TMP ) of visit.01, the partial transfer of the semantic content enables us to annotate s T L with the predicate bes\u00f6ka.01(f amiljen Simpsons A0 , Italien A1 ). s SL It A0 Entity Disambiguation Entity linking is the process of finding mentions, e.g. persons, cities, organizations, events, concepts, in text, and if available, assign them with a unique identifier provided by a knowledge base. We used Wikidata Q-numbers as identifiers as they provide globally unique identifiers between the different language editions of Wikipedia. As an example, consider the following entities: Beijing, P\u00e9kin, and Pequim as expressed in English, French, and Portuguese respectively. Although they have differing surface forms, they are all linked to the Q956 Wikidata number, as well as in 190 other languages. In total, Wikidata covers a set of more than 13 million items that defines the entity search space. To carry out entity linking, we reimplemented a variant of TagME (Ferragina and Scaiella, 2010) . The motivating factors behind our reimplementation were: 1. It enabled us to resolve mentions to identifiers in Wikidata, providing us with multilingual and coherent entity identifiers. 2. By using the same entity linker for multiple languages, we obtained a more consistent mention resolution across all the languages. 3. It eased the adaptation to new execution environments, in our case a cluster of computing nodes. Our implementation of TagME requires minimal grammatical information as it only needs mention statistics derived from anchors and a dictionary of mention-entity pairs and of incoming links. The entity linking algorithm consists of four steps: detection, candidate voting, selection, and resolution of overlapping mentions. 1. We find all the possible mentions consisting of tokens in sequences up to a maximum length of 6. The mentions found at this stage might be overlapping. We treat overlapped mentions independently and they contribute votes to all the other mentions. As an example, consider the following sentence: Prime Minister of Japan containing the two mentions: Japan and Prime Minister of Japan. In this case, the overlapped mention Japan will contribute a vote to the overlapping mention Prime Minister of Japan. 2. We compute the votes for each candidate belonging to a mention. To bound the computation time, we use voting groups consisting of a collection of mentions using a sliding window approach. The vote weight per candidate is the sum of all the inlink relatedness between all the candidates (Ferragina and Scaiella, 2010) . In our case, we use all the candidates in a voting group. 3. We rank all the candidates per mention using the computed votes. We then prune the mention list using a coherence criterion and a threshold that we set empirically. 4. In the final step, we resolve the mention overlap using a greedy algorithm. The algorithm selects the overlapping mention, where the entity candidate has the largest global vote, removing all the locally overlapping mentions, until there is no overlap globally. Syntactic and Semantic Annotation In our experimental setup, we used the English edition of Wikipedia as our SL, and we annotated it with syntactic and semantic dependencies. For the syntactic-semantic parsing, we used an open-source semantic role labeler (Choi, 2012) trained on OntoNotes 5.0 (Weischedel et al., 2013) . We transferred the semantic annotation to two TLs, the Swedish and French editions of Wikipedia, both annotated with syntactic dependencies. For French syntactic parsing, we applied a transition-based dependency parser (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) trained on a French Treebank described in Candito et al. (2010) . Correspondingly, to preprocess the Swedish edition of Wikipedia, we applied a pipeline consisting of a POS tagger ( \u00d6stling, 2013) and a syntactic dependency parser (Nivre et al., 2006) . Extension to Entity-like Tokens Entities have the property of being uniquely identifiable across languages on a global scope. However, an obvious drawback to using entities as a means of aligning sentences and transferring roles, is that roles are not always instantiated by entities. To reclaim these instances, we extended the entity alignment to include entity-like linguistic units (LU). We focused on units that have the property of being uniquely identifiable and limited to the scope of a sentence pair. Units correspond to sequences of tokens the entity disambiguator has either failed to classify as an entity or otherwise lack the ability to be uniquely identified in a global context. Our algorithm detects entity-like LUs as spans of tokens sharing the same surface form in both s SL and s T L . In addition, we set the constraint that they occur at most once in each sentence. As a consequence, this removes any misalignment issue since a LU in s SL can be matched to only one LU in s T L . This method enables us to include amounts, dates, and noun phrases that the entity disambiguator fails to detect. Using similar constraints, we also include pronouns in the detection of entity-like LUs. However, rather than using the surface form of pronouns, which would unlikely match across languages, we instead categorize them by case, gender, and number. For English, Swedish, and French, third person singular pronouns have different surface forms based on gender. Therefore, in order to increase precision, we limit the detection to only include third person pronouns. Although this constraint certainly limits the recall, this should not significantly impact the training procedure as the pronouns in the first and second persons are in very limited numbers in Wikipedia. Aligning Sentences The first challenge in transferring semantic annotation between loosely parallel corpora is to align sentences expressing the same semantic content. Our baseline method for aligning sentences extracts all the entities from a sentence and forms entity-sentence pairs, (e 1 ...e n , s). By aligning entities in different entity-sentence pairs, we form new triples containing a source sentence, a target sentence, and the subset of entities by which they are aligned (s SL , s T L , e 1 ...e s ), where k min \u2264 s \u2264 k max and k min , k max are prior parameters of our choice. The baseline method in its simplicity, independent of any syntactic or lexical markup. It only requires the annotations from an entity disambiguator. However, one drawback lies in the inclusion of entities ungoverned by any predicate. As a consequence, the alignment of partial semantic content, as described in Sect. 3.1, becomes problematic. We therefore extended this baseline algorithm by using sets of entities projected by either arguments in s SL or a verb in s T L . Using this projection method, we then form entity-sentence pairs: (e 1 ...e p , s), where each entity in (e 1 ...e p ) is governed by an argument belonging to a predicate in s SL and (e 1 ...e v , s), where each entity in (e 1 ...e v ) is governed by a verb in s T L . The method for aligning entities in different entity-sentence pairs remains the same as for the baseline method. In Sect. 4.1, we investigate the effectiveness of the two methods under different settings. Forming Predicate-Verb Alignments Although we use entities as a mechanism to align sentences and transfer predicate-argument roles, predicates in s SL and verbs in s T L cannot be aligned by entities alone. In addition, some sentence pairs contain more than one predicate or verb, sharing the same subset of entities. This creates a combinatorial problem, where one predicate in s SL could possibly be aligned to two or more verbs in s T L , or vice versa. Furthermore, the application of a semantic parser to each s SL annotates each predicate with a sense. This requires a method to induce new predicates and senses for the verbs in s T L . Most previous work relies on word alignments or uses bilingual dictionaries to transfer the predicate annotation between languages. However, when applied to new languages and domains, these approaches face a scaling problem requiring either training on parallel corpora or otherwise dictionaries which may not be available for every language. Our approach builds on Exner et al. (2015) and automatically infers new predicate labels while scaling with the size of corpora and domains. A formal description of our alignment is: 1. We determine all the combinations of predicate-verb pairs, (p i , v k ), extracted from all (s SL , s T L ) pairs, where p i \u2208 s SL and v k \u2208 s T L . 2. We assign count(p i , v k ) as the number of (p i , v k ) in all (s SL , s T L ), where s SL \u2208 SL and s T L \u2208 T L. 3. For each p i \u2208 SL, we form alignments as (p i \u2192 v k ) = max(count(p i , v 1 ), ..., count(p i , v n )). 4. For each v k \u2208 (p i \u2192 v k ), we form a new TL predicate by using the lemma of v k and an incremental counter based on the number of times v k has appeared in an alignment. We select the verb candidates for the alignment using lexical and syntactical rules to filter auxiliary verbs and other non-predicates. Transferring Propositions Given a pair of aligned sentences, (s SL , s T L ), we transfer the semantic annotation from a predicate, p SL \u2208 s SL , to a verb, v T L \u2208 s T L , if (p SL \u2192 v T L ) = max(count(p i \u2192 v T L )), (p i \u2192 v T L ) \u2208 (s SL , s T L ), \u2200p i \u2208 s SL . If a s T L is supervised by more than one s SL , we select the s SL having the larger subset of aligned entities with s T L . We restrict the semantic transfer to predicate-argument structures containing at least one numbered argument and a temporal or location modifying argument, or at least two numbered arguments. We transfer the argument roles by using the aligned entities between s SL and s T L . We assign the argument role to the governing token in the token span covered by each entity. However, if the argument token in s SL is dominated by a preposition, we search for a preposition in s T L governing the entity and assign it the argument role. We obtain the complete argument spans by taking the yield from the argument token. Evaluation In this section, we evaluate the approach described in Sect. 3 and we apply it to three language editions of Wikipedia in order to generate PropBanks for two languages: Swedish and French. The evaluation tries to answer the following questions: 1. How do different parameters and methods affect our approach? 2. What is the quality of the generated PropBanks and what level of performance can we expect in a practical setting? 3. Are there any differences between the languages, and if so what causes them? Experimental Setup For our experimentations, we chose the English, Swedish, and French editions of Wikipedia. These three Wikipedias are all among the top 6 in terms of article counts. As SL, we selected the English edition, and as TLs we select Swedish and French editions. We preprocessed all the articles to filter infoboxes, lists, diagrams, and to keep only text without any markup. Table 1 summarizes the statistics of the linguistic units in our chosen Wikipedias. LANGUAGE TOKENS SENTENCES ENTITIES PREDICATES ARGUMENTS English 3825M 279M 439M 186M 450M Swedish 481M 71M 58M - - French 1269M 74M 181M - - Table 1 : Characteristics of Wikipedias used in the experimental setup Predicate-Verb Alignment We first evaluated how the predicate\u2192verb alignment method described in Sect. 3.6 performs under different conditions and we examined how the number of entities, the method used, and the frequency affect the quality of the alignments. We grouped the English\u2192Swedish alignments by their frequency into three bands: High, medium, and low. We then randomly sampled alignments from each band, in total 100 alignments and we used them to evaluate their precision. We defined precision as the number of English\u2192Swedish alignments that we evaluate as correct divided by the total number of alignments in a sample. Figure 2 shows the precision and number of alignments using different number of entities and methods. We observe that the precision increases with the number of entities used in the alignments. However, this increase is followed by a decrease in the number of alignments created. We also note that in all the alignments, our projection method outperforms our baseline method for aligning sentences in terms of precision. Using three projected entities, we reach a precision of roughly 80% and 1,000 alignments. We also investigated if the higher frequency of an alignment improved precision. Figure 3 shows the breakdown of precision curves into three frequency bands, formed using projected alignments. We observe that using three projected entities, alignments with high-medium frequencies show little to no error. This provides empirical evidence to our hypothesis in Sect. 3.1, that the most frequent alignments of entities will elicit valid alignments and that precision will scale with the amount of data used by the method. The combination of aligning sentences with three projected entities gave us the optimal trade off between precision and number of alignments created. Therefore, in the rest of the evaluation, we use these settings. Generated PropBanks Using the annotation projection methods described in Sect. 3, we generated PropBanks in Swedish and French. We limited the PropBanks to only include fully annotated sentences and we removed the sentences exhibiting parsing errors, such as sentences having more than one syntactic root. We used these generated corpora to perform the error analysis in Sect. 4.5. To evaluate our approach in a practical and automatic setting, we used samples of two linguistic resources: the Swedish FrameNet project (Borin et al., 2010) and the French FrameNet (Candito et al., 2014; Djemaa et al., 2016) . We evaluated the generated Swedish and French corpora on a random sample of 100 sentences, from the Swedish FrameNet and the French FrameNet respectively. As PropBank and FrameNet have different annotation styles, we converted the sampled sentences from frame semantics to the semantics used in PropBank. Table 2 shows the characteristics of the generated PropBanks and the FrameNets used in the evaluations. We evaluated the quality of the generated PropBanks in a practical setting as well as the effectiveness of using entity-like LUs in addition to entities. To assess the usefulness of the generated corpora, we first trained a semantic role labeler (Bj\u00f6rkelund et al., 2010) on them. We split the generated corpora into 60:20:20 training, development, and testing sets, and we ran a selection process using a greedy forward selection and greedy backward elimination procedure to find the optimal set of features (Johansson and Nugues, 2008; Bj\u00f6rkelund et al., 2009) . We then used the trained models to automatically parse the test sets described in Sect. 4.3. Table 3 shows the evaluation of the semantic role labeler trained on the generated corpora. DATASET The performance of the semantic role labeler, trained on the generated PropBanks, compares favorably with the automatic evaluations on parallel corpora described in Pad\u00f3 and Lapata (2009) . For Swedish, using entity-like LUs, we observe an improvement of the labeled F1-measure by 10%. For French, we do not see the same dramatic increase, which we believe is caused by the large differences in pronoun classification and surface forms between English and French. We believe this discrepancy in improvement stems from projecting entity-like LUs across language groups: while English and Swedish belong to the Germanic branch, French belongs to the Romance group. Although more investigation is needed, these early results suggest that the annotation projection using entity-like LUs is most efficient when applied within a language group. LABELED Error Analysis To understand the quality of the generated PropBanks, we conducted an analysis of the predicate and argument errors. We randomly sampled 200 errors, of which 100 errors stemmed from the incorrect projection of argument labels and 100 were incorrect projections of predicates. Tables 4 and 5 show the type of errors for predicates and arguments respectively. Using loosely parallel corpora, it is no surprise that the largest group of errors in predicate projection stems from sentences expressing differing semantic content. This error comes from sentence pairs, that although they contain the same subset of entities, express differing semantic content. However, as shown in Sect. 4.2, the precision of alignments increases with the number of alignments, leading us to believe that this category of error can be corrected using more data. The second largest error group is formed by different types of parsing errors occurring during the preprocessing stage. Encouragingly, only 6% of predicate projection errors stem from translation shifts, which is a further indication that entities exhibit a constraining property on the types of predicates that can instantiate them, even across languages. Looking at argument projection errors, we again notice a group of errors stemming from misaligned sentences in loosely parallel corpora, Differing Semantic Content and No Source Equivalent. Looking beyond, alignment errors due to argument labels being assigned to the wrong token is the single most frequent error. The second largest category of errors is composed of expressions that can not be considered as entities, e.g. In other words and During this time. Finally, we observed a class of error stemming from entities undergoing a shift in specificity across sentences in two languages. These translation shifts included entities being referred to by their name in one language and by their entity type in the other language, e.g. London\u2192the city. Conclusion In this paper, we have described the construction of multilingual PropBanks by aligning loosely parallel corpora using entities. We have trained a semantic role labeler on the generated PropBanks and that we evaluated in a practical setting on frame-annotated corpora. Our results compares favorably to annotation transfer using parallel corpora. In addition, we have extended the entity alignment to include alignment by entity-like linguistic units such as pronouns and dates. We believe the growing source of loosely parallel corpora and their alignment using entities offers an alternative way to creating multilingual hand-annotated corpora. By performing a semantic projection on loosely parallel corpora, in our case multiple language editions of Wikipedia, we have presented an alternative approach to using parallel corpora. We believe our approach can be extended beyond encyclopedias to similar resources, such as news articles in multiple languages describing the same events. One future improvement could be to leverage ontologies that categorize entities into types. We believe that such ontologies would prove useful in adjusting the specificity of entities in order to handle some translation shifts across languages. In addition, our current method of forming predicate\u2192verb alignments could be extended by including information about the entity type. While projecting pronouns from English to Swedish showed an improvement, we did not observe the same improvement when projecting from English to French. Therefore, an additional avenue of investigation could compare the performance of annotation projection within versus across language groups. In addition, a coreference solver could provide an alternative means of resolving pronominal mentions to entities. Acknowledgements This research was supported by Vetenskapsr\u00e5det under grant 621-2010-4800, and the Det digitaliserade samh\u00e4llet and eSSENCE programs.",
    "abstract": "In this paper, we investigate the annotation projection of semantic units in a practical setting. Previous approaches have focused on using parallel corpora for semantic transfer. We evaluate an alternative approach using loosely parallel corpora that does not require the corpora to be exact translations of each other. We developed a method that transfers semantic annotations from one language to another using sentences aligned by entities, and we extended it to include alignments by entity-like linguistic units. We conducted our experiments on a large scale using the English, Swedish, and French language editions of Wikipedia. Our results show that the annotation projection using entities in combination with loosely parallel corpora provides a viable approach to extending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained.",
    "countries": [
        "Sweden"
    ],
    "languages": [
        "Swedish",
        "French"
    ],
    "numcitedby": "2",
    "year": "2016",
    "month": "December",
    "title": "Multilingual Supervision of Semantic Annotation"
}