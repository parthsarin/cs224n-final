{
    "article": "Recent natural language learning research has shown that structural kernels can be effectively used to induce accurate models of linguistic phenomena. In this paper, we show that the above properties hold on a novel task related to predicate argument classification. A tree kernel for selecting the subtrees which encodes argument structures is applied. Experiments with Support Vector Machines on large data sets (i.e. the PropBank collection) show that such kernel improves the recognition of argument boundaries. Introduction The design of features for natural language processing tasks is, in general, a critical problem. The inherent complexity of linguistic phenomena, often characterized by structured data, makes difficult to find effective linear feature representations for the target learning models. In many cases, the traditional feature selection techniques (Kohavi and Sommerfield, 1995) are not so useful since the critical problem relates to feature generation rather than selection. For example, the design of features for a natural language syntactic parse-tree re-ranking problem (Collins, 2000) cannot be carried out without a deep knowledge about automatic syntactic parsing. The modeling of syntactic/semantic based features should take into account linguistic aspects to detect the interesting con-text, e.g. the ancestor nodes or the semantic dependencies (Toutanova et al., 2004) . A viable alternative has been proposed in (Collins and Duffy, 2002) , where convolution kernels were used to implicitly define a tree substructure space. The selection of the relevant structural features was left to the voted perceptron learning algorithm. Another interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al., 2004) . The good results show that tree kernels are very promising for automatic feature engineering, especially when the available knowledge about the phenomenon is limited. Along the same line, automatic learning tasks that rely on syntactic information may take advantage of a tree kernel approach. One of such tasks is the automatic boundary detection of predicate arguments of the kind defined in PropBank (Kingsbury and Palmer, 2002) . For this purpose, given a predicate p in a sentence s, we can define the notion of predicate argument spanning trees (P AST s) as those syntactic subtrees of s which exactly cover all and only the p's arguments (see Section 4.1). The set of nonspanning trees can be then associated with all the remaining subtrees of s. An automatic classifier which recognizes the spanning trees can potentially be used to detect the predicate argument boundaries. Unfortunately, the application of such classifier to all possible sentence subtrees would require an exponential execution time. As a consequence, we can use it only to decide for a reduced set of subtrees associated with a corresponding set of candidate boundaries. Notice how these can be detected by previous approaches (e.g. (Pradhan et al., 2004) ) in which a traditional boundary classifier (tbc) labels the parse-tree nodes as potential arguments (PA). Such classifiers, generally, are not sensitive to the overall argument structure. On the contrary, a P AST classifier (past c ) can consider the overall argument structure encoded in the associated subtree. This is induced by the PA subsets. The feature design for the P AST representation is not simple. Tree kernels are a viable alternative that allows the learning algorithm to measure the similarity between two P AST s in term of all possible tree substructures. In this paper, we designed and experimented a boundary classifier for predicate argument labeling based on two phases: (1) a first annotation of potential arguments by using a high recall tbc and (2) a P AST classification step aiming to select the correct substructures associated with potential arguments. Both classifiers are based on Support Vector Machines learning. The past c uses the tree kernel function defined in (Collins and Duffy, 2002) . The results show that the P AST classification can be learned with high accuracy (the f-measure is about 89%) and the impact on the overall boundary detection accuracy is good. In the remainder of this paper, Section 2 introduces the Semantic Role Labeling problem along with the boundary detection subtask. Section 3 defines the SVMs using the linear kernel and the parse tree kernel for boundary detection. Section 4 describes our boundary detection algorithm. Section 5 shows the preliminary comparative results between the traditional and the two-step boundary detection. Finally, Section 7 summarizes the conclusions. Automated Semantic Role Labeling One of the largest resources of manually annotated predicate argument structures has been developed in the PropBank (PB) project. The PB corpus contains 300,000 words annotated with predicative information on top of the Penn Treebank 2 Wall Street Journal texts. For any given predicate, the expected arguments are labeled sequentially from Arg0 to Arg9, ArgA and ArgM. Figure 1 shows an example of the PB predicate annotation of the sentence: John rented a room in Boston. Predicates in PB are only embodied by verbs whereas most of the times Arg0 is the subject, Arg1 is the direct object and ArgM indicates locations, as in our example. Several machine learning approaches for automatic predicate argument extraction have been developed, e.g. (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003; Pradhan et al., 2004) . Their common characteristic is the adoption of feature spaces that model predicateargument structures in a flat feature representation. In the next section, we present the common parse tree-based approach to this problem. Predicate Argument Extraction Given a sentence in natural language, all the predicates associated with the verbs have to be identified along with their arguments. This problem is usually divided in two subtasks: (a) the detection of the target argument boundaries, i.e. the span of its words in the sentence, and (b) the classification of the argument type, e.g. Arg0 or ArgM in PropBank or Agent and Goal in FrameNet. The standard approach to learn both the detection and the classification of predicate arguments is summarized by the following steps: 1. Given a sentence from the training-set, generate a full syntactic parse-tree; 2. let P and A be the set of predicates and the set of parse-tree nodes (i.e. the potential arguments), respectively; 3. for each pair < p, a >\u2208 P \u00d7 A: \u2022 extract the feature representation set, F p,a ; \u2022 if the subtree rooted in a covers exactly the words of one argument of p, put F p,a in T + (positive examples), otherwise put it in T \u2212 (negative examples). For instance, in Figure 1 , for each combination of the predicate rent with the nodes N, S, VP, V, NP, PP, D or IN the instances F rent,a are generated. In case the node a exactly covers \"John\", \"a room\" or \"in Boston\", it will be a positive instance otherwise it will be a negative one, e.g. F rent,IN . The T + and T \u2212 sets are used to train the boundary classifier. To train the multi-class classifier T + can be reorganized as positive T + arg i and negative T \u2212 arg i examples for each argument i. In this way, an individual ONE-vs-ALL classifier for each argument i can be trained. We adopted this solution, according to (Pradhan et al., 2004) , since it is simple and effective. In the classification phase, given an unseen sentence, all its F p,a are generated and classified by each individual classifier C i . The argument associated with the maximum among the scores provided by the individual classifiers is eventually selected. Standard feature space The discovery of relevant features is, as usual, a complex task. However, there is a common consensus on the set of basic features. These standard features, firstly proposed in (Gildea and Jurasfky, 2002) , refer to unstructured information derived from parse trees, i.e. Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice. For example, the Phrase Type indicates the syntactic type of the phrase labeled as a predicate argument, e.g. NP for Arg1 in Figure 1 . The Parse Tree Path contains the path in the parse tree between the predicate and the argument phrase, expressed as a sequence of nonterminal labels linked by direction (up or down) symbols, e.g. V \u2191 VP \u2193 NP for Arg1 in Figure 1 . The Predicate Word is the surface form of the verbal predicate, e.g. rent for all arguments. In the next section we describe the SVM approach and the basic kernel theory for the predicate argument classification. Learning predicate structures via Support Vector Machines Given a vector space in n and a set of positive and negative points, SVMs classify vectors according to a separating hyperplane, H( x) = w \u00d7 x + b = 0, where w \u2208 n and b \u2208 are learned by applying the Structural Risk Minimization principle (Vapnik, 1995) . To apply the SVM algorithm to Predicate Argument Classification, we need a function \u03c6 : F \u2192 n to map our features space F = {f 1 , .., f |F | } and our predicate/argument pair representation, F p,a = F z , into n , such that: F z \u2192 \u03c6(F z ) = (\u03c6 1 (F z ), .., \u03c6 n (F z )) From the kernel theory we have that: H( x) = i=1..l \u03b1 i x i \u2022 x + b = i=1..l \u03b1 i x i \u2022 x + b = i=1..l \u03b1 i \u03c6(F i ) \u2022 \u03c6(F z ) + b. where, F i \u2200i \u2208 {1, .., l} are the training instances and the product K(F i , F z ) =<\u03c6(F i ) \u2022 \u03c6(F z )> is the kernel function associated with the mapping \u03c6. The simplest mapping that we can apply is \u03c6(F z ) = z = (z 1 , ..., z n ) where z i = 1 if f i \u2208 F z and z i = 0 otherwise, i.e. the characteristic vector of the set F z with respect to F. If we choose the scalar product as a kernel function we obtain the lin- ear kernel K L (F x , F z ) = x \u2022 z. An interesting property is that we do not need to evaluate the \u03c6 function to compute the above vector. Only the K( x, z) values are in fact required. This allows us to derive efficient classifiers in a huge (possible infinite) feature space, provided that the kernel is processed in an efficient way. This property is also exploited to design convolution kernel like those based on tree structures. The tree kernel function The main idea of the tree kernels is the modeling of a K T (T 1 , T 2 ) function which computes the number of common substructures between two trees T 1 and T 2 . Given the set of substructures (fragments) {f 1 , f 2 , ..} = F extracted from all the trees of the training set, we define the indicator function I i (n) which is equal 1 if the target f i is rooted at node n and 0 otherwise. It follows that: K T (T 1 , T 2 ) = n 1 \u2208N T 1 n 2 \u2208N T 2 \u2206(n 1 , n 2 ) (1) where N T 1 and N T 2 are the sets of the T 1 's and T 2 's nodes, respectively and \u2206(n 1 , n 2 ) = |F | i=1 I i (n 1 )I i (n 2 ). This latter is equal to the number of common fragments rooted at the n 1 and n 2 nodes. We can compute \u2206 as follows: 1. if the productions at n 1 and n 2 are different then \u2206(n 1 , n 2 ) = 0; 2. if the productions at n 1 and n 2 are the same, and n 1 and n 2 have only leaf children (i.e. they are pre-terminals symbols) then \u2206(n 1 , n 2 ) = 1; 3. if the productions at n 1 and n 2 are the same, and n 1 and n 2 are not pre-terminals then \u2206(n 1 , n 2 ) = nc(n 1 ) j=1 (1 + \u2206(c j n 1 , c j n 2 )) (2) where nc(n 1 ) is the number of the children of n 1 and c j n is the j-th child of the node n. Note that, as the productions are the same, nc(n 1 ) = nc(n 2 ). The above kernel has the drawback of assigning higher weights to larger structures 1 . In order to overcome this problem we scale the relative importance of the tree fragments imposing a parameter \u03bb in conditions 2 and 3 as follows: \u2206(n x , n z ) = \u03bb and \u2206(n x , n z ) = \u03bb nc(nx) j=1 (1 + \u2206(c j n 1 , c j n 2 )). 1 In order to approach this problem and to map similarity scores in the [0,1] range, a normalization in the kernel space, i.e. K T (T 1 , T 2 ) = K T (T 1 ,T 2 ) \u221a K T (T 1 ,T 1 )\u00d7K T (T 2 ,T 2 ) . is always applied Boundary detection via argument spanning Section 2 has shown that traditional argument boundary classifiers rely only on features extracted from the current potential argument node. In order to take into account a complete argument structure information, the classifier should select a set of parse-tree nodes and consider them as potential arguments of the target predicate. The number of all possible subsets is exponential in the number of the parse-tree nodes of the sentence, thus, we need to cut the search space. For such purpose, a traditional boundary classifier can be applied to select the set of potential arguments PA. The reduced number of PA subsets can be associated with sentence subtrees which in turn can be classified by using tree kernel functions. These measure if a subtree is compatible or not with the subtree of a correct predicate argument structure. The Predicate Argument Spanning Trees (P AST s) We consider the predicate argument structures annotated in PropBank along with the corresponding TreeBank data as our object space. Given the target predicate p in a sentence parse tree T and a subset s = {n 1 , .., n k } of the T's nodes, N T , we define as the spanning tree root r the lowest common ancestor of n 1 , .., n k . The node spanning tree (N ST ), p s is the subtree rooted in r, from which the nodes that are neither ancestors nor descendants of any n i are removed. Since predicate arguments are associated with tree nodes, we can define the predicate argu- ment spanning tree (P AST ) of a predicate argument set, {a 1 , .., a n }, as the N ST over such nodes, i.e. p {a 1 ,..,a n } . A P AST corresponds to the minimal subparse tree whose leaves are all and only the word sequence compounding the arguments. For example, Figure 2 shows the parse tree of the sentence \"John took the book and read its title\". took {ARG 0 ,ARG 1 } and read {ARG 0 ,ARG 1 } are two P AST structures associated with the two predicates took and read, respectively. All the other N ST s are not valid P AST s. Notice that, labeling p s , \u2200s \u2286 N T with a P AST classifier (past c ) corresponds to solve the boundary problem. The critical points for the application of this strategy are: (1) how to design suitable features for the P AST characterization. This new problem requires a careful linguistic investigation about the significant properties of the argument spanning trees and (2) how to deal with the exponential number of N ST s. For the first problem, the use of tree kernels over the P AST s can be an alternative to the manual features design as the learning machine, (e.g. SVMs) can select the most relevant features from a high dimensional feature space. In other words, we can use Eq. 1 to estimate the similarity between two P AST s avoiding to define explicit features. The same idea has been successfully applied to the parse-tree reranking task (Taskar et al., 2004; Collins and Duffy, 2002) and predicate argument classification (Moschitti, 2004) . For the second problem, i.e. the high computational complexity, we can cut the search space by us-ing a traditional boundary classifier (tbc), e.g. (Pradhan et al., 2004) , which provides a small set of potential argument nodes. Let PA be the set of nodes located by tbc as arguments. We may consider the set P of the N ST s associated with any subset of PA, i.e. P = {p s : s \u2286 PA}. However, also the classification of P may be computationally problematic since theoretically there are |P| = 2 |PA| members. In order to have a very efficient procedure, we applied past c to only the PA sets associated with incorrect P AST s. A way to detect such incorrect N ST s is to look for a node pair <n 1 , n 2 >\u2208 PA \u00d7 PA of overlapping nodes, i.e. n 1 is ancestor of n 2 or viceversa. After we have detected such nodes, we create two node sets P A 1 = PA \u2212 {n 1 } and P A 2 = PA \u2212 {n 2 } and classify them with the past c to select the correct set of argument boundaries. This procedure can be generalized to a set of overlapping nodes O greater than 2 as reported in Appendix 1. Note that the algorithm selects a maximal set of non-overlapping nodes, i.e. the first that is generated. Additionally, the worst case is rather rare thus the algorithm is very fast on average. The Figure 3 shows a working example of the multi-stage classifier. In Frame (a), tbc labels as potential arguments (gray color) three overlapping nodes (in Arg.1). The overlap resolution algorithm proposes two solutions (Frame (b)) of which only one is correct. In fact, according to the second solution the propositional phrase \"of the book\" would incorrectly be attached to the verbal predicate, i.e. in contrast with the parse tree. The past c , applied to the two N ST s, should detect this inconsistency and provide the correct output. Note that, during the learning, we generate the non-overlapping structures in the same way to derive the positive and negative examples. Engineering Tree Fragment Features In the Frame (b) of Figure 3 , we show one of the possible cases which past c should deal with. The critical problem is that the two N ST s are perfectly identical, thus, it is not possible to discern between them using only their parse-tree fragments. The solution to engineer novel features is to simply add the boundary information provided by the tbc to the N ST s. We mark with a progressive number the phrase type corresponding to an argument node, starting from the leftmost argument. For example, in the first N ST of Frame (c), we mark as NP-0 and NP-1 the first and second argument nodes whereas in the second N ST we have an hypothesis of three arguments on the NP, NP and PP nodes. We trasform them in NP-0, NP-1 and PP-2. This simple modification enables the tree kernel to generate features useful to distinguish between two identical parse trees associated with different argument structures. In order to verify the relevance of our model, the next section provides empirical evidence about the effectiveness of our approach. The Experiments The experiments were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999) . For tbc, we used the linear kernel with a regularization parameter (option -c) equal to 1 and a cost-factor (option -j) of 10 to have a higher Recall. For the past c we used \u03bb = 0.4 (see (Moschitti, 2004) ). As referring dataset, we used the PropBank cor-pora available at www.cis.upenn.edu/\u223cace, along with the Penn TreeBank 2 (www.cis.upenn.edu/\u223ctreebank) (Marcus et al., 1993) . This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches, e.g. (Pradhan et al., 2004; Gildea and Palmer, 2002) . We did not include continuation and co-referring arguments in our experiments. We used sections from 02 to 07 (54,443 argument nodes and 1,343,046 non-argument nodes) to train the traditional boundary classifier (tbc). Then, we applied it to classify the sections from 08 to 21 (125,443 argument nodes vs. 3,010,673 nonargument nodes). As results we obtained 2,988 N ST s containing at least an overlapping node pair out of the total 65,212 predicate structures (according to the tbc decisions). From the 2,988 overlapping structures we extracted 3,624 positive and 4,461 negative N ST s, that we used to train the past c . The performance was evaluated with the F 1 measure 2 over the section 23. This contains 10,406 argument nodes out of 249,879 parse tree nodes. By applying the tbc classifier we derived 235 overlapping N ST s, from which we extracted 204 P AST s and 385 incorrect predicate argument structures. On such test data, the performance of past c was very high, i.e. 87.08% in Precision and 89.22% in Recall. Using the past c we removed from the tbc the PA that cause overlaps. To measure the impact on the boundary identification performance, we compared it with three different boundary classification baselines: \u2022 tbc: overlaps are ignored and no decision is taken. This provides an upper bound for the recall as no potential argument is rejected for later labeling. Notice that, in presence of overlapping nodes, the sentence cannot be annotated correctly. \u2022 RN D: one among the non-overlapping structures with maximal number of arguments is randomly selected. \u2022 Heu (heuristic): one of the N ST s which contain the nodes with the lowest overlapping score is chosen. This score counts the number of overlapping node pairs in the N ST . For example, in Figure 3 .(a) we have a NP that overlaps with two nodes NP and PP, thus it is assigned a score of 2. The third row of Table 1 shows the results of tbc, tbc + RN D, tbc + Heu and tbc + past c in the columns 2,3,4 and 5, respectively. We note that: \u2022 The tbc F 1 is slightly higher than the result obtained in (Pradhan et al., 2004) rithm) . This is explained by the fact that we did not include the continuations and the coreferring arguments that are more difficult to detect. \u2022 Both RN D and Heu do not improve the tbc result. This can be explained by observing that in the 50% of the cases a correct node is removed. \u2022 When, to select the correct node, the past c is used, the F 1 increases of 1.49%, i.e. (96.86 vs. 95.37) . This is a very good result considering that to increase the very high baseline of tbc is hard. In order to give a fairer evaluation of our approach we tested the above classifiers on the overlapping structures only, i.e. we measured the past c improvement on all and only the structures that required its application. Such reduced test set contains 642 argument nodes and 15,408 non-argument nodes. The fourth row of Table 1 reports the classifier performance on such task. We note that the past c improves the other heuristics of about 20%. Related Work Recently, many kernels for natural language applications have been designed. In what follows, we highlight their difference and properties. The tree kernel used in this article was proposed in (Collins and Duffy, 2002) for syntactic parsing reranking. It was experimented with the Voted Perceptron and was shown to improve the syntactic parsing. A refinement of such technique was presented in (Taskar et al., 2004) . The substructures produced by the proposed tree kernel were bound to local properties of the target parse tree and more lexical information was added to the overall kernel function. In (Zelenko et al., 2003) , two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation. To measure the similarity between two nodes, the contiguous string kernel and the sparse string kernel (Lodhi et al., 2000) were used. The former can be reduced to the contiguous substring kernel whereas the latter can be transformed in the non-contiguous string kernel. The high running time complexity, caused by the general form of the fragments, limited the experiments on data-set of just 200 news items. In (Cumby and Roth, 2003) , it is proposed a description language that models feature descriptors to generate different feature type. The descriptors, which are quantified logical prepositions, are instantiated by means of a concept graph which encodes the structural data. In the case of relation extraction the concept graph is associated with a syntactic shallow parse and the extracted propositional features express fragments of a such syntactic structure. The experiments over the named entity class categorization show that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy. In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. The major novelty was the combination of the contiguous and sparse kernels with the word kernel. The results show that the contiguous outperforms the sparse kernel and the bag-of-words. Conclusions The feature design for new natural language learning tasks is difficult. We can take advantage from the kernel methods to model our intuitive knowledge about the target linguistic phenomenon. In this paper we have shown that we can exploit the properties of tree kernels to engineer syntactic features for the predicate argument boundary detection task. Preliminary results on gold standard trees suggest that (1) the information related to the whole predicate argument structure is important and (2) tree kernel can be used to generate syntactic features. In the future, we would like to use an approach similar to the P AST classifier on parses provided by different parsing models to detect boundary and to classify semantic role more accurately . Acknowledgements We wish to thank Ana-Maria Giuglea for her help in the design and implementation of the basic Semantic Role Labeling system that we used in the experiments. Appendix 1: Generalized Boundary Selection Algorithm Let O be the set of overlapping nodes of PA, and N O the set of non overlapping nodes of PA. Let subs (\u22121)",
    "abstract": "Recent natural language learning research has shown that structural kernels can be effectively used to induce accurate models of linguistic phenomena. In this paper, we show that the above properties hold on a novel task related to predicate argument classification. A tree kernel for selecting the subtrees which encodes argument structures is applied. Experiments with Support Vector Machines on large data sets (i.e. the PropBank collection) show that such kernel improves the recognition of argument boundaries.",
    "countries": [
        "Italy"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "18",
    "year": "2005",
    "month": "June",
    "title": "Engineering of Syntactic Features for Shallow Semantic Parsing"
}