{
    "framing_question": "To what extent do the authors engage with the communities that are the subject of their paper, and do they claim a participatory approach?",
    "response": "The authors do not mention engaging with communities that are involved in Text Simplification (TS) in their paper. Therefore, a participatory approach is not claimed or implied.",
    "article": "The lack of large and reliable datasets has been hindering progress in Text Simplification (TS). We investigate the application of the recently created Newsela corpus, the largest collection of professionally written simplifications available, in TS tasks. Using new alignment algorithms, we extract 550, 644 complex-simple sentence pairs from the corpus. This data is explored in different ways: (i) we show that traditional readability metrics capture surprisingly well the different complexity levels in this corpus, (ii) we build machine learning models to classify sentences into complex vs. simple and to predict complexity levels that outperform their respective baselines, (iii) we introduce a lexical simplifier that uses the corpus to generate candidate simplifications and outperforms the state of the art approaches, and (iv) we show that the corpus can be used to learn sentence simplification patterns in more effective ways than corpora used in previous work. Introduction Text Simplification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999) , Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006) , and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010) . Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011) . This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010) , tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013) , integer programming techniques (Woodsend and Lapata, 2011) , and discriminative linear models (Bach et al., 2011) . In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012) , Amancio and Specia (2014) and Xu et al. (2015) , it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011) ) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resource that allegedly addresses these limitations: the Newsela corpus (Newsela, 2016) . Unlike Simple Wikipedia, the Newsela corpus was created by professional editors and targets a specific audience (students), which should make it a more reliable resource for TS. However, the Newsela corpus has only recently started to be exploited for this task and not enough work has been done to understand its potential. In this paper, we investigate whether (Xu et al., 2015) 's claims hold in practice. We produce sentence alignments for the Newsela corpus (Section 2.) and conduct experiments to evaluate its effectiveness in TS tasks, namely: readability analysis (Section 4.), complex vs. simple classification, complexity level prediction, lexical simplification and MT-based sentence simplification (Section 5.). Aligning the Newsela Corpus The Newsela corpus (version 2016-01-29.1) is composed of 10, 787 news articles in English, which includes 1, 911 articles in their original form as well as in 4 or 5 versions rewritten by humans to suit different reading reading levels. Each document is characterised by a unique identifier, a version identifier (from 0 -most complex to 5 -simplest), and a reading level from 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d) , which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible paragraph/sentence alignments ( \u0160tajner et al., 2017) , while still offering comparable alignment accuracy. The result of the alignment is a corpus with 19, 198 pairs of articles aligned at both paragraph (300, 475 pairs) and sentence (550, 644 pairs) levels. This is over three times larger than the Wikipedia-Simple Wikipedia corpus (Coster and Kauchak, 2011) , making it the largest corpus of its kind. Columns 2 to 4 in Table 1 illustrate the number of paragraph and sentence alignments for all version pairs in the corpus. We categorise the sentence alignments according to four types of simplification: \u2022 None: Complex and simple sentences are identical (146, 251 pairs). On average, sentence lengths remain close for adjacent levels (e.g. 25.0 & 24.4 for levels 0-1), but sentences become shorter for higher levels (12.5 & 11.1 at levels 4-5). This shows that editors significantly compress text while simplifying. Related Work Xu et al. (2015) are the first to present an analysis of the Newsela corpus. They compare the Newsela and Wikipedia-Simple Wikipedia data using several metrics, showing that the Newsela corpus appears to be more useful. However, they do not use it in any tasks (e.g. lexical simplification) like we propose in this paper. Besides proposing alignment algorithms for the Newsela corpus, \u0160tajner et al. ( 2017 ) also build MT-based models with the aligned data. As test set, instead of using part of Newsela data, the Wikipedia-Simple Wikipedia dataset proposed by Xu et al. (2016) is used. Two out of three systems trained with Newsela aligned data perform better in terms of simplicity than state-of-the-art systems for the same corpus. Zhang and Lapata ( 2017 ) train an attention-based encoderdecoder model (Bahdanau et al., 2014) and use reinforcement learning with a reward policy combining SARI (to measure simplicity) (Xu et al., 2016) , BLEU (to measure grammaticality) (Papineni et al., 2002) and cosine similarity (to measure meaning preservation). This approach shows improvements over a model trained using a phrasebased MT approach in terms of BLEU and SARI. Alva-Manchego et al. (2017) propose a TS model that uses predicted simplification operations. Simplification operations automatically annotated in source sentences are predicted as a first step, using sequence labelling techniques. As operations, they model only replace and delete. Their TS model produces better results according to human judgements for simplicity than general-purpose MT-based models. In general, the aforementioned contributions explore MTbased techniques and train systems using the Newsela data in similar ways as it was done previously for Wikipedia-Simple Wikipedia data. However, none of them provide an analysis of the Newsela data in terms of readability of the aligned data, the use of the data for complex vs. simple classification or complexity level prediction, or the impact of the data in state-of-the-art LS approaches. Corpus Analysis We analyse the sentence-aligned Newsela corpus to (i) understand the differences between its various levels of simplification, and (ii) investigate how existing readability and psycholinguistic metrics fair in distinguishing these levels. Edit Rate This analysis focuses on the differences in edits between the various simplified versions. We use TER 1 as a metric of edit distance, as it is widely used for this purpose in MT evaluation. Columns 5 and 6 of Table 1 show the percentage of alignments with TER = 0 (\"% None\") and averaged TER. As expected, the non-adjacent versions have higher TER values, e.g. the distance between levels '0' and '1' is much smaller than the distance between levels '0' and '5'. The percentage of sentences with no edits decreases as we move from adjacent to non-adjacent levels. Interestingly, between the adjacent levels, the closer to the original level, the lower the TER, e.g. there are fewer edits between '0' and '1' than between '1' and '2'. Readability Metrics Here we evaluate standard readability metrics that aggregate shallow text information (such as number of syllables and words): Flesch Reading Ease, Flesch-Kincaid Grade Level, SMOG Index, Gunning Fog Index, Automated Readability Index, Coleman-Liau Index, Linsear Write Formula and Dale-Chall Readability Score from the TEXTSTAT toolkit 2 . Figure 1 shows the Flesch Reading Ease box plot for pairs of original and simplified sentences with level '0' as original version. Flesch varies from 0 (most complex) to 100 (simplest). The Flesch index for the simplified versions is higher than for the original version in all cases, which is an expected behaviour. Therefore, although the simplified versions of the Newsela corpus can be composed of more and/or longer sentences than the original, the information encoded in them is still simpler. A similar trend is observed for all other readability metrics and between all levels. For completion, we also show in Figure 2 the box plot for the Flesch-Kincaid Grade Level metric for 0-to-n original/simplified pairs. As expected, simplified versions have lower Flesch-Kincaid scores. Figure 1 : Flesch Reading Ease for simplified versions from level '0' \u2022 Mean age of acquisition / familiarity / imageability / concreteness score of content words For sentences without content words, the correspondent features were assigned zero. We extracted age of acquisition, familiarity, imageability and concreteness features from the bootstrapped MRC database (Paetzold and Specia, 2016b) , which is an extended version of the original MRC database (Coltheart, 1981) . Original sentences have a higher number of words, tokens, letters and syllables, as expected. Type/token ratio and number of content words are not considerably different between original and simplified versions. Figures 3 and 4 show the box plots for the ratio between number of letter and number words and the ration between number of syllables and number words, respectively, when '0' is the original level. Simplified versions have a higher ratio of letters per words and a lower ratio of syllables per words, when compared to original versions. It appears that even though simplified sentences have slightly longer words, such words have fewer syllables, which is often a sign of simplicity. Figure 3 : Ratio between number of letters and number of words for simplified versions from level '0' Figure 4 : Ratio between number of syllables and number of words for simplified versions from level '0' Figures 5 and 6 show the box plots for age of acquisition and imageability metrics, when '0' is the original. Age of acquisition aims to define the age at which a given word is learned, whilst imageability refers to the mental capability of retrieving an image, given a word. As expected, simplified sentences show lower values for age of acquisition and higher values for imageability than their original counterparts. Familiarity (the frequency to which a word is seen, heard or spoken daily) and concreteness did not show differences between simplified and original sentences. Using the Corpus in TS Tasks Complex vs. Simple Classification Here we present sentence-level binary classifiers created for all possible combinations of levels of simplification. Sentences from the more complex version were assigned the label \"complex\", while their simpler counterpart (at any level), the label \"simple\". For sentences pairs whose TER is 0 (no simplification made), both original and simplified sentences were considered \"simple\". We trained Stochastic Gradient Descent (SGD) classifiers (with hinge loss function) using the scikit-learn toolkit (Pe- dregosa et al., 2011) with hyperparameters optmised using grid search. As features we used the nine readability metrics from the TEXTSTAT toolkit and the 12 psycholinguistic features, mentioned in Section 4.. Three models were built: one with the readability metrics only, one with the psycholinguistic metrics only and another with both. The classifiers were evaluated by using 10-fold cross-validation. As a baseline, we used a majority class classifier (Majority). Table 2 shows the results for each complex-simple level. As expected, the classifiers built for non-adjacent levels achieve better performance than those for adjacent levels (in terms of F-measure). The opposite behaviour is ob-served for the majority class models. This is expected, however, since the number original/simplified pairs whose TER is 0 is much larger in adjacent levels than in non-adjacent levels, leading to more biased (and hence easier to predict) instances. It can also be noticed that the precision and Fmeasure of the classifiers follow the degree of difference between the complex and simple levels, as shown by TER (Table 1 ). Recall, on the other hand, is higher for adjacent levels. This is most likely caused also by the large number of sentences considered simple because of no changes in TER (% None in Table 1 ). All classifiers outperform the majority class baseline and the best classifiers use the combination of both types of metrics as feature. Complexity Level Prediction Here we directly predict the level of complexity of a sentence. These are defined as 2-12 reading proficiency levels, as explained in Section 2.: the higher the level, more complex the text. We use the same feature sets as in Section 5.1., but mix all sentences to build a single model and use the Ridge Regression algorithm. We evaluate the model in terms of Mean Absolute Error (MAE). As baseline, we considered the MAE obtained from applying the mean complexity level of the training set as the \"prediction\" for all instances in the test set. MAE values of 1.793, 1.962 and 1.715 were obtained for models built with readability, psycholinguistic and all features, respectively. The mean baseline MAE was of 2.247, and therefore all models outperformed the baseline, with the best model using all features.  Here we assess the potential of our corpus in LS. LS is commonly addressed as a pipeline of steps: candidates for a target complex word are produced via a Substitution Generation (SG) method, filtered with respect to the context of the complex word via a Substitution Selection (SS) method, and finally ordered for simplicity by a Substitution Ranking (SR) method. We use our aligned corpus for SG following the state of the art approach in (Horn et al., 2014) . First, we produce word alignments using Meteor (Denkowski and Lavie, 2011) and extract complex-to-simple word correspondences. Then we filter word pairs with different POS tags, where the complex word is a stop word, or either word is a proper noun. Finally, we generate all possible inflections for nouns and verbs (Burns, 2013) . We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a) : the Horn generator (Horn et al., 2014) , which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998) , Biran (Biran et al., 2011) , Glavas (Glava\u0161 and \u0160tajner, 2015) and Paetzold (Paetzold and Specia, 2016c) generators, which exploit WordNet, comparable complex-to-simple documents, typical word embeddings and context-aware word embeddings, respectively. All generators were implemented with the LEXenstein framework (Paetzold and Specia, 2015) . We use the BenchLS dataset as our gold-standard dataset (Paetzold and Specia, 2016a) . It is the largest dataset of its kind, with 929 instances, each composed by a sentence, a target complex word, and a set of gold substitutions given by humans. To compare the generators, we use standard metrics: Potential -the proportion of instances in which at least one of the candidates generated is in the gold-standard, Precision -the proportion of generated substitutions that are in the gold-standard, Recall -the proportion of goldstandard substitutions that are among the generated substitutions, and F1. Table 3 reveals that our approach achieves the highest Precision overall, as well as higher Potential and F1 scores. We also evaluated our generator in practice through a full pipeline evaluation, where the output is the best lexical simplification for each complex word. To do so, we paired all aforementioned generators with three state of the art SR strategies: Lexical Simplification \u2022 Familiarity (Paetzold and Specia, 2016b): Ranks candidates according to their word familiarity scores, as extracted from the bootstrapped MRC database. \u2022 Glavas (Glava\u0161 and \u0160tajner, 2015) : Ranks candidates according to various features, then obtains a final ranking for a candidate by averaging the ranks of said fea-tures. \u2022 Paetzold (Paetzold and Specia, 2015) : Learns a ranking model from a binary classification setup. All rankers were implemented with the LEXenstein framework with features and settings as in (Paetzold and Specia, 2016a) . The gold-standard test set used is also BenchLS, and the metric is Accuracy: the ratio with which the highest ranking candidate is not the target word itself and is among the gold-standard candidates. Table 4 shows that our generator outperformed all others with any ranking method, highlighting the potential of the Newsela corpus for LS. Sentence Simplification Simplification can be addressed as a \"translation approach\" (Shardlow, 2014) . This approach requires a large enough sentence-aligned complex-simple corpus and a method to learn simplification rules, such as off-the-shelf Statistical Machine Translation (SMT) toolkits. We experiment with the aligned Newsela corpus following an SMT-like pipeline, using only the adjacent levels of simplification (0-1, 1-2, 2-3, 3-4 and 4-5) (278.184 sentences). For comparison, we build a model using the Wikipedia-Simple Wikipedia sentence-aligned corpus (167, 689 sentences). Both datasets were divided in approximately 70% for training, 10% for development and 20% for test. Additionally, we also built a model using a subset of the Newsela dataset containing the same number of sentence pairs as the Wikipedia-Simple Wikipedia dataset, i.e. the training and development sets were reduced via random sampling to the size of the Wikipedia-Simple Wikipedia dataset. We train standard MOSES toolkit (Koehn et al., 2007) with default configurations. Table 5 shows the evaluation of the simplification systems built in terms of BLEU, SARI and Flesch Ease Index. In this table \"full\" refers to the model trained with the entire Newsela dataset and \"wiki size\" refers to the model trained with the portion of the Newsela dataset with the same size as the Wikipedia-Simple Wikipedia dataset. BLEU scores are higher for the Newsela trained system in both \"full\" and \"wiki size\" settings, which can indicate that the models trained with Newsela data are producing outputs more grammatically correct than the model trained with Wikipedia-Simple Wikipedia data. SARI, however, shows that models built with Wikipedia-Simple Wikipedia data seems to be producing slightly simpler outputs. The Flesch index for simplified sentences (FLESCH-S) is lower than that of the reference sentences (FLESCH-R) for both corpora. This seems to reflect the fact that automatic simplifications are closer to the original (FLESCH-O) than to the reference. We also experiment with variants of the test set where only sentences with at least one edit (TER > 0) or with no edits at all (TER = 0) are used. Sentences with TER = 0 should not be modified as they are already simple, and thus the MT output should be exactly the same as the reference (and the original). This is often a problem in SMT-based simplification approaches, which tend to over simplify and introduce noise. The Newsela trained models are still the best in terms of BLEU, while show slightly smaller SARI. However, as the results for all TER = 0 suggest, SARI is Table 5 : Results for SMT-based simplifiers not a reliable metric when original, reference and simplified sentences are the same. For all cases where TER = 0, the SARI value was 0.330, which can be seem as a low value if the systems are producing an output equal to the reference. Since this metric was designed for cases where sentences should also be simplified (as explained in Xu et al. (2016) ), the use of SARI for cases where the original sentences are already simple is not reliable. Conclusions Upon studying the sentence-aligned Newsela corpus we found that: (i) it follows an expected TER distribution, with the lowest TER being between adjacent levels; (ii) the simplified sentences score as more readable than their original counterparts according to traditional readability metrics, and (iii) the corpus proved a more reliable source of complex-simple correspondences for LS and MT-based simplification than the Wikipedia-Simple Wikipedia corpus. We achieve some the highest performance to date when generating candidate substitutions for complex words as well as when applying these into a full LS pipeline. Improvements for MT-based simplification using the Newsela corpus are also observed but more in depth (manual) evaluation is needed for these experiments. In the future, we hope that the aligned corpus will lead to better data-driven approaches to TS. We cannot release the aligned Newsela corpus, but it can be recreated using MAS-SAlign 4 (Paetzold et al., 2017) , which provides the alignment algorithm used. Acknowledgements This work was supported by the EC project SIMPATICO (H2020-EURO-6-2015, grant number 692819).",
    "funding": {
        "military": 0.0,
        "corporate": 0.0,
        "research agency": 0.014957662992269638,
        "foundation": 2.2603645533747496e-05,
        "none": 0.9999740587314805
    }
}