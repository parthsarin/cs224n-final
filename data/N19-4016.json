{
    "article": "Story composition is a challenging problem for machines and even for humans. We present a neural narrative generation system that interacts with humans to generate stories. Our system has different levels of human interaction, which enables us to understand at what stage of story-writing human collaboration is most productive, both to improving story quality and human engagement in the writing process. We compare different varieties of interaction in story-writing, story-planning, and diversity controls under time constraints, and show that increased types of human collaboration at both planning and writing stages results in a 10-50% improvement in story quality as compared to less interactive baselines. We also show an accompanying increase in user engagement and satisfaction with stories as compared to our own less interactive systems and to previous turn-taking approaches to interaction. Finally, we find that humans tasked with collaboratively improving a particular characteristic of a story are in fact able to do so, which has implications for future uses of human-in-the-loop systems. Introduction Collaborative human-machine story-writing has had a recent resurgence of attention from the research community (Roemmele and Swanson.,  2017; Clark and Smith, 2018). It represents a frontier for AI research; as a research community we have developed convincing NLP systems for some generative tasks like machine translation, but lag behind in creative areas like open-domain storytelling. Collaborative open-domain storytelling incorporates human interactivity for one of two aims: to improve human creativity via the aid of a machine, or to improve machine quality via the aid of a human. Previously existing approaches treat the former aim, and have shown that storytelling systems are not yet developed enough to help human writers. We attempt the latter, with the goal of investigating at what stage human collaboration is most helpful. Swanson and Gordon (2009) use an information retrieval based system to write by alternating turns between a human and their system. Clark  and Smith (2018) use a similar turn-taking approach to interactivity, but employ a neural model for generation and allow the user to edit the generated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. Roemmele and Swanson. ( 2017 ) experiment with a userdriven setup, where the machine doesn't generate until the user requests it to, and then the user can edit or delete at will. They leverage useracceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful generation. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user's ability to knit a coherent overall story together out of locally related sentences. They also do not allow a user to control the novelty or \"unexpectedness\" of the generations, which Clark and  Smith (2018) find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sentences and have the system update later generations. We develop a system 1 that allows a user to interact in all of these ways that were limitations in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged. We have two main interfaces that enable hu-  man interaction with the computer. There is crossmodel interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories. The full range of interactions available to a user is: select a model, provide a topic, change diversity of content, collaborate on the planning for the story, and collaborate on the story sentences. It is entirely user-driven, as the users control how much is their own work and how much is the machine's at every stage. It supports revision; a user can modify an earlier part of a written story or of the story plan at any point, and observe how this affects later generations. System Description System Overview Figure 1 shows a diagram of the interaction system. The dotted arrows represent optional user interactions. Cross-model mode requires the user to enter a topic, such as \"the not so haunted house\", and can optionally vary the diversity used in the STORY-LINE PLANNER or the STORY WRITER. Diversity numbers correspond directly to softmax temperatures, which we restrict to a reasonable range, determined empirically. The settings are sent to the STORYLINE PLANNER module, which generates a storyline for the story in the form of a sequence of phrases as per the method of Yao et al. (2019). Everything is then sent to the STORY WRITER, which will return three stories. Intra-model mode enables advanced interactions with one story system of the user's choice. The STORYLINE PLANNER returns either one storyline phrase or many, and composes the final storyline out of the combination of phrases the system generated, the user has written, and edits the user has made. These are sent to the STORY WRITER, which returns either a single sentence or a full story as per user's request. The process is flexible and iterative. The user can choose how much or little content they want to provide, edit, or re-generate, and they can return to any step at any time until they decide they are done. Pre-/Post-processing and OOV handling To enable interactive flexibility, the system must handle open-domain user input. User input is lowercased and tokenized to match the model training data via spaCy 2 . Model output is naively detokenized via Moses (Koehn et al., 2007) based on feedback from users that this was more natural. User input OOV handling is done via WordNet (Miller, 1995) by recursively searching for hypernyms and hyponyms (in that order) until either an in-vocabulary word is found or until a maximum distance from the initial word is reached. 3 We additionally experimented with using cosine similarity to GloVe vectors (Pennington et al., 2014), but found that to be slower and not qualitatively better for this domain. Web Interface Figure 2 shows screenshots for both the crossmodel and intra-model modes of interaction. Figure 2a shows that the cross-model mode makes clear the differences between different model generations for the same topic. Figure 2b shows the variety of interactions a user can take in intra-model interaction, and is annotated with an example-in-action. User inserted text is underlined in blue, generated text that has been removed by the user is in grey strike-through. The refresh symbol marks areas that the user re-generated to get a different sentence (presumably after being unhappy with the first result). As can be seen in this example, minor user involvement can result in a significantly better story.  The Title-to-Story system is a baseline, which generates directly from topic. The Plan-and-Write system adopts the static model in Yao et al. (2019) to use the storyline to supervise story-writing. Plan-and-Revise is a new system that combines the strengths of Yao et al. (2019) and Holtzman et al. (2018). It supplements the Plan-and-Write model by training two discriminators on the ROC data and using them to re-rank the LSTM generations to prefer increased creativity and relevance. 4 Thus the decoding objective of this system becomes f \u03bb (x, y) = log(P lm (y|x)) + k \u03bb k s k (x, y) where P lm is the conditional language model probability of the LSTM, s k is the discriminator scoring function, and \u03bb k is the learned weight of that discriminator. At each timestep all live beam hypotheses are scored and re-ranked. Discriminator weights are learnt by minimizing Mean Squared Error on the difference between the scores of gold standard and generated story sentences. Experiments We experiment with six types of interaction: five variations created by restricting different capabilities of our system, and a sixth turn-taking baseline that mimics the interaction of the previous work (Clark and Smith, 2018; Swanson and Gordon, 2009). We choose our experiments to address the research questions: What type of interaction is most engaging? Which type results in the best stories? Can a human tasked with correcting for certain weaknesses of a model successfully do so? The variations on interactions that we tested are: 1. Machine only: no human-in-loop. 2. Diversity only: user can compare and select models but only diversity is modifiable. 3. Storyline only: user collaborates on storyline but not story. 4. Story only: user collaborates on story but not storyline. 5. All: user can modify everything. 6. Turn-taking: user and machine take turns writing a sentence each (user starts). user can edit the machine-generations, but once they move on to later sentences, previous sentences are read-only. 5 We expand experiment 5 to answer the question of whether a human-in-the-loop interactive sys-tem can address specific shortcomings of generated stories. We identify three types of weaknesses common to generation systems -Creativity, Relevance, and Causal & Temporal Coherence, and conduct experiments where the human is instructed to focus on improving specifically one of them. The targeted human improvement areas intentionally match the Plan-and-Revise discriminators, so that, if successful, the \"human discriminator\" data can assist in training the machine discriminators. All experiments (save experiment 2, which lets the user pick between models) use the Plan-and-Revise system. Details We recruit 30 Mechanical Turk workers per experiment (270 unique workers total) to complete story writing tasks with the system. 6 We constrain them to ten minutes of work (five for writing and five for a survey) and provide them with a fixed topic to control this factor across experiments. They co-create a story and complete a questionnaire which asks them to self-report on their engagement, satisfaction, and perception of story quality. 7 For the additional focused errorcorrection experiments, we instruct Turkers to try to improve the machine-generated stories with regard to the given aspect, under the same time constraints. As an incentive, they are given a small bonus if they are later judged to have succeeded. We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis. Results User Engagement Self-reported scores are relatively high across the board, as can be seen in Table 1, with the majority of users in all experiments saying they would like to use the system again. The lower scores in the Diversity only and Storyline only experiments are elucidated by qualitative comments from users of frustration at the inability to sufficiently control the generations with influence over only those tools. Storyline only is low- 6 We enforce uniqueness to prevent confounding effects from varying levels of familiarity with the demo UI est for Use Again, which can be explained by the model behavior when dealing with unlikely storyline phrases. Usually, the most probable generated story will contain all storyline phrases (exact or similar embeddings) in order, but there is no mechanism that strictly enforces this. When a storyline phrase is uncommon, the story model will often ignore it. Many users expressed frustration at the irregularity of their ability to guide the model when collaborating on the storyline, for this reason. Users were engaged by collaboration; all experiments received high scores on being entertaining, with the collaborative experiments rated more highly than Diversity only. The pattern is repeated for the other scores, with users being more satisfied and feeling their stories to be higher quality for all the more interactive experiments. The Turntaking baseline fits into this pattern; users prefer it more than the less interactive Diversity only and Storyline only, but often (though not always) less than the more interactive Story only, All, All+ experiments. Interestingly, user perception of the quality of their stories does not align well with independent rankings. Self-reported quality is low in the Story only experiment, which contrasts with it being highest rated independently (as discussed below). Self-reported scores also suggest that users judge their stories to be much better when they have been focusing on causal-temporal coherence, though this focus carries over to a smaller improvement in independent rankings. While it is clear that additional interactivity is a good idea, the disjunct between user perception of their writing and reader perception under different experiment conditions is worthwhile to consider for future interactive systems. Story Quality As shown in Table 2 , human involvement of any kind under tight constraints helps story quality across all metrics, with mostly better results the more collaboration is allowed. The exception to this trend is Story only collaboration, which performs best or close to best across the board. This was unexpected; it is possible that these users benefited from having to learn to control only one model, instead of both, given the limited time. It is also possible that being forced to be reliant on system storylines made these users more creative. Turn-taking Baseline The turn-taking baseline performs comparably in overall quality and relevance to other equally interactive experiments (Story only, All, All+). It achieves highest scores in relevance, though the top five systems for relevance are not statistically significantly different. It is outperformed on creativity and causal-temporal coherence by the strong Story only variation, as well as the All, All+ systems. This suggests that local sentence-level editing is sufficient to keep a story on topic and to write well, but that creativity and causal-temporal coherence require some degree of global cohesion that is assisted by iterative editing. The same observation as to the strength of Story only over All applies here as well; turntaking is the least complex of the interactive systems, and may have boosted performance from being simpler since time was constrained and users used the system only once. Thus a turn-based system is a good choice for a scenario where users use a system infrequently or only once, but the comparative performance may decrease in future experiments with more relaxed time constraints or where users use the system repeatedly. Targeted Improvements The results within the All and All + setups confirm that stories can be im-proved with respect to a particular metric. The diagonal of strong scores displays this trend, where the creativity-focused experiment has high creativity, etc. An interesting side effect to note is that focusing on anything tends to produce better stories, reflected by higher overall ratings. All + Relevance is an exception which does not help creativity or overall (perhaps because relevance instantly becomes very high as soon a human is involved), but apart from that All + experiments are better across all metrics than All. This could mean a few things: that when a user improves a story in one aspect, they improve it along the other axes, or that users reading stories have trouble rating aspects entirely independently. Conclusions and Future Work We have shown that all levels of human-computer collaboration improve story quality across all metrics, compared to a baseline computer-only story generation system. We have also shown that flexible interaction, which allows the user to return to edit earlier text, improves the specific metrics of creativity and causal-temporal coherence above previous rigid turn-taking approaches. We find that, as well as improving story quality, more interaction makes users more engaged and likely to use the system again. Users tasked with collaborating to improve a specific story quality were able to do so, as judged by independent readers. As the demo system has successfully used an ensemble of collaborative discriminators to improve the same qualities that untrained human users were able to improve even further, this suggests promising future research into humancollaborative stories as training data for new discriminators. It could be used both to strengthen existing discriminators and to develop novel ones, since discriminators are extensible to arbitrarily many story aspects. A Demo Video The three-minute video demonstrating the interaction capabilities of the system can be viewed at https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote). B Training and Decoding Parameters B.1 Decoding Default diversity (Softmax Temperature) for Storyline Planner is 0.5, for Story Writer it is None (as beamsearch is used an thus can have but does not require a temperature). Beam size for all Story Writer models is 5. Additionally, Storyline Phrases are constrained to be unique (unless a user duplicates them), and Beamsearch is not normalized by length (both choices determined empirically). B.2 Training We follow the parameters used in Yao et al. (2019)  and Merity et al. (2018). C.2 Mechanical Turk Materials Following are examples of the materials used in doing Mechanical Turk User Studies. Figure 3 is an example of the All + Creative focused experiment for story-writing. The instructions per experiment differ across all, but the template is the same. Figure 4 is the survey for ranking stories across various metrics. This remains constant save that story order was shuffled every time to control for any effects of the order a story was read in. Please take a few minutes to read the instructions for our website and get used to the interface. It should be quick and will help you get a quality bonus. The objective is for you to take about five minutes and cowrite a story with our system and try to improve the Creativity of the story. Our system works by generating a storyline, and then a story based on it, and you collaborate with it. If your final story is judged to have improved Creativity, you will get a bonus. Note: We need unique Workers, so you are only allowed to do one of these. Please do not autoaccept the next one. Only five sentences. Please do collaborate as well if you just write a story yourself we will know and reject the HIT. This is an example of a story: bobby and his friends were fascinated by the dark. they dared each other to get close to a haunted house. bobby heard a noise coming from the window. he ran to the house to see what it was. it was a scary, scary house. Steps: Survey Instructions We are a group of researchers conducting research about storytelling. In this survey, you will be provided with a title and eight stories about that title. Please compare and rank them according to the given criteria. 1. Please read all the stories carefully, and give a score to each story based on: Relevance, Creativity, Overall Quality, and Event Coherence (we will explain these). 2. Multiple stories can receive the same score. However, please do read all the story versions before rating them and consider them in relation to each other. 3. Briefly explain why you made the decisions you did for each story. Just basic thoughts or bullet points is fine, but this is required. 4. Don't worry about punctuation and spelling of the stories, those don't matter 5. We do review every HIT response, and will reject it if your answers make no sense and you don't give any reasoning. Explanation of metrics: Relevance: Is the story relevant to the title, i.e. is it on topic? Creativity: Is the story unusual and interesting, in either vocabulary or content? Overall Quality: How good do you think the story is? Event Coherence: Do the things that happen in the story make sense together and are they in the right order? For example, for the sentences: \"the ice cream tasted delicious. she tasted the ice cream\" the events make sense, but are in the wrong order and should get a lower rating. For the sentences: \"jim's boss yelled at him. jim had a great day\" the ordering is fine but the events don't go together. Events that go together and are in the right order should have a higher rating, as in: \"jim's boss yelled at him. jim went home exhausted and unhappy.\" An Example: title: haunted house a) bobby and his friends were fascinated by the dark. they dared each other to get close to a haunted house. bobby heard a noise coming from the window. he ran to the house to see what it was. it was a scary, scary house. The story was on topic, and made sense, and was good, but the last sentence didn't really fit perfectly with earlier events. Acknowledgments We thank the anonymous reviewers for their feedback, as well as the members of the PLUS lab for their thoughts and iterative testing. This work is supported by Contract W911NF-15-1-0543 with the US Defense Advanced Research Projects Agency (DARPA).",
    "abstract": "Story composition is a challenging problem for machines and even for humans. We present a neural narrative generation system that interacts with humans to generate stories. Our system has different levels of human interaction, which enables us to understand at what stage of story-writing human collaboration is most productive, both to improving story quality and human engagement in the writing process. We compare different varieties of interaction in story-writing, story-planning, and diversity controls under time constraints, and show that increased types of human collaboration at both planning and writing stages results in a 10-50% improvement in story quality as compared to less interactive baselines. We also show an accompanying increase in user engagement and satisfaction with stories as compared to our own less interactive systems and to previous turn-taking approaches to interaction. Finally, we find that humans tasked with collaboratively improving a particular characteristic of a story are in fact able to do so, which has implications for future uses of human-in-the-loop systems.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "37",
    "year": "2019",
    "month": "June",
    "title": "Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation"
}