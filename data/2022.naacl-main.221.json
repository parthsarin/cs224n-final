{
    "article": "Warning: this paper contains content that may be offensive or upsetting. Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective datacentric baselines. Our results on English data across four platforms show that distinct spurious artifacts require different treatments to ultimately attain both robustness and fairness in hate speech detection. To encourage research in this direction, we release all baseline models and the code to compute artifacts, pointing it out as a complementary and necessary addition to the data statements practice. Introduction Hate speech in online social communities is a serious and pervasive concern, which requires fair and robust automated approaches to be tackled at scale. However, despite the great progress in natural language processing for detecting hate speech, current models have shown to be brittle when applied to real-world data, exhibiting limited out-ofdistribution (OOD) robustness (Vidgen et al., 2019) and perpetuating and amplifying harmful social biases (R\u00f6ttger et al., 2021) . Noticeably, hate speech detection systems are typically trained on data from limited language varieties such as individual platforms, which inevitably exhibit differences in writing norms, language use, and hate targets, hampering generalization (Vidgen and Derczynski, 2020) . One of the main reasons for limited robustness and fairness of mainstream hate speech detection 1 Code and resources are available at https://github. com/dhfbk/hate-speech-artifacts. fair robust what have jews done to you? RT [user] : I'm mad at this All black people literally go there Table 1 : Posts wrongly labeled as hateful by a finetuned BERT classifier due to the presence of spurious lexical artifacts (identity and non identity-related) and their negative impact () on fairness and robustness. systems is largely ascribable to spurious statistical correlations between surface lexical items and labels in training data, which models exploit to derive predictions. These biases are commonly referred to as lexical dataset artifacts, and have recently attracted attention in the NLP community, particularly in natural language inference (NLI) studies (Belinkov et al., 2019; Gururangan et al., 2018; Poliak et al., 2018, inter alia) . Efforts to tackle the issue in hate speech detection are instead rather scattered, and mainly focus on fairness using datasets from few platforms (Zhou et al., 2021; Kennedy et al., 2020b, inter alia) , leaving the study on OOD robustness largely unexplored. We instead argue that fairness and robustness are strongly intertwined aspects (Table 1 ), and thus should be studied jointly, with the goal to understand to what extent these two dimensions are related. Previous work has shown that state-of-the-art models overly rely on identity words (e.g., \"jews\", \"gay\") to predict hateful content (Zhou et al., 2021; Kennedy et al., 2020b, inter alia) , further demoting voices of people from already marginalized groups (Bender et al., 2021) . However, non identity-related lexical items -such as \"sport\", \"announcer\", and \"football\" in Waseem and Hovy (2016) -are also often spuriously associated with hate speech due to a biased data collection process (Wiegand et al., 2019) , undermining OOD robustness. Despite the recent trend in minimizing topic bias in data sampling, we show that some spurious lexical artifacts still remain highly-predictive on certain distributions even if data has been sampled in a more attentive fashion (e.g., artifacts that are potentially data-or platform-specific -Figure 1 , highlighted in gray). We argue that disentangling artifacts into finegrained categories by means of a cross-platform analysis may be beneficial to drive a broader debiasing of current hate speech models, ultimately improving both fairness and robustness to out-ofdistribution data. To this purpose, we critically analyze artifacts in hate speech detection across multiple platforms and propose simple yet effective data-centric baselines exploiting spurious lexical items. We show that although we achieve substantial improvements in OOD fairness by exploiting spurious identity-related artifacts, this comes at the cost of robustness. This confirms that fairness and robustness are strictly interrelated dimensions that should be studied together in future research. Contributions To the best of our knowledge, we are the first to (i) conduct a thorough investigation of lexical artifacts across online platforms; (ii) disentangle artifacts into fine-grained categories; and (iii) propose a viable data-centric approach based on masking that consistently improves fairness over all baselines across all platforms. To foster future research on the topic, we also release (iv) code to reproduce all experiments, and (v) disaggregated lexical artifact annotations, more broadly (vi) suggesting the inclusion of dataset artifacts in data statements (Bender and Friedman, 2018) , which can be easily revealed using our codebase. Lexical Artifacts are not all the Same We conceptualize dataset artifacts at the lexical level as emergent correlations between tokens and labels in input data, consistently to lexical annotation artifacts in NLI (Gururangan et al., 2018) . As such, given a target class c, we formally define lexical artifacts L c as the set of highly-discriminating 2 tokens for c, which comprise authentic artifacts A c -items that potentially carry useful information for the class at hand -and spurious artifacts S c -items that are spuriously (or undesirably) associated to the target class -such that L c = A c \u222a S c . In the context of hate speech detection, we consider the hateful class as c unless otherwise specified and simplify the notation (i.e., from \"\u2022 c \" to \"\u2022\"). We build our definitions on top of the categories of lexical biases by Zhou et al. (2021) , which originally identify three bias groups: i) minority identity mentions which are not offensive, ii) minority identity mentions which are potentially offensive, and iii) non-identity mentions which are possibly offensive. We enrich this categorization by introducing a high-level separation into spurious (i.e., group i) in Zhou et al. (2021) ) and authentic artifacts (i.e., group ii) and iii)), and including an additional spurious, non identity-related category (Section 2.2). Indeed, given the broad nature of authentic and spurious artifacts, we further categorize them in Section 2.1 and 2.2 (see Figure 1 for an overview). Authentic artifacts We define authentic lexical artifacts A as the subset of highly-discriminating tokens which potentially convey hatefulness, profanity, or are otherwise frequently associated with hateful contexts. Intuitively, A is the set of artifacts which is likely to be informative to detect hate speech across distributions. Authentic artifacts enclose minority identityrelated artifacts A I and non-identity artifacts A \u00acI . Identity-related (A I ) Potentially offensive or stereotyping terms towards minority identities (e.g., \"n*gro\", \"f*ggot\", \"k*ke\", \"wh*re\"), as well as reclaimed slurs (e.g., \"n*gga\") (Figure 1 , top left). Non-identity related (A \u00acI ) Swear words and profanities (e.g., \"f*ck\", \"sh*t\") as well as broad terms typically associated with hateful contexts (e.g., \"kill\", \"idiots\") (Figure 1 , bottom right). Spurious artifacts Spurious lexical artifacts S broadly enclose all tokens which we do not expect to be predictive for the target class at hand. As such, we postulate that those artifacts are a main reason for insufficient robustness and fairness of current hate speech detectors, and thus may play a positive role in lexical debiasing. We specifically focus on these artifacts in our experiments. As for authentic artifacts, spurious items can be grouped into minority identityrelated artifacts S I and non-identity artifacts S \u00acI , the latter being currently disregarded in research investigating fairness only (Zhou et al., 2021) . Identity-related (S I ) Terms describing a social minority, which are typically associated to hate speech due to their frequency on offensive statements on online fora (e.g., \"muslim\", \"woman\", \"Islam\", \"nigerian\", \"LGBT\") (Figure 1 , top right). Non-identity related (S \u00acI ) All non-identity tokens which are unexpectedly associated to hate speech, e.g., due to platform-specificity, bias in collection timeframe, etc. (e.g., \"people\", \"RT\", \"streets\", \"Trump\", \"yeah\") (Figure 1 , bottom left). Data In this work we focus on hate speech, i.e., messages whose content spreads hatred or incites violence, or threatens people's freedom, dignity and safety, and whose target is a protected group, or an individual targeted for belonging to such a group and not for his/her individual characteristics (Poletto et al., 2021) . Hate speech typically encompasses serious cases of offense with severe moral and legal implications, i.e., those cases that are of primary importance for content moderation. We collect hate speech corpora that meet the following criteria: (i) they minimize topic and author biases in data collection (Wiegand et al., 2019) , using alternatives to keyword and user searches such as pure or boosted random sampling, (ii) they pertain to different social media platforms, and (iii) they follow similar annotation guidelines, where hate speech is clearly defined and separated from other types of offensive language. For each corpus we create hateful and non-hateful examples. All datasets follow consistent preprocessing, deduplication, and anonymization (Appendices A.1 and A.2). REDDIT ( ) We use the recently introduced Reddit dataset (v1.1) by Vidgen et al. (2021) which preserves a variety of grammar, topic, and style features due to a community-based sampling approach. The corpus contains 27,494 entries annotated following a hate speech taxonomy comprising abusive (identity-directed, affiliation-directed, persondirected) and non-abusive labels (non-hateful slurs, counter speech, and neutral). We follow the widely accepted definition of hate speech as \"abuse targeting a protected group or its members for being a part of that group\" 3 (R\u00f6ttger et al., 2021; Banko et al., 2020; Vidgen et al., 2019, inter alia) to create the hateful label from identity-directed examples, and the non-hateful label from the remaining examples. For the purpose of this study, we discard instances marked as requiring previous content to be interpreted. 4 The final dataset after preprocessing consists of 1,688 hateful and 19,888 non-hateful examples, for a total of 21,576 unique instances. TWITTER ( ) We select a widely used hate speech dataset which has been collected following a bootstrap random sampling approach (Founta et al., 2018) . The dataset consists of 99,996 tweets annotated as hateful, abusive, spam, and normal. Similarly to previous work, we discard the spam category (Zhou et al., 2021) , forming the hateful class following the original classification provided by the authors. This led to 3,937 hateful and 70,554 non-hateful examples, for a total of 74,491 tweets. GAB ( ) We use the GAB hate corpus by Kennedy et al. (2020a) , whose data has been sampled purely randomly due to the frequency of hate speech of the \"free speech-preserving\" (Zannettou et al., 2018) GAB social network. The corpus (v.2021-03-03) consists of 27,546 posts annotated with (assault on) human dignity, call for violence, and vulgarity/offensive labels. Similarly to previous work, we take the union of human dignity and call for violence labels for the hateful class (Kennedy et al., 2020b) , whereas we create the non-hateful class from the remaining examples. We also leverage target annotations and consider messages towards ideology/political groups as nonhateful, to ensure consistency among datasets. As a result, the final dataset is made up of 27,014 messages: 1,785 hateful and 24,829 non-hateful. STORMFRONT ( \u0253 ) We use the dataset pertaining to a white supremacist web forum collected by de Gibert et al. ( 2018 ) following a random sampling procedure. It consists of a total of 10,944 messages with annotations for hate, no-hate, relation, and skip labels. We remove relation and skip examples, since they require previous context, or they represent spam / content written in other languages, respectively. We then use the hate examples for the hateful class, and the no-hate instances for the non-hateful class. This led to a total of 10,448 examples, 1,192 hateful and 9,256 normal. Disentangling Lexical Artifacts In order to disentangle lexical artifacts, we first compute the correlation between each token and the hateful class for each dataset (Section 4.1), then assessing the cross-distribution indicativeness of each token (Section 4.2). For segmenting texts into tokens, we rely on the training portion of each dataset only (Section 5) and employ the WordPiece (Schuster and Nakajima, 2012) subword tokenizer as used in BERT (Devlin et al., 2019) . 5 Finally, we perform lexical artifacts annotation (Section 4.3) following the categories defined in Section 2. In-distribution artifacts We follow Gururangan et al. ( 2018 ) and employ pointwise mutual information (PMI; Fano, 1961) to compute the discriminativeness of each token to the target class. 6 Since lexical artifacts are meant to be used for downstream debiasing, we argue that tokens should be consistent with inputs to the end model. As a result, we use tokens as given by the WordPiece subword tokenizer, the same tokenizer used by models employed in our experiments (Section 5). Formally, given a token t and a class c, the PMI is defined as follows: PMI(t, c) = log p(t, c) p(t|\u2022)p(\u2022|c) (1) We further apply reweighting to emphasize highly-discriminative token-class correlations, and normalize \u2264 0 values to zero since negative PMI scores are known to be unreliable on relatively small corpora (Jurafsky and Martin, 2021, Ch. 6 ). Discussion The top 10 tokens on each platform that are more associated with the hateful class are presented in Table 2 (left). All platforms exhibit a variety of lexical artifact types (cf. Section 2); however, we observe clear divergences across distributions. While artifacts in Stormfront data are mainly related to race, on Gab the focus is more on religion. Reddit and Twitter conversations are instead more varied, with higher occurrence of spurious, non-identity artifacts (e.g., \"RT\", \"people\"). Cross-distribution artifacts When datasets from multiple platforms are available, we hypothesize that leveraging individual scores makes possible to better identify artifacts. Given PMI(t, c) d the score of a token t for the hateful class c on a given distribution d \u2208 D (e.g., platform), we normalize it in [0, 1] by applying a min-max normalization function to enable cross-platform score comparability -obtaining PMI(t, c) d [0,1] -further applying a log 2 transformation to mitigate the skewness of the original PMI distribution. As a result, the final cross-distribution score S(t, c) for each token is given by the average of the corresponding individual scores: S(t, c) = 1 |D| D d=1 log 2 (PMI(t, c) d [0,1] ) (2) We then sort tokens by descending score, highlighting lexical artifacts that are highly discriminating for the hateful class across distributions. Table 2 (right) shows the top 10 tokens after the cross-platform computation is carried out. Discussion As shown in Table 2 (right), crossplatform importance of tokens for the hateful class demotes scores (and thus, ranks) of lexical artifacts which are likely to be more indicative on some platforms only (e.g., \"RT\"), while consolidating the informativeness of cross-platform items (e.g., \"jews\", \"hate\", \"##s\", \"##es\", 7 \"people\"). This confirms our hypothesis that encompassing multiple platforms is beneficial for capturing lexical items that are likely to be predictive across distributions. Artifacts annotation In order to disentangle lexical artifacts for further debiasing, we select the k most predictive tokens given by the cross-distribution rank of discriminativeness (Section 4.2) to be manually annotated. 8  In our experiments, we set k = 200 as it matches the subset of tokens which are highly informative (\u2265 0.33). 9 All k tokens have been labeled as potentially hateful and/or related to minority identities by two annotators -male and female, fluent in English -with background in linguistics and NLP, and past experience in hate speech activities with NGOs. Each annotator was provided with five examples of tokens in context for enabling more informed annotation decisions, represented by randomly sampled posts from the four platforms included in this study. After annotation, the two annotators were involved in an adjudication session in order to discuss the cases of disagreement, followed by correction wherever possible. We calculate the inter-annotator agreement (IAA) score before and after adjudication using Cohen's kappa (Cohen, 1960) . We obtain \u03ba = 0.6887 before and \u03ba = 0.8311 after the adjudication session, which is high agreement. Discussion Although some cases of disagreement were easily resolvable (e.g., annotation errors), we found tokens which are difficult to discern due to ambiguity -mostly subwords -or due to real disagreement in the interpretation of the terms. This is in line with existing works showing that disagreement in toxicity annotation is inherent to the task and cannot always be solved through majority voting or adjudication (Aroyo et al., Figure 2 : Cumulative Cohen's kappa (\u03ba) scores for the full annotation of lexical artifacts (top), and for decisions on potentially hateful or identity-related artifacts only (middle and bottom, respectively), ordered by informativeness according to cross-distribution scores. Leonardelli et al., 2021) . Interestingly, this disagreement follows the trend of cross-distribution rank of artifacts (Figure 2 ). We decided to leave the analysis of annotators' disagreement for future work, and we release these cases as disaggregated labels. In Table 3 previous work (Zhou et al., 2021) . We outline the experimental setup in Section 5.1, whereas data-centric baselines are presented in Section 5.2. Lastly, we present results and a thorough discussion in Section 5.3. Experimental setup For all our experiments, we employ the uncased BERT-base model (Devlin et al., 2019) as implemented in the MaChAmp v0.2 toolkit (van der Goot et al., 2021), since it has been shown to achieve state-of-the-art performance in hate speech detection (Tran et al., 2020) . We use default hyperparameters, and perform a grid search to determine the number of epochs, the learning rate, and the batch size, using the search space suggested by Devlin et al. (2019) -i.e., [2, 3, 4] for epochs, [2e-05, 3e-05, 5e-05] for learning rate, and [16, 32] for batch size. We use stratified 80% train, 10% development, 10% test splits for each dataset, selecting the best model based on the average macro F 1 score on the development test across all platforms. During fine-tuning, we emphasize the minority hateful class using a cross-entropy loss with balanced class weights. The final hyperparameters are: 4 for epochs, 2e-05 for learning rate, and 16 for batch size. All experiments have been run on a NVIDIA Tesla V100-SXM2 GPU, with a training time ranging from 10 to 40 minutes each. The number of trainable parameter for all models are \u2248110M. Baselines We investigate the impact of spurious identityrelated and non identity-related lexical artifacts on the robustness and fairness of hate speech detection by employing the following data-centric baselines. VANILLA We fine-tune the BERT-base model on each corpus, so that the proposed baselines can be directly compared to a commonly employed setup. FILTERING Swayamdipta et al. (2020) have shown that the most ambiguous training data instances promote OOD generalization while preserving in-distribution performance. We thus leverage the VANILLA model's training dynamics to filter training data to contain the 33% most ambiguous instances only, in line with the subset size in Swayamdipta et al. (2020) . 10 Intuitively, those are instances whose class probabilities fluctuate frequently across training epochs. We then finetune the BERT-base model on the resulting subset. This setup is similar in spirit to the one employed in Zhou et al. (2021) ; however, we assess it on data from multiple platforms, also removing duplicate instances (Appendix A.2) which may potentially confound the debiasing results. REMOVAL Prior to fine-tuning, we naively remove any occurrence of spurious lexical artifacts from training and development data. This matches previously employed baselines for assessing fairness in hate speech detection (Kennedy et al., 2020b) . However, since we are also interested in OOD robustness, we experiment with two removal variants: one for S I and one for S \u00acI artifacts. We hypothesize that removing S I tokens potentially improves fairness, whereas removing S \u00acI tokens mostly contributes to OOD robustness. MASKING We propose a novel data-centric debiasing alternative based on token masking. Instead of removing spurious artifacts altogether, we reserve a special token in the vocabulary of the model that we use as replacement for spurious artifacts. We then fine-tune the model on the masked data. Intuitively, this way we encourage the model to blend all artifacts to a single contextualized representation that will never appear during testing, also avoiding to redistribute the informativeness of spurious lexical items to surrounding tokens. As for REMOVAL, we experiment with S \u00acI and S I masking variants. Results and discussion In Table 4 we report the results for all baselines along the in-distribution and out-of-distribution dimensions from the lens of fairness and robustness. Table 4 : In-distribution and out-of-distribution results (F 1 for accuracy and FPR for fairness). Out-of-distribution results are on \u2192 : REDDIT, \u2192 : TWITTER, \u2192 : GAB, and \u2192 \u0253: STORMFRONT. Scores are averages of 3 runs with different seeds, whereas subscripts indicate standard deviation. \u2191: greater the better; \u2193: lower the better. In-distribution Out-of-distribution \u2192 \u2192 \u2192 \u2192 \u0253 F 1\u2191 FPR \u2193 F 1\u2191 FPR \u2193 F 1\u2191 FPR \u2193 F 1\u2191 FPR \u2193 F 1\u2191 FPR \u2193 VANILLA 75 Since we argue that in-distribution performance is not a reliable measure for the performance of a hate speech detection system in the wild, due to space constraints we here focus on the more realistic yet more challenging out-of-distribution setup. Filtering is not a one-size-fits-all solution Despite the improvements in OOD generalization on commonsense reasoning, question answering, and NLI tasks (Swayamdipta et al., 2020) , training on ambiguous instances collected from training dynamics is not as effective in hate speech detection. 11 Instead, our results show that FILTERING leads to mixed results for OOD fairness compared to the VANILLA baseline. This is consistent with results on Twitter data (Zhou et al., 2021) , and we further confirm it is the case also across platforms. Importantly, we also notice that FILTERING has a detrimental effect on OOD robustness, except for two cases only (i.e., \u2192 and \u2192 \u0253). This indicates that hate speech detection is a nuanced task requiring more targeted approaches than automated data filtering. Removing S I is not as strong as it has been previously thought Removing identity terms from data altogether is a commonly used baseline for testing downstream fairness (e.g., Kennedy et al., 2020b) . Indeed, our results confirm that RE-MOVAL(S I ) consistently reduces the FPR on test instances containing S I mentions compared to the VANILLA baseline -with the only exception of \u0253 \u2192 . However, it only improves OOD robustness on \u2192 and \u0253 \u2192 . Moreover, it consistently scores lower than MASKING(S I ) on fairness, as discussed below. This raises the question of whether REMOVAL(S I ) should continue to be used as fairness baseline in future studies. Masking S I improves fairness When masking S I , we notice a consistent improvement in fairness over all approaches, both in-distribution and outof-distribution, on all platforms. Reduction in FPR over the VANILLA baseline is as large as 3\u00d7, as results for { ; } \u2192 \u0253 and \u2192 show. Most of the remaining train-test pairs show a 2\u00d7 improvement in FPR, also compared to the common REMOVAL(S I ) baseline. We hypothesize the improved fairness performance with respect to RE-MOVAL(S I ) is due to the way contextualized representations are formed during training, as discussed in Section 5.2. Despite being surprisingly simple, we envision MASKING(S I ) as a strong baseline for future work on fairness in hate speech detection. S \u00acI artifacts are not as useful as S I We observe that methods exploiting S \u00acI artifacts lead to mixed results. This suggests that while a substantial FPR reduction can be achieved exploiting S I artifacts, robustness calls for more complex debiasing strategies to transfer well across distributions. Fairness comes at the cost of robustness Overall, we observe an important trade-off between fairness and robustness. Data-centric approaches that achieve a consistently high level of fairnessnamely, MASKING(S I ) and REMOVAL(S I ) -typically show a decrease in in-distribution and outof-distribution performance -with the exception of \u2192 and \u2192 for MASKING(S I ), and \u2192 and \u0253 \u2192 for REMOVAL(S I ). On one hand, this suggests that spurious, identity-related lexical artifacts do play an important role in performance across distributions. On the other hand, we believe this reflects the real performance of a prototypical model that is substantially fairer, and thus to which future work in hate speech detection should be compared to. We argue that MASKING(S I ) represents a starting point to achieve both fairness and OOD robustness, the latter requiring more complex, model-centric debiasing approaches. A summary of the results over all corpus pairs for each method is presented in Appendix C.2. Towards Artifacts Documentation The practice of data statements (Bender and Friedman, 2018) has been recently adopted by the NLP community as a way to include relevant information about the creators, the methodology and possible biases when a dataset is released. This should in turn have a positive impact on systems trained on such data, contributing to a better evaluation of models' generalization and fairness. We propose that an artifacts statement should be added to this documentation as a way to contribute to diagnosis (and thus mitigation) of pre-existing bias, which is also one of the goals of data statements. In particular, we propose a template for lexical artifacts documentation and publicly release code to easily compute ranked correlations between tokens and target classes of interest for a given annotated corpus. To ensure the process of documenting lexical artifacts will be as smooth as possible -and thus allows widespread adoption of artifacts statement in the future -our code automatically generates outputs in different formats, from raw text to LaTeX code for seamless inclusion in publications. We present the artifacts statement template below, and provide a full example in Appendix D. I) TOP LEXICAL ARTIFACTS. Which are the k most informative tokens in the corpus for the class(es) of interest? This can be a ranked list of (k \u2265 10) tokens in plain text or in a tabular format, optionally along with associated scores. If there are multiple classes of interest, top k lexical artifacts for each class should be included. II) CLASS DEFINITIONS. Different definitions for the same class may exist across datasets. This impacts the annotation, which in turn has an effect on resulting lexical artifacts. An explicit definition of the target class(es) for which the top lexical artifacts are computed should be provided here. III) METHODS AND RESOURCES. The method used to compute the correlation between tokens and class(es) (e.g., PMI, interpretability approaches) in the annotated corpus should be reported here, possibly with a link to code. If preprocessing and deduplication have been performed, they should be clearly reported. Resources such as full lists of lexical artifacts can be additionally included. Related Work The problem of models' generalizability related to hate speech detection has been extensively discussed in recent works (Vidgen and Derczynski, 2020; Yin and Zubiaga, 2021; Wich et al., 2021) . Indeed, it has been shown that state-of-the-art performance on this task overestimates the capability of models to yield the same results over time (Florio et al., 2020) or across different domains (Wiegand et al., 2019) . Possible mitigation strategies include domain adaptation techniques (Ramponi and Plank, 2020) , augmenting smaller datasets with a larger dataset from a different domain (Karan and \u0160najder, 2018) , the use of a domain lexicon to transfer knowledge across domains (Pamungkas and Patti, 2019) and the fine-tuning of HateBERT (Caselli et al., 2021) on the target corpus (Bose et al., 2021) , among others. Concerning bias and fairness, several works have pointed out the presence of bias in hate and abusive language datasets (Wiegand et al., 2019; Sap et al., 2019 Sap et al., , 2020)) . This issue has been addressed in different ways, including functional tests for hate speech detection models (R\u00f6ttger et al., 2021; Manerba and Tonelli, 2021) and post-hoc explanations to measure models' bias towards identity terms (Kennedy et al., 2020b) . As regards bias mitigation, the task has been addressed through a number of approaches, e.g., via adversarial feature learning (Vaidya et al., 2020) , by using debiased word embeddings and gender swap data augmentation (Park et al., 2018) or by adding non-toxic examples to better balance the data (Dixon et al., 2018) . The work probably most related to ours is Zhou et al. (2021) , which presents an analysis of lexical and dialectal biases in the dataset by Founta et al. (2018) . The authors propose lexical bias categories which we extend in this work (see Section 2). However, they focus only on one dataset and on indomain bias reduction. Moreover, they start from a list of \"bad words\", whereas we compute it from data. To our knowledge, this is the first work advocating for a joint view on fairness and robustness, both identified as critical aspects related to the classification of hate speech (Wich et al., 2021) . Limitations Our work is a step forward towards a better understanding of the bias that can be encoded in hate speech detection corpora (Blodgett et al., 2020) . However, we are aware of some limitations. First, all findings in this work are related to hate speech datasets in English. With the increasing availability of hate speech data in languages other than English, we aim to investigate our methods on other languages too. Second, annotated data from multiple platforms may not be available for some languages, and this can limit the cross-distribution computation of artifacts. Lastly, we acknowledge spurious statistical correlations may go beyond the token level. We believe our study is a first step towards contextual debiasing from spurious lexical artifacts, and thus can be of inspiration for future studies. Conclusion and Future Directions This paper investigates the impact of lexical artifacts on out-of-distribution fairness and robustness in hate speech detection, raising awareness on the interplay between the two dimensions that should be studied together in future work. We propose a fine-grained categorization of lexical artifacts and simple yet effective data-centric baselines, show-ing that while robustness calls for model-centric approaches, masking spurious identity artifacts is a viable approach that we argue should be used as strong baseline for fairness assessment in future research. In future work we aim to investigate the role of dialectal biases and non-lexical artifacts, extending the study on languages other than English. We release all baseline models, resources, and the code to compute lexical artifacts, broadly suggesting the inclusion of \"artifacts statement\" as a way to document potential lexical biases when a dataset is released, to provide a complementary view to data statements (Bender and Friedman, 2018) . Ethical Considerations The annotation task described in Section 4.3 was carried out by two researchers regularly employed at Fondazione Bruno Kessler as part of their work. Overall, we do not foresee any specific ethical concern related to this work. On the contrary, our goal is to propose artifacts statement as a desirable practice for documenting potential biases in newly released datasets, and improve current debiasing methods by distinguishing among different types of lexical artifacts. However, the (finite set of) identity-related and offensive tokens considered in this work are all in English and centered around Western cultural context. We leave the evaluation of our methodology to assess whether there are language-or more broadly culture-dependent changes for future work, following recent work on biases in geo-cultural contexts (Ghosh et al., 2021) . Acknowledgements Part of this work was funded by the PROTEC-TOR European project (ISFP-2020-AG-PROTECT-101034216-PROTECTOR). This research was also supported by the KID ACTIONS REC-AG project (n. 101005518) on \"Kick-off preventIng and re-sponDing to children and AdolesCenT cyberbul-lyIng through innovative mOnitoring and educa-tioNal technologieS\". Appendix A Data: Additional Details A.1 Preprocessing and anonymization We preprocess texts across platforms in a consistent way by anonymizing user mentions, URLs, and email addresses with [USER], [URL], and [EMAIL] placeholders, respectively. We segment hashtags into constituent words using the wordsegment package, 12 remove newlines, unescape HTML tags, and lowercase the texts. A.2 Deduplication We found many duplicates in the data for all platforms. We argue that retaining duplicates as done in most previous work could severely affect the reliability of any bias analysis (and debiasing method) and its subsequent conclusions. Specifically, duplicates can (i) skew the distribution of actual artifacts in the data, overamplifying some lexical items and demoting others, and (ii) result in unfair evaluations due to identical examples falling in multiple instances of the training, development and test sets, also potentially leading to overfitting. 13  Following this intuition, we thus remove duplicate instances after preprocessing. 14 B List of Spurious Artifacts In the following, we provide the list of all spurious lexical artifacts annotated as S I and S \u00acI as described in Section 4.3. All these are the ones that exhibit full agreement. In our shared repository we also release all artifacts that exhibit disagreement even after adjudication, in order to encourage future work on this direction. 12 https://github.com/grantjenks/ python-wordsegment 13 Among duplicates, we found some tweets with more than 100 duplicate instances in Founta et al. (2018) . 14 Most work do not explicitly mention if deduplication is carried before or after preprocessing texts. We believe this is an important detail to foster reproducibility -we found many examples with the same text but different URLs, unveiling possibly bot-generated messages we removed this way. Identity-related (S I ) \"white\", \"black\", \"jews\", \"women\", \"jew\", \"whites\", \"blacks\", \"muslim\", \"gay\", \"muslims\", \"islam\", \"woman\", \"jewish\", \"islamic\", \"immigrants\", \"mexican\", \"asian\", \"homosexual\", \"americans\", \"lesbian\", \"homo\", \"females\", \"america\", \"brown\", \"israel\", \"arabs\", \"zionist\", \"trans\", \"lgbt\", \"girl\", \"hispanic\", \"refugees\", \"male\", \"african\", \"africa\", \"girls\", \"indians\", \"queer\", \"##grate\", \"guy\". Non identity-related (S \u00acI ) \"##s\", \"##es\", \"people\", \"country\", \"##ing\", \"anti\", \"illegal\", \"bunch\", \"##t\", \"kids\", \"culture\", \"brain\", \"##ly\", \"##bt\", \"##d\", \"sex\", \"ho\", \"##nt\", \"countries\", \"##ic\", \"##ers\", \"liberal\", \"reason\", \"##y\", \"human\", \"genocide\", \"##ed\", \"##ists\", \"wrong\", \"lives\", \"bad\", \"god\", \"##oc\", \"lying\", \"##ard\", \"racism\", \"##e\", \"##oid\", \"##w\", \"yeah\", \"millions\", \"society\", \"##g\", \"leftist\", \"crime\", \"sp\", \"des\", \"##ist\", \"##ry\", \"mouth\", \"##ards\", \"##rs\", \"##ize\", \"burn\", \"murdered\", \"worship\", \"##ening\", \"##ism\", \"living\", \"##fa\", \"coming\", \"calling\", \"streets\", \"##ting\", \"force\", \"mis\", \"##ss\", \"blame\", \"typical\", \"##pe\", \"baby\", \"death\", \"talking\", \"##gen\", \"belong\", \"respect\", \"di\", \"##yp\", \"sexual\", \"##less\", \"mad\", \"war\". C Experiments: Additional Results C.1 Filtering with different thresholds In Table 5 we present results for the FILTER-ING baseline using different sampling thresholds. Specifically, in addition to using the 33% (1/3) most ambiguous training data instances as in Swayamdipta et al. (2020) , we provide full results using more aggressive (i.e., 25%, 1/4) and less aggressive (i.e., 50%, 1/2) filtering thresholds. We notice mixed results that make hard to determine which is the best threshold across platforms. FILTERING (25%) improves OOD robustness on \u2192 , and FILTERING (50%) provides best overall in-domain performance on . However, MASK-ING(S I ) outperforms all FILTERING approaches according to the FPR metric. C.2 Average results over all corpus pairs We provide a summary of the results for all methods in Table 6 , where we report average scores over all corpus pairs (refer to Table 4 for full results). On average, MASKING(S I ) improvement in FPR over the VANILLA baseline is as large as 2\u00d7, both indistribution and out-of-distribution. This comes at  the cost of a minimal in-distribution and OOD drop in macro F 1 (i.e., \u22121.26 and \u22121.95, respectively). D Lexical Artifacts Statement Example An example of lexical artifacts statement for the Reddit dataset (Vidgen et al., 2021) used in this study is presented in the following. I) TOP LEXICAL ARTIFACTS. We present the top k = 10 most informative tokens for the hateful class along with their scores in Table 7 . (Vidgen et al., 2021) , and is defined as \"Content which contains a negative statement made against an identity. An 'identity' is a social category that relates to a fundamental aspect of individuals' community, socio-demographics, position or self-representation [...]. It includes but is not limited to Religion, Race, Ethnicity, Gender, Sexuality, Nationality, Disability/Ableness and Class.\" (Vidgen et al., 2021) . Rank III) METHODS AND RESOURCES. In order to compute the correlation between tokens to the hateful class we employ PMI as implemented in [this work] (code: https://github.com/ dhfbk/hate-speech-artifacts). Input texts have been preprocessed by anonymizing user mentions, URLs, and email addresses with [USER], [URL], and [EMAIL] placeholders. Hashtags have been segmented using wordsegment, 15 and we remove newlines, unescape HTML tags, and lowercase texts. Duplicate instances have been removed after preprocessing. The full list of lexical artifacts along with associated scores is available at https://github. com/dhfbk/hate-speech-artifacts.",
    "abstract": "Warning: this paper contains content that may be offensive or upsetting. Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective datacentric baselines. Our results on English data across four platforms show that distinct spurious artifacts require different treatments to ultimately attain both robustness and fairness in hate speech detection. To encourage research in this direction, we release all baseline models and the code to compute artifacts, pointing it out as a complementary and necessary addition to the data statements practice.",
    "countries": [
        "Europe"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "1",
    "year": "2022",
    "month": "July",
    "title": "Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection"
}