{
    "article": "Several recent state-of-the-art transfer learning methods model classification tasks as text generation, where labels are represented as strings for the model to generate. We investigate the effect that the choice of strings used to represent labels has on how effectively the model learns the task. For four standard text classification tasks, we design a diverse set of possible string representations for labels, ranging from canonical label definitions to random strings. We experiment with T5 (Raffel et al., 2019) on these tasks, varying the label representations as well as the amount of training data. We find that, in the low data setting, label representation impacts task performance on some tasks, with task-related labels being most effective, but fails to have an impact on others. In the full data setting, our results are largely negative: Different label representations do not affect overall task performance. Introduction State-of-the-art transfer learning methods model classification tasks as text generation, such as GPT2 (Radford et al., 2019) and T5 (Raffel et al., 2019) , and have led to significant improvements across a variety of NLP tasks. In this setting, labels are represented as strings for the model to generate, and the pretrained language model is finetuned to maximize the probability of generating the chosen label representation. For example, for CoLA (Warstadt et al., 2019) task, a classifier will be trained to output 0 (representing unacceptable) or 1 (representing acceptable), while T5 models it as a generation task, it is trained to generate the string \"unacceptable\" or \"acceptable\". The advantage of this approach is that the language model can be applied to the classification task as-is, without the * These two authors contributed equally. need for any additional task-specific parameters or training. However, in this setting, the impact of the particular strings used to represent labels remains unclear on the end task performance. One of the few studies on this question find that the linguistic properties (relatedness, polarity scale, etc.) of the labels do affect task performance (Nogueira et al., 2020) , though their results are limited to document retrieval. We therefore further investigate the impact of string representation when modeling text classification as text generation. We experiment with T5-base and four diverse, standard text classification tasks. For each task, we design a wide range of label representations, including canonical task labels; task-unrelated antonyms; and completely random strings. As previous works by Nogueira et al. (2020) have noted that the impact of label representation is particularly noticeable in lower data settings, we also vary the amount of training data for each task. Our experiments reveal that, in the full data setting, the choice of label representation largely does not affect overall performance, with only one of the four datasets seeing any impact. In the low data setting, label representations sometimes have an impact on the overall performance, with task-related labels being the most effective in these cases. Related Work Target Word Probing Experiments Nogueira et al. (2020) probe the effects of label representation on document retrieval and ranking. They set the baseline mapping as {Positive \u2192 true, Negative \u2192 false}, and also try the reverse mapping, antonyms, related words, unrelated words and subwords. In the low data setting, they find that the baseline mapping yields accuracy significantly higher than other types of mappings do. In the high data setting, related words mapping is the most effective, though the differences between mappings are not as large as in the low-data regime. We extend these experiments with more diverse tasks and label representations. Cloze Reformulation Schick and Sch\u00fctze (2020) introduce Pattern Exploiting Training, where the input is transformed into a Cloze-style sentence. For example, the task of identifying whether two sentences a and b contradict or agree with each other is reformulated into \"a? [blank], b.\", and a pretrained language model is used as-is to generate Yes or No to fill in the blank. They claim that this procedure significantly improves the performance on several tasks in the zero-shot setting. Similarly, Petroni et al. (2019) probe the knowledge presented in state-of-the-art language models without fine-tuning using similar sentences with blanks. They find that these language models contain rich factual knowledge from pretraining, and are effective at recalling knowledge when answering fact-related questions. Prompt Design Jiang et al. (2020) propose several methods to automatically generate efficient prompts for extracting knowledge from pretrained models, rather than manually designing them. They find that different templates, e.g. \"x who converted to y\", compared to \"x is affiliated with y religion\", can improve accuracy by as much as 60%. This line of work is orthogonal to ours: We focus on optimal label representation whereas they focus on the best way to format inputs. Label Representations We consider four standard text classification datasets representing different tasks and textual genres: (i) sentence acceptability judgments with CoLA (Warstadt et al., 2019) ; (ii) sentiment analysis with SST-2 (Socher et al., 2013) ; (iii) paraphrase detection with PAWS (Zhang et al., 2019) ; and (iv) commonsense reasoning with COPA (Roemmele et al., 2011) . For each of these datasets, we test a wide variety of label representations. Random Labels As a simple baseline, we test whether the labels need to be semantic at all. We focus on random labels with short lengths, e.g. {unacceptable \u2192 i, acceptable \u2192 c}. Task-Unrelated Labels Next, we choose sets of words for labels based on their relationship to the task and to each other. We generate them by the following different rules: \u2022 Antonyms: We choose words that are antonyms but are semantically unrelated to our tasks, e.g. {unacceptable \u2192 cold, acceptable \u2192hot}. This setting tests whether it is important that labels be task related or if it is sufficient that they have opposing meanings. \u2022 Synonyms: To contrast the antonyms, we use words that are synonyms but are semantically unrelated to our tasks, e.g. {unacceptable \u2192 cold, acceptable \u2192 chilly}. \u2022 Irrelevant words: We choose words that are not related to the task nor each other, e.g. {unacceptable \u2192 ice, acceptable \u2192 happy}. \u2022 Relevant words: We pick words that are relevant to each other but are not antonyms or synonyms, e.g. {unacceptable \u2192 apple, acceptable: \u2192 orange}. Task-Related Labels We further study how much performance varies between semantically similar task-related label representations. We choose sets of words as labels that have the same polarity scale or meanings as the original targets, e.g. {unacceptable \u2192 no, acceptable \u2192 yes}; As an additional baseline, we use labels that have the opposite polarity or meaning as the original labels, e.g. {unacceptable \u2192 yes, acceptable \u2192 no}. 4 Experiments and Results Model and Optimization Inspired by Nogueira et al. (2020) , we experiment with T5 (Raffel et al., 2019) , specifically the T5base model. Among the four datasets, PAWS is the only dataset that has not been pretrained on by T5. For COPA, CoLA, and SST-2, we finetune with Adam (Kingma and Ba, 2014) for 2000 steps with learning rate 10 \u22124 . For PAWS, we finetune for 3000 steps and similarly evaluate the accuracy of the development set every 200 steps. For all datasets, we evaluate the accuracy of the development set every 200 steps and report the best accuracy. For each dataset and setting, we evaluate three runs with different random seeds. Low-Data Settings As Nogueira et al. (2020) found significant differences in performance with distinct data sizes, we also run all four tasks in lower-data regimes. We choose the datasets mainly following these rules: For all tasks, we use the full training sets. Then, we downsample the training set to sizes for which we observe larger gaps between different labels than at the full dataset size. For each run with the lowdata setting, we randomly generate a sample before training it, so the sample set normally varies for each run. To test an extremely low-data setting, for COPA and SST-2, we also choose a dataset with an absolute size of 10. Results We present results for SST-2 and COPA in Table 2 and for PAWS and CoLA in Table 3 . In the full data setting, we obtain performance near that of BERT (Devlin et al., 2018) for all tasks. It is notable that even in the extremely low data settings, we obtain nontrivial performance for all tasks except COPA, which is likely due to the amount of pretraining in even the T5-base model. For SST-2, by training on the full dataset, we get similar results for all choices of label representation. Notably, even the random strings perform as well as the task-specific labels. However, given the large size of the full dataset, it is unsurprising that the model can learn the class definitions from random strings. For the 100 and 10 example settings, we find that the original labels achieve the best accuracy, 2% less than the full data setting. However, a potential confounder is the fact that T5 was pretrained on SST-2 with these labels, which likely explains the high accuracy with only 10 examples. We find the mutually unrelated and task-unrelated labels perform as well as the original labels in the 100 example setting (ice/happy), suggesting that even with this few examples, the choice of labels or even the relationship between them is not crucial. Similarly, in the 10 example setting, these mutually unrelated labels (ice/happy) perform as well as task-specific labels and task-unrelated antonyms, providing further evidence that the choice of labels is not crucial to learning the task, even in extremely low data settings. On the other hand, reversing the original labels or reversing task-related labels consistently performs the worst, even worse than random labels. This suggests that the label representations do not matter, as long as we do not pick labels that flip the class definition. For COPA, in the full data setting, we observe notable performance differences between various labels. We get the best performance using the original labels and other task-related labels, while taskunrelated labels generally perform much worse. For the random labels, we observe high variability,  with one pair of labels (n/p) worryingly performing as well as the original task labels. Also concerning is the fact that reversing the original or other task-related labels performs nearly as well as not reversing them. We speculate that the small dataset size contributes to these odd performance trends, as has been previously noted for COPA (Sap et al., 2019) . In the extremely low data settings, differences between labels become more significant and noticeable, with several label representations failing to learn the task and obtaining accuracy near chance (50%). The original and reversed pairs, as well as the task-related pairs, show similar accuracies, and these labels generally perform the best among all labels. Similar to SST-2, reversing the original labels performs worse and shows more instability. In the smallest setting, matched task-related labels also perform better than reversed ones and all other labels except for the original label pair. For CoLA and PAWS, in full data and 4000 sample data regimes, the performances we get using dif-ferent labels vary, but by no more than 2%. When the data size is extremely low, we observe notable gaps among labels for CoLA, but not for PAWS. Similar to SST-2, for CoLA, we find that reversed labels perform much worse than original ones. Conclusion In this work, we investigate the impact of label representation in modeling classification as a seq2seq task. For four standard text classification datasets and task types, we design a wide range of label representations, ranging from canonical task-related labels to task-unrelated antonyms to random words and strings. We experiment with the T5-base model on these datasets and label representations in a range of regimes with various data sizes. Overall, we find that the choice of label representation largely does not affect task performance, though it varies by task and dataset size. In the high data settings, there is generally no differences between choices of label representations, and even random strings can function well as label repre-sentations. In low data settings, the influence of label representations varies significantly between tasks. For PAWS, we observe no variation, but for COPA and CoLA, we note that task-related labels generally perform best. Our experiments represent preliminary negative evidence that label representations have limited impact on task performance, but there are a number of dimensions that our work does not investigate that might affect a model's sensitivity to label representation. First, we take as given that canonical task labels as sufficient for all tasks. However, for some tasks, the canonical label representation might be suboptimal or not sufficiently convey the semantics of the task, e.g. choice1/choice2 for COPA. A natural way to mitigate this issue would be to use a much larger set of possible label representations, or even automatically discover label representations. However, we expect this issue will be especially problematic with more complex tasks that are difficult to represent with single-word labels or to succinctly represent in text at all. Second, we use a single task input format, but the task formatting may be suboptimal and affect the ability of the model to learn from the label semantics. Schick and Sch\u00fctze (2020) use multiple example templates per task and find that performance between templates can vary substantially. While an obvious direction would be to simply use multiple templates, automatically discovering effective templates also seems like a promising direction. Acknowledgements We thank Samuel R. Bowman, Nikita Nangia, and Marina Zavalina for their guidance and support; Yang Liu for his mentoring and advice.",
    "abstract": "Several recent state-of-the-art transfer learning methods model classification tasks as text generation, where labels are represented as strings for the model to generate. We investigate the effect that the choice of strings used to represent labels has on how effectively the model learns the task. For four standard text classification tasks, we design a diverse set of possible string representations for labels, ranging from canonical label definitions to random strings. We experiment with T5 (Raffel et al., 2019) on these tasks, varying the label representations as well as the amount of training data. We find that, in the low data setting, label representation impacts task performance on some tasks, with task-related labels being most effective, but fails to have an impact on others. In the full data setting, our results are largely negative: Different label representations do not affect overall task performance.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "1",
    "year": "2020",
    "month": "December",
    "title": "Label Representations in Modeling Classification as Text Generation"
}