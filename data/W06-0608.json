{
    "article": "Semantic information is important for precise word sense disambiguation system and the kind of semantic analysis used in sophisticated natural language processing such as machine translation, question answering, etc. There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties. Introduction While there has been considerable research on both structural annotation (such as the Penn Treebank (Taylor et al., 2003) or the Kyoto Corpus (Kurohashi and Nagao, 2003) ) and semantic annotation (e.g. Senseval: Kilgariff and Rosenzweig, 2000; Shirai, 2002) , there are almost no corpora that combine both. This makes it difficult to carry out research on the interaction between syntax and semantics. Projects such as the Penn Propbank are adding structural semantics (i.e. predicate argument structure) to syntactically annotated corpora, but not lexical semantic information (i.e. word senses). Other corpora, such as the English Redwoods Corpus (Oepen et al., 2002) , combine both syntactic and structural semantics in a monostratal representation, but still have no lexical semantics. In this paper we discuss the (lexical) semantic annotation for the Hinoki Corpus, which is part of a larger project in psycho-linguistic and computational linguistics ultimately aimed at language understanding (Bond et al., 2004) . Corpus Design In this section we describe the overall design of the corpus, and is constituent corpora. The basic aim is to combine structural semantic and lexical semantic markup in a single corpus. In order to make the first phase self contained, we started with dictionary definition and example sentences. We are currently adding other genre, to make the langauge description more general, starting with newspaper text. Lexeed: A Japanese Basic Lexicon We use word sense definitions from Lexeed: A Japanese Semantic Lexicon (Kasahara et al., 2004) . It was built in a series of psycholinguistic experiments where words from two existing machine-readable dictionaries were presented to subjects and they were asked to rank them on a familiarity scale from one to seven, with seven being the most familiar (Amano and Kondo, 1999) . Lexeed consists of all words with a familiarity greater than or equal to five. There are 28,000 words in all. Many words have multiple senses, there were 46,347 different senses. Definition sentences for these sentences were rewritten to use only the 28,000 familiar words. In the final configuration, 16,900 different words (60% of all possible words) were actually used in the definition sentences. An example entry for the word \u00fc\u00df doraib\u0101 \"driver\" is given in Figure 1 , with English glosses added. This figure includes the sense annotation and information derived from it that is described in this paper. Table 1 shows the relation between polysemy and familiarity. The #WS column indicates the average number of word senses that polysemous \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 INDEX \u00fc\u00df doraiba- POS noun Lexical-Type noun-lex FAMILIARITY 6.5 [1-7] (\u2265 5) Frequency 37 Entropy 0.79 SENSE 1 (0.11) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 DEFINITION \u00bd 1 / / \u00bc 1 / / \u00a1 / \u00b2\u00bd 1 / /\u00be / \u00a6 1 /\u00a2 a tool for inserting and removing screws . EXAMPLE \u00e1 \u00c0 \u00fc\u00df \u00bd \u00be \u00a2 he used a small screwdriver to tighten the screws on his glasses. HYPERNYM \u00a6 1 equipment \"tool\" SEM. CLASS 942:tool/implement (\u2282 893:equipment ) WORDNET screwdriver 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb SENSE 2 (0.84) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 DEFINITION 1 / / \u00cd 1 /\u00be /\u00bc 1 /\u00a2 Someone who drives a car. EXAMPLE \u00c0 \u00b8 \u00fc\u00df \u00bc \u00f5 \u00ba \u00a2 my father was given an award as a good driver. HYPERNYM \u00bc 1 hito \"person\" SEM. CLASS 292:chauffeur/driver (\u2282 5:person ) WORDNET driver 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb SENSE 3 (0.05) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 DEFINITION \u00fe 1 // \u00a1/\u00be 1 / / / \u00fc 3 /\u00a2 / \u00ea /\u00a2 In golf, a long-distance club. A number one wood. EXAMPLE \u00e1 \u00c0 \u00fc\u00df \u00f7 \u00c1\u00bc \u00a2 he hit (it) 30 yards with the driver. HYPERNYM \u00fc 3 kurabu \"club\" SEM. CLASS 921:leisure equipment (\u2282 921) WORDNET driver 5 DOMAIN \u00fe 1 gorufu \"golf\" \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb Ontology We also have an ontology built from the parse results of definitions in Lexeed (Nichols and Bond, 2005) . The ontology includes more than 50 thousand relationship between word senses, e.g. synonym, hypernym, abbreviation, etc. Goi-Taikei As part of the ontology verification, all nominal and most verbal word senses in Lexeed were Hinoki Treebank Lexeed definition and example sentences are syntactically and semantically parsed with HPSG and correct results are manually selected (Tanaka et al., 2005) . The grammatical coverage over all sentences is 86%. Around 12% of the parsed sentences were rejected by the treebankers due to an incomplete semantic representation. This process had been done independently of word sense annotation. Target Corpora We chose two types of corpus to mark up: a dictionary and two newspapers. Table 3 shows basic statistics of the target corpora. The dictionary Lexeed, which defined word senses, is also used for a target for sense tagging. Its definition (LXD-DEF) and example (LXD-EX) sentences consist of basic words and function words only, i.e. it is self-contained. Therefore, all content words have headwords in Lexeed, and all word senses appear in at least one example sentence. Both newspaper corpora where taken from the Mainichi Daily News. One sample (Senseval2) was the text used for the Japanese dictionary task in Senseval-2 (Shirai, 2002) , which has some words marked up with word sense tags defined in the Iwanami lexicon (Nishio et al., 1994) . The second sample was those sentences used in the Kyoto Corpus (Kyoto), which is marked up with dependency analyses (Kurohashi and Nagao, 2003) . We chose these corpora so that we can compare our annotation with existing annotation. Both these corpora were thus already segmented and annotated with parts-of-speech. However, they used different morphological analyzers to the one used in Lexeed, so we had to do some remapping. E.g. in Kyoto the copula is not split from nominal-adjectives, whereas in Lexeed it is: genkida \"lively\" vs genki da. This could be done automatically after we had written a few rules. Although the newspapers contain many words other than basic words, only basic words have sense tags. Also, a word unit in the newspapers does not necessarily coincide with the headword in Lexeed since part-of-speech taggers used for annotation are different. We do not adjust the word segmentation and leave it untagged at this stage, even if it is a part of a basic word or consists of multiple basic words. For instance, Lexeed has the compound entry \u2022 kahei-kachi \"monetary value\", however, this word is split into two basic words in the corpora. In this case, both two words \u2022 kahei \"money\" and kachi \"value\" are tagged individually. The corpora are not fully balanced, but allow some interesting comparisons. There are effectively three genres: dictionary definitions, which tend to be fragments and are often syntactically highly ambiguous; dictionary example sentences, which tend to be short complete sentences, and are easy to parse; and newspaper text from two different years. Annotation Each word was annotated by five annotators. We actually used 15 annotators, divided into 3 groups. None were professional linguists or lexicographers. All of them had a score above 60 on a Chinese character based vocabulary test (Amano and Kondo, 1998) . We used multiple annotators to measure the confidence of tags and the degree of difficulty in identifying senses. The target words for sense annotation are the 9,835 headwords having multiple senses in Lexeed ( \u00a7 2.1). They have 28,300 senses in all. Monosemous words were not annotated. Annotation was done word by word. Annotators are presented multiple sentences (up to 50) that contain the same target word, and they keep tagging that word until occurrences are done. This enables them to compare various contexts where a target word appears and helps them to keep the annotation consistent. Tool A screen shot of the annotation tool is given in Figure 2 . The interface uses frames on a browser, with all information stored in SQL tables. The left hand frame lists the words being annotated. Each word is shown with some context: the surrounding paragraph, and the headword for definition and example sentences. These can be clicked on to get more context. The word being annotated is highlighted in red. For each word, the annotator chooses its senses or one or more of the other tags as clickable buttons. It is also possible to choose one tag as the default for all entries on the screen. The right hand side frame has the dictionary definitions for the word being tagged in the top frame, and a lower frame with instructions. A single word may be annotated with senses from more than one headword. For example \u00e0 is divided into two headwords basu \"bus\" and basu \"bass\", both of which are presented. As we used a tab-capable browser, it was easy for the annotators to call up more information in different tabs. This proved to be quite popular. Markup Annotators choose the most suitable sense in the given context from the senses that the word have in lexicon. Preferably, they select a single sense for a word, although they can mark up multiple tags if the words have multiple meanings or are truly ambiguous in the contexts. When they cannot choose a sense in some reasons, they choose one or more of the following special tags. o other sense: an appropriate sense is not found in a lexicon. Relatively novel concepts (e.g. \u00fc\u00df doraib\u0101 \"driver\" for \"software driver\") are given this tag. c multiword expressions (compound / idiom): the target word is a part of a non-compositional compound or idiom. p proper noun: the word is a proper noun. x homonym: an appropriate entry is not found in a lexicon, because a target is different from head words in a lexicon (e.g. only a headword \u00e0 bass \"bus\" is present in a lexicon for \u00e0basu \"bass\"). e analysis error: the word segmentation or partof-speech is incorrect due to errors in preannotation of the corpus. Feedback One of the things that the annotators found hard was not knowing how well they were doing. As they were creating a gold standard, there was initially no way of knowing how correct they were. We also did not know at the start of the annotation how fast senses could or should be annotated (a test of the tool gave us an initial estimate of around 400 tokens/day). To answer these questions, and to provide feedback for the annotators, twice a day we calculated and graphed the speed (in words/day) and majority agreement (how often an annotator agrees with the majority of annotators for each token, measured over all words annotated so far). Each annotator could see a graph with their results labelled, and the other annotators made anonymous. The results are grouped into three groups of five annotators. Each group is annotating a different set of words, but we included them all in the feedback. The order within each group is sorted by agreement, as we wished to emphasise the importance of agreement over speed. An example of a graph is given in Figure 3 . When this feedback was given, this particular annotator has the second worst agreement score in their subgroup (90.27%) and is reasonably fast (1799 words/day) -they should slow down and think more. The annotators welcomed this feedback, and complained when our script failed to produce it. There was an enormous variation in speed: the fastest annotator was 4 times as fast as the slowest, with no appreciable difference in agreement. After providing the feedback, the average speed increased considerably, as the slowest annotators agonized less over their decisions. The final average speed was around 1,500 tokens/day, with the fastest annotator still almost twice as fast as the slowest. Inter-Annotator Agreement We employ inter-annotator agreement as our core measure of annotation consistency, in the same way we did for treebank evaluation (Tanaka et al., 2005) . This agreement is calculated as the average of pairwise agreement. Let w i be a word in a set of content words W and w i, j be the jth occurrence of a word w i . Average pairwise agreement between the sense tags of w i, j each pair of annotators marked up a(w i, j ) is: a(w i, j ) = \u2211 k ( m i, j (s ik ) C 2 ) n w i, j C 2 (1) where n w i, j (\u2265 2) is the number of annotators that tag the word w i, j , and m i, j (s ik ) is the number Figure 3 : Sample feedback provided to an annotator of sense tags s ik for the word w i, j . Hence, the agreement of the word w i is the average of a w i, j over all occurrences in a corpus: a(w i ) = \u2211 j a(w i, j ) N w i (2) where N w i is the frequency of the word w i in a corpus. Table 4 shows statistics about the annotation results. The average numbers of word senses in the newspapers are lower than the ones in the dictionary and, therefore, the token agreement of the newspapers is higher than those of the dictionary sentences. %Unanimous indicates the ratio of tokens vs types for which all annotators (normally five) choose the same sense. Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. It is hard to directly compare with our task since their corpus has only 2,212 words tagged by two or three annotators. Familiarity As seen in Table 5 , the agreement per type does not vary much by familiarity. This was an unexpected result. Even though the average polysemy is high, there are still many highly familiar words with very good agreement. Part-of-Speech Table 6 shows the agreement according to part of speech. Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004) . In contrast, adjectives have as low agreement as verbs, although the agreement of adjectives was the highest and that of verbs was the lowest in English. This partly reflects differences in the part of speech divisions between Japanese and English. Adjectives in Japanese are much close in behaviour to verbs (e.g. they can head sentences) and includes many words that are translated as verbs in English. Entropy Entropy is directly related to the difficulty in identifing senses as shown in Table 7 . Sense Lumping Low agreement words have some senses that are difficult to distinguish from each other: these senses often have the same hypernyms. For example, the agreement rate of \u00af\u00d7 kusabana \"grass/flower\" in LXD-DEF is only 33.7 %. It has three senses whose semantic class is similar: kusabana 1 \"flower that blooms in grass\", kusabana 2 \"grass that has flowers\" and souka 1 \"grass and flowers\" (hypernyms flower 1 , grass 1 and flower 1 & grass 1 respectively). In order to investigate the effect of semantic similarity on agreement, we lumped similar word senses based on hypernym and semantic class. We use hypernyms from the ontology ( \u00a7 2.1) and semantic classes in Goi-Taikei ( \u00a7 2.3), to regard the word senses that have the same hypernyms or belong to the same semantic classes as the same senses. Table 8 shows the distribution after sense lumping. Table 9 shows the agreement with lumped senses. Note that this was done with an automatically derived ontology that has not been fully hand corrected. As is expected, the overall agreement increased, from 0.787 to 0.829 using the ontology, and to 0.835 using the coarse-grained Goi-Taikei semantic classes. For many applications, we expect that this level of disambiguation is all that is required. Special Tags Table 10 shows the ratio of special tags and multiple tags to all tags. These results show   the differences in corpus characteristics between dictionary and newspaper. The higher ratios of Other Sense and Homonym at newspapers indicate that the words whose surface form is in a dictionary are frequently used for the different meanings in real text, e.g. gin \"silver\" is used for the abbrebiation of \u00a3 ginkou \"bank\". %Multiple Tags is the percentage of tokens for which at least one annotator marked multiple tags. Discussion Comparison with Senseval-2 corpus The Senseval-2 Japanese dictionary task annotation used senses from a different dictionary (Shirai, 2002) . In the evaluation, 100 test words were selected from three groups with different entropy bands (Kurohashi and Shirai, 2001) . D a is the highest entropy group, which contains the most hard to tag words, and D c is the lowest entropy group. We compare our results with theirs in Table 11 . The Senseval-2 agreement figures are slightly higher than our overall. However, it is impossible to make a direct comparison as the numbers of annotators (two or three annotators in Senseval vs more than 5 annotators in our work) and the sense inventories are different. Problems Two main problems came up when building the corpora: word segmentation and sense segmentation. Multiword expressions like compounds and idioms are tied closely to both problems. The word segmentation is the problem of how to determine an unit expressing a meaning. At the present stage, it is based on headword in Lexeed, in particular, only compounds in Lexeed are recognized, we do not discriminate nondecomposable compounds with decomposable ones. However, if the headword unit in the dictionary is inconsistent, word sense tagging inherits this problem. For examples, \u00a3 ichibu has two main usage: one + classifier and a part of something. Lexeed has an entry including both two senses. However, the former is split into two The second problem is how to mark off metaphorical meaning from literal meanings. Currently, this also depends on the Lexeed definition and it is not necessarily consistent either. Some words in institutional idioms (Sag et al., 2002) have the idiom sense in the lexicon while most words do not. For instance, \u00af shippo \"tail of animal\") has a sense for the reading \"weak point\" in an idiom \u00af \u00a8 shippo-o tsukamu \"lit. to grasp the tail, idiom. to find one's weak point\", while ase \"sweat\" does not have a sense for the applicable meaning in the idiom \u00e3\u00be ase-o nagasu \"lit. to sweat, idiom, to work hard\". Conclusions We built a corpus of over three million words which has lexical semantic information. We are currently using it to build a model for word sense disambiguation. Acknowledgement We thank the 15 sense annotators and 3 treebankers (Takayuki Kuribayashi, Tomoko Hirata and Koji Yamashita).",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.9361263126072004e-07,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, or foundations. Therefore, based on the provided text, there is no evidence to suggest that the research was funded by any of these entities.",
    "abstract": "Semantic information is important for precise word sense disambiguation system and the kind of semantic analysis used in sophisticated natural language processing such as machine translation, question answering, etc. There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Chinese",
        "English",
        "Japanese"
    ],
    "numcitedby": 10,
    "year": 2006,
    "month": "July",
    "title": "The Hinoki Sensebank {---} A Large-Scale Word Sense Tagged Corpus of {J}apanese {---}",
    "values": {
        "building on past work": "We have built a Japanese corpus of over three million words with both lexical and structural semantic information.",
        "performance": "Table 4 shows statistics about the annotation results. The average numbers of word senses in the newspapers are lower than the ones in the dictionary and, therefore, the token agreement of the newspapers is higher than those of the dictionary sentences. %Unanimous indicates the ratio of tokens vs types for which all annotators (normally five) choose the same sense. This was an unexpected result. Even though the average polysemy is high, there are still many highly familiar words with very good agreement. Table 6 shows the agreement according to part of speech. Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004) . In contrast, adjectives have as low agreement as verbs, although the agreement of adjectives was the highest and that of verbs was the lowest in English. This partly reflects differences in the part of speech divisions between Japanese and English. Adjectives in Japanese are much close in behaviour to verbs (e.g. they can head sentences) and includes many words that are translated as verbs in English. As is expected, the overall agreement increased, from 0.787 to 0.829 using the ontology, and to 0.835 using the coarse-grained Goi-Taikei semantic classes. For many applications, we expect that this level of disambiguation is all that is required."
    }
}