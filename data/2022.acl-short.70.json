{
    "article": "Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework. Introduction Suicide is a global phenomenon responsible for 1.3% of deaths worldwide (WHO, 2019) . While it is the leading cause of death among 14-35 year olds in the US (Hedegaard et al., 2021) , suicide rates have increased by 13% in Japan between July to September 2020 (Tanaka and Okamoto, 2021) . It hence becomes critical to extend clinical and psychiatric care, which relies heavily on identifying those at risk. While 80% of patients do not undergo clinical treatment, 60% of those who succumbed to suicide denied having suicidal thoughts to mental health experts (McHugh et al., 2019) . However, studies show eight out of ten people shared suicidal thoughts on social media (Golden et al., 2009) . The advent of Natural Language Processing (NLP) shows promise for suicide risk assessment based on online user behavior (Ji et al., 2021b; Figure 1 : End-to-end pipeline for suicide risk assessment. When SASI assesses the posts, it returns the predicted risk level along with a certainty score. With a human-in-the-loop framework, these predictions can be sorted into various risk levels. SASI assigns high priority to uncertain predictions, for an immediate review by mental health experts. Choudhury et al., 2016) , with automatic risk assessment algorithms outperforming traditional clinical methods (Coppersmith et al., 2018; Linthicum et al., 2019) . Numerous deep learning methods already exist, which include leveraging suiciderelated word-embeddings (Cao et al., 2019) , social graphs (Mishra et al., 2019; Sinha et al., 2019; Cao et al., 2022; Sawhney et al., 2021b) and historical context (Matero et al., 2019; Gaur et al., 2019) . However, mental health is a safety-critical realm, where technological failure could lead to severe harm to users on social media (Sittig and Singh, 2015) . One such case was covered by Register (2020), wherein a medical bot suggested a mock patient kill themselves, demonstrating that unintended harmful behavior can emerge from AI systems (Amodei et al., 2016; Chandler et al., 2020) . Despite the significant power of traditional NLP methods, such models are inherently designed to make a prediction even when not confident. This poses a challenge when working with critical tasks like suicide risk assessment, for which it may be hard to make a prediction due to various reasons such as task hardness or contained ambiguity. Such a system may associate a lower risk level to a user who needs urgent help. A resulting delayed response from mental health experts may lead to adverse consequences. We hence need systems that assign high priority to uncertain predictions, for immediate review and response. Contributions: We reformulate suicide risk assessment as a prioritized prediction task which factors in uncertainty, and propose SASI: A Risk-Averse Mechanism for Suicidality Assessment on Social MedIa. SASI is risk-averse in the sense that it is self-aware, as it incorporates a selection function to measure uncertainty. Based on a set threshold value, SASI refrains from making a prediction when it is uncertain. We show that SASI can act as a tool to efficiently prioritize users who need immediate attention. Through a human-in-the-loop framework that involves a domain expert, SASI assigns high priority to uncertain predictions to avoid critical failure (Figure 1 ). We demonstrate the effectiveness of SASI using a real-world gold standard Reddit dataset. Through a series of experiments, we show SASI refrains from making 83% of incorrect predictions. We further demonstrate its effectiveness through a qualitative study and discuss the ethical implications. Methodology Columbia Suicide Severity Risk Scale The Columbia Suicide Severity Rating Scale (C-SSRS) is an authoritative questionnaire employed by psychiatrists to measure suicide risk severity (Posner et al., 2011) . There are 3 items in the scale: Suicide Ideation, Suicide Behavior, and Suicide Attempt. Each C-SSRS severity class is composed of a conceptually organized set of questions that characterize the respective category. Responses to the questions across the C-SSRS classes eventually determine the risk of suicidality of an individual (Interian et al., 2018; McCall et al., 2021) . One of the challenges researchers face when it comes to dealing with social media content is the disparity in the level of emotions expressed (Gaur et al., 2019) . Since the C-SSRS was originally designed for use in clinical settings, adapting the same metric to a social media platform would require changes to address the varying nature of emotions expressed. For instance, while in a clinical setting, it is typically suicidal candidates that see a clinician; on social media, non-suicidal users may participate to offer support to others deemed suicidal (Gaur et al., 2021) . To address these factors, two additional classes were defined (Gaur et al., 2019) to the existing C-SSRS scale with three classes: Suicide Indicator and Supportive (Negative class). Problem Formulation Following existing work (Gaur et al., 2019; Sawhney et al., 2021a) , we formulate the problem as a classification task to predict the suicidal risk of the user u i \u2208 {u 1 , u 2 , \u2022 \u2022 \u2022 , u N }, whose posts P i = {p i 1 , p i 2 , \u2022 \u2022 \u2022 , p i T } are authored over time in a chronological order, with the latest post being p i T . We denote the label set Y = {Support (SU), Indicator (IN), Ideation (ID), Behaviour (BR), Attempt (AT)} in increasing order of severity risk, defined based on the C-SSRS. For a given Suicide Ideation Model, our goal is to expand the cardinality of the label space to |Y| + 1 so as to enable an option to refrain when the model is uncertain. Suicide Ideation Model (SIM) Each post made by a user could provide detailed context of suicidal thought manifestation over time (Oliffe et al., 2012) . To capture this property, we draw inspiration from existing state-of-the-art (SOTA) models (Gaur et al., 2019; Matero et al., 2019; Sawhney et al., 2021a; Ji et al., 2021a) which use LSTM based backbones. To encode each post p i k , we use the 768-dimensional representation of the [CLS] token obtained from BERT (Devlin et al., 2019 ) as e i k =BERT(p i k ). As shown in Figure 2 , we then pass each post embedding sequentially through a bi-directional LSTM, given as h i k = Bi-LSTM(e i k ) . We thus obtain the sequence of hidden states, x = [h i 1 , h i 2 , \u2022 \u2022 \u2022 , h i T ], where h i k \u2208 R H , and H is the hidden dimension. To filter out relevant signals from the potentially vast user history (Shing et al., 2020) , we pass the hidden state sequence through an attention layer. The final layer is a multilayer perceptron (MLP) to obtain the prediction vector \u0177, given as: \u0177 = f (x), where f (x) = Softmax(MLP(Attention(x))) (1) Self-Aware Mechanism To make the model self-aware, we transform the model such that it makes a prediction only when certain (Liu et al., 2019) . As shown in Figure 2 , the model f : R T \u00d7H \u2192 Y is augmented with a selection function g : R T \u00d7H \u2192 (0, 1), which is an extra logit. The augmented model is described as a piece-wise function, given by: (f, g)(x) := Refrain, if g \u2265 \u03c4 argmax(\u0177), otherwise (2) Where the threshold \u03c4 \u2208 (0, 1), It is essential to note that the confidence threshold \u03c4 is not utilized during training, rather as a threshold variable to calibrate data coverage (cov) during evaluation. The cov fraction of total samples is what SASI predicts on, leaving out (1\u2212cov) samples for which SASI is most uncertain. Specifically, we can choose some value \u03c4 such that there will be (1 \u2212 cov) samples for which g \u2265 \u03c4 . The idea behind this approach is to trade-off (1 \u2212 cov) samples for immediate review by mental health experts in exchange for higher model performance on the cov samples about which it is confident. argmax(\u0177) \u2208 Y. Let p = (f, g)(x), Network Optimization In any m-class classification problem, if the model assigns a high probability score to the wrong class, then learning becomes difficult due to vanishing gradients (Ziyin et al., 2020) . To account for the additional refrain option in the augmented label space, we train SASI using Gambler's Loss (Liu et al., 2019 ). Gambler's loss allows the gradients to propagate through g instead, by abstaining from assigning weights to any of the m classes. Thus, the model learns a distribution of noisy/uncertain data points characterized by the selection function g. The loss function is given as: L = \u2212 |Y| j y j \u2022 log(\u0177 j \u2022 r + g) (3) where y j is the true label, and the reward r is a hyperparameter. A higher value of r discourages restraint. Since the loss function directly learns g, it does not depend on the coverage (Liu et al., 2019) , and can be manually set to any value during evaluation. 3 Experimental Setup Dataset We use the dataset released by Gaur et al. (2019) , which contains Reddit posts of 500 users filtered from an initial set of 270,000 users across several mental health and suiciderelated subreddits, such as r/StopSelfHarm (SSH), r/selfharm (SLF), r/bipolar (BPL), r/BipolarReddit (BPR), r/BipolarSOs, r/opiates (OPT), r/Anxiety (ANX), r/addiction (ADD), r/BPD, r/SuicideWatch (SW), r/schizophrenia (SCZ), r/autism (AUT), r/depression (DPR), r/cripplingalcoholism (CRP), and r/aspergers (ASP). The posts were annotated by practicing psychiatrists into five increasing risk levels based on the Columbia Suicide Severity Risk Scale (Posner et al., 2011) , leading to an acceptable average pairwise agreement of 0.79 and a groupwise agreement of 0.73. The class distribution of each category with increasing risk level is: Supportive (20%), Indicator (20%), Ideation (34%), Behaviour (15%), Attempt (9%). On average, the number of posts made by a user is 18.25\u00b127.45 with a maximum of 292 posts. The average number of tokens in each post is 73.4\u00b197.7. Evaluation Metrics We first describe the evaluation metrics that measure how well the model performs on the cov samples. Following Gaur et al. (2019) , we use graded variants of F1 score, Precision, and Recall, where we alter the formulation of False Negatives (FN) and False Positives (FP). FN is modified as the ratio of the number of times predicted severity of suicide risk level (k p ) is less than the actual risk level (k a ) over N number of samples. FP is the ratio of the number of times the predicted risk (k p ) is greater than the actual risk (k a ), given as: F N = N i=1 I(k a i > k p i ) N F P = N i=1 I(k p i > k a i ) N (4) Let P T denote the total number of test samples, P corr+refrain the sum of samples that have either been correctly predicted or have been refrained, P refrain the total number of refrained samples, and P in the number of incorrect predictions among the refrained samples. We additionally introduce two metrics, Robustness and Fail-Safe Rejects, as: Robustness = P corr+refrain P T Fail-Safe Rejects = P in P refrain (5) Robustness captures the fraction of samples which are correctly classified or instead sent for immediate review. Fail-Safe Rejects captures the fraction of refrained samples which were indeed erroneous. A higher Fail-Safe Rejects score hence implies that human moderators will be subjected to a lesser amounts of redundant work. Results Performance Comparison We compare the performance of SASI with various state-of-the-art baselines in (Cao et al., 2019) and ContextBERT (Matero et al., 2019) generally outperform ContextualCNN (Gaur et al., 2019) , which uses a bag-of-posts approach. SISMO (Sawhney et al., 2021a) shows further improvements by modeling the ordinal nature of risk labels. SASI significantly outperforms (p < 0.005) these methods for various values of coverage (cov), demonstrating its ability to avoid committing to erroneous predictions by characterizing its confidence (Liu et al., 2019) . Coverage and Performance Trade-off We further evaluate SASI for various values of target coverage (cov) by calibrating the threshold \u03c4 . As shown in Figure 3 , lower coverage leads to an increase in Graded Recall, Precision, and FScore (Table 1 ), as the model only keeps cov predictions which it is highly certain about. However, we observe a decrease in Fail-Safe Rejects due to an increasingly cautious approach employed by the model, which implies an increased fraction of originally correct predictions that need to be manually reviewed. We hence observe a trade-off, wherein we must seek to achieve competitive performance on the cov samples, while at the same time not overburden moderators with the (1 \u2212 cov) samples. For lower coverage values (say 50%), human modera-  For each user, we show the real labels next to predicted labels, while also indicating whether SASI refrained from making that prediction. We further demonstrate how SASI sorts the users into priority levels. All examples in this paper have been paraphrased as per the moderate disguise scheme (Bruckman, 2002) to protect user privacy. tors may be overburdened by having to review a lot of redundant samples. On the other hand, we note that SASI (85%) provides more utility, as it statistically outperforms SOTA models like SISMO, while maintaining a fail-safe rejection score of 83% and a competitive robustness score of 61%. Qualitative Analysis The essence of SASI lies behind its ability to refrain from making misleading predictions over high-risk samples. We study five users with snippets of their posts, as shown in Figure 4 . We observe the model makes erroneous predictions on high-risk users A and D. However, SASI refrains from committing to these predictions, assigning these users a high priority for immediate review and response. SASI chooses to refrain despite predicting the risk level of user B correctly, possibly because it employs a cautious approach due to phrases such as 'take my life' scattered in the user's timeline. This user, who is already of relatively high risk, is hence assigned a high priority. User E shows a very low sign of risk, which is confidently captured by SASI without needing to refrain. User C is an erroneous case wherein SASI is confident, yet makes a wrong prediction. However, the user is not high risk and gets assigned to the same priority level as the true risk label. While this example is not a cause for concern, certain situations may arise where SASI also confidently assigns a low-risk score to a high-risk user, opening avenues for future work that involves integrating and reformulating ordinal regression over the principles of Gambler's loss. Conclusion With a motivation to provide a robust solution to fine-grained suicide risk assessment on social media, we present SASI, a framework that integrates the concept of selective prioritization to existing deep learning based risk-assessment techniques. SASI is self-aware, wherein it refrains from making a prediction when uncertain, and instead assigns high priority to such data samples for immediate review by mental health experts. We demonstrated the effectiveness of SASI through quantitative evaluations on real-world data, wherein SASI avoided high-risk situations by refraining from making 83% of incorrect predictions. Through a qualitative analysis, we described how SASI can be used as a part of a human-in-the-loop framework, facilitating efficient responses from mental health experts. ment. The primary source of the dataset used in this study is Reddit. Although Reddit is intended for anonymous posting, we take further precautions by performing automatic de-identification of the dataset using named entity recognition (Zirikly et al., 2019) . All examples used in this paper are further been anonymized, obfuscated, and paraphrased for user privacy (Benton et al., 2017) and to prevent misuse as per the moderate disguise scheme suggested by Bruckman (2002) . Taking inspiration from Benton et al. (2017) , we also keep the annotation of user data separate from raw user data on protected servers linked only through anonymous IDs. Our work focuses on building an assistive tool for screening suicidal users and providing judgments purely based on observational capacity. We acknowledge that it is almost impossible to prevent abuse of released technology even when developed with good intentions (Hovy and Spruit, 2016) . Hence, we ensure that this analysis is shared only selectively to avoid misuse such as Samaritan's Radar (Hsin et al., 2016) . We further acknowledge that the studied data may be susceptible to demographic, expert annotator, and medium-specific biases (Hovy and Spruit, 2016) . While the essence of our work is to aid in the early detection of at-risk users and early intervention, any interventions must be well-thought, failing which may lead to counter-helpful outcomes, such as users moving to fringe platforms, making it harder to provide assistance (Kumar et al., 2015) . Care should be taken to not to create stigma, and interventions must hence be carefully planned by consulting relevant stakeholders, such as clinicians, designers, and researchers (Chancellor et al., 2016) , to maintain social media as a safe space for individuals looking to express themselves (Chancellor et al., 2019) . It is also essential that clinicians and human moderators are not overburdened (Chancellor et al., 2019) . For instance, \"Alarm fatigue\" is when alarms are so excessive, many of which are false positives, that healthcare providers become desensitized from alarms (Drew et al., 2014) . We also agree that suicidality is subjective (Keilp et al., 2012) , wherein the interpretation may vary across individuals on social media (Puschman, 2017) . do not make any diagnostic claims, rather help prioritize the users that should be evaluated by the medical professionals first, as part of a distributed human-in-the-loop framework (de Andrade et al., 2018) . Acknowledgements We thank Prof. Amit Sheth for reviewing the paper and providing valuable feedback and support. We would also like to thank the anonymous reviewers for their insightful suggestions on various aspects of this work. Ethical Considerations We work within the scope of acceptable privacy practices suggested by Chancellor et al. (2019) and considerations presented by Fiesler and Proferes (2018) to avoid coercion and intrusive treat- ",
    "abstract": "Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework.",
    "countries": [
        "India",
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": "1",
    "year": "2022",
    "month": "May",
    "title": "A Risk-Averse Mechanism for Suicidality Assessment on Social Media"
}