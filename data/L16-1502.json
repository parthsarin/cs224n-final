{
    "article": "Dialogue breakdown detection is a promising technique in dialogue systems. To promote the research and development of such a technique, we organized a dialogue breakdown detection challenge where the task is to detect a system's inappropriate utterances that lead to dialogue breakdowns in chat. This paper describes the design, datasets, and evaluation metrics for the challenge as well as the methods and results of the submitted runs of the participants. Introduction Although voice agent services are beginning to appear on the market, the limited capabilities of these systems mean that humans and machines still cannot converse as naturally as two humans. The main problem is that systems typically make inappropriate utterances that lead to dialogue breakdowns. By dialogue breakdown, we mean a situation in a dialogue where users cannot proceed with the conversation (Martinovsky and Traum, 2003) . To avoid this situation, technology for dialogue breakdown detection is essential because such technology will enable systems to avoid the creation of inappropriate utterances and also to identify dialogue breakdowns when they occur and perform the necessary recovery procedures. This paper reports the results of our \"Dialogue breakdown detection challenge\", which is an evaluation workshop dedicated to dialogue breakdown detection. The aim of the challenge was to bring together various methods for dialogue breakdown detection and discuss possible evaluation metrics. Although there have been other dialoguerelated evaluation campaigns, including a spoken dialogue challenge (Black and Eskenazi, 2009) and a dialogue state tracking challenge (Williams et al., 2013) , our challenge is unique in that we deal with chat dialogues. We focus on chat because, compared to the errors in task-oriented dialogues (Bear et al., 1992; Carpenter et al., 2001; Bulyko et al., 2005) , we can observe more diverse dialogue breakdown phenomena (Higashinaka et al., 2015a) , which makes it possible to develop general and robust techniques for breakdown detection. Our challenge is also unique in that we deal with a highly subjective phenomenon, posing difficulties in creating appropriate evaluation metrics. In this paper, we present how we designed the task, prepared the datasets, and conceived the evaluation metrics for the challenge. The event was held in October 2015 and there were six participants. We also report the detection methods submitted by the participants and provide their results. We finish with a discussion of the validity of the evaluation metrics we utilized. Task description The task of the dialogue breakdown detection challenge is to detect whether the system utterance causes dialogue breakdown in a given dialogue context. For example, consider the following piece of dialogue: S1: It's nice to go shopping alone. U1: I agree. That's nice. S2: Shopping takes time. U2: Window shopping is also fun. S3: It's fun to go shopping with somebody. Here, system utterances are prefixed by S and user utterances by U. The dialogue context is from S1 to U2 and the target utterance for dialogue breakdown detection is S3 (underlined). In this example, S3 is likely to be causing a dialogue breakdown because S3 contradicts S1. Therefore, a detector that classifies this as a dialogue breakdown will be regarded as accurate. The reason we say \"likely\" is that a human's decision on dialogue breakdown is highly subjective and it is difficult to decide on a single gold label. For this reason, we use many annotators for dialogue breakdown annotation and opt for majority voting and their probability distribution as references. Given pairs of dialogue context and a succeeding system utterance, the participants of the challenge submit, for each pair, (1) a single dialogue breakdown label and (2) the probability distribution of the breakdown labels. Note that, although some utterances may exist after the target utterance, they cannot be used for prediction because, for this challenge, we focus on avoiding dialogue breakdown rather than recovery. In the challenge, each participant can submit up to three \"runs\", so several parameters for dialogue breakdown detection can be tested. Datasets We distributed two sets of data to participants: one consisting of training data and the other of development and test data. The training data are those that we previously made public as a \"chat dialogue corpus\" 1 . The development and test data were newly created for this challenge (these data have also been made public on the same website). Chat dialogue corpus The chat dialogue corpus contains 1,146 text chat dialogues conducted between human users and a chat system. The language is Japanese. The users were recruited from among dialogue researchers and their collaborators. We used a chat system based on NTT Docomo's chat API 2 , which is publicly available (technical details on the chat API can be found in (Onishi and Yoshimura, 2014) ). Each dialogue contains 21 utterances (one system prompt followed by ten utterances each from the system and user in an alternate manner). See (Higashinaka et al., 2015a; Higashinaka et al., 2015b; Higashinaka et al., 2015c) for details on the data; the types of errors made by the system are also discussed in these studies. The chat dialogue corpus is divided into two parts: init100, which contains 100 dialogues with dialogue breakdown annotation by 24 annotators for each system utterance, and rest1046, which was annotated by two to three annotators. The following three breakdown labels were used: (NB) Not a breakdown: It is easy to continue the conversation. (PB) Possible breakdown: It is difficult to continue the conversation smoothly. (B) Breakdown: It is difficult to continue the conversation. Here, the labels were annotated depending on how easy/difficult it is to continue the conversation after each system utterance if the annotators were the dialogue participants in the dialogues in question; they did not predict if dialogue breakdown would actually happen or not in subsequent dialogues because it would be too difficult a task and would be similar to random guessing. The statistics of the data are shown in Table 1 . As indicated by Fleiss' \u03ba, the breakdown annotation is highly subjective, which led to our decision to use majority voting and distribution-based evaluation metrics for evaluation (see the next section). Development and test sets For the challenge, we newly collected dialogue data and annotated them in the same manner as we collected the chat dialogue corpus, except that we used crowdsourcing (CrowdWorks 4 for dialogue collection and Yahoo! Crowdsourcing 5 for annotation). Here, each system utterance was annotated by 30 annotators. Fleiss' \u03ba is lower than that for the chat dialogue corpus, probably because the dialogues were annotated by the general public rather than researchers, who have some notion of how the system may Evaluation metrics Since there are no established metrics for dialogue breakdown detection, we enumerated possible metrics. We created two types of evaluation metrics: classification-related and distribution-related. Classification-related metrics Classification-related metrics evaluate the accuracy related to the classification of the breakdown labels. Here, the accuracy is calculated by comparing the output of the detector and the gold label determined by majority voting. We use a threshold t to obtain the gold label: that is, we first find the majority label and check if the ratio of that label is above t. If so, the gold label becomes that label and NB otherwise. We used the following metrics. \u2022 Accuracy: The number of correctly classified labels divided by the total number of labels to be classified. \u2022 Precision, Recall, F-measure (B): The precision, recall, and F-measure for the classification of the B labels. \u2022 Precision, Recall, F-measure (PB+B): The precision, recall, and F-measure for the classification of PB + B labels; that is, PB and B labels are treated as a single label. These metrics can provide intuitive results about the detection of dialogue breakdowns because they directly evaluate whether dialogue breakdowns are correctly classified or not. However, the choice of an appropriate t value remains an open issue. Distribution-related metrics Distribution-related metrics evaluate the similarity of the distribution of the breakdown labels, which is calculated by comparing the predicted distribution of the labels with that of the gold labels. We used the following metrics. \u2022 JS Divergence (NB,PB,B): Distance between the predicted distribution of the three labels and that of the gold labels calculated by Jensen-Shannon Divergence. \u2022 JS Divergence (NB,PB+B): JS divergence when PB and B are regarded as a single label. \u2022 JS Divergence (NB+PB,B): JS divergence when NB and PB are regarded as a single label. \u2022 Mean Squared Error (NB,PB,B): Distance between the predicted distribution of the three labels and that of the gold labels calculated by mean squared error. \u2022 Mean Squared Error (NB,PB+B): Mean squared error when PB and B are regarded as a single label. \u2022 Mean Squared Error (NB+PB,B): Mean squared error when NB and PB are regarded as a single label. These metrics compare the distributions of the labels, thus enabling a direct comparison with the gold labels. However, the results may not be as easily interpretable as the classification-related metrics because they do not directly translate to detection performance. Evaluation workshop Six teams of participants, hereafter referred to as Team 1 through Team 6, participated in the challenge. Below, we briefly overview the methods of the participants in addition to the baseline we prepared. The names/institutions of the participants are not stated here because the aim of the workshop was not to encourage competition and we hoped anonymization would encourage participation from both academia and industry. Although not linked with the team numbers in this paper, the details of the six teams' methods can be found in (Horii and Araki, 2015; Taniguchi and Kano, 2015; Kobayashi et al., 2015; Mizukami et al., 2015; Sugiyama, 2015; Inaba and Takahashi, 2015) . Methods Representing the current trends, four participants used deep learning or deep neural networks (DNNs). There was also one rule-based method and one SVM-based method to round out the six. Our baseline was based on conditional random fields (CRFs) (Lafferty et al., 2001) . Although space constraints prevent us from going into detail about the methods, brief descriptions are provided below. In these descriptions, RNN, LSTM, and NCM stand for recurrent neural network, long short-term memory, and neural conversational model (Vinyals and Le, 2015) , respectively. Baseline CRF-based method. The detector labels utterance sequences with the three breakdown labels. The features used are words in the target utterance and its previous utterances. To determine the gold label for training, the baseline uses the same threshold t as in the classification-related metrics. Team 1 LSTM-RNN-based method. The features used are word vector, co-occurrence frequency vector for words between system and user utterances, and a vector representing word and co-occurrence frequency vectors created by Sent2Vec (an extension of Word2Vec (Mikolov et al., 2013) ). Run 1 used RNN and Run 2 used LSTM. Team 2 LSTM-RNN-based method. The features used are word frequency vectors based on Word2Vec. The runs differ in the structures of the RNN. Team 3 Rule-based method. Keywords are extracted from user and system utterances and, on the basis of the keywords, heuristic rules are applied to detect whether breakdown has occurred. The runs differ in the rules applied. Team 4 SVM-based method. The features used are word frequency vectors of the system and previous user utterance. The runs differ in the degree of the kernel used. Team 5 DNN-based method. The features used are the dialogue act of the system and the previous user utterance, the dialogue act the system should produce next, the perplexity of the system utterance calculated from the word n-grams of valid utterances, and the question classification result for the user utterance. The runs differ in the data to which the parameters were tuned. Team 6 LSTM-RNN-based method. The features used are the word vector encoded by use of NCM, LSTM, bagof-word embedding, and an extended NCM. Only one run was submitted. Results Tables 2 and 3 show the results of the submitted runs of the participants in classification-related and distribution-related metrics, respectively. For the classification-related metrics, Team 5, who adopted a DNN-based method, achieved the highest F-measure. Team 6, who also used deep learning (LSTM-RNN), achieved good precision. Here, Team 5 used extensive external knowledge (dialogue-act annotated corpus, database of questions and answers, etc.), which probably led to its superiority in recall. We should point out that B label is more difficult to classify compared to PB+B; the best Fmeasure for the former is 0.468 while that for the latter is 0.798. This indicates that currently it is difficult to distinguish between PB and B, but it is possible to distinguish breakdowns from non-breakdowns. As for the distribution-related metrics, Team 6 achieved the best performance, followed by Team 2. They both utilized LSTM-RNN-based methods. As we discussed in the metrics section, it is difficult to interpret the performance of these detectors from the numeric values. However, it is interesting to note that the best performing team in the classification-related metrics did not perform as well in these metrics. It will be worthwhile to investigate further the relationship between the two types of metric (classification-based and distribution-based). It will also be necessary to examine which type of metric is most suitable in terms of improving end-to-end dialogue systems. For this purpose, we aim to build dialogue systems that utilize these detectors and calculate the correlations between the metrics of these values and the overall performance of the systems. Summary and future work We described our dialogue breakdown detection challenge in which the task was to detect dialogue breakdowns in chat dialogue. We created datasets, determined the eval-uation metrics, and held the event. Results of the submitted runs of the participants demonstrate that DNN-based methods work sufficiently well, as they enable breakdowns to be distinguished from non-breakdowns with an F-measure of 0.798. However, it is still difficult to detect severe breakdowns with high accuracy. In future work, we want to pursue the best metrics for dialogue breakdown detection and find relationships between the evaluation metrics we enumerated. We aim to hold a second challenge to further improve the detection performance so that dialogue systems with fewer breakdowns can be achieved. In addition, we want to deal with different languages and modalities, since we only dealt with Japanese and text chat in the challenge reported here.",
    "abstract": "Dialogue breakdown detection is a promising technique in dialogue systems. To promote the research and development of such a technique, we organized a dialogue breakdown detection challenge where the task is to detect a system's inappropriate utterances that lead to dialogue breakdowns in chat. This paper describes the design, datasets, and evaluation metrics for the challenge as well as the methods and results of the submitted runs of the participants.",
    "countries": [
        "Japan"
    ],
    "languages": [
        "Japanese"
    ],
    "numcitedby": "58",
    "year": "2016",
    "month": "May",
    "title": "The dialogue breakdown detection challenge: Task description, datasets, and evaluation metrics"
}