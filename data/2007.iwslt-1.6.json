{
    "article": "We present the University of Edinburgh's submission for the IWSLT 2007 shared task. Our efforts focused on adapting our statistical machine translation system to the open data conditions for the Italian-English task of the evaluation campaign. We examine the challenges of building a system with a limited set of in-domain development data (SITAL), a small training corpus in a related but distinct domain (BTEC), and a large out of domain corpus (Europarl). We concentrated on the corrected text track, and present additional results of our experiments using the open-source Moses MT system with speech input. Introduction The IWSLT 2007 shared task offered the University of Edinburgh a chance to expand our experience with spoken language translation and with translation using data from multiple corpora. We focused on the Italian-English challenge task because it offered a chance to explore spontaneous speech as well as an opportunity to use a corpus we are familiar with, Europarl [1] , as an additional data resource. In this paper we first present a summary of the phrasebased statistical machine translation system used for this shared task. We go on to discuss the data sources we used to train the system, and show the results of our analysis of the domains of the development test data sets compared to the corpora used for training. Having explained the framework and data used, in Section 5 we present the results of our experiments in crossdomain adaptation. In Section 6 we describe the experiments we conducted with ASR inputs to our system. Framework The Moses MT system The open source Moses [2] MT system was originally developed at the University of Edinburgh. It received a major boost through a 2006 Johns Hopkins workshop, and is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrasebased machine translation approach [3] . In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a maximum movement reordering limit is used. Phrase translation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation. The different system components h i (phrase translation probabilities, language model, etc.) are combined in a log-linear model to obtain the score for the translation e of an input sentence f: score(e, f) = exp i \u03bb i h i (e, f) (1) The weights of the components \u03bb i are set by minimum error rate training on held-out development data [4] . The basic components used in our experiments are: \u2022 two phrase translation probabilities (both p(e|f ) and p(f |e)) \u2022 two word translation probabilities (both p(e|f ) and p(f |e)) \u2022 phrase count \u2022 output word count \u2022 language model \u2022 distance-based reordering model \u2022 lexicalised reordering model For a more detailed description of this model, please refer to [5] . Lexicalised reordering There are various models for reordering words to match the target language's word order. A simplistic method is distance based reordering, which uses a factor \u03b4 n to penalise movements over n words. Moses also implements a more sophisticated method called lexicalised reordering [5] One of the recent additions to our system is the ability to use higher-order language models during decoding, and this TC-STAR translation task presented us with the opportunity to test it. For example, when our system is run using a 4-gram language model, the phrase in Equation 2 Recaser Our standard translation system is trained on and produces lowercased text. As the TC-STAR evaluation was on original-cased data, we implemented a recaser to capitalise our output translations. The recaser is a log-linear translation system with only two component features, a translation model and a language model. The decoding task then is to find the most probable n-Burch, H. Hoang, P. Koehn, M. Osborne, D. Talbot attention to the identity of the phrases being used. In lexicalised reordering, we learn for each phrase pair how likely it is to directly follow the previous phrase (monotone), to swap positions with the previous phrase (swap), or to not be connected at all with the previous phrase (discontinuous). An illustration of this is provided in Fig. 1 . We use bidirectional reordering, taking both the next and previous translated phrase into account [6] . As described above, phrase pairs are collected and classified based on their reordering type relative to other phrase alignments generated from the sentence pair: \u2022 monotone: a word alignment point to the top left exists \u2022 swap: an alignment point to the top right exists \u2022 discontinuous: no alignment points to the top left nor top right We can use the counts from this classification to learn a smoothed orientation probability distribution: p r (orientation|\u0113, f ) (2) Training data The IWSLT evaluation campaign for 2007 featured only the open data condition. One set of training data was provided in the BTEC domain. Five development sets were provided in the BTEC domain and one in the SITAL domain. For this evaluation, we chose to focus on the cleaned text transcriptions of the ASR data. Corpora A small BTEC corpus was provided. We used the raw Europarl data available online 1 to align over 800,000 Italian-English sentences and construct a corpus providing additional coverage for our translation system. See Table 1 for full corpora statistics. Development data Six development sets were provided for tuning and testing. Three of the sets (devsets 1, 2 and 3) did not provide ASR inputs. We focused on devsets 4, 5a, and 5b which contained non-punctuated Italian inputs and cased, punctuated English outputs. The inputs consisted of both ASR and cleaned text data. We converted the SLF inputs to a confusion network format that could be read by the Moses decoder. Additionally, we used the 1-best text input as another form of ASR data. Punctuation adjustments There is no punctuation in the BTEC and SITAL development sets that were derived from ASR. We addressed this by stripping punctuation from the source side for both Europarl and BTEC training corpora, thus creating an MT system that translates from un-punctuated Italian to punctuated English. The target side language model was capable of judging most punctuation re-insertion issues, and we added a small postprocessing script to eliminate multiple punctuation and ensure final punctuation. Phrase table coverage and domain perplexity A condition unique to the IWSLT 2007 Challenge Tasks is that the test set is not from the same domain as the training corpus. The BTEC corpus and development sets are tourismrelated sentences, and the SITAL data consists of dialogues between customers and phone operators at a travel agency. To analyse how similar the SITAL data was to the BTEC and Europarl corpora, we compared test sets from each of the 3 domains: \u2022 Europarl: WMT07 test2007 (2000 lines)  By using in-domain test sets as a baseline, we can see how much coverage each corpus's phrase table gives to the SITAL data. One way to measure how close our test data is to our training data is to look at the perplexity with respect to a language model. We trained a 5-gram language model using SRILM [7] on the source side of each of the BTEC and Europarl corpora, and computed the perplexity of each of the test sets. This is shown in Table 2 . From these two measures we can see that language model perplexity is much lower for the in-domain conditions. This gives us an idea of the \"distance\" between the domains of each test set. Devsets 4 and 5a are both in the BTEC corpus domain, and they have a similarly low perplexity with respect to that language model. As expected, the test2007 set has the lowest perplexity on the Europarl language model when compared to other sets. Interestingly, devset5b does not have as low of a perplexity as devsets 4 and 5a on either LM. This would seem to indicate that there is a difference in language between the SITAL data and the BTEC data. In Figs. 2 and 3 , we examine this relationship from the perspective of the phrase tables. We enumerated all phrases up to length 7 (our phrase table maximum) in each test set, and recorded what percentages of the unique phrases in each set were covered by the BTEC and Europarl phrase tables. Again, we see that the overall best coverage for a test set is provided by its in-domain phrase table (Europarl for test2007 and BTEC for devset4 and devset5a). In general, devset5b has fewer matches than the in-domain sets for both phrase tables. A subtle difference between devset5b and the BTEC test data (devset4, devset5a) is revealed in the unigram coverage for each test set. Devset5b actually has better single-word coverage than the two BTEC sets when measured on the Europarl phrase table, and falls between them on the BTEC phrase table. However, the coverage drops lower than the BTEC sets as soon as we consider phrases of two or more words. While the single word vocabulary has better coverage, multi-word phrases are not matched as well. We can see a possible explanation by looking at the number of unique words in each of the test sets. To control for set length, we examine the unigram and bigram counts only for the first 4,976 words of each file (the shortest test set, devset4, is 4,976 words long). This is shown in Table 3 . The SITAL set, devset5b, has less than half the number of unique words compared to the vocabulary of the other test sets. While many of these single words are present in both the BTEC and Europarl phrase tables, the bigrams they form are not. This may be a symptom of the devset5b SITAL data being spontaneous speech. Devset5b has a smaller set of vo- Experiments in cross-domain adaptation Given that the official test set for the evaluation would be in a domain not ideally covered by either corpus, we attempted to make all the data from each separate corpus available to the decoder. This can be looked at as a form of mixture modelling or cross-domain adaptation. There has been extensive research in the area of adapting SMT systems to new domains. Recent work has distinguished between cross-domain adaptation, where the domain of the test data is known ahead of time, and dynamic adaptation, where the system must adapt to the test domain on-the-fly without the ability to tune ahead of time [8] . In both cases, models from each domainspecific corpus are trained separately, and weighted relative to their fit with the test domain. In this case, the domain of the test set (SITAL) is known ahead of time. Given that there was only one set of development test data in the domain of the final test set, we chose to split devset5b into two equal halves, using one half for tuning and one half for testing. We refer to these sets below as devset5b-tune and devset5b-test. To minimise vocabulary shifts between individual dialogue sessions and speakers, we shuffled the sentences before splitting. Single corpus In these tests we used either the BTEC or Europarl corpus, and had only one phrase table, lexicalised reordering table, and language model. There is no adaptation in these cases, but each model is tuned to the SITAL data in devset5b-tune. Corpus combination The simplest way to combine two corpora is to append one to the other and train the system as if the data was from one cor-pus. Similar to single domain systems, there is still only one set of tables and language model. It is possible to do coarse weighting in this condition by duplicating the contents of one corpus multiple times within the file before training the system [9] . We conducted multiple experiments, incrementally increasing the number of copies of the smaller BTEC corpus from 1 to 8. We report our best results, which were obtained with a combination of six copies of BTEC and one copy of Europarl. Separate corpora In line with recent approaches to cross-domain adaptation, we take the components created from training each corpus separately and combine them at decoding time within one translation model. Using Moses' architecture, we can add a second language model and reordering model as additional, separate features in the model. In addition, we can use the multiple alternative decoding paths functionality to utilise multiple phrase tables [10] . We use two decoding paths (one for each corpus), each consisting of only one translation step. Like our previous approach to domain adaptation [11] , we maintain two separate phrase tables and language models. Unlike our previous work, we allow for two reordering tables, instead of one combined table as used in the previous system. While duplicating our previous best setup for separate language models and phrase tables, we tested using a BTEC lexicalised reordering table, a Europarl table, a combined table generated during the corpus combination experiment with one copy of each corpus, and finally, two separate lexicalised reordering tables. Cross-domain adaptation results The results of each of the approaches described above are presented in Table 4 . We show BLEU scores for both 1best and text input from devset5b-test. While having one combined reordering table and two separate reordering tables produced identical BLEU scores for the devset5b-test text data, the two table approach had a much better score on 1-best input. Experiments in ASR input The Moses MT system is capable of handling ASR input [12] . Previous work has shown that higher-scoring systems can be produced by using a confusion network than by using the 1best ASR output as a text translation source. We investigated this in the case of the SITAL devset5b speech data, provided in SLF format. For the results presented here in Table 5 , we used devset4, devset5a and devset5b-tune as tuning sets, and devset4, de-vset5a and devset5b-test as development test sets. We tuned and tested on confusion network (CN), 1-best, and cleaned text input. For devsets 4 and 5a we did not split into -test and -tune subsets, so only results on the non-tuned set are shown. The first three experiments use the single BTEC cor- As one would expect given previous work, the best ASR scores for devsets 4 and 5a are found on confusion network input using weights derived from confusion network tuning. However, we were unable to obtain satisfactory performance on the devset5b test set using confusion network input. The highest score for devset5b-test ASR input is always found in the 1-BEST column, not the CN column, and three of four times it is found from weights generated with text input tuning, not 1-best input. Experiments in cross-domain adaptation for ASR were unsuccessful for confusion network tuning: due to time and memory restrictions we were unable to complete tuning runs for the full separate corpora setup that was our most successful system for cross-domain text translation adaptation. Results Based on the experiments above, we used the cross-domain adaptation weights with separate phrase and reordering tables and language models for each corpus for our official evaluation submission. Though we ran many experiments with the confusion network functionality of Moses, we were unable to satisfactorily tune the final system under confusion network input and had disappointing results with 1-best tuning. So, for the ASR track we submitted our text-tuned settings on 1-best input. Conclusions Our analysis of the SITAL data and experiments in crossdomain adaptation confirmed the benefits of using multiple corpora when translating from test sets without a direct indomain training corpus, or with limited in-domain training data. For the two development test sets within the BTEC domain, our ASR results were in agreement with previous work There are a number of possible causes for this result. At the data creation stage, it is possible that the ASR data is of a different quality for the SITAL input. During the tuning phase, the lack of multiple references for the SITAL data may have made tuning less effective. Final scoring may have been affected by the small number of sentences and single references, making it difficult to get accurate scores. It is also possible that the lack of an in-domain training corpus made it more difficult to translate the noisier ASR inputs. In future work, we would like to improve the ability of the Moses system to dynamically cache phrase and reordering table entries for confusion network input. This would enable quicker confusion network decoding with a lower memory footprint than is currently possible, allowing us to scale to much larger training corpora. We would also like to test alternate methods for balancing and tuning multiple corpora within our system, scaling beyond two sets of training data. Acknowledgements Thanks to Chris Dyer at the University of Maryland for scripts to process SLF data to Moses PCN format, and assistance with our confusion network confusion.",
    "abstract": "We present the University of Edinburgh's submission for the IWSLT 2007 shared task. Our efforts focused on adapting our statistical machine translation system to the open data conditions for the Italian-English task of the evaluation campaign. We examine the challenges of building a system with a limited set of in-domain development data (SITAL), a small training corpus in a related but distinct domain (BTEC), and a large out of domain corpus (Europarl). We concentrated on the corrected text track, and present additional results of our experiments using the open-source Moses MT system with speech input.",
    "countries": [
        "United Kingdom"
    ],
    "languages": [
        "Italian",
        "English"
    ],
    "numcitedby": "7",
    "year": "2007",
    "month": "October 15-16",
    "title": "The {U}niversity of {E}dinburgh system description for {IWSLT} 2007"
}