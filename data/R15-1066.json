{
    "article": "We describe an algorithm for automatic classification of idiomatic and literal expressions. Our starting point is that idioms and literal expressions occur in different contexts. Idioms tend to violate cohesive ties in local contexts, while literals are expected to fit in. Our goal is to capture this intuition using a vector representation of words. We propose two approaches: (1) Compute inner product of context word vectors with the vector representing a target expression. Since literal vectors predict well local contexts, their inner product with contexts should be larger than idiomatic ones, thereby telling apart literals from idioms; and (2) Compute literal and idiomatic scatter (covariance) matrices from local contexts in word vector space. Since the scatter matrices represent context distributions, we can then measure the difference between the distributions using the Frobenius norm. We provide experimental results validating the proposed techniques. Introduction Despite the common belief that idioms are always idioms, potentially idiomatic expressions, such as hit the sack can appear in literal contexts. Fazly et al. (2009) 's analysis of 60 idioms from the British National Corpus (BNC) has shown that close to half of these also have a clear literal meaning; and of those with a literal meaning, on average around 40% of their usages are literal. Therefore, idioms present great challenges for many Natural Language Processing (NLP) applications. Most current translation systems rely on large repositories of idioms. In this paper we describe an algorithm for automatic classification of idiomatic and literal expressions. Similarly to Peng et al. (2014) , we treat idioms as semantic outliers. Our assumption is that the context word distribution for a literal expression will be different from the distribution for an idiomatic one. We capture the distribution in terms of covariance matrix in vector space. Previous Work Previous approaches to idiom detection can be classified into two groups: 1) type-based extraction, i.e., detecting idioms at the type level; 2) token-based detection, i.e., detecting idioms in context. Type-based extraction is based on the idea that idiomatic expressions exhibit certain linguistic properties such as non-compositionality that can distinguish them from literal expressions (Sag et al., 2002; Fazly et al., 2009) . While many idioms do have these properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al., 2007) . Katz and Giesbrech (2006) , Birke and Sarkar (2006) , Fazly et al. (2009) , Sporleder and Li (2009) , Li and Sporleder (2010) , among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for idiom recognition.To address these problems, Peng et al. (2014) investigate the bag of words topic representation and incorporate an additional hypothesiscontexts in which idioms occur are more affective. Still, they treat idioms as semantic outliers. Proposed Techniques We hypothesize that words in a given text segment that are representatives of a common topic of discussion are likely to associate strongly with a literal expression in the segment, in terms of projection (or inner product) of word vectors onto the vector representing the literal expression. We also hypothesize that the context word distribution for a literal expression in word vector space will be different from the distribution for an idiomatic one. Projection Based On Local Context Representation The local context of a literal target verb-noun construction (VNC) must be different from that of an idiomatic one. We propose to exploit recent advances in vector space representation to capture the difference between local contexts (Mikolov et al., 2013a; Mikolov et al., 2013b) . A word can be represented by a vector of fixed dimensionality q that best predicts its surrounding words in a sentence or a document (Mikolov et al., 2013a; Mikolov et al., 2013b) . Given such a vector representation, our first proposal is the following. Let v and n be the vectors corresponding to the verb and noun in a target verb-noun construction, as in blow whistle, where v, n \u2208 q . Let \u03c3 vn = v + n \u2208 q . Thus, \u03c3 vn is the word vector that represents the composition of verb v and noun n, and in our example, the composition of blow and whistle. As indicated in (Mikolov et al., 2013b) , word vectors obtained from deep learning neural net models exhibit linguistic regularities, such as additive compositionality. Therefore, \u03c3 vn is justified to predict surrounding words of the composition of, say, blow and whistle. For a given vocabulary of m words, represented by matrix V = [v 1 , v 2 , \u2022 \u2022 \u2022 , v m ] \u2208 q\u00d7m , we cal- culate the projection of each word v i in the vocab- ulary onto \u03c3 vn P = V t \u03c3 vn (1) where P \u2208 m , and t represents transpose. Here we assume that \u03c3 vn is normalized to have unit length. Thus, P i = v t i \u03c3 vn indicates how strongly word vector v i is associated with \u03c3 vn . This projection forms the basis for our proposed technique. Let D = {d 1 , d 2 , \u2022 \u2022 \u2022 , d l } be a set of l text segments, each containing a target VNC (i.e., \u03c3 vn ). Instead of generating a term by document matrix, where each term is tf-idf (product of term frequency and inverse document frequency), we compute a term by document matrix M D \u2208 m\u00d7l , where each term in the matrix is p \u2022 idf, (2) the product of the projection of a word onto a target VNC and inverse document frequency. That is, the term frequency (tf) of a word is replaced by the projection (inner product) of the word onto \u03c3 vn (1). Note that if segment d j does not contain word v i , M D (i, j) = 0, which is similar to tf-idf estimation. The motivation is that topical words are more likely to be well predicted by a literal VNC than by an idiomatic one. The assumption is that a word vector is learned in such a way that it best predicts its surrounding words in a sentence or a document (Mikolov et al., 2013a; Mikolov et al., 2013b) . As a result, the words associated with a literal target will have larger projection onto a target \u03c3 vn . On the other hand, the projections of words associated with an idiomatic target VNC onto \u03c3 vn should have a smaller value. We also propose a variant of p \u2022 idf representation. In this representation, each term is a product of p and typical tf-idf. That is, p \u2022 tf \u2022 idf. (3) Local Context Distributions Our second hypothesis states that words in a local context of a literal expression will have a different distribution from those in the context of an idiomatic one. We propose to capture local context distributions in terms of scatter matrices in a space spanned by word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b) . Let d = (w 1 , w 2 \u2022 \u2022 \u2022 , w k ) \u2208 q\u00d7k be a segment (document) of k words, where w i \u2208 q are represented by a vectors (Mikolov et al., 2013a; Mikolov et al., 2013b) . Assuming w i s have been centered, we compute the scatter matrix \u03a3 = d t d, (4) where \u03a3 represents the local context distribution for a given target VNC. Given two distributions represented by two scatter matrices \u03a3 1 and \u03a3 2 , a number of measures can be used to compute the distance between \u03a3 1 and \u03a3 2 , such as Choernoff and Bhattacharyya distances (Fukunaga, 1990) . Both measures require the knowledge of matrix determinant. In our case, this can be problematic, because \u03a3 (4) is most likely to be singular, which would result in a determinant to be zero. We propose to measure the difference between \u03a3 1 and \u03a3 2 using matrix norms. We have experimented with the Frobenius norm and the spectral norm. The Frobenius norm evaluates the difference between \u03a3 1 and \u03a3 2 when they act on a standard basis. The spectral norm, on the other hand, evaluates the difference when they act on the direction of maximal variance over the whole space. Experiments Methods We have carried out an empirical study evaluating the performance of the proposed techniques. For comparison, the following methods are evaluated: 1 tf-idf: compute term by document matrix from training data with tf-idf weighting; 2 pidf: compute term by document matrix from training data with proposed p-idf weighting (2); 3 p*tfidf: compute term by document matrix from training data with proposed p*tf-idf weighting (3); 4 CoVAR Fro : compute literal and idiomatic scatter matrices from training data (4). For a test example, compute a scatter matrix according to (4). Calculate the distance between the test scatter matrix and training scatter matrices using Frobenius norm; and 5 CoVAR Sp : compute literal and idiomatic scatter matrices from training data (4). For a test text segment, compute a scatter matrix according to (4). Calculate the distance between the test scatter matrix and training scatter matrices using the spectral norm. For methods from 1 to 3, we compute a space from a term by document matrix obtain from the training data that captures 80% variance. To classify a test example, we compute cosine similarity between the test example and the training data in the latent space to make a decision. Data Preprocessing We use BNC (Burnard, 2000) and a list of verbnoun constructions (VNCs) extracted from BNC by Fazly et al. (2009) , Cook et al. (2008) and labeled as L (Literal), I (Idioms), or Q (Unknown). The list contains only those VNCs whose frequency was greater than 20 and that occurred at least in one of two idiom dictionaries (Cowie et al., 1983; Seaton and Macaulay, 2002) . The dataset consists of 2,984 VNC tokens. For our experiments we only use VNCs that are annotated as I or L. We only experimented with idioms that can have both literal and idiomatic interpretations. We use the original SGML annotation to extract paragraphs from BNC. Each document contains three paragraphs: a paragraph with a target VNC, the preceding paragraph and following one. Since BNC did not contain enough examples, we extracted additional from COCA, COHA and GloWbE (http://corpus.byu.edu/). Two human annotators annotated this new dataset for idioms and literals. The inter-annotator agreement was relatively low (Cohen's kappa = .58); therefore, we merged the results keeping only those entries on which the two annotators agreed. Word Vectors For our experiments reported here, we obtained word vectors using the word2vec tool (Mikolov et al., 2013a; Mikolov et al., 2013b) and the text8 corpus. The text8 corpus has more than 17 million words, which can be obtained from mattmahoney.net/dc/text8.zip. The resulting vocabulary has 71,290 words, each of which is represented by a q = 200 dimension vector. Thus, this 200 dimensional vector space provides a basis for our experiments. Datasets Table 1 describes the datasets we used to evaluate the performance of the proposed technique. All these verb-noun constructions are ambiguous between literal and idiomatic interpretations. The examples below (from the corpora we used) show how these expressions can be used literally. BlowWhistle: we can immediately turn towards a high-pitched sound such as whistle being blown. The ability to accurately locate a noise \u2022 \u2022 \u2022 Lose-Head: This looks as eye-like to the predator as the real eye and gives the prey a fifty-fifty chance of losing its head. That was a very nice bull I shot, but I lost his head. MakeScene: \u2022 \u2022 \u2022 in which the many episodes of life were originally isolated and there was no relationship between the parts, but at last we must make a unified scene of our whole life. TakeHeart: \u2022 \u2022 \u2022 cutting off one of the forelegs at the shoulder so the heart can be taken out still pumping and offered to the god on a plate. BlowTop: Yellowstone has no large sources of water to create the amount of steam to blow its top as in previous eruptions. Results Table 2 shows the average precision, recall and accuracy of the competing methods on 12 datasets over 20 runs. The best performance is in bold face. The best model is identified by considering precision, recall, and accuracy together for each model. We calculate accuracy by adding true positives and true negatives and normalizing the sum by the number of examples. As for the individual model performance, the CoVAR model outperforms the rest of the models. Interestingly, the Frobenius norm outperforms the spectral norm. One possible explanation is that the spectral norm evaluates the difference when two matrices act on the maximal variance direction, while the Frobenius norm evaluates on a standard basis. That is, Frobenius measures the difference along all basis vectors. On the other hand, the spectral norm evaluates changes in a particular direction. When the difference is a result of all basis directions, the Frobenius norm potentially provides a better measurement. The projection methods (p-idf and p*tf-idf) outperform tf-idf overall but not as pronounced as CoVAR. Finally, we have noticed that even the best model (CoVAR Fro ) does not perform as well on certain idiomatic expressions. We hypothesized that the model works the best on highly idiomatic expressions. Idiomaticity is a continuum. Some idioms seem to be more easily interpretable than others. We conducted a small experiment, in which we asked two human annotators to rank VNCs in our dataset as \"highly idiomatic\" to \"easily interpretable/compositional\" (in context) on a scale of 5 to 1 (5: highly idiomatic; 1: low idiomaticity). While we cannot make strong claims based on a such small-scale experiment, the results of our pilot study suggest that there is a correlation between the idiomaticity scores and the performance of our model -the highly idiomatic expressions seem to be detected better. We plan to conduct an experiment with more human annotators and on an larger dataset to verify our hypothesis. Conclusions In our experiments we used a subset of Fazly et al. (2009) 's dataset plus some additional examples extracted from other corpora. Similarly to us, Fazly et al. ( 2009 )'s goal is to determine whether a given VNC is idiomatic or literal in context. Our model is comparable to and often outperforms Fazly et al. (2009) 's unsupervised CForm model. Our method can also be compared with Peng et al. (2014) who also experiment with LDA, use similar data, and frame the problem as classification. Acknowledgements This material is based in part upon work supported by the U.S. National Science Foundation under Grant Number 1319846. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The article explicitly mentions that the material is based in part upon work supported by the U.S. National Science Foundation under Grant Number 1319846. The U.S. National Science Foundation is a government-funded organization that provides grants for research, which classifies it as a research agency. There is no mention of funding from defense, corporate entities, foundations, or an indication that there were no other funding sources."
}