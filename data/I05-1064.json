{
    "article": "A speech act is a linguistic action intended by a speaker. It is important to analyze the speech act for the dialogue understanding system because the speech act of an utterance is closely tied with the user's intention in the utterance. This paper proposes to use a speech acts hierarchy and a discourse stack for improving the accuracy of classifiers in speech acts analysis. We first adopt a hierarchical statistical technique called shrinkage to solve the data sparseness problem. In addition, we use a discourse stack in order to easily apply discourse structure information to the speech acts analysis. From the results of experiments, we observed that the proposed model made a significant improvement for Korean speech acts analysis. Moreover, we found that it can be more useful when training data is insufficient. Introduction To understand a natural language dialogue, a dialogue system must be able to make out the speaker's intentions indicated by utterances. Since the speech act of an utterance is very important in understanding a speaker's intentions, it is an essential part of a dialogue system. However, it is difficult to infer the speech act from a surface utterance because the utterance may represent more than one speech act according to the context [5] [7] . Various machine learning models have been used to efficiently classify speech acts such as MEM (Maximum Entropy Model) [1] , HMM (Hidden Markov Model) with Decision Tree [8] [11] , Neural Network Model [5] . And there are also studies on methods of automatically selecting efficient features with useful information for speech acts analysis [5] [10] . Since the machine learning models can efficiently analyze a large quantity of data and consider many different feature interactions, they can provide a means of associating features of utterances with particular speech acts. Generally, it is hard to create enough the number of examples for each speech act in the training examples. Thus this situation has been one of the main causes for errors occurred in speech acts analysis. That is, the sparse data problem from low frequency of some speech acts has commonly occurred in the previous research [8] . Due to the problem, the accuracy of each speech act in previous research tends to be proportional to the frequency of each speech act in the training data. Therefore, we first focus on how to scale up statistical learning methods to solve the sparseness problem of training data in speech acts analysis. Then we propose to construct the commonly-available hierarchies of speech acts and apply a well-understood technique from Statistics called shrinkage to our speech acts analysis system. It provides improved estimates of parameters that would otherwise be uncertain due to limited amounts of training data [3] . The technique uses a hierarchy to shrink parameter estimates in data sparse children toward the estimates of the data-rich ancestors in ways that are probably optimal under the appropriate conditions [9] . We employ a simple form of shrinkage that creates new parameter estimates for a child by a linear interpolation of all hierarchy nodes from the child to the root. In addition, discourse structure information can be used to identify the speech acts of utterances [1] . But most previous research has used only speech acts of previous utterances without considering discourse structure information to determine the speech act of current utterance. Therefore, in order to use discourse structure information for analyzing speech acts, we design a simple discourse stack. By using the discourse stack, the discourse structure information is easily applied to speech acts analysis. In this paper, we propose a new speech acts analysis model to improve the performance by using shrinkage and discourse structure information. From the results of experiments, the proposed system showed significant improvement in comparison with previous research. The rest of this paper is organized as follows. Section 2 explains the proposed speech acts analysis system in detail. In section 3, we discuss the empirical results in our experiments. The final section presents conclusions. The Proposed Speech Acts Analysis System The proposed system consists of two modules as shown in Fig. 1 : one module to extract features from training data and the other module to build up a hierarchy of Feature Extraction Sentence Features Extraction We assume that clue words and a sequence of POS tags in an utterance provide very effective information for analyzing the speech act of the current utterance. We extract informative features for speech acts analysis using a Morphological analyzer; they are called the sentence features. The sentence features consist of content words annotated with POS tags and POS bi-grams of all words in an utterance. Fig. 2 shows an example of sentence feature extraction. Input: \u21e8 \u1198\u03a8\u1164 \u2071\u1f6c\u250d\u11a9\u23fc\u2418. (My name is HongKildong.) Morphological analyzer Morphological analyzer The result of morphological analysis: \u21cc/np \u117c/j \u1198\u03a8/ncn \u1164/j \u2071\u1f6c\u250d/nq \u1198/jcp \u0dec\u23fc\u2418/ef ./s. (My/np name/ncn is/jcp HongKildong/nq ./s.) Feature extractor Feature extractor Content Words: \u21cc/np \u1198\u03a8/ncn \u2071\u1f6c\u250d/nq \u1198/jcp (My/np name/ncn HongKilgong/nq is/jcp) POS bi-grams: np-j j-ncn ncn-j j-nq, nq-jcp jcp-ef ef-s. Fig. 2 An example of sentence feature extraction Context Features Extraction Most previous research uses the speech act of previous utterance as context feature (CF1 in Table 1 ) [5] [8] . Since discourse structure information represents the relationship between two consecutive utterances, it is efficient to use discourse structure Use speech acts that pop in discourse stack and Sub-dialogue End (SE) else Use speech acts of previous utterance and Dialogue Continue (DC) End information for speech acts analysis [1] . Especially, the speech act of seventh utterance in Table 1 (UID: 7) is tied with that of second utterance (UID: 2). In our system, we first design a discourse stack to easily detect discourse structure information and extract the discourse structure information from the discourse stack for context features. Context features of our system consist of speech acts of previous utterance and markers of discourse structure information (CF2 in Table 1 ). An algorithm for discourse stack is described as the following: Table 1 . An example of Context Feature * UID: ID of utterances, DS: Discourse Structure, CF1: Using speech acts of previous utterances as features (Context Feature Type1), CF2: Using Discourse Structure Information by Discourse Stack as features (Context Feature Type2), Speech acts and discourse structure information were annotated by human. The Feature Weight Calculation by Shrinkage in a Hierarchy of Speech Acts Data sparseness is a common problem in mechanical learning fields. For speech acts analysis, the problem becomes more serious because it is a time-consuming and difficult task to collect dialogue examples and construct dialogue training data tagged with a lot of information for various application areas. Therefore, we apply the shrinkage technique to solve this data sparseness problem in speech acts analysis. The shrinkage technique was verified in its efficiency for text classification tasks learned with insufficient training data. Therefore, we first build up a hierarchy of speech acts to estimate the weight of features for each speech act by the shrinkage technique. The Hierarchy Construction for Speech Acts To model a dialogue system, the dialogue grammar has commonly used and it has observed that dialogues consist of adjacency pairs of the types of utterances such as UID DS Utterance Speech Acts CF1 CF2 [8] . Therefore, our speech acts hierarchy is built up according to this grammar. Table 2 shows the structure of our speech acts hierarchy. Mixture Weighting Model by Shrinkage in a Hierarchy of Speech Acts The shrinkage technique estimates the probability of a word as the weighted sum of the maximum-likelihood estimates from leaf to root in a hierarchy [9] . This estimate process can give us a possibility to resolve the data sparseness problem in some speech acts with insufficient examples. Fig. 3 shows that the shrinkage-based estimate of the probability of a feature (\"\u22ee/np\") given a speech act class (\"Accept\") is calculated from a weighted sum of the maximum-likelihood estimates from leaf to root. . We write j \u03b8 for the new estimate of the speech act-conditioned feature probabilities based on shrinkage. The new estimate for the probability of feature t f given speech act j s is as follows: 1 1 2 1 1 ... ) ; ( jt k j jt j jt j j j t jt \u03bb \u03b8 \u03b8 + + + = = . ( 1 ) We derive empirically optimal weights using the following iterative procedure: The SVM Classifier Support Vector Machines (SVM) is one of the state-of-the-art classifiers for classification tasks [6][12] . Since SVM has shown the high performance in various research areas, we also employ it in our method. In our method, we use the linear models offered by SVM light [4] and jt \u03b8 , which are calculated by formula (1) , are used as the feature weights of speech acts for the SVM classifier. Initialize: Set the j \u03bb 's to some initial values, say  3 Empirical Evaluation Experimental Data We used the Korean dialogue corpus which has used in previous research [1] [5] [8] . This corpus was transcribed from recordings in real fields such as hotel reservation, airline reservation and tour reservation and consists of 528 dialogues, 10,285 utterances (19.48 utterances per dialogue). Each utterance in dialogues is manually annotated with a speaker (SP), a speech act (SA) and a discourse structure (DS). This annotated dialogue corpus has 17 types of speech acts. Table 4 shows a part of the annotated dialog corpus and Table 5 shows the distribution of speech acts in the annotated dialogue corpus.  We divided the annotated dialogue corpus into the training data with 428 dialogues, 8,349 utterances (19.51 utterances per dialogue), and the testing data with 100 dialogues, 1,936 utterances (19.36 utterances per dialogue). Primary Experimental Results The Performances of Speech Acts Analysis Model Using Shrinkage and Discourse Stack In order to verify the proposed method, we made four kinds of speech acts analysis systems which use different kind of features. The Baseline System used default features such as sentence features and context features [5] . The Second system (Type 1) was built up to verify the shrinkage technique. Its features were the same as those of the first system but they were weighted by the shrinkage technique. The third System (Type 2) used the discourse structure information from the proposed discourse stack without shrinkage. Finally, the fourth system (Type 3) combined the discourse structure information and the shrinkage technique. Table 6 shows the results of four speech acts analysis systems. As shown in Table 6 , the performances of the proposed systems (Type 1,2,3) are better than the baseline system. The proposed system of Type 3 reported the best performance. The Improvement of the Proposed System Using the Shrinkage Technique in Sparse Data Here, we verify the facts that the shrinkage technique can improve the speech acts analysis when training data is sparse. We first compare the system with shrinkage (Type 3) and the system without shrinkage (Type 2). Fig. 4 shows the changes of performance in each number of training data from 250 to 8439. The proposed system with shrinkage obtains the better performance over all intervals in Fig. 4 . Especially, the shrinkage technique provides more improvement when the amount of training data is small. This is a proof that the shrinkage technique can become an effective solution for sparse data problem from insufficient training data.  We then compare performances between the system of Type 2 and the system of Type 3 according to distribution of each speech act. As shown in Fig. 5 , the proposed system (Type 3) with the shrinkage technique shows higher performance in speech acts with insufficient examples such as 'Accept', 'Closing', 'Promise' and 'Suggest'. \u0361 \u0361\u035f\u0362 \u0361\u035f\u0363 \u0361\u035f\u0364 \u0361\u035f\u0365 \u0361\u035f\u0366 \u0361\u035f\u0367 \u0361\u035f\u0368 \u0361\u035f\u0369 \u0361\u035f\u036a \u0362 \u03a3\u0396 \u03a4 \u03a1 \u03a0 \u039f \u03a4 \u0396 \u0392 \u03a4 \u039c \u035e \u03a3\u0396 \u0397 \u039a\u039f \u0397\u03a0 \u03a3\u039e \u039a\u039f \u03a5\u03a3 \u03a0 \u0395 \u03a6 \u0394 \u039a\u039f \u0398 \u035e \u03a0 \u039f \u0396 \u03a4 \u0396 \u039d\u0397 \u03a0 \u03a1 \u0396 \u039f \u039a\u039f \u0398 \u0396 \u03a9 \u03a1 \u03a3\u0396 \u03a4 \u03a4 \u039a\u03a7 \u0396 \u0392 \u03a4 \u039c \u035e \u039a\u0397 \u03a3\u0396 \u03a6 \u0396 \u03a4 \u03a5 \u0392 \u03a4 \u039c \u035e \u0394 \u03a0 \u039f \u0397\u039a \u03a3\u039e \u0394 \u039d\u03a0 \u03a4 \u039a\u039f \u0398 \u0392 \u0394 \u039c \u039f \u03a0 \u03a8 \u039d\u0396 \u0395 \u0398 \u0396 \u0392 \u0394 \u0394 \u0396 \u03a1 \u03a5 \u03a1 \u03a3\u03a0 \u039e \u039a\u03a4 \u0396 \u03a4 \u03a6 \u0398 \u0398 \u0396 \u03a4 \u03a5 \u03a3\u0396 \u039b\u0396 \u0394 \u03a5 \u03a0 \u0397\u0397 \u0396 \u03a3 \u0394 \u03a0 \u03a3\u03a3 \u0396 \u0394 \u03a5 \u03a4\u0399\u039a\u039f\u039c\u0392\u0398\u0396\u0351\u0359\u03a5\u03aa\u03a1\u0396\u0351\u0364\u035a \u039f\u03a0\u0351\u03a4\u0399\u03a3\u039a\u039f\u039c\u0392\u0398\u0396\u0351\u0359\u03a5\u03aa\u03a1\u0396\u0351\u0363\u035a \u0395\u039a\u03a4\u03a5\u03a3\u039a\u0393\u03a6\u03a5\u039a\u03a0\u039f\u0351\u03a0\u0397\u0351\u03a4\u03a1\u0396\u0396\u0394\u0399\u0351\u0392\u0394\u03a5 Fig. 5 . The comparison of the performances for the shrinkage technique according to the distribution of speech acts The Comparison of Performance with Speech Acts Analysis Models Table 7 shows results from the proposed model and previous speech acts analysis models: the maximum entropy model (MEM) [1] , the decision tree model (DTM) [8] , and the neural network model (NNM) [5] . We report the performance of each system when using the same test data set as that of this paper. As a result, the proposed model achieved the highest performance. The propose model 87.0% In the experiment, it is difficult to compare the proposed model directly with the other models because input features are different respectively. Even though direct comparisons are impossible, we think that the proposed model is more robust and efficient than MEM and DTM. In MEM and DTM, they used many kinds of high level linguistic knowledge than ours such as sentence type, tense, modality and so on. Nevertheless, the performances of them are lower than that of the proposed model. Moreover, the proposed model is more effective than NNM because the performance of the proposed model is better than that of NNM in spite of using same features. Conclusions In this paper, we proposed the new speech analysis model to improve speech acts analysis by using the shrinkage technique and the discourse stack. We first made a Other hierarchy of speech acts by dialogue grammar for shrinkage and then estimate the probability of each feature on the hierarchy by the shrinkage technique. In experimental results, the proposed model is more effective for classifying speech acts. Especially, the shrinkage technique achieved more improvement when training data is sparse. Therefore, the shrinkage technique can be applied to the real applications that suffer from the data sparseness problem. We also proposed to use the discourse stack for easily extracting discourse structure information. As a result, the proposed model with shrinkage and the discourse stack showed the better performance than other speech acts analysis models. Acknowledgement This research was supported as a Brain Neuroinformatics Research Program sponsored by the Ministry of Commerce, Industry and Energy of Korea.",
    "abstract": "A speech act is a linguistic action intended by a speaker. It is important to analyze the speech act for the dialogue understanding system because the speech act of an utterance is closely tied with the user's intention in the utterance. This paper proposes to use a speech acts hierarchy and a discourse stack for improving the accuracy of classifiers in speech acts analysis. We first adopt a hierarchical statistical technique called shrinkage to solve the data sparseness problem. In addition, we use a discourse stack in order to easily apply discourse structure information to the speech acts analysis. From the results of experiments, we observed that the proposed model made a significant improvement for Korean speech acts analysis. Moreover, we found that it can be more useful when training data is insufficient.",
    "countries": [
        "South Korea"
    ],
    "languages": [
        "Korean"
    ],
    "numcitedby": "1",
    "year": "2005",
    "month": "",
    "title": "Improving {K}orean Speech Acts Analysis by Using Shrinkage and Discourse Stack"
}