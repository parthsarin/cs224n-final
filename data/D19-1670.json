{
    "article": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use. Introduction Text classification is a fundamental task in natural language processing (NLP). Machine learning and deep learning have achieved high accuracy on tasks ranging from sentiment analysis (Tang et al., 2015) to topic classification (Tong and Koller, 2002) , but high performance often depends on the size and quality of training data, which is often tedious to collect. Automatic data augmentation is commonly used in computer vision (Simard et al., 1998; Szegedy et al., 2014; Krizhevsky et al., 2017) and speech (Cui et al., 2015; Ko et al., 2015) and can help train more robust models, particularly when using smaller datasets. However, because it is challenging to come up with generalized rules for language transformation, universal data augmentation techniques in NLP have not been thoroughly explored. Previous work has proposed some techniques for data augmentation in NLP. One popular study generated new data by translating sentences into French and back into English (Yu et al., 2018) . Other work has used data noising as smoothing (Xie et al., 2017) and predictive language models for synonym replacement (Kobayashi, 2018) . Although these techniques are valid, they are not often used in practice because they have a high cost of implementation relative to performance gain. In this paper, we present a simple set of universal data augmentation techniques for NLP called EDA (easy data augmentation). To the best of our knowledge, we are the first to comprehensively explore text editing techniques for data augmentation. We systematically evaluate EDA on five benchmark classification tasks, showing that EDA provides substantial improvements on all five tasks and is particularly helpful for smaller datasets. Code is publicly available at http://github. com/jasonwei20/eda_nlp. EDA Frustrated by the measly performance of text classifiers trained on small datasets, we tested a number of augmentation operations loosely inspired by those used in computer vision and found that they helped train more robust models. Here, we present the full details of EDA. For a given sentence in the training set, we randomly choose and perform one of the following operations: 1. Synonym Replacement (SR): Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random. Random Insertion (RI): Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times. 3. Random Swap (RS): Randomly choose two words in the sentence and swap their positions. Do this n times. 4. Random Deletion (RD): Randomly remove each word in the sentence with probability p. Since long sentences have more words than short ones, they can absorb more noise while maintaining their original class label. To compensate, we vary the number of words changed, n, for SR, RI, and RS based on the sentence length l with the formula n=\u03b1 l, where \u03b1 is a parameter that indicates the percent of the words in a sentence are changed (we use p=\u03b1 for RD). Furthermore, for each original sentence, we generate n aug augmented sentences. Examples of augmented sentences are shown in Table 1 . We note that synonym replacement has been used previously (Kolomiyets et al., 2011; Zhang et al., 2015; Wang and Yang, 2015) , but to our knowledge, random insertions, swaps, and deletions have not been extensively studied. Experimental Setup We choose five benchmark text classification tasks and two network architectures to evaluate EDA. Benchmark Datasets We conduct experiments on five benchmark text classification tasks: (1) SST-2: Stanford Sentiment Treebank (Socher et al., 2013) , (2) CR: customer reviews (Hu and Liu, 2004; Liu et al., 2015) , (3) SUBJ: subjectivity/objectivity dataset (Pang and Lee, 2004 ), (4) TREC: question type dataset (Li and Roth, 2002) , and (5) PC: Pro-Con dataset (Ganapathibhotla and Liu, 2008) . Summary statistics are shown in Table 5 in Supplemental Materials. Furthermore, we hypothesize that EDA is more helpful for smaller datasets, so we delegate the following sized datasets by selecting a random subset of the full training set with N train ={500, 2,000, 5,000, all available data}. Text Classification Models We run experiments for two popular models in text classification. (1) Recurrent neural networks (RNNs) are suitable for sequential data. We use a LSTM-RNN (Liu et al., 2016) . (2) Convolutional neural networks (CNNs) have also achieved high performance for text classification. We implement them as described in (Kim, 2014) . Details are in Section 9.1 in Supplementary Materials. Results In this section, we test EDA on five NLP tasks with CNNs and RNNs. For all experiments, we average results from five different random seeds. EDA Makes Gains We  average accuracy of 88.6% while only using 50% of the available training data. Does EDA conserve true labels? In data augmentation, input data is altered while class labels are maintained. If sentences are significantly changed, however, then original class labels may no longer be valid. We take a visualization approach to examine whether EDA operations significantly change the meanings of augmented sentences. First, we train an RNN on the pro-con classification task (PC) without augmentation. Then, we apply EDA to the test set by generating nine augmented sentences per original sentence. These are fed into the RNN along with the original sentences, and we extract the outputs from the last dense layer. We apply t-SNE (Van Der Maaten, 2014) to these vectors and plot their 2-D representations (Figure 2 ). We found that the resulting latent space representations for augmented sentences closely surrounded those of the original sentences, which suggests that for the most part, sentences augmented with EDA conserved the labels of their original sentences. Ablation Study: EDA Decomposed So far, we have seen encouraging empirical results. In this section, we perform an ablation study Pro (original) Pro (EDA) Con (original) Con (EDA) Figure 2 : Latent space visualization of original and augmented sentences in the Pro-Con dataset. Augmented sentences (small triangles and circles) closely surround original sentences (big triangles and circles) of the same color, suggesting that augmented sentences maintianed their true class labels. to explore the effects of each operation in EDA. Synonym replacement has been previously used (Kolomiyets et al., 2011; Zhang et al., 2015; Wang and Yang, 2015) , but the other three EDA operations have not yet been explored. One could hypothesize that the bulk of EDA's performance gain is from synonym replacement, so we isolate each of the EDA operations to determine their individual ability to boost performance. For all four operations, we ran models using a single oper- ation while varying the augmentation parameter \u03b1={0.05, 0.1, 0.2, 0.3, 0.4, 0.5} (Figure 3 ). It turns out that all four EDA operations contribute to performance gain. For SR, improvement was good for small \u03b1, but high \u03b1 hurt performance, likely because replacing too many words in a sentence changed the identity of the sentence. For RI, performance gains were more stable for different \u03b1 values, possibly because the original words in the sentence and their relative order were maintained in this operation. RS yielded high performance gains at \u03b1\u22640.2, but declined at \u03b1\u22650.3 since performing too many swaps is equivalent to shuffling the entire order of the sentence. RD had the highest gains for low \u03b1 but severely hurt performance at high \u03b1, as sentences are likely unintelligible if up to half the words are removed. Improvements were more substantial on smaller datasets for all operations, and \u03b1=0.1 appeared to be a \"sweet spot\" across the board. How much augmentation? The natural next step is to determine how the number of generated augmented sentences per original sentence, n aug , affects performance. In Figure 4 , we show average performances over all datasets for n aug ={1, 2, 4, 8, 16, 32}. For smaller train- ing sets, overfitting was more likely, so generating many augmented sentences yielded large performance boosts. For larger training sets, adding more than four augmented sentences per original sentence was unhelpful since models tend to generalize properly when large quantities of real data are available. Based on these results, we recommend usage parameters in Table 3 . 0 2 4 8 16 32 0 1 2 3 naug Performance Gain (%) N =500 N =2,000 N =5,000 Full Data N Comparison with Related Work Related work is creative but often complex. Backtranslation (Sennrich et al., 2016) , translational data augmentation (Fadaee et al., 2017) , and noising (Xie et al., 2017) have shown improvements in BLEU measure for machine translation. For other tasks, previous approaches include task-specific heuristics (Kafle et al., 2017) and back-translation (Silfverberg et al., 2017; Yu et al., 2018) . Regarding synonym replacement (SR), one study showed a 1.4% F1-score boost for tweet classification by finding synonyms with k-nearest neighbors using word embeddings (Wang and Yang, 2015) . Another study found no improvement in temporal analysis when replacing headwords with synonyms (Kolomiyets et al., 2011) , and mixed results were reported for using SR in character-level text classification (Zhang et al., 2015) ; however, neither work conducted extensive ablation studies. Most studies explore data augmentation as a complementary result for translation or in a taskspecific context, so it is hard to directly compare EDA with previous literature. But there are two studies similar to ours that evaluate augmentation techniques on multiple datasets. Hu (2017) proposed a generative model that combines a variational auto-encoder (VAE) and attribute discriminator to generate fake data, demonstrating a 3% gain in accuracy on two datasets. Kobayashi (2018) showed that replacing words with other words that were predicted from the sentence context using a bi-directional language model yielded a 0.5% gain on five datasets. However, training a variational auto-encoder or bidirectional LSTM language model is a lot of work. EDA yields results on the same order of magnitude but is much easier to use because it does not require training a language model and does not use external datasets. In Table 4 (Sennrich et al., 2016) for translation 6 (Kolomiyets et al., 2011) for temporal analysis 7 (Kobayashi, 2018) for text classification 8 (Wang and Yang, 2015) for tweet classification 9 EDA does use a synonym dictionary, WordNet, but the cost of downloading it is far less than training a model on an external dataset, so we don't count it as an \"external dataset.\" recent years, we suspect that researchers will soon find higher-performing augmentation techniques that will also be easy to use. Notably, much of the recent work in NLP focuses on making neural models larger or more complex. Our work, however, takes the opposite approach. We introduce simple operations, the result of asking the fundamental question, how can we generate sentences for augmentation without changing their true labels? We do not expect EDA to be the go-to augmentation method for NLP, either now or in the future. Rather, we hope that our line of thought might inspire new approaches for universal or task-specific data augmentation. Now, let's note many of EDA's limitations. Foremost, performance gain can be marginal when data is sufficient; for our five classification tasks, the average performance gain for was less than 1% when training with full datasets. And while performance gains seem clear for small datasets, EDA might not yield substantial improvements when using pre-trained models. One study found that EDA's improvement was negligible when using ULMFit (Shleifer, 2019) , and we expect similar results for ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) . Finally, although we evaluate on five benchmark datasets, other studies on data augmentation in NLP use different models and datasets, and so fair comparison with related work is highly non-trivial. Conclusions We have shown that simple data augmentation operations can boost performance on text classification tasks. Although improvement is at times marginal, EDA substantially boosts performance and reduces overfitting when training on smaller datasets. Continued work on this topic could explore the theoretical underpinning of the EDA operations. We hope that EDA's simplicity makes a compelling case for further thought. Acknowledgements We thank Chengyu Huang, Fei Xing, and Yifang Wei for help with study design and paper revisions, and Chunxiao Zhou for insightful feedback. Jason Wei thanks Eugene Santos for inspiration."
}