{
    "article": "Recent researches on EL(Entity Linking) have attempted to disambiguate entities by using a knowledge base to handle the semantic relatedness and up-to-date information. However, EL for tweets using a knowledge base, leads to poor disambiguation performance, because the data tend to address short and noisy contexts and current issues that are updated in real time. In this paper, we propose an approach to building an EL system that links ambiguous entities to the corresponding entries in a given knowledge base through the news articles and the user history. Using news articles, the system can overcome the problem of Wikipedia coverage, which does not handle issues in real time. In addition, because we assume that users post tweets related to their particular interests, our system can also be effectively applied to short tweet data through the user history. The experimental results show that our system achieves a precision of 67.7% and outperforms the EL methods that only use knowledge base for tweets. Introduction Recent development of the internet and computing technologies makes the amount of information increasing rapidly. Therefore, many long-term studies have been conducted on retrieving the needed information from the huge data. Named entity recognition(NER) and entity linking(EL) to specific entities as a part of information extraction now are actively attempt to extract meaningful knowlege in the huge information. The EL is the task of linking entity mentions int text to entities in a knowlege base. As shown in Figure 1 , the goal of entity linking is to map an ambiguous entity to its corresponding entity in knowledgebase. By leveraging the context information around an entity and knowledge base, '\uc774\uc18c\ub77c (Sora Lee)' in the left box 'Text' in Figure 1 can be identified as the singer '\uc774\uc18c\ub77c (\uac00\uc218) (Sora Lee (singer))'. Context information can be a noun phrase; for example, '\uc774\uc18c\ub77c\uc758 \ud504\ub85c\ud3ec\uc988 (Sora Lee's propose)', which is the name of a music program hosted by '\uc774\uc18c\ub77c (\uac00\uc218) (Sora Lee (singer))', can be known by knowledgebase. Researchers have recently begun studying the problem of addressing named entities in informal and short texts. For example, Twitter, a popular microblogging platform, is updated and posted by users succinctly describing their current status within a limit of 140 characters. Java et al. (2007) showed that tweets address contents ranging from daily life to current events, news stories, and other interests. EL on Twitter has been used to identify entities from a structural knowledge base, e.g., Wikipedia, to enrich the task with additional features. To consider the characteristic of Twitter, state-of-the-art researches collectively link all entities in all tweets posted by a user via modeling the user's interest (Shen et al., 2013; Bansal et al. , Figure 1 : An example of entity linking. The bold type indicates an ambiguous named entity is in the text; the correct mapping entity is linked with the solid arrow 2014). However, such methods cannot cover a EL for tweet task completely, because the posting the latest issues on tweet mentions, the most important characteristic of Twitter, cannot be applied. In this paper, we first propose an EL method that considers Twitter contents addressing current issues and user interests through news articles and user history tweets besides knowledge base. In section 2, we describe recent EL studies. Section 3 provides our improved EL model with user history and news articles. Next, in section 4 we describe an experimental analysis in which we generated a Korean Twitter corpus and compared the contributions of each feature of our proposed method. Finally, we summarize our study with some concluding remarks in section 5. Related Works Traditional approaches have addressed the EL by dividing the task into two steps. The first step is NER, and the second step is entity disambiguation. Knowledge-based NER problem is different from the traditional NER. In the Traditional NER, while defining a class of such \"PER\" or \"ORG\" to entity, knowledge-based NER is to extract candidates of the fully qualified names of entities in knowledge base. For example, when recognizing the entity \"\uc774\uc18c\ub77c (Sora Lee)\", a common NER classifies it into classes such as \"PER\", while knowlege-based NER links it to specific entity such as \"\uc774\uc18c\ub77c (\ubaa8\ub378) (Sora Lee (model))\". Early models of EL had tried a method of extracting only those corresponding to the named entity existing in knowledge base of all possible n-gram terms within document (Mihalcea and Csomai, 2007) . Milne and Witten (2008) tried to utilize machine learning methods to recognize entities. Kim et al. (2014) uses hyperlinks within the Korean Wikipedia and a small amount of text manually annotated with entity information as training data. It employs a SVM model trained with chracterbased features to recognize entity. Liu et al. (2011) proposed an alternating two-step approach that alternates between the KNN classifier and CRF labeler in tweets. The KNN classifier models global features which span over long range of words. The CRF models the localized features among consecutive words. After recognizing entities in document, the next step is entity disambiguation. In section 2.1, we describe previous works about entity disambiguation based on Wikipedia as knowledge base. Next, in section 2.2 describes state-of-the-art researches on EL via user modeling on tweets. Entity Linking based on Wikipedia Approaches leveraging Wikipedia for entity disambiguation started with Bunescu and Pasca (2006) and have been proposed in Cucerzan (2007) , Han and Zhao (2009) , Milne and Witten (2008) , Charton et al. (2014) . Bunescu and Pasca (2006) defined a sematic relatedness by similarity measure using Wikipedia categories. Later studies developed methods using richer structural features from the Wikipedia. The semantic relatedness is measured through the co-occurrence of links in Wikipedia articles. Milne and Witten (2008) have proposed to compute the mention to entity compatibility by leveraging the interdependence between EL. The system proposed that referent entity of a name mention should be coherent with its unambiguous contextual entities. Han and Zhao (2009) demonstrated how to leverage the semantic knowledge in Wikipedia, so the performance of named entity disambiguation can be enhanced by obtaining a more accurate similarity measure between name observations. Charton et al. (2014) built a representation of named entities that do not appear same as the knowledge base named entities. Entity Linking via User Modeling For EL on tweets aimed at short and noisy texts, the system should cover the insufficient context information contained in a tweet. To overcome such a problem, Shen et al. (2013) and Bansal et al. (2014) proposed an EL system via user modeling. Shen et al. ( 2013 ) suggested the KAURI system, a graph-based framework to collectively link all named entity mentions in all tweets posted by a user via modeling the user's topics of interest. They assumed that each user has an underlying topic interest distribution over various named entities. Bansal et al. (2014) attempted to combine contextual and user models by analyzing a user's tweeting behavior from previous tweets. This approach can be used for modeling users and disambiguating entities in other streaming documents. EL applied through user modeling systems outperforms systems using only a knowledge base. In this paper, we adopted the traditional method that extracting only those corresponding to the named entity existing in knowledge base of all possible n-gram terms within document in the NER step. By proposing three models considering characteristics of the tweets, we focus on entity disambiguation step. 3 Entity Linking System with User History and News Articles Notation Framework Our system is applied based on the user's interest and current issues, and by considering which contents their Twitter mentions address. In this section, we introduce our proposed system, which consists of three scoring model systems, a Context modeling system, a User modeling system, and an Issue modeling system, as well the Linking model as shown in Figure 2 . We adopted an existing method into the Context modeling system, by considering the context information around an ambiguous entity. The User modeling system uses the history mentions of user who posted the targeted mention. Targeted mention means the tweet mention with ambiguous entity, which we have to disambiguate. The Issue modeling system enriches the information from news articles, and can extract information that Wikipedia does not handle. The following section focuses on how our User modeling system works well in comparison to a Context modeling system. Furthermore we describe how the Issue modeling system improves the entire system to obtain a high level of performance. \uf0b7 E -Target entity that should be linked \uf0b7 \ud835\udc52 \ud835\udc57j-th non-ambiguous entity, which corresponds to an Wikipedia entity \uf0b7 \ud835\udc50 \ud835\udc57j-th candidate entity for entity mention E that can be linked \uf0b7 \u2329D\u232a -Sets of all entities in a document D \uf0b7 \ud835\udc37 \ud835\udc50 \ud835\udc57 \ud835\udc56i-th news article that includes \ud835\udc50 \ud835\udc57 as a topic, \ud835\udc37 Context Modeling System The Context modeling system uses contextual information, meaning the non-ambiguous entities in a tweet including an ambiguous named entity. Most researches use this feature with Wikipedia category information, but we found that categories are often noisy (Milne and Witten, 2008) and that Korean Wikipedia pages provides insufficient category information compared with English Wikipedia pages. We therefore we adopted Eric Charton's scoring method \"mutual relation score\" (Charton et al., 2014) without category information, as defined by the following formula : \ud835\udc46 \ud835\udc50 (\ud835\udc50 \ud835\udc57 ) = \ud835\udf15 \ud835\udc51\ud835\udc60\ud835\udc5f \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\ud835\udc52 \ud835\udc57 , \ud835\udc50 \ud835\udc57 ) + (1 \u2212 \ud835\udf15)\ud835\udc50\ud835\udc60\ud835\udc5f \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\ud835\udc52 \ud835\udc57 , \ud835\udc50 \ud835\udc57 ) (1) \ud835\udc51\ud835\udc60\ud835\udc5f \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\ud835\udc52 \ud835\udc57 , \ud835\udc50 \ud835\udc57 ) = |\ud835\udc52 \ud835\udc57 \u2229 [\ud835\udc50 \ud835\udc57 ]| (2) \ud835\udc50\ud835\udc60\ud835\udc5f \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\ud835\udc52 \ud835\udc57 , \ud835\udc50 \ud835\udc57 ) = |[\ud835\udc52 \ud835\udc57 ]\u2229[\ud835\udc50 \ud835\udc57 ]| |[\ud835\udc52 \ud835\udc57 ]|+|[\ud835\udc50 \ud835\udc57 ]| (3) Figure 2 : EL system A Context modeling system fundamentally addresses the contextual features and links with a specific entity in Wikipedia. In Wikipedia, there is a \"disambiguation page\" that describes entities with the same name. As shown in Figure 3 , the disambiguation page for \uc774\uc18c\ub77c (Sora Lee) lists three other people with the same name. The first \uc774\uc18c\ub77c (Sora Lee) listed is a famous Korean model. The second \uc774\uc18c\ub77c (Sora Lee) is a famous Korean singer. The last \uc774\uc18c\ub77c (Sora Lee) is a member of the Korean national volleyball team. In this paper, we use the term \ud835\udc46 \ud835\udc50 to indicate the \"calculate score\" in (Charton et al., 2014) , which we use as our baseline system, as compared to systems with other added features. \ud835\udc46 \ud835\udc50 implies simply exploiting a tweet as a context, not considering the properties of the tweet. User Modeling System As attributes of Twitter mentions, tweet contents range from daily life to current issues. Because the User modeling system understands the above property, it handles the user's behaviors and interests. To address this concept, we utilize the past tweets of the user. We assumed that if a particular named entity is mentioned in a tweet, the user tends to have an interest in this named entity. Figure 4 describes the process of the User modeling system. When the User modeling system detects a certain tweet of a user that includes an ambiguous entity, then we extract the user's tweet history. The Entity Integration module then detects whether the feature \ud835\udc52 \ud835\udc57 exists in Wikipedia entity by using the left-longest-match-preference method with eojeol uni-gram and bi-gram, then generate \u2329D\u232a. j ranges from user's past tweets to their last present tweet which to be disambiguated as shown in (5). We exploit the left-longest-matchpreference method with eojeol uni-gram and bigram because tweet mentions tend to be grammatically incorrect and because in Korean, a noun always appears on the left-side in an eojeol, which consists of one or more morphemes comprising a spacing unit (Kang et al., 2014) . Finally, \ud835\udc46 \ud835\udc62 in the User scoring model is evaluated as in (4) and (5). In the example shown in Figure 4 , the system has detected an ambiguous entity '\uc774\uc18c\ub77c (Sora Lee)' in user p2's tweet and extracted p2's tweet history. The Entity Integration model then collect entities such as \"\ub098\uac00\uc218 (Naga-su)\", the TV program, and \"MBC\", which is the broadcast station from p2's tweet history. These features enhance the \ud835\udc46 \ud835\udc62 (\uc774\uc18c\ub77c (\uac00\uc218)), because [\uc774\uc18c\ub77c (\uac00\uc218) (Sora Lee (singer))] includes \"\ub098\uac00\uc218 (Na-ga-su)\" and \"MBC\". Issue Modeling System The model described in section 3.3 has a disadvantage in that it cannot consider current issues, which is one of the characteristics of Twitter. A knowledge base focuses on major issues and cannot provide all current issues in real time, for example, the recent events of a celebrity or trivial real-time events, which tend to be mentioned by Twitter users. Our Issue modeling system solves this problem by leveraging news articles. Because news articles address issues and events in real time, the Issue modeling system can extract current information. As shown in Figure 5 , when the Issue modeling system detects the ambiguous entity, \"\uc774\uc18c\ub77c (Sora Lee)\", the news linking module extracts current news articles issued in within k days from the date of the tweet posted. The news articles have title included the detected entity E, \uc774\uc18c\ub77c (Sora Lee). The module links the news articles to \ud835\udc50 \ud835\udc57 by applying a cosine similarity between the article and Wikipedia page contents. In this example, \ud835\udc50 1 , \ud835\udc50 2 , and \ud835\udc50 3 are \"\uc774\uc18c\ub77c (\uac00\uc218) (Sora Lee (singer))\", \"\uc774\uc18c\ub77c (\ubaa8\ub378) (Sora Lee (model))\" and \"\uc774\uc18c\ub77c (\ubc30\uad6c\uc120\uc218) (Sora Lee (volleyball player))\" respectively. Accordingly, the Issue modeling system exploits the news articles as Wikipedia articles. Each article can be a \ud835\udc37 \ud835\udc50 \ud835\udc57 \ud835\udc56 , which means the i-th news article which addresses \ud835\udc50 \ud835\udc57 as a topic. The Entity Generative model generates the Wikipedia links in each news article. Single quotes in newspaper articles has the ability to display title or name. For example, the title of the book, the title of the movie, the title of the album and the title of the drama can be placed in single quotes [13] . Therefore we assume that news articles include important entities explicitly notated by punctuation. The Entity Generative model recognizes phrases or words in single punctuation marks and nouns as entities, similar to Wikipedia links. Table 1 describes the generation rules of the Entity Generative model and the example news article from Figure 5 . Because the entity \"\uad6c\uc6d0\uc758 \ubc25\uc0c1 (table of salvation)\" is not actually included in the Wikipedia page on \"\uc774\uc18c\ub77c (\ubaa8\ub378) (Sora Lee (model))\", it can be important information to link E , \"\uc774\uc18c\ub77c\" to the correct answer, \ud835\udc50 2 , \"\uc774\uc18c\ub77c (\ubaa8\ub378) (Sora Lee (model))\". The second generation rule, extracting entities that exist in Wikipedia article titles, the Entity generative model exploits a morpheme analyzer to extract nouns from news articles. This model uses a noun uni-gram and bi-gram to match the entity in Wikipedia article titles. Finally Issue scoring module determines the score \ud835\udc46 \ud835\udc56 , as shown in (6). \ud835\udc46 \ud835\udc56 (\ud835\udc50 \ud835\udc57 ) = \u2211 \ud835\udf15 \ud835\udc51\ud835\udc60\ud835\udc5f \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\ud835\udc52 \ud835\udc57 ,\ud835\udc37 \ud835\udc50 \ud835\udc57 \ud835\udc56 )+(1\u2212\ud835\udf15)\ud835\udc50\ud835\udc60\ud835\udc5f \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\ud835\udc52 \ud835\udc57 ,\ud835\udc37 \ud835\udc50 \ud835\udc57 \ud835\udc56 ) |\ud835\udc6b \ud835\udc84 \ud835\udc8b | \ud835\udc56=0 |\ud835\udc6b \ud835\udc84 \ud835\udc8b | (6) Generation Rule Extracted Example phrases or words in single punctuation marks and nouns \"\uad6c\uc6d0\uc758 \ubc25\uc0c1\" (\"table of salvation\") entities that exist in Wikipedia article titles \"\uceec\ud22c\", \"\uc815\ucc2c\uc6b0\", \"\ubaa8\ub378\" (\"Cultwo\", \"Chanwoo Jung\", \"model\") Linking Model The Linking model finally disambiguates the entities based on the three models above, computes the Total Relatedness score, \ud835\udc47\ud835\udc45, and combines the scores \ud835\udc46 \ud835\udc50 (\ud835\udc50 \ud835\udc57 ), \ud835\udc46 \ud835\udc62 (\ud835\udc50 \ud835\udc57 ) and \ud835\udc46 \ud835\udc56 (\ud835\udc50 \ud835\udc57 ) with parameter \u03b1, \u03b2, and \u03b3, which are defined empirically. Equation ( 7 ) shows how this works. \ud835\udc47\ud835\udc45(\ud835\udc38, \ud835\udc50 \ud835\udc57 ) = \ud835\udefc \ud835\udc46 \ud835\udc50 (\ud835\udc50 \ud835\udc57 ) + \ud835\udefd \ud835\udc46 \ud835\udc62 (\ud835\udc50 \ud835\udc57 ) + \ud835\udefe \ud835\udc46 \ud835\udc56 (\ud835\udc50 \ud835\udc57 ) (7) ( \u03b1 + \u03b2 + \u03b3 = 1 ) Dataset and Framework In the experiments, we evaluate our proposed method on the disambiguation of personal names, which is the most common type of named entity disambiguation. We created data set by collecting 50~60 tweets per 300 users who use twitter actively. Finally we collected 16,367 tweets in total. Then we selected tweets that contain the one person entity which exists in the list of entities in the Wikipedia's disambiguation page. Finally we selected 248 tweets annotated same entity by 3 different annotators to verify reliability. The data set consists of 248 tweets including 248 disambiguous entities and they represent 33 ambiguous PERSON named entities. 33 ambiguous entities have 4.75 disambiguation pages on average in Wikipedia, 3.45 in data set. We conducted our experiments using k = 3. We defined \u03b1,\u03b2, and \u03b3 empirically because they depend on the dataset. We used Korean Wikipedia as a knowledge base, the contents of which can be downloaded from http://download.wikipedia.org/ kowiki. In our experiment, we dumped the latest Korean Wikipedia dump file, kowiki-2015-6-2pages-articles. In the Issue modeling system, we adopted a Korean morpheme analyzer, \"Jhannanum\" 1 . In addition, we constructed a Wikipedia PER entity dictionary using the category information. Experimental Result Table 2 shows the performance of our proposed system. In the first row, the system is evaluated using only the Context modeling system. For the second row, we applied the User modeling system along with the Context modeling system. The system with the complete entity linking system obtained the results provided in the third row. We applied the accuracy score(number of true positives + number of true negatives/number of data set) to evaluate the system. We observe that the results of complete entity linking system are shown in the third row. We applied a precision score to evaluate the system. We observed that the complete algorithm provides the best results for 1. Semantic Web Research Center , JHannanum, http://semanticweb.kaist.ac.kr/home/index.php/HanNanum our created test set. Considering that the 33 PER entities in our test set have 4.75 disambiguation pages on average, our proposed system performed well within a 67.7% level of accuracy. We also measured how precisely the Issue modeling system linked between news articles and Wikipedia pages. A 70.2% level of precision was achieved using only the cosine similarity. We showed that the complete system improves the performance of the Context modeling system when using user history and news articles. 4 shows the extracted entities from each systems. First example shows improved performance by extracted entities from User Modeling System. This tweet user likes baseball as usual because he mentioned \"\uc0bc\uc131(Samsung)\", \"\ub86f\ub370(Lotte)\", \"\uc57c\uad6c\uc7a5(ballpark)\", \"\uc870\uc131\ud658 (sunghwan Cho)\" in previous tweets. Among them, \"\uc0bc\uc131(Samsung)\" and \"\ub86f\ub370(Lotte)\" appear in Instead, Issue Modeling system extracted \"\ub4dc\ub77c\ub9c8(drama)\", \"\uc601\ud654(movie)\", \"SBS\", \"\ubc30\uc6b0 (actor)\" that support the system can link to \"\uc870\uc778\uc131(\ubc30\uc6b0) (Insung Cho(Actor))\". Conclusion In this paper, we propose an entity linking system that, consists of three scoring model systems, a Context modeling system, a User modeling system and an Issue modeling system, along with a Linking model to integrate the three systems. We adopted an existing entity linking method as a baseline for Korean tweets, and by applying the User modeling system and Issue modeling system, it outperforms the baseline system, just using knowledge base. Our system handles the characteristics of tweet mentions, such as current issues or trivial events that not described in a knowledge base, by using the User modeling system and Issue modeling system effectively. However, because our work is the first to link entities with three different scoring model systems, the User modeling system does not use the additional features such as Twitter hashtag information. Furthermore, because only the leftlongest-match-preference model and a noun unigram and bigram were used to detect entities in news articles in this experiment, the accuracy was not very high and should be improved later. Further analysis is required for the user modeling and Issue modeling aspects of the system. Future work will also involve applying statistical methods to identify entities in news articles and using additional features appearing in Twitter for the User modeling system. Furthermore, we will adopt an efficient method to link news articles with tweets. We also plan to experiment on larger datasets and adopt our system to English tweets such as TAC_KBP. Tweet Extracted entities @DooBoo_2 - \uae40\ud0dc\uade0\uc774\ub791 \ub3d9\uc120\uc774\ub77c\ub2e0\u314b\u314b\u314b\u314b (\"Same line with Taegyun Kim kkk\") Context modeling system \"\uae40\ud0dc\uade0(Taegyun Kim)\" User modeling system \"\uc0bc\uc131(Samsung)\", \"\ub86f\ub370(Lotte)\", \"\uc57c\uad6c\uc7a5(ballpark)\", \"\uc870\uc131\ud658(sungh wan Cho)\" ... Issue modeling system \"SK wyverns\", \" Samsung Lions\", \"\uc6b0\uc2b9(victory)\", \"\ud55c\uad6d\ud504\ub85c\uc57c\uad6c(K orean Baseball )\"... \"\uc870\uc778\uc131(Insung Cho)\" User modeling system \"\ucd95\uad6c(soccer)\", \"\uc77c\ubcf8(Japan)\", \"\uc911 \uad6d(China)\"... Issue modeling system \"\ub4dc\ub77c\ub9c8(drama)\", \"\uc601\ud654(movie)\",\" SBS\", \"\ubc30\uc6b0(actor)\", \"\ud0dc\uad6d(Thailan d)\"... Acknowledgment ",
    "abstract": "Recent researches on EL(Entity Linking) have attempted to disambiguate entities by using a knowledge base to handle the semantic relatedness and up-to-date information. However, EL for tweets using a knowledge base, leads to poor disambiguation performance, because the data tend to address short and noisy contexts and current issues that are updated in real time. In this paper, we propose an approach to building an EL system that links ambiguous entities to the corresponding entries in a given knowledge base through the news articles and the user history. Using news articles, the system can overcome the problem of Wikipedia coverage, which does not handle issues in real time. In addition, because we assume that users post tweets related to their particular interests, our system can also be effectively applied to short tweet data through the user history. The experimental results show that our system achieves a precision of 67.7% and outperforms the EL methods that only use knowledge base for tweets.",
    "countries": [
        "South Korea"
    ],
    "languages": [
        "Korean",
        "English"
    ],
    "numcitedby": "1",
    "year": "2015",
    "month": "October",
    "title": "Improved Entity Linking with User History and News Articles"
}