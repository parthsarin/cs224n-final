{
    "article": "In American Sign Language (ASL) as well as other signed languages, different classes of signs (e.g., lexical signs, fingerspelled signs, and classifier constructions) have different internal structural properties. Continuous sign recognition accuracy can be improved through use of distinct recognition strategies, as well as different training datasets, for each class of signs. For these strategies to be applied, continuous signing video needs to be segmented into parts corresponding to particular classes of signs. In this paper we present a multiple instance learning-based segmentation system that accurately labels 91.27% of the video frames of 500 continuous utterances (including 7 different subjects) from the publicly accessible NCSLGR corpus <http://secrets.rutgers.edu/dai/queryPages/> (Neidle and Vogler, 2012). The system uses novel feature descriptors derived from both motion and shape statistics of the regions of high local motion. The system does not require a hand tracker. Introduction Computer-based ASL recognition often focuses on signs that have been pre-extracted from video (with known start and end frames) (Elons et al., 2013a; Mohandes et al., 2012, e.g.) . Learning methods are typically employed for sign recognition; these are frequently based on Hidden Markov Models (HMMs) (Vogler and Metaxas, 2004) or Conditional Random Fields (CRFs) (Wang et al., 2006) , trained for each sign in the vocabulary. However, continuous signing presents a significantly greater challenge, not only because of co-articulation effects, but also because of the existence of multiple classes of signs with fundamentally different internal composition. Unlike spoken languages, in which phonemes are concatenated, in signed languages the components of words combine both sequentially and non-sequentially (simultaneously), subject to different linguistic constraints depending on the morphological class to which the signs belong, and these linguistic constraints can be leveraged to improve computer-based recognition (Athitsos et al., 2010; Thangali et al., 2011) . The three most prevalent classes in signed languages are lexical signs, fingerspelled signs, and classifier constructions. Lexical sign production involves combinations of specific hand configurations, orientations, locations in signing space, and movement trajectories. Strict linguistic constraints govern the relationships between start and end handshapes of a given sign, and between the two hands (in 2-handed signs) with respect to hand configuration and movement trajectory (Battison, 1978; Brentari, 1998; Thangali et al., 2011) . Fingerspelled signs consist of sequences of letter handshapes from the manual alphabet, produced with rapid finger movements at a fairly constant global hand location (Athitsos et al., 2010) , potentially with relatively small leftto-right movement. Although some fingerspelled signs are in frequent usage, fingerspelling is often used for proper names and spoken language borrowings. Thus there is no fixed fingerspelled vocabulary: many fingerspelled productions would not be included in any ASL dictionary. Classifier constructions incorporate substantial variability in their realizations (Emmorey, 2013, e.g.) . In current work, we omit from consideration so-called name signs, loan signs (which originated as fingerspelled, but have become more like lexical signs), and index signs (used for pronominal reference). In light of the fundamentally different nature of these sign classes, distinct, class-specific, recognition strategies are needed. Previous work Techniques for recognition from continuous signing based on HMMs (Assaleh et al., 2008; Theodorakis et al., 2012; Vogler and Metaxas, 2001; Kong and Ranganath, 2014) , CRFs (Yang and Lee, 2013) , or intelligent search (Gao et al., 2004; Elons et al., 2013b; Sarkar et al., 2011) can be applied to entire sentences or individual signs (presegmented from sentences in a preprocessing step). Some approaches also exploit movement epenthesis (between signs) for sign recognition (Gao et al., 2004) or segmentation (Kong and Ranganath, 2014; Yang and Sarkar, 2006) . Other researchers have attempted semi-supervised (Theodorakis et al., 2012; Madeo et al., 2012; Bowden et al., 2004) or unsupervised (Han et al., 2013; Cooper et al., 2012; Nayak et al., 2012) sign decomposition into subunits (a.k.a. signemes or atomic shapes or motions). Subunits are then used (instead of per-frame features) for further recognition using learning methods, such as HMMs or CRFs. Another semi-supervised approach involves learning specific signs from sentences containing them (Nayak et al., 2012; Sarkar et al., 2011) , e.g., from subtitled TV programs (Pfister et al., 2013; Pfister et al., 2014) . Recognition rates for these semi-supervised and unsupervised approaches are in the 80-93% range for limited vo- cabularies (20-142 signs). Existing supervised sign recognition methods achieve accuracy of 91-97% for a limited inventory (25-80 signs) (Assaleh et al., 2008; Vogler and Metaxas, 2001; Yang and Lee, 2013; Nayak et al., 2012; Sarkar et al., 2011; Yang and Sarkar, 2006) , and often from single-subject data (Assaleh et al., 2008; Nayak et al., 2012; Sarkar et al., 2011; Yang and Sarkar, 2006; Gao et al., 2004) . Accuracy quickly degrades to 85-90% with somewhat more extensive vocabulary (100-250 signs) (Elons et al., 2013b; Kong and Ranganath, 2014) and subject independence (Kong and Ranganath, 2014) . None of the work mentioned above takes into account the existence of sign types other than lexical and/or fingerspelled. Classifier constructions, which occur with high frequency in signed languages, are largely ignored. Furthermore, prior work on recognition from continuous signing does not apply different strategies based on differentiation of types of signs; and recognition research generally focuses on a single type of sign (e.g., there is some research on recognition of lexical signs, other research on recognition of fingerspelling). Our contribution In order to tailor recognition strategies to the distinct sign classes (Tsechpenakis et al., 2006; Tsechpenakis et al., 2008; Dilsizian et al., 2014) , we must first be able to segment continuous signing video into subsequences corresponding to the distinct types of sign production (i.e., sequences of one or more signs of the same class). We introduce here a multiple instance learning (MIL) system for this task. Our contributions are three-fold: \u2022 Unlike most previous research, we detect classifier signs. \u2022 Our novel feature extraction method does not require pre-segmented hands of specific size and scale, thus eliminating the need for a hand tracker. \u2022 We formulate sign classification as an intra-frame MIL problem, allowing us to capture indirectly important relationships among multiple moving regions in the image. We test our system on 500 utterances containing 3,085 signs captured across 7 subjects from the National Center for Sign Language and Gesture Resources (NCSLGR <http://secrets.rutgers.edu/dai/queryPages/>) corpus of utterances collected and annotated at Boston University (Neidle and Vogler, 2012) . Our system produces sign class labels that match those of human annotators for 91.27% of the video frames; the remaining 8.73% include marginal cases that are too short (e.g., fingerspelled \"on\"), cases with articulatory properties consistent with more than one sign class, as well as some cases where the human annotation turned out to be inaccurate (see section 7). Problem formulation In ASL, hand, arm, upper body, and head movement conveys important linguistic information of various kinds, as do facial expressions. However, since moving body parts are more relevant for recognition than stationary ones, we extract relevant features solely from image regions with significant motion, without restricting attention to specific body parts. We formulate sign class recognition as a multiple instance learning (MIL) problem (Ben-Hur and others, 2012). In the MIL context a video frame is represented by a bag of multiple moving regions (e.g., hands, arms, face). The number of moving regions is not regulated. The task is then to find the frame type (sign class) given the set of moving regions, without attempting to find the sign class for each moving region. Thus, MIL indirectly captures the relationship between different moving body parts. Not all moving regions inside the frame are relevant to the actual signing. For example, background motion or clothing with salient texture can produce significant local motion detector output. Given sufficient quantities of data, the MIL framework will filter out the moving regions (like legs in Figure 1a ) that are inconsistent with the training data. We use the Citation KNN (k-Nearest Neighbors) (Wang and Zucker, 2000) implementation of the MIL paradigm. This implementation builds a distance map between the training examples using ranked Hausdorff distance between two point sets (sets of moving regions in the case of our application). Hausdorff distance is not necessarily symmetric. Therefore, there is a difference between the nearest neighbors of the given frame and the frames that would consider the given frame as their nearest neighbor. For each new frame, R references and C citers are computed using training data. References are the nearest neigh-bors of the new frame. Citers are the training frames that would consider the new frame their nearest neighbor within a certain rank. The rank is passed to the algorithm as parameter c. We used R = 2 and c = 2. The resulting system captures local motion inside the frame, but does not capture global temporal changes. However, local motion within a frame can be consistent with multiple sign classes. Therefore, we used a one state per frame CRF (Lafferty et al., 2001) on top of the MILframework output to model the global interframe dynamics. For the MIL-framework input, we use a set of fixed-size feature descriptors for each of the moving parts. Popular feature descriptors, such as Shape Context (Belongie et al., 2002) , SIFT (Lowe, 1999) , or HOG (Dalal and Triggs, 2005) , either do not describe regions or do not result in constant-size feature vectors. We, therefore, propose a new feature descriptor suitable for our application. Feature Extraction Our approach extracts two types of features: 1. features to detect the moving regions of interest, e.g., the hands; and 2. spatio-temporal features from the regions of interest, suitable for recognizing the type of sign For (1), we use optical flow (Fleet and Weiss, 2006) to find the moving regions with significant speed (sX, sY ) at location X, Y in the image. We compute the PDF (probability density function) of the distribution of the product |sX| \u2022 |sY | (Figure 1a-1c ) and threshold the data on probability density <0.1 (Figure 1d ) to include only highly noticeable motion. We then collect regions with an area of >100 pixels (10\u00d710 pixels, which is of the order of magnitude of a fingernail for the 640\u00d7480 frames in the NCSLGR dataset); see Figure 1e . In step 2, we extract features related to the shape and velocity of the moving regions (e.g., hands). For implementation, we choose the set of image filters shown in Table 1 , which allows us to capture predominant orientations and ridge strength signatures of both shape (intensity) and local motion (optical flow speeds). Each filter response is computed over the Gaussian pyramid of the input image to account for scale issues. We have observed that each filter response consistently follows a specific parametric distribution for all regions of interest (Figure 2 ). The parameters of that distribution become a part of the feature vector. The total feature vector length is 14; and each frame would have approximately from 1 to 6 feature vectors corresponding to the high local motion regions. Experimental evaluation We used the Weka (Hall et al., 2009) implementation of the Citation KNN MIL (Wang and Zucker, 2000) classifier, trained on the pre-segmented NCSLGR data. For training we used 570 randomly chosen instances of each sign class (lexical, fingerspelled, classifiers) for a total of 1,710 sign instances across 7 subjects. Using the trained MIL classifier, we obtained frame labels for 1,110 utterances. We Table 1 : Image filter responses and their parametric distributions over the Gaussian pyramid. Total feature descriptor length is 14. removed movement epenthesis frames and frames belonging to sign classes not included in the current research, and used the rest to train a CRF. We tested on 500 complete utterances across the same 7 subjects. These utterances consisted of 3,085 relevant signs (42,947 relevant frames plus 21,535 frames of movement epenthesis or sign classes not considered here). Therefore, our test and training datasets were significantly different in temporal structure. Results and Discussion The system labeling matched that of the human annotators for 91.27% of the 42,947 relevant frames. Sample utterances are presented in Figures 3 and 4 (gloss labels include the prefix \"fs-\" for fingerspelled signs and \"BCL-\", \"DCL-\" or \"SCL-\" for (different types of) classifier constructions).   The remaining 8.73% include cases involving human inaccuracy in labeling sign boundaries (Figure 3 ), signs with properties consistent with more than one sign class (Figure 4 ) or short fingerspelled signs. In Figure 3 , orange and magenta insets display frames in which the hand is getting into position for fingerspelling. That transition is included in the region that had been annotated as fingerspelling; i.e., a small human labeling error as to the precise start point of the fingerspelling is corrected by our system. In Figure 4 , \"(2h)alt.GAMBLE++\", which had been annotated as lexical, was identified by our system as a classifier. However, this sign is profoundly classifier-like and was only considered lexical by the annotators because it has come into frequent usage. (It involves a kind of acting out of rolling dice; the linguistic properties are not typical of lexical signs.) Thus, the result from the system is reasonable in light of the nature of this sign. Table 2 shows the per-frame confusion matrix. Fingerspelled signs and classifiers have a bigger overlap with lexical signs than with each other. In the presentation, we discuss some of the sources of confusion seen in Table 2 (such as the confusion between wrist rotation and finger movement). A more comprehensive analysis of the cases where the system and the human disagreed on classification is reported in the conference presentation. It should be noted that this performance is based on a clean dataset with no background clutter or movement. Therefore, the regions of interest can be trivially extracted based on local motion. In future work, we will use background subtraction methods (Cui et al., 2012 ) to test our system in cases with cluttered backgrounds. Conclusion and Future Work In order to tailor sign recognition strategies to structurally different types of signs, we have developed a system for segmentation of continuous signing based on the linguistic type of sign production (lexical signs, fingerspelling, classifier constructions). The system labeling matched that of human annotators for 91.27% of the frames from 500 utterances consisting of 3,085 signs. The remaining 8.73% contain cases that clearly reflect either human inaccuracy in the \"ground truth\" labeling of boundaries, or signs with properties consistent with more than one class of signs. The segmentation results could be improved by running different sign type HMMs simultaneously in cases where the segmentation system produces a low-confidence result. This is a first step towards creating a complete system for real-time sign recognition that leverages the linguistic properties of the distinct sign classes. Incorporation of such linguistic information would 1) lead to improvements in the segmentation itself, and 2) be used for sign identification within the segmented regions. There is considerable research on recognition strategies for fingerspelled signs (Rioux-Maldague and Giguere, 2014; Kim et al., 2013; Pugeault and Bowden, 2011; Ricco and Tomasi, 2009, e.g.) . For lexical signs, linguistic constraints on the relationships between the start and end handshapes of a given sign, and between the handshapes used on the left and right hands, have already been demonstrated to improve accuracy of handshape recognition (a crucial linguistic component) (Dilsizian et al., 2014) . Work is now in progress on incorporating 3D tracking of hands, arms, and upper body to exploit the motion properties of lexical signs for sign identification (Dilsizian et al., 2016) . This approach, sensitive to the fundamentally different internal structure of distinct sign types, holds great promise for recognition of large-scale vocabulary from continuous signing. Acknowledgments We thank all of the many Deaf consultants and students at Boston University who have participated in and contributed to the data collection, annotation, and analysis. We thank especially Rachel Benedict, Ben Bahan, Tory Sampson, Corbin Kuntze, Blaze Travis, Chelsea Hammond, Indya Oliver, Emma Preston, and Rebecca Lopez. We are also grateful for support in software development from Augustine Opoku and Gregory Dimitriadis (Rutgers University, Laboratory for Computer Science Research). This research was partially funded by grants from the National Science Foundation (#1065013, 0964385, 1059218,  1064965, 1355304, 1447037).",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgments section of the article explicitly mentions that the research was partially funded by grants from the National Science Foundation, which is a government-funded organization that provides grants for research. There is no mention of funding from defense, corporate entities, foundations, or an indication that there was no funding.",
    "abstract": "In American Sign Language (ASL) as well as other signed languages, different classes of signs (e.g., lexical signs, fingerspelled signs, and classifier constructions) have different internal structural properties. Continuous sign recognition accuracy can be improved through use of distinct recognition strategies, as well as different training datasets, for each class of signs. For these strategies to be applied, continuous signing video needs to be segmented into parts corresponding to particular classes of signs. In this paper we present a multiple instance learning-based segmentation system that accurately labels 91.27% of the video frames of 500 continuous utterances (including 7 different subjects) from the publicly accessible NCSLGR corpus <http://secrets.rutgers.edu/dai/queryPages/> (Neidle and Vogler, 2012). The system uses novel feature descriptors derived from both motion and shape statistics of the regions of high local motion. The system does not require a hand tracker.",
    "countries": [
        "United States"
    ],
    "languages": [
        ""
    ],
    "numcitedby": 13,
    "year": 2016,
    "month": "May",
    "title": "Detection of Major {ASL} Sign Types in Continuous Signing For {ASL} Recognition"
}