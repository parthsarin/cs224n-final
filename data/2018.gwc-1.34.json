{
    "article": "Given a word, what is the most frequent sense in which it occurs in a given corpus? Most Frequent Sense (MFS) is a strong baseline for unsupervised word sense disambiguation. If we have large amounts of sense-annotated corpora, MFS can be trivially created. However, senseannotated corpora are a rarity. In this paper, we propose a method which can compute MFS from raw corpora. Our approach iteratively exploits the semantic congruity among related words in corpus. Our method performs better compared to another similar work. Introduction Word Sense Disambiguation (WSD) remains to be one of the relatively hard problems in the field of Natural Language Processing. Machine Learning approaches to WSD can be broadly classified into two categories: supervised and unsupervised. Supervised techniques rely on learning patterns from sense-annotated training data. However, such data are hard to come by. SemCor, one of the most common sense-annotated corpus in English language, contains around 700k tokens, 200k of which have been sense-annotated. It is really small as compared to raw corpora such as ukWAC, where the number of tokens is close to 2 billion. On the other hand, unsupervised techniques do not require sense-annotated corpora. A strong baseline for unsupervised WSD is the Most Frequent Sense (MFS) baseline. While performing sense disambiguation, the baseline completely ignores the context, and simply assigns the most frequent sense to the target word. Inspite of ignoring context, which is one of the main source of information for performing sense disambiguation, the MFS baseline gives re-ally strong results. This is because of the inherent skew in the sense distribution of the data. Computing MFS baseline is trivial, if one has access to large amounts of sense-annotated corpora. However, that is not the case as explained earlier. Thus there is a need for uncovering MFS from raw data itself. Word embeddings collectively refers to the set of language modelling and feature learning techniques, which maps words to real valued vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al., 2010; Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) . Do note that most word embedding models only output one embedding per word, instead of the ideal case of outputting one embedding per sense of a word. Though, some models do exist, which provide one embedding per sense of a word by inferring number of senses either through context clustering approaches (Neelakantan et al., 2015) , or by using sense inventory (Chen et al., 2014) . For the rest of this paper, we mean one embedding per word models, when we use the phrase word embeddings. The field of Natural Language Processing is increasingly seeing the use of word embeddings for various problems, and MFS is no exception. Bhingardive et al. (2015) showed that pretrained word embeddings can be used to compute most frequent sense. In this paper, we propose an iterative approach for extracting most frequent sense of words in a raw corpus. The approach uses word embeddings as an input. Thereby, in order to obtain MFS from some raw corpus, one need to apply the following two steps: 1. Train word embeddings on the raw corpus. 2. Apply our approach on the trained word embeddings. The key points of this paper are: \u2022 Our work further strengthens the claim by (Bhingardive et al., 2015) that word embeddings indeed capture most frequent sense. \u2022 Our approach outperforms others at the task of MFS extraction. The rest of the paper is organized as follows: Section 2 describes the related work. Section 3 explains our approach. Section 4.1 details our experimental setup and results. Section 5 provides some error analysis, followed by conclusion and future work. 2 Related Work Buitelaar and Sacaleanu (2001) present an approach for domain specific sense assignment. They rank GermaNet synsets based on the cooccurrence in domain corpora. Lapata and Brew (2004) acquire predominant sense of verbs. They use Levin's classes as their sense inventory. Mc-Carthy et al. (2007) use a thesaurus automatically constructed from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. Bhingardive et al. (2015) exploit word embeddings trained on untagged corpora to compute the most frequent sense. Our work is most similar to Bhingardive et al. (2015) owing to our reliance on word embeddings. We therefore evaluate our approach against theirs. Approach Our approach relies on the semantic congruity of raw text. Consider the following example: Consider the word cricket having two senses sport and insect, and the word bat having two senses sport instrument and reptile. Then, if in our corpus, we already know that bat is in sport instrument sense for most cases, then in order for the corpus to be semantically congruent, the most frequent sense of cricket has to be sport. So, in order to find most frequent sense of all words in the vocabulary of the corpus, we start with the word whose sense is already known. So, the approach begins with monosemous words, for which MFS is trivial. Next, it moves on to bisemous words, and uses the monosemous words sense information to detect most frequent sense. Then it moves on to trisemous words, and use the hitherto resolved words for detecting most frequent sense, and so on. Thus the approach iterates over the degree of polysemy, and uses the computed MFS of words with degree of polysemy 1 to n \u2212 1 to compute the MFS of words with degree of polysemy n. At any point of time, we call the words whose MFS is already established as tagged words. For a given word whose MFS is to be computed, we enumerate all senses, and then compute the vote for each senses by the top k nearest neighbors who are already tagged. The vote is a product of two measures: the cosine similarity (w i ) between the embedding of the current tagged word and the target word, and the wordnet similarity (s i ) between the MFS of current tagged word (which would have been established in the previous iteration), and the sense for which the vote is being computed. The votes are summed over, and the sense with the highest sum is considered to be the Most Frequent Sense of the target word. The basic flow is illustrated in figure 1 . The major parameters in our approach are: 1. K: The number of nearest neighbors who will vote. Through experimentation, we found K=50 to be a reasonable value. 2. WordNet Similarity measure (s i ): We tried all combinations of the six available similarity measures in Princeton WordNet, namely Path similarity, Leacock Chodorow Similarity, Wu Palmer Similarity, Resnik Similarity, Jiang Conrath Similarity, and Lin Similarity. Our experiments found the average of normalized Wu Palmer and Lin similarity performs slightly better than other combinations. 3. Vector space similarity measure (w i ): We tried both dot and cosine similarity. Dot performed slightly better. In future, we would try other similarity measures such as Tanimoto coefficient. 4 Experiments and Results Datasets We have used the following datasets for our evaluation: 1. SemCor: Sense-annotated corpus, annotated with Princeton WordNet 3.0 senses using WordNet 1.7 to WordNet3.0 mapping by Rada Mihalcea 2. Senseval 2: Sense-annotated corpus, annotated with Princeton WordNet 3.0 senses as above 3. Senseval 3: Sense-annotated corpus, annotated with Princeton WordNet 3.0 senses as above Evaluating MFS as solution for WSD Given that MFS is a strong baseline for unsupervised WSD, a good MFS will give good performance on unsupervised WSD. This is what this experiment evaluates. While in theory, our approach can also use embeddings trained on test corpora directly, we use pretrained word2vec embeddings, as they are crucial to Bhingardive et al. (2015) with whom we are comparing. Table 1 shows the results of using MFS for WSD on Senseval 2 and Senseval 3 only for nouns. We report this noun specific result for comparison with (Bhingardive et al., 2015) , who have reported results only for nouns. Here, Bhingardive(reported) and SemCor(reported) are the results as reported in the paper. However, their exact experiment settings are not clear from their paper. Thus we used also computed their results in our setting, which are reported as Bhingardive and SemCor respectively. In addition to this, we also report the performance on all parts of speech, in table 2. Here, Bhingardive(reported) is the result with the parameter configuration for their approach as reported Method Senseval2 Senseval3 in their paper. We also tried out different parameter settings for their algorithm, and Bhingardive(optimal) is the best result obtained with optimal parameter setting. It is clear that our approach outperforms both their reported approach and the one with empirically obtained optimal parameters. Method Evaluating MFS as classification task Another way to evaluate our approach was to learn MFS from pretrained word embeddings which were trained on large corpora, and compare it with WordNet First Sense (WFS). Table 3 shows how our approach fares against Bhingardive et al. (2015)'s when both the approaches are applied on pretrained word2vec embeddings (trained on Google News Dataset with billions of tokens and released by them). A similar evaluation can also be done by using true MFS obtained from frequencies in senseannotated corpora. Tables 4 show the results for the same. Discussion Even though our approach performs better than Bhingardive et al. (2015) , we are not able to cross SemCor and WFS results. The following are the reasons for the same: \u2022 In some cases, the tagged words actually rank very low in order for them to make a significant impact. For instance, while detecting MFS for a bisemous word, it may happen that the first monosemous neighbour actually ranks 1101, i.e. a 1000 polysemous words are closer than this monosemous word. Thus in such cases, the monosemous word may not be the one who can influence the MFS. Conclusion In this paper, we proposed an iterative approach for unsupervised most frequent sense detection in raw corpus. The approach uses word embeddings. Our results bears similar trends to those of Bhingardive et al. (2015) , thereby strengthening the claim that word embeddings do indeed capture most frequent sense. Through 2 different categories of experiments, we established that our method is better than theirs. Since there are no language specific restrictions, we believe that our approach should be easily applicable to other languages. In the future, we would like to experimentally validate this claim.",
    "abstract": "Given a word, what is the most frequent sense in which it occurs in a given corpus? Most Frequent Sense (MFS) is a strong baseline for unsupervised word sense disambiguation. If we have large amounts of sense-annotated corpora, MFS can be trivially created. However, senseannotated corpora are a rarity. In this paper, we propose a method which can compute MFS from raw corpora. Our approach iteratively exploits the semantic congruity among related words in corpus. Our method performs better compared to another similar work.",
    "countries": [
        "India"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2018",
    "month": "January",
    "title": "An Iterative Approach for Unsupervised Most Frequent Sense Detection using {W}ord{N}et and Word Embeddings"
}