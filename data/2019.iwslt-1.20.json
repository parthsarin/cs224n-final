{
    "article": "In this work we analyze and compare the behavior of the Transformer architecture when using different positional encoding methods. While absolute and relative positional encoding perform equally strong overall, we show that relative positional encoding is vastly superior (4.4% to 11.9% BLEU) when translating a sentence that is longer than any observed training sentence. We further propose and analyze variations of relative positional encoding and observe that the number of trainable parameters can be reduced without a performance loss, by using fixed encoding vectors or by removing some of the positional encoding vectors. Introduction In this work we analyze the performances of different methods to encode positional information in the Transformer architecture [1] . The Transformer architecture relies on selfattention layers to handle sequences of varying length. However, self-attention layers themselves provide little positional information. Keys and values in self-attention are treated as sets without an ordering and reordering the queries simply results in a reordered output. While the decoder may get some positional information from the left-to-right masking scheme, the encoder does not have access to any positional information. To solve this problem, Transformer models rely on explicit positional information, typically in form of a (absolute) positional encoding vector that is added to each embedded word. Recent works suggest to use relative positional encodings instead. This can be done by modifying the self-attention layer, which considers each pair of tokens from the input sentence, to include a new vector that encodes the distances of the tokens [2, 3] . In this work we empirically analyze different positional encoding schemes for the use of machine translation. We compare absolute and relative positional encodings with a strong focus on their behaviour for different sentence lengths. Furthermore we propose and analyze different variations and combinations of absolute and relative positional encoding. Related Work The Transformer [1] model consists of stacked encoder and decoder layers. Since the model uses self-attention layers to handle sequences, information about the sequence order has to be included explicitly into the model. Vaswani et al. [1] introduced the Transformer using absolute positional encodings based on sinusoid functions. Absolute positional encodings are also used in convolutional systems [4] . The positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. Shaw et al. [2] propose an alternative approach, in which the self-attention mechanism is extended to efficiently account for representations of relative positions or distances between tokens. The relative positional information is included by adding vectors, which represent the pairwise relationship between the current position and other positions, to the projected keys and values. Our experiments are mainly based on this technique. Chen et al. [5] use bidirectional long short-term memory (LSTM) [6] cells in a recurrent neural networks (RNN) and combine it with a self-attentive architecture, showing that recurrence can contribute to their strongest model. The proposed cascaded model involves the fine-tuning of selfattention layers stacked on pre-trained frozen LSTM layers. With this structure, the positional encoding technique in the standard Transformer model is no longer required, as the LSTM-RNN layers can embed contextual information of unlimited length. Inspired by this idea, we also try to apply LSTM layers for the purpose of positional encoding (for both encoders and decoders) and compare them with other positional encoding techniques. Dai et al. [3] also claim that absolute positional encoding does not contain information to distinguish positional differences, which can lead to a performance loss. To avoid this the authors offer a different derivation than [2] and test their method on the task of language modeling. Positional Encoding The Transformer architecture is a sequence-to-sequence architecture, consisting of stacked encoder and decoder layers. To generate sequence representations the Transformer em-ploys self-attention. An encoder layer consists of a self-attention sub-layer followed by a feedforward sub-layer. Similarly, a decoder layer consists of a self-attention sub-layer, followed by an encoder-decoder attention sub-layer, which is followed by a feedforward sub-layer. The output of each sub-layer for a given time step is of dimension d model . However, the selfattention mechanism ignores the sequence order, which is why positional encodings are used in the Transformer architecture. Absolute Positional Encoding In the original Transformer architecture, an absolute positional encoding vector PE j is added to each embedded source and target word to denote its position j. The idea is to use sinusoids of different wavelengths to encode different positions. For the position j \u2208 {1, . . . , J} in a sequence of length J, the absolute positional encoding vector PE j \u2208 R dmodel is defined by PE j,2\u03b2 = sin(j/10000 2\u03b2/d model ), (1) PE j,2\u03b2+1 = cos(j/10000 2\u03b2/d model ) (2) where \u03b2 = 1, . . . , d model 2 . Vaswani et al. [1] propose that sinusoidal positional embeddings perform equally well as learned positional encodings and hypothesize that the former are capable of generalizing to longer sequences. Relative Positional Encoding Absolute positional encoding does not explicitly consider distance relationships. In the framework of relative positional encoding [2] a trainable distance encoding is added to every self-attention layer. Consider the n-th head of a self-attention layer. First, a vector \u03b3 K j \u2212j \u2208 R d k is added to the projected keys when computing the energy: e j,j = (h j W Q n )(h j W K n + \u03b3 K j \u2212j ) T \u221a d k , (3) where W K n , W Q n \u2208 R dmodel\u00d7d k are the projection matrices of the n-th attention head, {h j | j = 1, . . . , J} is the set of keys and h j is the j -th query of the self-attention mechanism. Second, the projected values are shifted by \u03b3 V j \u2212j \u2208 R dv when computing the weighted sum: c j = J j =1 \u03b1 j,j (h j W V n + \u03b3 V j \u2212j ), (4) where \u03b1 j,j = exp e j,j J j=1 exp e j, j and W V n \u2208 R dmodel\u00d7dv is a projection matrix. Using the two different distance terms \u03b3 K j \u2212j and \u03b3 V j \u2212j for the projected keys and the projected values allows to project these keys and queries onto different dimensions, i.e. d k = d v . The distance terms are based on trainable parameters r K \u2212\u03c4 , . . . r K \u03c4 \u2208 R d k and r V \u2212\u03c4 , . . . r V \u03c4 \u2208 R dv respectively, where the hyperparameter \u03c4 describes the clipping distance. A distance term \u03b3 j \u2212j , which encodes a distance that exceeds \u03c4 , will fall back to the value of \u03b3 \u03c4 (or \u03b3 \u2212\u03c4 , if j < j): \u03b3 K j \u2212j = r K clip \u03c4 (j \u2212j) (5) \u03b3 V j \u2212j = r V clip \u03c4 (j \u2212j) (6) clip \u03c4 (x) = max(\u2212\u03c4, min(\u03c4, x)). (7) In order to facilitate computation, the distance terms are shared across attention heads of a multi-head attention layer, but not across self-attention sub-layers or Transformer layers. A variation of relative positional encoding is to use the sinusoids from the absolute positional encoding approach instead of trained parameters (relative sinusoidal positional encoding): \u03b3 K j \u2212j = [PE clip \u03c4 (j \u2212j),1 , . . . , PE clip \u03c4 (j \u2212j),d k ] (8) \u03b3 V j \u2212j = [PE clip \u03c4 (j \u2212j),1 , . . . , PE clip \u03c4 (j \u2212j),dv ]. (9) We also investigate a variation of relative positional encoding where the distance terms \u03b3 V are omitted. LSTM-based Positional Encoding Since the Transformer employs self-attention layers for sequence handling it needs explicit positional information to avoid bag-of-word modeling. Bahdanau et al. [7] handle this problem by using LSTM layers that incorporate the positional information implicitly. This LSTM-based positional encoding can be used for the Transformer architecture. In the encoder, the embeddings are passed into a BiLSTM layer before being passed into the first encoder layer. Similarly, we introduce a single LSTM layer between the embedding layer and the first decoder layer. Note that this approach significantly increases the number of parameters (by 20% in our case), increases the model depth and removes parts of the parallelism for the Transformer architecture. This addition of two LSTM layers is similar to the idea of a LSTM-Transformer Cascaded Encoder [5] , i.e. to pass the source sequence first through an LSTM-based encoder before feeding the output representations to a regular Transformer encoder. Differently than Chen et al. [5] we only intend to retain the positional information, hence we use a single BiLSTM layer in the encoder. Furthermore Chen et al. do not consider the LSTM layers in the decoder. Experimental Results Experimental Setup We use a 6 layer Transformer model that closely matches the 'base' configuration that was introduced together with the Transformer architecture [1] . In particular we use 8 We train our models on the data from the De\u2192En and the Zh\u2192En news translation task of WMT 2019. For the De\u2192En task we train on CommonCrawl, Europarl, News-Commentary and Rapid summing up to a total of 6M lines with newstest2015 as development set. During preprocessing we apply byte pair encoding [9] with 50k joint merge operations. Our Zh\u2192En systems are trained on all WMT2019 parallel data, namely NewsCommentary, the UN and CWMT corpus totaling 13.7M lines. As development set the concatenation of newsdev2017 and newstest2017 is used. All Zh\u2192En data is preprocessed using the unigram language model segmentation algorithm [10] with separate vocabularies of size 32k. If not stated otherwise, all sequences longer than 100 tokens are ignored during training which make about 0.4% (De\u2192En) and 0.2% (Zh\u2192En) of the training data. Note that throughout the paper we determine the length of a sentence by counting the tokens on tokenized, subworded text. Our results on Zh\u2192En and De\u2192En are given in Table 1 . While both positional encoding schemes perform equally on the De\u2192En task , we see a clear improvement of 1.0% BLEU for the case of Zh\u2192En. Note that during training we discard sentence pairs if source or target exceeds 100 tokens. This is not a problem for newstest2018 of the De\u2192En task since no such sentence pair occurs, however for Zh\u2192En out of the 2000 sentence pairs from newstest2019 about 1.4% of the source sentences and 4.2% of the target sentences are longer than 100 tokens. In the following we take a closer look at the performance of different positional encoding schemes for sequences of various length. Sequence Length Analysis In order to get a bigger test set that provides a higher variety of sequence lengths we consider the long-standing De\u2192En task and concatenate all old test sets newtest2008-2018 with the exception of newstest2015 which is used as the development set. This results in the test set newstest all consisting of 28k sentence pairs. We partition this test set in 5 groups based on their source sequence length, such that each group gathers sentence pairs of similar length. Formally we define: G \u03b1:\u03b2 := (f J 1 , e I 1 ) \u2208 newstest all | \u03b1 \u2264 J \u2264 \u03b2 where f J 1 is the source sentence of length J and e I 1 the corresponding target sentence of length I. In the rest of this work we group all sentence pairs with a source length difference of 25, e.g. all source sentences between 0 and 25 in G 0:25 . In Table 2 Lines 1-3 we report the performance of the three positional encoding schemes presented in Section 3 on G 0:25 , . . . , G 101:\u221e as well as the size of the resulting groups. Note that in this section the only relative positional encoding considered is the one proposed by [2] with trainable \u03b3 K and \u03b3 V . See Line 1 for a baseline using absolute positional encoding which is the default setting of many Transformer implementations. Note that the performance is very similar for different length source sentences up to 75 tokens (G 0:25 to G 51:75 ) and drops about 2% BLEU when source sentences of length 76-100 are considered. The translation of source sentences of length higher than 100 tokens however results in a BLEU score of 21.5% i.e. a drop of 7.8% BLEU and an increase of 7.0% TER. Compared to this the model with relative positional encoding (Line 2) performs equally in the cases G 0:25 , . . . , G 76:100 but does not drop in performance when the source sequences extend beyond length 100. If we use an LSTM-based positional encoding the performance for sequences up to length 100 is equal to both absolute and relative positional encoding but in-between the two approaches for sequence longer than that. Since LSTMbased positional encodings are weaker than pure relative positional encoding we ignore them for our further investigation. We observe that absolute and relative positional encoding perform extremely similar in most cases, however for sequences longer than 100 tokens relative positional encoding establishes a very strong lead of around 8% BLEU. We would like to point out that in our setup the maximum source length observed in training is also 100 tokens (see Section 4.1). This raises the question whether absolute positional suffers a) when generalizing to unseen sequence lengths or b) on long sequences in general. To answer the above question we train two models using absolute respectively relative positional encoding with the same setups as before but restricting the sequence length to 75 tokens in training. This removes 71k sentence pairs, i.e. 1.2% of the training data. Comparing the two versions of the absolute positional encoding model ( vs 4) we notice that they have equal performance for for all sequence lengths up to 75. For sequences with length between 76 and 100 the absolute positional encoding model loses 3.4% BLEU if it does not see these lengths in training. For sequences longer than 100 tokens this difference grows to 6.2% BLEU. Note that this drop in BLEU can not be explained by a loss of training data since relative positional encoding systems on the same training data do not suffer from the same performance loss (Table 2 Line 5 ). These systems perform equally good as the absolute positional encoding baseline (Line 1) up to source lengths of 100 even though it is not trained to deal with them. For sequences longer than 100 tokens the relative positional encoding system with reduced training data outperforms the baseline by 5.8% BLEU but stays 2.1% BLEU behind its counterpart on the full data (Line 2). In total we conclude that absolute and relative positional encoding are equally strong for sequence lengths observed in training. Since these make up the majority of the test data that all five systems in Table 2 have a very similar absolute performance with BLEU scores ranging from 39.9% to 40.4% on the newstest2018. However relative positional encoding is vastly superior if applied to unseen sequence lengths. A problem of this kind of analysis is the lack of long sentences available. While G 0:25 , G 26:50 and G 51:75 all contain more than 2k sentence pairs note that we just end up with 40 sequences longer than 100 tokens even when combining 9 test sets. This is obviously a very thin basis to draw conclusions from. While we could extract long sentence pairs from the training data it is quite possible that this results in a very biased selection, e.g. by selecting sentences with very uncommon words which are split heavily by a subword segmentation. In order to increase the amount of good-quality long sentences we take consecutive sentence pairs from newstest all and concatenate their source respectively target sides. This yields a new test set concat seq with 14k sentence pairs which we group again into G0:25 , . . . , G101:\u221e which are translated with the above mod-els. The smallest group contains | G101:\u221e | = 731 sentence pairs which allows for more reliable observations. This creates a small asymmetry between training (where the models learns to translate one source to one target sentence) and translation (where the models now translate two source to two target sentences). Since all models are equally exposed to this we do not believe that this changes the relative performance difference between the systems. Table 3 shows that all four models drop quite a bit in performance, however their relative behaviour remains the same: Up to sequences of length 75, all 4 models are equally strong, but for sequence lengths between 75 and 100 only the model using absolute positional encoding with maximum sequence length 75 in training drops in performance. Similar to Table 2 this is a very significant drop by about 4% BLEU. For sequences longer than 100 tokens both relative positional encoding models perform equally well beating the absolute positional encoding baseline by 4% BLEU. This confirms our observation that relative positional encoding is superior to absolute positional encoding in terms of generalization to unseen sequence lengths. Further analysis of relative positional encoding Since relative positional encoding has proven to be beneficial for sequences of unseen lengths, we further analyze three variations of this method: \u2022 Relative sinusoidal positional encoding does not use trainable parameters for \u03b3 vectors but instead uses the sinusoids from absolute positional encoding as described in Section 3.2. \u2022 Relative \u03b3 K -only positional encoding by using the relative positional information \u03b3 K j \u2212j for the selfattention key but omitting \u03b3 V j \u2212j for the value. \u2022 Relative and absolute positional encoding by applying both encoding approaches in the same model. We report the results for all 3 systems trained with maximum sequence length of 100 tokens on G0:25 , . of concat seq in Table 4 . Surprisingly all variations show a performance very similar to the pure relative positional baseline. Relative sinusoidal positional encodings and the combination of relative and absolute positional encoding are slightly weaker for short sentences. However in further experiments on newstest all we do not observe the same pattern and conclude that it is most likely noise. More meaningful is that the combination of relative and absolute positional encoding lacks behind 2.0% BLEU on G101:\u221e . We observe similar behaviour on G 101:\u221e of newstest all where the combination is 4.5% BLEU weaker than pure relative positional encoding. This indicates that half the parameter added by relative positional encoding can be omitted since the addition of \u03b3 K j \u2212j alone yields equal performance and length generalization. Conclusion We analyze the behaviour of the Transformer architecture with absolute and relative positional encoding and show that relative positional encoding is strongly superior when translating a source sequence that is longer than any observed training sequence. By excluding long sequences from the training we verify that this gap of 4.4% to 11.9% BLEU is an effect of generalization not of absolute sentence length. We further analyze variations of relative positional encoding and observe that the number of trainable parameters can be reduced by using fixed positional encodings or by removing the weight vectors \u03b3 K without a performance loss. Because long sequences are rare in test sets, the described effects are often not visible when total BLEU or TER performance is considered. For a strong and stable general purpose system however, these differences are crucial. We restrict our analysis to the relative positional encoding presented by Shaw et al. [2] and further research should include alternatives such as the one presented by Dai et al. [3] . It remains open for investigation whether the results carry over to other task such as language modeling or automatic speech recognition. Acknowledgements This work has received funding from the European Research Council (ERC) (under the European Union's Horizon 2020 research and innovation programme, grant agreement No 694537, project \"SEQCLAS\") and the Deutsche Forschungsgemeinschaft (DFG; grant agreement NE 572/8-1, project \"CoreTec\"). The work reflects only the authors' views and none of the funding agencies is responsible for any use that may be made of the information it contains.",
    "funding": {
        "defense": 0.0,
        "corporate": 0.0,
        "research agency": 1.0,
        "foundation": 0.0,
        "none": 3.128162811005808e-07
    },
    "reasoning": "Reasoning: The article explicitly mentions funding from the European Research Council (ERC), under the European Union's Horizon 2020 research and innovation programme, and the Deutsche Forschungsgemeinschaft (DFG), which are both research agencies. There is no mention of funding from defense, corporate entities, foundations, or an indication that there were no funding sources.",
    "abstract": "In this work we analyze and compare the behavior of the Transformer architecture when using different positional encoding methods. While absolute and relative positional encoding perform equally strong overall, we show that relative positional encoding is vastly superior (4.4% to 11.9% BLEU) when translating a sentence that is longer than any observed training sentence. We further propose and analyze variations of relative positional encoding and observe that the number of trainable parameters can be reduced without a performance loss, by using fixed encoding vectors or by removing some of the positional encoding vectors.",
    "countries": [
        "Germany"
    ],
    "languages": [
        ""
    ],
    "numcitedby": 10,
    "year": 2019,
    "month": "November 2-3",
    "title": "Analysis of Positional Encodings for Neural Machine Translation",
    "values": {
        "building on past work": " Our experiments are mainly based on this technique.  Inspired by this idea, we also try to apply LSTM layers for the purpose of positional encoding (for both encoders and decoders) and compare them with other positional encoding techniques.  This addition of two LSTM layers is similar to the idea of a LSTM-Transformer Cascaded Encoder [5] , i.e. to pass the source sequence first through an LSTM-based encoder before feeding the output representations to a regular Transformer encoder. Differently than Chen et al. [5] we only intend to retain the positional information, hence we use a single BiLSTM layer in the encoder. Furthermore Chen et al. do not consider the LSTM layers in the decoder.",
        "novelty": " In this work we empirically analyze different positional encoding schemes for the use of machine translation.  We compare absolute and relative positional encodings with a strong focus on their behaviour for different sentence lengths.  Furthermore we propose and analyze different variations and combinations of absolute and relative positional encoding.   Consider the n-th head of a self-attention layer. First, a vector \u03b3 K j \u2212j \u2208 R d k is added to the projected keys when computing the energy:  Second, the projected values are shifted by \u03b3 V j \u2212j \u2208 R dv when computing the weighted sum:  Using the two different distance terms \u03b3 K j \u2212j and \u03b3 V j \u2212j for the projected keys and the projected values allows to project these keys and queries onto different dimensions, i.e.  A variation of relative positional encoding is to use the sinusoids from the absolute positional encoding approach instead of trained parameters (relative sinusoidal positional encoding):  We also investigate a variation of relative positional encoding where the distance terms \u03b3 V are omitted.  Since the Transformer employs self-attention layers for sequence handling it needs explicit positional information to avoid bag-of-word modeling.  Bahdanau et al. [7] handle this problem by using LSTM layers that incorporate the positional information implicitly.  This LSTM-based positional encoding can be used for the Transformer architecture.  In the encoder, the embeddings are passed into a BiLSTM layer before being passed into the first encoder layer.  Similarly, we introduce a single LSTM layer between the embedding layer and the first decoder layer.  Note that this approach significantly increases the number of parameters (by 20% in our case), increases the model depth and removes parts of the parallelism for the Transformer architecture.",
        "performance": "     "
    }
}