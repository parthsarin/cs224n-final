{
    "article": "In this paper, we propose a new pipelined multi-engine approach to machine translation, which can take advantage of the previously proposed methods, such as rule-based, example-based, pattern-based and statistics-based methods, and eliminate their disadvantages. Some key new techniques in the multi-engine approach, including attribute knowledge classifications, statistical decision-making, pattern transfer, are discussed. MATES/CK, a Chinese-to-Korean Machine Translation system based on the proposed approach, has been developed. The Design Philosophy of MATES/CK -Multi-Engine Model Pipelined Multi-Engine MT Model from the Engine Viewpoint The core idea of MATES/CK system is \"pipelined multi-engine\". Each MT engine employs a different MT technology. When using the pipelined multi-engine MT approach, an MT task is divided into many sub-problems and we start up an engine to resolve the corresponding sub-problem that is most suitable for being resolved by the most appropriate engine. According to Frederking et al.'s definition (Frederking et al. 1994) , multi-engine machine translation (MEMT) feeds an input text to several MT engines in parallel. But MATES/CK employs different engines serially, not in parallel. So we terms our proposed approach as a pipelined multi-engine approach to distinguish it from Frederking et al.'s definition (Frederking et al. 1994) . The pipelined multi-engine MT model here also follows the typical three-phase scheme (analysis/transfer/synthesis) of a conventional transfer-based system. Rule-based Engine The rule-based engine is mainly used in the post-processing of Chinese morphological analysis and the pruning processing in the syntactical analysis stage (Zhang & Choi 1999; Zhang 1997 ). To improve the robustness of the rule-based engine, we propose a linguistic attribute knowledge classification method to quantify the attribute knowledge descriptions slightly, based on which, a new attribute-pruning algorithm is proposed in the Chinese syntactic analysis stage. Further details see section 3. Statistics-based Engine or Corpus-based Engine The statistics-based or corpus-based engine is rather encouraging than other engines. We use it in POS tagging, best syntactic tree selection, mapping pattern extraction, and lexical translation. A new probabilistic model was proposed and adopted to select the best syntactic tree from the syntactic tree candidate set (Zhang & Choi 1999) . A new lexical selection algorithm was proposed by using Viterbi algorithm and some statistical knowledge (Zhang & Choi 1999) . Pattern-based Engine and Example-based Engine Patterns usually can capture more sensitive context than rules, for example, a sentence level pattern can describe the whole sentence structural information, but a rule can not. So we use the pattern-based engine in the structural transfer. Our patterns are extracted from examples semi-automatically. We proposed a parameterized pattern-based transfer philosophy (Zhang & Choi 1999 ). We will elaborate the pattern-based engine in section 3. The example-based engine is partly used in the lexical translation. Translation Flow Figure 1 illustrates the architecture of the pipelined multi-engine model from the translation flow viewpoint, where \"PA-Structure Analyzer\" is a Chinese predicate-argument (PA) structure analyzer and \"P-Bilingual Dictionary\" is a bilingual dictionary with the wordaligned translation probabilities. The proposed MT model is described as follows: \u2022 Analysis module is composed of a Chinese morphological analyzer, a parser and a PA detector \u2022 Synthesis module consists of a generator and a Korean morphological table. The rulebased engine is triggered in this module. The final Korean translation is: Your paper me make your work about very be interested in (Your paper makes me more interested in your works.) Attribute Knowledge Classification and Attribute Pruning Algorithm As above-discussed, GLR algorithm (M. Tomita ed. 1991) and attribute-pruning 2 algorithm are used to construct the syntactic tree candidate set in the analysis module. A parsing rule is a CFG-type rule, where several pieces of linguistic attribute knowledge can be attached as 1 \"CS\" and \"SS\" mean complete sentence and simple sentence, respectively. \"NP1\" ( /pron /u /n, your papers) is the TOPIC, \"VP1\"( /v+ /pron +VP2, make sb. do sth.) is a typical Chinese PIVOTAL structure, \"VP3\" ( /prep+ NP2+ /v+ /n, be interested in NP2) is a COLLOCATION, so in the PA structure detecting PA(VPl) = \"pivotal\" and PA(VP3) = \"collocation\", \" /adv\" (very much) modifies \"VP4\" as an adverbial. 2 Here attribute knowledge includes the lexical, syntactic and semantic knowledge of each word pre-defined in electronic dictionary (Yu et al 1998; Mei et al. 1985) . matching conditions for pruning out the incorrect branches. The pruning error by the improper attribute knowledge descriptions is the big problem for the general attributepruning algorithm. To attack this problem, we propose an attribute knowledge classification method. All the attribute knowledge descriptions attached to the CFG rule are divided into four classifications from two dimensions as follows: Definition 1: \"Strongly-restricted\" and \"Weakly-restricted\" attribute knowledge If a piece of attribute knowledge with a CFG-rule can describe a certain of natural language phenomenon exactly and completely, we define the corresponding attribute knowledge in this CFG-rule as \"strongly-restricted\" attribute knowledge (briefly, \"SR\"), otherwise the corresponding attribute knowledge in this CFG-rule is called \"weakly-restricted\" attribute knowledge (briefly, \"WR\"). Definition 2: \"Positive\" and \"Negative\" attribute knowledge If a CFG-rule is allowed to reduce to a non-terminal symbol while a piece of attribute knowledge attached to the rule is satisfied, we say this piece of attribute knowledge in this CFG-rule is \"positive\" (briefly, \"P\"). In contrast, when a piece of attribute knowledge is satisfied, but the CFG-rule is prohibited to carry out a reduce action, we say this attribute knowledge in this CFG-rule is \"negative\" (briefly, \"N\"). The following is a typical parsing rule: where \"NP/adj/n\" stands for noun phrase, adjective and noun, respectively. The first line is the CFG-rule itself, where \"CenterNode=l\" means the central node of this CFG-rule is the second node \"n\", 1524 is the occurrence frequency of this CFG rule in our training corpus. The second line is two examples of attribute knowledge, where \"SubClass\" and \"Attributive\" are two kinds of attribute knowledge of an adjective, which are defined in the electronic dictionary (Yu et al. 1998 ). The first attribute knowledge \"0:SubClass:l\" is used to judge if the sub-classification of the adjective is class 1 among several adjective subclasses, where the adjective is the first Chinese word of RHS (right-hand side) of the CFG rule. All the adjectives in class 1 may modify a noun directly. The adjective feature \"attributive\" means that adjective can modify noun without \" (Genitive marker)\" between adjective and noun. \"Attributive:N\" means that the adjective cannot modify noun without \" \", namely, [0: Attributive:N SR N] means that if adj+n has no \" \" between them and the feature \"attributive\" of this adjective is 'N', then they can not be reduced to NP. According to Chinese grammar, our statistical results from our corpus reveal that: \u2022 Even though an adjective can modify a noun directly, the adjective and the noun are not always reduced to a noun phrase. So the first attribute knowledge is \"WR\" and \"P\". \u2022 If an adjective can not modify a noun without \" \" between them, then adj+n are strictly prohibited to reduce to a noun phrase directly. So the second attribute knowledge is \"SR\" and \"N\". In general, if a piece of attribute knowledge with a CFG-rule occurs frequently in a balanced tree-tagged corpus, then this attribute knowledge is great possible to be \"weakly-restricted\" attribute knowledge, and vice versa. The classifications depend on both the occurrence frequency of the rule with attribute knowledge in a tree-tagged corpus and the linguist's judgement (Zhang 1997) . For the limit of the paper's length, we have to discuss the algorithm to acquire the parsing rules with attribute knowledge classifications in our other paper (Zhang & Choi 1999A ). Linguist's judgements are necessary in the knowledge acquisition algorithm, but the judgement is not time-consuming and limited within a small scope. In addition, apart from treebank we need not any other specially tagged corpus in our knowledge acquisition algorithm. We have obtained 1174 pieces of parsing rules from our training corpus, including 5964 pairs of CFG-rule and attribute knowledge, in which 2710 pieces is \"SR\" and 3254 is \"WR\". \"Strongly-restricted\" means that we can describe a certain of language phenomenon clearly and exactly, so we can use the \"strongly-restricted\" attribute knowledge without bringing any bias. Algorithm 1 illustrates the construction of the syntactic tree candidate set: (4.5) Execute reduce action AR , P Tree = P Tree * P A , go to (2). where, three empirical parameters \u03b1 1 , \u03b1 2 and \u03b1 3 are assigned to compute the penalty value a, we adjust the value of the three parameters so that the correct tree can be ranked as top as possible. In this paper, \u03b1 1 =0.1, \u03b1 2 = 0.6 and \u03b1 3 =0.3. Please note that, the smaller penalty means that the corresponding tree is less possible to be the correct one. Algorithm 1 consists of a basic GLR algorithm, a new attribute-pruning algorithm and two scoring functions (calculating \u03b1 and P Tree ). GLR algorithm acts as a basic skeleton parsing algorithm. The attribute-pruning algorithm is used to prune out some of the meaningless candidate trees. The first scoring function is to calculate the attribute penalty value \u03b1, and the second one is used to calculate the probability P Tree of each candidate tree. Our pruning algorithm plays an important role in algorithm 2. Only in the second case in step 4.4, when the attribute knowledge is annotated with \"SR\" and \"N\", we can prune out the meaningless branches. In the other cases, we will give the attribute penalty. This can, not only prune out lots of useless candidates, but also guarantee the correct one reserved. Furthermore the attribute penalty reflects the inexactness of the attribute knowledge description, namely, a can indicate which trees are more possible to become the correct one. So our attribute classification method is an effective way to avoid the pruning errors. Once the Chinese syntactic tree candidate set is constructed, the statistics-based engine will be started up to select the best syntactic tree from the candidate set. Based on the algorithm 1, we propose and employ an integrated scoring function to select the best tree (Zhang & Choi 1999) . The scoring function combines the candidate tree probability P Tree with the penalty value a, which can describe the syntactic tree both quantitatively and qualitatively. For further discussion of the scoring function, please see Zhang & Choi (1999) . Parameterized-pattern-based Structural Transfer Structural transfer is carried out by means of the pattern-based engine. A mapping pattern is a typical parameterized bilingual sentence or sub-sentence pair with some parameters, which is formalized as: CST 0 +...+ CST n |(Ph) \u2192 KST0|[t 0 ] + ... + KSTm|[tm] {P Score } (0<m<n) \"CST\" and \"KST\" stand for \"Chinese Sub-sTructure\" and \"Korean Sub-sTructure\". \"Ph\" is a Chinese phrase tag, which means that \"CST 0 +...+CST n \" should be finally reduced to a phrase \"Ph\". The integer index t i attached to KST i means that is transferred to KST i which is used in lexical selection. P Score is a priority evaluation function, which is defined as follows: (1) where \"?\" means that there should be a Korean morphological change or a postposition or an auxiliary word in this position, but which can not be determined in this pattern currently. 0 CST i \u2208 phrase tag 1 CST i \u2208 POS tag E(CST i ) = (2) 2 CST i \u2208 sub- From the pattern definition and formula (1), we can find that our patterns are parameterized by associated with a priority evaluation function P Score and a corresponding position index t i . We can draw some hidden features: \u2022 Formula ( 1 ) is a priority evaluation function, which is used to reduce the conflict among patterns. When a conflict occurs, the preferred one is the pattern whose evaluation value is higher. The idea behind formula ( 2 ) is that, the more fine-grained linguistic knowledge a pattern contains or the longer a pattern is, the more preferred a pattern is. According to formula (2), the lexical patterns are the most preferred, for example, pattern P4 is preferred to P3, and P1 is preferred to P2. \u2022 \"Ph\" defined in a pattern can guarantee that a pattern must be matched with a complete syntactic structure. Pattern is a linear continuous string, but it contains a certain of linguistic information, so we limit that only a complete syntactic structure can be matched with a pattern. \"A complete syntactic structure\" means a truncation of a sub-tree. Figure 2 illustrates what is a truncation. In figure 2 , \"T 1 =pron+v+num+mea+n\" is an invalid truncation, but \"T 2 =NP+v+PP\" is a valid truncation, SS is the root of the truncation T 2 . Only when T 2 and SS are matched with Figure 2 . An example of truncation a mapping pattern successfully, we can say the matching is right. The sentence in figure 3 can be matched with P1 and P2, but P1 is preferred. \u2022 The integer index t i in a pattern records the important position mapping relation between a Chinese word and its possible Korean translation, which is very useful in lexical selection (Zhang & Choi 1999) . Our transfer patterns are extracted semi-automatically from our bilingual corpus. 23,200 mapping patterns are obtained from the corpus. Once the final syntactic structure and word order of the Korean translation are determined by the mapping patterns, the statistics-based engine will be started up to carry out the lexical transfer processing. We propose a statistics-based lexical transfer method that uses bilingual lexical transfer probability and Korean word co-occurrence statistics as well as Viterbi algorithm. For further discussion, please see Zhang & Choi (1999) . Experiment We built a Chinese-Korean bilingual corpus to train and test MATES/CK system. The corpus contains 115,960 sentences, all of the sentences are Chinese-Korean bilingual pairs, and out of which 61,599 sentences are Chinese-Korean-English trilingual pairs. The average length of the sentences is 13.2 Chinese words per Chinese sentence and 9.2 eojeois per Korean sentence. The corpus includes daily sentences and economic texts. Grammatical Knowledge-Base of Contemporary Chinese (Yu et al. 1998 ) is used as Chinese syntactic knowledge database and <<TongYiCi CiLin>> (Mei et al. 1985) as a Chinese thesaurus. <<Chinese-Korean Dictionary>> (Hong et al. 1989 ) is used as a basic Chinese-Korean dictionary to tag corpus and get the word translation dictionary for lexical selection. Total 2100 typical bilingual sentences are selected from our corpus to test MATES/CK system. The test set is also used to train MATES/CK system. 1500 sentences are selected on purposes so that the Chinese syntactic features and the Chinese-Korean bilingual mapping issues can be considered fully in the testing corpus, the other sentences are selected randomly. The average length of the testing sentences is 15.2 Chinese words per sentence. Based on the above sources, we have got 1174 CFG rules with 2710 \"SR\" attribute knowledge and 3254 \"WR\" attribute knowledge as well as a probabilistic LR table for Chinese analysis. We have also obtained 23,200 parameterized mapping patterns for structural transfer and a 4200-entry transfer dictionary for lexical selection 5 . In the analysis module, based on the rule-driven engine, 92.9% syntactic trees are pruned out by our attribute-pruning algorithm 6 , at the same time, no any correct syntactic trees are pruned out by mistake. In contrast, if all the \"WR\" attribute knowledge is changed to \"SR\", sentences are rather free. A test (Zhang 1997 ) reveals that there will generate 15743 syntactic candidate trees for a simple Chinese sentence \" (we can not learn English)\" by using our CFG parsing rules and GLR algorithm without any pruning process. Another example is: considering the CFG rule \"SS(Simple Sentence)\u2192n(noun)+adv\", \"n\" and \"adv\" occur immediately 126,076 times in the corpus, but only in two cases, \"n\" and \"adv\" can be reduced to \"SS\"(one is \" (It is raining heavily)\", one is \" (It is snowing heavily)\"), so we can find that, in this case, there must be large number of branches should be pruned out. then there will be 99.1% syntactic trees to be pruned out, but unfortunately 27.2% correct syntactic trees are also pruned out in the meantime. This reveals that the traditional attributebased method is too rigid to be robust and our attribute knowledge classification method is an effective way to improve the robustness of the attribute-based method. We give a decision criteria of four levels: best(score=1.0), good(0.6), poor(0.2) and error(0.0) to evaluate the structural transfer and whole translation quality (Choi et al. 1994) . The final score for evaluation (FSFE) is equal to the arithmetical mean of all the scores: 1.0*#of \"best\" + 0.6*#of \"good\"+0.2*#of \"poor\" FSFE = number of sentences 1 , the performance of our approach is promising. Please note that the whole translation accuracy should be more than the product of parsing accuracy and transfer accuracy, because in some cases even if the parsing tree is not right, maybe the Korean translation is also right by our transfer patterns. The speed of MATES/CK is very high. It only takes 270 seconds to translate all of the 2100 Chinese sentences with IBM PC 586/400 128M. The main translation errors arise from the analysis and structure transfer of some complex Chinese syntactic or semantic structures and some idiomatic expression translation as well as the Korean generation. Conclusion Distinguished from the Frederking et al.'s definition (Frederking et al. 1994) , we propose a hybrid pipelined multi-engine approach to MT in this paper, based on which MATES/CK system was implemented. We aim at making full use of the different translation engines. According to the proposed MT module, the various problems of a translation task in each phase are decomposed into some sub-problems and each sub-problem is tried to be solved by the most appropriate translation engine. In summary, the proposed approach has the following features and advantages compared with some traditional approach: \u2022 It can integrate the different MT approach naturally, and each MT sub-problem can be resolved by the most appropriate translation. \u2022 Linguistic attribute-knowledge classification method can improve the linguistic knowledge-based methods greatly. Our future research will be directed towards the construction of large training corpus and exploitation of more powerful hybrid MT language model. Acknowledgments: We are grateful to Miss Song, Heejung, Ms. Huang, Jinxia, Prof. Wu, Yonghua, Miss Song, Youngmi and Miss Kim, Jihyoun, who are our partners, for their fruitful collaboration and help. Algorithm 1: Construction of the candidate set of Chinese syntactic trees Input: Chinese POS-Tagged words Output: A candidate set of Chinese Tree Method: (1) GLR as a basic algorithm. Let \u03b1 stand for an attribute penalty value of a candidate tree and P Tree stand for the probability of a candidate tree. Their initial values are \u03b1 =l and P Tree =1. (2) Get an action from LR Table . Every action is associated with an action conditional probability P A 3 . (3) If the current action is a shift action A s , then do as a standard GLR algorithm and P Tree = P Tree * P A (4) If the current action is a reduction action A R 4 , then do as follows: (4.1) Get a piece of attribute knowledge K c , and let T KC stand for the corresponding tags of K c . (4.2) If K c == NULL, then \u03b1 = a * a 1 , go to (4.5). // The above line is to calculate attribute penalty value when no any attribute knowledge is satisfied. (4.3) If K c is not satisfied with the current input, then go to (4.1). (4.4) If K c is satisfied, then do: \u2022 If T KC = \"SR\" + \"P\", then go to (4.5). \u2022 If T KC = \"SR\"+\"N\", then go to (2). // Only in this case, pruning action occurs. \u2022 If T KC = \"WR\"+\"P\", \u03b1 = \u03b1 * \u03b1 2 , go to (4.5). //Calculate penalty value \u2022 If T KC = \"WR\" + \"N\", \u03b1 = \u03b1 * \u03b1 3 , go to (4.5). //Calculate penalty value 3 P A is the action probability in LR table, for whose definition please see Zhang & Choi (1999) . 4 A R consists of a CFG-rule, several pieces of attribute knowledge description annotated with \"stronglyrestricted\" or \"weakly-restricted\" and \"negative\" or \"positive\" tags, a probability P A .",
    "abstract": "In this paper, we propose a new pipelined multi-engine approach to machine translation, which can take advantage of the previously proposed methods, such as rule-based, example-based, pattern-based and statistics-based methods, and eliminate their disadvantages. Some key new techniques in the multi-engine approach, including attribute knowledge classifications, statistical decision-making, pattern transfer, are discussed. MATES/CK, a Chinese-to-Korean Machine Translation system based on the proposed approach, has been developed.",
    "countries": [
        "South Korea"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "2",
    "year": "1999",
    "month": "August 23-25",
    "title": "Pipelined multi-engine Machine Translation: accomplishment of {MATES}/{CK} system"
}