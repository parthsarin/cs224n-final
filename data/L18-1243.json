{
    "article": "Automatically scoring metaphor novelty is an unexplored topic in natural language processing, and research in this area could benefit a wide range of NLP tasks. However, no publicly available metaphor novelty datasets currently exist, making it difficult to perform research on this topic. We introduce a large corpus of metaphor novelty scores for syntactically related word pairs, and release it freely to the research community. We describe the corpus here, and include an analysis of its score distribution and the types of word pairs included in the corpus. We also provide a brief overview of standard metaphor detection corpora, to provide the reader with greater context regarding how this corpus compares to other datasets used for different types of computational metaphor processing. Finally, we establish a performance benchmark to which future researchers can compare, and show that it is possible to learn to score metaphor novelty on our dataset at a rate ignificantly better than chance or na\u00efve strategies. Introduction Metaphors exist along a novelty continuum, with highly conventional metaphors (those that a person is likely to encounter on a day-to-day basis, e.g., \"I spent an hour on my homework.\") at one end, and highly novel metaphors (those that stand out as being particularly uncommon or creative, e.g., \"She frowned like a thunderstorm.\") at the other, with many falling somewhere in between. Despite this, research on computational metaphor processing to date has largely viewed metaphor as a binary phenomenon. This simplification has at least partially arisen due to a lack of metaphor novelty-annotated data, as pointed out by Haagsma and Bjerva (2016) . Resource scarcity notwithstanding, having a reliable means of automatically scoring metaphor novelty could be of benefit to many tasks. Word sense disambiguation, for example, can be quite adept at handling conventional metaphors (simply treating them as additional word senses), but novel metaphors require more complex processing (Shutova, 2015; Haagsma and Bjerva, 2016) . Automatically grading essays, asessing text difficulty levels, and identifying interesting topics for book discussion systems could all benefit from an ability to automatically score metaphors. Metaphor novelty can also be leveraged for psychological and cognitive health assessment. For example, Gutierrez et al. (2017) applied standard, binary metaphor detection to open-ended patient interviews to aid in the detection of schizophrenia, motivated by clinical research indicating that schizophrenic patients tend to produce particularly bizarre figurative speech (Kuperberg, 2010; Billow et al., 1997) . Although the binary metaphor labels were useful, one can hypothesize that performance could be further magnified with continuous novelty scores. Likewise, research in cognitive neuroscience has found that patients with Alzheimer's disease often struggle with comprehending novel metaphors but not conventional metaphors (Amanzio et al., 2008) . Systems capable of distinguishing between different grades of metaphor novelty could thus learn ways to assess cognitive health based on a user's per-ceived comprehension of different metaphors. However, data scarcity currently acts as a barrier to research activity in these promising application areas. In this work, we remove that barrier by presenting a large (18,000+ instances) corpus of syntactically-related word pairs from four domains annotated for metaphor novelty, and make it publicly available to researchers under the Creative Commons Attribution ShareAlike 3.0 Unported License. We first provide an overview of existing metaphor datasets. Then, we describe our data collection process, and present an analysis of this new corpus. Finally, as a proof-ofconcept we develop a metaphor novelty scoring approach and train it on this new dataset to establish a performance benchmark for this task. Related Work Although no datasets annotated for metaphor novelty currently exist, a number of datasets have been released in the past for research on traditional metaphor detection. Details about currently-existing free, publicly-available, Englishlanguage metaphor datasets are provided in Table 1 . The most widely-used of these has been the Vrije Universiteit Amsterdam Metaphor Corpus (VUAMC) (Steen et al., 2010) . The VUAMC is a subset of the BNC Baby corpus (Consortium, 2005) , comprised of document fragments from four domains: news, academic text, fiction, and transcribed conversations. Individual words in those documents are labeled as metaphors. Those metaphors are of all partof-speech types, ranging from conventional to novel; however, novelty is not specified in the annotations. Other existing metaphor datasets have filled more specialized needs for different tasks. Some of these have included data from multiple languages. Levin et al. (2014) , Tsvetkov et al. (2014), and Mohler et al. (2016) all included instances in English, Spanish, Russian, and Farsi in their data. Levin et al.'s dataset contains conventional metaphors from each of those languages belonging to three target domains: poverty, wealth, and taxation. Tsvetkov et al. (2014) 1 : Metaphor Datasets small test sets in Spanish, Russian, and Farsi (as well as English) to study whether metaphor detection models trained on English samples could be applied to other languages. Mohler et al. (2016) produced metaphoricity annotations on a scale from 0-3; theirs is the only existing metaphor dataset to use non-binary labels. The free version of their dataset contains English and Spanish instances, and the paid version contains additional instances for those languages as well as instances in Russian and Farsi. Some metaphor datasets have been principally concerned with the metaphoric and literal uses of different verbs. The TroFi Metaphor Dataset (Birke and Sarkar, 2006 ) is one of these, comprised of sentences originating in the Wall Street Journal Corpus (Charniak et al., 2000) . The sentences contain metaphoric or literal uses of 50 verbs. Similarly, Mohammad et al. (2016) created a dataset containing sentences with metaphoric or literal uses of 440 verbs. Their data also includes emotionality annotations, and an-notations indicating relative metaphoricness and emotionality (more, less, or equal) for paired uses of the same verb. Finally, a few metaphor datasets are simply lists of metaphors. These datasets were developed for research in linguistics or the humanities, rather than for computational analysis. Perhaps the most famous collection of metaphors, the Master Metaphor List (Lakoff, 1994) , falls under this category. The Master Metaphor List contains conceptual metaphor mappings that guide the generation of linguistic metaphors (e.g., the conceptual metaphor, TIME is MONEY, gives rise to the linguistic metaphor, \"I spent an hour on my homework.\"). Also a member of this category is the large list of metaphors created by Pasanek (2015) . This dataset is comprised of sentences from primarily 18thcentury British literature that convey metaphors of the mind (a sample from the dataset is Jane Austen's \"Astonishment and doubt first seized them; and a shortly succeeding ray of common sense added some bitter emotions of shame.\"). Figure 1: Annotation Instructions The Metaphors Mind dataset is still growing, as Pasanek continues to add to it. Of the datasets described, the closest to ours is the LCC Metaphor Dataset, since it labels word pairs with a range of scores rather than with binary labels. However, the focus of the LCC Metaphor Dataset is on metaphoricity (ranging from no metaphoricity to clear metaphor) rather than metaphor novelty. We found upon analysis of the dataset that instances rated with high metaphoricity were not necessarily novel metaphors. The following sample from the LCC Metaphor Dataset illustrates this: A measure of the protection provided to an industry by the entire structure of tariffs, taking into account the effects of tariffs on inputs as well as on outputs. In the LCC dataset, the word pair structure of tariffs is scored as a \"3\" (high metaphoricity), and this is a reasonable metaphoricity score; clearly, a tariff cannot have a physical structure. However, this metaphor is also quite conventional, and unlikely to strike any reader as particularly creative (in fact, many readers may not notice that the expression is figurative at all unless it is explicitly pointed out to them). Thus, its novelty score should be low. Data Collection Our dataset contains continuous metaphor novelty annotations for syntactically-related word pairs extracted from the VUAMC. Word pairs are comprised either of two content words (nouns, verbs, adverbs, and adjectives, excluding stopwords, proper nouns, and some auxiliary verbs) or a content word and a personal pronoun. Syntactic relations were identified using Stanford CoreNLP (Manning et al., 2014) . We required that each word pair contain at least one content word labeled as a metaphor using the original binary annotations provided with the VUAMC, thus entailing that each word pair was potentially metaphoric. 1 An-Figure 2 : Original VUAMC Corpus and Our Annotations notators were asked to score the metaphor novelty of each word pair, given the enclosing sentence as context, on a discrete scale from 0 (non-metaphoric) to 3 (highly novel metaphor); the multiple annotations collected for a given word pair were later aggregated to a single continuous label. A sample of the annotation instructions is shown in Figure 1 , and Figure 2 illustrates both the annotations included in the original VUAMC and the final, continuous labels included in our dataset, for the same source sentence. We collected five annotations from crowd workers for all instances (18,439 word pairs) using Amazon Mechanical Turk (https://www.mturk.com). Additionally, we collected annotations from two trained annotators for each instance in the test set (3162 word pairs). The trained annotators used the same labeling interface as the crowd workers. Instances were grouped into Human Intelligence Tasks (HITs), each containing all of the instances associated with 10 sentences, and crowd workers were paid $0.20 per HIT. Overall, 479 crowd workers participated in annotating the dataset. Workers were required to have an overall HIT approval rate (across all HITs on Amazon Mechanical Turk) greater than or equal to 90%. Crowdsourced annotations were filtered using the algorithm described in Parde and Nielsen (2017) , which identifies substandard workers based on their poor correlation with other workers. Workers who were filtered by the algorithm for a given batch of HITs were also disqualified from accepting future HITs. Adjudication The crowdsourced annotations for the training instances were automatically aggregated to continuous labels using the regression-based approach developed by Parde and Nielsen (2017) . Briefly, this approach trains a random subspace regression model on data that has been labeled by both crowd workers and experts, using features based on annotation distribution and estimated annotator quality, to predict optimal aggregations of crowd labels. The annotations provided by trained annotators for the test set were averaged, unless the annotators disagreed strongly (e.g., a 0 and a 3) or if one of the annotators did not agree with the score produced by averaging. In those cases (111 total), instances were forwarded to a third-party adjudicator to make the final decision. The adjudicated labels comprise the dataset's gold standard test labels. Inter-annotator agreement between trained annotators was measured across all 3162 test instances, prior to any adjudication or annotator discussion, using kappa (\u03ba) with quadratic weights between the four \"classes\" of 0, 1, 2, and 3, resulting in \u03ba = 0.435. This highlights the fact that scoring metaphor novelty is a difficult task even for humans; however, most of the annotators' disagreements were just between two adjacent (similar) scores. We also computed \u03ba with the more relaxed constraints established by Mohler et al. (2016) (considering scores within a distance of 1 from one another to agree), and this resulted in \u03ba = 0.897. In our published dataset, we include the original crowdsourced annotations for each instance, the aggregated label for each training instance, and the gold standard label for each test instance. Corpus Analysis Score Distribution The corpus contains many more conventional metaphors than novel metaphors, which mirrors the distribution seen in naturally-occurring text. The score distribution across the full corpus, with scores binned in 0.125 intervals, is shown in Figure 3 . Aggregated training scores were scaled to fit the 0-3 range (the aggregation model originally predicted scores within a more narrow range). Scores from 2.0-3.0 represent novel metaphors, scores from 0.0-1.0 represent conventional (fossilized and non-) metaphors, and scores from 1.0-2.0 are metaphors that fall somewhere between conventional and novel. Pair Type Distribution Unlike many existing metaphor detection corpora, our corpus contains a wide variety of word pairs. This makes it possible to capture metaphors that may be missed by more constrained datasets. Table 2 lists the frequencies with which different types of word pairs occur in our corpus, with the horizontal axis corresponding to the pair's focus word (the word labeled as a metaphor in the VUAMC), and the vertical axis corresponding to the word paired with that term. The most common pair type in our dataset is verb metaphors paired with nouns, followed closely by noun metaphors paired with nouns. Other common pair types include noun metaphors paired with verbs, noun metaphors paired with adjectives, adjective metaphors paired with nouns, and verb metaphors paired with verbs (e.g., \"She aims to explore the Denton coffee scene.\"). Dependency Type Distribution Finally, we analyze the various syntactic relations included in our corpus. Table 3 shows the frequency of each dependency type, the average score associated with it, and the standard deviation of the scores associated with it. The most common relation in our corpus is nmod (nominal modifier), followed by amod (adjectival modifier), dobj (direct object), and nsubj (nominal subject). Instances with dependency amod had the highest average score, and instances with dependency nmod:tmod (temporal modifier) had the lowest. The largest standard deviation among scores was found with instances of type amod. Benchmark Although the focus of this paper is on the corpus itself, as a proof of learnability and to establish a benchmark we also train a metaphor novelty scoring model using our data. To do so, we construct a simple neural network with one hidden layer using Keras (https: //keras.io/) with a TensorFlow (https://www. tensorflow.org/) backend. We apply a random normal initialization function, tanh activation, and dropout of 0.1 to both the input layer and hidden layer, and set the number of output nodes to 256 for the input layer and 32 for the hidden layer. We optimize the network using RM-Sprop, an optimization technique that updates learning rates for weights based on running averages of the magnitudes of their recent gradients (Hinton et al., 2012) , and use a mean squared error loss function. We train the network (5 epochs, with a training batch size of 32) using the following features: \u2022 Word Embeddings: 300-dimensional vectors for the word pair's governor and modifier, extracted from the pretrained Google News word embeddings (Mikolov et al., 2013) . \u2022 Dependency Relation: One-hot encoded vector indicating the relation between the words in the pair. \u2022 Word Distance: Absolute distance between the words in the sentence from which the pair was extracted. We compare our benchmark approach to several baselines, described in Table 4 . Our results are shown in Table 5 ; we ran our approach 10 times (weights from our approach were randomly initialized) and report average correlation (r) and root mean squared error (RMSE). Differences between our approach and others are statistically significant (p < 0.0001). The results illustrate that it is possible to learn to score metaphor novelty from our dataset at a rate significantly better than chance or na\u00efve strategies like predicting the mean training label. They also provide a performance benchmark for future researchers who use this dataset to develop their own approaches. We explore the task of automatically scoring metaphor novelty further in Parde and Nielsen (2018) . Conclusion In of the VUAMC. We make this dataset publicly available 3 under the Creative Commons Attribution ShareAlike 3.0 Unported License. We provide an overview of existing metaphor datasets, and a comprehensive analysis of our new corpus. Finally, we establish a performance benchmark on this dataset to which other researchers may compare their work 4 . In the future, we plan to improve upon the performance of our metaphor novelty scoring system. It is our hope that the availability of this dataset stimulates further research in this area by others as well. Acknowledgements This material is based upon work supported by the NSF Graduate Research Fellowship Program under Grant 1144248, and the NSF under Grant 1262860. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Bibliographical References Amanzio, M., Geminiani, G., Leotta, D., and Cappa, S. (2008) . Metaphor comprehension in alzheimer's disease: Novelty matters. Brain and Language, 107(1):1 -10. Angeli, G., Johnson Premkumar, M. J., and Manning, C. D. (2015)",
    "abstract": "Automatically scoring metaphor novelty is an unexplored topic in natural language processing, and research in this area could benefit a wide range of NLP tasks. However, no publicly available metaphor novelty datasets currently exist, making it difficult to perform research on this topic. We introduce a large corpus of metaphor novelty scores for syntactically related word pairs, and release it freely to the research community. We describe the corpus here, and include an analysis of its score distribution and the types of word pairs included in the corpus. We also provide a brief overview of standard metaphor detection corpora, to provide the reader with greater context regarding how this corpus compares to other datasets used for different types of computational metaphor processing. Finally, we establish a performance benchmark to which future researchers can compare, and show that it is possible to learn to score metaphor novelty on our dataset at a rate ignificantly better than chance or na\u00efve strategies.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Spanish",
        "Russian",
        "English"
    ],
    "numcitedby": "9",
    "year": "2018",
    "month": "May",
    "title": "A Corpus of Metaphor Novelty Scores for Syntactically-Related Word Pairs"
}