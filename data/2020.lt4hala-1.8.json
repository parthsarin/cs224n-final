{
    "article": "The basic tasks of ancient Chinese information processing include automatic sentence segmentation, word segmentation, part-of-speech tagging and named entity recognition. Tasks such as lexical analysis need to be based on sentence segmentation because of the reason that a plenty of ancient books are not punctuated. However, step-by-step processing is prone to cause multi-level diffusion of errors. This paper designs and implements an integrated annotation system of sentence segmentation and lexical analysis. The BiLSTM-CRF neural network model is used to verify the generalization ability and the effect of sentence segmentation and lexical analysis on different label levels on four cross-age test sets. Research shows that the integration method adopted in ancient Chinese improves the F1-score of sentence segmentation, word segmentation and part of speech tagging. Based on the experimental results of each test set, the F1-score of sentence segmentation reached 78.95, with an average increase of 3.5%; the F1-score of word segmentation reached 85.73%, with an average increase of 0.18%; and the F1-score of part-of-speech tagging reached 72.65, with an average increase of 0.35%. Introduction Lexical analysis is the most basic task of Chinese information processing, including automatic word segmentation, part of speech tagging, and named entity recognition. Besides the above tasks, the basic task of information processing in ancient Chinese also includes automatic sentence segmentation. Chinese ancient books have a vast number of texts, and most of them are unpunctuated, which brings great difficulties for readers to read and study. The use of advanced natural language processing technology for automatic sentence segmentation and lexical analysis of ancient Chinese can not only facilitate readers to read, but also of great significance to the arrangement of ancient books, the development of ancient Chinese and the intelligent application of ancient Chinese. Most of the research on information processing in ancient Chinese is focused on a specific subtask, such as automatic sentence segmentation and word segmentation, part of speech tagging and named entity recognition. To complete the basic task of ancient Chinese information processing, most scholars adopt different research methods and techniques, and each subtask need to be completed in turn, which greatly affects the processing efficiency of the machine. Moreover, using sentence segmented by machine to go on doing word segmentation and part of speech tagging are easy to result in multi-level diffusion of tagging errors, which affects the accuracy of overall tagging task. In this paper, a tagging system integrating automatic sentence segmentation and lexical analysis in ancient Chinese is designed and completed. BiLSTM-CRF model is used to joint learn sentence segmentation, word segmentation and part of speech information. Due to the relative shortage of tagged ancient Chinese corpus, most of the previous studies were conducted according to a special book, and the corpus scales were relatively small, so the training model could not be well applied to other types of ancient Chinese texts. Based on the existing resources, this paper constructs four kinds of annotated corpus written in different ages, and verifies the effect of the integrated annotation on different test sets by using the neural network model. Model introduction RNN model and its variants, which are suitable for sequence tagging, have greatly changed the research methods of natural language processing. RNN can be regarded as a multiple overlay structure of the same network. It performs the same operation for each element in the sequence, and each operation depends on the previous calculation results. In theory, RNN can use any length of sequence information, but in practice, only some previous steps can be reviewed. LSTM neural network is a kind of special RNN. Based on the original RNN model, input gate, forgetting gate and output gate are added. Neurons will selectively forget the useless information for current output. It inherits the advantage that RNN can keep the preorder's information, and overcomes the problem that RNN can't really capture the long-distance dependency in the text. BiLSTM is a model put forward by Schuster in 1997 to solve the problem that LSTM can't retain the post information. The main idea of the model is to set up two LSTM structures in the front and back direction of the training sequence. By splicing the LSTM in two directions to capture the preorder and post order's information, the information in the whole training sequence can be retained to the greatest extent. The BiLSTM-CRF model structure used in this paper was first proposed by Huang et al. The output of BiLSTM layer is a probability matrix, which is calculated by BiLSTM based on the optimal result of each moment. In this way, the output tag doesn't consider the influence of the previous tag. For example, the word \"\u5b5f\u5b50\" appears in the input sequence \" \u5b5f \u5b50 (name) \u5352 (die) \u7ee7 \u5ba4 (second wife) \u4ee5 (a conjunction) \u99a8 \u5b50 (name)\", in which \" \u5b5f \" is the first character and \"\u5b50\" is the last character. The model may predict both \"\u5b5f\" and \"\u5b50\" as the first character, such situation should be avoided in the lexical analysis task of ancient Chinese. CRF is a framework for an undirected graph model that can be used to define the joint probability distribution of a tag sequence in a situation that a set of observed sequences need to be tagged. Assume that X is the random variable of the data sequence to be annotated, and Y is the random variable of the corresponding tag sequence. For example, X is the set of sentences in natural language, and Y is the part of speech set that used to mark these sentences. Random variables X and Y are jointly distributed and a conditional model P(Y|X) is constructed according to the pairs of observation sequence and label sequence. The CRF layer is matched with the output layer of BiLSTM, so that the output sequence of BiLSTM becomes the observation sequence of CRF, and then CRF calculates the optimal solution of the whole sequence in probability without ignoring the interaction between sequence element tags. Construction of corpus Ancient texts were selected according to different historical stages, and the corpus with the same size was extracted from the traditional version of Tso Chuan ( \u5de6\u50b3, Han dynasty, 722BC~468BC), Brush Talks from Dream Brook (\u5922\u6eaa\u7b46\u8ac7, Song dynasty, AD1086~AD1093), Fantastic Tales by Ji Xiaolan ( \u95b1 \u5fae \u8349 \u5802 \u7b46 \u8a18 , Qing dynasty, language style is more colloquial, AD1789~AD1798), and Documents of History of Qing Dynasty (\u6e05\u53f2\u7a3f, Republic of China, AD1914~AD1927) as the experimental data set of this paper. The purpose of constructing a corpus by age is to explore the generalization ability of the model for text annotation in different ages after training based on mixed corpus of different ages. The data set is manually proofread on the basis of machine-assisted word segmentation and POS tagging. Kappa was used for labeling consistency test and the Kappa value was higher than 0.8, indicating a higher degree of labeling consistency. The specification of POS tags refers to Ancient Chinese Corpus published by LDC 1 , totaling 21 tags. The experimental data set is divided into training set, development set and test set according to the ratio of 8:1:1. Among them, the training set is a mixed corpus composed of 80% of the corpus in Tso Chuan, Brush Talks from Dream Brook, Fantastic Tales by Ji Xiaolan, and Documents of History of Qing Dynasty. Based on this mixed corpus, this paper discusses the annotation ability of the model to texts of various ages. The experimental corpus set \"\uff1a\uff0c\u3002\uff1b\uff01\uff1f\" six kinds of punctuation as sentence breaks, and each text sequence divided by two sentence breaks is treated as a sentence, with all other punctuation ignored. Table 1 is a general overview of the experimental data set. Integrated word position tag design Xue is the first to put forward a character-based learning method of sequential annotation, who uses four kinds of tags, which is LL(stands for left boundary of a word), LR(stands for monosyllabic word), MM(stands for the middle of a word) and RR(stands for the right boundary of a word), to express the segmentation and annotation information of characters, thus it translates word segmentation task into serialized annotation task formally for the first time. Table 1 : Experimental data set This paper uses this method of character annotation to construct an ancient Chinese integrated-analysis annotation system. For this model, the problem is actually a tag multiclassification problem, where each character needs to be assigned to a specific tag type. Word segmentation layer (WS): Using B, I, E, S four tags. B means that the current character is at the beginning of a multi-character word. I means that the current character is at the middle of a multi-character word. E means that the current character is at the ending of a multi-character word. S represents the current character is a one-character word. After transforming the character annotation sequence, the 1 LDC Ancient Chinese Corpus sentence segmentation results can be calculated out. For example: Character annotation: \u4e5d B \u6708 E \uff0cS \u6649 B \u60e0 I \u516c E \u5352 S \u3002 S \u61f7 B \u516c E \u7acb S \uff0cS After the transformation: \u4e5d\u6708(September) \uff0c\u6649\u60e0\u516c \u5352 (die) \u3002\u61f7\u516c \u7acb(ascend the throne) \uff0c POS tagging layer (POS): Tagging the part of speech of the word to which each character belongs. Meanwhile, incorporating physical tags (personal name nr, place name ns) into POS. Then, adding POS on the basis of WS so that each character can corresponds to its position in the word https://catalog.ldc.upenn.edu/LDC2017T14 \u4e5d B-t \u6708 E-t \uff0cS-w \u6649 B-nr \u60e0 I-nr \u516c E-nr \u5352 S-v \u3002S-w \u61f7 B-nr \u516c E-nr \u7acb S-v \uff0cS-w Each character is tagged word segmentation tag and POS tag, connected by \"-\".Take \"\u6649 B-nr \u60e0 I-nr \u516c E-nr\" as example, \"\u6649\" is the first character of a personal name, \"\u60e0\" is a character in the middle of a personal name, \"\u516c\" is the last character in a personal name, so that \"\u6649\u60e0\u516c\" can be segmented and recognized to a person's name, whose reality tag is represented as \"nr\". Sentence segmentation layer (SS): Tagging whether a character is at the end of a sentence. Adding SS layer on the basis of WS and POS, so that each character can be corresponded with three layers, i.e., word segmentation, part of speech and sentence segmentation. \u4e5d B-t-O \u6708 E-t-L \u6649 B-nr-O \u60e0 I-nr-O \u516c E-nr-O \u5352 S-v- L \u61f7 B-nr-O \u516c E-nr-O \u7acb S-v-L If a character in the corpus is at the break of a sentence, such as \"\u6708\", \"\u5352\" and \"\u7acb\" in the sentence, then tag \"L\" will be put after the part of speech tag, otherwise, tag \"O\" will be put after the part of speech tag. During the process of corpus preprocessing, three-layers tags categories (WS, POS, SS) can be processed in different ways: WS+POS+SS (e.g., \u5352 S-v-L) is a three-layers tag. Under this annotation level, the annotation effect of each subtask, such as sentence segmentation (SS), can be calculated. There is WS+POS (e.g., \u5352 S-v) in two-layers tags. Under this annotation level, the effects of word segmentation (WS) and POS tagging (WS+POS) can be calculated. There is WS (e.g., \u5352 S) and SS (e.g., \u5352 L) in one-layer tags. The effect of sentence segmentation or word segmentation can be calculated. Evaluation indexes The experimental training set is used for feature learning and training of the model, and the test set is used to verify the results of automatic tagging. For the evaluation of automatic tagging results, F1-score (harmonic mean), the most commonly used evaluation index in sequence tagging, is used to measure the effect of the model. F1-score is calculated from P(precision) and R(recall), and the calculation formula is: F1 = 2 * P * R P + R The calculation of Precision is as follows: P = Correct number of tags Number of machine tags The calculation of Recall is as follows: R = Correct number of tags Number of all tags in the corpus Based on the above evaluation metrics, sentence segmentation, word segmentation, part of speech tagging results are calculated. Sentence segmentation calculation is based on sentence rather than characters, that is, according to the label \"L\". If both machine and manual tagging results are \"L\", it is correct. Word segmentation and part of speech are calculated on the basis of words rather than characters. Taking POS tagging as example, it is assumed that the word \u5b5f\u5b50(Mencius) is predicted as \"\u5b5fS-nr\u5b50Snr\". Although the model gets a correct part of speech based on characters, however, the word segmentation is wrong, and the correct answer should be \"\u5b5fB-nr\u5b50Enr\". To determine whether a word belongs to the correct part of speech, whether the character is correctly divided into words should be determined first, that is, determination should be based on the correct word segmentation. Experimental design and result analysis The results of Experiment 1 are the super parameters obtained by manual parameter adjustment on the development set, and the results of Experiment 2, Experiment 3 and Experiment 4 are obtained on the test set. Experiment 1 will verify the necessity of adding word vectors into the integration analysis of ancient Chinese and investigate the effect of word vectors of different dimensions on the results of integrated annotation. Generally speaking, the higher the dimension of the word vector, the more semantic features it contains, but they are not absolute positively correlated. Based on nearly 1.5 billion characters of traditional ancient Chinese raw corpus (from Imperial Collection of Four and other ancient Chinese corpus), selecting word2vec as the tool, CBOW (Continuous Bag of-Words Model) as the model, we carry out character vector pretraining. The experiment sets the word vector dimension to 50, 100, 128 and 200 respectively, selects Tso Chuan test set as the test corpus, and adopts \"WS+ POS+ SS\" as its tagging layer, which is a tagging method of integrating sentence segmentation and lexical analysis. By manually adjusting parameters on the development set, the final hyper-parameter adopted is shown in Table 2 In the BiLSTM-CRF structure, based on experiments on the development set, it is found that the number of layers in BiLSTM had little influence on the precision, so the number of hidden layers in the model, namely the number of layers in BiLSTM, is set as 1. The number of hidden nodes in the sequence tagging task is usually from 200 to 600, and 200 is taken as the parameter here. The minimum sample size is set to 64, with each sample size controlled between 50 and 60. The optimization of the model adopts the \"Adam\" algorithm, which has a good effect in the sequence tagging task. The Dropout method is used to reduce overfitting. A Dropout with a parameter of 0.5 is added between the BiLSTM layer and the full connection layer, which can weaken the excessive interaction between various features caused by the small amount of data, so that the model has the optimal generalization ability and the lowest degree of overfitting. The experimental results are shown in Table 3 . Table 3 : The F1-score of integration of sentence segmentation and lexical analysis(unit %) As can be seen in table 3 , the addition of word vector is necessary for sentence segmentation and lexical analysis tasks in ancient Chinese, especially for POS tagging tasks, which increased by 2.5 percentage points. In the word vector dimension setting, the experiment shows that 128 dimensions is the best for the integrated automatic tagging of ancient Chinese. In order to verify the training effect of the word vector under this dimension, cosine similarity is used to calculate the semantic correlation between the two word vectors: Assume word vector A=(A1,A2,\u2026,An), B=(B1, B2,\u2026,Bn), the formula for cosine similarity is as follows: cos \u03b8 = \u2211 (Ai \u00d7 Bi) n 1 \ufffd\u2211 (Ai) 2 n i=1 \u00d7 \ufffd\u2211 (Bi) 2 n i=1 i represents the dimension of the vector, and Ai represents the specific value of the i-dimension of the character A. Taking characters \u4e5f (modal particle) and \u66f0 (say) as examples, the calculation results are as follows in After analyzing the model tagging errors, we found that Brush Talks from Dream Brook contains a large number of non-repetitive professional terms in various disciplines, for example, in sentence \"\u5357\u5442\u8abf\u7686\u7528\u4e03\u8072(scales)\uff1a\u4e0b\u4e94\u3001\u9ad8\u51e1\u3001\u9ad8\u5de5\u3001 \u5c3a\u3001\u9ad8\u4e00\u3001\", the words \"\u4e0b\u4e94\", \"\u9ad8\u51e1\" are proper names related to music. The relatively sparse data of proper names makes it difficult for the model to learn the relevant features, which is the main reason that Brush Talks from Dream Brook performances worse in POS tagging task. Experiment 4 is designed from two dimensions: (1) in the horizontal dimension, the experiment discusses the differences of model based on mixed corpus, tagging in different ages' corpus under a same tagging layer, and investigates the models' generalization ability considering the result of experiment 3; (2) in the vertical dimension, the experiment compares the tagging differences of same testing corpus under different tagging layers. The performance of the joint model is almost unaffected by the mixed corpus, so the experiment can verify the effectiveness of the integrated tagging method of word segmentation, POS tagging and sentence segmentation. The experiment selects BiLSTM-CRF as model, mixed corpus as training corpus, and 128-word vector dimensions. The experimental results are shown in Table 7 . (1) By observing the F1-score of each testing set in the same tagging layer, it is found that taking mixed corpus as training set, tagging results of the model applying to various testing corpus are not balanced, which are similar to experiment 3's result. By comparing the results under the layer that integrates sentence segmentation and lexical analysis with experiment 3, we found that Brush Talks from Dream Brook's performance in sentence segmentation, word segmentation and POS tagging tasks are 0.7, 1.0, 0.5 percentage points higher respectively; Fantastic Tales by Ji Xiaolan's performance in sentence segmentation and POS tagging tasks are 2.3, 0.7 percentage points higher respectively; Tso Chuan declines slightly in all tasks. This result indicates that the integration model based on mixed corpus has learnt some homogeneity features of each corpus, which improves some testing sets' tagging performances. However, in the meantime, the differences among corpus interferes with the comprehensive judgment of the model, resulting in some testing sets' performance degradation. Therefore, the generalization ability of the integrated tagging model applying to different ages' texts needs to be improved. (2) By observing the F1-score of each testing set's word segmentation task under different tagging layers, the layer that integrates sentence segmentation and lexical analysis performances best in its entirely. Regardless of which testing set, the F1-score of the tagging layer that only segments word is lower than the integrated layer, which means that integrated tagging method of sentence segmentation and lexical analysis can improve word segmentation task in ancient Chinese. (3) By observing the F1-score of each testing set's sentence segmentation task under different tagging layers, the layer that integrates sentence segmentation and lexical analysis performances best in its entirely, which shows that integrated tagging method can improve sentence segmentation task in ancient Chinese. Taking Tso Chuan as example, the F1-score of sentence segmentation under integrated tagging layer is 2.6 percentage higher than the layer only segment sentence. Similar improvement happens in other testing sets, reflecting that in automatic sentence segmentation task of ancient Chinese, integration of sentence segmentation and lexical analysis is better than step-by-step tagging method. (4) Comparing the layer of integrated tagging and the layer of POS tagging, we can find that the F1-score of integrated tagging in most testing sets is higher than POS tagging layer. Taking Tso Chuan as example, the performance of word segmentation and part-of-speech tagging under integrated tagging layer is 0.2 and 0.7 percentage higher than the POS tagging layer respectively. This result verifies that the integration of sentence segmentation and lexical analysis performances better in word segmentation task and POS tagging task than those methods without adding information of sentence break. A comprehensive analysis based on (2), ( 3 ), ( 4 ) can find that the sentence segmentation, word segmentation, and POS tagging tasks have improvement because of the integrated annotation system, and the promotion(F1-score) is not limited to one kinds of testing set. The concrete conditions are shown in Table 8 . Table 8 : The promotion of F-score in each task after using the integrated annotation system Although the integrated tagging method has limit in task promotion, the experiment proves the feasibility of it. It can avoid multi-level spread of tagging errors in single task. For example, if performing tasks step-by-step, we need segment sentence first, and then perform word segmentation task and POS tagging task, which will cause erroneous multi-level accumulation, and the whole performance is not as good as the integrated method. What's more, the tagging method of integrating sentence segmentation and lexical analysis can greatly improve the efficiency of processing words and sentences in ancient Chinese. Conclusion This paper designs and implements the annotation systerm of integrating sentence segmentation and lexical analysis of ancient Chinese. Based on BiLSTM-CRF neural network model, we verify the intergrated tagging model's generalization ability on different ages' texts, as well as the model's effects on sentence segmentation, word segmentation and part of speech tagging of ancient Chinese under different tagging layers on four different historical testing sets, including Tso Chuan, Brush Talks from Dream Brook, Fantastic Tales by Ji Xiaolan and Documents of History of Qing Dynasty. The results appeal that the integrated tagging method performs better among tasks of sentence segmentation, word segmentation and POS tagging. The F1-score of sentence segmentation reached 78.95, with an average increase of 3.5%; the F1-score of word segmentation reached 85.73%, with an average increase of 0.18%; and the F1-score of part-of-speech tagging reached 72.65, with an average increase of 0.35%. Future research will expand the scale of corpus and improve the model. Focusing on the design of deep learning model in the context of large-scale cross era corpus, the model will include attention system and transfer learning method to explore the adaptability of model to different times' texts. Finally, we will develop an integrated analysis system of ancient Chinese with better performance across the ages and styles. Acknowledgements Supported by the project for Jiangsu Higher Institutions& Excellent Innovative Team for Philosophy and Social Sciences. A Project Funded by the Priority Academic Program Deve lopment of Jiangsu Higher Education Institutions. National Language Committee Project of China (YB135-61). Researches of Building Syntactic Chinese-English Parallel Corpus and Humanities Computing Based on Ancient Classics Index(NO. 71673143) national natural science foundation of China. Bibliographical References",
    "abstract": "The basic tasks of ancient Chinese information processing include automatic sentence segmentation, word segmentation, part-of-speech tagging and named entity recognition. Tasks such as lexical analysis need to be based on sentence segmentation because of the reason that a plenty of ancient books are not punctuated. However, step-by-step processing is prone to cause multi-level diffusion of errors. This paper designs and implements an integrated annotation system of sentence segmentation and lexical analysis. The BiLSTM-CRF neural network model is used to verify the generalization ability and the effect of sentence segmentation and lexical analysis on different label levels on four cross-age test sets. Research shows that the integration method adopted in ancient Chinese improves the F1-score of sentence segmentation, word segmentation and part of speech tagging. Based on the experimental results of each test set, the F1-score of sentence segmentation reached 78.95, with an average increase of 3.5%; the F1-score of word segmentation reached 85.73%, with an average increase of 0.18%; and the F1-score of part-of-speech tagging reached 72.65, with an average increase of 0.35%.",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "3",
    "year": "2020",
    "month": "May",
    "title": "Integration of Automatic Sentence Segmentation and Lexical Analysis of {A}ncient {C}hinese based on {B}i{LSTM}-{CRF} Model"
}