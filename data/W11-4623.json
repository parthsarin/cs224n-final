{
    "article": "We show that the quality of sentence-level subjectivity classification, i.e. the task of deciding whether a sentence is subjective or objective, can be improved by incorporating hitherto unused features: readability measures. Hence we investigate in 6 different readability formulae and propose an own. Their performance is evaluated in a 10-fold cross validation setting using machine learning. Thereby, it is demonstrated that sentence-level subjectivity classification benefits from employing readability measures as features in addition to already well-known subjectivity clues. 1 Introduction Wiebe et al. (2004) refer to subjectivity in natural language as \"aspects of language used to express opinions, evaluations, and speculations\". For example, an utterance like \"In the end, though, it is only mildly amusing when it could have been so much more.\" clearly bears an opinion, i.e. is subjective, whereas an utterance like \"The movie takes place in mexico, 2002.\" clearly does not 1 . Readability is usually refered to as \"the degree to which a given class of people find certain reading matter compelling and, necessarily, comprehensible\" (cf. McLaughlin (1969) ). So whereas the meaning of a sentence like \"Nanometer-sized single crystals, or single-domain ultrafine particles, are often referred to as nanocrystals.\" is quite difficult to grasp, a sentence like \"Wills and Kate get into marriage mode.\" is much easier to understand 2 . Why is that? The former sentence not only exhibits a more complex syntactic structure than the latter, but also extensively utilises domain-specific terminology that many readers would not be familiar with. Although counter-intuitive on first sight, we pose the following hypothesis: There is a connection between subjectivity and readability in natural language text. If so, we may assume that knowing about its readability possibly yields valuable information regarding its subjectivity. Related Work To our best knowledge, readability measures have not been used to assess the subjectivity of any lexical units so far, be it word forms, phrases, sentences or whole documents. However, there is Hoang et al. (2008) 's work on evaluating the quality of user-created documents, and recent work on grading the helpfulness of reviews by (O'Mahony and Smyth, 2010) , both incorporating readability measures. Close to our research is Nishikawa et al. (2010) 's study on sentiment summarisation which utilises measures both for informativeness and readability. Very recent support in favour of our hypothesis is provided by (Lahiri et al., 2011) , who measure a correlation between informality and readability. As subjectivity classification poses many interesting challenges and has many applications in NLP including genre detection, flame recognition and information extraction, the identification of features for subjectivity classification and the classification itself has been extensively studied. Especially Wiebe et al. contributed a lot to the field: Wiebe (2000) learns subjective adjectives from corpora in a semi-supervised fashion, while Wiebe et al. (2001) identify other subjectivity clues using collocations. Riloff et al. (2003) and Riloff and Wiebe (2003) present ways to mine extraction patterns for subjective expressions. Wiebe et al. (2004) summarise these findings and show how different features work \"together in concert\". Wiebe et al. (2005) introduce a scheme for annotating opinions and the like in Wiebe et al. (2003) 's English-language Multi-Perspective Question Answering (MPQA) corpus. Wilson et al. (2004) assess the strength of deeply-nested opinions. Wiebe and Riloff (2005) create high-precision classifiers for distinguishing between subjective and objective sentences and use them as a source for learning additional subjectivity clues. Yu and Hatzivassiloglou (2003) perform both document-and sentence-level subjectivity classification using Na\u00efve Bayes classifiers and several unsupervised approaches. Pang and Lee (2004) use a graph-based formalism to first tell subjective and objective sentences apart, then perform a polarity classification employing both Na\u00efve Bayes classifiers and Support Vector Machines. Outline This paper is structured as follows: In the next section, we describe our method by presenting hitherto unused features for subjectivity classification: readability measures. In Section 3 we describe our experimental setup and evaluate its performance. Finally, we draw conclusions and point out possible directions for future work in Section 4. Method Following our assumption that knowing about the readability of natural language text possibly yields valuable information regarding its subjectivity, we will try to measure readability and later exploit this information for sentence-level subjectivity classification. According to Klare (1974) 's survey there are 3 possible solutions to \"tell whether a particular piece of writing is likely to be readable to a particular group of readers\": A first solution is simply to guess. A second solution are tests, manually built and refined. A third solution are readability measures. We will assess readability by such measures for the obvious reason that some of them are automatically computable. Readability Measures We chose 6 different readability formulae from the large body of available readability measures. All measures we chose are automatically computable and do not depend on lexical resources like word lists and the like. We solely present the 6 formulae themselves. The reader interested in their underlying ideas, their development and the derivation of their constants and variables may be refered to the aforementioned Klare (1974) , or the original work cited below. Additionally, we propose an easy to calculate formula that embodies our own intuition for assessing readability. Devereux Readability Index The Devereux Readability Index D was introduced by Smith (1961) and is calculated as shown in Equation 1 , D = 1.56 \u00d7 wl + 0.19 \u00d7 sl \u2212 6.49 (1) where wl is the average word length in characters and sl is the average sentence length in words. The Devereux formula was designed to cover school grades 4 to 12. Hence, the higher the value of D, the less readable the graded text according to the formula. Easy Listening Formula The Easy Listening Formula EL was introduced by Fang (1966) and is calculated simply as shown in Equation 2 , EL = npsw (2) where npsw is the average number of polysyllabic words per sentence, i.e. words with more than just one syllable. The Easy Listening Formula is, as the name suggests, tailored to \"listenability\" rather than readability. Therefore, the higher the value of EL, the less \"listenable\" the graded text according to the formula. Fog Index The Fog Index was introduced in Gunning (1952) and reformulated by Powers et al. (1958) . It is calculated as shown in Equation 3 , FI = 3.068 + 0.0877 \u00d7 sl + 0.0984 \u00d7 nosw (3) 169 where sl is the average sentence length in words and nosw is the average number of one-syllable words per sentence. The higher the value of FI, the less readable the graded text according to the formula. FORCAST The FORCAST formula F was introduced in Caylor et al. (1973) . It is calculated as shown in Equation 4, F = 20.41 \u2212 0.11 \u00d7 nosw (4) where nosw is the average number of one-syllable words per sentence. The higher the value of F, the less readable the graded text according to the formula. New Reading Ease Index The New Reading Ease Index NREI was introduced by Farr et al. (1951) and is calculated as shown in Equation 5 , NREI = 1.599\u00d7nosw \u22121.015\u00d7sl \u221231.517 (5) where nosw is the average number of one-syllable words per sentence and sl is the average sentence length in words. The higher the value of NREI, the less readable the graded text according to the formula. SMOG The SMOG grading S was introduced by McLaughlin (1969) and is calculated as shown in Equation 6 , S = 3 + \u221a npsw (6) where npsw is the number of polysyllabic words. Again, the higher the value of SMOG, the less readable the graded text according to the formula. An Own Formula Following our intuition for how to assess readability, we propose an easy to calculate formula, shown in Equation 7 . W = wl \u00d7 sl \u00d7 ntop (7) Here, wl is the average word length per sentence, sl is the average sentence length in words and ntop is the average number of words in each sentence, that are not among the top 1,000 most frequent words of a large reference corpus. This list was automatically extracted from D EL FI F NREI S W wl \u00d7 \u00d7 sl \u00d7 \u00d7 \u00d7 \u00d7 nosw \u00d7 \u00d7 \u00d7 npsw \u00d7 \u00d7 ntop \u00d7 Table 1 : Comparison of language characteristics captured by the presented readability formulae. wl denotes the average word length, sl the average sentence length in words, nosw the average number of one-syllable words per sentence, npsw the average number of polysyllabic words per sentence. an English-language newspaper corpus of University of Leipzig's Wortschatz 3 project consisting of 49,628,893 distinct sentences, 4,785,862 word types and 926,766,504 word tokens. The idea behind ntop is, that high-frequency words are common to all readers, whereas medium-to lowfrequency words are not necessarily. The more unfamiliar words a reader encounters in a text, the less readable it is. wl and sl basically capture the same idea: both longer sentences and longer words lead to less readable text. Just as for all the other formulae, the higher the value of W, the less readable the graded text according to our formula. Other Formulae There are other well-known readability formulae we did not investigate in yet, e.g. Lorge (1939) and Lorge (1948)'s Lorge formula, Flesch (1944)'s Flesch formula and Dale and Chall (1948) 's Dale-Chall formula. These rely on lexical resources some of which are not publicly available and additionally introduce stronger language dependency. Summary Different readability formulae capture different language characteristics, as summarised in Table 1 . Not only do they differ in their encoded features, but also in their intended outcome. Whereas some aim to determine a school grade, some refer to tables for further interpretation. For those reasons, the readabilities calculated by the presented measures are not comparable in general, although they do have in common, that higher values signalise less readable (or less listenable) text. Evaluation In order to evaluate whether the presented readability measures indeed yield possibly valuable information regarding a natural language text's subjectivity or not, we perform a sentence-level subjectivity classification using readability formulae as features: i.e., given a sentence, extract its features and classify it as being either subjective or objective. Experimental Setup The evaluation data set, the features and the text classifier we used in our experiments are now briefly described. Evaluation Data Set To ensure comparability and reproducibility of our results we use Pang and Lee (2004) 's publicly available subjectivity data set v1.0 4 . This widelyused data set consists of 5,000 sentences marked as \"subjective\" and 5,000 sentences marked as \"objective\". Features Our baseline features are subjectivity clues provided by Wilson et al. (2005) . Their freely available English-language lexical resource encompasses 8,221 word forms, each manually annotated for being either a strong or a weak subjectivity clue and for its polarity being either positive, negative or neutral. We only used the 5,569 strong subjectivity clues to form a solid baseline. Examples of strong subjectivity clues include disagree, love and overstate. Our additional features are the readability formulae presented in Section 2. Although most of them were developed to capture readability of whole texts, we apply them to single sentences. Their minima, maxima, averages and standard deviances measured in the 5,000 subjective and 5,000 objective sentences are shown in Table 4 . Text Classifier The actual text classification is performed by Support Vector Machines (SVMs) (cf. Vapnik (1995) and Cortes and Vapnik (1995) (Wilson et al., 2005) 's strong subjectivity clues. trained using an radial basis function kernel as provided by LibSVM (cf. Chang and Lin (2001) ). Even though it is highly probable that some of the baseline features are either redundant, misleading, or both, no feature selection (cf. for example Weston et al. (2001) ) was carried out. Results As we use 8 features (7 readability formula and a \"feature package\" consisting of 5,569 strong subjectivity clues), there are 8 k=1 8 k = 255 possible feature combinations. For each feature combination a SVM was trained and tested in a 10-fold cross validation setting. In this paper we only report on the best performing feature combinations regarding precision, recall and f-score plus each single feature on its own. The results of all feature combinations will be made accessible through the author's web site 5 by the time of the publication of this work. Results of single features are shown in Table 2 , results for best performing feature combinations are shown in Table 3 . -85.21 -111.79 -32.18 -33.09 -49.78 -53.82 8.18% 8.74 Discussion Although the presented measures may be considered as rather crude approximations of readability, it is quite clear from the results shown in Table 2 and Table 3 that they provide a valuable source of information regarding the sentence-level subjectivity. Whereas for \"single features\" the strong subjectivity clues perform best in regards to precision, every single readability formula significantly outperforms them in regards to both recall and f-score as shown in Table 2 . The best performing readability measure in terms of precision is the Devereux Readability Index, the Fog Index performs best in recall and f-score. Even though these results look promising on their own, it is noteworthy that a classifier that simply always chooses the same class reaches P = 0.5, R = 1.0 and F = 0.67 on the given data set. Combinations of different readability formulae show considerable improvement in precision, recall and f-score over single readability formula features as shown in Table 3 . Finally, combining different readability formulae with strong subjectivity clues shows further improvement and outperforms using these clues alone in precision, recall and f-score, as also shown in Table 3 . It is remarkable that FORCAST appears in every single feature combination shown in Table 3 , both with and without the subjectivity clues. Noticeably Easy Listening Formula only appears in feature combinations without the subjectivity clues. Fog Index, the best performing single readability formula, appears only in combinations including the subjectivity clues. Our own formula does not contribute a lot -it only appears once. Comparison (Pang and Lee, 2004 ) report 92% accuracy on sentence-level subjectivity classification using Na\u00efve Bayes classifiers and 90% accuracy using SVMs on the same data set. (Wiebe et al., 2004) report 94% accuracy on document-level subjectivity classification using the k-nearest-neighbour algorithm. Although these results are not directly comparable to ours, our approach seems to perform not as good as theirs. Conclusion & Future Work We have shown that using readability formulae and their combinations as features in addition to already well-known subjectivity clues leads to significant quality improvements in sentence-level subjectivity classification. Therefore, one might argue in favour of our initial hypothesis and say that there is a connection between readability and subjectivity. We will carry out a detailed error analysis to shed light on their relationship. Although our approach does not yet perform as good as current state-of-the-art, we believe that readability is a feature with less language dependency and a greater generalisation power than the pure presence or absence of certain word ngrams. Thus, it looks promising to further investigate in readability formulae as features for subjectivity classification. Thereby, it is possibly worthwhile to choose more complex formulae, e.g. ones that incorporate syntactic knowledge like the depth of parse trees or the number of subtrees of parse trees (cf. Schwarm and Ostendorf (2005) ). Such formulae might be more reliable predictors of readability than the one used in our current work. Questions still remaining open include: do we need readability formulae themselves or is it sufficient to just use the language characteristics captured by them? Are readability formulae independent from each other, and if so, to what degree? Are our results reproducible on other data sets, in other domains and even for languages other than English? If so, is there a plausible linguistic explanation for a correlation between subjectivity and readability? We will address these points in future work. Acknowledgements I'm grateful to Bo Pang and Lilian Lee, Chih-Chung Chang and Chih-Jen Lin as well as Theresa Wilson, Janyce Wiebe and Paul Hoffmann for making their data sets, their software and their lexical resources publicly available. Special thanks go to the anonymous reviewers whose useful comments and suggestions considerably improved the original paper.",
    "abstract": "We show that the quality of sentence-level subjectivity classification, i.e. the task of deciding whether a sentence is subjective or objective, can be improved by incorporating hitherto unused features: readability measures. Hence we investigate in 6 different readability formulae and propose an own. Their performance is evaluated in a 10-fold cross validation setting using machine learning. Thereby, it is demonstrated that sentence-level subjectivity classification benefits from employing readability measures as features in addition to already well-known subjectivity clues.",
    "countries": [
        "Germany"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "10",
    "year": "2011",
    "month": "May",
    "title": "Improving Sentence-level Subjectivity Classification through Readability Measurement"
}