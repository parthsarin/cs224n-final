{
    "article": "State-of-the-art statistical machine translation systems use hypotheses from several maximum a posteriori inference steps, including word alignments and parse trees, to identify translational structure and estimate the parameters of translation models. While this approach leads to a modular pipeline of independently developed components, errors made in these \"single-best\" hypotheses can propagate to downstream estimation steps that treat these inputs as clean, trustworthy training data. In this work we integrate N -best alignments and parses by using a probability distribution over these alternatives to generate posterior fractional counts for use in downstream estimation. Using these fractional counts in a DOPinspired syntax-based translation system, we show significant improvements in translation quality over a single-best trained baseline. Introduction Modern statistical machine translation systems are becoming more accurate, but also more complex. To cope with increased system complexity, it is convenient to carve systems into modules that can be separately developed, improved, and tested. In this paper, we explore the cost of such modularization on overall system performance by increasing the amount of information that flows between the training modules of one competitive machine translation approach. Specifically, we consider the pipelining of word alignment and syntactic parsing information in the construction of translation rules and the estimation of statistics used to decode with those rules. As Chiang (2005) and Koehn et al. (2003) note, lexical \"phrase-based\" translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on simple distance and lexical distortion models to represent the reordering effects across language pairs. Such models are typically applied over limited source sentence ranges for reasons of model strength (i.e., translation constraints that help prevent errors) and decoding time efficiency (Och and Ney, 2004) . Hierarchically structured models as in Chiang (2005) define weighted transduction rules, interpretable as components of a probablistic synchronous grammar (Aho and Ullmann, 1969) , that represent translation and re-ordering operations. As in monolingual parsing models, such rules make use of nonterminal categories to extend the domain of locality, beyond string-local effects, for resolving ambiguity and making translation decisions. Chiang (2005) uses a single nonterminal category (X), while others use syntactically-motivated nonterminal categories, thus bearing the \"syntax-based\" designation (Galley et al., 2006; Zollmann and Venugopal, 2006) . Chiang (2005) and Venugopal et al. (2007) demonstrate efficient translation with probabilistic synchronous CFGs (hereafter, PSCFGs), and Marcu et al. (2006) and Zollmann et al. (2008) present results that show significant improvements in translation quality over a phrase based system on languages where long distance re-ordering effects exist. Current phrase-based and hierarchically structured systems rely on the output of a sequential [8th AMTA conference, Hawaii, 21-25 October 2008] \"pipeline\" of maximum a posteriori inference steps to identify hidden translation structure and estimate the parameters of their translation models. The first step in this pipeline typically involves learning word-alignments (Brown et al., 1993) over parallel sentence aligned training data. The outputs of this step are the model's most probable word-to-word correspondences within each parallel sentence pair. These alignments are used as the input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints (Koehn et al., 2003; Och and Ney, 2004) . Thus, errors made within the singlebest alignment are propagated (1) to the identification of phrases, since errors in the alignment affect which phrases are extracted, and (2) to the estimation of phrase weights, since each extracted phrase is counted as evidence for relative frequency estimates. Methods like those described in Wu (1997) , Marcu and Wong (2002), and DeNero et al. (2006) address this problem by jointly modeling alignment and phrase identification, yet have not achieved the same empirical results as surface heuristic based methods, or require substantially more computational effort to train. In this work we describe an approach that \"widens\" the pipeline, rather than performing two steps jointly. We present N -best alignments and parses to the downstream phrase extraction algorithm and define a probability distribution over these alternatives to generate expected, possibly fractional counts for the extracted translation rules, under that distribution. These fractional counts are then used when assigning weights to rules. This technique is directly applicable to both flat and hierarchically-structured translation models. In syntax-based translation, single-best target language parse trees (given by a statistical parser) are used to assign syntactic categories within each rule, and to constrain the combination of those rules. Decisions made during the parsing step of the pipeline affect the choice of nonterminals used for each rule in the PSCFG. Presenting N -best parse alternatives to the rule extraction process allows the identification of more diverse structures for use during translation and, perhaps, better generalization ability. We integrated our 'wider-pipeline' model into the PSCFG grammar construction process of the publicly available Syntax-Augmented Machine Translation system (Zollmann and Venugopal, 2006) . We first review PSCFG grammars (Section 2), and then, in Section 3, present a method of integrating PSCFG rules extracted from N -best alignments and parses and allow the posterior fractional counts to influence the rule weights. In Section 4, we show how the widened pipeline improves translation performance on a limited-domain domain speech translation task, the IWSLT Chinese-English data track (Paul, 2006) . 2 Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) T S , a target terminal set (target vocabulary) T T , a shared nonterminal set N and induce rules of the form X \u2192 \u03b3, \u03b1, \u223c, w where \u2022 X \u2208 N is a nonterminal, \u2022 \u03b3 \u2208 (N \u222a T S ) * is a sequence of nonterminals and source terminals, \u2022 \u03b1 \u2208 (N \u222a T T ) * is a sequence of nonterminals and target terminals, \u2022 the count #NT(\u03b3) of nonterminal tokens in \u03b3 is equal to the count #NT(\u03b1) of nonterminal tokens in \u03b1, \u2022 \u223c: {1, . . . , #NT(\u03b3)} \u2192 {1, . . . , #NT(\u03b1)} is a one-to-one mapping from nonterminal tokens in \u03b3 to nonterminal tokens in \u03b1, and \u2022 w \u2208 [0, \u221e) is a nonnegative real-valued weight assigned to the rule. In our notation, we will assume \u223c to be implicitly defined by indexing the NT occurrences in \u03b3 from left to right starting with 1, and by indexing the NT occurrences in \u03b1 by the indices of their corresponding counterparts in \u03b3. Syntax-oriented PSCFG approaches often ignore source structure, instead focusing on generating syntactically well-formed target derivations. Chiang (2005) uses a single nonterminal category, Galley et al. (2006) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advantage of CCG-inspired \"slash\" categories (Steedman, 2000) and also concatenated \"plus\" categories. We now briefly describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006) . Note, however, that this paper's contribution of integrating evidence from N -best alignments and/or parses can be applied to any of the other PSCFG approaches mentioned above in a straight-forward manner. Grammar Construction Zollmann and Venugopal (2006) describe a process to generate a PSCFG given parallel sentence pairs f, e , a parse tree \u03c0 for each e, the maximum a posteriori word alignment a over f, e , and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. Koehn et al. (2003; Och and Ney (2004) . Each phrase in Phrases(a) is first annotated with a syntactic category to produce initial rules, where \u03b3 is set to the source side of the phrase, \u03b1 is set to the target side of the phrase, and X is assigned based on the corresponding target side span in \u03c0. If the target span of the phrase does not match a constituent in \u03c0, heuristics are used to assign categories that correspond to partial rewriting of the tree. These heuristics first consider concatenation operations, forming categories like \"NP+VP\", and then resort to CCG style \"slash\" categories like \"NP/NN.\". The system described in Zollmann and Venugopal (2006) can be used to create a Syntax Augmented grammar as well as a purely hierarchical grammar that uses a single generic nonterminal symbol Chiang (2005) . The Syntax Augmented system also generates a purely hierarchical variant for each syntactic rule that is identified, giving the decoder the option of using labelled or non-labelled rules during translation. These initial rules form the lexical basis for generalized rules that include labeled syntactic categories in \u03b3 and \u03b1. Following the DOPinspired (Scha, 1990) rule generalization technique proposed by Chiang (2005) , one can now generalize each identified rule (initial or already partially generalized) N \u2192 f 1 . . . f m /e 1 . . . e n for which there is an initial rule M \u2192 f i . . . f u /e j . . . e v where 1 \u2264 i < u \u2264 m and 1 \u2264 j < v \u2264 n, to obtain a new rule N \u2192 f 1 . . . f i\u22121 M f u+1 . . . f m / e 1 . . . e j\u22121 M e v+1 . . . e n where the two instances of M are mapped under \u223c. The recursive form of this generalization operation allows the generation of rules with multiple nonterminal symbols. Since we only conside initial phrases up to a fixed length (10 in this work), and only allow a fixed number of nonterminals per rule (2), this operation has a runtime that is polynomial as a function of |Phrases(a)|. Decoding Given a source sentence f , the translation task under a PSCFG grammar can be expressed analogously to monolingual parsing with a CFG. We find the most likely derivation D of the input source sentence while reading off the English translation from this derivation: \u00ea = tgt arg max D:src(D)=f p(D) (1) where tgt(\u2022) maps a derivation to its target yield and src(\u2022) maps a derivation to its source yield. Our distribution p over derivations is defined by a log-linear model. The probability of a derivation D is defined in terms of the rules r that are used in D: p(D) = p LM (tgt(D)) \u03bb LM \u00d7 r\u2208D i \u03c6 i (r) \u03bb i Z(\u03bb) (2 ) where \u03c6 i is a feature function on rules, p LM is a g-gram probability of the target yield tgt(D), and Z(\u03bb) is a normalization constant chosen such that the probabilities sum up to one. 1 The computational challenges of this search task (compounded by the integration of the language model) are addressed elsewhere (Chiang, 2007; Venugopal et al., 2007) . All feature weights \u03bb i are trained in concert with the language model weight \u03bb LM via minimumerror training (MER) (Och, 2003) . Here, we focus on the estimation of the feature values \u03c6 during the grammar construction process. The feature values are statistics estimated from rule counts. Feature Value Statistics The features \u03c6 represent multiple criteria by which the decoding process can judge the quality of each rule and, by extension, each derivation. We include both real-valued and boolean-valued features for each rule. The following probabilistic quantities are estimated and used as feature values: \u2022 p(r| lhs(X)): probability of a rule given its lefthand side category; \u2022 p(r| src(r)): probability of a rule given its source side; \u2022 p(r| tgt(r)): probability of a rule given its target side; \u2022 p(ul(tgt(r))| ul(src(r))): probability of the unlabeled target side of the rule given its unlabeled source side; and \u2022 p(ul(src(r))| ul(tgt(r))): probability of the unlabeled source and target side of the rule given its unlabeled target side. In our notation, lhs returns the left-hand side of a rule, src returns the source side \u03b3, and tgt returns the target side \u03b1 of a rule r. The function ul removes all syntactic labels from its arguments, but retains ordering notation. For example, ul(NP+AUX 1 does not go) = 2 1 does not go. The last two features represent the same kind of relative frequency estimates commonly used in phrase-based systems. The ul function allows us to calculate these estimates for rules with nonterminals as well. To estimate these probabilistic features, we use maximum likelihood estimates based on counts of the rules extracted from the training data. For example, p(r|lhs(r)) is estimated by computing #(r)/#(lhs(r)), aggregating counts from all extracted rules. As in phrase-based translation model estimation, \u03c6 also contains two lexical weights pw (lex(src(r))| lex(tgt(r))) and pw (lex(tgt(r))| lex(src(r))) (Koehn et al., 2003) that are based on the lexical symbols of \u03b3 and \u03b1. These weights are estimated based on an pair of statistical lexicons that represent p(s|t), p(t|s), where s and t are single words in the source and target vocabulary. These word-level translation models are typically estimated by maximum likelihood, considering the word-to-word links from \"single-best\" alignments as evidence. \u03c6 contains several boolean features that indicate whether: (a.) the rule is purely lexical in \u03b1 and \u03b3, (b.) the rule is purely non-lexical in \u03b1 and \u03b3, (c.) the ratio of lexical source and target words in the rule is between 1/5 and 5. \u03c6 also contains a feature that reflects the number of target lexical symbols and a feature that is 1 for each rule, allowing the decoder to prefer shorter (or longer) derivations based on the corresponding weight in \u03bb. N -best Evidence The PSCFG rule extraction procedure described above relies on high quality word alignments and parses. The quality of the alignments affects the set of phrases that can be identified by the heuristics in (Koehn et al., 2003) . Improving or diversifying the set of initial phrases also affects the rules with nonterminals that are identified via the procedure described above. Since PSCFG systems rely on rules with nonterminal symbols to represent reordering operations, the set of these initial phrases has the potential to have a profound impact on translation quality. The quality of the parses affects the syntactic categories assigned to the left-hand-side and nonterminal symbols of each rule. These categories play an important role in constraining the decoding process to grammatically feasible target parse trees. Several recent studies explore the relationship between the quality of the initial models in the \"pipeline\" and final translation quality. Quirk and Corston-Oliver (2006) show improvements in translation quality when the quality of parsing is improved by adding additional training data within the \"treelet\" paradigm introduced by Quirk et al. (2005) . Koehn et al. (2003) show that translation quality in a phrase based system does not vary significantly when increasing the complexity of the model used for alignment (ranging from IBM model 1 through 4), but that increasing the amount of parallel training [8th AMTA conference, Hawaii, 21-25 October 2008] data does improvement alignment quality. Ganchev et al. (2008) demonstrate significant improvements in both alignment quality (as measured by alignment error rate (Och and Ney, 2003) ) and translation quality when using a posterior decoding method to select alignments (as opposed to the single-best Viterbi alignment). Xue et al. (2006) apply nbest alignments to improve phrase-based translation, while Dyer et al. (2008) and Mi et al. (2008) widen the pipeline by considering word-lattice and forestbased translation rather than translating the singlebest hypothesis from a previous stage in the pipeline. Our approach considers alignment and parse quality for a fixed training data size and model complexity. The alignment model and the parser are capable of generating N -best alternative candidates along with corresponding probabilities for each candidate. Informal examination of the highest probability alignment and target parse tree reveals two important arguments in favor of integrating N -best hypotheses into the rule extraction process. Firstly, there are often multiple reasonable alignments and parses that can model the bilingual sentence pair and the target sentence. We can expect that rules extracted from more diverse, correct evidence can improve translation quality on new sentences, since more (good) rules will be extracted. Secondly, where there is a high degree of agreement across each alternative in the N -best lists, the remaining differences between alternatives are often the source of error or ambiguity. Attempts to reduce the use (in decoding) of rules extracted from sections of the alignment and parse that are not consistent with other alternatives could reduce errors made during translation. Put another way, the more complete hypotheses a word-link or constituent appears in, and the more probable those hypotheses, the more we should trust rules that use these links. Our approach toward the integration of N -best evidence into the grammar construction process allows us to take advantage of the diversity found in the N best alternatives, while reducing the negative impact of errors made in these alternatives. Counting from N -Best Lists In this work we propose extraction of complex rules over N -best alignments and N -best parses, mak-ing use of probability distributions over these alternatives to assign fractional posterior counts to each extracted rule. Taking the alignment N -best list to define a posterior distribution over alignments and the parse Nbest list to define a posterior over parse trees, we can estimate the posterior probability of each rule that might be extracted for each (alignment, tree) pair. Assuming that the alignment module gives alignments a 1 , ..., a N , with posterior probabilities p(a 1 | e, f ), ..., p(a N | e, f ), we approximate the posterior by renormalizing: p(a i ) = p(a i | e, f ) N j=1 p(a j | e, f ) (3) The same is applied to the parser's N -best parses, \u03c0 1 , ..., \u03c0 N . Given a single alignment-parse pair, we can extract rules as described in Section 2.1. Our approach is to extract rules from the cross-product {a 1 , ..., a N } \u00d7 {\u03c0 1 , ..., \u03c0 N }, incrementing the partial count of each rule extracted by p(a i ) \u2022 p(\u03c0 j ). A rule r's total count for the sentence pair f, e is: N i=1 N j=1 p(a i )\u2022p(\u03c0 j )\u2022 \uf8f1 \uf8f2 \uf8f3 1 if r can be extracted from e, f , a i , \u03c0 j 0 otherwise (4) In practice, this can be computed more efficiently through structure-sharing. Note that if N = N = 1, this counting method generalizes the original counting method. Note that GIZA++ (Och and Ney, 2003) can infer the N -best word alignments under IBM Model 4 and the Charniak parser (Charniak, 2000) outputs its Nbest parses, with their associated probabilities. Instead of using the simple counts for rules given the derivation inferred using the maximum a posteriori estimated alignment and parse (a 1 , \u03c0 1 ), we now use the expected counts under the approximate posterior. These posteriors encode (in a principled way) a measurement of confidence in substructures used to generate each rule. Possible rule instances supported by more and more likely alignments and parses should, intuitively, receive higher counts (approaching 1 as certainty increases, supported by more and higher-probability alternatives), while rule [8th AMTA conference, Hawaii, 21-25 October 2008] instances that rely on low probability or fewer alignments and parses will get lower counts (approaching 0 as certainty increases). Refined Alignments Work by Och and Ney (2004) and Koehn et al. (2003) demonstrates the value of generating word alignments in both source-to-target and target-tosource directions in order to facilitate the extraction of phrases with many-to-many word relationships. We follow Koehn et al. (2003) in generating a refined bidirectional alignment using the heuristic algorithm \"grow-diag-final-and\" described in that work. Since we require N -best alignments, we first extract N -best alignments in each direction, and then perform the refinement technique to all N 2 bidirectional alignment pairs. The resulting alignments are assigned the probability (p f .p r ) \u03b1 where p f is the candidate probabilty for the forward alignment and p r is the candidate probability to the reverse alignment. We then remove any duplicate refined alignments (the refined alignment with the highest probability is retained) that came about due to the refinement process. Finally, we select the top N alignments from this set of refined alignments. The selection of \u03b1 controls the entropy of the resulting distribution over candidate alignments (after normalization). Higher values of \u03b1 > 1 make the distribution more peaked (affecting the estimation of features on rules from these alignments), while values of 0 \u2264 \u03b1 < 1 make the distribution more uniform. A more peaked distribution favors rules from the top alignments, while a more uniform one gives rules from lower performing aligments more of a chance to participate in translation. We can also use this same technique to control the distribution over parses. Translation Results Experimental Setup We present results on the IWSLT 2007 and 2008 Chinese-to-English translation task, based on the full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2M target words) as well as the evaluation corpora from the evaluation years preceding 2007. The develop-ment data consists of 489 sentences (average length of 10.6 words) from the 2006 evaluation, the 2007 test set contains 489 sentence (average length of 6.47 words) sentences and the 2008 test set contains 507 sentences (average length of 5.59 words). Word alignment was trained using the GIZA++ toolkit, and N -best parses generated by the Charniak (2000) parser, without additional re-ranking. 2 N -best alignments were generated from source to target and target to source, refined as described above. Initial phrases of up to length 10 were identified using the heuristics proposed by Koehn et al. (2003) . Rules with up to 2 nonterminals are extracted using the SAMT toolkit (Zollmann and Venugopal, 2006) , modified to handle N -best alignments and parses and posterior counting. Note that lexical weights (Koehn et al., 2003) as described above are assigned to \u03c6 based on \"single-best\" word alignments. Rules that receive zero probability value for their lexical weights are immediately discarded, since they would then have a prohibitively high cost when used during translation. Rules extracted from single-best evidence as well as N best evidence can be discarded in this way. The n-gram language model is trained on the target side of the parallel training corpus 3 and translation experiments use the decoder and MER trainer available in the same toolkit. We use the cubepruning option (Chiang, 2007) in these experiments. Cumulative (N, N )-Best We measure translation quality using the mixedcased IBM-BLEU (Papineni et al., 2002 ) metric as we vary the size of N and N for alignments and parses respectively. Each value of N implies that the first N alternatives have been considered when building the grammar. For each grammar we also track the number of rules relevant for the first sentence in the IWSLT 2007 test set (grammars are subsampled on a per-sentence basis to keep memory requirements low during decoding). We also note the number of seconds required to translate each test set. Due to time and resource constraints we limit our evaluation to varying the number of alignments and parses separately, and we limit N to 10 (due to the significant increase in decoding time that results from adding more nonterminal labels to the grammar). As noted above, many rules extracted based on N -best alignments cannot participate in the decoding process because lexical weight features can have costs of infinity if the underlying word based models p(s|t) and p(t|s), estimated based on \"singlebest\" alignments, yield zero probabilities. Smoothing these models alleviates the problem, but does not fix it at its root. In the spirit of softening our pipelined decisions, we create lexical weight features based on the IBM Model 4 tables output by GIZA++ at the end of its training, instead of singlebest alignment relative frequencies. Using these IBM Model 4 weights allows a larger number of rules to be added to the grammar since more rules have non-zero lexical weights. We also investigate the impact of the shape Nbest probability distribution used to estimate features \u03c6 by varying \u03b1. N -best alignments. Table 1 shows translation results on the IWSLT translation task for the Development (IWSLT 2006) and two test corpora (IWSLT 2007 and 2008) using the Syntax Augmented grammar. In this table we vary the number of alternative alignments, consider first-best (1), 5, 10 and 50 best alternatives. We also experiment with lexical weights from the first-best alignment (lex = 1st) and directly from IBM Model 4 (lex = m4), while \u03b1 controls the entropy of the normalized distribution over alternative alignments. For the Syntax-Augmented grammar, using lex = m4 slightly increases the number of rules in the grammar, but only adds benefit for the 2007 test set. We continue to use lex = m4 for the remaining experiments since we do not want to discard rules based on the lexical weights. Increasing N = 1 to N = 5 brings significant improvements in translation quality on all 3 evaluation corpora, while increasing N further to N = 10 and N = 50 retain the improvements but at the cost of a significantly larger grammar and decoding times. Varying \u03b1 to modify the entropy of the alignment distribution does not seem to have a consistent impact on translation quality; some test sets show improvements while others suffer. N -best alignments (hierarchical grammar). Similar results with the purely hierarchical grammar are shown in Table 2 . We see clear improvements when moving to N = 5, and even further small improvement up to N = 10, but a slight degradation going further to N = 50. Again, we do not see a clear benefit from varying \u03b1. Surprisingly, while Dev. scores are significantly lower with the purely hierarchical grammar compared to the Syntax Augmented grammar, unseen test set scores are very similar, and achieved at significantly lower decoding times. Since the number of features in \u03c6 are very similar for both models, it is unlikely that this discrepancy is solely due to overfitting during MER training. It is more likely that this discrepancy is related to the relative lengths of each evaluation corpus. The development corpus contains longer sentences on average than the evaluation corpora. The number of rules used in purely hierarchical grammar is significantly lower than in the Syntax Augmented grammar, and increasing N does not exhibit the same growth in the number of rules either. The Syntax Augmented grammar grows much faster since rule identified from alternative alignment candiates have syntactic nonterminal symbols and are less likely to be duplicates of already identified rules. N -best parses. Table 3 summarizes results when varying the number of alternative parses. These experiments use \u03b1 = 1, lex = m4 and 1-best alignments only. We also additionally track the number of nonterminal labels represented in the grammar. Using additional evidence from N -best parses seems to have a overall slight negative impact on translation quality while taking significantly longer to perform decoding. The growth in the number of nonterminal labels and as a consequence the number of rules has a dramatic impact on decoding time and likely contributes to additional search errors. The one corpus where alternative parses (N = 10) produces results comparable to using N best alignments is IWSLT 2008, which is also the corpus with the shortest sentences on average, thus reducing the potential impact of search error. Grammar Rules Figure 1 shows the most frequently occurring rules that exist only in the best performing N = 10, N = 1 grammar, and not in the baseline (Model-4 lexicon) grammar. We show the estimated counts on these rules as well as their source, target and lefthand-side nonterminal symbol. These rules are particularly interesting when considering the domain of this translation task. The source side of the training data contains no punctuation (since it is transcribed speech), while the target side does (since they were manually generated translations). The system therefore attempts to generate punctuation during translation. Consider the first example, where the Chinese word for \"please\" (often found at the beginning of a sentence) is aligned to the English \"please .\" (at the end of the sentence as indicated by the punctuation). This rule is extracted from a lower-probability alignment with high levels of distortion. This pattern was not seen in any single-best alignments. Conclusion In this work we have demonstrated the feasibility and benefits of widening the MT pipeline to include additional evidence from N -best alignments and parses. We integrate this diverse knowledge under a principled model that uses a probability distribution over these alternatives. We achieve significant improvements in translation quality over grammars built on \"single-best\" evidence alone when considering N -best alignments, while N -best parses seem to have no impact on translation quality. Using a relatively small number of additional alternative alignments results in significant improvements in quality, with minimal impact on the number of rules in the grammar and the translation runtime for a hierarchical system, but at significantly increased grammar size and runtime for a syntax-augmented system. In future work we plan to focus on methods to take better advantage of the syntactic labels from alternative parse candidates. Acknowledgments This work has been partly funded by GALE HR0011-06-2-0001. N. Smith is supported by NSF IIS-0836431 and an IBM faculty award.",
    "abstract": "State-of-the-art statistical machine translation systems use hypotheses from several maximum a posteriori inference steps, including word alignments and parse trees, to identify translational structure and estimate the parameters of translation models. While this approach leads to a modular pipeline of independently developed components, errors made in these \"single-best\" hypotheses can propagate to downstream estimation steps that treat these inputs as clean, trustworthy training data. In this work we integrate N -best alignments and parses by using a probability distribution over these alternatives to generate posterior fractional counts for use in downstream estimation. Using these fractional counts in a DOPinspired syntax-based translation system, we show significant improvements in translation quality over a single-best trained baseline.",
    "countries": [
        "United States"
    ],
    "languages": [
        "English",
        "Chinese"
    ],
    "numcitedby": "38",
    "year": "2008",
    "month": "October 21-25",
    "title": "Wider Pipelines: N-Best Alignments and Parses in {MT} Training"
}