{
    "article": "We present our Charles-UPF submission for the Shared Task on Evaluating Accuracy in Generated Texts at INLG 2021. Our system can detect the errors automatically using a combination of a rule-based natural language generation (NLG) system and pretrained language models (LMs). We first utilize a rulebased NLG system to generate sentences with facts that can be derived from the input. For each sentence we evaluate, we select a subset of facts which are relevant by measuring semantic similarity to the sentence in question. Finally, we finetune a pretrained language model on annotated data along with the relevant facts for fine-grained error detection. On the test set, we achieve 69% recall and 75% precision with a model trained on a mixture of human-annotated and synthetic data. Introduction Recent neural NLG systems can easily generate fluent texts from linearized structured data (Zhao et al., 2020; Kale and Rastogi, 2020; Castro Ferreira et al., 2020) . However, the systems cannot guarantee that the output is properly grounded in the input -hallucination (outputs not supported by input data) is a notorious problem in neural NLG (Tian et al., 2019; Harkous et al., 2020; Filippova, 2020; Rebuffel et al., 2021) . Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017) , where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Du\u0161ek et al., 2020; Castro Ferreira et al., 2020) . This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007) , reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019) . Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Du\u0161ek and Kasner (2020) 's metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Du\u0161ek et al., 2019) , which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a) . The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1 ) defined in Thomson and Reiter (2020b) . The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task falls into the first category: we developed an automatic metric for token-level error annotation which combines a  rule-based generation system with a neural retrieval model and a pretrained neural LM used for error tagging. We evaluated our approach in a crossvalidation scenario to select the best configuration for the shared task. Overall, our system is able to reach 65% error detection F1 score and ranked first out of four automatic submissions in the shared task. The code for our experiments is freely available on Github. 1 Our System Our system is composed of 3 steps: A rule-based generator for fact descriptions (see Figure 1 and Section 2.1), a retrieval system for selecting facts relevant for a given sentence (Section 2.2), and a token-level error tagger based on the RoBERTa pretrained LM (Section 2.3). The latter two steps are summarized in Figure 2 . The LM tagger is trained on examples provided by shared task organizers, as well as on synthetic data based on the Rotowire training set (Section 2.4). Rule-based Fact Descriptions We use rule-based systems to generate natural language descriptions of facts from the input tables, relating to all players and both teams. The facts are later supplied to the error-checking model for grounding the evaluated sentence (see Section 2.3). We experiment with both simple descriptions created by filling in sentence templates, and compact descriptions generated using a grammar-based system. The simple system produces about 569 facts/sentences for each game. The compact system generates about 112 sentences per game, i.e., 5 times less; the game descriptions contain the same amount of information but the individual sentences are more syntactically complex. Facts generated For each game, we first generate every fact in the input table, i.e., 44 facts about the game (hosting team, visiting team, date converted to weekday) and so-called line-score objects 1 https://github.com/kasnerz/ accuracySharedTask_CUNI-UPF (team name and statistics) and box-score objects (player name, player team, player starting position and their personal statistics). Subsequently, we generate 85 further facts that can be inferred from the input table. These are based on reading the first 20 human-written summaries in the training data and finding frequently mentioned facts that can easily be derived from input, such as which team won and by how much, comparisons between the team and player raw data (e.g., Team A dominated the defensive rebounding, Team A and Team B committed the same number of fouls; Player X was the (second) best scorer of the game/his team), complex statistics (e.g., Team A totaled X steals; Player X (almost) recorded a double-double), or an interpretation of some numbers (e.g., Team A came back in the 4th quarter; Team A was efficient/bad at shooting). 2 Simple descriptions are produced by a templatebased system, with one template per fact. We handcrafted 129 sentence templates to cover all the facts described above. A sentence template looks like the following: \"[PLAYER_NAME] scored [PTS] points.\", where square brackets indicate variables that are instantiated with the corresponding input values (see Figure 1 for sample sentences). Compact descriptions are produced by the FORGe system (Mille et al., 2019) , which allows for the generation of more compact sentences by instantiating abstract (predicate-argument) templates instead of full sentences for each fact. For instance, the template for the points scored would be: [PLAYER_NAME] \u2190A1 provide A2\u2192 point NonCore\u2192 [PTS] , where A1 and A2 denote the first and second arguments respectively, and Non-Core a non-argumental relation. FORGe receives a series of instantiated templates and performs surface realization in several steps, by first aggregating the templates based on predicate and/or argument identity, and then structuring, linearizing and inflecting components of the sentences. The FORGe grammars were used off-the-shelf, 3 with cross-sentence referring expression generation deactivated so that each generated sentence can be used on its own. We manually crafted 98 abstract templates and added a description of the included  Figure 2 : An overview of our system. First, we generate the facts from the input table with a rule-based NLG system (see Figure 1 ). For each evaluated sentence s, we select c facts with the highest semantic similarity, getting a context C. The pair (C, s) is given as an input to a pretrained LM for token-level error classification. lexical units into the FORGe lexicon. For instance, the five simple sentences shown at the bottom of the yellow column in Figure 1 are covered by a single compact sentence shown at the bottom of the orange column. Context Retrieval Since the maximum length of the input sequence for our error-checking model (see Section 2.3) is 512 tokens (about 10% of the total length of the generated sentences G), we need to select only a subset of G, which we refer to as context C. We want to put the relevant sentences into C and filter out the rest to make the error tagging easier. This problem is hard in general, as any string matching (e.g. using numbers or names mentioned in the sentence) will fail on lexical variations. Our solution is based on selecting sentences with the highest semantic similarity. For each generated sentence g i \u2208 G, we measure semantic similarity between g i and the evaluated sentence s using Sentence Transformers (Reimers and Gurevych, 2019) . 4 In particular, we embed the sentence tokens by applying mean pooling on the output of paraphrase-distilroberta-base-v2, getting the embedding vectors e s and e g i . Then we compute the cosine similarity between the embeddings. For the context C, we select the top c sentences from G that have the highest cosine similarity to s. LM-based Error Tagger We use a RoBERTa LM (Liu et al., 2019) with a token-level classification head as our errorchecking model. Unlike unsupervised approaches based on examining attention values (Thorne et al., 2019; Li et al., 2020) or input perturbations (Kim et al., 2020) , we train the model directly to predict error categories using annotated data, similarly to Yoosuf and Yang (2019) . The model receives an input X = (C, s), composed of the context C, i.e., relevant background facts selected by context retrieval in Section 2.2, and the generated sentence s to be tagged. The inputs are separated by the delimiter </s>. The model is trained to annotate each token in s either with an OK label, or with a label corresponding to one of the error categories. We experiment with two data sources for training the model: (1) gold-standard annotated data from the shared task (which contains all error types), ( 2 ) synthetic data created by perturbing the human-written summaries from Rotowire (which contains only NAME and NUMBER errors; see Section 2.4 for details). Synthetic Data The gold-standard data contains only 60 games, as opposed to 3,395 games in the Rotowire training set. This led us to an idea of using the training set as a source of synthetic data for our model. We create the synthetic data by introducing errors into human-written descriptions. We focus only on the NAME and NUMBER errors-the categories which are the most represented and also easiest to generate. In each sentence, we identify named entities in the text using spaCy. 5 We modify only certain portion of entities according to the entity modification rate, which we treat as a hyperparameter. We introduce the NAME errors by: (1) swapping the names of teams with opponent teams, (2) swapping the names of players with other players in the game, (3) swapping the names of cities with other cities in the Rotowire dataset, (4) modifying the days of the week. For NUMBER errors, we take an integer n identified in the text, sample a number from a normal distribution with \u00b5 = n and \u03c3 = 3, and truncate 5 https://spacy.io it to get the integer. We re-sample if the output equals the original number, or for negative outputs. If the number is spelled out, we use text2num 6 and num2words 7 to convert to digits and back. Experiments We train a PyTorch version of RoBERTa from the Huggingface Transfomers repository (Wolf et al., 2019) using the AdamW optimizer (Loshchilov and Hutter, 2019) , learning rate 5 \u00d7 10 \u22125 and linear warmup. We finetune the model for 10 epochs and select the model with the highest validation score. We experiment with several hyperparameters: (a) simple vs. compact sentences in G, (b) number of sentences retrieved for the context: c = 5, 10, 20 or 40; (c) entity modification rate (EMR): proportion of entities which are modified in the synthetic data: 0.25, 0.5, or 0.75. We evaluate the model using a script provided by the organizers, which computes recall and precision of the model output with respect to the human-annotated data. Since we use the humanannotated data for training, we perform 6-fold cross-validation: in each run, we use 45 games for training, 5 games for validation, and 10 games for evaluation. The results of our model on the development data are listed in we selected the model with the best F1-score overall, which is 0.65 (61% recall and 69% precision). The model uses 40 compact sentences in context, 0.25 EMR and was trained on both synthetic and human-annotated data. However, note that the hyperparameters of the best models are quite varied. Although compact texts are generally helpful, there are also some well-performing models using simple templates only. A higher number of sentences in context may help to achieve better F1-score, but not always (the longer context is also sometimes cropped to fit the input). Using a higher EMR then generally leads to higher recall, suggesting that the model adapts to the base rate of errors. Results of our Charles-UPF submission Table 3 shows the results of our model on the official test data of the task, broken down by error types. The overall scores are higher than on the development set -test set recall is 0.691 (vs. 0.614 on the development set) and precision is 0.756 (vs. 0.690). The fact that we used the whole available human annotated data for training the final model may have contributed to the difference, but it is also possible that the test data was somewhat less challenging. We note that our model was able to identify only three types of errors (NAME, NUM-BER, WORD), having better results for the NAME and NUMBER errors. We believe the explanation is two-fold: the names and numbers are often found verbatim in the input data (and in our generated facts), which makes them easy to detect, and also the corresponding error types were the most represented in the training data. In contrast, the three error types which were not detected are much less represented in the training data and hard to detect in our setup. model trained only on annotated data. The results were overall in the 0.3-0.5 range for both recall and precision, and no model was the best-performing one in terms of any metric. Discussion Our Charles-UPF submission achieved the best results in the automatic metrics category, but there is still a gap with what humans can achieve, as shown by the Laval University submission's (Garneau and Lamontagne, 2021) overall 0.841 recall and 0.879 precision. One way to improve our system would be to enrich the reference fact descriptions, by either inferring more information from the raw data, or by extracting additional data from external databases. 2 Another option would be to add surrounding sentences to the context -this could help to resolve coreferences (e.g., if a player is referred to as \"He\") and to detect the CONTEXT errors. We also note that our approach requires the real system outputs manually annotated with errors in order to work well -using only synthetic data results in low recall (see Table 2 ). However, we believe that more sophisticated techniques for creating the synthetic data could help to achieve same results with less human-annotated data. We also believe that our system is in general applicable to new games or seasons. The rule-based generator does not need any adapting, the vocabulary of both neural parts (context selector and error tagger) is based on subwords and thus also able to handle unseen player/team/city names. The model effectively learns to compare entities from the context and the evaluated sentence, the absolute values are thus less important than their agreements and differences. Conclusion We presented our system for detecting errors in generated descriptions of basketball matches. Our system can automatically classify the errors on token level, using a pretrained language model and textual description of data generated by a rule-based NLG system. Our system reached 0.691 recall and 0.756 precision on the test data, finishing first out of four automatic metric submissions in the INLG 2021 Accuracy Evaluation shared task. Acknowledgements This work was supported by the Charles University projects GAUK 140320, SVV 260575, and PRIMUS/19/SCI/10, an Apple NLU Research Grant for Heriot-Watt University and Charles University, and by the European Commission via UPF under the H2020 program contract numbers 786731, 825079, 870930 and 952133.",
    "funding": {
        "defense": 1.9361263126072004e-07,
        "corporate": 1.0,
        "research agency": 0.9999440206399028,
        "foundation": 0.03392036183199665,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section of the article mentions support from Charles University projects (GAUK 140320, SVV 260575, and PRIMUS/19/SCI/10), an Apple NLU Research Grant for Heriot-Watt University and Charles University, and the European Commission via UPF under the H2020 program contract numbers 786731, 825079, 870930, and 952133. This indicates funding from a corporate source (Apple), and research agency or foundation sources (European Commission and Charles University projects, with the European Commission being a government-funded organization and Charles University projects possibly being internal funding mechanisms that could be considered research agency or foundation in nature, depending on their funding source)."
}