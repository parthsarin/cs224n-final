{
    "article": "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications. 1 Introduction Neural language models have shown impressive performance across many applications such as machine translation and summarization where the text is generated from scratch (Bahdanau et al., 2014; Rush et al., 2015) . However, a broader set of text generation tasks -including text editing, information fusion, and ancient text restoration -requires the model to start with partially specified text and generate the missing fragments. In the general setup, the input document may have any number of missing spans, and each span may have an unknown number of missing tokens. To perform this text infilling task (Zhu et al., 2019) , a model should: (1) provide fine-grained control over the generation location, (2) accommodate a variable number of missing tokens, and (3) respect both the preceding and following context. They also have which . They also have ice cream which is really good . Existing approaches focus on adapting left-toright language models for text infilling. Intricate inference algorithms leveraging dynamic programming or gradient search are proposed to find the filling content that has a high likelihood within the surrounding context (Sun et al., 2017; Liu et al., 2019a; Zaidi et al., 2020) . These methods make simplified Markov assumptions, require high decoding time complexity, and cannot adapt to variable infilling length. Alternatively, Donahue et al. (2020) predict the concatenation of the infilling content, but do not guarantee that the output will match the number of missing spans in the input. In this work, we introduce the Blank Language Model (BLM), which uses a special \" \" symbol to control where tokens can be placed. The generation of BLM follows the grammar of replacing a blank with a word and possibly adjoining blanks. By jointly modeling context and missing content, BLM supports the control of generation location and produces consistent infilling of variable length. Our model can start from a single blank or partial text with blanks in specified locations. It maps the entire input into a sequence of vector representations, and further processes the representations in blank positions to determine the generation action. Generation actions are performed iteratively until there are no blanks. Since multiple trajectories of BLM actions can produce the same final text, we train the model by maximizing a lower bound of the log-likelihood marginalized over trajectories. At test time, we can use simple greedy decoding or beam search to fill in the blanks in the input text. BLM shows superior performance in text infilling (Zhu et al., 2019) , ancient text restoration (As- 0. #1 #1 is Y Y 1. #1 is #2 #1 customer N Y 2. customer #1 is #2 #2 awesome N N 3. customer #1 is awesome #1 service N N 4. customer service is awesome -End-Figure 2 : An example trajectory that generates the sentence \"customer service is awesome\". Each action is a tuple (b, w, l, r), indicating the blank location b selected for expansion, the word w to fill in, whether to create a left blank l, and whether to create a right blank r. sael et al., 2019) and style transfer (Shen et al., 2017) , demonstrating its flexibility to generate text in diverse conditions. Our model achieves 92.5% accuracy and BLEU score of 23.1 on the Amazon dataset for sentiment transfer. On the task of restoring ancient text that lost half of the characters, we reduce the error rate by 3.3 points compared to previous methods. Related Work Recent work has explored various sequence models for non-autoregressive machine translation (Gu et al., 2017) . The Insertion Transformer supports dynamic canvas with word insertion (Stern et al., 2019) , but does not allow users to specify where to insert. The model is unaware of which parts of the canvas are contiguous text spans that should remain intact, and which (potentially scattered) parts need to be filled in. Directly forcing the Insertion Transformer to perform text infilling can therefore lead to suboptimal solutions. The Levenshtein Transformer combines insertion and deletion through complex policy learning (Gu et al., 2019b) . Its insertion mechanism is a two-stage process in which placeholders are first predicted and then filled-in in a masked language model (MLM) manner. In text infilling where the blanks/placeholders are given, it reduces to an MLM. MLMs are commonly used in representation learning (Devlin et al., 2018; Joshi et al., 2020) . To use them in rewriting tasks, one needs to specify the insertion length in advance and heuristically determine the generation order among the masks (Fedus et al., 2018; Wang and Cho, 2019; Ghazvininejad et al., 2019) . Similarly, XL-Net requires absolute positional embedding and thus does not support unknown-length text infilling (Yang et al., 2019; Shih et al., 2019) . BLM provides a natural formulation for generative modeling that can dynamically accommodate insertions of various length. Another line of work focuses on finding an optimal language generation order, such as syntaxbased generation (Dyer et al., 2016) and learning adaptive generation order (Gu et al., 2019a) . These approaches are tailored to generation from scratch in a specific order. Our model instead is attuned for text rewriting, where the missing parts can be located anywhere in the input text, and the algorithm must flexibly complete them. Blank Language Models A blank language model (BLM) generates sequences by creating and filling in blanks. Generation starts with a single blank and ends when there is no blank. In each step, the model selects a blank \" \", predicts a word w, and fills the blank with \"w\", \" w\", \"w \", or \" w \". This way, a blank can be expanded to any number of words. We define a canvas as a sequence of words interspersed with special \" \" tokens. The subsequent action is conditioned on this intermediate stage of generation. Suppose the current canvas is c = (c 1 , \u2022 \u2022 \u2022 , c n ) with blanks located at indices b 1 , \u2022 \u2022 \u2022 , b k (i.e. c b i = \" \", for i = 1, . . . , k). BLM maps this canvas to a distribution over actions specifying how the canvas is to be revised: p(b, w, l, r|c; \u03b8) = BLM(c) (1) where b \u2208 {b 1 , \u2022 \u2022 \u2022 , b k } is a blank location; w is a word in the vocabulary V ; l, r \u2208 {0, 1} denote whether or not to create a blank to the left and right of w; and \u03b8 are the model parameters. The action, defined as the tuple (b, w, l, r) uniquely specifies the next state of canvas (see Fig. 2 for illustration). Alternatively, we can view the actions in BLM as production rules in a grammar. Each blank represents a nonterminal symbol or the start symbol, In the first stage, an index is chosen among all current blank positions. For that location, a word is selected in the second stage. In the final stage, the blank representation is concatenated with the chosen word's embedding and fed into an MLP to determine the creation of the following blanks. and the terminal symbols come from the vocabulary V . The production rules are restricted to be of the form \" \" \u2192 \" ?w ?\" for w \u2208 V , where \"?\" indicates that the preceding symbol is optional. In contrast to context-free grammars, the probability distribution over production rules in BLM is conditioned on the entire canvas generated so far. Model Architecture We encode the canvas c into a sequence of representations (z 1 , \u2022 \u2022 \u2022 , z n ), and take representations Z = (z b 1 , \u2022 \u2022 \u2022 , z b k ) where the blanks are located. Let d denote the dimension of z's. We factorize the joint distribution p(b, w, l, r|c; \u03b8) into three parts (shown in Fig. 3 ): 1. Choose a blank: p(b i |c; \u03b8) = Softmax(u T Z) (2) where u \u2208 R d is a parameter vector to project z's into one-dimensional logits. 2. Predict a word for the selected blank: p(w|c, b i ; \u03b8) = Softmax(W z b i ) (3) where W \u2208 R |V |\u00d7d is a parameter matrix to project z b i into the vocabulary. 3. Decide whether or not to create blanks to the left and right of the predicted word: p(l, r|c, b i , w; \u03b8) = MLP(z b i , v w ) (4) where v w is the word vector of w, and MLP is a multilayer perceptron with 4 output classes: Left.Yes/No \u00d7 Right.Yes/No. Likelihood Now let us consider the probability p(x; \u03b8) of generating a sentence/paragraph x = (x 1 , \u2022 \u2022 \u2022 , x n ) under the BLM. We call the generating process from an initial blank to complete text a trajectory. The same final text x can be realized by multiple trajectories. However, if we specify the order in which the words in x are generated, the trajectory will be uniquely determined. Consider the example trajectory of a 4-word sentence in Fig. 2 . Given the order (3, 1, 4, 2), at step 0 when we generate x 3 , both left and right blanks are created for future generations of x 1 and x 2 , x 4 . In step 1 of generating x 1 , only a right blank is created for the future x 2 . Subsequent steps can be deduced by analogy. The correspondence between trajectories and generation orders allows us to write the marginal likelihood as: p(x; \u03b8) = \u03c3\u2208Sn p(x, \u03c3; \u03b8) = \u03c3\u2208Sn n\u22121 t=0 p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8) (5) where S n is the set of all n-permutations; a x,\u03c3 t , c x,\u03c3 t denote the action and canvas at step t under sentence x and order \u03c3, respectively (cf. Fig. 2 ). Training Different losses have been proposed to train generalized sequence models. For instance, BERT and XL-Net mask and predict 15% of tokens conditioned on the rest. This strategy is more suitable for representation learning rather than generation. Insertion Transformer masks different numbers of tokens and weights them with uniform loss or binary tree loss (Stern et al., 2019; Chan et al. , Algorithm 1 BLM training 2 1: Initialize model parameters \u03b8 2: repeat 3: Sample a training example x = (x 1 , \u2022 \u2022 \u2022 , xn) 4: Sample t from 0 to n \u2212 1 5: Sample an n-permutation \u03c3 6: Construct canvas c that keeps tokens x\u03c3 j (j = 1, \u2022 \u2022 \u2022 , t) and collapses remaining tokens as blanks 7: Get n \u2212 t target actions a j\u2212t for filling x\u03c3 j (j = t + 1, \u2022 \u2022 \u2022 , n) into canvas c 8: Compute loss({a 1 , \u2022 \u2022 \u2022 , a n\u2212t }, model.forward(c)) from Eq. ( 8 ) 9: Update \u03b8 by gradient descent 10: until Convergence 2019). It aims to perform fast inference through parallel decoding. Here, we present a training objective from the language modeling perspective by estimating the log likelihood of generating x. Directly computing the marginal likelihood over n! orders is intractable. We apply Jensen's inequality to lower bound the log likelihood: log p(x; \u03b8) = log \u03c3\u2208Sn n\u22121 t=0 p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8) \u2265 log(n!) + 1 n! \u03c3\u2208Sn n\u22121 t=0 log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8) (6) where equality holds when the posterior p(\u03c3|x; \u03b8) is uniform. By maximizing this lower bound, we do not favor any particular order, but encourage the model to realize x equally well in all orders. It can help the model to complete any partial input text regardless of the position of blanks. A naive training algorithm is to directly estimate the lower bound in Eq. ( 6 ): first uniformly sample a permutation \u03c3 from S n and a step t from 0 to n \u2212 1, then construct the canvas c x,\u03c3 t , and compute the estimated loss [\u2212 log(n!) \u2212 n \u2022 log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8)]. However, this procedure has a large variance and can only compute the loss of a single action in one pass (in contrast to left-to-right language models that compute n word losses per pass). To train the model more efficiently, we note that the canvas c x,\u03c3 t depends only on the first t elements of \u03c3. Hence we can combine into one pass the loss calculations of trajectories that are the same in the first t steps but different at the t + 1 step. Switching 2 We implement a batch version of the algorithm. They also have which . They also have ice cream which is really good . \u03c4\u03b5 \u03b5\u03b3\u03b3\u03bf\u03bd\u03bf\u03bd \u03b5\u03b9\u03c3\u03b1\u03b9? ? ? ? ? ? ?\u03c3\u03bf\u03d5\u03b9\u03b1\u03b9 \u03c4\u03b5 \u03b5\u03b3\u03b3\u03bf\u03bd\u03bf\u03bd \u03b5\u03b9\u03c3\u03b1\u03b9\u03bf\u03c5 \u03c4\u03bf\u03c5 \u03c3\u03bf\u03d5\u03b9\u03b1\u03b9 The employees were super nice and efficient ! The employees were rude and unprofessional ! the summation order of \u03c3 and t, we have: n\u22121 t=0 1 n! \u03c3\u2208Sn log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8) = n \u2022 E t E \u03c3 1:t E \u03c3 t+1 E \u03c3 t+2:n [log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8)] = n \u2022 E t E \u03c3 1:t E \u03c3 t+1 [log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8)] = E t E \u03c3 1:t \uf8ee \uf8f0 n n \u2212 t \u03c3 t+1 log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8) \uf8f9 \uf8fb (7) which leads to our efficient training algorithm: sample t from 0 to n \u2212 1 and partial permutation \u03c3 1:t , construct the canvas c x,\u03c3 t , and compute loss: \u2212 log(n!) \u2212 n n \u2212 t \u03c3 t+1 log p(a x,\u03c3 t |c x,\u03c3 t ; \u03b8) (8) The whole process is illustrated in Algorithm 1. In this way, we can compute in expectation n/2 action losses per pass. Experiments We test BLM's capacity to rewrite specified portions of text on three tasks: text infilling (Zhu et al., 2019) , ancient text restoration (Assael et al., 2019) and style transfer (Shen et al., 2017) . Fig. 4 displays example inputs and outputs for these tasks. We also measure the perplexity of BLM on language modeling benchmarks and compare with traditional left-to-right language models. Experimental Details In all experiments, the sequence representations in BLM are obtained using the encoder module of transformer base (Vaswani et al., 2017) observed on the validation set. We note that beam search in BLM does not search for the sentence with the maximum marginal likelihood p(x; \u03b8), but instead for a sentence and a trajectory that have the maximum joint likelihood p(x, \u03c3; \u03b8). Text Infilling Dataset We experiment on the Yahoo Answers dataset, which has 100K/10K/10K documents for train/valid/test respectively (Yang et al., 2017) . A document has a maximum length of 200 words, with an average of 78 words. Following Zhu et al. (2019) , we automatically compile test data by deleting portions of documents. For each document x, we randomly mask a given ratio r of its tokens. Contiguous masked tokens are collapsed into a single \" \", resulting in a canvas c to be completed. Metrics We measure generation's accuracy by computing its BLEU score against the original document x, and fluency as its perplexity evaluated by a pre-trained (left-to-right) language model. We also report the failure rate, which is the percentage of invalid generations, such as missing existing words or not filling in all the blanks. Baselines We compare BLM with five baselines: \u2022 Insertion Transformer (InsT): By default, InsT does not support controlling the insertion position. We force it to produce valid generations by normalizing the predictions over valid locations, disabling the eos prediction unless all blanks have been filled, and prioritizing slots that have not been filled yet. Without these steps, InsT would have a failure rate \u2265 88%. \u2022 MLM (oracle length): MLM for text infilling requires predicting the length of each blank. Here we replace blanks with the target number of mask tokens, and fill them autoregressively by the most-confident-first heuristic. \u2022 BERT+LM: We use BERT's representation of each blank as seed for a left-to-right language model that learns to generate the tokens in the corresponding blank. At inference time, the multiple blanks are filled in one after another, conditioned on previous generations. \u2022 Seq2seq-full (Donahue et al., 2020) : We train a seq2seq model to output the full document x from input c. Note that it may have invalid outputs that do not match the input format, such as missing existing tokens in c or generating tokens in incorrect locations. \u2022 Seq2seq-fill (Donahue et al., 2020) : We train a seq2seq model to output only tokens to be placed in the blanks, with a special '|' token to indicate separation. For the example in Fig. 4 , its target output will be \"ice cream |is really good\". Unlike seq2seq-full, seq2seq-fill does not have the problem of losing existing tokens in c. However, it may still fail to generate the correct number of '|' that matches the input. BLM when time flies , where does it go ? for the center of the earth to be recycled and made into new time . Results As shown in when time was created , where did it come from ? it was the first part of the universe to be recycled and made into space . InsT when time flies , where does it go ? for the center of the earth has to be recycled and made into new time . when time was created , where was it ? what was the name of the universe to be recycled and made into space . MLM (oracle len) when time flies , where does it go ? from the center of the earth to be recycled converted made into new time . when time is , where is the universe ? from the creation of the universe to be recycled and made into the universe . BERT+LM when time flies , where does it go ? to the center of the earth to be recycled came made into new time . when time is , where to ? i need to find the way of the universe to be recycled and made into a lot . Seq2seqfull when time flies , where does it go ? at the center of the earth to be recycled and made into new time . when time heals , where does it go ? it 's the end of the universe to be recycled and made into space . Seq2seqfill when time flies , how does it go ? at the center of the earth to be recycled and made into new time . when  completions. MLM is trained to independently predict masked tokens instead of jointly modeling them. Even with the target number of mask tokens given, its performance is still inferior to BLM. BERT+LM lags behind other models. In BERT training, one mask corresponds to one token, whereas a blank here can cover multiple tokens, and the distance between words is not fixed. Hence, it is difficult for the LM to complete the sentence from BERT representations. Seq2seq-full has BLEU scores closest to BLM. However, its failure rate ranges from 15% to 40.6% as the mask ratio increases. Seq2seq-fill performs worse than Seq2seq-full, possibly because the decoder has to model segmented text while counting the number of blanks. In terms of fluency, outputs of BLM, InsT and Seq2seq-full all have perplexity lower than original data perplexity. This is because with beam search, models tend to generate the most typical output with the highest likelihood (Holtzman et al., 2019) . Examination of model generations confirms the superiority of BLM. In Fig. 5 , we showcase example outputs by each model at different mask ratios. In low mask ratio settings, models only need to fill in the blanks with a single word to produce grammatical completions. Most models succeed in this task. With a higher mask ratio of 50%, the main ideas of the document are concealed, and the infilling task is much more challenging. Models need to creatively generate sentences that fit the imposed canvas. Although the original meaning of the sentence is not recovered, BLM is the only model able to produce a coherent document with consistency between the question and the answer. Overall, BLM displays the best performance both quantitatively and qualitatively. Its inherent text infilling ability frees it from length, order, or termination heuristics used by other methods. Ancient Text Restoration Ancient text restoration is a form of text infilling where there are fragments in ancient documents that are illegible due to time-related damages and need to be recovered. Assael et al. (2019) introduces the PHI-ML dataset made of fragments of ancient Greek inscriptions. Restoration is performed at the character-level. The number of characters to recover is assumed to be known and indicated by a corresponding number of '?' symbols, as shown in the second row of Fig. 4 . In reality, when epigraphists restore a deteriorated document, the length of the lost fragment is unknown and needs to be guessed as a first step. While models proposed by Assael et al. (2019) relies on expert conjectures, we note that BLM can bypass this limitation and flexibly generate completions without this additional knowledge. However, in order to compute the character error rate (CER) for each '?' and have a fair comparison with previous work, we evaluate our model in the length-aware setting.  Length-aware BLM (L-BLM) We present a variant of BLM adapted to the specific features of this task. The vocabulary V is an alphabet of characters from the ancient Greek language. We extend V with special \" [t] \" tokens that denote the length of the fragment to recover. Specifically, as a preprocessing step, consecutive '?' characters are collapsed into a single \" [t] \" token, where t is the number of '?' symbols. For each such blank token, L-BLM is trained to predict a character to fill in and the length l \u2208 {0, \u2022 \u2022 \u2022 , t \u2212 1} of the new blank to its left. The length of the new blank on the right is accordingly t \u2212 1 \u2212 l. Dataset The PHI-ML dataset contains about 3 million words / 18 million characters. We evaluate models in two settings: single-slot and multi-slot. For the single-slot setting, we use the testing script of Assael et al. (2019) which samples a context of length L = 1000 from an inscription, then samples a slot of length C \u2208 [1, 10] from that context. The characters from the slot are replaced with '?' and constitute the target. For the multi-slot setting, we progressively increase the number of slots, yielding mask ratios of 25%, 40% and 50% respectively. Baselines Assael et al. (2019) proposed two models: Pythia, a character-level seq2seq-based approach; and Pythia-Word, a variant of Pythia that uses both character and word representations as input. During training, the model learns to recover the missing characters of examples where a random slot has been masked. When testing on the multislot setting, Pythia(-Word) is applied iteratively with beam size 20 for each slot. Results Table 3 summarizes the CER of all models in both settings. L-BLM achieves similar CER as Pythia in the single-slot setting, significantly outperforming human experts. Augmented with word representations, Pythia-Word further decreases the error rate compared to character-only methods. In reality, restoring damaged inscriptions re-quires reconstructing multiple lost fragments. As a larger proportion of text is missing, Pythia-Word's performance is degraded. L-BLM is robust to the setting change and outperforms Pythia-Word at the mask ratio of 40% and 50% by 4.4 and 3.3 points, respectively. We posit that L-BLM's advantage lies in its ability to maximize the joint likelihood of the completions over all slots. In contrast, Pythia-Word's is only aware of one slot at a time, and beam search is performed locally within each slot. Sentiment Transfer The goal of sentiment transfer is to modify the sentiment of a sentence while maintaining its topic (Shen et al., 2017 ). An example is described on the third row of Fig. 4 . Inspired by the way humans perform rewriting, we follow a recent line of work in style transfer that adopts a two-step approach (Li et al., 2018; Xu et al., 2018; Wu et al., 2019b) : 1. Remove words and expressions of high polarity from the source sentence; 2. Complete the partial sentence with words and expressions of the target sentiment. Specifically, we adapt the Mask-And-Infill (M&I) framework of Wu et al. (2019b) . We perform Step 1 by training a Bi-LSTM sentiment classifier and masking words whose attention weight is above average. We evaluate the contribution of our model as an infilling module in Step 2 in place of their fine-tuned BERT model. To this end, we train two instances of BLM on the dataset, one for each sentiment. At test time, the corresponding BLM is used to produce completions of the target sentiment. Wu et al. (2019b) further train the infilling model with the classifier to improve transfer accuracy. They use soft words relaxation to backprop gradients from the classifier to the generator. For BLM, however, we cannot pick locations or insert blanks as \"soft\" choices, making it challenging to employ a classifier at training time. Nevertheless, we can easily apply the classifier to guide inference. We sample 10 outputs and keep the one with the highest classifier ranking. It is not slower than beam search with size 10 and can be fully parallelized. Datasets We test on the Yelp and Amazon review datasets (Shen et al., 2017; Li et al., 2018) . The Yelp dataset has 450K/4K/1K non-parallel sentences for train/valid/test respectively, and the Amazon dataset has 555K/2K/1K sentences. Each sentence is labeled as either positive or negative. everyone that i spoke with was very helpful and kind . everyone that i spoke with was rude and unprofessional . everyone that i spoke with wasn't helpful or kind. the beans were in the burro in the rice was nowhere to be found . the beans were in the burro in the rice was the best i found . the beans were in the burro and the rice was plentiful there is definitely not enough room in that part of the venue . there is always enough parking in that part of the venue . there is so much room in that part of the venue it is n't terrible , but it is n't very good either . it is n't fancy , but it is still very good either . it is n't perfect , but it is very good . Metrics We use evaluation methods introduced by prior work (Shen et al., 2017; Li et al., 2018) . To assess the accuracy of generated sentences with respect to the target sentiment, we use a pretrained CNN classifier that achieves 97.7% accuracy on the Yelp dataset and 82.2% accuracy on the Amazon dataset. We also measure the BLEU score between transferred sentences and human references. Results In Table 4 canvas and fill in blanks with expressions of varied lengths, e.g., \"nowhere to be found\" \u2192 \"the best i found\" and \"definitely not\" \u2192 \"always\". We note that failure cases arise when negative words like \"either\" are left unmasked; BLM is then unable to produce satisfactory outputs from the canvas. Language Modeling Language modeling is a special case of text infilling where sequences are generated from scratch. Traditional left-to-right models dominate this task, but are not suitable for text infilling. Conversely, unconventional sequence models are rarely evaluated on language modeling. Here, we study the perplexity of BLM and Insertion Transformer, and compare them with left-to-right language models to provide additional insights. We use the Monte-Carlo method to estimate the likelihood in Eq. ( 5 ) with m samples. While the estimate is unbiased, given that per-word perplexity is a convex function of per-sentence likelihood, sampling estimates like ours are likely yielding a value higher than the actual perplexity (see Appendix B for a proof). As m increases, it converges to the actual perplexity. Datasets We test on three benchmark datasets: Penn Treebank (PTB) which has about 1M tokens (Mikolov et al., 2010) , WikiText-2 (WT2) which has 2M tokens, and WikiText-103 (WT103) which has 103M tokens (Merity et al., 2016) . Results Table 5 shows the trend of estimated PPL with the number of samples m. We choose m = 1000 in our evaluation, which is close to convergence. Table 6 summarizes the perplexity of our model in comparison with previous work. The top results are achieved by the Transformer-XL (Dai et al., 2019) and the adaptive embedding method (Baevski and Auli, 2018) . They use larger model sizes and supplementary techniques that can also be combined with our model. BLM rivals the Insertion Transformer and outperforms left-to-right language models with LSTM and Temporal Convolutional Network (TCN) architecture. Language modeling seems to still be challenging for free-order models. By reporting the perplexity of unconventional models like BLM, we hope to stimulate future work in this area to close the performance gap with traditional left-to-right models. Conclusion In this paper, we proposed the Blank Language Model for flexible text generation. Given partially specified text with one or more blanks, BLM will fill in the blanks with a variable number of tokens consistent with the context. We demonstrate the effectiveness of our model on various text rewriting tasks, including text infilling, ancient text restoration and style transfer. The action of BLM consists of selecting a blank and replacing it with a word and possibly adjoining blanks. We train BLM by optimizing a lower bound on the marginal data likelihood that sums over all possible generation trajectories. In this way, we encourage the model to realize a sentence equally well in all orders, which is suitable for filling arbitrary blanks. Appendix C shows examples generated by BLM along with their trajectories. Depending on the application, we could also train the model to generate in specific orders by placing higher weights on the corresponding trajectories. BLM has plenty of future applications, including template filling, information fusion, assisting human writing, etc. Moreover, we can extend our formulation to a conditional generative model. Such models can be used in machine translation to support editing and refining translation, as well as in dialogue systems to compose a complete sentence with given elements. While we proposed BLM for language generation, it would also be interesting to compare the representations learned by BLM with those produced by other pre-training methods. Acknowledgments We thank all reviewers and the MIT NLP group for their thoughtful feedback. Appendix A Implementation Details for Text Infilling Baselines A.1 Insertion Transformer We implement the Insertion Transformer in our own framework, using the same Transformer encoder module as for BLM and replacing the prediction layers by Insertion Transformer's mechanism. The canvas is also generated according to the training procedure of Insertion Transformer. A.2 Masked Language Model We use the RobertaForMaskedLM architecture in the Transformers library for MLM (Wolf et al., 2019; Liu et al., 2019b) . At test time, the model is given an easier version of the text infilling task where blanks are expanded into sequences of mask tokens of the target length (or equivalently, the model uses an oracle to predict the length of the infilling). We experiment with three decoding strategies: (1) one-shot: the model predicts all masks simultaneously (2) left-to-right: the model fills in the masks from left to right (3) confident-first: the model fills one mask at a time that has the highest score. We report results for the confident-first strategy which has the best performance. A.3 BERT+LM We use the bert-base-uncased model as served by the Transformers library (Wolf et al., 2019; Devlin et al., 2018) . The left-to-right language model is a Transformer decoder to predict tokens in a blank. Its input word embedding is concatenated with BERT's output in the blank position at each time step. A.4 Seq2seq-full and Seq2seq-fill For both seq2seq baselines, we use Fairseq's transformer iwslt de en architecture (Ott et al., 2019) . To generate training data, we apply the blanking procedure to the input dataset and generate k copies of each sentence with different masks. We experiment with k \u2208 {1, 10, 100} and report the best performance, obtained by k = 10. C Generation Trajectory also the also the also choice the salsa also choice the salsa was also choice the salsa was also only choice the salsa was also only choice . the salsa was also my only choice .",
    "abstract": "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications. 1",
    "countries": [
        "United States"
    ],
    "languages": [
        "Greek"
    ],
    "numcitedby": "36",
    "year": "2020",
    "month": "November",
    "title": "Blank Language Models"
}