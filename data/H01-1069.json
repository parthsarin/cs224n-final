{
    "article": "We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system. INTRODUCTION Several research projects have recently investigated the problem of automatically answering simple questions that have brief phrasal answers ('factoids'), by identifying and extracting the answer from a large collection of text. The systems built in these projects exhibit a fairly standard structure: they create a query from the user's question, perform IR with the query to locate (segments of) documents likely t o contain an answer, and then pinpoint the most likely answer passage within the candidate documents. The most common difference lies in the pinpointing. Many projects employ a window-based word scoring method that rewards desirable words in the window. They move the window across the candidate answers texts/segments and return the window at the position giving the highest total score. A word is desirable if it is a content word and it is either contained in the question, or is a variant of a word contained in the question, or if it matches the words of the expected answer. Many variations of this method are possible-of the scores, of the treatment of multiword phrases and gaps between desirable words, of the range of variations allowed, and of the computation of the expected answer words. Although it works to some degree (giving results of up to 30% in independent evaluations), the window-based method has several quite serious limitations: \u2022 it cannot pinpoint answer boundaries precisely (e.g., an exact name or noun phrase), \u2022 it relies solely on information at the word level, and hence cannot recognize information of the desired type (such as Person or Location), \u2022 it cannot locate and compose parts of answers that are distributed over areas wider than the window. Window-based pinpointing is therefore not satisfactory in the long run, even for factoid QA. In this paper we describe work in our Webclopedia project on semantics-based answer pinpointing. Initially, though, recognizing the simplicity and power of the window-based technique for getting started, we implemented a version of it as a fallback method. We then implemented two more sophisticated methods: syntacticsemantic question analysis and QA pattern matching. This involves classification of QA types to facilitate recognition of desired answer types, a robust syntactic-semantic parser t o analyze the question and candidate answers, and a matcher that combines word-and parse-tree-level information to identify answer passages more precisely. We expect that the two methods will really show their power when more complex nonfactoid answers are sought. In this paper we describe how well the three methods did relative to each other. Section 2 outlines the Webclopedia system. Sections 3, 4, and 5 describe the semantics-based components: a QA Typology, question and answer parsing, and matching. Finally, we outline current work on automatically learning QA patterns using the Noisy Channel Model. WEBCLOPEDIA Webclopedia's architecture (Figure 1 ) follows the pattern outlined above: Question parsing: Using BBN's IdentiFinder [1] , our parser CONTEX (Section 4) produces a syntactic-semantic analysis of the question and determines the QA type (Section 3). Query formation: Single-and multi-word units (content words) are extracted from the analysis, and WordNet synsets are used for query expansion. A Boolean query is formed. See [9] . IR: The IR engine MG [12] returns the top-ranked 1000 documents. Segmentation: To decrease the amount of text to be processed, the documents are broken into semantically coherent segments. Two text segmenter-TexTiling [5] and C99 [2] -were tried; the first is used; see [9] . Ranking s e g m e n t s : For each segment, each sentence i s scored using a formula that rewards word and phrase overlap with the question and its expanded query words. Segments are ranked. See [9] Parsing s e g m e n t s : CONTEX parses each sentence of the top-ranked 100 segments (Section 4). Pinpointing: For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers' scores are compared and the winner(s) are output. THE QA TYPOLOGY In order to perform pinpointing deeper than the word level, the system has to produce a representation of what the user i s asking. Some previous work in automated question answering has categorized questions by question word or by a mixture of question word and the semantic class of the answer [11, 10] . To ensure full coverage of all forms of simple question and answer, and to be able to factor in deviations and special requirements, we are developing a QA Typology. We motivate the Typology (a taxonomy of QA types) as follows. There  America?, which is the QA type Proper-Person) and Abbreviation-Expansion (for What does HLT stand for?). In addition, the QA Typology becomes increasingly specific as one moves from the root downward. To create the QA Typology, we analyzed 17,384 questions and their answers (downloaded from answers.com); see (Gerber, i n prep.). The Typology (Figure 2 ) contains 72 nodes, whose leaf nodes capture QA variations that can in many cases be further differentiated. Each Typology node has been annotated with examples and typical patterns of expression of both Question and Answer, using a simple template notation that expressed configurations of words and parse tree annotations (Figure 3 ). Question pattern information (specifically, the semantic type of the answer required, which we call a Qtarget) is produced by the CONTEX parser (Section 4) when analyzing the question, enabling it to output its guess(s) for the QA type. At the time of the TREC-9 Q&A evaluation, we had produced approx. 500 patterns by simply cross-combining approx. 2 0 Question patterns with approx. 25 Answer patterns. To our disappointment (Section 6), these patterns were both too specific and too few to identify answers frequently-when they applied, they were quite accurate, but they applied too seldom. We therefore started work on automatically learning QA patterns in parse trees (Section 7). On the other hand, the semantic class of the answer (the Qtarget) is used to good effect (Sections 4 and 6). PARSING CONTEX is a deterministic machine-learning based grammar learner/parser that was originally built for MT [6] . For English, parses of unseen sentences measured 87.6% labeled precision and 88.4% labeled recall, trained on 2048 sentences from the Penn Treebank. Over the past few years it has been extended to Japanese and Korean [7] . Parsing Questions Accuracy is particularly important for question parsing, because for only one question there may be several answers in a large document collection. In particular, it is important t o identify as specific a Qtarget as possible. But grammar rules  for declarative sentences do not apply well to questions, which although typically shorter than declaratives, exhibit markedly different word order, preposition stranding (\"What university was Woodrow Wilson President of?\"), etc. Unfortunately for CONTEX, questions to train on were not initially easily available; the Wall Street Journal sentences contain a few questions, often from quotes, but not enough and not representative enough to result in an acceptable level of question parse accuracy. By collecting and treebanking, however, we increased the number of questions in the training data from 250 (for our TREC-9 evaluation version of Webclopedia) to 400 on Oct 16 to 975 on Dec 9. The effect i s shown in Table 1 . In the first test run (\"[trained] without [additional questions]\"), CONTEX was trained mostly o n declarative sentences (2000 Wall Street Journal sentences, namely the enriched Penn Treebank, plus a few other nonquestion sentences such as imperatives and short phrases). In later runs (\"[trained] with [add. questions]\"), the system was trained on the same examples plus a subset of the 1153 questions we have treebanked at ISI (38 questions from the pre-TREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and 222 others). The TREC-8 and TREC-9 questions were divided into 5 subsets, used in a five-fold cross validation test in which the system was trained on all but the test questions, and then evaluated on the test questions. Reasons for the improvement include (1) significantly more training data; (2) a few additional features, some more treebank cleaning, a bit more background knowledge etc.; and (3) the 251 test questions on Oct. 16 were probably a little bit harder on average, because a few of the TREC-9 questions initially treebanked (and included in the October figures) were selected for early treebanking because they represented particular challenges, hurting subsequent Qtarget processing. Parsing Potential Answers The semantic type ontology in CONTEX was extended t o include 115 Qtarget types, plus some combined types; more details in [8] . Beside the Qtargets that refer to concepts i n CONTEX's concept ontology (see first example below), Qtargets can also refer to part of speech labels (first example), to constituent roles or slots of parse trees (second and third examples), and to more abstract nodes in the QA Typology (later examples). For questions with the Qtargets Q-WHY-FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and others, the parser also provides Qargs-information helpful for matching (final examples). Semantic ontology types (I-EN-CITY) and part of speech labels (S-PROPER-NAME): What is the capital of Uganda? QTARGET: (((I-EN-CITY S-PROPER-NAME)) ((EQ I-EN-PROPER-PLACE))) Parse tree roles: Why can't ostriches fly? QTARGET: (((ROLE REASON))) Name a film in which Jude Law acted. QTARGET: (((SLOT TITLE-P TRUE))) QA Typology nodes: What are the Black Hills known for? Q-WHY-FAMOUS What is Occam's Razor? Q-DEFINITION What is another name for nearsightedness? Q-SYNONYM Should you exercise when you're sick? Q-YES-NO-QUESTION Qargs for additional information: Who was Betsy Ross? QTARGET: (((Q-WHY-FAMOUS-PERSON))) QARGS: ((\"Betsy Ross\")) How is \"Pacific Bell\" abbreviated? QTARGET: (((Q-ABBREVIATION))) QARGS: ((\"Pacific Bell\")) What are geckos? QTARGET: (((Q-DEFINITION))) QARGS: ((\"geckos\" \"gecko\") (\"animal\")) These Qtargets are determined during parsing using 276 handwritten rules. Still, for approx. 10% of the TREC-8&9 questions there is no easily determinable Qtarget (\"What does the Peugeot company manufacture?\"; \"What is caliente i n English?\"). Strategies for dealing with this are under investigation. More details appear in (Hermjakob, 2001) . The current accuracy of the parser on questions and resulting Qtargets sentences is shown in Table 2 . ANSWER MATCHING The Matcher performs three independent matches, in order: \u2022 match QA patterns in the parse tree, \u2022 match Qtargets and Qwords in the parse tree, \u2022 match over the answer text using a word window. Details appear in [9] . RESULTS We entered the TREC-9 short form QA track, and received an overall Mean Reciprocal Rank score of 0.318, which put Webclopedia in essentially tied second place with two others. (The best system far outperformed those in second place.) In order to determine the relative performance of the modules, we counted how many correct answers their output contained, working on our training corpus. Table 3 shows the evolution of the system over a sample one-month period, reflecting the amount of work put into different modules. The modules QA pattern, Qtarget, Qword, and Window were all run in parallel from the same Ranker output. The same pattern, albeit with lower scores, occurred in the TREC test (Table 4 ). The QA patterns made only a small contribution, the Qtarget made by far the largest contribution, and, interestingly, the word-level window match lay somewhere in between. We are pleased with the performance of the Qtarget match. This shows that CONTEX is able to identify to some degree the semantic type of the desired answer, and able to pinpoint these types also in candidate answers. The fact that it outperforms the window match indicates the desirability of looking deeper than the surface level. As discussed in Section 4, we are strengthening the parser's ability to identify Qtargets. We are disappointed in the performance of the 500 QA patterns. Analysis suggests that we had too few patterns, and the ones we had were too specific. When patterns matched, they were rather accurate, both in finding correct answers and more precisely pinpointing the boundaries of answers. However, they were too sensitive to variations in phrasing. Furthermore, it was difficult to construct robust and accurate question and answer phraseology patterns manually, for several reasons. First, manual construction relies on the inventiveness of the pattern builder to foresee variations of phrasing, for both question and answer. It is however nearly impossible to think of all possible variations when building patterns. Second, it is not always clear at what level of representation t o formulate the pattern: when should one specify using words? Parts of speech? Other parse tree nodes? Semantic classes? The patterns in Figure 3 include only a few of these alternatives. Specifying the wrong elements can result in non-optimal coverage. Third, the work is simply tedious. We therefore decided to try to learn QA patterns automatically. TOWARD LEARNING QA PATTERNS AUTOMATICALLY To learn corresponding question and answer expressions, we pair up the parse trees of a question and (each one of) its answer(s). We then apply a set of matching criteria to identify potential corresponding portions of the trees. We then use the EM algorithm to learn the strengths of correspondence combinations at various levels of representation. This work i s still in progress. In order to learn this information we observe the truism that there are many more answers than questions. This holds for the two QA corpora we have access to-TREC and an FAQ website (since discontinued). We therefore use the familiar version of the Noisy Channel Model and Bayes' Rule. For each basic QA type (Location, Why-Famous, etc.): As usual, many variations are possible, including how t o determine likelihood of expressing a true answer; whether t o consider all nodes or just certain major syntactic ones (N, NP, VP, etc.); which information within each node to consider (syntactic? semantic? lexical?); how to define 'covarying information'-node identity? individual slot value equality?; what to do about the actual answer node in the A trees; if (and how) to represent the relationships among A nodes that have been found to be important; etc. Figure 4 provides an answer parse tree that indicates likely Location nodes, determined b y appropriate syntactic class, semantic type, and syntactic role in the sentence. Our initial model focuses on bags of corresponding QA parse tree nodes, and will help to indicate for a given question what type of node(s) will contain the answer. We plan to extend this model to capture structured configurations of nodes that, when matched to a question, will help indicate where in the parse tree of a potential answer sentence the answer actually lies. Such bags or structures of nodes correspond, at the surface level, t o important phrases or words. However, by using CONTEX output we abstract away from the surface level, and learn t o include whatever syntactic and/or semantic information is best suited for predicting likely answers.",
    "abstract": "We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.",
    "countries": [
        "United States"
    ],
    "languages": [
        "Korean",
        "Japanese"
    ],
    "numcitedby": "177",
    "year": "2001",
    "month": "",
    "title": "Toward Semantics-Based Answer Pinpointing"
}