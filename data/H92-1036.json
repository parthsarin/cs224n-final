{
    "article": "We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM). The classical MLE reestimation algorithms, namely the forward-backward algorithm and the segmental k-means algorithm, are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities. Because of its adaptive nature, Bayesian learning serves as a unified approach for the following four speech recognition applications, namely parameter smoothing, speaker adaptation, speaker group modeling and corrective ~aining. New experimental results on all four applications are provided to show the effectiveness of the MAP estimation approach. INTRODUCTION Estimation of hidden Marknv model (HMM) is usually obtained by the method of maximum likelihood (ML) [1, 10, 6] assuming that the size of the training data is large enough to provide robust estimates. This paper investigates maximum a posteriori (MAP) estimate of continuous density hidden Markov models (CDHMM). The MAP estimate can be seen as a Bayes estimate of the vector parameter when the loss function is not specified [2] . This estimation technique provides a way of incorporatimg prior information in the training process, which is particularly useful to deal with problems posed by sparse training data for which the ML approach gives inaccurate estimates. This approach can be applied to two classes of estimation problems, namely, parameter smoothing and model adaptation, both related to the problem of sparse training data. In the following the sample x = (zl, ...,z,~) is a given set of n observations, where zl, ..., z n are either independent and identically distributed (i.i.d.), or are drawn from a probabilistic function of a Markov chain. The difference between MAP and ML estimation lies in the assumption of an appropriate prior disliibution of the parameters to be estimated. If 0, assumed to be a random vector taking values in the space O, is the parameter vector to be estimated from the sample x with probability density function (p.d.f.) f(.lO), and if g is the prior p.d.f, of 0, then the MAP estimate, 0~p, is defined as the mode of the posterior p.d.f, of 0, i.e. Oma, = argmoax f(xlO)g(O) (I) If 9 is assumed to be fixed but unknown, then there is no knowledge about 8, which is equivalent to assuming a non-informative improper prior, i,e. g (8) ----constant. Equation (1) then reduces to the familiar ML formulation. Given the MAP formulation two problems remain: the choice of the prior distribution family and the evaluation of the maximum a ~This work was done while Jean-Luc Gauvain was on leave from the Speech Communication Group at LIMSI/CNRS, Orsay, France. posteriori. These two problems are closely related, since the appropilate choice of the prior distribution can greatly simplify the MAP estimation. Like for ML estimation, MAP estimation is relatively easy if the famay ofp.d.f.'s {f(-10), 0 ~ O} possesses a sufficient statistic of fixed dimension t(x). In this case, the natural solution is to choose the prior density in a conjugate family, {k(.ko), ~o E ~}, which includes the kernel density of f(. lO), i.e. Vx t(x) e ~b [4, 2] . The MAP estimation is then reduced to the evaluation of the mode of k(Ol~o' ) = k(Oko)k(Olt(x)), a problem almost identical to the ML estimation problem. However, among the families of interest, only exponential families have a sufficient statistic of fixed dimension [7] . When there is no sufficient statistic of fixed dimension, MAP estimation, like ML estimation, is a much more difficult problem because the posterior density is not expressible in terms of a fixed number of parameters and cannot be maximized easily. For both finite mixture density and hidden Markov model, the lack of a sufficient statistic of fixed dimension is due to the underlying hidden process, i.e. a multinomial model for the mixture and a Markov chain for an HMM. In these cases ML estimates are usually obtained by using the expectation-maximization (EM) algorithm [3, I, 13] . This algorithm exploits the fact that the complete-data likelihood can be simpler to maximize than the likelihood of the incomplete data, as in the case where the complete-data model has sufficient statistics of fixed dimension. As noted by Dempster et al. [3] , the EM algorithm can also be applied to MAP estimation. In the next two sections the formulations of this algorithm for MAP estimation of Gaussian mixture and CDHMM with Gaussian mixture observation densities are derived. MAP ESTIMATES FOR GAUSSIAN MIXTURE Suppose that x = (zl,. where (rk,/zk, t~k, Uk) are the prior density parameters such that ak > p --1, rk > 0,/~k is a vector of dimension p and uk is a p \u00d7 p positive definite matrix. Assuming independence between the parameters of the mixture components and the mixture weights, the joint prior density g(0) is taken to be a product of the prior p.d.f.'s defined in equations \u2022 K \u2022 (2) and (3), Le. g(0) = g(w~, ...,~K)FL:, z(m~,,-,). As will be shown later, this choice for the prior density family can also be justified by noting that the EM algorithm can be applied to the MAP estimation problem if the prior density is in the conjuguate family of the complete-data density. The EM algorithm is an iterative procedure for approximating maximum-likelihood estimates in an incomplete-data context such as mixture density and hidden Markov model estimation problems [1, 3, 13] . This procedure consists of maximizing at each iteration the auxilliary function Q(O, ~) defined as the expectation of the complete-data log-likelihood log h(y[0 ) given the incomplete data x = (~, ...,x,) and the current fit 0, i.e. Q(0, ~) = E[log h(yl0)lx, ~. For a mixture density, the completedata likelihood is the joint likelihood of x and \u00a3 = (\u00a3t, ..., \u00a3n ) the unobserved labels referring to the mixture components, i.e. y = (x, \u00a3). The EM procedure derives from the fact that log f( at each iteration instead of Q(O, 0) [3] . For a mixture of K densities {f(.10~)}~=L...,g with mixture weights {wk } k= ~,...,K, the auxilliary function Q takes the following form [13] : Q(O, #)= ~ ~ ~\"'~f(zt!O~)log~f(ztlo~) (4) From ( 2 ), ( 3 ) and ( 5 ) it can easily be verified that ~(.,0) belongs to the same family as g, and has parameters O,L ' ' ' ' rk,/~k, t~k, uk}k:l,...,K satisfying the following conditions: The considered family of distributions is therefore a conjugate family for the complete-data density. The mode of ~P(., 0), denoted J i , obtained (wk, ink, rk), may be from the modes of the Dirichlet and normal-Wishart densities: w~ = (.~ -1)/~c:t(.~ _ 1), m~ = p~, and r~ = (~ -p)u~-'. Thus, the EM iteration is as follows: , Vk --1 + ~=! Ok, ( 11 ) (M k = mk= (12) , -, ,,~ + ~'k(#~ -mg)(~,~ -rag) ~ + T k = ~L, ~k,(~, --~)(~, --rag) ~ (13) If it is assumed &k > 0, then ckl, ck2, ...,ck, is a sequence of n i.i.d, random variables with a non-degenerate distribution and limsupn_o o ~=. ckt = co with probability one. It follows that w~ converges to ~=l Ckt/n with probability one when n ~ oo. Applying the same reasoning to m~ and r~, it can be seen that the EM reestimation formulas for the MAP and ML approaches are asymptotically similar. Thus as long as the initial estimates are identical, the EM algorithm will provide identical estimates with probability one when n ~ c\u00a2. MAP ESTIMATES FOR CDHMM The results obtained for a mixture of normal densities can be extended to the case of HMM with Gaussian mixture state observation densities, assuming that the observation p.d.f.'s of all the states have the same number of mixture components. We consider an N-state HMM with parameter vector A = (x, A, 0), where r is the initial probability vector, A is the transition matrix, and 0 is the p.d.f, parameter vector composed of the mixture parameters 0i = {Wik,mik,rik}kfl,...,K for each state i. For a sample x = (2~1, ..., zn), the complete data is y = (x, s,Q where s = (so,..., s,) is the unobserved state sequence, and l = (\u00a3h ..., l,~) are the unobserved mixture component labels, si E [1, N] and li E [1, K] . The joint p.d.f, h(.lX) of x, s, and\u00a3 is defined as [1] rl h(x, s,llA ) = a'. o Hao,_,,,w,,t,f(xtlO,,t,) (14) t=| where 7ri is the initial probabilty of state i, aij is the transition probability from state i to state j, and Oik =(mik, rik) is the parameter vector of the k-th normal p.d.f, associated to state i. It follows that the likelihood of x has the form where f(x,lOi ) K = ~k=t w~kA/'(x*lralk, rik), and the summation is over all possible state sequences. In the general case where MAP estimation is to be applied not only to the observation density parameters but also to the initial and transition probabilities, a Dirichlet density can also be used for the initial probability vector ~r and for each row of the transition probability matrix A. This choice directly follows the results of the previous section: since the complete-data likelihood satisfies h(x, s,tlA ) = h(s, A)h(x, tls , A) where h(s, A) is the product of N + 1 multinomial densities with parameters {n, a't, ..., ~N} and { n, air ..... a i N } if l,...,N . The prior density for all the HMM parameters is thus G(A) oc ~rT'-Ig(Oi) H a?;J-I (16) i:1 j=l J In the following subsections we examine two ways of approximating AMAp by local maximization of f(xl~)G(~) and f(x, sI~)G(A). These two solutions are the MAP versions of the B aura-Welch algorithm [1 ] and of the segmental k-means algorithm [12] , algorithms which were developed for ML estimation. Forward-Backward MAP Estimate From ( 14 ) it is straightforward to show that the auxilliary function of the EM algorithm applied to MLE of A, Q(A, ~) = E[log h(Yi~)lx, \u00a3], can be decomposed into a sum of three auxilliary functions: Q,~(a', X), Q~(A, X) and Qo(O, ~) [6] . These functions which can be independently maximized take the following forms: N Q'rOr' ~) = E ~io log ~ri (17) i=1 QA(A, ~) = fist log aij (18) i=1 t=l j=l N Qo(o,\u00a3) = ~ Qo,(od\u00a3) (19) i--1 with ~ ~ ~ikf(zt[~ik) Qo,(Oi,X)= 7\" logoJikf(xtlOik) (20) ,=, k=t f(z,l@,) where ~i/t = Pr(st-t =i, st =jlx, ~) and 3'. , .q -1 + ~=, 6,. (23) aq = EjN= 1 \"'J _ N -F Eju_t E~:a ~q' For multiple independent observation sequences { xo } q= l,...,Q, t~(q) ~(q)~ with Xq = x't .... , ~., ,, we maximize G(A) lq?:l f(xqlA)' where f(.[A) is defined by (15). The EM auxilliary function is then R(A, X) = logG(A) + ~qQ=t E[l\u00b0gh(Yql~)lxq, X], where h(.lA) is defined by equation (14) . It follows that the reestimation formulas for A and 0 still hold if the summations over t are ~(q) and -(q) replaced by summations over q and t. The values \",~jt 7. are then obtained by applying the forward-backward algorithm for each observation sequence. The reestimation formula for the initial probabilities becomes , T/, -1 + Eq%l ,~, = (24) N Q (q) Ei:, ', -Iv + E.:, ,,o As for the mixture Gaussian case, it can be shown that as Q ~ co, the MAP reestimation formulas approach the ML ones, exhibiting the asymptotic similarity of the two estimates. These reestimation equations give estimates of the HMM parameters which correspond to a local maximum of the posterior density. The choice of the initial estimates is therefore essential to finding a solution close to a global maximum and to minimize the number of EM iterations needed to attain the local maximum. When using an informative prior, one natural choice for the initial estimates is the mode of the prior density, which represents all the available information about the parameters when no data has been observed. The corresponding values are simply obtained by applying the reestimation formulas with n equal to 0. When using a non-informative prior, i.e. for ML estimation, while for discrete HMMs it is possible to use uniform initial estimates, there is no trivial solution for the continuous density case. Segmental MAP Estimate By analogy with the segmental k-means algorithm [12] , a different optimization criterion can be considered. Instead of maximizing G(AIx), the joint posterior density of A and s, G(A, slx ), is maximized. The estimation procedure becomes = argmax max G(A, six ) (25) ), s = argm~x m~x f(x, s[A)G(A) ( 26 ) and A is called the segmental MAP estimate of A. As for the segmental k-means algorithm, it is straightforward to prove that starting with any estimate A (m), alternate maximization over s and It is straightforward to show that the forward-backward reestimation equations still hold with fijt= 6ts('n)~ t-t -i)6(s~ m) -J) and \"fit = ~(s~ '~) --i), where ~ denotes the Kronecker delta function. PRIOR DENSITY ESTIMATION In the previous sections it was assumed that the prior density G(A) is a member of a preassigned family of prior distributions defined by ( 16 ). In a strictly Bayesian approach the vector parameter of this family ofp.d.f.'s {G(.[~), ~ E ~b} is also assumed known based on common or subjective knowledge about the stochastic process. Another solution is to adopt an empirical Bayesian approach [14] where the prior parameters are estimated directly from data. The estimation is then based on the marginal disttrbution of the data given the prior parameters. Adopting the empirical Bayes approach, it is assumed that the sequence of observations, X, is composed of multiple independent sequences associated with different unknown values of the HMM parameters. Letting (X,A) = [(xt, Ai), (x2, A2) .... ] be such a multiple sequence of observations, where each pair is independent of the others and the Aq have a common prior distribution G(.[~). Since the Aq are not directly observed, the prior parameter estimates must be obtained from the marginal density f(X[~), f(Xl~) --~ f(XIA)G(A[~) dA (29) where f(XIA ) = I~Iq f(xqlAq) and G(AIg~ ) = I~q G(AqI~)\" However, maximum likelihood estimation based on f(Xl~ ) appears rather difficult. To simplify this problem, we can choose a simpler optimization criterion by maximizing the joint p.d.f, f(X, A I~) over A and ~ instead of the marginal p.d.f, of X given ~. Starting with an initial estimate of ~o, we obtain a hill climbing procedure by alternate maximization over A and ~o, i.e. Finding this estimate poses two problems. First, due to the Wishart and Dirichlet components, ML estimation for the density defined by ( 16 ) is not trivial. Second, since more parameters are needed for the prior density than for the HMM itself, there can be a problem of overparametrization when the number of pairs (xq, Aq) is small. One way to simplify the estimation problem is to use moment estimates to approximate the ML estimates. For the overparametrization problem, it is possible to reduce the size of the prior family by adding constraints on the prior parameters. For example, the prior family can be limited to the family of the kernel density of the complete-data likelihood, i.e. the posterior density family of the complete.data model when no prior information is available. Doing so, it can be verified that the following constraints hold v~k = r~k (32) aik = rik-t-p (33) A (m) = argmAax f(X, AIr <m)) (30) Parameter tying can also be used to further reduce the size of the prior family. We use this approach for approach for two types of applications: parameter smoothing and adaptation learning. For parameter \"smoothing\", the goal is to estimate {Al, A2, ...}. The previous algorithm offers a direct solution to \"smooth\" these different estimates by assuming a common prior density for all the models. For adaptative learning, we observe a new sequence of observations Xq associated with the unobserved vector parameter value Aq. The MAP estimate of A, can be obtained by using for prior parameters a point estimate ~ obtained with the previous algorithm. Such a training process can be seen as an adaptation of an a priori model = argmaxx G(A[~) (when no training data is available) to more specific conditions corresponding to the new observation sequence Xq. In the applications presented in this paper, the prior density parameters were estimated along with the estimation of the SI model parameters using the segmental k-means algorithm. Information about the variability to be modeled with the prior densities was associated with each frame of the SI training data. This information was simply represented by a class number which can be the speaker ID, the speaker sex, or the phonetic context. The HMM parameters for each class given the mixture component were then computed, and moment estimates were obtained for the tied prior parameters also subject to conditions (32-33) [5] . EXPERIMENTAL SETUP The experiments presented in this paper used various sets of context-independent (CI) and context-dependent (CD) phone models. Each model is a left-to-right HMM with Gaussian mixture state observation densities. Diagonal covariance matrices are used and the transition probabilities are assumed fixed and known. As described in [8] , a 3g-dimensional feature vector composed of LPC-derived cepstrum coefficients, and first and second order time derivatives. Results are reported for the RM task with the standard word pair grammar and for the TI/NIST connected digits. Both corpora were down-sampled to telephone bandwidth. MODEL SMOOTHING AND ADAPTATION Last year we reported results for CD model smoothing, speaker adaptation, and sex-dependentmodeling [5] . CD was tested on the JUN90 data with 1 minute and 2 minutes of speaker-specific adaptation data. A 16% and 31% reduction in word error were obtained compared to the SI results [5] . On the FEB91 test, using Bayesian learning for CD model smoothing combined with sex-dependent modeling, a 21% word error reduction was obtained compared to the baseline results [5] . In order to compare speaker adaption to ML training of SD models, an experiment has been carded out on the FEB91-SD test material including data from 12 speakers (7m/5f), using a set of 47 CI phone models. Two, five and thirty minutes of the SD training data were used for training and adaptation. The SD, SA (SI) word error rates are given in the two first rows of Table 1 . The SD word error rate for 2 min of training data was 31.5%. The SI word error rate (0 minutes of adaptation data) was 13.9%, somewhat comparable to the SD results with 5 min of SD training data. The SA models are seen to perform better than SD models when relatively small amounts of data were used for training or adaptation. When all the available training data was used, the SA and SD results were comparable, consistent with the Bayesian formulation that the MAP estimate converges to the MLE. Relative to the SI results, the word error reduction was 37% with 2 rain of adaptation data, an improvement similar to that observed on the JUN90 test data with CD models [5] . As in the previous experiment, a larger improvement was observed for the female speakers (51%) than for the male speakers (22%). Speaker adaptation was also performed starting with sexdependent models (third row of Table 1 ). The word error rate with no speaker adaptation is 11.5%. The error rate is reduced to 7.5% with 2 rain, and 6.0% with 5 rain, of adaptation data. Comparing the last 2 rows of the table it can be seen that SA is more effective when sex-dependent seed models are used. The error reduction with 2 rain of training data is 35% compared to the sex-dependent model results and 46% compared to the SI model results. P.D.F. SMOOTHING We have shown that Bayesian learning can be used for CD model smoothing [5] . This approach can be seen either as a way to add extra constraints to the model parameters so as to reduce the effect of insufficient training data, or it can be seen as an \"interpolation\" between two sets of parameter estimates: one corresponding to the desired model and the other to a smaller model which can be trained using MLE on the same data. Instead of defining a reduced parameter set by removing the context dependency, we can alternatively reduce the mixture size of the observation densities and use a single Ganssian per state in the smaller model. Cast in the Bayesian learning framework, this implies that the same marginal prior density is used for all the components of a given mixture. Variance clipping can also be viewed as a MAP estimation technique with a uniform prior density constrained by a maximum (positive) value for the precision parameters [9] . However, this does not have the appealing interpolation capability of the conjugate priors. We For the RM tests summarized in Table 3 , a consistent improvement over the variance clipping scheme (MLE+VC) is observed when p.d.f, smoothing is applied. Combined with sex-dependent modeling, the MAP(M/F) scheme gives an average word accuracy of about 95.8%. CORRECTIVE TRAINING Bayesian learning provides a scheme for model adaptation which can also be used for corrective training. Corrective training maximizes the recognition rate on the training data hoping that that will also improve performance on the test data. One simple way to do corrective training is to use the training sentences which were incorrectly recognized as new data. In order to do so, the state segmentation step of the segmental MAP algorithm was modified to obtain not only the frame/state association for the sentence model states but also for the states corresponding to the model of all the possible sentences (general model). In the reestimation formulas, the values cikt for each state si are evaluated using (21), such that 7it is equal to 1 in the sentence model and to -1 in the general model. While convergence is not guaranteed, in practice it was found that by using large values for rik(_ ~ 200), the number of training sentence errors decreased after each iteration until convergence. If we use the forward-backward MAP algorithm we obtain a corrective training algorithm for CDHMM's very similar to the recently proposed corrective MMIE training algorithm [11 ] . Corrective training was evaluated on both the TI/NIST SI connected digit and the RM tasks. Only the Ganssian mean vectors and the mixture weights were corrected. For the TI digits a set of 21 phonetic HMMs were ~ained on the 8565 digit strings. Results are given in The corrective training procedure is also effective for continuous sentence recognition of the RM task. Table 5 gives results for the RM task, using 47 SI-CI models with 32 mixture components. The CT-32 corrective training assumes a fixed beam width. Since the number of string errors was small in the training set, the amount of data for corrective training was rather limited. To increase the amount, a smaller beam width was used to recognize the training data. It was observed that this improved corrective training (ICT-32) procedure not only reduced the error rate in training but also increased the separation between the conect string and the other competing strings. The number of training errors also increased as predicted. The regular and the improved corrective training gave an average word error rate reduction of 15% and 20% respectively on the test data. SUMMARY The theoretical framework for MAP estimation of multivariate Gaussian mixt~e density and HMM with mixture Gaussian state observation densities was presented. Two MAP training algorithms, the forward-baclovard MAP estimation and the segmental MAP estimation, were formulated. Bayesian learning serves as a unified approach for speaker adaptation, speaker group modeling, parameter smoothing and corrective training. Tested on the RM task, encouraging results have been obtained for all four applications. For speaker adaptation, a 37% word error reduction over the SI results was obtained on the FEB91-SD test with 2 minutes of speaker-specific training data. It was also found that speaker adaptation is more effective when based on sex-dependent models than with an SI seed. Compared to speakerdependent training, speaker adaptation achieved a better performance with the same amount of training/adaptation data. Corrective training appfied to CI models reduced word errors by 15-20%. The best SI results on RM tests were obtained with p.d.L smoothing and sex-dependent modeling, an average word accuracy of about 95.8% on four test sets. Only corrective training and p.d.L smoothing were applied to the TI/NIST connected digit task. It was found that corrective training is effective.for improving CI models, reducing the number of string errors by up to 27%. Corrective training was found to be more effective for models having smaller numbers of parameters. This implies that we can reduce computational requierements by using corrective training on a smaller model and achieve performance comparable to that of a larger model. Using 213 CD models, p.d.L smoothing provided a robust model that gave a 99.1% string accuracy on the test data, the best performance reported on this corpus.",
    "abstract": "We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM). The classical MLE reestimation algorithms, namely the forward-backward algorithm and the segmental k-means algorithm, are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities. Because of its adaptive nature, Bayesian learning serves as a unified approach for the following four speech recognition applications, namely parameter smoothing, speaker adaptation, speaker group modeling and corrective ~aining. New experimental results on all four applications are provided to show the effectiveness of the MAP estimation approach.",
    "countries": [
        "United States"
    ],
    "languages": [],
    "numcitedby": "81",
    "year": "1992",
    "month": "",
    "title": "{MAP} Estimation of Continuous Density {HMM} : Theory and Applications"
}