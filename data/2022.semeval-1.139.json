{
    "article": "In this paper, we present our system and findings for SemEval-2022 Task 6 -iSarcasmEval: Intended Sarcasm Detection in English. The main objective of this task was to identify sarcastic tweets. This task was challenging mainly due to (1) the small training dataset that contains only 3468 tweets and (2) the imbalanced class distribution (25% sarcastic and 75% non-sarcastic). Our submitted model (ranked eighth on Sub-Task A and fifth on Sub-Task C) consists of a Transformer-based approach (BERTweet model). Introduction The Cambridge Dictionary 1 defines sarcasm as \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\" Due to the Web openness, sarcastic content becomes very frequent in social media and ecommerce platforms, which may cause misunderstandings. Furthermore, identifying such content is a very challenging task even for humans (Farias and Rosso, 2017) . Also, it could impact some natural language processing tasks such as sentiment analysis (Farias and Rosso, 2017; Do et al., 2019; Tubishat et al., 2018; Balazs and Vel\u00e1squez, 2016; Maynard and Greenwood, 2014; Pt\u00e1\u010dek et al., 2014; Bouazizi and Otsuki Ohtsuki, 2016; Ren et al., 2018) . We introduce the following example: \"The movie was enjoyable to the point that I clapped because it is finished.\" For an opinion mining system, this sentence could be considered positive. However, the author expresses a negative judgment against the movie since the expression \"I clapped because it is finished\" means that it was boring. * contributed equally 1 https://dictionary.cambridge.org/fr/ dictionnaire/anglais/sarcasm For this reason, SemEval 2022 set up Task 6: iSarcasmEval -Intended Sarcasm Detection in English and Arabic to detect sarcastic and nonsarcastic tweets (Abu Farha et al., 2022) . Our submitted system consisted of a pre-trained transformer model for English Tweets named BERTweet (Nguyen et al., 2020) , secured 8th and 5th positions respectively on Sub-Task A and Sub-Task C leaderboard. The rest of the paper is structured in the following manner: Section 2 provides the data structure and the main objective of each Sub-Task. Section 3 describes our system. Section 4 details the experiments. And finally, Section 5 concludes this paper. Task Description The organizers of this task introduced two tweet datasets for both English and Arabic languages that contain: \u2022 a label specifying whether a tweet is sarcastic or non-sarcastic, provided by its author. \u2022 a non-sarcastic rephrase of a sarcastic tweet provided by its author. \u2022 a label specifying the category of ironic speech that it reflects, provided by a linguistic expert (English only). \u2022 a label specifying the dialect (Arabic only). This task consists mainly of three sub-tasks for the English dataset and two sub-tasks for the Arabic dataset where Sub-Task A aims at determining whether a tweet is sarcastic or non-sarcastic, Sub-Task B, which is available for English only, is a binary multi-label classification task that intends to determine which ironic speech category a sarcastic tweet belongs to if any, and finally, Sub-Task C that takes two inputs: a sarcastic tweet and its non-sarcastic rephrase, and focuses on identifying the sarcastic one between them. System Description In this section, we describe our proposed system that tackles Sub-Task A and Sub-Task C English. Sub-Task A In order to tackle Sub-Task A, we adopted a transformer-based (Vaswani et al., 2017) approach that consists of fine-tuning BERTweet 2 , which is a language model pre-trained on 850M English Tweets, and it has the same architecture as BERTbase (Devlin et al., 2019) , as well as it is was trained using the RoBERTa pre-training procedure (Liu et al., 2019) . Before feeding the training data to BERTweet model, we preprocessed them by removing URLs and then replacing emojis with their English textual meaning (Alami et al., 2020) using BERTweet demojizer 3 . Figure 1 depicts the Tweets preprocessing pipeline. After the preprocessing phase, we fine-tuned BERTweet model on the training dataset that contains 3468 tweets (867 sarcastic tweets and 2601 non-sarcastic tweets). Sub-Task C The same model of Sub-Task A was used to handle Sub-Task C by feeding two texts to the BERTWeet model that was already fine-tuned on the training dataset. The text with the highest probability of being sarcastic is considered the sarcastic one. Experimental Results We experimented our model on the SemEval 2022 Task 6: iSarcasmEval -Intended Sarcasm Detection in English Sub-Task A and Sub-Task C datasets. All our experiments have been conducted in Google Colab environment 4 , The following libraries: Transformers -Hugging Face 5 (Wolf et al., 2020) , Scikit-Learn 6 (Pedregosa et al., 2011) , and Keras 7 were used to train and to asses the performance of our model. Evaluation Metric To evaluate the performance of the submitted results, the organizers adopted the F1-score for the sarcastic class as the main metric for Sub-Task A as well as the accuracy for Sub-Task C. The F1score and accuracy are computed in the following manner where P sarcastic and R sarcastic are respectively the precision and recall of the sarcastic class, and T P , T N , F P and F N are respectively the true positive, true negative, false positive and false negative. Accuracy = T P + T N T P + T N + F P + F N (1) P sarcastic = T P sarcastic T P sarcastic + F P sarcastic (2) R sarcastic = T P sarcastic T P sarcastic + F N sarcastic (3) F 1 sarcastic = 2 \u00d7 P sarcastic \u00d7 R sarcastic P sarcastic + R sarcastic (4) Experimental Settings During the fine-tuning of BERTWeet model, we set the hyper-parameters as follows: 10 \u22125 as the learning rate, 15 epochs, 128 as the max sequence length, and 32 as batch size. The same settings were adopted for DistilBERT (Sanh et al., 2019) and BERT base uncased. Table 1 summarizes the hyperparameters settings of BERTWeet model. For the Bidirectional Long Short-Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) and Bidirectional Gated Recurrent Unit (Bi-GRU) (Cho et al., 2014) , we set 10 epochs, 128 as the max sequence length, and 16 as batch size. System Performance We evaluated various models on Sub-Task A test set including Linear Support Vector Classification (LinearSVC) (Boser et al., 1992) , Logistic Regression, Multinomial Naive Bayes (MultinomialNB), Bi-LSTM, Bi-GRU, DistilBERT, BERT base uncased, RoBERTa base, and BERTweet base. We picked the combination of unigrams, bigrams, and trigrams of token counts as features for LinearSVC, Logistic Regression, and MultinomialNB since this combination delivered the best results in terms of the F-1 sarcastic metric. For non-transformer-based models, we preprocessed the data by removing stop words and special characters. For transformer-based models, two approaches were adopted during the evaluation phase. In the first approach, we preprocessed the data as described in Figure 1 . In the second one, we fine-tuned the model without applying any preprocessing to the data. Table 2 depicts the obtained results of various models on Sub-Task A -English. We can see from Table 2 that BERTweet base model achieved the best results in detecting sarcastic tweets succeeded by RoBERTa base. Surprisingly, LinearSVC achieved better results than BERT base and DistilBERT. We evaluated various models on Sub-Task C test set including LinearSVC, Logistic Regression, MultinomialNB, Bi-LSTM, Bi-GRU, RoBERTa base, and BERTweet base. Table 3 depicts the obtained results of various models on Sub-Task C -English. We mention that the same preprocessing approaches applied on Sub-Task A tweets were applied on Sub-Task C test set. According to the reported results in Table 3 , we can see that BERTweet base model achieved the best results succeeded by RoBERTa base. Moreover, we notice that traditional machine learning approaches such as LinearSVC, Logistic Regression, and MultinomialNB outperformed Recurrent Neural Networks: Bi-LSTM and Bi-GRU. Conclusion In this paper, we described our approach for tackling Sub-Task A and Sub-Task C of SemEval 2022 Task 6: iSarcasmEval -Intended Sarcasm Detection in English. Our submitted system consisted of a pre-trained transformer model for English Tweets named BERTweet, secured 8th and 5th positions respectively on Sub-Task A and Sub-Task C leaderboard. Since the top-ranked system for the English Sub-Task A scored about 0.6052 F1-score for the sarcastic class, future studies and works will focus on improving the performance of sarcasm detection tasks by adopting other approaches such as data augmentation and oversampling. Sub-Task",
    "abstract": "In this paper, we present our system and findings for SemEval-2022 Task 6 -iSarcasmEval: Intended Sarcasm Detection in English. The main objective of this task was to identify sarcastic tweets. This task was challenging mainly due to (1) the small training dataset that contains only 3468 tweets and (2) the imbalanced class distribution (25% sarcastic and 75% non-sarcastic). Our submitted model (ranked eighth on Sub-Task A and fifth on Sub-Task C) consists of a Transformer-based approach (BERTweet model).",
    "countries": [
        "Morocco"
    ],
    "languages": [
        "English"
    ],
    "numcitedby": "0",
    "year": "2022",
    "month": "July",
    "title": "{LISACT}eam at {S}em{E}val-2022 Task 6: A Transformer based Approach for Intended Sarcasm Detection in {E}nglish Tweets"
}