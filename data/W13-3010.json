{
    "article": "The consistency method has been established as the standard strategy for extracting high quality translation rules in statistical machine translation (SMT). However, no attention has been drawn to why this method is successful, other than empirical evidence. Using concepts from graph theory, we identify the relation between consistency and components of graphs that represent word-aligned sentence pairs. It can be shown that phrase pairs of interest to SMT form a sigma-algebra generated by components of such graphs. This construction is generalized by allowing segmented sentence pairs, which in turn gives rise to a phrase-based generative model. A by-product of this model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence. Introduction A parallel corpus, i.e., a collection of sentences in a source and a target language, which are translations of each other, is a core ingredient of every SMT system. It serves the purpose of training data, i.e., data from which translation rules are extracted. In its most basic form, SMT does not require the parallel corpus to be annotated with linguistic information, and human supervision is thus restricted to the construction of the parallel corpus. The extraction of translation rules is done by appropriately collecting statistics from the training data. The pioneering work of (Brown et al., 1993) identified the minimum assumptions that should be made in order to extract translation rules and developed the relevant models that made such extractions possible. These models, known as IBM models, are based on standard machine learning techniques. Their output is a matrix of word alignments for each sentence pair in the training data. These word alignments provide the input for later approaches that construct phrase-level translation rules which may (Wu, 1997; Yamada and Knight, 2001) or may not (Och et al., 1999; Marcu and Wong, 2002) rely on linguistic information. The method developed in (Och et al., 1999) , known as the consistency method, is a simple yet effective method that has become the standard way of extracting (source, target)-pairs of phrases as translation rules. The development of consistency has been done entirely on empirical evidence and it has thus been termed a heuristic. In this work we show that the method of (Och et al., 1999) actually encodes a particular type of structural information induced by the word alignment matrices. Moreover, we show that the way in which statistics are extracted from the associated phrase pairs is insufficient to describe the underlying structure. Based on these findings we suggest a phraselevel model in the spirit of the IBM models. A key aspect of the model is that it identifies the most likely partitions, rather than alignment maps, associated with appropriately chosen segments of the training data. For that reason, we provide a general construction of probability mass functions for partitions and, in particular, an exact formula for the probability of a segmentation of a sentence. Definition of Consistency In this section we provide the definition of consistency, which was introduced in (Och et al., 1999) , refined in (Koehn et al., 2003) , and we follow (Koehn, 2009) in our description. We start with some preliminary definitions. Let S = s 1 ...s |S| be a source sentence, i.e., a string that consists of consecutive source words; each word s i is drawn from a source language vocabulary and i indicates the position of the word in S. The operation of string extraction from the words of S is defined as the construction of the string s = s i 1 ...s in from the words of S, with 1 \u2264 i 1 < ... < i n \u2264 |S|. If i 1 , ..., i n are consecutive, which implies that s is a substring of S, then s is called a source phrase and we write s \u2286 S. As a shorthand we also write s in i 1 for the phrase s i 1 ...s in . Similar definitions apply to the target side and we denote by T, t j and t a target sentence, word and phrase respectively. Let (S = s 1 s 2 ...s |S| , T = t 1 t 2 ...t |T | ) be a sentence pair and let A denote the |S|\u00d7|T | matrix that encodes the existence/absence of word alignments in (S, T ) as A(i, j) = 1, if s i and t j are aligned 0, otherwise, (1) for all i = 1, ..., |S| and j = 1, .. A(i k , j) = 1, then j \u2208 {j 1 , ..., j |t| }. 3. \u2200l \u2208 {1, ..., |t|} such that A(i, j l ) = 1, then i \u2208 {i 1 , ..., i |s| }. 4. \u2203k \u2208 {1, ..., |s|} and \u2203l \u2208 {1, ..., |t|} such that A(i k , j l ) = 1. Condition 1 guarantees that (s, t) is a phrase pair and not just a pair of strings. Condition 2 says that if a word in s is aligned to one or more words in T , then all such target words must appear in t. Condition 3 is the equivalent of Condition 2 for the target words. Condition 4 guarantees the existence of at least one word alignment in (s, t). For a sentence pair (S, T ), the set of all consistent pairs with an alignment matrix A is denoted by P (S, T ). Figure 1 (a) shows an example of a sentence pair with an alignment matrix together with all its consistent pairs. In SMT the extraction of each consistent pair (s, t) from (S, T ) is followed by a statistic f (s, t; S, T ). Typically f (s, t; S, T ) counts the occurrences of (s, t) in (S, T ). By considering all sentence pairs in the training data, the translation probability is constructed as p(t|s) = (S,T ) f (s, t; S, T ) (S,T ) t f (s, t ; S, T ) , (2) and similarly for p(s|t). Finally, the entries of the phrase table consist of all extracted phrase pairs, their corresponding translation probabilities and other models which we do not discuss here. Consistency and Components For a given sentence pair (S, T ) and a fixed word alignment matrix A, our aim is to show the equivalence between consistency and connectivity properties of the graph formed by (S, T ) and A. Moreover, we explain that the way in which measurements are performed is not compatible , in principle, with the underlying structure. We start with some basic definitions from graph theory (see for example (Harary, 1969) ). Let G = (V, E) be a graph with vertex set V and edge set E. Throughout this work, vertices represent words and edges represent word alignments, but the latter will be further generalized in Section 4. A subgraph H = (V , E ) of G is a graph with V \u2286 V , E \u2286 E and the property that for each edge in E , both its endpoints are in V . A path in G is a sequence of edges which connect a sequence of distinct vertices. Two vertices u, v \u2208 V are called connected if G contains a path from u to v. G is said to be connected if every pair of vertices in G is connected. A connected component, or simply component, of G is a maximal connected subgraph of G. G is called bipartite if V can be partitioned in sets V S and V T , such that every edge in E connects a vertex in V S to one in V T . The disjoint union of graphs, or simply union, is an operation on graphs defined as follows. For n graphs with disjoint vertex sets V 1 , ..., V n (and hence disjoint edge sets), their union is the graph (\u222a n i=1 V i , \u222a n i=1 E i ). Consider the graph G whose vertices are the words of the source and target sentences, and whose edges are induced by the non-zero entries { } of the matrix A. There are no edges between any two source-type vertices nor between any two target-type vertices. Moreover, the source and target language vocabularies are assumed to be disjoint and thus G is bipartite. The set of all components of G is defined as C 1 and let k denote its cardinality, i.e., |C 1 | = k. From the members of C 1 we further construct sets C 2 , ..., C k as follows: C 1 = G s 1 s 3 t 1 t 4 t 5 s 2 s 4 t 2 t 3 t 6 s 5 t 7 { } C 2 = s 1 s 3 t 1 t 4 t 5 t 6 s 5 t 7 s 2 s 4 t 2 t 3 t 6 s 2 s 4 t 2 t 3 s 5 t 7 s 1 s 3 t 1 t 4 t 5 s 1 s 3 t 1 t 4 t 5 s 2 s 4 t 2 t 3 s 5 t 7 { } C 3 = s 1 s 3 t 1 t 4 t 5 t 6 s 5 t 7 s 2 s 4 t 2 t 3 t 6 s 2 s 4 t 2 t 3 s 5 t 7 s 1 s 3 t 1 t 4 t 5 t 6 s 1 s 3 t 1 t 4 t 5 s 2 s 4 t 2 t 3 { } C 4 = t 6 s 5 t 7 (s 5 , t 7 ) , (s 1 4 , t 1 5 ) ,(s 5 , t 6 7 ) , ( s 1 4 , t 1 6 ) , (S , T ) (a) (b) P (S , T )= { } , , , , , , , , , , Figure 1: For each i, 2 \u2264 i \u2264 k, any member of C i is formed by the union of any i distinct members of C 1 . In other words, any member of C i is a graph with i components and each such component is a member of C 1 . The cardinality of C i is clearly k i , for every i, 1 \u2264 i \u2264 k. Note that C k = {G}, since G is the union of all members of C 1 . Moreover, observe that C * = \u222a k i=1 C i is the set of graphs that can be generated by all possible unions of G's components. In that sense C = {\u2205} \u222a C * (3) is the power set of G. Indeed we have |C| = 1 + k i=1 k i = 2 k as required. 1 Figure 1(b) shows the graph G and the associated sets C i of (S, T ) and A in Figure 1(a) . Note the bijective correspondence between consistent 1 Here we used the fact that for any set X with |X| = n, the set of all subsets of X, i.e., the power set of X, has cardinality P n i=0 `n i \u00b4= 2 n . pairs and the phrase pairs that can be extracted from the vertices of the members of the sets C i . This is a consequence of consistency Conditions 2 and 3, since they provide the sufficient conditions for component formation. In general, if a pair of strings (s, t) satisfies the consistency Conditions 2 and 3, then it can be extracted from the vertices of a graph in C i , for some i. Moreover, if Conditions 1 and 4 are also satisfied, i.e., if (s, t) is consistent, then we can write P (S, T ) = k i=1 (S H , T H ) : H \u2208 C i , S H \u2286 S, T H \u2286 T , (4) where S H denotes the extracted string from the source-type vertices of H, and similarly for T H . Having established this relationship, when referring to members of C, we henceforth mean either consistent pairs or inconsistent pairs. The latter are pairs (S H , T H ) for some H \u2208 C such that at least either S H \u2286 S or T H \u2286 T . The construction above shows that phrase pairs of interest to SMT are part of a carefully constructed subclass of all possible string pairs that can be extracted from (S, T ). The power set C of G gives rise to a small, possibly minimal, set in which consistent and inconsistent pairs can be measured. 1 In other words, since C is (by construction) a sigma-algebra, the pair (C 1 , C) is a measurable space. Furthermore, one can construct a measure space (C 1 , C, f ), with an appropriately chosen measure f : C \u2192 [0, \u221e). Is the occurrence-counting measure f of Section 2 a good choice? Fix an ordering for C i , and let C i,j denote the jth member of C i , for all i, 1 \u2264 i \u2264 k. Furthermore, let \u03b4(x, y) = 1, if x = y and 0, otherwise. We argue by contradiction that the occurrence-counting measure f (H) = {H : H \u2208C, H is consistent} \u03b4(H, H ), (5) fails to form a measure space. Suppose that more than one component of G is consistent, i.e., suppose that 1 < k j=1 f (C 1,j ) \u2264 k. ( 6 ) By construction of C, it is guaranteed that 1 = f (G) = f (C k,1 ) = f (\u222a k j=1 C 1,j ). (7) The members of C 1 are pairwise disjoint, because each of them is a component of G. Thus, since f is assumed to be a measure, sigma-additivity should be satisfied, i.e., we must have f (\u222a k j=1 C 1,j ) = k j=1 f (C 1,j ) > 1, (8) which is a contradiction. In practice, the deficiency of using eq. 5 as a statistic could possibly be explained by the fact that the so-called lexical weights are used as smoothing. Consistency, Components and Segmentations In Section 3 the only relation that was assumed among source (target) words/vertices was the order of appearance in the source (target) sentence. As a result, the graph representation G of (S, T ) and A was bipartite. There are several, linguistically motivated, ways in which a general graph can be obtained from the bipartite graph G. We explain that the minimal linguistic structure, namely 1 See Appendix for definitions. sentence segmentations, can provide a generalization of the construction introduced in Section 3. Let X be a finite set of consecutive integers. A consecutive partition of X is a partition of X such that each part consists of integers consecutive in X. A segmentation \u03c3 of a source sentence S is a consecutive partition of {1, ..., |S|}. A part of \u03c3, i.e., a segment, is intuitively interpreted as a phrase in S. In the graph representation G of (S, T ) and A, a segmentation \u03c3 of S is realised by the existence of edges between consecutive source-type vertices whose labels, i.e., word positions in S, appear in the same segment of \u03c3. The same argument holds for a target sentence and its words; a target segmentation is denoted by \u03c4 . Clearly, there are 2 |S|\u22121 possible ways to segment S and, given a fixed alignment matrix A, the number of all possible graphs that can be constructed is thus 2 |S|+|T |\u22122 . The bipartite graph of Section 3 is just one possible configuration, namely the one in which each segment of \u03c3 consists of exactly one word, and similarly for \u03c4 . We denote this segmentation pair by (\u03c3 0 , \u03c4 0 ). We now turn to extracting consistent pairs in this general setting from all possible segmentations (\u03c3, \u03c4 ) for a sentence pair (S, T ) and a fixed alignment matrix A. As in Section 3, we construct graphs G \u03c3,\u03c4 , associated sets C \u03c3,\u03c4 i , for all i, 1 \u2264 i \u2264 k \u03c3,\u03c4 , and C \u03c3,\u03c4 , for all (\u03c3, \u03c4 ). Consistent pairs are extracted in lieu of eq. 4, i.e., P \u03c3,\u03c4 (S, T ) = k \u03c3,\u03c4 i=1 (S H , T H ) : H \u2208 C \u03c3,\u03c4 i , S \u2286 S, T H \u2286 T , (9) and it is trivial to see that {(S, T )} \u2286 P \u03c3,\u03c4 (S, T ) \u2286 P (S, T ), (10) for all (\u03c3, \u03c4 ). Note that P (S, T ) = P \u03c3 0 ,\u03c4 0 (S, T ) and, depending on the details of A, it is possible for other pairs (\u03c3, \u03c4 ) to attain equality. Moreover, each consistent pair in P (S, T ) can be be extracted from a member of at least one C \u03c3,\u03c4 . We focus on the sets C \u03c3,\u03c4 1 , i.e., the components of G \u03c3,\u03c4 , for all (\u03c3, \u03c4 ). In particular, we are interested in the relation between P (S, T ) and C \u03c3,\u03c4 1 , for all (\u03c3, \u03c4 ). Each consistent H \u2208 C \u03c3 0 ,\u03c4 0 can be converted into a single component by appropriately forming edges between consecutive sourcetype vertices and/or between consecutive targettype vertices. The resulting component will evidently be a member of C \u03c3,\u03c4 1 , for some (\u03c3, \u03c4 ). It is important to note that the conversion of a consistent H \u2208 C \u03c3 0 ,\u03c4 0 into a single component need not be unique; see Figure 2 for a counterexample. Since (a) such conversions are possible for all consistent H \u2208 C \u03c3 0 ,\u03c4 0 and (b) P (S, T ) = P \u03c3 0 ,\u03c4 0 (S, T ), it can be deduced that all possible consistent pairs can be traced in the sets C \u03c3,\u03c4 1 , for all (\u03c3, \u03c4 ). In other words, we have: P (S, T ) = \u03c3,\u03c4 (S H , T H ) : H \u2208 C \u03c3,\u03c4 1 , S H \u2286 S, T H \u2286 T . ( 11 ) The above equation says that by taking sentence segmentations into account, we can recover all possible consistent pairs, by inspecting only the components of the underlying graphs. It would be interesting to investigate the relation between measure spaces (C \u03c3,\u03c4 1 , C \u03c3,\u03c4 , f \u03c3,\u03c4 ) and different configurations for A. We leave that for future work and focus on the advantages provided by eq. 11. s 2 s 1 s 3 s 4 t 1 t 2 t 3 s 2 s 1 s 3 s 4 t 1 t 2 t 3 s 2 s 1 s 3 s 4 t 1 t 2 t 3 s 2 s 1 s 3 s 4 t 1 t 2 t 3 s 2 s 1 s 3 s 4 t 1 t 2 t 3 Towards a phrase-level model that respects consistency The aim of this section is to exploit the relation established in eq. 11 between consistent pairs and components of segmented sentence pairs. It was also shown in Section 2 that the computation of the translation models is inappropriate to describe the underlying structure. We thus suggest a phrasebased generative model in the spirit of the IBM word-based models, which is compatible with the construction of the previous sections. Hidden variables All definitions from the previous sections are carried over, and we introduce a new quantity that is associated with components. Let G \u03c3,\u03c4 and C \u03c3,\u03c4 1 , for some (\u03c3, \u03c4 ) be as in Section 4, then the set K is defined as follows: Each member of K is a pair of (source, target) sets of segments that corresponds to the pair of (source, target) vertices of a consistent member of C \u03c3,\u03c4 1 . In other words, K is a bisegmentation of a pair of segmented sentences that respects consistency. Figure 3 shows three possible ways to construct consistent graphs from (S, T ) = (s 4 1 , t 6 1 ), \u03c3 = {{1, 2}, {3}, {4}} \u2261 {x 1 , x 2 , x 3 } and \u03c4 = {{1}, {2, 3, 4}, {5}, {6}} \u2261 {y 1 , y 2 , y 3 , y 4 }. In each case the exact alignment information is unknown and we have: (a) K = {x 1 }, {y 1 } , {x 2 }, {y 2 } , {x 3 }, {y 3 , y 4 } . (b) K = {x 1 , x 2 }, {y 1 , y 2 , y 3 } , {x 3 }, {y 4 } . (c) K = {x 1 }, {y 3 , y 4 } , {x 2 , x 3 }, {y 1 , y 2 } . t 1 s 1 s 2 s 3 t 2 t 3 t 5 s 4 t 4 t 6 t 1 s 1 s 2 s 3 t 2 t 3 t 5 t 4 t 6 s 4 t 1 s 1 s 2 s 3 t 2 t 3 t 5 t 4 t 6 s 4 (a) (b) (c) Figure 3 : Three possible ways to construct consistent graphs for (s 4 1 , t 6 1 ) and a given segmentation pair. Exact word alignment information is unknown. In the proposed phrase-level generative model the random variables whose instances are \u03c3, \u03c4 and K are hidden variables. As with the IBM models, they are associated with the positions of words in a sentence, rather than the words themselves. Alignment information is implicitly identified via the consistent bisegmentation K. Suppose we have a corpus that consists of pairs of parallel sentences (S, T ), and let f S,T denote the occurrence count of (S, T ) in the corpus. Also, let l S = |S| and l T = |T |. The aim is to maximize the corpus log-likelihood function = S,T f S,T log p \u03b8 (T |S) = S,T f S,T log \u03c3,\u03c4,K p \u03b8 (T, \u03c3, \u03c4, K|S), (12) where \u03c3, \u03c4 and K are hidden variables parameterized by a vector \u03b8 of unknown weights, whose values are to be determined. The expectation maximization algorithm (Dempster et al., 1977) suggests that an iterative application of \u03b8 n+1 = arg max \u03b8 S,T f S,T \u03c3,\u03c4,K p \u03b8n (\u03c3, \u03c4, K|S, T )\u00d7 log p \u03b8 (T, \u03c3, \u03c4, K|S), (13) provides a good approximation for the maximum value of . As with the IBM models we seek probability mass functions (PMFs) of the form p \u03b8 (T, \u03c3, \u03c4, K|S) = p \u03b8 (l T |S)p \u03b8 (\u03c3, \u03c4, K|l T , S)\u00d7 p \u03b8 (T |\u03c3, \u03c4, K, l T , S), (14) and decompose further as p \u03b8 (\u03c3, \u03c4, K|l T , S) = p \u03b8 (\u03c3, \u03c4 |l T , S)p \u03b8 (K|\u03c3, \u03c4, l T , S) (15) A further simplification of p \u03b8 (\u03c3, \u03c4 |l T , S) = p \u03b8 (\u03c3|S)p \u03b8 (\u03c4 |l T ) may not be desirable, but will help us understand the relation between \u03b8 and the PMFs. In particular, we give a formal description of p \u03b8 (\u03c3|S) and then explain that p \u03b8 (K|\u03c3, \u03c4, l T , S) and p \u03b8 (T |\u03c3, \u03c4, K, l T , S) can be computed in a similar way. Constrained, biased sampling without replacement The probability of a segmentation given a sentence can be realised in two different ways. We first provide a descriptive approach which is more intuitive, and we use the sentence S = s 4 1 as an ex-ample whenever necessary. The set of all possible segments of S is denoted by seg(S) and trivially |seg(S)| = |S| |S| + 1 /2. Each segment x \u2208 seg(S) has a nonnegative weight \u03b8(x|l S ) such that x\u2208seg(S) \u03b8(x|l S ) = 1. (16) Suppose we have an urn that consists of |seg(S)| weighted balls; each ball corresponds to a segment of S. We sample without replacement with the aim of collecting enough balls to form a segmentation of S. When drawing a ball x we simultaneously remove from the urn all other balls x such that x \u2229 x = \u2205. We stop when the urn is empty. In our example, let the urn contain 10 balls and suppose that the first draw is {1, 2}. In the next draw, we have to choose from {3}, {4} and {3, 4} only, since all other balls contain a '1' and/or a '2' and are thus removed. The sequence of draws that leads to a segmentation is thus a path in a decision tree. Since \u03c3 is a set, there are |\u03c3|! different paths that lead to its formation. The set of all possible segmentations, in all possible ways that each segmentation can be formed, is encoded by the collection of all such decision trees. The second realisation, which is based on the notions of cliques and neighborhoods, is more constructive and will give rise to the desired PMF. A clique in a graph is a subset U of the vertex set such that for every two vertices u, v \u2208 U , there exists an edge connecting u and v. For any vertex u in a graph, the neighborhood of u is defined as the set N (u) = {v : {u, v} is an edge}. A maximal clique is a clique U that is not a subset of a larger clique: For each u \u2208 U and for each v \u2208 N (u) the set U \u222a {v} is not a clique. Let G be the graph whose vertices are all segments of S and whose edges satisfy the condition that any two vertices x and x form an edge iff x \u2229 x = \u2205; see Figure 4 for an example. G essentially provides a compact representation of the decision trees discussed above. It is not difficult to see that a maximal clique also forms a segmentation. Moreover, the set of all maximal cliques in G is exactly the set of all possible segmentations for S. Thus, p \u03b8 (\u03c3|S) should satisfy p \u03b8 (\u03c3|S) = 0, if \u03c3 is not a clique in G, (17) and where the sum is over all maximal cliques in G. \u03c3 p \u03b8 (\u03c3|S) = 1, (18) In our example p \u03b8 { {1}, {1, 2} }|S = 0, because there is no edge connecting segments {1} and {1, 2} so they are not part of any clique. In order to derive an explicit formula for p \u03b8 (\u03c3|S) we focus on a particular type of paths in G. A path is called clique-preserving, if every vertex in the path belongs to the same clique. Our construction should be such that each cliquepreserving path has positive probability of occurring, and all other paths should have probability 0. We proceed with calculating probabilities of clique-preserving paths based on the structure of G and the constraint of eq. 16. The probability p \u03b8 (\u03c3|S) can be viewed as the probability of generating all clique-preserving paths on the maximal clique \u03c3 in G. Since \u03c3 is a clique, there are |\u03c3|! possible paths that span its vertices. Let \u03c3 = {x 1 , ..., x |\u03c3| }, and let \u03c0 denote a permutation of {1, ..., |\u03c3|}. We are interested in computing the probability q \u03b8 (x \u03c0(1) , ..., x \u03c0(|\u03c3|) ) of generating a cliquepreserving path x \u03c0(1) , ..., x \u03c0(|\u03c3|) in G. Thus, p \u03b8 (\u03c3|S) = p \u03b8 ({x 1 , ..., x |\u03c3| }|S) = \u03c0 q \u03b8 (x \u03c0(1) , ..., x \u03c0(|\u03c3|) ) = \u03c0 q \u03b8 (x \u03c0(1) ) q \u03b8 (x \u03c0(2) |x \u03c0(1) ) \u00d7 ... ... \u00d7 q \u03b8 (x \u03c0(|\u03c3|) |x \u03c0(1) , ..., x \u03c0(|\u03c3|\u22121) ). ( 19 ) The probabilities q \u03b8 (\u2022) can be explicitly calculated by taking into account the following observation. A clique-preserving path on a clique \u03c3 can be realised as a sequence of vertices x \u03c0(1) , ..., x \u03c0(i) , ..., x \u03c0(|\u03c3|) with the following constraint: If at step i \u2212 1 of the path we are at vertex x \u03c0(i\u22121) , then the next vertex x \u03c0(i) should be a neighbor of all of x \u03c0(1) , ..., x \u03c0(i\u22121) . In other words we must have x \u03c0(i) \u2208 N \u03c0,i \u2261 i\u22121 l=1 N (x \u03c0(l) ). (20) Thus, the probability of choosing x \u03c0(i) as the next vertex of the path is given by q \u03b8 (x \u03c0(i) |x \u03c0(1) , ..., x \u03c0(i\u22121) ) = \u03b8(x \u03c0(i) |l S ) x\u2208N \u03c0,i \u03b8(x|l S ) , (21) if x \u03c0(i) \u2208 N \u03c0,i and 0, otherwise. When choosing the first vertex of the path (the root in the decision tree) we have N \u03c0,1 = seg(S), which gives q \u03b8 (x \u03c0(1) ) = \u03b8(x \u03c0(1) |l S ), as required. Therefore eq. 19 can be written compactly as p \u03b8 (\u03c3|S) = \uf8eb \uf8ed |\u03c3| i=1 \u03b8(x i |l S ) \uf8f6 \uf8f8 \u03c0 1 Q \u03b8 (\u03c3, \u03c0; S) , (22) where Q \u03b8 (\u03c3, \u03c0; S) = |\u03c3| i=1 x\u2208N \u03c0,i \u03b8(x|l S ) . (23) The construction above can be generalized in order to derive a PMF for any random variable whose values are partitions of a set. Indeed, by allowing the vertices of G to be a subset of a power set, and keeping the condition of edge formation the same, probabilities of clique-preserving paths can be calculated in the same way. Figure 5 shows the graph G that represents all possible instances of K with (S, T ) = (s 4 1 , t 5 1 ), \u03c3 = {1, 2}, {3}, {4} and \u03c4 = {1}, {2, 3, 4}, {5} . Again each maximal clique is a possible consistent bisegmentation. In order for this model to be complete, one should solve the maximization step of eq. 13 and calculate the posterior p \u03b8n (\u03c3, \u03c4, K|S, T ). We are not bereft of hope, as relevant techniques have been developed (see Section 6). Related Work To our knowledge, this is the first attempt to investigate formal motivations behind the consistency method.  ) and a given segmentation pair (see text). For clarity, we show the phrases that are formed from joining contiguous segments in each pair, rather than the segments themselves. Several phrase-level generative models have been proposed, almost all relying on multinomial distributions for the phrase alignments (Marcu and Wong, 2002; Zhang et al., 2003; Deng and Byrne 2005; DeNero et al., 2006; Birch et al., 2006) . This is a consequence of treating alignments as functions rather than partitions. Word alignment and phrase extraction via Inversion Transduction Grammars (Wu, 1997) , is a linguistically motivated method that relies on simultaneous parsing of source and target sentences (DeNero and Klein, 2010; Cherry and Lin 2007; Neubig et al., 2012) . The partition probabilities we introduced in Section 5.2 share the same tree structure discussed in (Dennis III, 1991) , which has found applications in Information Retrieval (Haffari and Teh, 2009) . Conclusions We have identified the relation between consistency and components of graphs that represent word-aligned sentence pairs. We showed that phrase pairs of interest to SMT form a sigmaalgebra generated by components of such graphs, but the existing occurrence-counting statistics are inadequate to describe this structure. A generalization of our construction via sentence segmentations lead to a realisation of random partitions as cases of constrained, biased sampling without re-placement. As a consequence, we derived an exact formula for the probability of a segmentation of a sentence. Appendix: Measure Space The following standard definitions can be found in, e.g., (Feller, 1971) . Let X be a set. A collection B of subsets of X is called a sigma-algebra if the following conditions hold: 1. \u2205 \u2208 B. 2. If E is in B, then so is its complement X \\ E. 3. If {E i } is a countable collection of sets in B, then so is their union \u222a i E i . Condition 1 guarantees that B is non-empty and Conditions 2 and 3 say that B is closed under complementation and countable unions respectively. The pair (X, B) is called a measurable space. A function f : B \u2192 [0, \u221e) is called a measure if the following conditions hold: 1. f (\u2205) = 0. 2. If {E i } is a countable collection of pairwise disjoint sets in B, then f (\u222a i E i ) = i f (E i ). Condition 2 is known as sigma-additivity. The triple (X, B, f ) is called a measure space. Acknowledgments This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430 (GALATEAS) and by the EC funded project CoSyne (FP7-ICT-4-24853).",
    "abstract": "The consistency method has been established as the standard strategy for extracting high quality translation rules in statistical machine translation (SMT). However, no attention has been drawn to why this method is successful, other than empirical evidence. Using concepts from graph theory, we identify the relation between consistency and components of graphs that represent word-aligned sentence pairs. It can be shown that phrase pairs of interest to SMT form a sigma-algebra generated by components of such graphs. This construction is generalized by allowing segmented sentence pairs, which in turn gives rise to a phrase-based generative model. A by-product of this model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence.",
    "countries": [
        "Netherlands"
    ],
    "languages": [
        "Wu"
    ],
    "numcitedby": "2",
    "year": "2013",
    "month": "August",
    "title": "Investigating Connectivity and Consistency Criteria for Phrase Pair Extraction in Statistical Machine Translation"
}