{
    "article": "Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize cooccurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community 1 . for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models (Haffner et al., 2003; Sarikaya et al., 2011) for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs (Xu and Sarikaya, 2013; Gupta et al., 2019) , RNNs (Guo et al., 2014a; Liu and Lane, 2016) , and asynchronous bi-model (Wang et al., 2018) . Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure 1 and Table 1 . Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task # Utterance Slot tag Intent 1 play Roy Orbison tunes now artist PlayMusic 2 add this Roy Orbison song onto Women of Comedy artist AddToPlaylist 3 book a spot for seven at a bar with chicken french served dish BookRestaurant 4 book french food for me and angeline at a restaurant cuisine BookRestaurant Table 1 : Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018) , while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a ) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling (Mesnil et al., 2014) , is not explicitly modeled. In this paper, we try to address these issues, and thus propose a novel Collaborative Memory Network, named CM-Net. The main idea is to directly capture semantic relationships among words, slots and intents, which is conducted simultaneously at each word position in a collaborative manner. Specifically, we alternately perform information exchange among the task-specific features referred from memories, local context representations and global sequential information via the well-designed block, named CM-block, which consists of three computational components: \u2022 Deliberate Attention: Obtaining slotspecific and intent-specific representations from memories in a collaborative manner. Above components in each CM-block are conducted consecutively, which are responsible for encoding information from different perspectives. Finally, multiple CM-blocks are stacked together, and construct our CM-Net. We firstly conduct experiments on two popular benchmarks, SNIPS (Coucke et al., 2018) and ATIS (Hemphill et al., 1990; Tur et al., 2010) . Experimental results show that the CM-Net achieves the state-of-the-art results in 3 of 4 criteria (e.g., intent detection accuracy on ATIS) on both benchmarks. Additionally, trials on our self-collected dataset, named CAIS, demonstrate the effectiveness and generalizability of the CM-Net. Our main contributions are as follows: \u2022 We propose a novel CM-Net for SLU, which explicitly captures semantic correlations among words, slots and intents in a collaborative manner, and incrementally enriches the specific features, local context representations and global sequential representations through stacked CM-blocks. \u2022 Our CM-Net achieves the state-of-the-art results on two major SLU benchmarks (ATIS and SNIPS) in most of criteria. \u2022 We contribute a new corpus CAIS with manual annotations of slot tags and intent labels to the research community. Background In principle, the slot filling is treated as a sequence labeling task, and the intent detection is a classification problem. Formally, given an utterance X = {x 1 , x 2 , \u2022 \u2022 \u2022 , x N } with N words and its corresponding slot tags Y slot = {y 1 , y 2 , \u2022 \u2022 \u2022 , y N }, the slot filling task aims to learn a parameterized mapping function f \u03b8 : X \u2192 Y from input words to slot tags. For the intent detection, it is designed to predict the intent label \u0177int for the entire utterance X from the predefined label set S int . Typically, the input utterance is firstly encoded into a sequence of distributed representations X = {x 1 , x 2 , \u2022 \u2022 \u2022 , x N } by character-aware and pretrained word embeddings. Afterwards, the following bidirectional RNNs are applied to encode the embeddings X into context-sensitive representations H = {h 1 , h 2 , \u2022 \u2022 \u2022 , h N }. An external CRF (Lafferty et al., 2001) layer is widely utilized to calculate conditional probabilities of slot tags: p(y slot |H) = e F (H,y slot ) y slot \u2208Yx e F (H, y slot ) (1) Here Y x is the set of all possible sequences of tags, and F (\u2022) is the score function calculated by: F (h, y) = N i=1 A y i ,y i+1 + N i=1 P i,y i (2) where A is the transition matrix that A i,j indicates the score of a transition from i to j, and P is the score matrix output by RNNs. P i,j indicates the score of the j th tag of the i th word in a sentence (Lample et al., 2016) . When testing, the Viterbi algorithm (Forney, 1973) is used to search the sequence of slot tags with maximum score: \u0177slot = arg max y slot \u2208Yx F (H, y slot ) (3) As to the prediction of intent, the word-level hidden states H are firstly summarized into a utterance-level representation v int via mean pooling (or max pooling or self-attention, etc.): v int = 1 N N i=1 h t (4) The most probable intent label \u0177int is predicted by softmax normalization over the intent label set: \u0177int = arg max y\u2208S int P ( y|v int ) P ( y = j|v int ) = sof tmax(v int )[j] (5) Generally, both tasks are trained jointly to minimize the sum of cross entropy from each individual task. Formally, the loss function of the join model is computed as follows: where y int i and y slot i,j are golden labels, and \u03bb is hyperparameter, and |S int | is the size of intent label set, and similarly for |S slot | . L = (1 \u2212 \u03bb) \u2022 L slot + \u03bb \u2022 L int L int = \u2212 |S int | i=1 \u0177int i log(y int i ) L slot = \u2212 N j=1 |S slot | i=1 \u0177slot i,j log(y slot i,j ) (6) CM-block \u2022\u2022\u2022 Slot Memory Intent Memory Inference Layer # 2 # 1 # L Embedding Layer CM-block x 1 \u2022\u2022\u2022 \u2022\u2022\u2022 CM-block y 1 y N y t \u2022\u2022\u2022 \u2022\u2022\u2022 y 2 y int slots x 2 x t x N CM-Net Overview In this section, we start with a brief overview of our CM-Net and then proceed to introduce each module. As shown in Figure 2 , the input utterance is firstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and finally make predictions in the Inference Layer. Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove 2 (Pennington et al., 2014) to initialize word embeddings, and keep them frozen. Character-aware Word Embedding It has been demonstrated that character level information (e.g. capitalization and prefix) (Collobert et al., 2011) is crucial for sequence labeling. We use one layer of CNN followed by max pooling to generate character-aware word embeddings. CM-block The CM-block is the core module of our CM-Net, which is designed with three computational com-ponents: Deliberate Attention, Local Calculation and Global Recurrence respectively. Deliberate Attention To fully model semantic relations between slots and intents, we build the slot memory M slot and intent memory M int , and further devise a collaborative retrieval approach. For the slot memory, it keeps |S slot | slot cells which are randomly initialized and updated as model parameters. Similarly for the intent memory. At each word position, we take the hidden state h t as query, and obtain slot feature h slot t and intent feature h int t from both memories by the deliberate attention mechanism, which will be illustrated in the following. Specifically for the slot feature h slot t , we firstly get a rough intent representation h int t by the wordaware attention with hidden state h t over the intent memory M int , and then obtain the final slot feature h slot t by the intent-aware attention over the slot memory M slot with the intent-enhanced representation [h t ; h int t ]. Formally, the abovementioned procedures are computed as follows: h int t = AT T (h t , M int ) h slot t = AT T ([h t ; h int t ], M slot ) (7) where AT T (\u2022) is the query function calculated by the weighted sum of all cells m x i in memory M x (x \u2208 {slot, int}) : AT T (h t , M x ) = i \u03b1 i m x i \u03b1 i = exp(u s i ) j exp(u s j ) s i = h t Wm x i (8) Here u and W are model parameters. We name the above calculations of two-round attentions (Equation 7 ) as \"deliberate attention\". The intent representation h int t is computed by the deliberate attention as well: h slot t = AT T (h t , M slot ) h int t = AT T ([h t ; h slot t ], M int ) (9) These two deliberate attentions are conducted simultaneously at each word position in such collaborative manner, which guarantees adequate knowledge diffusions between slots and intents. The retrieved slot features H slot t and intent features H int t are utilized to provide guidances for the next local calculation layer. h t int \u2022\u2022\u2022 h t+1 int Embeddings \u2022\u2022\u2022 x t-1 x t \u2022\u2022\u2022 x t+1 Bi-LSTM Slot Memory Intent Memory Global Recurrence Deliberate Attention Local Calculation Output states Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b) . Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the S-LSTM with slot-specific features H slot t and intentspecific features H slot t retrieved from memories. Specifically, at each input position t, we take the local window context \u03be t , word embedding x t , slot feature h slot t and intent feature h int t as inputs to conduct combinatorial calculation simultaneously. Formally, in the l th layer, the hidden state h t is updated as follows: \u03be l\u22121 t = [h l\u22121 t\u22121 , h l\u22121 t , h l\u22121 t+1 ] \u00eel t = \u03c3(W i 1 \u03be l\u22121 t + W i 2 x t + W i 3 h slot t + W i 4 h int t ) \u00f4l t = \u03c3(W o 1 \u03be l\u22121 t + W o 2 x t + W o 3 h slot t + W o 4 h int t ) f l t = \u03c3(W f 1 \u03be l\u22121 t + W f 2 x t + W f 3 h slot t + W f 4 h int t ) ll t = \u03c3(W l 1 \u03be l\u22121 t + W l 2 x t + W l 3 h slot t + W l 4 h int t ) rl t = \u03c3(W r 1 \u03be l\u22121 t + W r 2 x t + W r 3 h slot t + W r 4 h int t ) u l t = tanh(W u 1 \u03be l\u22121 t + W u 2 x t + W u 3 h slot t + W u 4 h int t ) i l t , f l t , l l t , r l t = sof tmax( \u00eel t , f l t , ll t , rl t ) c l t = f l t c l\u22121 t + l l t c l\u22121 t\u22121 + r l t c l\u22121 t+1 + i l t u l\u22121 t h l t = o l t tanh c l t (10) 1055 where \u03be l t is the concatenation of hidden states in a local window, and i l t , f l t , o l t , l l t and r l t are gates to control information flows, and W x n (x \u2208 {i, o, f, l, r, u}, n \u2208 {1, 2, 3, 4}) are model parameters. More details about the state transition can be referred in (Zhang et al., 2018b) . In the first CM-block, the hidden state h t is initialized with the corresponding word embedding. In other CM-blocks, the h t is inherited from the output of the adjacent lower CM-block. At each word position of above procedures, the hidden state is updated with abundant information from different perspectives, namely word embeddings, local contexts, slots and intents representations. The local calculation layer in each CMblock has been shown highly useful for both tasks, and especially for the slot filling task, which will be validated in our experiments in Section 5.2. Global Recurrence Bi-directional RNNs, especially the BiLSTMs (Hochreiter and Schmidhuber, 1997) are regarded to encode both past and future information of a sentence, which have become a dominant method in various sequence modeling tasks (Hammerton, 2003; Sundermeyer et al., 2012) . The inherent nature of BiLSTMs is able to supplement global sequential information, which is insufficiently modeled in the previous local calculation layer. Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By taking the slot-and intent-specific local context representations as inputs, we can obtain more specific global sequential representations. Formally, it takes the hidden state h l\u22121 t inherited from the local calculation layer as input, and conduct recurrent steps as follows: h l t = [ \u2212 \u2192 h l t ; \u2190 \u2212 h t l ] \u2212 \u2192 h l t = \u2212\u2212\u2212\u2212\u2192 LSTM(h l\u22121 t , \u2212 \u2192 h l t\u22121 ; \u2212 \u2192 \u03b8 ) \u2190 \u2212 h l t = \u2190\u2212\u2212\u2212\u2212 LSTM(h l\u22121 t , \u2190 \u2212 h l t+1 ; \u2190 \u2212 \u03b8 ) (11) The output \"states\" of the BiLSTMs are taken as \"states\" input of the local calculation in next CMblock. The global sequential information encoded by the BiLSTMs is shown necessary and effective for both tasks in our experiments in Section 5.2. Inference Layer After predictions upon the final CM-block. For the predictions of slots, we take the hidden states H along with the retrieved slot H slot representations (both are from the final CM-block) as input features, and then conduct predictions of slots similarly with the Equation (3) in Section 2: \u0177slot = arg max y slot \u2208Yx F ([H; H slot ], y slot ) (12) For the prediction of intent label, we firstly aggregate the hidden state h t and the retrieved intent representation h int t at each word position (from the final CM-block as well) via mean pooling: v int = 1 N N t [h t ; h int t ] (13) and then take the summarized vector v int as input feature to conduct prediction of intent consistently with the Equation (5) in Section 2. Experiments Datasets and Metrics We evaluate our proposed CM-Net on three realword datasets, and statistics are listed in Table 2 . ATIS The Airline Travel Information Systems (ATIS) corpus (Hemphill et al., 1990) dataset are more balanced when compared with the ATIS. We split another 700 utterances for validation set following previous works (Goo et al., 2018; Zhang et al., 2018a) . CAIS We collect utterances from the Chinese Artificial Intelligence Speakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme (Ratinov and Roth, 2009) in the sequence labeling field. Metrics Slot filling is typically treated as a sequence labeling problem, and thus we take the conlleval 4 as the token-level F 1 metric. The intent detection is evaluated with the classification accuracy. Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works (Tur et al., 2010; Zhang and Wang, 2016) , we count an utterrance as a correct classification if any ground truth label is predicted. Implementation Details All trainable parameters in our model are initialized by the method described in Glorot and Bengio (2010) . We apply dropout (Srivastava et al., 2014) to the embedding layer and hidden states with a rate of 0.5. All models are optimized by the Adam optimizer (Kingma and Ba, 2014) with gradient clipping of 3 (Pascanu et al., 2013) . The initial learning rate \u03b1 is set to 0.001, and decrease with the growth of training steps. We monitor the training process on the validation set and report the final result on the test set. One layer CNN with a filter of size 3 and max pooling are utilized to generate 100d word embeddings. The cased 300d Glove is adapted to initialize word embeddings, and kept fixed when training. In auxiliary experiments, the output hidden states of BERT are taken as additional word embeddings and kept fixed as well. We share parameters of both memories with the parameter matrices in the corresponding softmax layers, which can be taken as introducing supervised signals into the memories to some extent. We conduct hyper-parameters tuning for layer size (finally set to 3) and loss weight \u03bb (finally set to 0.5), and empirically set other parameters to the values listed in the supplementary material. Main Results Main results of our CM-Net on the SNIPS and ATIS are shown in Table 3 . Our CM-Net achieves the state-of-the-art results on both datasets in terms of slot filling F 1 score and intent detection Figure 4 : Investigations of the collaborative retrieval approach on slot filling (on the left) and intent detection (on the right), where \"no slot2int\" indicates removing slow-aware attention for the intent representation, and similarly for \"no int2slot\" and \"neither\". accuracy, except for the F 1 score on the ATIS. We conjecture that the named entity feature in the ATIS has a great impact on the slot filling result as illustrated in Section 4.1. Since the SNIPS is collected from multiple domains with more balanced labels when compared with the ATIS, the slot filling F 1 score on the SNIPS is able to demonstrate the superiority of our CM-Net. # Models SNIPS Slot (F 1 ) Intent (Acc) 0 CM- It is noteworthy that the CM-Net achieves comparable results when compared with models that exploit additional language models (Siddhant et al., 2018; Chen et al., 2019) . We conduct auxiliary experiments by leveraging the well-known BERT (Devlin et al., 2018) as an external resource for a relatively fair comparison with those models, and report details in Section 5.3. Analysis Since the SNIPS corpus is collected from multiple domains and its label distributions are more balanced when compared with the ATIS, we choose the SNIPS to elucidate properties of our CM-Net and conduct several additional experiments. Whether Memories Promote Each Other? In the CM-Net, the deliberate attention mechanism is proposed in a collaborative manner to perform information exchange between slots and intents. We conduct experiments to verify whether such kind of knowledge diffusion in both memories can promote each other. More specifically, we remove one unidirectional diffusion (e.g. from slot to intent) or both in each experimental setup. The results are illustrated in Figure 4 . We can observe obvious drops on both tasks when both directional knowledge diffusions are removed (CM-Net vs. neither). For the slot filling task (left part in Figure 4 ), the F 1 scores decrease slightly when the knowledge from slot to intent is blocked (CM-Net vs. \"no slot2int\"), and a more evident drop occurs when the knowledge from intent to slot is blocked (CM-Net vs. \"no int2slot\"). Similar observations can be found for the intent detection task (right part in Figure 4 ). In conclusion, the bidirectional knowledge diffusion between slots and intents are necessary and effective to promote each other. Ablation Experiments We conduct ablation experiments to investigate the impacts of various components in our CM-Net. In particular, we remove one component among slot memory, intent memory, local calculation and global recurrence. Results of different combinations are presented in Table 4 . Once the slot memory and its corresponding interactions with other components are removed, scores on both tasks decrease to some extent, and a more obvious decline occurs for the slot filling (row 1 vs. row 0), which is consistent with the conclusion of Section 5.1. Similar observations can be found for the intent memory (row 2). The local calculation layer is designed to capture better local context representations, which has an evident impact on the slot filling and slighter effect on the intent detection (row 3 vs. row 0). Opposite observations occur in term of global recurrence, which is supposed to model global sequential information and thus has larger effect on the intent detection (row 4 vs. row 0). Effects of Pre-trained Language Models Recently, there has been a growing body of works exploring neural language models that trained on massive corpora to learn contextual representations (e.g. BERT (2018) and EMLo ( 2018 )). Inspired by the effectiveness of language model embeddings, we conduct experiments by leveraging the BERT as an additional feature. The results emerged in Table 5 show that we establish new state-of-the-art results on both tasks of the SNIPS. Evaluation on the CAIS We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture (Huang et al., 2015) for sequence labeling task, and the other one is the more powerful sententce-state LSTM (Zhang et al., 2018b) . The results listed in Table 6 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages. Related Work Memory Network Memory network is a general machine learning framework introduced by Weston et al. (2014) , which have been shown effective in question answering (Weston et al., 2014; Sukhbaatar et al., 2015) , machine translation (Wang et al., 2016a; Feng et al., 2017) , aspect level sentiment classification (Tang et al., 2016) , etc. For spoken language understanding, Chen et al. (2016) introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the se-mantic correlations between slots and the intent in a given utterance, and devise a novel collaborative retrieval approach. Interactions between slots and intents Considering the semantic proximity between slots and intents, some works propose to enhance the slot filling task unidirectionally with the guidance of intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018) . Intuitively, the slot representations are also instructive to the intent detection task and thus bidirectional interactions between slots and intents are benefical for each other. Zhang et al. (2018a) propose a hierarchical capsule network to perform interactions from words to slots, slots to intents and intents to words in a pipeline manner, which is relatively limited in capturing the complicated correlations among them. In our CM-Net, information exchanges are performed simultaneously with knowledge diffusions in both directions. The experiments demonstrate the superiority of our CM-Net in capturing the semantic correlations between slots and intents. Sentence-State LSTM Zhang et al. 2018b propose a novel graph RNN named S-LSTM, which models sentence between words simultaneously. Inspired by the new perspective of state transition in the S-LSTM, we further extend it with task-specific (i.e., slots and intents) representations via our collaborative memories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. Conclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Experiments on two standard benchmarks and our CAIS corpus demonstrate the effectiveness and generalizability of our proposed CM-Net. In addition, we contribute the new corpus (CAIS) to the research community. Acknowledgments Liu, Chen and Xu are supported by the National Natural Science Foundation of China (Contract  61370130, 61976015, 61473294 and 61876198), and the Beijing Municipal Natural Science Foundation (Contract 4172047), and the International Science and Technology Cooperation Program of the Ministry of Science and Technology (K11F100010). We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions.",
    "abstract": "Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize cooccurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community 1 .",
    "countries": [
        "China"
    ],
    "languages": [
        "Chinese"
    ],
    "numcitedby": "32",
    "year": "2019",
    "month": "November",
    "title": "{CM}-Net: A Novel Collaborative Memory Network for Spoken Language Understanding"
}