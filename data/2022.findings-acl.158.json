{
    "article": "In order to equip NLP systems with 'selective prediction' capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-ofdomain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities. Introduction Despite impressive progress made in Natural Language Processing (NLP), it is unreasonable to expect models to be perfect in their predictions. They often make incorrect predictions, especially when inputs tend to diverge from their training data distribution (Elsahar and Gall\u00e9, 2019; Miller et al., 2020; Koh et al., 2021) . While this is acceptable for tolerant applications like movie recommendations, high risk associated with incorrect predictions hinders the adoption of these systems in realworld safety-critical domains like biomedical and autonomous robots. In such scenarios, selective prediction becomes crucial as it allows maintaining high accuracy by abstaining on instances where error is likely. Selective Prediction (SP) has been studied in machine learning (Chow, 1957; El-Yaniv et al., 2010) and computer vision (Geifman and El-Yaniv, 2017, 2019) , but has only recently gained attention in NLP. Kamath et al. (2020) proposed a posthoc calibration-based SP technique for Question-Answering (QA) datasets. Garg and Moschitti (2021) distill the QA model to filter out error-prone questions. Unfortunately, despite the shared goal of making NLP systems robust and reliable for real-world applications, SP has remained underexplored; the community does not know which techniques work best across tasks/settings or even if they consistently outperform the simplest baseline MaxProb (Hendrycks and Gimpel, 2017) (that uses a threshold over the maximum softmax probability for selective prediction). In this work, we address the above point and study selective prediction in a large-scale setup of 17 datasets spanning over Natural Language Inference (NLI), Duplicate Detection, and QA tasks. Our comprehensive experiments under In-Domain (IID), Out-Of-Domain (OOD), and Adversarial (ADV) settings result in the following findings: 1. None of the existing SP approaches consistently and considerably outperforms MaxProb. Slight improvement in IID: Most of the approaches outperform MaxProb in the IID setting; however, the magnitude of improvement is very small (Figure 1 ). For instance, MCD achieves an average improvement of just 0.28 on AUC value across all NLI datasets. Negligible improvement in OOD: The magnitude of improvement in OOD is even lesser (0.08) than that observed in the IID (Figure 2a ). In a few cases, we also observe performance degradation (higher AUC than MaxProb). Performance degradation in ADV: Most of the approaches fail to even match the MaxProb's performance in ADV setting (Figure 2b ). For instance, MCD degrades the AUC value by 1.76 on Duplicate Detection datasets and Calibration degrades by 1.27 on NLI datasets. Approaches do not translate well across tasks: We find that a single approach does not achieve the best performance across all tasks. For instance, MCD outperforms all other approaches on Duplicate Detection datasets but does not fare well on the NLI datasets. 3. Existing approaches fail to outperform Max-Prob despite leveraging additional resources: MCD requires additional computation (for multiple inferences) while calibration-based approaches require a held-out dataset. In contrast, MaxProb does not require any such resources and still outperforms them, especially in the ADV setting. Overall, our results highlight that there is a need to develop stronger selective prediction approaches that perform well across tasks while being computationally efficient. Selective Prediction Formulation A selective prediction system comprises of a predictor (f ) that gives the model's prediction on an input (x), and a selector (g) that determines if the system should output the prediction made by f i.e. (f, g)(x) = f (x), if g(x) = 1 Abstain, if g(x) = 0 Usually, g comprises of a confidence estimator g that indicates f \u2032 s prediction confidence and a threshold th that controls the abstention level: g(x) = 1[g(x)) > th] An SP system makes trade-offs between coverage and risk. For a dataset D, coverage at a threshold th is defined as the fraction of total instances answered by the system (where g > th) and risk is the error on the answered instances: coverage th = x i \u2208D 1[g(x i )) > th] |D| risk th = x i \u2208D 1[g(x i )) > th]l i x i \u2208D 1[g(x i )) > th] where, l i is the error on instance x i . With decrease in th, coverage will increase, but the risk will usually also increase. The overall SP performance is measured by the area under Risk-Coverage curve (El-Yaniv et al., 2010) which plots risk against coverage for all threshold values. Lower the AUC, the better the SP system as it represents lower average risk across all confidence thresholds. We note that 'confidence calibration' and 'OOD detection' are related tasks but are nontrivially different from selective prediction as detailed in Appendix A. Approaches Usually, the last layer of models has a softmax activation function that gives the probability distribution P (y) over all possible answer candidates Y . Y is the set of labels for classification tasks, answer options for multiple-choice QA, all input tokens (for start and end logits) for extractive QA, and all vocabulary tokens for generative tasks. Thus, predictor f is defined as: argmax y\u2208Y P (y) Maximum Softmax Probability (MaxProb): Hendrycks and Gimpel (2017) introduced a simple method that uses the maximum softmax probability across all answer candidates as the confidence estimator g i.e. max y\u2208Y P (y) Monte-Carlo Dropout (MCD): Gal and Ghahramani ( 2016 ) proposed to infer a test input multiple times using different dropout masks and ensemble them to get the confidence estimate. Label Smoothing (LS): Szegedy et al. (2016) proposed to compute cross-entropy loss value with a weighted mixture of target labels during training instead of one hot 'hard' label. This prevents the network from becoming over-confident in its predictions. Calibration (Calib): In calibration, a held-out dataset is annotated conditioned on the correctness of the model's predictions (correct as 'positive' class and incorrect as 'negative' class), and another model (calibrator) is trained on this annotated binary classification dataset. Softmax probability assigned to the positive class by this trained calibrator is used as the confidence estimator for SP. Kamath et al. (2020) study a calibration-based SP technique for Question Answering datasets. They train a random forest model using features such as input text length and probabilities of top 5 predictions and use it as a calibrator. We refer to this approach as Calib C. Inspired by the calibration technique presented in Jiang et al. ( 2021 ), we also train calibrator as a regression model (Calib R) by annotating the heldout instances on a continuous scale instead of categorical labels 'positive' and 'negative' (unlike the annotation done in Calib C). We compute these annotations using MaxProb as: QA: We train with SQuAD (Rajpurkar et al., 2016) and evaluate on NewsQA (Trischler et al., 2017) , TriviaQA (Joshi et al., 2017) , SearchQA (Dunn et al., 2017) , HotpotQA (Yang et al., 2018) , and Natural Questions (Kwiatkowski et al., 2019) . s = 0.5 + maxP rob 2 , if correct 0.5 \u2212 maxP rob Training Details: We run all our experiments using bert-base model (Devlin et al., 2019) with batch size of 32 and learning rate ranging in {1\u22125}e\u22125. All experiments are done with Nvidia V100 16GB GPUs. Calibration: For calibrating QA models, we use input length, predicted answer length, and softmax probabilities of top 5 predictions as the features (similar to Kamath et al. (2020) ). For calibrating NLI and Duplicate Detection models, we use input lengths (of premise/sentence1 and hypothesis/sentence2), softmax probabilities assigned to the labels, and the predicted label as the features. We train calibrators using random forest implementations of Scikit-learn (Pedregosa et al., 2011) for Calib C and Calib R approaches, and train a bert-base model for Calib T. In all calibration approaches, we calibrate using the IID held-out dataset and use softmax probability assigned to the positive class as the confidence estimate for SP. Label Smoothing: For LS, we use MaxProb of the model trained with label smoothing as the confidence estimator for SP. To the best of our knowledge, LS is designed for classification tasks only. Hence, we do not evaluate it for QA tasks. Results and Analysis Slight Improvement in IID We compare the selective prediction performance of various approaches in the IID setting in Figure 1 . Though all the approaches except Calib T outperform MaxProb in most cases, the magnitude of improvement is very small. For instance, MCD achieves an average AUC improvement of just 0.28 across all NLI datasets. Calib C and Calib R achieve the highest improvement on DNLI: We find that these approaches benefit from using the predicted label as a feature for calibration. Specifically, the model's prediction accuracy varies greatly across labels (0.94, 0.91, and 0.76 for entailment, contradiction, and neutral predictions respectively). This implies when the model predicts the label to be neutral, it is relatively less likely to be correct as compared to the scenario when the prediction is entailment or contradiction. Calib C and R approaches leverage this signal by training a calibrator over a held-out dataset and thus achieve superior SP performance. Negligible Improvement / Degradation in OOD and ADV In Figure 2 , we compare the selective prediction performance of various approaches in OOD and ADV settings. To highlight the general trend, the results have been averaged over all the task-specific OOD/ADV datasets mentioned in Section 3. Individual scores are provided in Appendix. In OOD setting, we find that the approaches lead to a negligible improvement in AUC. Notable improvement is achieved only by MCD in the case of the QQP dataset. In the ADV setting, all approaches degrade SP performance. Surprisingly, MCD that performed relatively well in IID and OOD settings, degrades more (by 1.74 AUC) in comparison to other approaches (except Calib T which does not perform well in all three settings). This is because the individual models of the ensemble achieve poor prediction accuracy in the ADV setting and thus ensembling them further degrades the overall confidence estimate. Calib T Degrades Performance Calib C and Calib R slightly outperform MaxProb in most IID and OOD cases. However, Calib T considerably degrades the performance in nearly all the cases. We hypothesize that associating correctness directly with the input text embeddings could be a harder challenge for the model as embeddings of correct and incorrect instances usually do not differ significantly. In contrast, as discussed before, providing features such as predicted label and softmax probabilities explicitly assists Calib C and R approaches in finding some distinguishing patterns that improve the selective prediction performance. Existing Approaches Fail to Utilize Additional Resources Unlike typical ensembling, MCD does not require training or storing multiple models but, it requires making multiple inferences (using different dropout masks) and can still become practically infeasible for large models such as BERT as their inference cost is high. Calibration-based approaches need additional held-out data and careful feature engineering to train the calibrator. Despite being computationally expensive, these approaches fail to consistently outperform MaxProb that does not require any such additional resources. Effect of Increasing Dropout Masks in Monte-Carlo Dropout With the increase in number of dropout masks used in MCD, the SP performance improves (from MCD lite with 10 masks to MCD with 30 masks). This is due to the ensembling effect as combining more predictions on the same input results in a more accurate overall output. However, we note that both MCD lite and MCD degrade SP performance in the ADV setting as discussed in 4.2. No Clear Winner None of the approaches consistently and considerably outperforms MaxProb in all three settings. Most approaches do not fare well in OOD and ADV settings. Furthermore, a single approach does not achieve the highest performance across all tasks. For instance, MCD outperforms all other approaches on Duplicate Detection datasets but does not perform well on NLI datasets (Calib C achieves better performance, especially in the OOD setting). This reveals that the existing selective prediction approaches do not translate well across tasks. Conclusion Selective prediction ability is crucial for NLP systems to be reliably deployed in real-world applications and we presented the most systematic study of existing selective prediction approaches. Our study involved experiments in IID, OOD, and ADV settings with 17 datasets across several NLP tasks. We showed that despite leveraging additional resources (held-out data/computation), existing approaches fail to consistently and considerably outperform the simplest baseline (MaxProb). Furthermore, we demonstrated that these approaches do not translate well across tasks. Overall, our results highlight that there is a need to develop stronger selective prediction approaches that perform well across multiple tasks (QA, NLI, etc.) and settings (IID, OOD, and ADV) while being resource-efficient. D MaxProb for Selective Prediction Figure 3a shows the trend of accuracy against max-Prob for various models in the IID setting. It can be observed that with the increase in MaxProb the accuracy usually increases. This implies that a higher value of MaxProb corresponds to more likelihood of the model's prediction being correct. Hence, MaxProb can be directly used as the confidence estimator for selective prediction. We plot the riskcoverage curves using MaxProb as the SP technique in Figure 3b . As expected, the risk increases with the increase in coverage for all the models. We plot such curves for all techniques and compute area under them to compare their SP performance. This shows that MaxProb is a simple yet strong baseline for selective prediction.    We compare the risk-coverage curves of MCD and Calib C approaches on DNLI in Figure 4 . We observe that at all coverage points, Calib C achieves lower risk than MCD and hence is a better SP technique. We find that they benefit from using the predicted label as a feature for calibration. Specifically, the model's prediction accuracy varies greatly across labels (0.94, 0.91, and 0.76 for entailment, contradiction, and neutral labels respectively). This implies that when the model's prediction is neutral, it is relatively less likely to be correct (at least in the IID setting). Calib C and R approaches leverage this signal and tune the confidence estimator using a held-out dataset and thus achieve superior SP performance. F Composite SP Approach: We note that calibration techniques can be used in combination with Monte-Carlo dropout to further improve the SP performance. However, it would require even more additional resources i.e held-out datasets in addition to multiple inferences. Acknowledgements We thank the anonymous reviewers for their insightful feedback. This research was supported by DARPA SAIL-ON and DARPA CHESS programs. Appendix A Related Tasks A.1 Confidence Calibration Selective Prediction is closely related to confidence calibration (Platt et al., 1999) i.e aligning model's output probability with the true probability of its predictions. Calibration focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples i.e systems are judged on their ability to rank correct predictions higher than incorrect predictions. A.2 Out-of-Domain Detection Using OOD Detection systems for selective prediction (abstain on all detected OOD instances) would be too conservative as it has been shown that models are able to correctly answer a significant fraction of OOD instances (Talmor and Berant, 2019; Hendrycks et al., 2020; Mishra et al., 2020) . B Why Lower AUC is Better? Small magnitude values of area under curve (AUC) are preferred as they represent low average risk across all confidence thresholds.",
    "funding": {
        "defense": 1.0,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 0.0
    },
    "reasoning": "Reasoning: The acknowledgements section of the article mentions that the research was supported by DARPA SAIL-ON and DARPA CHESS programs. DARPA (Defense Advanced Research Projects Agency) is a branch of the United States Department of Defense, which classifies it under defense funding. There is no mention of corporate, research agency, foundation funding, or an absence of funding."
}