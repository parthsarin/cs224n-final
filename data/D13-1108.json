{
    "article": "We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006) , and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011) . These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and se-quences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011) , exhibit better capability of long distance reorderings. In this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal nodes, and the target side as strings. A head-dependents relation (Xie et al., 2011) is composed of a head and all its dependents in dependency trees, and it encodes phrase pattern and sentence pattern (typically long distance reordering relations). With the advantages of head-dependents relations, the translation rules of our model hold the property of long distance reorderings and the compatibility with phrases. Our new model (Section 2) extracts rules from word-aligned pairs of source trees (constituency and dependency) and target strings (Section 3), and translate source trees into target strings by employing a bottom-up chart-based algorithm (Section 4). Compared with the constituency-to-string (Liu et al., 2006) and dependency-to-string (Xie et al., 2011) models that only employ a single type of trees, our approach yields encouraging results by exploiting two types of trees. Large-scale experiments (Section 5) on Chinese-English translation show that our model significantly outperforms the state-ofthe-art single constituency-to-string model by averaged +2.45 BLEU points, dependency-to-string model by averaged +0.91 BLEU points, and hierarchical phrase-based model (Chiang, 2005) by averaged +1.12 BLEU points, on three Chinese-English NIST test sets. Grammar We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. A headdependents relation consists of a head and all its dependents in dependency trees, and it can represent long distance dependencies. Incorporating phrasal nodes of constituency trees into head-dependents relations further enhances the compatibility with phrases of our rules. Figure 1 shows an example of phrases which can not be captured by a dependency tree while captured by a constituency tree, such as the bold phrasal nodes NP 1 ,VP 2 and VP 3 . The phrasal node NP 1 in the constituency tree indicates that \" \u00b5\u00c8 \" is a noun phrase and it should be translated as a basic unit, while in the dependency tree it is a non-syntactic phrase. The headdependents relation in the top level of the dependency tree presents long distance dependencies of the words \" \", \" \", \" \", and \"\u00b5\u00c8 \" in a concise manner, which is useful for long distance reordering. We adopt this kind of rule representation to hold the property of long distance reorderings and the compatibility with phrases. Figure 2 shows two examples of our translation rules corresponding to the top level of Figure 1-(b) . We can see that r 1 captures a head-dependents relation, while r 2 extends r 1 by incorporating a phrasal node VP 2 to replace the two nodes \" /VV\" and \"\u00b5\u00c8 /NN\". As shown in Figure 1-(b) , VP 2 consists of two parts, a head node \" /VV\" and a subtree rooted at the dependent node \"\u00b5\u00c8 /NN\". Therefore, we use VP 2 and the POS tags of the two nodes VV and NN to denote the part covered by VP 2 in r 2 , to indicate that the source sequence covered by VP 2 can be translated by a bilingual phrase. Since VP 2 covers a head node \" /VV\", we represent r 2 by constructing a new head node VP 2 |||VV NN. For simplicity, we use a shorten form CHDR to represent the head-dependents relations with/without constituency phrasal nodes. Formally, our grammar G is defined as a 5-tuple G = \u03a3, N c , N d , \u2206, R , where \u03a3 is a set of source language terminals, N c is a set of constituency phrasal categories, N d is a set of categories (POS tags) for the terminals in \u03a3, \u2206 is a set of target language terminals, and R is a set of translation rules that include bilingual phrases for translating source language terminals and CHDR rules for translation and reordering. A CHDR rule is represented as a triple t, s, \u223c , where: \u2022 t is CHDR with each node labeled by a terminal from \u03a3 or a variable from a set X = {x 1 , x 2 , \u2022 \u2022 \u2022 } constrained by a terminal from \u03a3 or a category from N d or a joint category (constructed by the categories from N c and N d ); \u2022 s \u2208 (X \u222a \u2206) denotes the target side string; \u2022 \u223c denotes one-to-one links between nonterminals in t and variables in s. We use the lexicon dependency grammar (Hellwig, 2006) t = ( ) ( ) x 1 :VP 2 |||VV NN s = Intel will x 1 \u223c= x 1 :VP 2 |||VV NN \u2194 x 1 where the underline indicates a leaf node. Figure 3 gives an example of the translation derivation in our model, with the translation rules listed in (g). r 3 , r 6 and r 8 are CHDR rules, while r 4 , r 5 and r 7 are bilingual phrases, which are used for translating source language terminals. Given a sentence to translate in (a), we first parse it into a constituency tree and a dependency tree, then label the phrasal nodes from the constituency tree to the dependency tree, and yield (b). Then, we translate it into a target string by the following steps. At the root node, we apply rule r 3 to translate the top level head-dependents relation and results in four unfinished substructures and target strings in (c). From (c) to (d), there are three steps (one rule for one step). We use r 4 to translate \" \" to \"Intel\", r 5 to translate \" \" to \"will\", and r 6 to translate the rightmost unfinished part. Then, we apply r 7 to translate the phrase \" \u00b5\u00c8 \" to \"Ultrabook\", and yield (e). Finally, we apply r 8 to translate the last fragment to \"the first\", and get the final result (f). Rule Extraction In this section, we describe how to extract rules from a set of 4-tuples C, T, S, A , where C is a source constituency tree, T is a source dependency tree, S is a target side sentence, and A is an word alignment relation between T /C and S. We extract CHDR rules from each 4-tuple C, T, S, A based on GHK-M algorithm (Galley et al., 2004) with three steps: 1. Label the dependency tree with phrasal nodes from the constituency tree, and annotate alignment information to the phrasal nodes labeled dependency tree (Section 3.1). 2. Identify acceptable CHDR fragments from the annotated dependency tree for rule induction (Section 3.2). 3. Induce a set of lexicalized and generalized CHDR rules from the acceptable fragments (Section 3.3). Annotation Given a 4-tuple C, T, S, A , we first label phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the common covered source sequences. As dependency trees can capture some phrasal information by dependency syntactic /VV {3-3}{1-8} /NR {1-1}{1-1} /AD {2-2}{2-2} /NN {6-6}{4-8} /NR {7-8}{7-8} /M {null}{4-5} /JJ {6-6}{6-6} /OD {4-5}{4-5} NP1 <6-6> VP2 <3-8> VP3 <2-8> Figure 4 : An annotated dependency tree. Each node is annotated with two spans, the former is node span and the latter subtree span. The fragments covered by phrasal nodes are annotated with phrasal spans. The nodes denoted by the solid line box are not nsp consistent. phrases, in order to complement the information that dependency trees can not capture, we only label the phrasal nodes that cover dependency non-syntactic phrases. Then, we annotate alignment information to the phrasal nodes labeled dependency tree T , as shown in Figure 4 . For description convenience, we make use of the notion of spans (Fox, 2002; Lin, 2004) . Given a node n in the source phrasal nodes labeled T with word alignment information, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. As shown in Figure 4 , we annotate each node n of phrasal nodes labeled T with two attributes: node span and subtree span; besides, we annotate phrasal span to the parts covered by phrasal nodes in each subtree rooted at n. The three types of spans are defined as follows: Definition 1 Given a node n, its node span nsp(n) is the consecutive target word sequence aligned with the node n. Take the node \" /NR\" in Figure 4 for example, nsp( /NR)={7-8}, which corresponds to the target words \"in\" and \"Asia\". Definition 2 Given a subtree T \u2032 rooted at n, the subtree span tsp(n) of n is the consecutive target word sequence from the lower bound of the nsp of all nodes in T \u2032 to the upper bound of the same set of spans. For instance, tsp(\u00b5\u00c8 /NN)={4-8}, which corresponds to the target words \"the first Ultrabook in Asia\", whose indexes are from 4 to 8. Definition 3 Given a fragment f covered by a phrasal node, the phrasal span psp(f ) of f is the consecutive target word sequence aligned with source string covered by f . For example, psp(VP 2 )= 3-8 , which corresponds to the target word sequence \"launch the first Ultrabook in Asia\". We say nsp, tsp and psp are consistent according to the notion in the phrase-based model (Koehn et al., 2003) . For example, nsp( /NR), tsp(\u00b5\u00c8 /NN) and psp(NP 1 ) are consistent while nsp( /JJ) and nsp(\u00b5\u00c8 /NN) are not consistent. The annotation can be achieved by a single postorder transversal of the phrasal nodes labeled dependency tree. For simplicity, we call the annotated phrasal nodes labeled dependency tree annotated dependency tree. The extraction of bilingual phrases (including the translation of head node, dependency syntactic phrases and the fragment covered by a phrasal node) can be readily achieved by the algorithm described in Koehn et al., (2003) . In the following, we focus on CHDR rules extraction. Acceptable Fragments Identification Before present the method of acceptable fragments identification, we give a brief description of CHDR fragments. A CHDR fragment is an annotated fragment that consists of a source head-dependents relation with/without constituency phrasal nodes, a target string and the word alignment information between the source and target side. We identify the acceptable CHDR fragments that are suitable for rule induction from the annotated dependency tree. We divide the acceptable CHDR fragments into two categories depending on whether the fragments contain phrasal nodes. If an acceptable CHDR fragment does not contain phrasal nodes, we call it CHDR-normal fragment, otherwise CHDR-phrasal fragment. Given a CHDR fragment F rooted at n, we say F is acceptable if it satisfies any one of the following properties: 1. Without phrasal nodes, the node span of the root n is consistent and the subtree spans of n's all dependents are consistent. For example, Figure 5 -(a) shows a CHDR-normal fragment that identified from the top level of the annotated dependency tree in Figure 4 , since the nsp( /VV), tsp( /NR), tsp( /AD) and tsp(\u00b5\u00c8 /NN) are consistent. 2. With phrasal nodes, the phrasal spans of phrasal nodes are consistent; and for the other nodes, the node span of head (if it is not covered by any phrasal node) is consistent, and the subtree spans of dependents are consistent. For instance, Figure 5 The identification of acceptable fragments can be achieved by a single postorder transversal of the annotated dependency tree. Typically, each acceptable fragment contains at most three types of nodes: head node, head of the related CHDR; internal nodes, internal nodes of the related CHDR except head node; leaf nodes, leaf nodes of the related CHDR. Rule Induction From each acceptable CHDR fragment, we induce a set of lexicalized and generalized CHDR rules. We induce CHDR-normal rules and CHDR-phrasal rules from CHDR-normal fragments and CHDRphrasal fragments, respectively. We first induce a lexicalized form of CHDR rule from an acceptable CHDR fragment: 1. For a CHDR-normal fragment, we first mark the internal nodes as substitution sites. This forms the input of a CHDR-normal rule. Then we generate the target string according to the node span of the head and the subtree spans of the dependents, and turn the word sequences covered by the internal nodes into variables. This forms the output of a lexicalized CHDRnormal rule. 2. For a CHDR-phrasal fragment, we first mark the internal nodes and the phrasal nodes as substitution sites. This forms the input of a CHDRphrasal rule. Then we construct the output of the CHDR-phrasal rule in almost the same way with constructing CHDR-normal rules, except that we replace the target sequences covered by the internal nodes and the phrasal nodes with variables. For example, rule r 1 in Figure 5 As we can see, these CHDR-phrasal rules are partially unlexicalized. To alleviate the sparseness problem, we generalize the lexicalized CHDR-normal rules and partially unlexicalized CHDR-phrasal rules with unlexicalized nodes by the method proposed in Xie et al., (2011) . As the modification relations between head and dependents are determined by the edges, we can replace the lexical word of each node with its category (POS tag) and obtain new head-dependents relations with unlexicalized nodes keeping the same modification relations. We generalize the rule by simultaneously turn the nodes of the same type (head, internal, leaf) into their categories. For example, CHDR-normal rules r 2 \u223c r 7 are generalized from r 1 in Figure 5-(d) . Besides, r 10 and r 12 are the corresponding generalized CHDRphrasal rules. Actually, our CHDR rules are the superset of head-dependents relation rules in Xie et al., (2011) . CHDR-normal rules are equivalent with the head-dependents relation rules and the CHDRphrasal rules are the extension of these rules. For convenience of description, we use the subscript to distinguish the phrasal nodes with the same category, such as VP 2 and VP 3 . In actual operation, we use VP instead of VP 2 and VP 3 . We handle the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004) . During this process, we might obtain m(m \u2265 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). Decoding and the Model Following Och and Ney, ( 2002 ), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: P (d) \u221d i \u03c6 i (d) \u03bb i (1) where \u03c6 i are features defined on derivations and \u03bb i are feature weights. In our experiments of this paper, the features are used as follows: \u2022 CHDR rules translation probabilities P (t|s) and P (s|t), and CHDR rules lexical translation probabilities P lex (t|s) and P lex (s|t); \u2022 bilingual phrases translation probabilities P bp (t|s) and P bp (s|t), and bilingual phrases lexical translation probabilities P bplex (t|s) and P bplex (s|t); \u2022 rule penalty exp(\u22121); \u2022 pseudo translation rule penalty exp(\u22121); \u2022 target word penalty exp(|e|); \u2022 language model P lm (e). We have twelve features in our model. The values of the first four features are accumulated on the CHDR rules and the next four features are accumulated on the bilingual phrases. We also use a pseudo translation rule (constructed according to the word order of head-dependents relation) as a feature to guarantee the complete translation when no matched rules can be found during decoding. Our decoder is based on bottom-up chart-based algorithm. It finds the best derivation that convert the input phrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than \u03b2 times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subset of the candidates. This can also be done by a fixed limit (stack-limit) and a threshold (stack-threshold). Experiments We evaluated the performance of our model by comparing with hierarchical phrase-based model (Chiang, 2007) , constituency-to-string model (Liu et al., 2006) and dependency-to-string model (Xie et al., 2011) on Chinese-English translation. First, we describe data preparation (Section 5.1) and systems (Section 5.2). Then, we validate that our model significantly outperforms all the other baseline models (Section 5.3). Finally, we give detail analysis (Section 5.4). Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2 . We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002) . The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the \"grow-diag-finaland\" balance strategy (Koehn et al., 2003) . We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) The \"+\" denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The \"*\" denotes that the results are significantly better than all the other systems (p<0.01). language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system's BLEU score on the set. The statistical significance test is performed by sign-test (Collins et al., 2005) . Systems We take the open source hierarchical phrase-based system Moses-chart (with default configuration), our in-house constituency-to-string system cons2str and dependency-to-string system dep2str as our baseline systems. For cons2str, we follow Liu et al., (Liu et al., 2006) to strict that the height of a rule tree is no greater than 3 and phrase length is no greater than 7. To keep consistent with our proposed model, we implement the dependency-to-string model (Xie et al., 2011) with GHKM (Galley et al., 2004) rule extraction algorithm and utilize bilingual phrases to translate source head node and dependency syntactic phrases. Our dep2str shows comparable performance with Xie et al., (2011) , which can be seen by comparing with the results of hierarchical phrase-based model in our experiments. For dep2str and our proposed model consdep2str, we set rulethreshold and stack-threshold to 10 \u22123 , rule-limit to 100, stack-limit to 300, and phrase length limit to 7. Experimental Results Table 1 illustrates the translation results of our experiments. As we can see, our consdep2str system has gained the best results on all test sets, with +1.12 BLEU points higher than Moses-chart, +2.45 BLEU points higher than cons2str, and +0.91 BLEU points higher than dep2str, averagely on MT03, MT04 and MT05. Our model significantly outper-forms all the other baseline models, with p<0.01 on statistical significance test sign-test (Collins et al., 2005) . By exploiting two types of trees on source side, our model gains significant improvements over constituency-to-string and dependencyto-string models, which employ single type of trees. Table 1 also lists the statistical results of rules extracted from training data by different systems. According to our statistics, the number of rules extracted by our consdep2str system is about 18.88% larger than dep2str, without regard to the 32.5M bilingual phrases. The extra rules are CHDR-phrasal rules, which can bring in BLEU improvements by enhancing the compatibility with phrases. We will conduct a deep analysis in the next sub-section. Analysis In this section, we first illustrate the influence of CHDR-phrasal rules in our consdep2str model. We calculate the proportion of 1-best translations in test sets that employ CHDR-phrasal rules, and we call this proportion \"CHDR-phrasal Sent.\". Besides, the proportion of CHDR-phrasal rules in all CHDR rules is calculated in these translations, and we call this proportion \"CHDR-phrasal Rule\". Table 2 lists the using of CHDR-phrasal rules on test sets, showing that CHDR-phrasal Sent. on all test sets are higher than 50%, and CHDR-phrasal Rule on all three test sets are higher than 10%. These results indicate that CHDR-phrasal rules do play a role in decoding. Furthermore, we compare some actual translations of our test sets generated by cons2str, de-p2str and consdep2str systems, as shown in  phrase+verb+noun\". Cons2str gives a bad result with wrong global reordering, while our consdep2str system gains an almost correct result since we capture this pattern by CHDR-normal rules. In the second example, we can see that the Chinese phrase \"\u00be \u00dd\" is a non-syntactic phrase in the dependency tree, and this phrase can not be captured by head-dependents relation rules in Xie et al., (2011) , thus can not be translated as one unit. Since we encode constituency phrasal nodes to the dependency tree, \"\u00be \u00dd\" is labeled by a phrasal node \"VP\" (means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result \"reemergence\" with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012) . Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011) . These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005) . Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constituency and dependency trees, while their work applied two types of trees on two sides. Instead, our model simultaneously utilizes constituency and dependency trees on the source side to direct the translation, which is concerned with combining the advantages of two types of trees in translation rules to advance the state-of-the-art machine translation. Conclusion In this paper, we present a novel model that simultaneously utilizes constituency and dependency trees on the source side to direct the translation. To combine the merits of constituency and dependency trees, our model employs head-dependents relations incorporating with constituency phrasal nodes. Experimental results show that our model exhibits good performance and significantly outperforms the state-of-the-art constituency-to-string, dependencyto-string and hierarchical phrase-based models. For the first time, source side constituency and dependency trees are simultaneously utilized to direct the translation, and the model surpasses the state-of-theart translation models. Since constituency tree binarization can lead to more constituency-to-string rules and syntactic phrases in rule extraction and decoding, which improve the performance of constituency-to-string systems, for future work, we would like to do research on encoding binarized constituency trees to dependency trees to improve translation performance. Acknowledgments The authors were supported by National Natural Science Foundation of China (Contracts 61202216), MT05 ----segment 448 cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops . consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops . reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.",
    "funding": {
        "defense": 1.9361263126072004e-07,
        "corporate": 0.0,
        "research agency": 0.0,
        "foundation": 0.0,
        "none": 1.0
    },
    "reasoning": "Reasoning: The article does not mention any specific funding sources, including defense, corporate, research agencies, foundations, or any other type of financial support for the research presented.",
    "abstract": "We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.",
    "countries": [
        "China",
        "Ireland"
    ],
    "languages": [
        "Chinese",
        "English"
    ],
    "numcitedby": 14,
    "year": 2013,
    "month": "October",
    "title": "Translation with Source Constituency and Dependency Trees",
    "values": {
        "novelty": " We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees.   Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal nodes, and the target side as strings.   Our new model (Section 2) extracts rules from word-aligned pairs of source trees (constituency and dependency) and target strings (Section 3), and translate source trees into target strings by employing a bottom-up chart-based algorithm (Section 4).   Compared with the constituency-to-string (Liu et al., 2006) and dependency-to-string (Xie et al., 2011) models that only employ a single type of trees, our approach yields encouraging results by exploiting two types of trees.   Since constituency tree binarization can lead to more constituency-to-string rules and syntactic phrases in rule extraction and decoding, which improve the performance of constituency-to-string systems, for future work, we would like to do research on encoding binarized constituency trees to dependency trees to improve translation performance. ",
        "performance": "Our model significantly outper-forms all the other baseline models, with p<0.01 on statistical significance test sign-test (Collins et al., 2005) . By exploiting two types of trees on source side, our model gains significant improvements over constituency-to-string and dependencyto-string models, which employ single type of trees. CHDR-phrasal Sent. on all test sets are higher than 50%, and CHDR-phrasal Rule on all three test sets are higher than 10%. These results indicate that CHDR-phrasal rules do play a role in decoding. For the first time, source side constituency and dependency trees are simultaneously utilized to direct the translation, and the model surpasses the state-of-theart translation models."
    }
}